{"id": "Uh8NGna3VE", "number": 7124, "cdate": 1758008695878, "mdate": 1763172962081, "content": {"title": "Background Matters: Robust 3D Human Pose Estimation via Controllable Video Generation", "abstract": "n/a", "tldr": "", "keywords": ["3D Human Pose Estimation", "Domain Generalization", "Video Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d1f4b0ea2f58c44f0da327365750d39e3772b6ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "While the paper is well written and experimentally detailed, it lacks sufficient novelty and fails to demonstrate competitive performance compared to existing methods. The absence of comparisons with strong baselines further weakens the technical contribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper is clearly written and the proposed idea is clearly presented. Overall the paper is easy to follow.\n2) It includes a comprehensive set of experiments."}, "weaknesses": {"value": "1) The idea of using pose guidance for video generation to improve generalizability has been explored in prior work (e.g., PoseSyn [1]). The novelty of this paper is therefore limited.\n\n2) The reported results fall significantly behind SOTA performance. For instance, on the 3DHP dataset, PersPose [2] (ICCV 2025) achieves less than 75 MPJPE, while this paper reports 124 MPJPE.\n\n3) No quantitative comparison with relevant prior work, both in:\n\n   - Dataset generation methods: PoseExaminer [3], PoseGen [4], PoseSyn, IDOL [5], AdaptPose [6];\n\n   - SOTA 3D pose estimation models: PersPose, PostoMETRO [7]\n\n\n[1] PoseSyn: Synthesizing Diverse 3D Pose Data from In-the-Wild 2D Data\n[2] PersPose: 3D Human Pose Estimation with Perspective Encoding and Perspective Rotation\n[3] PoseExaminer: Automated Testing of Out-of-Distribution Robustness in Human Pose and Shape Estimation\n[4] PoseGen: Learning to Generate 3D Human Pose Dataset with NeRF\n[5] IDOL: Instant Photorealistic 3D Human Creation from a Single Image\n[6] AdaptPose: Cross-Dataset Adaptation for 3D Human Pose Estimation by Learnable Motion Generation\n[7] PostoMETRO: Pose Token Enhanced Mesh Transformer for Robust 3D Human Mesh Recovery"}, "questions": {"value": "The authors might want to clarify the contributions; compare with related SOTAs; and add more discussions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XZnZG9ZsmS", "forum": "Uh8NGna3VE", "replyto": "Uh8NGna3VE", "signatures": ["ICLR.cc/2026/Conference/Submission7124/Reviewer_TW7p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7124/Reviewer_TW7p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841137990, "cdate": 1761841137990, "tmdate": 1762919291535, "mdate": 1762919291535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "W1nE1MoLAt", "forum": "Uh8NGna3VE", "replyto": "Uh8NGna3VE", "signatures": ["ICLR.cc/2026/Conference/Submission7124/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7124/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763172961388, "cdate": 1763172961388, "tmdate": 1763172961388, "mdate": 1763172961388, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper utilizes controllable video generation for learning to estimate 3D human poses robustly. RGB Video generation is supposed to help in generating diverse 2D human motion sequence through varying poses, scenes, and viewpoints. Besides using real 2D inputs, the idea behind using real-world detections is to design a robust and generalizable pose estimation model. The authors conduct experimental evaluation on various 3DHPE datasets to show the effectiveness of the proposed approach on real world and corrupt 2D inputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper reframes the usual pose-only augmentation approach into a multi-modal data generation pipeline that explicitly models scene diversity in terms of pose, viewpoint, lighting, etc. which helps in building a generalizable 3D human pose estimation model.\n2. The idea of leveraging pose-guided video diffusion models is intuitive and a straightforward implementation should be easily achievable.\n3. Experiments include multiple datasets (H36M, PMR, 3DHP, 3DPW) and diverse metrics (MPJPE, P-MPJPE, velocity error). Moreover, the paper evaluates robustness under real-world corruptions (blur, compression, spatter), which strengthens the practical motivation.\n4. The effects of filtering ratios, pretraining strategies, and GT vs. detected 2D inputs are well-studied. Also, the results seem to demonstrate consistent improvements across nearly all configurations which suggests robustness of the method."}, "weaknesses": {"value": "1. The technical contribution mainly lies in data generation and composition rather than in a novel model or algorithm which I feel is a major discussion point. The method builds on existing diffusion video generation models with minimal architectural innovation.\n2. The paper relies heavily on pretrained models like Animate Anyone (Hu et al.) and Latent Diffusion model (Rombach et al.) without domain-specific adaptation. It is unclear how much of the improvement comes from the inherent realism of these models rather than the proposed pipeline design.\n3. The method is validated primarily on H36M, PMR, and 3DHP which are all captured in a controlled environment. A demonstration on truly unseen in-the-wild datasets (e.g., MPII, COCO-Video, AMASS-based scenes) is missing and would support generalization claims.\n4. One of the concerns is that generating and filtering large-scale video data using diffusion models is resource-intensive. The paper lacks any discussion regarding this, e.g., training time, compute requirements, or efficiency trade-offs compared to simpler augmenters like PoseAug (Zhang et al.). No user or perceptual evaluation is provided for the realism or physical plausibility of generated videos."}, "questions": {"value": "1. What is the precise novelty beyond combining existing diffusion video generation models with pose augmentation?\n2. What is the core mechanism by which background variation improves generalization? The hypothesis is intuitive (“background matters”), but the paper lacks an analysis or visualization showing how added background diversity affects learned representations.\n3. What is the computational overhead of generating and filtering data? Since video diffusion generation is extremely costly, the scalability of this method to larger datasets or real-time adaptation is unclear. Additionally, for a generalizable model, training a video generation model on extensively diverse datasets seems to be crucial but at the same time unscalable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uVxCgENd4K", "forum": "Uh8NGna3VE", "replyto": "Uh8NGna3VE", "signatures": ["ICLR.cc/2026/Conference/Submission7124/Reviewer_WKNr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7124/Reviewer_WKNr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965702401, "cdate": 1761965702401, "tmdate": 1762919291042, "mdate": 1762919291042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes using diffusion models to generate and augment the in-studio dataset to improve the generalization of human pose estimators. Specifically, they argue that 2D-to-3D pose lifting techniques are often trained on in-studio, near-perfect data, whereas in the real world such data are scarce, and artifacts from noise and occlusion can undermine the robustness of pose estimation systems. To address this challenge, the paper introduces a two-stage augmentation pipeline that first trains a controllable video generator (Animate Anyone) on a dataset and then feeds it poses from diverse domains to generate new postures with different backgrounds. The paper supports its claims through extensive experiments, showing that by augmenting and training the models on corrupted/synthesized datasets, results in improved performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-written and easy to follow. It explains the technical details and provides adequate clarification.\n- The cross-dataset analysis clearly shows that by augmenting RGB videos rather than 2D poses, the performance of cross-dataset generalization can increase"}, "weaknesses": {"value": "- Alternatives to video generation are not compared against. For instance, a simple background-pasting algorithm can serve as a straightforward baseline.\n- The computational cost and practicality of the approach are questionable. Training a large video generation model can be much more computationally expensive than rendering a synthetic dataset. Additionally, the paper mentions that 90% of the data is discarded, meaning that the process is highly inefficient and uncontrollable. This limitation and the lack of controllability are not fully addressed in the paper.\n- No other baselines are compared against. For instance, while PoseAug tries to augment the 2D poses, it can be a point of comparison. The paper cites these methods, but does not include them in the comparisons."}, "questions": {"value": "1. Could you please provide a detailed breakdown of the computational cost for 1) training/fine-tuning the video generator, 2) generating the dataset, and 3) training the HPE model? Please provide a comparison with other available approaches cited in the paper on line 58.\n2. Please address my points in the above section, and the point about comparing with PoseAug specifically."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1YpRG8TNmD", "forum": "Uh8NGna3VE", "replyto": "Uh8NGna3VE", "signatures": ["ICLR.cc/2026/Conference/Submission7124/Reviewer_Kei3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7124/Reviewer_Kei3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977766657, "cdate": 1761977766657, "tmdate": 1762919290588, "mdate": 1762919290588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}