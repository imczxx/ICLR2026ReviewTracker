{"id": "ZWwNVIr0PV", "number": 15112, "cdate": 1758247929023, "mdate": 1759897327845, "content": {"title": "DiSA: Diffusion Step Annealing in Autoregressive Image Generation", "abstract": "An increasing number of autoregressive (AR) models, such as MAR, FlowAR, xAR, and Harmon adopt diffusion sampling to improve the quality of image generation. However, this strategy leads to low inference efficiency, because it usually takes 50 to 100 steps for diffusion to sample a token. This paper explores how to effectively address this issue.\nOur key motivation is that as more tokens are generated during the AR process, subsequent tokens follow more constrained distributions and are easier to sample. To intuitively explain, if a model has generated part of a dog, the remaining tokens must complete the dog and thus are more constrained. Empirical evidence supports our motivation: at later generation stages, the next tokens can be well predicted by a multilayer perceptron, exhibit low variance, and follow closer-to-straight-line denoising paths from noise to tokens. \nBased on our finding, we introduce diffusion step annealing (DiSA), a training-free method that gradually uses fewer diffusion steps as more tokens are generated, e.g., using 50 steps at the beginning and gradually decreasing to 5 steps at later stages. Because DiSA is derived from our finding specific to diffusion in AR models, it is complementary to existing acceleration methods designed for diffusion alone. \nDiSA can be implemented in only a few lines of code on existing models, and albeit simple, achieves $5-10\\times$ faster inference for MAR and Harmon and $1.4-2.5\\times$ for FlowAR and xAR, while maintaining the generation quality.", "tldr": "We propose DiSA, a simple training-free method that reduces diffusion steps during autoregressive generation, achieving significantly faster inference while preserving generation quality.", "keywords": ["Autoregressive Image Generation", "Diffusion Model", "Fast Sampling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ccc37f5ffef0bc0f045397d9e6782b5e2caaab0d.pdf", "supplementary_material": "/attachment/dc0706b7ed9cab59cb89a28cb7a179f099e65ef5.pdf"}, "replies": [{"content": {"summary": {"value": "This paper investigates the inefficiency problem in autoregressive image generation models that incorporate diffusion-based sampling (e.g., MAR, FlowAR, xAR, Harmon). These models typically require 50–100 diffusion denoising steps for each token, leading to high inference latency. The paper proposes Diffusion Step Annealing, a training-free inference-time strategy that gradually reduces the number of diffusion steps as more tokens are generated"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a convincing empirical study demonstrating that later AR steps have more constrained distributions.\n2. Experiment results demonstrate consistent speed-ups across four major AR-diffusion models (MAR, FlowAR, xAR, Harmon) with minimal loss in quality.\n3. The paper is well-written, logically structured, concise, and clear, making it easy for readers to understand."}, "weaknesses": {"value": "1. The annealing schedule (linear, cosine, two-stage) and the choice of T_early, T_late are not extensively analyzed. The robustness of these settings across datasets and models could be better demonstrated.\n2. The idea of step annealing has precedents in pure diffusion models (e.g., DDIM, DPM-Solver). The novelty here lies in transferring and validating this principle within autoregressive-diffusion frameworks."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UJ4ZpMBbSH", "forum": "ZWwNVIr0PV", "replyto": "ZWwNVIr0PV", "signatures": ["ICLR.cc/2026/Conference/Submission15112/Reviewer_phup"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15112/Reviewer_phup"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15112/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761431560678, "cdate": 1761431560678, "tmdate": 1762925434249, "mdate": 1762925434249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response to reviewers: robustness and scalability"}, "comment": {"value": "We sincerely thank the reviewers for their insightful and constructive feedback. The reviewers commonly raised two primary concerns regarding DiSA: (1) Sensitivity of DiSA to hyperparameters, including the annealing schedule (linear, cosine, two-stage) and the choice of $T_{early}$ and $T_{late}$; (2) Extension to high-resolution generation. New experiments are detailed in this response to address the two concerns.\n\n> **Sensitivity of DiSA to hyperparameters**\n\n### MAR-B + DiSA (linear)\n\n| Model | AR steps | Diff steps  | FID$\\downarrow$ | IS$\\uparrow$ | Time (s)$\\downarrow$ | Speed-Up$\\uparrow$ |\n| - | - | - | - | - | - | - |\n| MAR-B | 256 | 100 | 2.31 | 281.7 | 0.650 | 1.0$\\times$ |\n| | 64 | 50 | 2.39 (+0.08) | 281.0 (-0.7) | 0.134 | 4.8$\\times$ |\n| +DiSA | 64 | 50$\\rightarrow$25 | 2.31 (+0.00) | 278.9 (-2.8) | 0.126 | 5.2$\\times$ |\n|  | 64 | 50$\\rightarrow$15 | 2.26 (-0.05) | 281.0 (-0.7) | 0.120 | 5.4$\\times$ |\n|  | 64 | 50$\\rightarrow$10 | 2.29 (-0.02) | 279.3 (-2.4) | 0.119 | 5.5$\\times$ |\n|  | 64 | 50$\\rightarrow$5 | 2.31 (+0.00) | 282.3 (+0.6) | 0.114 | 5.7$\\times$ |\n|  | 64 | 25$\\rightarrow$15 | 2.31 (+0.00) | 280.6 (-1.1) | 0.097 | 6.7$\\times$ |\n|  | 64 | 25$\\rightarrow$10 | 2.38 (+0.07) | 283.6 (+1.9) | 0.094 | 6.9$\\times$ |\n|  | 64 | 25$\\rightarrow$5 | 2.38 (+0.07) | 283.3 (+1.6) | 0.090 | 7.2$\\times$ |\n|  | 64 | 15$\\rightarrow$10 | 2.53 (+0.22) | 287.9 (+6.2) | 0.088 | 7.4$\\times$ |\n|  | 64 | 15$\\rightarrow$5 | 2.86 (+0.55) | 283.9 (+2.2) | 0.084 | 7.7$\\times$ |\n| - | - | - | - | - | - | - |\n|  | 32 | 50$\\rightarrow$25 | 2.56 (+0.25) | 272.0 (-9.7) | 0.071 | 9.2$\\times$ |\n|  | 32 | 50$\\rightarrow$15 | 2.47 (+0.16) | 272.0 (-9.7) | 0.067 | 9.7$\\times$ |\n|  | 32 | 50$\\rightarrow$10 | 2.42 (+0.11) | 273.9 (-7.8) | 0.065 | 10.0$\\times$ |\n|  | 32 | 50$\\rightarrow$5 | 2.32 (+0.01) | 279.3 (-2.4) | 0.063 | 10.4$\\times$ |\n|  | 32 | 25$\\rightarrow$15 | 2.45 (+0.14) | 276.3 (-5.4) | 0.053 | 12.3$\\times$ |\n|  | 32 | 25$\\rightarrow$10 | 2.39 (+0.08) | 279.8 (-1.9) | 0.051 | 12.8$\\times$ |\n|  | 32 | 25$\\rightarrow$5 | 2.34 (+0.03) | 285.6 (+3.9) | 0.049 | 13.3$\\times$ |\n|  | 32 | 15$\\rightarrow$10 | 2.44 (+0.13) | 284.3 (+2.6) | 0.048 | 13.6$\\times$ |\n|  | 32 | 15$\\rightarrow$5 | 2.63 (+0.32) | 286.7 (+5.0) | 0.046 | 14.1$\\times$ |\n| - | - | - | - | - | - | - |\n|  | 16 | 50$\\rightarrow$25 | 4.33 (+2.02) | 246.4 (-35.3) | 0.045 | 14.4$\\times$ |\n|  | 16 | 50$\\rightarrow$15 | 4.15 (+1.84) | 247.8 (-33.9) | 0.042 | 15.6$\\times$ |\n|  | 16 | 50$\\rightarrow$10 | 4.01 (+1.70) | 249.5 (-32.2) | 0.040 | 16.3$\\times$ |\n|  | 16 | 50$\\rightarrow$5 | 3.65 (+1.34) | 255.2 (-26.5) | 0.038 | 16.9$\\times$ |\n| - | - | - | - | - | - | - |\n|  | 8 | 50$\\rightarrow$25 | 13.59 (+11.28) | 179.6 (-102.1) | 0.031 | 20.7$\\times$ |\n|  | 8 | 50$\\rightarrow$15 | 13.15 (+10.84) | 182.9 (-98.8) | 0.029 | 22.5$\\times$ |\n|  | 8 | 50$\\rightarrow$10 | 12.62 (+10.31) | 185.8 (-95.9) | 0.027 | 23.8$\\times$ |\n|  | 8 | 50$\\rightarrow$5 | 10.97 (+8.66) | 196.2 (-85.5) | 0.026 | 25.5$\\times$ |\n\n### MAR-B + DiSA (cosine)\n\n| Model | AR steps | Diff steps  | FID$\\downarrow$ | IS$\\uparrow$ | Time (s)$\\downarrow$ | Speed-Up$\\uparrow$ |\n| -- | -- | -- | -- | -- | -- | -- |\n| MAR-B | 256 | 100 | 2.31 | 281.7 | 0.650 | 1.0$\\times$ |\n| | 64 | 50 | 2.39 (+0.08) | 281.0 (-0.7) | 0.134 | 4.8$\\times$ |\n| +DiSA | 64 | 50$\\rightarrow$25 | 2.31 (+0.00) | 279.3 (-2.4) | 0.117 | 5.5$\\times$ |\n|  | 64 | 50$\\rightarrow$15 | 2.32 (+0.01) | 279.4 (-2.3) | 0.112 | 5.8$\\times$ |\n|  | 64 | 50$\\rightarrow$10 | 2.30 (-0.01) | 280.1 (-1.6) | 0.109 | 5.9$\\times$ |\n|  | 64 | 50$\\rightarrow$5 | 2.33 (+0.02) | 281.3 (-0.4) | 0.105 | 6.2$\\times$ |\n| - | - | - | - | - | - | - |\n|  | 32 | 50$\\rightarrow$25 | 2.53 (+0.22) | 273.0 (-8.7) | 0.065 | 10.0$\\times$ |\n|  | 32 | 50$\\rightarrow$15 | 2.44 (+0.13) | 273.0 (-8.7) | 0.061 | 10.7$\\times$ |\n|  | 32 | 50$\\rightarrow$10 | 2.40 (+0.09) | 277.6 (-4.1) | 0.060 | 10.9$\\times$ |\n|  | 32 | 50$\\rightarrow$5 | 2.35 (+0.04) | 277.4 (-4.3) | 0.057 | 11.4$\\times$ |\n| - | - | - | - | - | - | - |\n|  | 16 | 50$\\rightarrow$25 | 4.30 (+1.99) | 244.0 (-37.7) | 0.040 | 16.2$\\times$ |\n|  | 16 | 50$\\rightarrow$15 | 4.04 (+1.73) | 248.7 (-33.0) | 0.038 | 17.3$\\times$ |\n|  | 16 | 50$\\rightarrow$10 | 3.81 (+1.50) | 251.9 (-29.8) | 0.036 | 18.3$\\times$ |\n|  | 16 | 50$\\rightarrow$5 | 3.31 (+1.00) | 260.2 (-21.5) | 0.034 | 19.1$\\times$ |\n| - | - | - | - | - | - | - |\n|  | 8 | 50$\\rightarrow$25 | 13.43 (+11.12) | 179.2 (-102.5) | 0.028 | 23.0$\\times$ |\n|  | 8 | 50$\\rightarrow$15 | 12.98 (+10.67) | 182.5 (-99.2) | 0.026 | 25.4$\\times$ |\n|  | 8 | 50$\\rightarrow$10 | 12.15 (+9.84) | 188.5 (-93.2) | 0.024 | 26.9$\\times$ |\n|  | 8 | 50$\\rightarrow$5 | 10.09 (+7.78) | 201.2 (-80.5) | 0.023 | 27.9$\\times$ |"}}, "id": "f2lDvdpOws", "forum": "ZWwNVIr0PV", "replyto": "ZWwNVIr0PV", "signatures": ["ICLR.cc/2026/Conference/Submission15112/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15112/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15112/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763544443792, "cdate": 1763544443792, "tmdate": 1763544443792, "mdate": 1763544443792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors show that in AR with diffusion architectures, the later AR steps have tighter token distributions and straighter denoising paths, so diffusion can run with fewer steps without hurting quality. They propose DiSA, a training‑free schedule that uses more diffusion steps early and gradually fewer later (two‑stage / linear / cosine schedulers), reducing per‑token diffusion effort as conditions strengthen. Evidence includes: (i) MLPs can better predict later tokens; (ii) the variance of diffusion‑sampled tokens decreases with AR progress; and (iii) path straightness increases (Fig. 3). Finally, the authors show the effectiveness of DiSA on ImageNet‑256 with various AR models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Clear empirical insight (straighter late‑stage denoising) turned into a simple, general sampler schedule that’s easy to be equipped with various AR + diffusion architectures (Fig. 1)\n\n* Strong evaluation: various image‑level metrics (such as FID/IS/Precision/Recall), per‑image time, and complements existing diffusion accelerators (Table 3).\n\n* Practical wins on both ImageNet 256 x 256 and T2I GenEval (Harmon) with concrete speed–quality curves (Fig. 5)."}, "weaknesses": {"value": "I'm not an expert in this area, but I have some concerns and questions based on my understanding.\n\n* About novelty\n\nI appreciate the practical acceleration idea, but the paper mainly relies on the diffusion-step annealing strategy without much theoretical/mathematical evidence. Even a bit of math or intuition on why this schedule makes sense would make the work solid.\n\n* Scheduler robustness\n\nHow sensitive is performance to the exact annealing schedule (e.g., 50 -> 5)? Could the authors provide a hyper‑sweep and an auto‑tuning rule per model?\n\n* Automatic scheduling\n\nThe proposed heuristics look promising but ad‑hoc. Can the method learn a schedule online from uncertainty/variance signals?\n\n* About experiments\n\nThe experiments appear to focus mainly on 256×256-scale data (largely ImageNet or similar), which raises questions about generalization. It would be helpful to see results on higher-resolution settings (e.g., 512×512, 1024×1024) since frequency characteristics can change substantially with resolution and texture complexity."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Ibt6l8CK48", "forum": "ZWwNVIr0PV", "replyto": "ZWwNVIr0PV", "signatures": ["ICLR.cc/2026/Conference/Submission15112/Reviewer_XbsH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15112/Reviewer_XbsH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15112/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988558032, "cdate": 1761988558032, "tmdate": 1762925433718, "mdate": 1762925433718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free strategy to accelerate AR diffusion models by gradually reducing the number of diffusion steps during token generation. Through empirical analysis, the authors find that as more tokens are generated, the diffusion head becomes increasingly constrained, and later tokens require fewer denoising steps. Based on this observation, DiSA linearly anneals the diffusion steps, achieving notable speed-up while maintaining comparable FID and IS scores. The method is plug-and-play and complementary to existing diffusion samplers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "DiSA introduces a new interpretation of diffusion dynamics within AR generation. As conditioning strengthens across timesteps, the diffusion process becomes inherently easier. Unlike prior accelerators (e.g., DDIM, DPM-Solver, LazyMAR), which assume uniform difficulty and globally reduce steps, DiSA models the heterogeneity of diffusion necessity over AR progression. This observation is theoretically supported through denoising-path straightness analysis, linking DiSA to the geometry of diffusion ODEs and providing a principled foundation rather than a heuristic adjustment.\n\nThe paper substantiates its hypothesis through three orthogonal metrics (prediction accuracy, variance reduction, and trajectory straightness) forming a multi-faceted empirical argument. This triangulated evidence distinguishes DiSA from earlier works that rely solely on output metrics like FID or IS. The combination of quantitative analysis and visual trajectory interpretation strengthens the empirical credibility of its claims.\n\nDiSA is a training-free and architecture-agnostic plug-in, requiring no parameter updates or structural modifications. By only modifying the diffusion step schedule (e.g., from 50→5), it achieves up to 5–10× acceleration on MAR and 1.4–2.5× on FlowAR/xAR with negligible degradation in generation quality. The efficiency-to-complexity ratio clearly surpasses methods like FAR or speculative decoding, demonstrating elegance through minimal intervention."}, "weaknesses": {"value": "While DiSA is empirically well-justified, it remains largely heuristic. The diffusion-step schedule is fixed (typically linear), without a principled derivation from diffusion dynamics or uncertainty theory. In contrast, prior works like AdaDiff or Rectified Flow introduce adaptive step sizes based on explicit error or confidence estimation. DiSA assumes the AR step index monotonically correlates with conditional strength, an assumption not guaranteed for complex prompts. A theoretical analysis linking token entropy or local curvature to optimal diffusion steps would strengthen generality and interpretability.\n\nThe training-free nature is practical but introduces a potential mismatch: DiSA modifies the inference-time denoising schedule without retraining the diffusion head, which was originally optimized for uniform timesteps. This causes instability in some models (e.g., MAR required time-offset corrections). By contrast, retraining-based accelerators (e.g., FAR) maintain consistency between training and inference dynamics. Exploring fine-tuning or joint schedule learning could reduce this gap.\n\nAll evaluations are limited to ImageNet-256 and GenEval benchmarks. The method’s behavior under higher resolutions, complex spatial layouts, or multimodal conditioning remains untested. Additionally, the study reports only FID and IS, omitting perceptual (LPIPS), semantic (CLIP-Sim), or human-alignment metrics used in recent acceleration research. This narrower evaluation spectrum makes it difficult to quantify subtle degradation patterns or aesthetic trade-offs."}, "questions": {"value": "The experiments focus on 256×256 ImageNet and GenEval.\nCould the authors comment on expected behavior for higher resolutions or long-horizon text prompts? For instance, when later tokens represent finer local details, does the “easier-later” assumption still hold?\n\nDiSA shows minimal quality loss, but where does degradation start?\nPlease provide an analysis or visualization showing at what step reduction threshold (e.g., 50→1, 50→10) artifacts begin to appear. This would help position DiSA’s safe operating range.\n\nSection 5 briefly mentions heuristics (variance, uncertainty, straightness) for online adjustment.\nCould the authors expand on how these heuristics performed and whether they can form the basis of a truly adaptive DiSA variant? This seems like a promising direction that could elevate the contribution beyond a fixed schedule"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KKQOVNlxdt", "forum": "ZWwNVIr0PV", "replyto": "ZWwNVIr0PV", "signatures": ["ICLR.cc/2026/Conference/Submission15112/Reviewer_9xYm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15112/Reviewer_9xYm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15112/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066653923, "cdate": 1762066653923, "tmdate": 1762925433246, "mdate": 1762925433246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}