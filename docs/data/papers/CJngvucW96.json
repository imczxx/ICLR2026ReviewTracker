{"id": "CJngvucW96", "number": 24522, "cdate": 1758357591673, "mdate": 1759896761938, "content": {"title": "Fine-Tuned In-Context Learners", "abstract": "When adapting large language models (LLMs) to a specific downstream task,\ntwo primary approaches are commonly employed: (1) prompt engineering with\nin-context few-shot learning, leveraging the model’s inherent generalization abil-\nities, and (2) fine-tuning on task-specific data, directly optimizing the model’s\nparameters. While prompt-based methods excel in few-shot scenarios, their effec-\ntiveness often plateaus as more data becomes available. Conversely, fine-tuning\nscales well with data but may underperform when training examples are scarce.\nWe investigate a unified approach that bridges these two paradigms by incorpo-\nrating in-context learning directly into the fine-tuning process. Specifically, we\nfine-tune the model on task-specific data augmented with in-context examples,\nmimicking the structure of k-shot prompts. This approach, while requiring per-\ntask fine-tuning, combines the sample efficiency of in-context learning with the\nperformance gains of fine-tuning, leading to a method that consistently matches\nand often significantly exceeds both these baselines. With an emphasis on practi-\ncality, we introduce a hyperparameter optimization strategy based on prequential\nevaluation, which is effective in data-limited scenarios and eliminates the need for\nexpensive cross-validation. We conduct an extensive empirical study to investi-\ngate the sample efficiency of fine-tuning, in-context learning, and the proposed\nunified approach across a diverse range of downstream tasks.", "tldr": "", "keywords": ["model-adaptation", "in-contex-learning", "sample-efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/babf0ccc0d0df8d54a0c751808704dade0bcdc6b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes ICL+FT, a unified adaptation method that fine-tunes LLMs with k-shot in-context (x, y) examples and then uses the in-context pairs again at inference. It selects hyperparameters via prequential next-step evaluation, eliminating the need for a held-out dev set. Across Gemma-2 model sizes and several benchmarks (e.g., BBH), ICL+FT consistently matches or outperforms ICL-only and FT-only baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well structured and easy to follow.\n2. Using prequential evaluation to bypass cross-validation significantly reduces the cost of hyperparameter selection.\n3. Results span multiple datasets, including 23 BBH tasks, an NLP task suite, Parity-20, and FLoRes. This provides strong evidence that ICL+FT delivers gains over the ICL-only and FT-only baselines."}, "weaknesses": {"value": "1. Section 3 states that prequential selection is computationally efficient, but the paper lacks explicit runtime or FLOP comparisons against CT-only, FT-only, and a simple hold-out cross-validation baseline. Concrete wall-clock measurements would help substantiate the efficiency claim.\n2. Beyond ICL-only and FT-only, comparisons to other adaptation methods like prefix tuning, prompt tuning, and context tuning are missing. These would help clarify whether ICL+FT is a state-of-the-art method in performance and/or efficiency."}, "questions": {"value": "1. How sensitive is ICL+FT to the order of training examples in Algorithm 1? Do different permutations lead to significantly different hyperparameters or final performance? How does this sensitivity compare to standard ICL-only?\n2. Have you evaluated ICL+FT on models outside of the Gemma family? For example, Llama and Qwen are also open-source model families with varying model sizes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yy2YrjSrdj", "forum": "CJngvucW96", "replyto": "CJngvucW96", "signatures": ["ICLR.cc/2026/Conference/Submission24522/Reviewer_vqXF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24522/Reviewer_vqXF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530588527, "cdate": 1761530588527, "tmdate": 1762943111709, "mdate": 1762943111709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider fine-tuning in a low-data regime. They introduce a method to fine-tune on $k$ in-context examples concatenated together with each training example and additionally design a hyperparameter selection method which is better adapted to low-data contexts. They study both their training approach and hyperparameter selection method in a variety of low-data tasks, such as Big Bench Hard and low-resource translation, among others. They compare their fine-tuning method against both fine-tuning alone as well as in-context learning alone. To study their hyperparameter selection method, they compare its performance against hyperparameter selection using an i.i.d. evaluation set as well as against using a fixed global set of hyperparameters."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The prequential hyperparameter selection algorithm is is original and novel. There is some prior work on fine-tuning with few-shot examples (some cited as well as other recent work [1]), but the contribution on this front is still novel. The paper is well-written and figures are clear.\n\n[1] Lu, Jack, Ryan Teehan, Zhenbang Yang, and Mengye Ren. \"Context Tuning for In-Context Optimization.\" https://arxiv.org/abs/2507.04221"}, "weaknesses": {"value": "The claim, \"We emphasize that the prequential training and evaluation protocol described in Section ?? does not necessitate a separate held-out set, allowing practitioners to utilize all data points for training\" is too strong and not justified by the paper. At best, the paper seems to indicate that, in the low-data regime, we do not need a separate test set for hyperparameter tuning specifically. If we want to assess overfitting and generalization, we would still need a held-out test set. \n\nSome comparisons with baselines do not seem precisely 1-1.\n\nOther comments:\n\nThere is a broken section reference on page 5 in the Big Bench Hard paragraph and a broken citation on page 14."}, "questions": {"value": "Can you explain this point: \"Note that globally-chosen hyper-parameters introduce information leakage as a large number of test-set examples are used to chose these\"? Wouldn't the evaluation set still be held-out?\n\nDoes your FT baseline also take multiple gradient steps per example? \n\nHow do you account for the fact that the ICL+FT setting has seen some datapoints multiple times (because they later appear as ICL examples for future training steps)? Do your baselines see each example the same number of times as your method does?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SQIxVGics5", "forum": "CJngvucW96", "replyto": "CJngvucW96", "signatures": ["ICLR.cc/2026/Conference/Submission24522/Reviewer_ynkh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24522/Reviewer_ynkh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949890812, "cdate": 1761949890812, "tmdate": 1762943111467, "mdate": 1762943111467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified approach that combines in-context learning (ICL) and fine-tuning (FT) to improve task adaptation in large language models. Instead of using these methods separately, the model is fine-tuned on prompts that already include k-shot examples, and during inference it again receives k in-context examples. Technically, the ICL+FT method forms training sequences consisting of k demonstrations followed by the target query, and updates model parameters using the likelihood over all answer tokens. For hyperparameter tuning, the authors introduce a prequential evaluation scheme, which incrementally trains and evaluates the model without requiring a separate validation set. Experiments on various benchmarks show that ICL+FT typically matches or sometimes outperforms both ICL-only and FT-only baselines across Gemma 2 based models (2B, 9B, 27B)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Clear and simple idea that is easy to implement.\n- Broad empirical performance outperformed other baselines across tasks, model sizes, and data budgets.\n- Useful ablations on number of in context examples, instruction prompting, and LoRA."}, "weaknesses": {"value": "- **Conceptual novelty is limited and close to MetaICL style training.** Prior work on MetaICL and related meta learning frameworks already trains on k shot episodic inputs so that models learn to use in context examples. This paper differs mainly in scope, since it targets a single downstream task with task specific fine tuning rather than cross task generalization without parameter updates. The core learning signal of using in prompt examples is therefore very similar. \n- **Limited and Unbalanced Efficiency Claims:** While the paper suggests both data and computation efficiency, this claim appears overstated. The approach still requires per-task fine-tuning and utilizes k in-context examples during inference, resulting in cumulative rather than reduced compute cost. Therefore, although data efficiency may be plausible, there is no clear evidence of computational efficiency.\n- **Ambiguity in training details and sensitivity.** The algorithm samples k context examples from previously seen data, but the selection strategy and order effects aren’t analyzed. (performance of ICL is known to highly sensitive to those factors.) There is no report of variance across different context selection policies or data orderings, which can be substantial for in context methods."}, "questions": {"value": "- Is there a reason why the title shown on OpenReview and the title in the PDF are different?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T26V0BcGZa", "forum": "CJngvucW96", "replyto": "CJngvucW96", "signatures": ["ICLR.cc/2026/Conference/Submission24522/Reviewer_S6Dd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24522/Reviewer_S6Dd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142450042, "cdate": 1762142450042, "tmdate": 1762943111219, "mdate": 1762943111219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}