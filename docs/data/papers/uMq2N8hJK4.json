{"id": "uMq2N8hJK4", "number": 18099, "cdate": 1758283811895, "mdate": 1763659533214, "content": {"title": "Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs", "abstract": "Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a popular pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities.\nTo address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) - a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters.\nWe test SGTM's effectiveness in two applications: removing knowledge of a language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing.\nUnlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring 7 times more fine-tuning steps to reach baseline performance on the forget set compared to a traditional unlearning method (RMU).\nOur results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.", "tldr": "We localize unwanted LLM capabilities to specific parameters during training to later remove them.", "keywords": ["machine unlearning", "knowledge localization", "gradient routing", "capability removal", "ai safety", "llms", "large language models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d651639f440fa183a629cb2ae8635b29bce31d29.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Selective GradienT Masking (SGTM) (an improved version of gradient routing) to remove undesirable knowledge while maintaining desirable performance for LLM pretraining. This approach works by performing:\n\n- Partitioning the model parameters into retain and forget sets;\n- During the backward pass, selectively masking out retain gradients when the data are sourced from the forget dataset;\n- During the forward pass, selectively masking out forget parameters when the data are sourced from the retain dataset;\n\nThe authors evaluate their approach on two datasets: a synthetic bilingual TinyStories case wherein the English version is the retain set and the Spanish version is the forget set, and Wikipedia, wherein the biology knowledge is the forget set. Empirical results show that SGTM consistently outperform gradient routing on both setups, and has the potential to close the gap with data filtering. Additionally, models trained with SGTM are more robust to adversarial finetuning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is very well-written. I must admit that I'm not an expert in pretraining approaches to unlearning, but the authors have demonstrated expertise in their exposition, overview, and contributions. The literature review also gives a concrete sense of SOTA approaches.\n- The proposed SGTM approach is simple and intuitive, making them suitable for large-scale training runs for frontier models. \n- The experiments are well-designed, with a more controlled, stylized setting of bilingual TinyStories as a proof of concept, and a more realistic Wikipedia pretraining as validation. \n- The authors demonstrate that SGTM is more robust to unlabeled forget data Figure 3(b), albeit at the cost of higher retain loss on identically sized models. One can argue that the effective parameter count is lower due to the masking schemes, and the authors conduct targeted scaling law analysis in the appendix to investigate this.\n- It is somewhat expected (but interesting!) that SGTM models are more robust to adversarial finetuning"}, "weaknesses": {"value": "- Scale remains a primary concern, as the largest models studied in this paper is 254M. While I don't deduct points from it due to the expense of pretraining, whether this approach can be scaled to frontier systems is an open question, and whether the compute penalty (6% for general knowledge according to the authors) is a worthy trade-off remains to be studied."}, "questions": {"value": "- I really like the masking idea, but I'm curious if the authors should explore masking in the output space, i.e. by masking out the loss of undesirable tokens so that the model can have a good understanding of the surrounding context, but at inference time they will not generate these undesirable knowledge."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HaDXLt7fld", "forum": "uMq2N8hJK4", "replyto": "uMq2N8hJK4", "signatures": ["ICLR.cc/2026/Conference/Submission18099/Reviewer_K9fW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18099/Reviewer_K9fW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761006829451, "cdate": 1761006829451, "tmdate": 1762927870425, "mdate": 1762927870425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for unlearning in a LLM at the pretraining stage, by masking gradients to a parameter set on the backward pass if an example is part of the forget set. They argue this is more robust to label noise than data filtering or the similar Gradient Routing. They support this with empirical evidence."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- clearly written, seems novel - caveat that I am not deeply familiar with the unlearning literature\n- experiments seem to support the basic point of improvement the authors suggest for SGTM, and are fairly thorough (ablations with related data categories are cool)\n- Fig 1 is great! In general the communication around tradeoffs is well done"}, "weaknesses": {"value": "- it's odd to me that there aren't results shown for Fig 4 for GR - isn't this the main baseline we should be comparing to?\n- some contradictory statements around parameter subsets: in Fig 2 caption the authors that the after forget parameters are assigned, “the remaining parameters are designated to the retain data” but then discuss something called \"joint\" parameters in line 183\n- it would be good to give more intuition here - why is SGTM more robust to label noise? it's not a priori obvious to me that it should be, some exploration about the difference to baselines would be helpful\n- I'm not sure exactly how this is usually handled in unlearning, but there doesn't seem to be a lot of information about how the SGTM model performs without the parameters masked\n- as someone not deeply familiar with the literature, it's not quite clear to me if the only difference between SGTM and GR is activation vs. parameter gradient masking? would be good to state this more clearly\n- Leakage: defining this as a percentage is odd to me - it's misleading that the number is constant between 5% and 50% tokens, since that's the equivalent of 4x tokens, which is a lot! I think something that scales with the token equivalence number (eg 707k in Fig 5a) is more sensible"}, "questions": {"value": "- how does GR perform on the real data in Fig 4?\n- why does SGTM display more robustness to label noise?\n- clarify: main difference between GR and SGTM is activation vs parameter gradient masking?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mxBGhAtyWc", "forum": "uMq2N8hJK4", "replyto": "uMq2N8hJK4", "signatures": ["ICLR.cc/2026/Conference/Submission18099/Reviewer_FJtB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18099/Reviewer_FJtB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944668548, "cdate": 1761944668548, "tmdate": 1762927870002, "mdate": 1762927870002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response"}, "comment": {"value": "We thank all reviewers for their thoughtful and constructive feedback. Their suggestions significantly improved the clarity of the paper and motivated several new experiments that strengthened our empirical findings.\n\nBelow we highlight the main changes to the paper, focusing on issues that appeared across multiple reviews. Significant changes are highlighted in blue in the revised version of the paper.\n\n**1. Difference with Gradient Routing (Cloud et al., 2024)**\n\nWe have elaborated on the difference between the two methods in Section 3.2 and more explicitly reference the Appendix B where we provide detailed comparison.\n\n**2. Scaling experiments**\n\nWe partially address reviewers’ concerns regarding the small scale of our experiments by showing that SGTM becomes more robust to label noise as model and dataset size increases (Section 4.3, Fig 5(b)). While these experiments still involve only relatively small models and do not replace large-scale evaluation, we believe they provide preliminary evidence that our method is likely to remain effective at a larger scale.\n\n**3. Method intuition and gradient norm analysis**\n\nTo provide clearer intuition for why SGTM is robust to label noise, we added a new experiment (Section 4.4, Fig. 6) measuring per-sample gradient norms while treating all data as unlabeled (i.e. without masking). These results reveal a specialization effect: **unlabeled forget data predominantly updates forget parameters, and unlabeled retain data predominantly updates retain parameters**. \n\n**4. Joint parameters**\n\nWe have clarified the notation and have now removed $\\theta_{\\text{joint}}$ from Section 3.1 (“Notation”), as it remains empty in the standard version of our approach. We now introduce it later in Appendix B, where we examine alternative gradient-masking configurations that include non-empty joint parameter sets."}}, "id": "FmUicX6GVM", "forum": "uMq2N8hJK4", "replyto": "uMq2N8hJK4", "signatures": ["ICLR.cc/2026/Conference/Submission18099/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18099/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18099/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763676562298, "cdate": 1763676562298, "tmdate": 1763676562298, "mdate": 1763676562298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Undesirable data is often filtered out during pre-training, but filters have false negatives, so some undesirable content remains in the pre-training data. This paper proposes a training technique that encourages such undesirable data to reside in a small subset of parameters that can then be removed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Empirical results on multiple settings show an improved trade-off between general capabilities and forgetting of undesirable content, compared with filtering. Moreover, it has much better performance against fine-tuning compared with a strong unlearning method, RMU.\n\nThe method is quite simple and intuitive. Gradient masking sequesters undesirable knowledge into a small subset of parameters, while parameter masking encourages the rest of the parameters to function well even when those parameters are removed.\n\nThe paper is well-written and clear."}, "weaknesses": {"value": "This paper compares only with filtering and a similar previous work (Gradient Routing), but other methods have also been developed as alternatives to filtering:\n* https://arxiv.org/abs/2302.08582 This paper explores several training objectives and finds that a \"conditional training\" approach works well. It seems that SGTM could directly compare with this approach.\n* https://arxiv.org/abs/2505.03052 This paper has a somewhat different motivation, but they can use a more aggressive threshold on the classifier because their intervention is less strict than filtering. It also seems worth discussing and possibly comparing with\n\n$\\theta_{\\text{retain}}$ is used but is not clearly defined (e.g. Line 182). It's also confusing that the retain parameters are not mentioned in Lines 250-253.\n\nThe experimental settings are somewhat toy. For TinyStories, 64M is a very small model. It is also quite synthetic to generate the Spanish data with translation, when multilingual corpora also exist. The noise model is also not realistic, as it is pure iid noise. Finally, the motivation of this experiment is a bit unclear, since in practice one would not want to prohibit the model from learning a second language. The Wikipedia experiments are more realistic in noise, though 254M is still quite a small model.\n\nThe experiments are also somewhat narrow. Only these two model sizes are considered, and for Wikipedia only one possible forget set is considered. Perhaps toxic text could be considered as another type of data that is typically filtered but only imperfectly.\n\nFinally, the paper does not clearly explain the methodological difference with the previous version of Gradient Routing from Cloud et al. (2024). This is important for explaining the novelty of this method."}, "questions": {"value": "Could you explain more what $\\theta_{\\text{retain}}$ is, how it differs from $\\theta_{\\text{joint}}$, and how it is used?\n\nWhat are the main differences between SGTM and Cloud et al.? From looking at that paper, it seems that the difference may be from the selective parameter masking, but I am curious if this is correct and if there are other differences."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SiQA5L2oD2", "forum": "uMq2N8hJK4", "replyto": "uMq2N8hJK4", "signatures": ["ICLR.cc/2026/Conference/Submission18099/Reviewer_w7AC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18099/Reviewer_w7AC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962135118, "cdate": 1761962135118, "tmdate": 1762927869277, "mdate": 1762927869277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces selective gradient masking, a pretraining time technique to localize and remove specific capabilities from LLMs. The authors evaluates the method on two settings of (1). synthetic bilingual data and (2). wikipedia corpus. Across both, SGTM achieves a better retain/forget tradeoff under label noise compared to the baselines. The authors also show SGTM is more robust to adversarial fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Adversarial robustness**: the detailed discussions on mislabeled content and adversarial fine-tuning are valuable and highly relevant to the community. \n2. **Clear presentations**: the figures and visualizations are informative and well-designed."}, "weaknesses": {"value": "1. **Insufficient Evidence**: This is my primary concern. The evaluation relies solely on model loss, which may not adequately capture downstream perfromance differences that truly matter. It is unclear to me whether higher loss indeed indicates better forgetting. Including additional evaluations for forgetting and general performance retention would substantially strengthen the paper's empirical support. \n\n2. **Limited Scale**: As noted in section 6, the experiments use very small model and dataset sizes. It remains uncertain whether the findings would generalize to larger models of real-world training scales."}, "questions": {"value": "1. How were the data points in Figure 1 obtained? Do they represent Pareto frontiers or averages? \n\n2. What is $\\theta_{joint}$ specifically? How are the $\\theta_{joint}$ parameters selected, and how are they different from $\\theta_{retain}$ or $\\theta_{forget}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T9y0EJcFPU", "forum": "uMq2N8hJK4", "replyto": "uMq2N8hJK4", "signatures": ["ICLR.cc/2026/Conference/Submission18099/Reviewer_JtvZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18099/Reviewer_JtvZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762133339101, "cdate": 1762133339101, "tmdate": 1762927868623, "mdate": 1762927868623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}