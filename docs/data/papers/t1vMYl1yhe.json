{"id": "t1vMYl1yhe", "number": 9975, "cdate": 1758153662294, "mdate": 1759897682391, "content": {"title": "What Happens Next? Anticipating Future Motion by Generating Point Trajectories", "abstract": "We consider the problem of forecasting motion from a single image, i.e., predicting how objects in the world are likely to move, without the ability to observe other parameters such as the object velocities or the forces applied to them. We formulate this task as conditional generation of dense trajectory grids with a model that closely follows the architecture of modern video generators but outputs motion trajectories instead of pixels. This approach captures scene-wide dynamics and uncertainty, yielding more accurate and diverse predictions than prior regressors and generators. Although recent state-of-the-art video generators are often regarded as world models, we show that they struggle with forecasting motion from a single image, even in simple physical scenarios such as falling blocks or mechanical object interactions, despite fine-tuning on such data. We show that this limitation arises from the overhead of generating pixels rather than directly modeling motion.", "tldr": "We develop a video-like diffusion model that generates (quasi-)dense motion trajectories rather than pixels, achieving significantly more accurate and efficient motion generation than prior motion forecasters and video generators.", "keywords": ["motion generation", "point trajectories", "flow matching"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/afbd6e4db3e672c699879015b6bca236e23a0ddd.pdf", "supplementary_material": "/attachment/480215eac9c47279026bff9178d867e795deb8c5.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors focus on efficient approaches to forecasting dynamics in an image. To this end, they propose, instead of modeling pixels directly, to forecast point trajectories. They utilize a beta-VAE combined with Flow Matching to generate future point trajectories conditioned on an image. The authors then comprehensively evaluate the performance of their model, looking at both statistical and physical plausibility. They evaluate on robotic datasets such as Libero as well as synthetic datasets (Kubric). The also include a user study of performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper explores a less studied avenue of visual dynamics: predicting pixel motion versus pixels themselves. This approach makes intuitive sense.\n\n2. Quantitative evaluation is comprehensive and convincing. \n\n3. The paper is well-written and easy to understand."}, "weaknesses": {"value": "1. There is a prior work, \"An Uncertain Future: Forecasting from Static Images using Variational Autoencoders\", Walker et al, ECCV 2016, which is similar and also uses a VAE to forecast pixel trajectories from a single image. This work should be cited.\n\n2. Results of the user study are somewhat weak (preferred over other approaches only 52% of the time).\n\n3. It would be helpful to see a few more qualitative results from the Physics 101 dataset."}, "questions": {"value": "1. Please cite the paper above. It is similar to approach mentioned in this paper.\n\n2. Please elaborate more on the user study, and would it be possible to include more results from Physics 101?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZfSNrjcSQY", "forum": "t1vMYl1yhe", "replyto": "t1vMYl1yhe", "signatures": ["ICLR.cc/2026/Conference/Submission9975/Reviewer_GBJN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9975/Reviewer_GBJN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861940576, "cdate": 1761861940576, "tmdate": 1762921413248, "mdate": 1762921413248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of predicting object movement from a single static image, where future motion is inherently uncertain. The authors propose a method that directly generates a dense grid of point trajectories (coordinates) conditioned on the input image, rather than generating future video frames (pixels). They utilize a generative architecture (similar to modern video generators, specifically using flow matching) to model the distribution of possible future movements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Generates point trajectories instead of pixels, bypassing the complexity of video generation. This allows the model to efficiently learn physical dynamics and object consistency.\n2. Uses a generative model to predict the distribution of trajectories, producing multiple physically plausible futures, which better reflects real-world information gaps.\n3. Achieves superior performance in physical reasoning tasks compared to prior trajectory regressors and state-of-the-art video generators pre-trained on massive datasets."}, "weaknesses": {"value": "1. Predictions are based on a single image, lacking precise initial velocity or force data. Accuracy heavily relies on the model's learned \"physical common sense\" rather than precise observation. I think this task setting quite strange.\n2. Evaluation primarily uses synthetic datasets (e.g., Kubric, Physion). Its generalization capability to complex, \"in-the-wild\" real-world physics (e.g., fluid, soft bodies) remains questionable."}, "questions": {"value": "1. The point I understand the least is why, for a scenario where object motion needs to be predicted, it is limited to using only a single static image. Alternatively, is there a way to adapt this model to also support video or multi-frame input? And how would its performance be?\n2. It seems to lack some relevant paper citations, such as Flow-grounded spatial-temporal video prediction from still images, papers about future frame synthesis/prediction"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V7seuVrVJJ", "forum": "t1vMYl1yhe", "replyto": "t1vMYl1yhe", "signatures": ["ICLR.cc/2026/Conference/Submission9975/Reviewer_VFf8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9975/Reviewer_VFf8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142225483, "cdate": 1762142225483, "tmdate": 1762921412858, "mdate": 1762921412858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of forecasting object motion from a single imageâ€”predicting potential object movements without observing velocities or applied forces. It formulates the task as conditional generation of dense point trajectory grids, drawing inspiration from modern video generator architectures but outputting trajectories instead of pixels. The model uses a trajectory VAE for latent space mapping and flow matching for sampling, capturing scene-wide dynamics and uncertainty."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It innovatively redefines motion forecasting as dense trajectory grid generation, moving beyond prior work focused on sparse active points (e.g., ATM, Tra-MoE) or pixel-based video generation, enabling better global scene dynamics modeling.\n2. The study uses diverse datasets and comprehensive metrics to assess accuracy, diversity, and physical plausibility, with ablations strengthening conclusions"}, "weaknesses": {"value": "- **Lack of Downstream Task Evaluation for Video Generation**: While the paper argues that trajectory generation outperforms video generation for motion forecasting, it does not test how the two approaches perform on downstream tasks that rely on motion, e.g., robot manipulation, video interpolation, or action prediction.   \n- **No Analysis of Query Point Sampling Density**: The paper only discusses ablations of latent code dimensions but does not explore the impact of query point sampling density.   \n- **Insufficient Real-World Data Results and Visualizations**: Real-world validation is limited to the Physics101 dataset without testing on other real datasets, e.g., egocentric motion datasets."}, "questions": {"value": "In the absence of a task description (e.g., for the action \"Pick up the book on the right and place it under the cabinet shelf\"), how does the paper address the diversity of action prediction, and are there any controllable settings integrated into the pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6fFTppJH2T", "forum": "t1vMYl1yhe", "replyto": "t1vMYl1yhe", "signatures": ["ICLR.cc/2026/Conference/Submission9975/Reviewer_h2xU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9975/Reviewer_h2xU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762182169175, "cdate": 1762182169175, "tmdate": 1762921412532, "mdate": 1762921412532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}