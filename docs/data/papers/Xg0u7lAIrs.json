{"id": "Xg0u7lAIrs", "number": 18152, "cdate": 1758284391014, "mdate": 1759897125159, "content": {"title": "Post-hoc Compression of LoRA Adapters via Singular Value Decomposition", "abstract": "Low-Rank Adaptation (LoRA) has become a widely used technique for personalizing text-to-image models such as Stable Diffusion. Although LoRA greatly reduces fine-tuning costs by introducing low-rank updates, practical deployments often involve one large backbone model combined with thousands of LoRA adapters. This creates a new challenge: even though each adapter is relatively small, the collective storage and transmission cost of large LoRA libraries becomes substantial.\nIn this paper, we investigate whether existing LoRA adapters can be further compressed without retraining. We begin from the observation that while LoRA constrains updates to rank $r$ matrices, the effective rank of the learned updates is often significantly smaller, in practice, require only rank 1 to capture its expressive power. Motivated by this redundancy, we introduce a simple singular value decomposition (SVD)–based method to compress pretrained LoRA adapters. Through extensive experiments on a variety of text-to-image LoRA models, we show that trained LoRA models indeed contain considerable rank redundancy and SVD compression can consistently reduce adapter dimension without notable loss in generation quality.", "tldr": "We show that LoRA adapters for text-to-image models contain significant rank redundancy and can be compressed with SVD without retraining or quality loss.", "keywords": ["Text-to-Image Generation", "Stable Diffusion", "LoRA", "Model Compression"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/590a643fa39b59401001012ac0ec2c0eb40be926.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a simple yet effective post-hoc compression technique for Low-Rank Adaptation (LoRA) adapters used in text-to-image diffusion models. While LoRA is already a parameter-efficient fine-tuning method, the authors observe that trained LoRA adapters often exhibit redundant rank components, i.e., their effective rank is much smaller than the nominal one. To exploit this redundancy, the authors apply Singular Value Decomposition (SVD) on the trained LoRA update matrices, truncate to the top-k singular values, and reconstruct a smaller adapter without retraining. Extensive experiments on DreamBooth and Civitai community LoRAs demonstrate that this post-hoc SVD compression achieves large reductions in storage (up to 90%+) while preserving CLIP-based similarity scores and visual quality."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- *Timely and practical contribution:* Managing large repositories of LoRA adapters is of imminent value to real-world deployment.\nThe proposed method is thus highly valuable for practitioners and large-scale personalization pipelines.\n- *Simplicity and compatibility:* The SVD-based method is conceptually simple, theoretically grounded (optimal low-rank approximation), and fully compatible with existing LoRA implementations.\n- *Comprehensive empirical validation:* The authors evaluate across multiple datasets (DreamBooth, Civitai), LoRA variants (LoRA, PaRa, LyCoris), and ranks. Both quantitative (CLIP similarity) and qualitative results are clear and convincing.\n- *Practical impact and reproducibility:* Demonstrations such as α-scaling (controllability) and multi-LoRA composition confirm usability after compression. The authors commit to open-sourcing code and datasets for reproducibility."}, "weaknesses": {"value": "- *Limited theoretical depth:* While the SVD rationale is sound, the paper remains largely empirical. There is no theoretical analysis of why LoRA training yields such rapid spectral decay or how compression affects gradient directions in the fine-tuned subspace.\n- *Evaluation scope:* The evaluation relies heavily on CLIP similarity, which may not fully capture perceptual or artistic fidelity. More human or FID-based evaluations could strengthen claims about “no notable loss in generation quality.”\n- *Fixed rank truncation:* The method uses a uniform rank across layers, even though the singular value spectra differ significantly.\nAn adaptive or energy-based rank selection criterion could yield better trade-offs between quality and compression.\n- *Lack of runtime evaluation:* While storage reduction is well demonstrated, it would be useful to quantify potential improvements in inference latency, memory footprint, or mobile deployment feasibility.\n- *Positioning vs. prior work:* The paper could more explicitly compare post-hoc SVD compression with training-time compactness methods (SVDiff, TriLoRA, PaRa) under matched compression ratios to highlight relative benefits or trade-offs.\n\nMinor:\n- A schematic illustration of how SVD compression integrates into LoRA loading code would be helpful.\n- Line 169: \"where the diagonal entries $\\sigma_1$\" - by this, I assume \"diagonal entries of $\\epsilon$\"?"}, "questions": {"value": "1. How sensitive is the compressed LoRA quality to the chosen truncation rank k?\n2. Could an adaptive criterion based on explained variance (e.g., retaining 95% of singular value energy) yield more consistent results?\nDoes SVD compression affect downstream finetuning or composition with other adapters (e.g., VeRA)?\n3. Could the authors provide insights into per-layer rank redundancy—which layers or modules (e.g., cross-attention vs. convolution) are most compressible?\n4. Have the authors considered a hybrid approach, where SVD-compressed adapters are fine-tuned briefly to recover any lost quality?\n\n**On broader impacts of the proposed method:**\n\n- What concerns me is the broader applicability of this test-time method to LoRA adapters for other downstream tasks. Can the authors provide results on standard classification task(s) if the rebuttal time period allows for this? \n- *Thoughts on LoRA merging*: Given that the proposed test-time LoRA compression method preserves only few top-k singular values, effectively reducing the rank to 1, can the authors comment if this method could be applied for effective LoRA merging as well? It would be interesting to see how multiple rank 1 LoRA adapters (obtained by the proposed decomposition method) be composed/merged together to capture meaningful downstream task semantics by not accumulating errors - this could clearly impact the wider deployment of the proposed compression method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w8SwFzgh8E", "forum": "Xg0u7lAIrs", "replyto": "Xg0u7lAIrs", "signatures": ["ICLR.cc/2026/Conference/Submission18152/Reviewer_AnW8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18152/Reviewer_AnW8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760795729990, "cdate": 1760795729990, "tmdate": 1762927910671, "mdate": 1762927910671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In my understanding, this method compresses LoRA adapters after training by using SVD. It takes the learned update matrix, decomposes it into singular values, keeps only the top few, and rebuilds a smaller adapter. This reduces rank and storage without retraining, while keeping most of the personalization quality. It is a straightforward approach.\n\nHere are the authors claimed contributions: \nTheir post-hoc compression strategy.\nEmpirical evidence of redundancy.\nShowing that this is broadly applicable."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Clear idea: The paper explains the SVD-based compression method in a straightforward way, making it easy to understand and implement.\nEmpirical backing: The paper shows experiments and spectral analysis to demonstrate that LoRA adapters have significant rank redundancy, supporting the feasibility of post-hoc compression."}, "weaknesses": {"value": "Here are the major weaknesses of this approach:\n1. The contribution is not novel.\n2. The authors observations have already been demonstrated in previous work.\n3. The experimental analysis does not contain any comparison with related work.\n4. The authors don't discuss what is the motivation to reduce the rank of an adapter after training. In most solutions, the adapters are merged into the model during inference. If there's no inference saving or training compute saved, how is the method efficient?\n\n\nThere exists a lot of related work not discussed, which have already come to these conclusions and the approach which is presented as novel has already been implemented in libraries such as Kohya and Peft. Some of the works are listed below. The authors are encouraged to perform a thorough literature review.\n\nPost-hoc Compression Methods:\n\nPHLoRA: Post-hoc Low-Rank Adapter Extraction from Full-Rank Checkpoints.\nCompress-then-Serve: Efficient Multi-LoRA Serving via Shared Low-Rank Basis.\nKnOTS: Knowledge Transfer via Orthogonal Subspaces for LoRA Merging.\nDyLoRA: Dynamic Low-Rank Adaptation for Parameter-Efficient Fine-Tuning.\n\n\nWorks Highlighting Redundancy in Low-Rank Dimensions:\n\nSARA: Singular-Value Based Adaptive Low-Rank Adaption\nFouRA: Fourier Low-Rank Adaptation\nSVDiff: Compact Parameter Space for Diffusion Fine-Tuning\nPaRa: Personalizing Text-to-Image Diffusion via Parameter Rank Reduction\nTriLoRA: Integrating SVD for Advanced Style Personalization in Text-to-Image Generation\nLyCoris: Navigating Text-to-Image Customization with Flexible Low-Rank Structures"}, "questions": {"value": "What is the motivation to reduce the rank of an adapter after training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yQ2xcSyOn7", "forum": "Xg0u7lAIrs", "replyto": "Xg0u7lAIrs", "signatures": ["ICLR.cc/2026/Conference/Submission18152/Reviewer_suHJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18152/Reviewer_suHJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903346407, "cdate": 1761903346407, "tmdate": 1762927910318, "mdate": 1762927910318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free method for compressing LoRA adapters. The core idea is to apply singular value decomposition (SVD) to the LoRA parameters and reconstruct the adapters using a small number of singular components. The method aims to maintain model performance while significantly reducing the number of trainable or stored parameters. Extensive experiments are conducted to demonstrate the efficiency–effectiveness trade-off."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The method is simple, intuitive, making it easy to implement in existing LoRA-based systems.\nThe presentation is clear and concise, allowing readers to easily understand the procedure and reproduce the approach."}, "weaknesses": {"value": "1. The paper does not sufficiently justify the use of SVD as the decomposition technique. Why is SVD preferred over alternative low-rank approximation methods (e.g., randomized projections, or tensor decompositions)? Additionally, the choice of using rank-1 compression appears subjective and is not empirically validated. A broader exploration of decomposition and rank selection strategies would strengthen the technical contribution.\n\n2. According to Figure 3, the spectral decay of the LoRA updates is not particularly sharp, suggesting that the weight updates may not be strongly low-rank. This observation undermines one of the central assumptions of the method. A more detailed analysis or justification of this assumption is needed.\n\n3. The presentation of results in Table 1 is unclear. It is not obvious which method is the proposed one, and whether LoRA, PaRa, or LyCoris serve as baselines or components of the method. Clearer labeling and explanation are required to interpret the results properly."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JzVpCHxL9a", "forum": "Xg0u7lAIrs", "replyto": "Xg0u7lAIrs", "signatures": ["ICLR.cc/2026/Conference/Submission18152/Reviewer_HaoF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18152/Reviewer_HaoF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762048565689, "cdate": 1762048565689, "tmdate": 1762927909913, "mdate": 1762927909913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}