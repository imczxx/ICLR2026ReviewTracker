{"id": "LxkfjD81xB", "number": 19681, "cdate": 1758298306446, "mdate": 1759897026220, "content": {"title": "Mending synthetic data with MAPS: Model Agnostic Post-hoc Synthetic Data Refinement Framework", "abstract": "Generating high-quality synthetic data with privacy protections remains a challenging ad-hoc process, requiring careful model design and training often tailored to the characteristics of a targeted dataset. We present MAPS, a model-agnostic post-hoc framework that improves synthetic data quality for any pre-trained generative model while ensuring sample-level privacy standards are met. Our two-stage approach first removes synthetic samples that violate privacy by being too close to real data, achieving 0-identifiability guarantees. Second, we employ importance weighting via a binary classifier to resample the remaining synthetic data according to estimated density ratios. We evaluate MAPS across two healthcare datasets (TCGA-metadata, GOSSIS-1-eICU-cardiovascular) and four generative models (TVAE, CTGAN, TabDiffusion, DGD), demonstrating significant improvements in fidelity and utility while maintaining privacy. Notably, MAPS achieves substantial improvements in fidelity metrics, with 40 out of 48 statistical tests demonstrating significant improvements in marginal distributional measures and notable enhancements in correlation structure preservation and joint distribution similarity. For example, Joint Jensen-Shannon Distance reduced from ranges of 0.7888-0.8278 to 0.5434-0.5961 on TCGA-metadata and 0.6192-0.7902 to 0.3633-0.4503 on GOSSIS-1-eICU-cardiovascular. Utility improvements are equally impressive, with classification F1 scores improving from ranges of 0.0866-0.2400 to 0.3043-0.3848 on TCGA-metadata and 0.1287-0.2085 to 0.2104-0.2497 on GOSSIS-1-eICU-cardiovascular across different model-dataset combinations. Additionally, uncertainty quantification analysis via split conformal prediction demonstrates that MAPS considerably improves calibration quality, reducing average prediction set sizes by 55-77\\% while maintaining target coverage on TCGA-metadata. The code of this project is available at   https://anonymous.4open.science/r/MAPS-EBF8.", "tldr": "MAPS refines synthetic data via identifiability filtering and importance-weighted resampling, improving fidelity and utility while ensuring 0-identifiability guarantees.", "keywords": ["Genertive modeling", "Synthetic data", "Post-hoc refinement", "Privacy-Fidelity tradeoff"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f829011e38405ff6a3e468b5e52770b1f1d689ee.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MAPS, a model-agnostic post hoc framework designed to enhance the quality of synthetic data while ensuring per-sample 0-identifiability of private samples through a two-step sample selection process. The authors present extensive comparisons between the raw generated synthetic data and the refined dataset, demonstrating improved dataset distribution and downstream task fine-tuning utility. They also evaluate resistance to membership inference attacks to assess the extent of privacy protection. However, several important baseline methods are missing, and the strength of the claimed privacy guarantees remains uncertain."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tExtensive experiments are included for comparing the refined dataset and the raw dataset from both distribution perspective and model training utility perspective.\n2.\tMIA is performed as a direct demonstration of the privacy protection extent."}, "weaknesses": {"value": "1.\tSince real data privacy is repeatedly mentioned in this paper, I am curious, why do you choose 0-idenfiability metric instead of the currently widely applied differential privacy as the privacy guarantee criteria for this work? How is the differential privacy guarantee attribute of the proposed MASP? My concern main rise from experiments in section 4.4, where under some cases, MIA gains better success using 0-identifiability (refined dataset) compared with using no protection at all (raw dataset).\n2.\tLack of baseline comparisons. Privacy-first methods are only mentioned in the introduction part with a short assessment that they produce “synthetic data with notably degraded utility” without any statistical support in this paper or citation of previous papers. These methods are not compared in the experiments. How bad are these methods? If only the second selection step is applied to the generated samples of these methods, the utility of the refined dataset is at what level? Will this dataset be more robust to MIAs or will it be less robust?\n3.\tThe latest generative model used in this paper is DGD which is a model introduced in the year 2023, which is a little bit out of date. These years, there arise another line of work for private dataset synthesis, namely Private Evolution, PE, (Differentially Private Synthetic Data via Foundation Model APIs 1: Images, ICLR2024). I think this line of work should at least be mentioned. I would like to see the outcome of the combination of MAPS with PE or some reason why this should not be considered."}, "questions": {"value": "1.\tWhat’s the logic of bolding in Table 4? Some rows (e.g. Accuracy for DOMIAS) do not have a bolded number at all.\n2.\tWill the final refined dataset $\\tilde{\\mathcal{D}}$ contain repeated samples? As the selection method given in line35 to 39 in Algorithm 1 is a sampling with replacement. Why do you choose to do this? Will sampling without replacement be better as it contains more distinct samples?\n3.\tSome citations are not used in a correct format, e.g. “(Grover et al. 2019)” should be something like “Grover et al. (2019)” in line 130."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IOblnWyeJW", "forum": "LxkfjD81xB", "replyto": "LxkfjD81xB", "signatures": ["ICLR.cc/2026/Conference/Submission19681/Reviewer_VsXX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19681/Reviewer_VsXX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760604672931, "cdate": 1760604672931, "tmdate": 1762931525050, "mdate": 1762931525050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MAPS, a two stage, model agnostic post hoc refinement for tabular synthetic data. Stage 1 enforces a sample level 0 identifiability privacy constraint by removing synthetic samples closer to a real point than that point’s nearest real neighbor, implemented with an unweighted feature metric, w = 1. Stage 2 improves fidelity through classifier based density ratio estimation and Sampling Importance Resampling, with importance weights derived from a discriminator, followed by a dataset specific flattening power $\\alpha$ to stabilize resampling. Experiments across TCGA metadata and GOSSIS eICU cardiovascular with four generators show large gains in marginal and joint fidelity, correlation structure, and downstream utility, plus strong reductions in Identifiability Score to zero. Privacy results include mixed changes in membership inference recall on TCGA for some generators, which the paper discusses as a privacy fidelity trade off."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- A modular refinement pipeline that decouples privacy filtering from fidelity enhancement, applicable to diverse generators without retraining.\n- Solid empirical study with multiple fidelity metrics, utility under train on synthetic and test on real, uncertainty quantification with split conformal prediction, and several privacy probes.\n- Problem setup, equations for density ratio based weighting, and the SIR procedure are clearly laid out, with an algorithmic summary and reproducibility information.\n- Demonstrates a practical path to rescue weak synthetic tabular outputs, which is valuable for real world pipelines in health data and beyond."}, "weaknesses": {"value": "1. Stage 1 metric choice lacks statistical justification for mixed type data. The 0 identifiability guarantee is defined using a distance with w = 1 across features. For heterogeneous tabular data, unweighted norms can be dominated by scaling and encoding choices. A simple fix would be to evaluate at least one mixed data appropriate metric such as Gower distance and to report sensitivity of the Identifiability Score and the number of removed samples to the metric choice [2]. The paper should also discuss relative density based criteria such as nearest neighbor distance ratio or distance to closest record, which can better reflect risk in sparse regions than a single absolute threshold [4].\n2. Stage 2 relies on classifier based DRE, yet stability and bias are controlled with an ad hoc flattening exponent $\\alpha$ chosen per dataset, $\\alpha$ = 1.4 on TCGA and $\\alpha$ = 0.8 on GOSSIS, without a principled selection rule or sensitivity analysis. Classifier based DRE is known to suffer high variance when class separation is large. At minimum, provide an ablation of $\\alpha$ over a grid and report its effect on joint Jensen-Shannon distance, downstream F1, and conformal set size. Also justify CDRE versus alternatives like KLIEP or uLSIF that directly fit density ratios and can be more stable under shift [5, 6]. If CDRE is retained, consider early stopping and calibrated probabilities for the discriminator, and report diagnostics such as effective sample size under importance weights.\n3. Privacy interpretation requires more nuance. While Identifiability Score reaches zero across settings, some models on TCGA exhibit higher membership inference recall after refinement. This indicates that geometric privacy via nearest neighbor distance is not sufficient for statistical attacks that exploit higher fidelity to p(x). The paper should reconcile this by: clarifying the formal scope of 0 identifiability, adding density based privacy probes, and, if possible, tuning Stage 1 thresholds jointly with Stage 2 to chart a true utility privacy Pareto frontier [4].\n4. Relation to adjacent lines of work is thin. The selection discriminator and resampling resembles adversarial filtering and train reject reweight ideas. Position MAPS against such filtering frameworks and two sample testing with discriminators to make the contribution line crisper [7, 8]. The paper would also benefit from a discussion contrasting its focus on fidelity and privacy with fairness centric synthetic data work. For instance, SynthFair constructs semi synthetic imaging datasets with controllable confounders to study bias, which is complementary to MAPS. MAPS could be a pre step to high utility data on which post hoc fairness methods operate, but this interaction should be articulated and, if possible, briefly tested on a fairness proxy [1, 9].\n5. Statistical testing and calibration details could be strengthened. The paper reports paired t tests across runs but does not state checks for normality or effect sizes. Reporting confidence intervals and standardized effect sizes would make claims more robust. For conformal prediction, include the nominal coverage $\\alpha$ and show calibration curves or coverage versus set size plots to demonstrate that improvements are not due to parameter choices alone [10].\n\nReferences\n\n[1] Ribeiro FD, Claucich E, Stanley EA, Dimitrakopoulos P, Tsaftaris SA, Ferrante E, Glocker B, Echeveste R. SynthFair: A Semi-Synthetic Medical Imaging Dataset to Propel Research on Bias Detection & Mitigation. InNeurIPS 2025 AI for Science Workshop.\n\n[2] Gower JC. A general coefficient of similarity and some of its properties. Biometrics. 1971 Dec 1:857-71.\n\n[4] Elliot M, Mackey E, O'Hara K, Tudor C. The Anonymisation Decision Making Framework.\n\n[5] Sugiyama M, Nakajima S, Kashima H, Buenau P, Kawanabe M. Direct importance estimation with model selection and its application to covariate shift adaptation. Advances in neural information processing systems. 2007;20.\n\n[6] Sugiyama M, Yamada M, Von Buenau P, Suzuki T, Kanamori T, Kawanabe M. Direct density-ratio estimation with dimensionality reduction via least-squares hetero-distributional subspace search. Neural Networks. 2011 Mar 1;24(2):183-98.\n\n[7] Zellers R, Bisk Y, Schwartz R, Choi Y. Swag: A large-scale adversarial dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326. 2018 Aug 16.\n\n[8] Lopez-Paz D, Oquab M. Revisiting classifier two-sample tests. arXiv preprint arXiv:1610.06545. 2016 Oct 20. ICLR'17\n\n[9] Bellamy RK, Dey K, Hind M, Hoffman SC, Houde S, Kannan K, Lohia P, Martino J, Mehta S, Mojsilović A, Nagar S. AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. IBM Journal of Research and Development. 2019 Sep 18;63(4/5):4-1.\n\n[10] Angelopoulos AN, Barber RF, Bates S. Theoretical foundations of conformal prediction. arXiv preprint arXiv:2411.11824. 2024 Nov 18."}, "questions": {"value": "1. What exact preprocessing and encoding are used to compute distances for mixed numerical and categorical variables, and why is w = 1 appropriate for both datasets? Please provide an ablation with a mixed data metric and report its impact on the number of filtered samples and Identifiability Score. \n2. Please sweep $\\alpha$ over a broad grid on both datasets and report JSD, F1, and average conformal set size, to demonstrate robustness and to justify the chosen values. \n3.  Why CDRE over KLIEP or uLSIF for these regimes, especially when raw synthetic and real are highly separable?\n4.  Can you quantify a Pareto curve between MIA recall and JSD by jointly varying Stage 1 strictness and Stage 2 $\\alpha$, to show trade offs rather than single operating points? \n5. could you demonstrate that applying a simple post hoc fairness constraint to MAPS refined data leads to improved fairness utility trade offs compared to operating on raw synthetic or real data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NHFvszo6wf", "forum": "LxkfjD81xB", "replyto": "LxkfjD81xB", "signatures": ["ICLR.cc/2026/Conference/Submission19681/Reviewer_PzXw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19681/Reviewer_PzXw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761417354595, "cdate": 1761417354595, "tmdate": 1762931524548, "mdate": 1762931524548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed a generator-agnostic with post-hoc refinement method to improve tabular synthetic data utility while helping to comply with a record-level privacy constraint. The first stage of the method employs a privacy filter that enforces 0-identifiability by discarding synthetic samples that are closer to any real record than that record’s nearest real neighbor and also a fidelity sampler that trains a real-vs-synthetic binary classifier to estimate the density ratio. \n\nThis is a very interesting approach, especially in pairing a hard, nearest-neighbor privacy screen with a discriminative density-ratio surrogate followed by sampling-importance-resampling to curate a refined synthetic set without retraining the generator. The idea helps to solve a relevant problem where teams often cannot retrain heterogeneous tabular generators but do need a way to improve outputs for better downstream performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I liked the idea of a generator-agnostic and the refine-after-generate pipeline. While each component is not new in the literature, the combination and the operational framing for tabular synthetic data are very useful. However, the authors should improve the discussion about novelty in the paper.\n\nThe authors offer an extensive evaluation across distribution metrics, correlation structure, utility (clustering/classification), uncertainty quantification, and privacy. The results show gains across four diverse generators.\n\nThe addressed problem is a real deployment gap because practitioners often inherit synthetic data from heterogeneous models and need a post-hoc way to improve them without retraining."}, "weaknesses": {"value": "Despite the results, to rise from “useful engineering” to “field-shaping”, the paper needs stronger evidence isolating adaptation versus decoding effects and broader comparisons against alternative post-hoc curation strategies.\n\nI am not sure I understand correctly, but if the refinement classifier and distance thresholds are fitted on the full real dataset while downstream models are evaluated on a split of that same dataset, the selection step has indirectly “seen” the test distribution, biasing utility metrics upward. Please make that point clear for the reader.\n\nIn my point of view, there are insufficient baselines in experiments. For example, there is no head-to-head comparison against simple k-nearest-neighbor deduplication, discriminator rejection sampling, KDE/flow-based reweighting, or off-the-shelf curation in popular synthetic-data toolkits, making it hard to isolate where the gains actually come from. \n\nOne interesting addition to the paper is reporting probability calibration or effective sample size under different flattening exponents.\n\nI think that a single global metric for mixed-type data risks over-penalizing legitimate rare patterns or under-penalizing high-variance numeric columns, which can distort both privacy and utility in ways the current analyses do not expose.\n\nPlease discuss the computational costs for generating 30 N samples and training a sizable classifier. For example, runtime/memory vs. dimension and N."}, "questions": {"value": "I am concerned about leakage control. To avoid problems, the authors can re-run with a three-way split of real data. For training (for downstream oracle), calibration for MAPS (both Stage-1 ri and Stage-2 ccϕ ), and held-out test used only for downstream evaluation.\n\nI was wondering about distance metrics. What about comparing Stage-1 using Gower distance for mixed types, Mahalanobis, DCR/NNDR thresholds, and learned metric (e.g., via autoencoder latent space)? Maybe that can help with the privacy–utility trade-off."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wlytACUpRu", "forum": "LxkfjD81xB", "replyto": "LxkfjD81xB", "signatures": ["ICLR.cc/2026/Conference/Submission19681/Reviewer_i8fJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19681/Reviewer_i8fJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946651574, "cdate": 1761946651574, "tmdate": 1762931524243, "mdate": 1762931524243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework for refining synthetic tabular data to reduce the risk of privacy violation by removing samples that are too similar to real ones and re-sampling with a learned weight-function to improve data fidelity."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- In general, the paper is well structured.\n- The two steps of first cleaning out potential copies from the original data and then re-balancing the data can be broadly applied across multiple domains.\n- The method is evaluated on synthetic data from a representative set of generative models."}, "weaknesses": {"value": "- No related work section comparing to prior work on improving synthetic data using filtering and sampling strategies. The following works might be relevant:\n\t- Alaa, Ahmed, et al. \"How faithful is your synthetic data? sample-level metrics for evaluating and auditing generative models.\" _International conference on machine learning_. PMLR, 2022.\n\t- Wang, Hao, et al. \"Post-processing private synthetic data for improving utility on selected measures.\" _Advances in Neural Information Processing Systems_ 36 (2023): 64139-64154.\n- Privacy is defined in terms of a distance function, where samples within a certain distance of the original samples are discarded. This could open up the risk of attacks where the identity can be recovered, e.g. in cases where a person has multiple entries associated with them. For large synthetic dataset sizes, it might also be possible to discover empty hyper-spheres in the synthetic data. The original data points could then possibly be detected by interpolating the centroid from the synthetic samples on the hyper-spheres surface."}, "questions": {"value": "- How does your framework relate to previous frameworks on refining synthetic data?\n- Could you elaborate on what identifiability protection is in the context of your problem statement? More specifically in the following part: \"Our objective is to refine $\\hat{\\mathcal{D}}$  to produce a subset $\\tilde{\\mathcal{D}} \\subset  \\hat{\\mathcal{D}}$  of size $N$ that provides (1) identifiability protections with respect to $\\mathcal{D}$, [...]\". \n\t- What is the type of attack the method should protect against?\n\t- At what point do we consider an individual to be identified?\n\t- What if an individual's data is spread across multiple entries in the tables? \n- Is distance-based filtering sufficient to satisfy the privacy requirement?\n- Does the distance-based filtering approach create identifiable empty hyperspheres in the feature space? If so, could an adversary exploit these gaps to infer the approximate locations of original data points by analyzing the boundaries of the synthetic data distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IFtuqR5R3D", "forum": "LxkfjD81xB", "replyto": "LxkfjD81xB", "signatures": ["ICLR.cc/2026/Conference/Submission19681/Reviewer_KA2J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19681/Reviewer_KA2J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992798778, "cdate": 1761992798778, "tmdate": 1762931523880, "mdate": 1762931523880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}