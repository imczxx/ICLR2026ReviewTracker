{"id": "e7gbgdw05A", "number": 8997, "cdate": 1758106574967, "mdate": 1759897749108, "content": {"title": "PEERING INTO THE UNKNOWN: ACTIVE VIEW SELECTION WITH NEURAL UNCERTAINTY MAPS FOR 3D RECONSTRUCTION", "abstract": "Imagine trying to understand the shape of a teapot by viewing it from the front—you might see the spout, but completely miss the handle. Some perspectives naturally provide more information than others. How can an AI system determine which viewpoint offers the most valuable insight for accurate and efficient 3D object reconstruction? Active view selection (AVS) for 3D reconstruction remains a fundamental challenge in computer vision. The aim is to identify the minimal set of views that yields the most accurate 3D reconstruction.\nInstead of learning radiance fields, like NeRF or 3D Gaussian Splatting, from a current observation and computing uncertainty for each candidate viewpoint, we introduce a novel AVS approach guided by neural uncertainty maps predicted by a lightweight feedforward deep neural network, named UPNet. \nUPNet takes a single input image of a 3D object and outputs a predicted uncertainty map, representing uncertainty values across all possible candidate viewpoints. By leveraging heuristics derived from observing many natural objects and their associated uncertainty patterns, we train UPNet to learn a direct mapping from viewpoint appearance to uncertainty in the underlying volumetric representations. \nNext, our approach aggregates all previously predicted neural uncertainty maps to suppress redundant candidate viewpoints and effectively select the most informative one. Using these selected viewpoints, we train 3D neural rendering models and evaluate the quality of novel view synthesis against other competitive AVS methods. Remarkably, despite using half of the viewpoints than the upper bound, our method achieves comparable reconstruction accuracy. In addition, it significantly reduces computational overhead during AVS, achieving up to a 400 times speedup along with over 50\\% reductions in CPU, RAM, and GPU usage compared to baseline methods. Notably, our approach generalizes effectively to AVS tasks involving novel object categories, without requiring any additional training. We will release all code, models, and datasets.", "tldr": "An efficient active viewpoint selection algorithm by replacing time costly online reconstruction with neural uncertainty map", "keywords": ["Active viewpoint selection; efficient 3D reconstruction; viewpoint sampling; uncertainty-driven learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/26674e3c86ea932f6502791c249816d2cb7adaae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces PUN, a novel and highly efficient method for Active View Selection (AVS) in 3D reconstruction. Instead of the conventional, computationally expensive approach of retraining a neural rendering model at each step, PUN employs a lightweight network, UPNet, to directly predict a \"Neural Uncertainty Map\" from a single input view. This map guides the selection of the next most informative viewpoint. To enable this, the authors created a large-scale dataset (NUM) of images paired with pre-computed uncertainty maps. The method demonstrates significant improvements in speed (up to 400x) and computational efficiency while achieving reconstruction quality comparable to using a full set of views, and impressively generalizes to novel objects and real-world scenes without retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The core contribution is reformulating the AVS problem from an iterative optimization into a direct prediction task. This is a significant conceptual shift that effectively breaks the efficiency bottleneck plaguing prior AVS methods for neural rendering.\n- The experimental validation is extensive and convincing. The method is tested across multiple datasets ranging from synthetic objects (ShapeNet) to complex real-world scenes (NeRFAssets, MIP360), demonstrating robust generalization. The ablation studies are thorough and provide clear insights into the method's design."}, "weaknesses": {"value": "- The \"ground truth\" for the uncertainty maps is generated using the reconstruction error from a single synthesis model (Splatter-Image). UPNet may be learning the specific failure modes of this proxy model rather than a universal notion of 3D uncertainty, potentially limiting its theoretical soundness.\n- Several key design choices lack clear justification. For instance, the paper does not provide a strong rationale for using multiplicative aggregation of uncertainty over time, which is highly sensitive to low values, compared to simpler alternatives like addition.\n- The paper demonstrates impressive generalization from synthetic to real-world data but offers limited insight into why this works so well. The discussion on learning priors like object symmetry is speculative and would be strengthened by targeted experiments."}, "questions": {"value": "The \"ground truth\" uncertainty is dependent on the Splatter-Image model. How sensitive is UPNet's performance if the NUM dataset were generated with a different backbone model, for example, a NeRF-based one? Does the model learn general geometric uncertainty or just the artifacts of the proxy?\n\nCould you elaborate on the choice of the aggregation for historical uncertainty values? What are the theoretical or empirical advantages of this strategy over additive aggregation or simply taking the maximum uncertainty?\n\nThe paper speculates that generalization stems from learning priors like symmetry. Have you considered experiments on highly asymmetric or topologically complex objects (e.g., a tangled rope) to test the limits of this learned prior?\n\nHow were the hyperparameters, such as the 48 anchor points for the UMap and the 0.1 redundancy filter threshold, determined? How sensitive is the final reconstruction performance to these choices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qRoVu3rn48", "forum": "e7gbgdw05A", "replyto": "e7gbgdw05A", "signatures": ["ICLR.cc/2026/Conference/Submission8997/Reviewer_bRcV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8997/Reviewer_bRcV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760914733269, "cdate": 1760914733269, "tmdate": 1762920726296, "mdate": 1762920726296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to address the high computational cost of Active View Selection (AVS) for neural rendering methods. Traditional AVS methods for these models are slow because they require retraining the rendering model at each step to estimate uncertainty.\nThe paper proposes Peering into the UnkNowN (PUN), a method that uses a lightweight feedforward network, UPNet, to directly predict a neural uncertainty map (UMap) from a single input image. This decouples the view selection process from the costly model retraining.\n\nTo train UPNet, the paper proposes generating a Neural Uncertainty Map (NUM) dataset from ShapeNet objects. It takes a single view of an object, trains a single-view synthesis model (Splatter-Image), and then calculates the reconstruction error (e.g., PSNR) between its synthesized images and ground-truth images at 48 fixed \"anchor\" viewpoints. This 48-dimensional error vector serves as the ground-truth UMap for the initial input view.\n\nAt inference time, PUN takes the current view, passes it through the trained UPNet to predict a UMap, and aggregates these maps over time to select the next-best-view with the highest uncertainty, while filtering out redundant views. The paper claims this method is up to 400x faster than baselines, achieves comparable reconstruction accuracy to an upper bound with half the views, and generalizes to novel objects and complex, realistic scenes."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is very well-written and easy to follow. The overview figures (e.g., Figs. 1 and 2) are high-quality and provide a good overview of the proposed method and task. The structure is logical, and the claims are stated unambiguously.\n\n2. The paper's primary strength is its computational efficiency at inference time. By replacing a full retraining loop with a single forward pass of the lightweight UPNet, the authors achieve a reported 400x speedup in selection time and massive reductions in resource usage.\n\n3. The method empirically outperforms the selected baselines (A-NeRF, NVF, WD)  in reconstruction quality across most metrics on the test datasets (e.g., Table 1).\n\n4. The paper shows surprisingly good zero-shot generalization results, where UPNet, trained only on synthetic ShapeNet objects, is used to select views for complex realistic scenes from NeRFAssets and MIP360, outperforming baselines"}, "weaknesses": {"value": "1. The papers premise rests on the proposed ground truth uncertainty maps. However, they do not actually measure uncertainty (as do entropy or variance), but it is a map of the reconstruction error for a specific single-view reconstruction model. I would like to see a more formal analysis of why the model should be an effective policy for guiding a multi-view reconstruction, especially for a completely different model class like NeRF.\n\n2. I also believe that the main claim regarding efficiency is a bit misleading. The stated gains only refer to inference time. It completely ignores the massive offline computational cost required to generate the NUM dataset. The paper states it's impractical to train a model for each viewpoint. However, the solution still requires taking 1300 objects (13 categories x 100 instances), sampling 48 viewpoints for each, running a synthesis model (Splatter-Image) from each view, and generating 48 novel views from each of those. This process, repeated to create 62,400 UMap pairs, represents a colossal, one-time computational burden. The paper does not solve the computational problem; it merely amortizes it.\n\n3. In terms of novelty, I am questioning what we learn from this paper. The UPNet is trained entirely on the ShapeNet dataset. This dataset consists of single, isolated objects on white backgrounds, with views sampled on a sphere. This setup has a significant domain gap compared to any real-world application. It is highly questionable how a model trained on this data can learn a meaningful uncertainty prior for complex, 360-degree, cluttered real-world scenes beyond single objects.\n\n4. I am wondering how this method would compare against an active heuristic-based method, such as, for example, a greedy farthest point sampling instead of simply uniform sampling. \n\n5. The UMap is not a continuous map. It is a fixed 48-dimensional vector corresponding to 48 predefined, fixed relative anchor poses. This is a strong, discretizing assumption that limits the granularity of the uncertainty prediction."}, "questions": {"value": "Can the paper provide a stronger conceptual justification for why predicting the reconstruction error of a single-view 3DGS model is a valid and effective proxy for guiding a multi-view NeRF reconstruction?\n\nGiven the massive domain gap between the ShapeNet training data (isolated objects, white background) and the MIP360 test data (complex, cluttered 360 scenes), what features does UPNet actually learn that allow it to generalize so effectively? \n\nHow are the ground truth views chosen? Are they always the same 48 poses, or are they variable? Why do we choose 48 views? Why not 24, 64, or some other number? Is this ablated somewhere?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TjodcmIMqK", "forum": "e7gbgdw05A", "replyto": "e7gbgdw05A", "signatures": ["ICLR.cc/2026/Conference/Submission8997/Reviewer_yuxC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8997/Reviewer_yuxC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864048303, "cdate": 1761864048303, "tmdate": 1762920725876, "mdate": 1762920725876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles next-best-view selection for 3D reconstruction. The authors fine-tune a ViT to predict, from a single input image, a reconstruction uncertainty/score map. Supervision for fine-tuning is derived from PSNR, SSIM, LPIPS, or MSE computed on outputs of Splatter-Image, a monocular 3D reconstruction model; training data is curated from ShapeNet. At inference, the next best view is selected sequentially: uncertainties at each candidate view are interpolated from a fixed set of spherical anchors and aggregated across timesteps. Experiments on synthetic and real scenes show the method achieves comparable reconstruction quality with fewer views and lower computation, and the selection generalizes to different reconstruction backbones (e.g., 3DGS)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Originality: Next-best-view selection is a classic problem in 3D vision; the paper’s backbone-agnostic approach—predicting a feed-forward uncertainty/score map for view selection—is a novel angle within this space.\n\n* Quality:\n\n  * The solution is simple and effective.\n  * The evaluation is comprehensive, covering both synthetic and real scenes, with robustness tests (e.g., lighting and distance). Generalization to multiple reconstruction backbones is also tested and supports the claim.\n\n* Clarity:\n\n  * The paper is well organized and clearly written, especially in framing the problem and motivation.\n  * The proposed method is introduced clearly, with the selection pipeline easy to follow.\n\n* Significance:\n\n  * The approach improves efficiency of existing 3D reconstruction pipelines while maintaining comparable performance.\n  * Backbone agnosticism enables plug-and-play use in arbitrary pipelines.\n  * The method’s simplicity makes it easy to extend for follow-up research."}, "weaknesses": {"value": "* Uncertainty definition: The paper uses PSNR/SSIM/LPIPS/MSE as “uncertainty” labels; all are photometric and ignore geometry quality. This limits relevance for downstream tasks that depend on accurate shape.\n\n* Anchor design lacks justification: The anchor layout (48 HEALPix points) is fixed without analysis; it is unclear whether performance is bottlenecked by anchor density or discretization choice.\n\n* Candidate sampling is unclear: “Randomly sample 512 candidates” is under-specified (sphere-uniform vs. azimuth/elevation uniform vs. more sophisticated sampling). The chosen distribution can materially affect results.\n\n* Scope beyond object-centric canonical sphere: Training/evaluation assume canonical camera shells around single objects; performance and assumptions may not transfer to indoor/outdoor scenes with general camera placement, depth variation, and occlusions.\n\n* Single-image conditioning vs. relative view dependency: The ViT takes only one image, yet the output implicitly depends on the relative pose between the input view and the 48 anchors; the current design does not encode this dependency explicitly."}, "questions": {"value": "- First, please refer to the weakness.\n- Second, are there cases where high uncertainty ≠ reconstruction gain?\n- Third, does the model has explicit pose conditioning and why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gd8pPL1J8l", "forum": "e7gbgdw05A", "replyto": "e7gbgdw05A", "signatures": ["ICLR.cc/2026/Conference/Submission8997/Reviewer_3VjJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8997/Reviewer_3VjJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937826276, "cdate": 1761937826276, "tmdate": 1762920725239, "mdate": 1762920725239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles AVS for 3D reconstruction by predicting a neural uncertainty map from a single current view. A lightweight feed-forward network, UPNet, takes one image and outputs uncertainty values over all candidate viewpoints. Using the selected views, standard neural rendering models (NeRF/3DGS) are trained and evaluated against competing AVS methods. The approach attains comparable reconstruction accuracy while dramatically reducing computational cost with up to 400× speedup and >50% lower CPU/RAM/GPU usage. It can also generalize to novel object categories without extra training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "This idea is very interesting: training a feed-forward uncertainty prediction network on a self-constructed dataset with ground truth supervision for AVS task. This can greatly reduce the computational resources and time required for next-best-view selection of AVS task. Experiments show that, compared with baseline AVS methods, the proposed approach exhibits clear improvements."}, "weaknesses": {"value": "This paper proposes an interesting and effective method for the AVS task. However, my main concerns are the experimental evaluation and the value of this task. I find it hard to imagine a scenario where, given a single input view, the system should output where the next view should be obtained. For robotic applications, if it has the ability to move from one viewpoint to another, then it can capture dense views at arbitrary positions. For multi-view reconstruction, all ground truth captured views are already provided, and there is no need to select a subset for training.\n\nIf the authors claim that the goal of this task is to reduce the number of training views so as to reduce training time while maintaining quality, then I believe the evaluation in this work is incomplete. At a minimum, it should include comparisons of rendering quality and training resources with the following: (1) 3DGS trained on all available views. (2) pipelines of sparse-view NVS 3DGS (e.g. FSGS, Binocular3DGS) based on the selected small set of views."}, "questions": {"value": "1. In the experimental setup, the authors removed real-scene views that are not centered on the object. In practical applications, what happens if the initial view is not oriented toward the object center?\n\n2. The UPNet gives sampling quality at 48 uniformly distributed positions on the sphere. However, in real scenes, a full 360-degree range of view may not be available, for example, some of the views may be under the ground (MipNeRF360 dataset). If the next best view is predicted in such a region, how should it be handled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zRY9kzCkBC", "forum": "e7gbgdw05A", "replyto": "e7gbgdw05A", "signatures": ["ICLR.cc/2026/Conference/Submission8997/Reviewer_Wvnw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8997/Reviewer_Wvnw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762099186854, "cdate": 1762099186854, "tmdate": 1762920724515, "mdate": 1762920724515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}