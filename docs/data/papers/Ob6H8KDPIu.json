{"id": "Ob6H8KDPIu", "number": 13596, "cdate": 1758219607623, "mdate": 1763629877204, "content": {"title": "Large Language Models Develop Novel Social Biases Through Adaptive Exploration", "abstract": "As large language models (LLMs) are adopted into frameworks that grant them capacities to make real decisions, the consequences of their social biases intensify. Yet, we argue that simply removing biases from models is not enough. Using a paradigm from the psychology literature, we demonstrate that LLMs can spontaneously develop novel social biases about artificial demographic groups even when no inherent differences exist. These biases lead to highly stratified task allocations, which are less fair than assignments by human participants and are exacerbated by newer and larger models. Emergent biases like these have been shown in the social sciences to result from exploration-exploitation trade-offs, where the decision-maker explores too little, allowing early observations to strongly influence impressions about entire demographic groups. To alleviate this effect, we examine a series of interventions targeting system inputs, problem structure, and explicit steering. We find that explicitly incentivizing exploration most robustly reduces stratification, highlighting the need to incorporate better multifaceted objectives to mitigate bias. These results reveal that LLMs are not merely passive mirrors of human social bias, but can actively create new ones from experience, raising urgent questions about how these systems will shape societies over time.", "tldr": "", "keywords": ["Large Language Models", "Psychology", "Cognitive Science", "Fairness", "Bias"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/33ea58b009ac26ec1837d5f1450df5ee6edad071.pdf", "supplementary_material": "/attachment/e7576e9708c50dc7d9fd37c8f98464774b08ff20.zip"}, "replies": [{"content": {"summary": {"value": "This paper analyzes the biases present in LLMs, particularly those generated in multi-turn simulations. Different metrics are proposed to analyze a wide range of experimental results. The authors also investigate the forms that biases and explore mitigation methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The multi-turn scenarios that the authors attempt to explore have not been widely studied, which could be regarded as a novel research topic.\n\n2. The authors use multiple LLMs and attempt to evaluate them using different metrics."}, "weaknesses": {"value": "1. The authors compare the simulation results of LLMs with those from human participants, but lack descriptions of the human participants, such as the sample size and distribution of demographic variables. \n\n2. The authors need to provide more explanation for the three newly defined metrics. For example, how do SI and mutual information differ in form? What are the similarities and differences among BGD, GASI, and JSD? \n\n3. If I understand correctly, the values of SI and BGD should be 0 under random conditions, but this doesn't seem to be the case in Figures 2, 4 and 5.\n\n4. There are still some points that are not easy to understand, please refer to the questions section below.\n\nMinor issue: The Figures in Appendix B are difficult to read."}, "questions": {"value": "1. If the same person or multiple individuals' information is reused for prompts, how will the results differ across different rounds?\n\n2. The authors use the default temperature in their simulations. Would the conclusion change if the temperature are set to the maximum or minimum?\n\n3. Does the width of the human data band in Figure 2 represent the standard deviation? If not, what is their standard deviation?\n\n4. In the era before LLMs, biases may also be amplified in multi-turn evaluations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iGxhrmG5Vk", "forum": "Ob6H8KDPIu", "replyto": "Ob6H8KDPIu", "signatures": ["ICLR.cc/2026/Conference/Submission13596/Reviewer_kwFn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13596/Reviewer_kwFn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874740037, "cdate": 1761874740037, "tmdate": 1762924183290, "mdate": 1762924183290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to mention the potential of LLM developing novel social biases about artificial demographic groups through interaction. The authors directly compare the LLM results with the human results and find that LLMs can develop these biases even when no inherent differences exist. They then propose a serious of interventions."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is very well-motivated: lots of work have looked at how LLMs could be biased due to fundamental human data distribution but there’s little work in exploring how LLMs might form novel biases through interactions. I like that the authors engage with the literature across many fields with a good amount of depth in making the arguments and describing the background of this study. It is also very nice to have human baseline in a directly comparable setting."}, "weaknesses": {"value": "Weaknesses:\n\n1)\tThe study models agentic behavior using a multi-turn dialogue where the entire history is passed in-context. This setup, while controlled, does not fully capture the architecture of modern agentic systems. Such systems often employ more sophisticated mechanisms like structured memory, explicit reflection steps (e.g., ReAct), and meta-cognitive abilities to decide whether a given experience is valuable enough to be integrated into its knowledge base. By \"forcing\" the model to learn from every turn via in-context learning, the experiment may be inadvertently creating a scenario that is highly conducive to the over-generalization it observes. The degree to which these biases emerge in agents with more robust memory and reflection capabilities remains an open question.\n\n2)\tNewer and larger models have a greater tendency to stratify -> an explanation is simply that larger model learn better in context? \nAlso it might be better to present the same result in Figure 3 by plotting the stratification index against standard capability benchmarks (e.g., MMLU, Arena). I suppose with figure 3 the point you are really making is how the stratification tendency changes with model capability, right?\n\n3)\tTo what degree the result of this study generalize to more real-life cases? I get that in order to avoid measuring existing biases and establish internal validity, you have to create an artificial city with artificial group labels? But then because the LLM also clearly knows this is an artificial setting, it might just act without the normative constraints that it might apply to real demographic groups? In other words, perhaps you are testing bias formation in a “jail-breaked” setting?\n\n4)\tRegarding the hair color and tattoo shape result is line 376, do you have any evidence to indicate that these are indeed spurious signals? These features might be correlated with sociodemographic features that might be important?\n\n5)\tJust out of curiosity, how do you think your result interact with pre-existing bias, in a more realistic setting? If the task used real demographic groups. would the models lock onto existing stereotypes even faster? Or perhaps the random evidence in context actually reduce the pre-existing model bias?"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rG0ZRqEtFA", "forum": "Ob6H8KDPIu", "replyto": "Ob6H8KDPIu", "signatures": ["ICLR.cc/2026/Conference/Submission13596/Reviewer_vqkN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13596/Reviewer_vqkN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996402399, "cdate": 1761996402399, "tmdate": 1762924182755, "mdate": 1762924182755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates emerging biases that LLMs develop in the multi-turn setting. They find that not only existing bias but also emerging bias can be significant issues for real-world applications such as hiring decision-making. They conduct a game of a sequential hiring paradigm following the existing work. Their result reveals that even though four demographic groups actually have the same success rate across jobs, LLMs develop their own biases for each demographic group, similar to human participants. Even LLMs showed bigger biases than humans. Lastly, they test several interventions, such as prompt steering, to reduce the emerging bias."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Investigate emerging biases, which have been underexplored\n- Test many models across six families and various schemes such as CoT\n- Explore interventions to reduce the emerging biases"}, "weaknesses": {"value": "While the paper is well written and offers insights into emerging biases in LLMs, the paper has limited novelty and contribution in my opinion. \n\nThe results themselves are straightforward; when only demographic group information is available, models should naturally use that information to maximize their incentives. Providing more information about candidates would have reduced this effect (as demonstrated in the paper), because additional information allows the model to rely on other signals for decision-making. However, this introduces existing biases in the models.\n\nThe paper evaluates bias emergence within a single domain-specific scenario (hiring simulation). The task itself is from the existing paper, and the observed biases are easily mitigated using straightforward prompt-steering techniques. These points collectively limit the paper’s overall novelty and contribution."}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kdP2IEctkY", "forum": "Ob6H8KDPIu", "replyto": "Ob6H8KDPIu", "signatures": ["ICLR.cc/2026/Conference/Submission13596/Reviewer_MUv3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13596/Reviewer_MUv3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762043862263, "cdate": 1762043862263, "tmdate": 1762924181743, "mdate": 1762924181743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank the reviewers for each providing constructive and insightful feedback for our paper! \n\nIn particular, we appreciate that all three reviewers noted that our work investigates underexplored/novel questions, and that reviewer vqkN specifically acknowledged our paper as very well-motivated and engaging with a broad literature. \n\nIn our rebuttals, **we provide a tailored response to each weakness and question from each reviewer, backed up by new experiments and changes to the pdf** (marked in blue). \n\nWe also provide a list of key changes and invite reviewers to peruse them at their convenience:\n- Domain generality: new experiments in military conscription and refugee reallocation settings. (MUv3 point 1)\n&nbsp;\n- Removal of gamified incentives: these new settings do not contain points for successful assignments. (MUv3 point 2)\n&nbsp;\n- Generalization to agentic systems: ReAct framework (vqkN point 1)\n&nbsp;\n- Comparison between stratification and BBQ performance: strong opposite trend (vqkN point 2)\n&nbsp;\n- Real demographics: new experiment testing socially salient (White, Asian, Black, Hispanic) demographics (vqKN point 5)\n&nbsp;\n- Expanded human participant information (kwFn point 1)\n&nbsp;\n- Metric justification: equivalence proof of Mutual Information vs. Stratification Index (kwFn point 2)\n&nbsp;\n- Metric explanation: BGD, GASI, and JSD (kwFn point 2)\n&nbsp;\n- Metric convergence under large sample sizes (kwFn point 3)\n&nbsp;\n- Interpretation for Appendix E (old Appendix B): Visualization of model differences (kwFn point 4)\n&nbsp;\n- Expanded temperature generality: T=0 and T=1.5 across frontier models (kwFn point 6)\n&nbsp;\n\nTo all reviewers, please feel free to respond with any questions or comments. \n\nSincerely, \n\nAuthors"}}, "id": "QFaJFDXdS6", "forum": "Ob6H8KDPIu", "replyto": "Ob6H8KDPIu", "signatures": ["ICLR.cc/2026/Conference/Submission13596/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13596/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission13596/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763629746626, "cdate": 1763629746626, "tmdate": 1763629746626, "mdate": 1763629746626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}