{"id": "f4TACE7HhU", "number": 5500, "cdate": 1757915966306, "mdate": 1759897970810, "content": {"title": "GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs", "abstract": "Object hallucination in Multimodal Large Language Models (MLLMs) is a persistent failure mode that causes the model to perceive objects absent in the image. This weakness of MLLMs is currently studied using static benchmarks with fixed visual scenarios, which preempts the possibility of uncovering model-specific or unanticipated hallucination vulnerabilities. We introduce GHOST (Generating Hallucinations via Optimizing Stealth Tokens), a method designed to stress-test MLLMs by actively generating images that induce hallucination. GHOST is fully automatic and requires no human supervision or prior knowledge. It operates by optimizing in the image embedding space to mislead the model while keeping the target object absent, and then guiding a diffusion model conditioned on the embedding to generate natural-looking images. The resulting images remain visually natural and close to the original input, yet introduce subtle misleading cues that cause the model to hallucinate. We evaluate our method across a range of models, including reasoning models like GLM-4.1V-Thinking, and achieve a hallucination success rate exceeding 28%, compared to around 1% in prior data-driven discovery methods. We confirm that the generated images are both high-quality and object-free through quantitative metrics and human evaluation. Also, GHOST uncovers transferable vulnerabilities: images optimized for Qwen2.5-VL induce hallucinations in GPT-4o at a 66.5% rate. Finally, we show that fine-tuning on our images mitigates hallucination, positioning GHOST as both a diagnostic and corrective tool for building more reliable multimodal systems.", "tldr": "", "keywords": ["Hallucinations", "Multimodal Large Language Models", "Spurious Correlations"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dccd627d91130cd479858b6bc17d6d3d6b29cfda.pdf", "supplementary_material": "/attachment/a1cad0ee21935eba9f26151a0dbc0e88a28984a4.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces GHOST, a method for automatically generating images designed to induce object hallucinations in VLM. Unlike static benchmarks , GHOST actively synthesizes images by optimizing a CLIP image embedding to elicit a hallucinated response while simultaneously regularizing against inserting the object's semantics and staying close to an original image embedding . The authors demonstrate GHOST's effectiveness in inducing hallucinations across various MLLMs, including reasoning models, and show that these hallucinations are highly transferable, even to closed-source models like GPT-4o. Finally, preliminary experiments suggest that finetuning an MLLM on GHOST-generated images can improve its robustness against hallucinations on GHOST's own generated data and the POPE benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The work constructs a benchmark for VLM hallucination vulnerability from a novel perspective by optimizing visual embeddings, demonstrating effective hallucination-inducing attacks against sota open-source models. Furthermore, it shows that hallucinations induced on open-source models can effectively transfer to sota closed-source VLMs, suggesting shared vulnerabilities.\n\n2.The paper demonstrates that finetuning models using the data generated by GHOST can reduce object hallucination, showing improvements on both the GHOST-generated adversarial images and established hallucination benchmarks like POPE."}, "weaknesses": {"value": "1.A significant concern is the potential distribution gap between the images decoded from the optimized visual embeddings and natural images. For example, the right image in Figure 1 appears somewhat unnatural . It is questionable whether using such potentially out-of-distribution images constitutes a fair or meaningful way to attack and evaluate the robustness of MLLMs against real-world hallucinations.\n\n2.The experiments validating the mitigation effect of finetuning on GHOST data seem insufficient. While results on POPE and GHOST's own data are shown, the evaluation would be more convincing if it included results on other recognized hallucination benchmarks such as CHAIR or HallusionBench to demonstrate broader robustness improvements."}, "questions": {"value": "1.GHOST's attack generation involves an adversarial loss tailored to a specific victim model . Given the observed transferability, could a potentially universal hallucination dataset be constructed by optimizing against a set of diverse open-source models? Could such a dataset be effectively used for evaluating and perhaps even finetuning other models to improve general hallucination robustness?\n\n2.The approach of finetuning on GHOST images, adversarially generated images causing hallucination, conceptually resembles methods like PerturboLLaVA, which finetunes on perturbed text to mitigate hallucinations. Could the mathematical framework or explanation provided by PerturboLLaVA be adapted to theoretically explain why finetuning on GHOST's visually perturbed, hallucination-inducing images improves the model's resistance to object hallucination?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ivUlUgzXcZ", "forum": "f4TACE7HhU", "replyto": "f4TACE7HhU", "signatures": ["ICLR.cc/2026/Conference/Submission5500/Reviewer_GNtm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5500/Reviewer_GNtm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559567728, "cdate": 1761559567728, "tmdate": 1762918098389, "mdate": 1762918098389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GHOST (Generating Hallucinations via Optimizing Stealth Tokens), a pipeline for stress‑testing multimodal large language models (MLLMs) by automatically generating images that induce object hallucinations. Starting from an input image and a target object absent from that image, GHOST optimizes the image’s CLIP embedding to increase the MLLM’s probability of answering “yes” to a query about the target’s presence, while regularizing the embedding to remain close to the original and discouraging direct encoding of the object’s semantics. To avoid backpropagating through the diffusion model and the MLLM, the authors train a mapper (a simple MLP) that aligns the CLIP vision embedding space with the MLLM’s vision encoder, enabling efficient gradient updates. Once the optimized embedding reaches a confidence threshold, an unCLIP diffusion model generates an image conditioned on this embedding, starting from a partially noised latent of the original image to preserve overall structure. An open‑vocabulary detector filters out images where the target object actually appears, ensuring that only hallucinations are counted."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. GHOST introduces a principled pipeline to generate hallucination‑inducing images.\n\nS2. Images optimized for one model induce hallucinations in others, highlighting shared failure modes across architectures.\n\nS3. Fine‑tuning on GHOST images reduces hallucinations and improves POPE scores while retaining performance on VQAv2 and captioning."}, "weaknesses": {"value": "W1. Experiments focus on 10 COCO object classes. The method may perform differently on a broader set of objects, particularly those with abstract or ambiguous semantics. The human evaluation sample is limited to 40 volunteers, which may not capture diverse perceptions.\n\nW2. The OWLv2 detector filters out samples where the target object is present, but open‑vocabulary detectors are not perfect; subtle insertions might slip through, biasing success rates.\n\nW3. Fine‑tuning is demonstrated only on Qwen2.5‑VL with synthetic positives and negatives. More robust experiments (e.g., across models or with real images) and analysis of downstream safety implications would strengthen the claim that GHOST can act as a corrective tool.\n\nW4. quantify the per‑image runtime or GPU requirements; scaling to many objects or higher resolutions could be prohibitive.\n\nW5. GHOST hinges on the availability of CLIP vision embeddings and an unCLIP diffusion model. It is unclear whether other vision encoders or diffusion architectures would offer similar alignment."}, "questions": {"value": "Q1. What is the average time to generate a hallucination‑inducing image, and how does this scale with image resolution and the number of target objects? Could the process be parallelized or approximated to reduce cost?\n\nQ2. Have you tested GHOST with alternative diffusion architectures (e.g., SDXL, DiT) or other vision encoders (e.g., EVA CLIP)? Would the mapper still align the embedding spaces effectively?\n\nQ3. How sensitive are the results to the choice of OWLv2? Could false negatives in detection inflate success rates? Did you manually inspect filtered samples to ensure the target object was truly absent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qsdaRpJadl", "forum": "f4TACE7HhU", "replyto": "f4TACE7HhU", "signatures": ["ICLR.cc/2026/Conference/Submission5500/Reviewer_8Ved"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5500/Reviewer_8Ved"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716519211, "cdate": 1761716519211, "tmdate": 1762918098066, "mdate": 1762918098066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new method for inducing object hallucination in MLLMs. \nTo address this issue, the paper utilizes an image generation model to automatically create images that preserve the semantic meaning of the original inputs while still being able to trigger hallucinations.\nThrough extensive experiments, the method demonstrates a significantly higher hallucination success rate compared to previous approaches across multiple models, confirming its effectiveness. \nAdditionally, the paper shows that images generated for one model can induce hallucinations in other models, and that fine-tuning with generated data helps mitigate hallucination, highlighting its dual role as both a diagnostic and corrective tool."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Unlike prior benchmarks relying on static datasets,  this approach dynamically generates images that preserve the original semantics while triggering hallucinations, providing a fully automated way.  \n    \n* The framework is well-designed and clearly presented. Each component is justified, and the ablation studies effectively demonstrate their contributions.  \n    \n* The experiments are extensive and evaluate on a diverse set of MLLMs using various metrics."}, "weaknesses": {"value": "Major Weakness\n\n* **Insufficient Motivation for Human-Perceptible Modifications:** While GHOST introduces “human-perceptible” changes, the paper does not explain why human perceptibility is a necessary design choice. This aspect distinguishes GHOST from adversarial methods but is presented without motivation.  \n    \n* **Unclear Justification for Using SD unCLIP as the Semantic Preservation Backbone:** The paper adopts SD unCLIP as the image generation backbone to preserve semantic content from the original image, yet it does not clearly justify why this model was chosen over other diffusion variants.   \n  Moreover, Table 2 shows inconsistent FID scores across models: While SD unCLIP achieves low FID values for some settings, in other cases its FID is high. This raises questions about whether SD unCLIP is a reliable method for maintaining semantic similarity.\n\n* **Fairness of Comparison with DASH:** Table 1 compares GHOST with DASH, yet the evaluation settings differ significantly. Although both methods share the same goal of inducing hallucinations via false-positive responses, DASH is fundamentally designed for large-scale, open-world datasets (e.g., ReLAION-5B) to detect *systematic hallucination patterns* across diverse object classes. In contrast, GHOST operates on a COCO-based subset. Given this difference in scale and design philosophy, directly comparing hallucination success rates on COCO may not be fair. \n\nMinor Weakness\n\n* **Clarity in Figure 3**  \n  * The black dashed arrow is ambiguous; please clarify what operation or information flow it represents.\n\n* **Typos and Formatting Issues**  \n  * Line 235–236: In Equation (2), the two terms involving X\\_v​ appear inconsistent.  \n      \n  * Appendix H.2: There seems to be an empty placeholder for *unCLIP*."}, "questions": {"value": "* Could you clarify why *human-perceptible* modifications were chosen as a design goal? What advantages do these perceptible cues provide over imperceptible perturbations in evaluating hallucination?  \n    \n* Why was **SD unCLIP** selected as the semantic preservation backbone instead of other diffusion variants? Table 2 shows inconsistent FID results across models, suggesting that SD unCLIP may not always guarantee semantic fidelity  \n    \n* In Table 1, the results may not be entirely fair since GHOST and DASH were designed with different objectives. How could the two methods be compared in a more balanced and fair manner?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RvDLY7l3RO", "forum": "f4TACE7HhU", "replyto": "f4TACE7HhU", "signatures": ["ICLR.cc/2026/Conference/Submission5500/Reviewer_PFEa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5500/Reviewer_PFEa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995903922, "cdate": 1761995903922, "tmdate": 1762918096185, "mdate": 1762918096185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GHOST, a gradient-free method that generates images designed to induce hallucinations in text-to-image and multimodal models, and establishes a benchmark for measuring hallucination robustness. Experiments show that GHOST achieves high attack success rates and strong transferability across multiple open-source models, revealing both model-specific weaknesses and shared vulnerabilities. The authors also explore simple mitigation strategies, such as fine-tuning, which help reduce but not fully eliminate the hallucination effect, demonstrating GHOST’s potential as a tool for evaluating and improving model robustness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper proposes an effective model-specific attack that generates adversarial images capable of inducing hallucinations in a target MLLM regarding a given target object in the original image.\n\n* The generated images demonstrate strong transferability, degrading the performance of other models beyond the specific target model. This suggests that the proposed approach reveals not only model-specific weaknesses but also shared vulnerabilities across models."}, "weaknesses": {"value": "* The proposed method is limited to inducing misperception of object existence and does not address other types of hallucination, such as attribute or relationship hallucinations. This limitation is reflected in Table 4, where improvements are observed mainly on POPE—a benchmark focused on object-centric VQA—while no significant gains are found on VQAv2 or Caption benchmarks. This implies that the method may not generalize to mitigating broader hallucination types.\n\n* The adversarially generated images (e.g., Figures 1, 2, 4, and 6) appear somewhat unnatural. This raises concerns about the practical impact of improving robustness against such images in real-world applications that primarily involve natural images. From an impact perspective, it would strengthen the work if the human evaluation included not only whether the target object was preserved or removed, but also whether the generated images looked natural to human observers. The current reliance on FID scores is insufficient to fully assess perceptual naturalness.\n\n* The proposed method requires multiple optimization steps per image, which may reduce its efficiency. A quantitative analysis of this efficiency—such as reporting the hit ratio (i.e., success rate among optimized samples)—would be helpful.\n\n* The approach involves numerous hyperparameters (e.g., thresholds and control factors), which implies that substantial model-specific tuning may be required to achieve optimal attack performance. This limits its scalability and reproducibility.\n\n* Minor issue: The paper could improve notation consistency—for example, the fonts for λ_clip and λ_reg differ in several equations."}, "questions": {"value": "* The method regularizes the generated image to remain close to the original image (Equation 3). From an attack perspective, it is unclear why maintaining similarity to the original image is important, since the primary goal is to induce hallucination in the model. A clearer justification for this design choice would be valuable.\n\n* It would be interesting to see how the proposed method performs on image domains other than COCO, as this would test the generalization and robustness of GHOST across more diverse visual distributions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XT6x7g6hYP", "forum": "f4TACE7HhU", "replyto": "f4TACE7HhU", "signatures": ["ICLR.cc/2026/Conference/Submission5500/Reviewer_6FPP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5500/Reviewer_6FPP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006543447, "cdate": 1762006543447, "tmdate": 1762918095700, "mdate": 1762918095700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GHOST. It first optimizes a CLIP image embedding. It keeps the image close to the source. It pushes away from the target object. It also raises the model’s chance of saying “Yes” to the object query. A small mapper links CLIP space to the vision tokens of the MLLM. After optimization, the method uses unCLIP to render the final image. The images do not contain the object, yet many MLLMs claim the object is present. The method beats a prior approach by a large margin. The authors also use the images to fine-tune models and reduce hallucinations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The pipeline is simple, modular and well motivated. \n\n- The success rates are strong across many MLLMs. The transfer rate to closed models is also high.\n\n- Preliminary mitigation results via fine-tuning suggest utility beyond red-teaming."}, "weaknesses": {"value": "- The pipline heavly relianced on a single detector (OWLv2); possible false-negatives not quantified; no ensemble verification.\n\n- Limited baseline coverage beyond DASH; no ablations versus strong pixel-space attacks adapted to MLLMs. It should include more public hallucination benchmarks.\n\n- The closed-model settings are not fully described. The compute cost and runtime are not reported. Please ddd default hyperparameters in the main text.\n\n- Minors writing issues (Also in Appendix)"}, "questions": {"value": "What detector threshold did you use?\n\nDid you try an ensemble of detectors?\n\nWhat are the default hyperparameters for the main results?\n\nWhat are the prompts and decoding settings for closed models?\n\nFor reasoning models, when does a “Yes” in thinking flip to “No” in the final answer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s2ThdhqFu3", "forum": "f4TACE7HhU", "replyto": "f4TACE7HhU", "signatures": ["ICLR.cc/2026/Conference/Submission5500/Reviewer_ZKuB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5500/Reviewer_ZKuB"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission5500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762161791804, "cdate": 1762161791804, "tmdate": 1762918094763, "mdate": 1762918094763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}