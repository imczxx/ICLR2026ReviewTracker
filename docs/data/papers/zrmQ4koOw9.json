{"id": "zrmQ4koOw9", "number": 23091, "cdate": 1758339453617, "mdate": 1759896832591, "content": {"title": "WSVD: Weighted Low-Rank Approximation for Fast and Efficient Execution of Low-Precision Vision-Language Models", "abstract": "Singular Value Decomposition (SVD) has become an important technique for reducing the computational burden of Vision Language Models (VLMs), which play a central role in tasks such as image captioning and visual question answering. Although multiple prior works have proposed efficient SVD variants to enable low-rank operations, we find that in practice it remains difficult to achieve substantial latency reduction during model execution. To address this limitation, we introduce a new computational pattern and apply SVD at a finer granularity, enabling real and measurable improvements in execution latency. Furthermore, recognizing that weight elements differ in their relative importance, we adaptively allocate relative importance to each element during SVD process to better preserve accuracy, then extend this framework with quantization applied to both weights and activations, resulting in a highly efficient VLM. Collectively, we introduce~\\textit{Weighted SVD} (WSVD), which outperforms other approaches by achieving over $1.8\\times$ decoding speedup while preserving the accuracy performance.", "tldr": "", "keywords": ["Vision language model", "singular value decomposition", "quantization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39fcf8746ff7f3823acfa5a3500007499c92ac5e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method that accelerates vision-language model inference by combining Weighted Singular Value Decomposition (WSVD) with low-precision quantization. Unlike standard SVD-based compression, WSVD assigns importance weights to parameters to better preserve critical information during low-rank approximation. The approach further integrates quantization, leading to significant latency reduction, reportedly achieving faster decoding with minimal accuracy loss. Overall, WSVD aims to offer a practical and effective way to deploy large VLMs more efficiently without sacrificing performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a practical and well-motivated approach to enhancing the efficiency of vision-language models. In particular, it provides a comprehensive exploration of memory reduction strategies from multiple perspectives, including low-rank decomposition and quantization, and proposes a corresponding system implementation to support these ideas.\n- The paper is clearly written and well organized, making the contributions easy to follow."}, "weaknesses": {"value": "I'm not very familiar with this specific research area, such as quantization and system implementation. Therefore, it is difficult for me to precisely assess the novelty of the proposed method relative to existing work.\n\n- From my perspective, the contribution over prior work appears incremental. Please refer to the question below for clarification.\n- The experimental validation is limited to only two datasets, and broader comparisons across more diverse datasets are missing.\n- The related work section omits several important studies on weighted matrix factorization, which weakens the paper’s novelty claim."}, "questions": {"value": "- The computational cost of reconstruction for each head is $LrH$. Wouldn’t it be equivalent to simply reducing $R$ to $r$ in the original $LRH$ formulation? How is this approach fundamentally different?\n- Why is the square root used in the Fisher matrix in Equation (8)?\n- There exist several established methods for measuring parameter importance [1, 2]. Why were these approaches not explored or compared? Is the proposed method guaranteed to be optimal?\n- The paper states that FWSVD utilizes Fisher information. How exactly do FWSVD and WSVD differ in the way they incorporate Fisher information?\n\nReferences\\\n[1] Ancona, M., Ceolini, E., Öztireli, C., & Gross, M. (2017). Towards better understanding of gradient-based attribution methods for deep neural networks. arXiv preprint arXiv:1711.06104.\\\n[2] Bach, S., Binder, A., Montavon, G., Klauschen, F., Müller, K. R., & Samek, W. (2015). On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7), e0130140."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nIiZDxqkmC", "forum": "zrmQ4koOw9", "replyto": "zrmQ4koOw9", "signatures": ["ICLR.cc/2026/Conference/Submission23091/Reviewer_YkZ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23091/Reviewer_YkZ1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23091/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716310683, "cdate": 1761716310683, "tmdate": 1762942507653, "mdate": 1762942507653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "WSVD proposes a practical way to make vision-language models decode faster—not just smaller—by combining per-head low-rank factorization with smart tuning, quantization, and a fused kernel. It starts from the observation that plain SVD shrinks parameters and KV cache but often slows decoding because full keys/values must be reconstructed every step. WSVD instead factorizes each attention head separately so each head caches tiny latents and reconstructs only what it needs; then it uses a Fisher-weighted objective to fine-tune the low-rank factors so the most important weights are preserved. Afterward, it applies rotation-aware low-precision quantization and a short round of QAT to recover accuracy. On the systems side, a Triton kernel folds reconstruction directly into Flash Decoding so K/V are never materialized in VRAM, cutting memory traffic and kernel launches. Across LLaVA-Next and SmolVLM families on 4090/5090-class GPUs, this yields up to ~1.8× lower latency at similar cache sizes (≈25% of FP16) with ~1% average accuracy drop, and ablations show both the weighted objective and QAT are key to those results. The approach shines in memory-bound decoding (long contexts, image-heavy prompts) if brief local QAT is acceptable, though gains can vary with model and hardware."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Overall, this paper excels by sharply diagnosing why plain SVD can slow decoding and giving a compact analysis showing that per-head SVD cuts reconstruction compute and memory by a factor of r/R; it then backs that theory with a well-engineered Triton fused kernel that integrates into Flash Decoding to avoid materializing full K/V in VRAM; and it demonstrates measured gains—up to 1.8× faster decoding—while maintaining a strong accuracy–efficiency trade-off (≈1% average drop with the KV cache shrunk to ~25% of FP16). Careful ablations isolate the contributions of Fisher-weighted local fine-tuning and local QAT, and the rotation-aware quantization design (Hadamard S1 + learnable S2) is practical and lightweight compared with full end-to-end training."}, "weaknesses": {"value": "WSVD’s evidence is somewhat narrow and system-dependent: accuracy and tuning are demonstrated mainly on ScienceQA-IMG and SEED-Bench-IMG with just ~256 calibration samples and a small set of VLMs, so generalization is unclear; its headline latency gains are reported for decoding on LLaVA-Next-7B under specific, favorable settings (e.g., batch 16, 8K tokens) on 4090/5090-class GPUs, and the paper itself notes access to high-end hardware as a limitation; practical speedups hinge on a custom Triton fused kernel integrated with Flash Decoding, raising portability questions to other runtimes/accelerators; per-head SVD increases approximation error, so Fisher-weighted fine-tuning and local QAT are required to recover accuracy—adding calibration data needs and engineering complexity; some baseline comparisons (e.g., Palu) adapt settings due to Palu’s batch-1 constraint, which complicates fairness; the method focuses on attention (Q/K/V) rather than also optimizing FFN blocks that can dominate runtime; and, until code/models are publicly released, independent replication and broader portability studies remain gated."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1ttEYT7xTX", "forum": "zrmQ4koOw9", "replyto": "zrmQ4koOw9", "signatures": ["ICLR.cc/2026/Conference/Submission23091/Reviewer_9c8b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23091/Reviewer_9c8b"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23091/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866987021, "cdate": 1761866987021, "tmdate": 1762942507469, "mdate": 1762942507469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WSVD, a weighted low-rank approximation framework for efficient low-precision vision-language models (VLMs). It addresses latency issues of existing SVD methods by applying fine-grained per-head SVD, assigning weight importance via Fisher information to preserve accuracy, and integrating quantization-aware training. A fused kernel merges low-rank reconstruction with Flash Decoding to cut I/O overhead. Experiments on 5 VLMs show WSVD achieves up to 1.8× decoding speedup, maintains accuracy (≤1% drop even at 50% parameter compression), and reduces KV cache to 25% of FP16 models, outperforming baselines like ASVD and SVD-LLM"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. In terms of originality, the paper innovatively combines fine-grained per-head SVD, Fisher information-based weight importance scoring, and quantization-aware training with a fused kernel design, effectively addressing the latency-accuracy trade-off in VLM optimization that prior SVD methods failed to resolve.\n2. Regarding quality, the work demonstrates rigorous experimental validation across 5 VLMs, 2 benchmarks, and multiple GPUs, with detailed ablation studies confirming the effectiveness of each component and fair comparisons against diverse baselines.\n3. For significance, the WSVD framework achieves up to 1.8× decoding speedup while preserving accuracy and reducing KV cache to 25% of FP16 models, providing a practical solution for efficient VLM deployment on resource-constrained devices."}, "weaknesses": {"value": "1. The paper’s evaluation of WSVD’s generalization across VLM architectures is limited, as it only tests models from the LLaVA family and SmolVLM—excluding other representative VLMs like BLIP-2 or Qwen-VL. This gap makes it unclear if WSVD’s per-head SVD and fused kernel design work equally well for VLMs with different vision-text fusion mechanisms (e.g., Qwen-VL’s high-resolution perception), and expanding tests to these models would strengthen its generalizability.\n2. The analysis of WSVD’s performance under varying sequence lengths is insufficient; experiments only use a fixed sequence length of 8192, while real-world VLM use cases (e.g., long image captions or multi-image QA) involve variable lengths. Without testing how WSVD’s latency and accuracy scale with shorter/longer sequences, it is hard to assess its adaptability to diverse input scenarios, which is critical for practical deployment.\n3. The paper lacks a detailed comparison of WSVD’s computational overhead during the fine-tuning phase. While it mentions local fine-tuning (100 steps) and QAT (50 steps) are lightweight, it does not quantify this overhead (e.g., training time, GPU memory usage) against end-to-end finetuning or baselines like QSVD’s optimization process. Providing such data would help users evaluate if WSVD’s efficiency gains justify its fine-tuning costs, especially for resource-limited teams."}, "questions": {"value": "1. Could you provide experimental results of WSVD on other representative VLMs (e.g., BLIP-2, Qwen-VL) beyond the LLaVA family and SmolVLM? This would clarify whether WSVD’s design adapts to VLMs with different vision-text fusion mechanisms .\n2. Can you supplement latency and accuracy data of WSVD under variable sequence lengths (not just 8192)? This helps assess its adaptability to real-world scenarios with short/long inputs .\n3. Could you quantify the computational overhead (e.g., training time, GPU memory) of WSVD’s local fine-tuning and QAT, and compare it with baselines like QSVD? This enables evaluating if its efficiency gains justify the tuning costs ."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TyvMbVegoT", "forum": "zrmQ4koOw9", "replyto": "zrmQ4koOw9", "signatures": ["ICLR.cc/2026/Conference/Submission23091/Reviewer_9Jd6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23091/Reviewer_9Jd6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23091/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879497581, "cdate": 1761879497581, "tmdate": 1762942507249, "mdate": 1762942507249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes WSVD. By leveraging SVD, local weighted fine-tuning, and quantization-aware training, the method addresses the issues of high latency and accuracy loss in low-precision quantization of VLMs. Its fine-grained SVD reduces memory access overhead, and weighted decomposition preserves the importance of critical weight. The authors claim that the method achieves a 1.8× speedup in VLM decoding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper adopts per-head SVD operations, reducing memory overhead and latency in KV cache reconstruction. The method has a clear motivation, targeting the high memory access latency of traditional SVD. The paper has clear writing logic and readability, with a rigorous structure."}, "weaknesses": {"value": "The paper lacks sufficient analysis on the theoretical basis for per-head SVD rank selection, and the sensitivity validation of weighted scoring is absent. Baseline comparisons fail to include some of the latest low-rank quantization methods, and the performance stability in extreme low-rank scenarios has not been fully explored."}, "questions": {"value": "I mainly have some questions about the validation of the method.\n1. I am quite curious about the basis for selecting rank r in per-head SVD. Is there any quantitative analysis on the specific impacts of different rank settings on model performance and latency?\n2. When using the Fisher information score as the weight importance indicator in weighted fine-tuning, has it been compared with other indicators (such as gradient magnitude)? What is the proportion of its contribution to performance improvement?\n3. In quantization-aware training, what impact do the settings of orthogonal matrices S₁ and S₂ have on the quantization effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y2StCWEsFp", "forum": "zrmQ4koOw9", "replyto": "zrmQ4koOw9", "signatures": ["ICLR.cc/2026/Conference/Submission23091/Reviewer_ewCT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23091/Reviewer_ewCT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23091/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762104037145, "cdate": 1762104037145, "tmdate": 1762942506972, "mdate": 1762942506972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}