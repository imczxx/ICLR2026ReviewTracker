{"id": "tMzp5mr5jN", "number": 24324, "cdate": 1758355451534, "mdate": 1763722588671, "content": {"title": "CLARA: Convex Low-resource Accent-Robust Language Detection in ASR", "abstract": "Globalization and multiculturalism continue to produce increasingly diverse speech varieties. Yet current spoken dialogue systems frequently fail on under-represented dialects and accents, often misidentifying the input language and causing cascading failures in downstream dialogue tasks. Addressing this dialectal variance under low-resource constraints remains an open challenge, as standard fine-tuning is computationally expensive and prone to overfitting on high-dimensional speech data. We propose Convex Language Detection (CLD), a novel framework that integrates a theoretically grounded convex optimization techniques into the spoken dialogue systems pipeline. Our method is efficiently implemented via multi-GPU Alternating Direction Method of Multipliers (ADMM) in JAX, thus providing global optimality guarantees and fast training in polynomial time. Theoretically, we prove that our convex objective induces certified margin stability and provide rigorous guarantees against feature perturbations. Empirically, we demonstrate sample efficiency and robustness to input dialectical variance, to significantly reduce language misidentification rates for low-resource dialects within high-resource languages. Our open-source package is available at \\url{https://anonymous.4open.science/r/CLD-845F/README.md}.", "tldr": "Using convex neural networks to improve ASR transcription accuracy in low-resource dialectical spoken input.", "keywords": ["Convex Neural Networks", "ASR", "Spoken Dialogue Systems", "Low-resource Languages"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/410b127d8329ae17325b05d08cce6cbf13f8d596.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper targets language misidentification in ASR for accented and low-resource settings. It proposes Convex Language Detection, a lightweight head trained via an ADMM-solved convex reformulation of a two-layer ReLU network on top of Whisper encoder features, then uses the predicted language token to initialize decoding. The authors claim benefits in sample efficiency, convergence guarantees, and human-level sub-500 ms latency, and present theoretical analysis on optimization and stability alongside experiments in binary and five-way language identification with dialectal variation, using datasets such as Singaporeâ€™s National Speech Corpus and Lahaja."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear and practical problem focus. Improving language identification for accented speech directly addresses a common failure mode that propagates to transcription.\n\n2. Lightweight plug-in design. The CLD head trains on encoder features and supplies an initial language token to the decoder with minimal architectural disruption.\n\n3. Evidence of sample efficiency. Results highlight stable accuracy across small sample sizes in the binary setting."}, "weaknesses": {"value": "1. Baseline coverage is limited. Strong linear probes and shallow classifiers on Whisper features, as well as standard LID models, are needed to establish the incremental value of the convex head.\n\n2. Causal attribution is under-specified. The pipeline injects a predicted language token before decoding; WER gains should be disentangled by comparing decoding with gold language tokens, predicted tokens, and no token.\n\n3. Ablations on the convex modeling choices are missing. The number of sampled activation patterns, regularization, and ADMM hyperparameters should be related to accuracy and latency to assess robustness."}, "questions": {"value": "1. How many activation patterns are sampled in practice and by what selection strategy; how does the number of patterns trade off with accuracy and training time?\n\n2. Can you provide a correlation analysis between any stability certificate produced by the convex program and empirical robustness under noise, speed, or pitch perturbations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WdCCZM1vLj", "forum": "tMzp5mr5jN", "replyto": "tMzp5mr5jN", "signatures": ["ICLR.cc/2026/Conference/Submission24324/Reviewer_DNpu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24324/Reviewer_DNpu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801653063, "cdate": 1761801653063, "tmdate": 1762943042737, "mdate": 1762943042737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of language misidentification in Automatic Speech Recognition (ASR) systems when processing low-resource dialects and accents, such as Singaporean English. It proposes a robust ASR framework named CLARA, centered around a novel Convex Language Detection (CLD) algorithm. The core of this framework replaces the standard language detection layer, which follows the ASR encoder (like Whisper), with a convex reformulation (cvxNN) of a vanilla neural network (NN). This convex problem is then solved efficiently using ADMM-based methods. The authors claim that CLD offers strong convergence guarantees, high sample efficiency, and lightweight training costs. Experimental results show that in both binary and multiclass accent-detection tasks, CLD outperforms baselines in accuracy and Word Error Rate (WER), especially in low-data regimes (e.g., 100-1000 samples)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.\tStrong empirical performance in low-resource environments. The CLD model demonstrates remarkable sample efficiency, maintaining high accuracy and low WER even with minimal data, as shown clearly in Fig. 1 & 2. In the multi-class experiment (Table 2) and accent-specific analysis (Table 3-6), CLD (97.15% acc) also far surpasses the NN and WSP-SFT baselines, which collapse in low-data regimes.\n\n2.\tComputational efficiency and practical application value. The convex formulation provides significant computational efficiency by allowing the model to find a global optimum, thus avoiding costly hyper-parameter tuning. As shown in Table 1, CLD trains in just 64.45 seconds (a 13-17x speedup) and requires orders of magnitude fewer TFLOPS than baselines.\n\n3.\tTheoretical analysis and reproducibility. The work provides theoretical grounding for the CLD's robustness via a margin stability analysis (Section 4.2), which links the convex program to a formal bound on margin degradation under feature perturbations. Furthermore, the authors' commitment to open-sourcing their JAX code and custom dialect datasets strongly supports reproducibility.\n\n4.\tValidation with Human Feedback. The paper strengthens its claims by moving beyond automated metrics to include validation from human testers in a practical scenario (Sec 5.6). This analysis uncovers crucial baseline failure modes (Table 9) and confirms that CLD received the highest human satisfaction with the fewest language detection and transcription errors (Table 11)."}, "weaknesses": {"value": "1.\tThere are significant problems with the presentation. The paper is riddled with spelling and grammatical errors (e.g., lines 78, 377), non-standard use of quotation marks (line 40), and inconsistent citation formats (mixing cite and citet). The caption for Table 3 appears above the table, and the table itself exceeds the page margins. Furthermore, the legends in Figures 1, 2, and 3 are too small to be legible. The authors need to dedicate time to improving the paper's presentation.\n\n2.\tApplying cvxNN to the ASR language detection task to optimize training and improve efficiency is an interesting idea. However, if the paper's main novelty lies in the application of cvxNN rather than a more fundamental innovation, the paper might be a better fit for a speech-focused conference (e.g., ICASSP). The authors need to further clarify their novelty, as this is essential for understanding the paper's contribution.\n\n3.\tThe writing needs improvement; for instance, the conclusion should not merely summarize the experimental section. \n\n4.\tThe clarity of mathematical notation and theory needs improvement. For example, in Algorithm 1, the ADMM dual variable update step written as \"uu+ (primal residual)\" is incomplete and unclear.\n\n5.\tThe experimental section is somewhat thin. Experiments with different sizes of Whisper (e.g., small, large, turbo) could be added to strengthen the paper's claims of scalability."}, "questions": {"value": "1.\tWhat is the difference between this paper's CLD algorithm and CRONOS [1]? Why was CRONOS not compared as a baseline in the experiments?\n\n2.\tIs the CLD algorithm in this paper specifically designed and optimized for the task of Language Detection, or is it an application of the existing cvxNN and CRONOS to this task? \n\n[1] Feng, Miria, Zachary Frangella, and Mert Pilanci. \"Cronos: Enhancing deep learning with scalable gpu accelerated convex neural networks.\" Advances in Neural Information Processing Systems 37 (2024): 102973-103004."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WVvMakrWuz", "forum": "tMzp5mr5jN", "replyto": "tMzp5mr5jN", "signatures": ["ICLR.cc/2026/Conference/Submission24324/Reviewer_cuBn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24324/Reviewer_cuBn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888515115, "cdate": 1761888515115, "tmdate": 1762943042510, "mdate": 1762943042510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a convex language detection framework for integration into ASR systems. The aim is to improve the performance of these systems for diverse dialects."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem is interesting and potentially useful in improving the performance of ASR systems."}, "weaknesses": {"value": "The description is not clear.  For example, how is the vanilla neural network trained? Is whisper used to extract embeddings which are then used to train the models?\n\nThe writing can be improved - some terms are not explained such as ADMM. Also, cvxNN is used before it is explained in section 3.2.\n\nIt is not clear how this approach will be integrated into the ASR pipeline. How will models be modified to integrate the convex language detection component.\n\nThe impact of the system on actual ASR performance is not presented.\n\nMinor issues\nFix the latex issue with opening quotations marks.\nThe resolution of Figure 1 is too low"}, "questions": {"value": "How will models be modified to integrate the convex language detection component?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LpVJbaxHlQ", "forum": "tMzp5mr5jN", "replyto": "tMzp5mr5jN", "signatures": ["ICLR.cc/2026/Conference/Submission24324/Reviewer_84ss"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24324/Reviewer_84ss"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946455786, "cdate": 1761946455786, "tmdate": 1762943042333, "mdate": 1762943042333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors demonstrate how using a small, simple two layer ReLU neural network can improve language detection due to lack of robustness to accent variations in the Whisper ASR model. They reformulate the ReLU network into a convex optimization problem that they show is theoretically more stable, more data efficient and computationally efficient to train. To do so, they extend Feng et al. (2024) work from binary classification to multi-class to be applied in an ASR model. They refer to this technique as Convex Language Detection (CLD). Results are demonstrated how CLD can be added to Whisper to improve language accuracy and WER while being an order of magnitude faster to train."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Paper demonstrates clear improvement in efficiency and efficacy in recognizing multi-accented speech with the Whisper model by incorporating their CLD algorithm. There is novel application of a lifting technique to greatly stabilize, robustly and rapidly estimate a small neural network model for detecting language with minimal samples of multi-accented data. They outline theoretical gains by reforming the optimization of the two-layer ReLU network and  demonstrate empirically these aspects on real world data sets. They also provide github repository of code to replicate experimental results (both training and test) for high chance of reproducibility. The results appear original and while there are a number of minor typos, the paper as a whole is well organized and structured."}, "weaknesses": {"value": "The paper seems to have a relatively narrow focus of the CLD algorithm on a small ReLU network for language detection used in conjuction with a Whisper ASR model. The contribution of \"demonstrates promising directions for principled statistical generalization in spoken dialogue systems for low-resource languages\" is a limited claim focusing on convex optimization for a specific task in a low-resource language condition. A more expansive finding would be to contrast this work with other convex optimization approaches for training for a full large neural network model in a limited data setting. Thus, this limits the overall impact of the paper by limiting the scope of interest."}, "questions": {"value": "How could this approach be scaled up to training the entire model?\nDoes CLD work with all neural network encoder models?\nCan it be applied to other tasks and scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Mri6e8l6kn", "forum": "tMzp5mr5jN", "replyto": "tMzp5mr5jN", "signatures": ["ICLR.cc/2026/Conference/Submission24324/Reviewer_fU6n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24324/Reviewer_fU6n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762319549900, "cdate": 1762319549900, "tmdate": 1762943042091, "mdate": 1762943042091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}