{"id": "AgZr1UEoQI", "number": 8186, "cdate": 1758072873152, "mdate": 1759897801160, "content": {"title": "DiagVuln: A Holistic Conversational Benchmark for Evaluating LLMs on Vulnerability Assessment", "abstract": "With over 20,000 Common Vulnerabilities and Exposures (CVEs) reported an-\nnually, software vulnerabilities represent a critical cybersecurity challenge re-\nquiring automated assessment tools. While large language models (LLMs) show\npromise as cybersecurity assistants, existing benchmarks exhibit fundamental\nlimitations: narrow data sources, neglect of contextual information, and focus\non single-turn tasks rather than multi-turn analyst workflows. To bridge this\ngap, we introduce DiagVuln, the first multi-turn conversational benchmark for\nLLM-based vulnerability assessment. DiagVuln comprises 2,000 CVEs across 23\nquestion-and-answer categories, encompassing detection, localization, classifi-\ncation, root cause analysis, exploit reasoning, impact assessment, and patch. We\nconstruct high-quality QA pairs in DiagVuln using retrieval-augmented gen-\neration (RAG) based on data collected from diverse sources, validated through\nLLM-as-a-Judge and conformal prediction based on human expert annotations.\nEvaluation of five state-of-the-art LLMs using DiagVuln reveals substantial lim-\nitations, with low top-1 accuracy (below 60%) in vulnerable code detection and\nCVE identification. Our evaluation also demonstrates that current models lack\ncritical reasoning capabilities for reliable vulnerability assessment. DiagVuln\nprovides a valuable resource for advancing research in evaluating and fine-tuning\nLLMs for vulnerability assessment.", "tldr": "A comprehensive benchmark dataset for vulnerability identification and assessment.", "keywords": ["Vulnerability Benchmark", "Large Language Model", "CVE", "RAG"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c068ee29b08713da5e5ac4d97a35a167a7e88e6c.pdf", "supplementary_material": "/attachment/266a53e41ccff1685372587b9cc4807ffce93589.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a new vulnerability dataset including CVEs in the Linux kernel. The raw information was collected from NVD as well as public websites and forums, and RAG was used to create question answer pairs about the vulnerabilities. The quality of the generated QA pairs is validated using human effort. Finally, the paper benchmarks existing SOTA commercial and open-source LLMs on the proposed dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The ability of the RAG method to generate QA pairs is sufficiently validated by the human assessment.\n- The collected dataset includes useful information that could boost the performance of LLMs, either by using RAG or fine-tuning."}, "weaknesses": {"value": "- Overall the paper does not well-motivate the need for the proposed dataset. The results section did not show any comparison with previous datasets (e.g., in base LLM performance, or in how training on the dataset improves performance).\n- It seems natural to evaluate to which extent augmenting an LLM with the knowledge in the proposed dataset (through RAG or fine-tuning) would improve the performance of LLMs in the task. The authors mention this as a potential future work, but in my opinion this should have been included in the paper.\n- The paper only benchmarks LLMs, without mentioning other vulnerability detection/identification tools. Both the detection and identification tasks could be carried out by either static analyzers or by DNN-based methods.\n- The related work section did not sufficiently mention existing vulnerability detection and assessment datasets, and how the proposed dataset is different/better. This is currently mentioned in the introduction, but only very briefly\n- The separate evaluation of vulnerability detection and identification might be misleading. I think all false positives from the detection step should also be considered in the input to the identification step, which should significantly degrade the identification results."}, "questions": {"value": "- How does your proposed dataset help push the state of the art in vulnerability detection and assessment?\n- Did you only focus on the vulnerabilities in the Linux kernel project, or do you include any vulnerabilities that affect Linux systems?\n- Related to the above point, why did you only focus on Linux kernel vulnerabilities? How does this affect the applicability of your dataset?\n- Why did not you benchmark other vulnerability detection/identification methods (e.g., static analyzers, GNNs, RNNs)?\n- Could you elaborate more on the developed RAG system used to create the dataset? What was the knowledge base items? what were the queries used?\n- What was the source for benign code in your dataset? How would having a more balanced (benign vs vulnerable ratio) dataset affect the results in Section 5?\n- How would unifying the detection and identification pipeline affect the results in Section 5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sYNDq7178r", "forum": "AgZr1UEoQI", "replyto": "AgZr1UEoQI", "signatures": ["ICLR.cc/2026/Conference/Submission8186/Reviewer_kj1d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8186/Reviewer_kj1d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760631267838, "cdate": 1760631267838, "tmdate": 1762920144837, "mdate": 1762920144837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DIAGVULN, a new multi-turn, conversational benchmark for evaluating the vulnerability assessment capabilities of LLMs.  The authors identify critical limitations in existing benchmarks, such as a narrow focus on single data sources, a lack of contextual information (e.g., root cause, exploit details), and a preference for single-turn tasks.  To address this, DIAGVULN is constructed by aggregating data from 13 diverse sources into a comprehensive \"Vulnerability Portfolio\".  A Retrieval-Augmented Generation (RAG) system is then used to generate 46,000 QA pairs across 23 categories for 2,000 CVEs.  A key part of the methodology is a novel validation pipeline that uses an \"LLM-as-a-Judge\" calibrated with Conformal Prediction based on human annotations for 100 CVEs, which provides a scalable way to ensure the quality of the RAG-generated answers.  The authors use DIAGVULN to benchmark five SOTA LLMs, finding that while they perform moderately on surface-level information extraction, they substantially fail at tasks requiring deeper reasoning, such as mitigation, exploit explanation, and patch analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a critical and timely need. As security teams are overwhelmed and LLMs are being integrating into security workflows, a benchmark to measure their true assessment capabilities is urgently required. The paper correctly identifies the flaws of existing benchmarks (single-source, single-turn).\n\n2. The paper's strongest methodological contribution is the validation pipeline. Manually labeling 46,000 QA pairs is infeasible. The proposed solution—using LLM-as-a-Judge and then statistically calibrating this judge using Conformal Prediction (CP) from a small is an elegant and sound approach to building a high-confidence synthetic dataset.\nIt provides a robust guarantee on the quality of the RAG generation process.\n\n3. The \"Vulnerability Portfolio\" concept, which aggregates 13 diverse structured and unstructured sources (from NVD to mailing lists), is a major strength. It provides the \"holistic\" context that is missing from other benchmarks and necessary for real-world analysis.\n\n4. The 3-step evaluation procedure (Detection $\\rightarrow$ Identification $\\rightarrow$ QA) effectively mimics a practical analyst workflow, making the benchmark far more realistic than simple, single-shot QA datasets."}, "weaknesses": {"value": "1. This overclaiming of \"ground truth\" taints the main experiment in Section 5. The benchmark evaluates SOTA LLMs (using web search) by comparing their answers against the RAG-generated answers (from the static portfolio). A low \"Correctness\" score (Table 6) implies the SOTA LLM is wrong. However, it could simply be a disagreement between the SOTA LLM's retrieval (web search) and the paper's RAG system. The live web search might even be more correct or up-to-date. The paper fails to address this ambiguity, fundamentally weakening the evaluation claims. The benchmark measures deviation from the DIAGVULN-RAG system, not necessarily deviation from objective truth.\n\n2. The paper claims “first conversational benchmark” yet Ruan et al. (VulZoo, ASE’24) already pair CVEs with summaries, exploits and patches; the only novelty is turning these into 23 QA templates.\n\n3. The evaluation pits LLMs against each other, not against (i) classical vulnerability detectors (CodeQL, Clang SA, CPAChecker) or (ii) smaller BERT-style models fine-tuned on existing corpora. \n\n4. Limited Scope of Validation: The human validation (used to calibrate the CP) checked 100 CVEs. While the CP framework provides statistical guarantees, the paper isn't fully transparent about the diversity of these 100 CVEs or how representative they are of the 2,000 CVEs in the final QA set."}, "questions": {"value": "1. My primary concern is the \"ground truth\" claim. Would you be willing to re-frame this? The RAG-generated answers are a high-quality synthetic baseline, not \"ground truth.\" How do you account for the possibility, in your Section 5 evaluation, that a SOTA LLM's answer (from a web search) is correct and your RAG-generated answer (from a static portfolio) is incorrect or outdated? A disagreement does not automatically mean the SOTA LLM failed.\n\n2. The paper aggregates scores across 23 attributes but never shows concrete failure cases. Could you provide a concrete failure analysis, for example, list 10 CVEs where all five LLMs score <3 on Mitigation, and explain the common pattern (missing context, ambiguous patch, etc)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ggRUAtfZKw", "forum": "AgZr1UEoQI", "replyto": "AgZr1UEoQI", "signatures": ["ICLR.cc/2026/Conference/Submission8186/Reviewer_fiHy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8186/Reviewer_fiHy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719256282, "cdate": 1761719256282, "tmdate": 1762920144437, "mdate": 1762920144437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DiagVuln, a novel, large-scale conversational benchmark dataset to evaluate the SOTA LLMs on vulnerability assessment. The author aimed to address the gaps in the existing vulnerability detection datasets by curating data from 13 structured and non-structured sources, and developed “Vulnerability Portfolio” with rich context information on the Linux Kernel CVEs. The dataset contains 2000 CVEs across 23 question-answer categories, including detection, localisation, classification, root cause analysis, exploit reasoning, impact assessment, and patch. The key contribution of the work is an automated framework that generates QA pairs from the vulnerability portfolio using RAG. The authors used an LLM-as-a-judge framework with conformal prediction to validate the quality of the automatically generated data. Finally, the paper benchmarks five SOTA LLMs on a subset of the dataset, and the results show that LLMs struggle with complex, reasoning-heavy vulnerability analysis tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Collection of holistic vulnerable data from diverse data sources (structured and unstructured) and curating a large-scale scale rich contextful “Vulnerability Portfolio”, which might also be used for future fine-tuning tasks.\n- The data collection pipeline is scalable with RAG for QA generation and LLM-as-a-judge for validation.\n- The results demonstrate the limitations of the current LLMs and provide a future research roadmap for software security."}, "weaknesses": {"value": "- The CVE Attribution method for the unstructured data solely depends on the regular expressions to extract CVEs. This approach assumes that the unstructured developer discussion and code comments are correctly tagged with a complete single CVE ID, which may not be true in the real-world developers' settings. There might not be a mention of CVE IDs, or the CVE references can be partial, or even the discussion may contain multiple CVE references. How do the authors address this issue?\n- No mention of the number of samples in the held-out calibration set for the conformal prediction. How did the authors ensure the calibration set size was sufficiently large to represent the whole dataset?\n- The authors provided a detailed process of human assessment with the required time on 100 QA pairs. However, no quantitative data on this assessment are provided. How to compare the performance of LLM-as-a-Judge against the human annotators?\n- In the evaluation section, the authors have mentioned that the 200 CVE subset was selected based on the “richest contextual information”. Do the authors consider the potential data leakage issue in this case?  How will the model perform against the less documented vulnerabilities? \n- LLMs have poor capabilities of reasoning about vulnerabilities. Using LLMs to curate vulnerability questions/answers may generate low quality data"}, "questions": {"value": "- What steps were taken in case of missing, ambiguous CVE IDs for the unstructured sources beyond the regular expression? How much does this affect the portfolio curation of less documented vulnerabilities?\n\n- The \"Correctness\" of the RAG-generated answers is validated against the source portfolio. How does the pipeline evaluate for scenarios where the source data itself is incorrect or incomplete?\n\n- Is the evaluation result also generalizable to the less documented vulnerabilities?\n\n- Among 6,342 CVEs, how many are used to build RAG database, how the RAG database is built?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZqxvbDFlV1", "forum": "AgZr1UEoQI", "replyto": "AgZr1UEoQI", "signatures": ["ICLR.cc/2026/Conference/Submission8186/Reviewer_qNHz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8186/Reviewer_qNHz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868892003, "cdate": 1761868892003, "tmdate": 1762920143857, "mdate": 1762920143857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a conversational benchmark about CVEs from multiple from multiple data sources to evaluate the capabilities of LLMs as cybersecurity assistants in three key areas (vulnerability detection, CVE identification and Q&A about the identified vulnerability). It also introduces a dataset curation and validation pipeline to extend the benchmark. The evaluation on the benchmark with a combination of LLM-as-a-judge and conformal prediction shows significant gaps in current LLMs"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "•\tIt is a valuable benchmark for evaluating the possible extent of use of LLMs in critical cybersecurity situations and the fine-grained questions provide more insight than previous benchmarks.\n\n\n•\tIn addition to LLM-as-a-judge, the authors use conformal prediction techniques to establish high-confidence bounds on the acceptable difference between human and LLM labels."}, "weaknesses": {"value": "•\tIn Section 5 Prompting Strategy, the authors mention a model-agnostic system prompt, but recent research (https://aclanthology.org/2025.naacl-long.73/, https://arxiv.org/abs/2408.11865) has shown that LLMs are very sensitive to prompt construction. As a result, the assumption that a single prompt will work equally well across all the tested models might not hold true. It will be interesting to see how optimized prompts can work for each LLM, specially the open-weights ones. \n\n•\tThe authors do not use structured generation for the model responses even though all the models are capable of structured generation, either through vLLM or their own API. This can reduce the error rates significantly.\n\n•\tWhile the authors mention that the dataset can be used to fine-tune domain-specific LLMs, it introduces benchmark contamination and any results on the benchmark by a fine-tuned LLM will not be admissible. To do that successfully needs a privately held-out test set."}, "questions": {"value": "1.\tWhy was structured generation skipped? It makes for a more structured evaluation as well as improving the capabilities of less-sophisticated models\n\n2.\tDo you have any plans for a test-set that can be used to evaluate fine-tuned models in the future? Specifically, it would be helpful to have the test set from sources other than the train set. \n\n3.\tWere any other models evaluated for RAG besides GPT-4o? Since this model is used for both the RAG pipeline and the LLM-as-a-judge, the scores cannot be compared similarly for other LLMs"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F7ve2RSAo4", "forum": "AgZr1UEoQI", "replyto": "AgZr1UEoQI", "signatures": ["ICLR.cc/2026/Conference/Submission8186/Reviewer_h3gR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8186/Reviewer_h3gR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939449174, "cdate": 1761939449174, "tmdate": 1762920143427, "mdate": 1762920143427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}