{"id": "FMM6U22voO", "number": 21900, "cdate": 1758323360389, "mdate": 1759896897695, "content": {"title": "MentalBlackboard : Evaluating Spatial Visualization via Mathematical Transformations", "abstract": "Spatial visualization is the mental ability to imagine, transform, and manipulate the spatial characteristics of objects and actions. This intelligence is a part of human cognition where actions and perception are connected on a mental level. Do state-of-the-art Vision-Language Models (VLMs) also exhibit this ability? To explore this, we develop MentalBlackboard, an open-ended spatial visualization benchmark for Paper Folding and Hole Punching tests within two core tasks: prediction and planning. Our prediction experiments reveal that models mostly overpredict the final hole numbers and struggle with applying symmetrical transformations, even when they predict the sequence of unfolding steps correctly. The backward folding process (folding the paper away from the camera/observer), which leads to limited vision, reduces the accuracy of spatial arrangement construction. Rotations, which alter the orientation of the unfolding actions, introduce a significant challenge for models to understand the physical orientation of the paper. The planning task, in which models are required to identify the sequence of folds that match the final hole pattern, shows models' limitations in analyzing symmetrical relations and creating the multi-stage symmetry process. In the task of generalization, which does not require spatial visualization, models reason through the visual analogies involving two visual examples of the same paper-folding process, along with a distinct spatial property and text-based hole information. Although the best-performing model, o3, achieves a peak performance of 71.6\\% in transferring spatial data, it only obtains 25\\% accuracy on text-based prediction tasks. Claude Opus 4.1 achieves the highest planning score with 10\\%. The field-wise performance shows that models struggle more with locating and orienting the holes.", "tldr": "", "keywords": ["spatial visualization", "spatial cognition", "spatial reasoning", "VLMs"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d946a4e8d4220f35c63551dbd6c72b51a259409.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a new benchmark called MentalBlackBoard, evaluation framework based on the Paper Folding Test (PFT) to assess the reasoning capabilities of video-language models and vision-language models (VLMs). The authors introduce an automatic pipeline for generating 3D, 2D, and textual representations of the PFT.\nThe novelty of this work lies in its feasible and large-scale evaluation setup, which encompasses tasks involving memory, relational reasoning, and symmetry understanding. The paper defines two primary tasks. \n(1) Prediction, where the model is asked to infer the sequence or attributes of holes after observing the folding steps. \n(2) Planning, where the model must generate the correct sequence of folding actions to reach a given final paper configuration.\n\nExperimental results reveal that even SOTA reasoning models (e.g., o3) achieve only about 25% exact-match accuracy, especially when interacting with rich multimodal representations (video or image) for this task. The models fail to reliably understand the symmetry of holes in the folded paper.\nFurthermore, performance on the planning task remains very low, with only about 10% success in generating correct folding sequences. The paper also investigates task transferability, showing that while models can recognize hole size and type, they struggle to generalize symmetry and spatial structure in hole patterns."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a benchmark designed to evaluate the ability of vision-language models (VLMs) to understand spatial information using the Paper Folding Test (PFT).\n\n- The experiments are conducted for both forward and backward folding, as well as other action settings, indicating the comprehensive of the evaluation.\n\n- An automatic benchmark creation pipeline is introduced, enabling large-scale dataset generation and ensuring reproducibility.\n\n- The benchmark includes tasks that test both inference and planning abilities, two essential reasoning skills for VLMs. Additional annotations are provided to support detailed performance analysis across different difficulty levels for the inference task.\n\n- The results reveal the limitations of large language and vision-language models in performing this task effectively. Even advanced reasoning models (e.g., o3) achieve only around 10% success on the planning task, highlighting a significant gap in current spatial reasoning capabilities.\n\n- The paper is well-written and easy to follow"}, "weaknesses": {"value": "- Figures and their first references are often placed far apart throughout the paper, making it difficult for readers to go back and forth to understand the context.\n\n- The tables are quite large, making it hard to locate specific numbers mentioned in the discussion. Currently, only the overall best results are highlighted across all three settings; it would be clearer to highlight the best results within each setting to improve readability.\n\n- While the authors provide detailed numerical results for analyzing VLM failure cases, there is limited qualitative discussion or example-based analysis to illustrate these failures. \n\n- The paper could be strengthened by incorporating problem-specific prompt engineering or additional strategies to address the current shortcomings observed in both the planning and prediction tasks. Initial approach is fine.\n\n- Although the dataset groups sequences by step for prediction, it would be valuable to include a range of planning problems of varying complexity to evaluate the model’s reasoning progression—from simple to more challenging planning scenarios."}, "questions": {"value": "- It is unclear whether the experimental setup follows a zero-shot or few-shot approach. Additionally, does the VLM have access to information about how to reverse each folding action? Understanding this is important, as it may not be trivial for a model to infer correct unfolding steps directly from a list of candidate actions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EalSiLDg2c", "forum": "FMM6U22voO", "replyto": "FMM6U22voO", "signatures": ["ICLR.cc/2026/Conference/Submission21900/Reviewer_uNrQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21900/Reviewer_uNrQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941149754, "cdate": 1761941149754, "tmdate": 1762941973783, "mdate": 1762941973783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MentalBlackboard, a new, large-scale, open-ended benchmark designed to evaluate the spatial visualization capabilities of VLMs. The benchmark is based on the Paper Folding Test (PFT) and consists of two primary tasks: prediction and planning .\n\nUnlike previous spatial reasoning benchmarks that often rely on multiple-choice formats , the paper requires open-ended generation, allowing for a deeper analysis of failure modes. The benchmark is procedurally generated using a dynamic 3D VPython pipeline and includes complex transformations like diagonal folds and rotations, which are absent in simpler PFT datasets.\n\nThe authors evaluate several proprietary and open-source VLMs. The results are stark: even the best-performing models fail significantly. The top model achieves only 25.07% exact match accuracy on the text-based prediction task and around 10% on the planning task.\n\nGeneralization task shows that models can perform good on simple spatial data transfer when the complex, multi-step mental visualization is removed. Backward prediction task shows that performance decreases when folds are partially occluded, highlighting a failure to reason about unseen physical transformations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Contribution to Evaluation: This work addresses a clear and important gap. Most spatial benchmarks test static relational understanding or simplified transformations. This paper tackles the much harder, multi-step cognitive process of visualization. The move from multiple-choice to open-ended generation is a major methodological strength, enabling fine-grained error analysis.\n2. Insightful ablation: The Generalization task is a particularly clever control; by showing models excel at this visual analogy task (which doesn't require mental unfolding), the authors effectively prove that the core failure is in the multi-step visualization process itself, not just in understanding basic spatial properties."}, "weaknesses": {"value": "1. Speculative Failure Analysis: The analysis of failure modes, while insightful, remains largely speculative. For instance, in Section 5.1, the paper lists several \"possible reasons\" for why models generate extra holes, but these hypotheses are not empirically validated. The paper would be much stronger if it included analysis to confirm these claims, perhaps by correlating specific error types with specific task configurations.\n\n2. Missing Highly Relevant Related Work: The related work section, while good, overlooks some very recent and highly relevant work. For example,*\"Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations.\"* and *\"SpatialViz-Bench: An MLLM Benchmark for Spatial Visualization\"*\n\n\n3. Limited Argument for Novelty: While the paper's open-ended format and inclusion of rotations are clear improvements, the core task is still the PFT , which is well-established in both cognitive science and recent AI benchmarks (e.g., Mind the Gap, OmniSpatial, SpatialViz, STARE). The paper could do a better job of arguing why its specific additions (like 3D generation and rotation) constitute a qualitatively new cognitive challenge for VLMs, rather than just a more difficult, incremental version of the same underlying problem.\n\n\n4. Unclear Presentation of Methods and Data: There are a few points of unclear presentation.\n - Action Space: The 'action space' for unfolding (e.g., 'H1-F', 'D2-F') is defined in the Appendix prompts (e.g., ) but should be made explicit in the main paper's methodology section for clarity.\n - Dataset Size: There is a confusing discrepancy in the reported dataset size. Table 1 claims a size of more than 1M , but Section 3 states the pipeline generates \"over 12K unique configurations\". The authors should clarify if the \"1M\" refers to total generated frames versus the number of problems."}, "questions": {"value": "1. Model Selection:  Given that GPT-4o is a premier, publicly available vision-language model, why was it only evaluated on 2D image and text inputs, and not on the video-based prediction task? Also performance of gpt-5 should also be included. \n2. Need for Qualitative Analysis: The quantitative analysis is thorough, but the paper would significantly benefit from qualitative case studies. Could the authors include examples showing a model's actual incorrect output (e.g., the JSON it produced) versus the ground truth? Seeing a concrete example of a model misapplying a symmetry rule would make the quantitative findings in Table 2 much more concrete. Also error types should be included in case studies.\n3. Visual vs. Text Performance Gap: The performance gap between text and vision is striking. The paper notes the text task is simpler (it omits the 'direction' feature). How much of this performance gap do you attribute to that simplification versus a more fundamental failure in visual perception (i.e., models struggling to parse the video/images into a stable symbolic representation)?\n4. Human Baseline: To properly calibrate the difficulty of this benchmark, what is human performance on these tasks? Establishing a human-level baseline (even on a small subset of the test data) would be invaluable for contextualizing the models' very low scores."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gTdypPApjf", "forum": "FMM6U22voO", "replyto": "FMM6U22voO", "signatures": ["ICLR.cc/2026/Conference/Submission21900/Reviewer_a6NR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21900/Reviewer_a6NR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975945777, "cdate": 1761975945777, "tmdate": 1762941973462, "mdate": 1762941973462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MentalBlackboard, a large-scale benchmark designed to evaluate spatial visualization ability in vision-language models through paper folding and hole-punching tasks. It offers open-ended prediction and planning tasks in text, image, and video modalities. Results show that even top proprietary models (GPT-o3, Claude Opus 4.1) perform poorly on spatial transformations involving symmetry, rotation, and sequential reasoning."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The benchmark’s formulation of Prediction (forward transformation) and Planning (inverse transformation) tasks—each available in video, 2D image, and symbolic-text modalities—provides a balanced framework for evaluating both causal and inverse spatial reasoning.\n\n2. Technically rigorous dataset generation pipeline with physical validation. Used VPython to build a physically feasible 3D folding and rotation environment, automatically generating over 12,000 unique configurations (including folding, rotation, and hole-punching) to form an open-ended evaluation benchmark.\n\n3. The evaluation dimensions are detailed: in addition to the overall accuracy, the analysis also covers four attribute dimensions: shape, size, location, and direction."}, "weaknesses": {"value": "Overinterpretation of “Generalization” Results：\nThe discussion interprets the analogy-style generalization task as evidence of spatial information transfer. However, this task no longer requires mental folding or 3D transformation—only attribute mapping between similar examples. The observed success (e.g., on shape/size but not direction/location) more likely reflects linguistic or pattern-matching biases rather than genuine spatial reasoning. The authors should clarify this conceptual boundary and avoid conflating visual analogy with mental visualization.\n\nInsufficient Analysis of Backward Folding：\nThe sharp performance drop on backward folds is attributed to view-dependent difficulty, but the paper provides no causal validation. It remains unclear whether the degradation stems from missing visual cues, lack of depth/layer representation, or intrinsic spatial limitations. Additional controlled ablations (e.g., providing visibility masks or depth cues) are necessary to substantiate the explanation.\n\nLack of Human Baseline or Cognitive Reference：\nDespite frequent references to cognitive psychology and mental imagery, no human performance baseline is reported. Without such a comparison, it is difficult to gauge how far current models deviate from human-level spatial visualization. Even a small-scale human study on 10 items would provide valuable calibration.\n\nLimited Error Analysis and Visualization：\nThe discussion mentions typical failure types (extra holes, missing holes, direction errors) but does not present quantitative distributions or visual examples. A systematic confusion analysis or complexity-wise breakdown would make the findings more interpretable.\n\nVague Future Directions.\nThe discussion ends with general remarks about “training models to perform mental transformations” but lacks concrete technical proposals. Potential routes such as geometry-aware modules, symmetry-consistency losses, or layer/depth supervision could be explicitly discussed to guide future research.\n\nOverall, the Discussion section identifies important phenomena but stops short of explaining why they occur. It remains descriptive rather than diagnostic, missing an opportunity to provide causal insight and concrete improvement pathways."}, "questions": {"value": "Missing and Unbalanced Related Work:\n\nThe section on Spatial Reasoning Benchmarks overlooks several recent and influential datasets that are directly relevant, such as VSI-Bench. In contrast, the included works (e.g., GOAT-Bench, Navi2Gaze) are tangentially related and focus on embodied or navigation-based reasoning.\n\nThe paper would benefit from a systematic taxonomy contrasting real image spatial reasoning benchmark.\n\nNext plan:\nFuture versions could be extended to other cognitive dimensions, such as mental rotation, surface development, and cross-section, to form a true “multidimensional spatial intelligence” benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "v8E7PpyHcL", "forum": "FMM6U22voO", "replyto": "FMM6U22voO", "signatures": ["ICLR.cc/2026/Conference/Submission21900/Reviewer_wAZc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21900/Reviewer_wAZc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994465867, "cdate": 1761994465867, "tmdate": 1762941973007, "mdate": 1762941973007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}