{"id": "KiWcaQaf3n", "number": 22472, "cdate": 1758331558883, "mdate": 1759896864337, "content": {"title": "SpeedCP: Fast Kernel-based Conditional Conformal Prediction", "abstract": "Conformal prediction provides distribution-free prediction sets with finite-sample conditional guarantees. We build upon the RKHS-based framework of Gibbs et al. (2023), which leverages families of covariate shifts to provide approximate conditional conformal prediction intervals, an approach with strong theoretical promise, but with prohibitive computational cost. To bridge this gap, we develop a stable and efficient algorithm that computes the full solution path of the regularized RKHS conformal optimization problem, at essentially the same cost as a single kernel quantile fit. Our path-tracing framework simultaneously tunes hyperparameters, providing smoothness control and data-adaptive calibration. To extend the method to high-dimensional settings, we further integrate our approach with low-rank latent embeddings that capture conditional validity in a data-driven latent space. Empirically, our method provides reliable conditional coverage across a variety of modern black-box predictors, improving the interval length of Gibbs et al. (2023) by 30%, while achieving a 40-fold speedup.", "tldr": "", "keywords": ["conformal prediction", "conditional coverage", "reproducing kernel Hilbert spaces", "quantile regression"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f06a48691bce25495a91acba41d9b1068ed9778e.pdf", "supplementary_material": "/attachment/1b81f4e26465c32a8d4f13603b93e1cd1b6eca7c.zip"}, "replies": [{"content": {"summary": {"value": "This paper improves the practical implementation of the RKHS-based conformal prediction framework of Gibbs, Cherian, and Candès (2023). That earlier framework guarantees approximate conditional coverage over a class of covariate shifts \\mathcal{F}, which was shown in Gibbs et al. to be equivalent to a relaxed form of conditional coverage. In their method, Gibbs et al. construct a quantile regression model to estimate the (1-alpha) quantile of nonconformity score S(x*, y*) as a function of new test data x* within an RKHS defined by \\mathcal{F}. While searching for the model in the space, they solve a regularized optimization problem with a regularization parameter \\lambda. To ensure exchangeability of the calibration set and the new data point to achieve theoretical coverage, this optimization needs to be repeated for different possible candidate values of S(x*, y*). Further, it's ideal to tune \\lambda, so this optimization would be repeated for different values of \\lambda. These repetitions make the computation expensive.\nThe key contribution of the proposed SpeedCP is to accelerate the aforementioned computation by finding a solution path over \\lambda and S, specifically, by reducing computation to the tracking and computation concerning an (often much smaller than size n) elbow set. In addition, SppedCP allows low-rank latent embeddings of X to handle high-dimensional covariates that further reduces computational cost when p is large. Theoretical results on conditional validity in terms of the low-rank embedding is provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Well written (literature review, motivation, logical flow, clarity, rigorosity)"}, "weaknesses": {"value": "The calibration set was further split into K folds, to help find best (\\lambda, \\gamma). This procedure was only very lightly mentioned in the end of sec.2.1.1. Was this expensive? Was the cost reported/included in the empricial examples? (E.g. when SpeedCP was claimed to be 50 times faster in figure 1, and in table 4)\n\nI'd appreciate a clearer presentation (say a table) that shows the samples size (training and calibration), the dimension of x (say, p), the dimension of the embedding, (and perhaps the computing cost of SpeedCP) for all examples. This will provide a good practical guidance for future users. How large a p is too large, and how much should we reduce the dimension. The pro and con of reducing p.  Was the embedding of dim 256 or further reduced for SpeedCP in the Brain MRI example? \n\nI don't get the arxiv example, e.g., it can be made clear in the beginning what is the purpose of the application including what is the output, number of citations? What is this supposed to mean:\"For the predictor, we choose linear regression of citation counts on raw word frequencies, which fails to extract any meaningful associations between words and citation counts.\"\n\nminor: covariate shift was not defined explicitly. Consider introducing it in a way that readers do not have to seek its definition in Gibbs et. al. 2023."}, "questions": {"value": "See the weakness section for some Questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The statements in page 10 are well written and look truthful to me."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pxQ3qkOBpM", "forum": "KiWcaQaf3n", "replyto": "KiWcaQaf3n", "signatures": ["ICLR.cc/2026/Conference/Submission22472/Reviewer_qs9h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22472/Reviewer_qs9h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675665027, "cdate": 1761675665027, "tmdate": 1762942232427, "mdate": 1762942232427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SpeedCP, a novel framework for conditional conformal prediction that addresses the computational limitations of prior work while providing stronger coverage guarantees than marginal methods. The authors build upon the RKHS-based framework of Gibbs et al. (2023), which provides approximate conditional coverage guarantees but suffers from high computational costs. SpeedCP's key idea is a solution-path algorithm that efficiently computes the full regularization path of the RKHS conformal optimization problem at essentially the same cost as a single kernel quantile fit. The method simultaneously tunes hyperparameters for smoothness control and data-adaptive calibration. To extend applicability to high-dimensional settings, the authors integrate their approach with low-rank latent embeddings that capture conditional validity in a data-driven latent space."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The solution-path approach for RKHS-based conformal prediction appears to be interesting. The $\\lambda$-path and S-path formulations are well-designed and theoretically sound. The paper provides solid theoretical foundations with finite-sample guarantees for approximate conditional coverage in both standard and high-dimensional settings. Theorems 1 and 2 characterize the coverage properties under different assumptions.  In addition. the integration with low-rank latent embeddings addresses the difficulty of applying conditional conformal prediction to high-dimensional settings."}, "weaknesses": {"value": "The coverage guarantees in Theorem 1 involve a coverage gap that depends on the embedding quality, which is usually unknown is practice.\nThis raises the concern that whether the proposed method can guarantee coverage in pratice.\n\nThere is only limited comparison to existing ethods:  the paper compares against CondCP, SplitCP, PCP, and RLCP, it would benefit from comparisons to more recent methods like those in Xu et al. (2024) or Shahrokhi et al. (2025) mentioned in the references and perhaps recent methods based on normalizing flows.\n\nThe method involves selecting a two-dimensional $\\gamma, \\lambda)$ grid. It is not clear how sensitive/robust the method is with respect to \nthe selection."}, "questions": {"value": "The paper describes cross-validating over a grid of $(\\gamma, \\lambda)$ values, but doesn't discuss how sensitive the results are to this choice. How the conditional coverage and prediction interval size vary with different $(\\gamma,\\lambda)$ values, particularly in the high-dimensional settings?\n\nIs Theorem 1 for a fixed pair of $(\\gamma, \\lambda)$? Does it  depend on the randomness in the selected $(\\gamma, \\lambda)$ values?\n\nSame question as above for Theorem 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uXKeH4m250", "forum": "KiWcaQaf3n", "replyto": "KiWcaQaf3n", "signatures": ["ICLR.cc/2026/Conference/Submission22472/Reviewer_HaCa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22472/Reviewer_HaCa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974271581, "cdate": 1761974271581, "tmdate": 1762942232159, "mdate": 1762942232159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Submission SpeedCP tackles conditional (not just marginal) coverage in conformal prediction by enforcing the guarantee over a rich RKHS class $\\mathcal{F}^\\*=\\\\{f\\_{\\psi^\\*}+\\Phi^\\* \\eta\\\\}$. To make similarity meaningful in high dimensions, the kernel $\\psi^\\*(x\\_1, x\\_2)$ is defined on a low-rank embedding $\\hat{\\pi}(X) \\in \\mathbb{R}^K$ (e.g., LDA/PCA), with distance computed in centered log-ratio space. Prediction sets are obtained by solving a regularized RKHS quantile regression on calibration points plus the test point with an imputed score $S$ (4), yielding the representer form $\\hat{g}\\_S(x)=\\Phi^*(x)^{\\top} \\hat{\\eta}\\_S+\\frac{1}{\\lambda} \\sum\\_i \\hat{\\nu}\\_{S, i} \\psi^\\*(x, X\\_i)$ and the elbow/left/right partition.\n\n\nComputationally, the authors proposed a pair of solution-path algorithms, i.e. a ${\\lambda}$-path for smoothness selection on calibration data only, and an S-path that traces the test score until it reaches the conformal threshold.\nBetween \"events\", coefficients evolve affinely in $\\lambda$ (resp. in S). Algorithm 1 couples $\\lambda$-path cross-validation over ($\\gamma, \\lambda$) with per-test S-paths, offering substantial speedups.\n\nOn theory, a randomized tilted-expectation identity (Lemma 4) extends Gibbs to this RKHS/representer scaling, expressing coverage deviation as $-\\hat{\\lambda} \\mathbb{E}\\langle\\hat{g}\\_{\\psi^\\*}, f\\_{\\psi^\\*}\\rangle$. A localized guarantee (thm 1) conditions on a random neighborhood $W^{\\prime}$ drawn from a density kernel over the latent space, giving an explicit coverage ratio (10); a group-conditional guarantee (thm 2) holds for features that encode latent components under a topic-alignment condition."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation of conditional conformal prediction in RKHS appears reasonable\n\n- On my verification, the technical development is overall correct and sound. The piecewise linearity of the derived path is a nice property for both implementers and theorists\n\n- Empirically, SpeedCP attains more uniform coverage with smaller sets and reports observed speedups over CondCP on a series of provided benchmarks"}, "weaknesses": {"value": "Major weaknesses:\n\n- The paper oversells the algorithmic novelty relative to what’s already in the ML toolbox. The $\\lambda$-path derivation, KKT structure, and  active set partitions are garden-variety kernel quantile-regression path-following; this is the same playbook [1] (and additionally [2]) used, and the paper just carry along an extra linear term $\\Phi^*$ and then code the usual event-driven updates. The $\\lambda$-path piece is incremental, basically a straightforward adaptation. The S -path is a natural, low-effort extension (change the homotopy parameter from $\\lambda$ to $S$ and keep the same active-set bookkeeping). To me the only novelty is the use of those paths inside the RKHS-CP pipeline. It also should be noted that even just the kernel solution path itself is by no means a novel concept in ML/optimization, see e.g. [3,4]\n\n\n- Hyperparameter selection leaks calibration and targets the wrong objective. The paper tunes $(\\gamma, \\lambda)$ by ${k}$-fold cross-validation on the calibration set to minimize pinball loss (Algorithm 1, sec. 2.1.1). That 1) uses the same calibration points later used for conformal calibration, and 2) optimizes a regression loss rather than the quantity that matters-coverage (or the coverage gap in (10)). There is no formal theorem showing that this two-stage selection preserves the stated finite-sample coverage properties. This philosophy is exactly the kind of selection-induced calibration drift CP tries to avoid.\n\n\n\n\nMinor concerns:\n\n- The central theoretical claim (Theorem 1) is an identity that expresses the coverage error as a ratio involving unknown terms, specifically an RKHS‑weighted residual term over a kernel density in the latent space. There's no non‑asymptotic bound that forces this gap to be small, nor any verifiable a‑priori control (i.e. users can’t evaluate without extra modeling assumptions)\n\n- The entire work assumes the columns of $\\Phi^*$ are linearly independent. The assumption itself is standard and reasonable, however, the usage of it is highly problematic. First, several experiments specify $\\Phi^\\*$ with intercept + one-hot indicators (synthetic, MRI), which is rank-deficient by construction. Importantly, you actually need a (stronger) elbow-level rank condition ( $|E| \\geq d$ and $\\operatorname{rank}(\\Phi\\_E^{\\*})=d$) along the entire path; this is neither stated nor enforced. I mean, the work initialize the $\\lambda$-path at the largest value for which \"at least two points are in the elbow\". Two points is not enough unless you have $d \\leq 2$. For general $d$, one need $|E| \\geq d$ and $\\operatorname{rank}(\\Phi\\_E^{{ }^\\*})=d$ so that $(\\Phi\\_E^{^*T } \\Phi\\_E^{^\\*})^{-1}$ exists at every segment. \n\n- The simulations fix exactly one base model per dataset. The authors never test whether SpeedCP's gains persist if you swap $\\hat{\\mu}$ for a stronger or different architecture on the same task (e.g., transformer text models instead of naive bag-of-words linear regressor; alternative GNNs on molecules). That omission weakens the generality claim because cp methods are supposed to be model-agnostic\n\nBib:\n\n[1] Li, Youjuan, Yufeng Liu, and Ji Zhu. \"Quantile regression in reproducing kernel Hilbert spaces.\" Journal of the American Statistical Association 102.477 (2007): 255-268.\n\n[2] Takeuchi, Ichiro, Kaname Nomura, and Takafumi Kanamori. \"Nonparametric conditional density estimation using piecewise-linear solution path of kernel quantile regression.\" Neural Computation 21.2 (2009): 533-559.\n\n[3] Gu, Bin, Ziran Xiong, Xiang Li, Zhou Zhai, and Guansheng Zheng. \"Kernel path for ν-support vector classification.\" IEEE Transactions on Neural Networks and Learning Systems 34, no. 1 (2021): 490-501.\n\n[4] Zhai, Zhou, Heng Huang, and Bin Gu. \"Kernel path for semisupervised support vector machine.\" IEEE Transactions on Neural Networks and Learning Systems 35, no. 2 (2022): 1512-1522.\n\n(COI: i'm not one of the authors of these citations)"}, "questions": {"value": "- Theorem 2 demands perfect topic alignment $\\hat{T}(X)=T(X)$ *almost surely*, which is equivalent to requiring a bayes-optimal argmax classifier on the latent mixture with exact zero error. Handwaving that this holds \"under a margin condition\" doesn't make it realistic. Without that, the group-conditional guarantee collapses. May the authors have more comment over this?\n\n\n- the work rely on randomized cutoffs S_rand via $U$~$Unif(-\\alpha, 1-\\alpha$) (see eq. (9)) to avoid *inflated coverage*. That makes predictions non-deterministic across runs; could the authors explore the variance induced by this step instead of reporting means over 50 runs?\n\n- in experiment, the work excludes CondCP *entirely* on the arXiv and MRI experiments for computational difficulty, then claim superiority; that’s not a real fair comparison\n\n\n\n\nWhile reading, I noticed a few minor issues that don't affect the evaluation, e.g.\n\n- l79, a new path algorithms: remove s\n\n- l83, the authors used \"event\" point which is not defined until page 4\n\n- l134, preliminary notations: usually an uncountable noun\n\n- eq4: an additional $^2$; consider to rearrange the footnote\n\n- l218, We proceed to constructing: construct\n\n- Theorem 1: you assume a density kernel $\\psi\\_W^\\*$ with $\\psi\\_W^\\*(\\hat{\\pi}(x\\_1), \\hat{\\pi}(x\\_2))=\\psi^\\*(x_1, x_2)$. Since you define $\\psi^\\*(x\\_1, x\\_2)= \\exp \\\\{-\\gamma d_\\pi(\\hat{\\pi}(x\\_1), \\hat{\\pi}(x\\_2))\\\\}$, to interpret $\\psi\\_W^\\*$ as a probability density in its second argument; please state explicitly that $\\psi^\\*$ is either normalized already or that the equality holds up to a constant that cancels in the ratio on the RHS of eq.10\n\n- l823, wider prediction intervel: interval\n\n- From the Lagrangian (21) and stationarity condition (22), you correctly get $v_{S, i}=\\sigma_i-\\tau_i$, $\\sigma_i=1-\\alpha-\\kappa_i$, $\\tau_i=\\alpha-\\rho_i$. But in (24), for positive residual $S_i-g_S\\left(X_i\\right)>0$ you wrote $\\sigma_i=\\alpha$, $\\tau_i=0$. It should be $\\sigma_i=1-\\alpha, \\tau_i=0$ (values are swapped with negative residual)\n\n- l1299, Then We have: we\n\n- p.28, proof of Lemma 4: when you borrowed results from Gibbs et al. (2023), the $-\\mathbb{E}[f(X\\_{n+1}) \\hat{v}\\_{n+1}]= -2 \\mathbb{E}[\\lambda\\langle\\hat{g}, f\\rangle]$ is only correct for their scaling. (no factor 2 as your own objective uses $\\frac{\\lambda}{2}\\\\|g\\|\\^2$)\n\nFor completeness, the authors should consider incorporating portions of appendix A into the main body, especially definitions and key related work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "18zhSISOIJ", "forum": "KiWcaQaf3n", "replyto": "KiWcaQaf3n", "signatures": ["ICLR.cc/2026/Conference/Submission22472/Reviewer_vevS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22472/Reviewer_vevS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991863085, "cdate": 1761991863085, "tmdate": 1762942231837, "mdate": 1762942231837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a general nonparametric method for constructing conformal prediction sets with specific conditional guarantees. The authors complement the algorithm with a theoretical analysis. The experimental study shows some benefits of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Extending conditional properties of conformal prediction approaches is of high interest\n- The proposed algorithm looks sound as well as Theorem 1 (I have some questions with respect to Theorem 2; see below)\n- Experimental study is generally well executed"}, "weaknesses": {"value": "- The worst-case complexity of the algorithm is high. I would recommend doing an experimental study, varying the size of the dataset to see what the pattern is.\n\n- Theorem 2 assumes that $\\hat{T} = T$. It is looks like extremely strong assumption to recover this mapping precisely.\n\n- In many experiments, FastCP is closely matched by simple SplitCP (especially, Figures 2 and 3). It is unclear what the benefit of FastCP is, then."}, "questions": {"value": "- What is the map $\\Phi^*$ introduced on page 3? It seems that you do not explain what it is.\n\n- What is the benefit of FastCP over SplitCP? Why the differences are small between them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RSGW5RnTfL", "forum": "KiWcaQaf3n", "replyto": "KiWcaQaf3n", "signatures": ["ICLR.cc/2026/Conference/Submission22472/Reviewer_yCxV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22472/Reviewer_yCxV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762236320077, "cdate": 1762236320077, "tmdate": 1762942231587, "mdate": 1762942231587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}