{"id": "i2fkuh0uzA", "number": 24738, "cdate": 1758359846486, "mdate": 1759896751756, "content": {"title": "Aligning Rotational and Hierarchical Geometry in Molecular Representation Learning with Product-Manifold Latent Spaces", "abstract": "Learning effective molecular representations requires capturing two fundamental but largely disjoint aspects of the structure of molecules: rotational symmetries in 3D conformations and the hierarchical organization of chemical scaffolds. We introduce a new paradigm of product-manifold representation learning with product-manifold message passing on $\\mathrm{SO}(3) \\times \\mathbb{H}^d$, which couples equivariant geometric features with hyperbolic embeddings of chemical hierarchy. Our construction preserves $\\mathrm{SO}(3)$-equivariance in the geometric channel and uses an $\\mathrm{E}(3)$‑invariant readout for scalar properties while enabling curvature-aware aggregation in the hyperbolic channel, with cross-coupling restricted to scalar invariants to maintain symmetry. Unlike prior approaches that fuse equivariant and hierarchical encoders via concatenation or stacking, our method defines message passing directly on the product manifold, yielding a unified representation. We outline how such models could be evaluated on molecular property prediction, scaffold-split generalization, and generative design, and discuss how embeddings in $\\mathrm{SO}(3) \\times \\mathbb{H}^d$ provide a natural surrogate space for manifold Bayesian optimization, enabling more sample-efficient discovery of high-value molecules compared to Euclidean BO. Together, these results suggest a principled path toward unifying physical symmetries and chemical hierarchies within a single geometric learning framework.", "tldr": "", "keywords": ["molecular representation learning", "molecular machine learning", "equivariant graph neural networks", "rotational symmetry", "SO(3)-equivariance", "hyperbolic geometry", "product manifold", "hierarchical chemical scaffolds", "geometric deep learning", "message passing", "manifold Bayesian optimization", "scaffold split generalization", "molecular property prediction", "generative molecular design", "curvature-aware aggregation", "symmetry-preserving learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ac4ea7fb4789555c73bae97e572ceccd498b44d.pdf", "supplementary_material": "/attachment/245e5c9be9f0e41a6fe3658e538855b8466f72f3.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel graph neural network architecture designed to unify two critical but typically separate concepts in molecular representation learning: the 3D rotational symmetries of molecular conformations and the hierarchical organization of chemical scaffolds.\n\nThe authors propose a message-passing framework that operates on a product manifold, $\\mathcal{M} = SO(3) \\times \\mathbb{H}^{d}$. The key idea is to maintain two coupled channels:\n* An SO(3)-equivariant channel that uses irreducible representations (irreps) to process 3D geometric information, preserving physical symmetries.\n* A hyperbolic channel that embeds the molecule's chemical hierarchy (derived from a junction tree) using curvature-aware operations like Fréchet means.\n\nThe paper's main contribution is the \"symmetry-safe\" coupling mechanism: the two channels interact only through scalar ($l=0$) invariants (e.g., Euclidean distances, hyperbolic distances, feature norms). This allows hierarchical information to gate and influence the geometric representations without breaking the fundamental SO(3)-equivariance. The authors evaluate this product-manifold model on QM9 (property prediction), OGB-MolHIV (scaffold-split generalization), and Guacamol (Bayesian optimization), arguing that their unified latent space improves performance, particularly in generalization and sample-efficient search.\n\nWhile I quite like the idea of the paper, there are some major concerns that need to be addressed."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The intuition of the idea is good. Instead of the popular \"late-fusion\", the authors propose to use a \"product-manifold message passing\" mechanism for exchanging information between the geometric and hyperbolic channels. This is surely more advanced than the \"late-fusion\" approach in concept and should bring *some* performance gains.\n* The proposed product-manifold latent space and network architecture are tested in various experimental settings -- QM9 (property prediction), OGB-MolHIV (scaffold-split generalization), and Guacamol (Bayesian optimization). This provides a comprehensive evaluation of the proposed method."}, "weaknesses": {"value": "* The presentation of this paper is problematic, especially in the Methods section where the details of the model definition are described. The formulas are simply hard to follow due to undefined or exotic notations. For example, what does the $\\times$ mean in $[M_i^{(R)}]_{\\times}$ in Equation 6? For another example, the $dist_H$ symbol appears in Equation 4, but is only defined in the text around Equation 7. I therefore suggest a major revision for the Methods section to increase its readability.\n* A preliminary section should be included in the main text or in the Appendix. The proposed method uses two kinds of advanced concepts (the SO(3) irreps and the hyperbolic geometry), and an average reader is only familiar with at most one of them.\n* The performance of the proposed architecture falls behind SoTA. The authors has mainly shown the efficacy of the proposed product-manifold message passing through ablation study, but the performance is still not as good as the SoTA models."}, "questions": {"value": "* Could you improve the presentation of this paper, especially that of the Methods section?\n* What is $\\phi_{ij}$?\n* In addition to the current experiments, could you also compare your proposed approach with \"late-fusion\"?\n* Given an existing model architecture that has SO(3) irreps representations for nodes (such as MACE, Nequip, etc.), how easy can one modify the architecture to adopt the proposed product-manifold latent space and message passing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BDlLNreyY8", "forum": "i2fkuh0uzA", "replyto": "i2fkuh0uzA", "signatures": ["ICLR.cc/2026/Conference/Submission24738/Reviewer_qFv9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24738/Reviewer_qFv9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742949787, "cdate": 1761742949787, "tmdate": 1762943181118, "mdate": 1762943181118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on enhancing the expressiveness of graph neural network. The authors introduce a product-manifold message passing to learn equivariant geometric features with hyperbolic embeddings of chemical hierarchy. Additionally, authors show the model could be evaluated on molecular property prediction, scaffold-split generalization, and discuss how embeddings provide a natural surrogate space for manifold Bayesian optimization."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes an interesting structure that has the potential to introduce some n-body priors into molecular systems, which may be beneficial for molecular systems."}, "weaknesses": {"value": "1. $S(u)$ is an important variable introduced in this paper, used to provide information about the junction tree. However, I could not find its definition in the paper. How is this tree defined? If it is as shown in Figure 1, is this kind of tree only related to the cutoff of the number of edges? How should this tree be defined in 3D space without explicit bonds?\n\n2. The core idea of this paper is somewhat similar to introducing richer n-body information in the message passing process. If so, I think the authors should refer to MACE [1], SLEM [2], which author mentioned in Introduction. In these methods, the model no longer processes the features of each node and its single neighbor, but instead processes the fused features of each node and multiple neighbors, or the fused features with the mean of multiple neighbor nodes.\n\n3. Does the hyperbolic channel interact with the SO(3) channel at every layer of the model?\n\n4. The experimental results still have a gap compared to the state-of-the-art (SOTA), not to mention the lack of the latest baselines such as Equiformer v2 and GotenNet. There is also a lack of more extensive experiments, such as on OC20 and Molecule3D. This makes it hard for me to consider the proposed tree structure in the paper as effective, although it is interesting. I suggest that the authors directly incorporate the product manifold into Equiformer and train for enough epochs to evaluate whether the method can improve performance with sufficient fitting. The results in Table 1 currently appear to be underfitted.\n\n5. The completeness of this paper still needs to be improved. The model architecture, experimental settings, and baseline configurations all need to be detailed in the paper. The equations in Section 3 are somewhat confusing. The position of the table captions is incorrect. In the code section, I only saw a model class, and not even a training script.\n\n[1] MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields\n\n[2] Learning local equivariant representations for quantum operators"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E09tN8HtXr", "forum": "i2fkuh0uzA", "replyto": "i2fkuh0uzA", "signatures": ["ICLR.cc/2026/Conference/Submission24738/Reviewer_j638"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24738/Reviewer_j638"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889609827, "cdate": 1761889609827, "tmdate": 1762943180763, "mdate": 1762943180763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method that combines the 3D rotational equivariance of small molecule conformations with hyperbolic embeddings of their chemical hierarchy. Instead of combining these independent modes of describing molecules following their distinct symmetry-respecting embeddings, the authors show how to develop a joint embedding that exchanges only scalar invariant information within a layer. The proposed method shows some promise in early benchmark evaluations, which indicates that the idea might merit further investigation as a way to perform optimization in this product-manifold latent space."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper contributes an original idea that integrates two independent concepts at an architectural level in a coherent way, providing a symmetry-safe unification of geometric and hierarchical molecular representations.  The background and method descriptions are clear, and the early tests show that the fusion works better than the independent components, supporting further research."}, "weaknesses": {"value": "Although the architecture preserves the required symmetries, the current implementation of this symmetry preservation appears to come at a cost in performance on standard benchmarks, possibly due to the constraint of having per-layer equivariance and the use of scalar-only couplings.  The demonstrations are limited, lacking directly reproducible code and comprehensive comparisons of the graph-only and coordinate-only components.  The validation performance in OGB-MOLHIV is noteworthy, however, even Gabriele Corso's PNA work from 2020 had validation above 0.85 while performing poorly on the test.  The guacamol results could have been the most interesting, however, that particular benchmark is no longer maintained and there is no clear leaderboard to compare against. The fact that GraphGA, an simple genetic algorithm, performed unexpectedly well in the rankings up to 3 years ago underscores the benchmark's uncertain relevance today.  More generally, it is unclear why the authors evaluated only a handful of GuacaMol task, especially as there appears to be some merit to their argument for creating a better joint embedding space."}, "questions": {"value": "Does the current layer-wise equivariant message-passing architecture suffer from similar problems as those identified in general GNNs in the zero-one laws of graph neural networks (https://arxiv.org/abs/2301.13060)?  In particular, do the authors anticipate that the architecture might not support all-atom representations of large biomolecules, including proteins, due to the same type of collapse?\n\nCan the authors provide detailed protocols/scripts for the training and evaluation and also describe (perhaps in a supplement) the computational performance and stability of the architecture with respect to hyperparameters?\n\nIs it feasible to pretrain this model on large mixed datasets and then finetune it on datasets with only partial information (e.g. no coordinates or no graph)?  An benefit of certain late-fusion techniques, or of ways to align the embeddings of separate techniques, is that one could potentially use partial information in followup work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XhrTGYZ3jq", "forum": "i2fkuh0uzA", "replyto": "i2fkuh0uzA", "signatures": ["ICLR.cc/2026/Conference/Submission24738/Reviewer_zXkb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24738/Reviewer_zXkb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963788709, "cdate": 1761963788709, "tmdate": 1762943180360, "mdate": 1762943180360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}