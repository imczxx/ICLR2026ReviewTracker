{"id": "XSHP62BCXN", "number": 21588, "cdate": 1758319329306, "mdate": 1759896913637, "content": {"title": "Lost in the Middle: An Emergent Property from Information Retrieval Demands in LLMs", "abstract": "The performance of Large Language Models (LLMs) often degrades when crucial information appears in the middle of a long context, a “lost-in-the-middle” phenomenon that mirrors the primacy and recency effects in human memory. We propose that this behavior is not simply a flaw indicative of information loss but an adaptation to different information retrieval demands during pre-training: some tasks require uniform recall across the entire input (a long-term memory demand), while others prioritize the most recent information (a short-term memory demand). Consistent with this view, we show that this U-shaped performance curve emerges when LLMs (GPT-2 and Llama variants) are trained from scratch on two simple human memory paradigms simulating long-term and short-term memory demands. Our analysis reveals that while the recency effect directly aligns with short-term memory demand in the training data, the primacy effect is induced by the uniform long-term memory demand and is further influenced by the model's autoregressive properties and the formation of attention sinks. Our main findings from simple human memory paradigms also generalize to a sequence completion task, which more closely resembles the next-token prediction process used in LLM pre-training. Together, our findings reveal how information retrieval demands, model architecture, and structural attention dynamics during model training can jointly produce positional bias observed in LLMs.", "tldr": "The lost-in-the-middle phenomenon in LLMs can arise through fundamental human memory tasks with different retrieval demands.", "keywords": ["language modeling", "lost-in-the-middle phenomenon", "attention dynamics", "human memory parallels"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c25408a58e3ac4cfd9f4d8f42820e1f1a710768f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper examines the now well know \"lost in the effect\" of LLMs and show that the phenomenon originates from training (akin to objectives in psychology) that maximizes recency objectives as well as maximizing some long term objectives using an autoregressive model. The paper shows the effect by first constructing the learning from scratch experiment using synthetic data and later on turn to language modelling real use cases.\n\nThe paper is able to notably contribute to the understanding of \"primacy\" effect and shows that 1) the primacy effect is due to autoregressive training with long term tasks 2) the primacy effect is strongly related to the attention sink phenomenon"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Although the hypothesis has been around in the community for a while for the root case of \"lost in the middle effect\", this is the first paper that I have seen to illustrate the cause clearly convincingly without involving other causal factors where the paper 1) trains from scratch 2) using synthetic almost toy tasks to illustrate this. \n\nThe experimental designs are set up nicely in this paper. The paper investigates into if primacy is related to the autoregressive training and examines a positive case (RNN) and a negative case (T5) for this which are well chosen since the architecture is significantly different to be convincing and informative. The attention sink link is built with its own contribution as well."}, "weaknesses": {"value": "I think the organization of the paper can be made better. \n\nIn terms of what I want to see more: in the discussion section, the paper mentions quite some related works mitigate the \"lost in the middle effect\". The paper posits that intervention on positional attention should have more effect when applied to long term tasks; however, the paper does not state which techniques exactly and don't performs experiments and cite related works for if what the paper posits are true. It would be something informative for the readers.\n\nIn terms of what I think can be simplified: the paper examines 3 metrics, however, they seem to connect to each other and lag is not very much used in the texts. Furthermore, although it is valuable to examine real cases (3.4); the conclusion is not expected to differ since the settings are very similar and the phenomenon is relatively well known."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RbdzSkm08B", "forum": "XSHP62BCXN", "replyto": "XSHP62BCXN", "signatures": ["ICLR.cc/2026/Conference/Submission21588/Reviewer_Bp4r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21588/Reviewer_Bp4r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760826146153, "cdate": 1760826146153, "tmdate": 1762941846369, "mdate": 1762941846369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the \"lost-in-the-middle\" phenomenon, where Large Language Models (LLMs) often struggle to recall information from the middle of their context window. The authors posit that this U-shaped recall curve is not an inherent architectural flaw but rather an emergent property from training on mixed retrieval objectives. To test this, they train several models (GPT-2 Small/Large, Llama 3.2-1B, RNNs, and T5) from scratch on synthetic memory tasks. They demonstrate that tasks requiring \"free recall\" tend to produce a primacy effect (good recall at the beginning), while \"running span\" tasks produce a recency effect (good recall at the end). When trained on a mixture of these objectives, the models exhibit the full U-shaped curve."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Controlled Experimental Setup:** Training models from scratch on well-defined, synthetic tasks is a significant strength. This methodology provides a clean environment for causal attribution, effectively isolating the training objectives as the primary variable and avoiding the confounding factors of large, diverse pre-training datasets.\n\n**Systematic Ablation Studies:** The paper provides valuable mechanistic insights, particularly through the attention sink dropout experiments. These studies effectively demonstrate how different components (like attention sinks) contribute selectively to different task-driven biases (e.g., primacy).\n\n**Inclusion of Multiple Architectures:** Testing across decoder-only (GPT, Llama), encoder-decoder (T5), and RNN seq2seq models helps to disentangle architectural contributions from training-objective effects, strengthening the paper's claims about the generality of the phenomenon."}, "weaknesses": {"value": "While the paper presents a novel and interesting hypothesis, its conclusions would be strengthened by addressing the following points regarding the generalizability and implications of the findings.\n\n1. Generalizability and Model Scale A key point of tension arises from the Llama 3.2-1B results (Fig 3G, H). These larger, more modern models do not seem to exhibit the same pronounced U-shaped phenomenon observed in the smaller GPT-2 models. This divergence raises crucial questions about the paper's central claim:\n\nDoes this suggest the 'lost-in-the-middle' effect, as framed here, is primarily an artifact of limited model capacity rather than a fundamental property of the training objective?\n\nThe paper would be significantly strengthened if the authors could elaborate on how their explanation generalizes to modern, more capable architectures that seem to mitigate this issue. If the phenomenon disappears with scale, it suggests the retrieval-objective trade-off may not be the primary bottleneck for today's SOTA models.\n\n2. Situating the Work within Existing Literature The paper would benefit from a more thorough engagement with related literature on positional biases. For instance:\n\nRecent work (e.g., \"Transformers need glasses!\" [2406.04267], \"Why do LLMs attend to the first token?\" [2504.02732]) has also explored primacy and recency effects, attributing them to different mechanisms like \"information over-squashing.\"\n\nConnecting to this adjacent work would help clarify the unique contributions of this paper's retrieval-objective hypothesis versus other explanations.\n\n3. Implications for Modern Architectures The reliance on older architectures like GPT-2 (2019) and RNNs, while valuable for the controlled comparison, makes it difficult to ascertain the implications for current SOTA models.\n\nHow do the authors reconcile their findings with models like Gemini 1.5 Pro, which demonstrate near-perfect recall across million-token contexts?\n\nA discussion on this point is essential. Are the mechanisms identified here still at play, but simply overcome by sheer scale? Or do modern architectures (e.g., different positional encodings, data mixtures, and attention mechanisms) fundamentally resolve the retrieval-objective trade-off described in this work?"}, "questions": {"value": "To help clarify the paper's contributions, I would encourage the authors to elaborate on the following in a revised version:\n\n1. What are the unique contributions of the mixed-retrieval-objective hypothesis when viewed against prior work on positional biases and information \"squashing\"?\n\n2. What are the practical implications of these findings? Do the results suggest concrete, recommended changes to LLM architectures or pre-training data mixtures?\n\n3. How does the paper's thesis account for the capability of modern SOTA LLMs (and the paper's own Llama 3.2 results) that seem to largely solve the 'lost-in-the-middle' problem at scale? Is the U-shape a temporary phase of training/scale, or a fundamental trade-off that is simply \"solved\" by other, more dominant mechanisms in larger models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jDu1MKsICr", "forum": "XSHP62BCXN", "replyto": "XSHP62BCXN", "signatures": ["ICLR.cc/2026/Conference/Submission21588/Reviewer_L5FX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21588/Reviewer_L5FX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908575055, "cdate": 1761908575055, "tmdate": 1762941846044, "mdate": 1762941846044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper draws parallels with resource-rational perspectives in cognitive psychology, explaining the recency and primacy effect usually observed in autoregressive LLMs. It interprets the positional bias as an emergent behavior arising due to different memory demands and architectural design.\n\nIt trains model from scratch with minimal training tasks to show emergence of U-shaped curve. Their analysis reveals that training for free recall task shows primacy effect and short-term running span task shows recency effect. And combining both tasks yield U-shaped curve, also referred to as lost-in-the-middle effect. \n\nThe paper performs controlled study to show these effects over 3 small to medium scale autoregressive LLMs. Where the U-shaped phenomena is weakened for the largest model (LLama 3B). This is attributed to the large scale of the model. The paper shows that causal architecture is one of the main reason for the emergence of lost-in-the-middle effect and it disappears when using bidirectional encoder-decoder architecture. The causal architecture leads to the formation of attention sinks which is responsible for the primacy effect. The paper performs ablation on attention sinks by destroying stronger sinks to confirm their effect."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work proposed an isolated setup with minimal tasks to demonstrate the formation attention sinks using a causal architecture. This is a novel contribution of this work. The memory demand task design is simple and easy to interpret. \n\n2. The study makes clear observations by training the model from scratch. It makes a convincing case for the role of short- and long-term memory demands and for the role of causal model for the observed primacy and recency bias. \n\n3. The study conducts thorough empirical analysis with different memory demands. The study disrupting attention sinks with long-term memory demands is particularly insightful. Additional experiment with masking strategy shows that the observations are generalizable.\n\n4. The paper is well-structured and easy to read."}, "weaknesses": {"value": "1. Large-scale experiment is only limited to one Llama model which does show weak U-shaped curve. It is unclear whether the weakened U-shape performance is due to the scale of the model, model architecture or training dynamics. More baseline at multiple scales are needed to confirm the effect of scaling.\n\n2. While the proposed memory tasks allow clean analysis, but it is a much simplified task compared to real LLM tasks. Although masked sequence completion does show similar U-shaped behavior, it is unclear whether such attention-sink issue arise in general-purposed LLMs in practice. For example, do recent large-scale (>10B) general-purpose LLMs trained with standard next-token prediction format also show this U-shaped effect?\n\n3. The contribution of this work are unclear. Which experiment setup and evaluation metrics are novel in this work? Please clarify which findings are confirmations of previous work and which are new insights (include comparison to Liu et al 2023 and Wu et al. 2025) . Also, is the usage SPC, PFR, and CRP to study positional bias novel to this work or has it been proposed before? \n\nSome more questions are included in the Questions section below."}, "questions": {"value": "1. It is an interesting observation that larger models show flattened U-shaped recall curves. It is unclear how scaling changes the attention pattern and distributes it more evenly. A quantitative analysis of attention distribution and sink strength would clarify this observation.\n\n2. Wu et al. 2025 already showed in their work that causal nature of the autoregressive model is responsible for primacy effect due to attention sink formation. It is unclear what new insights does this study offer in this regard specifically?\n\n3. It is insightful to know that attention sinks formed at early positions in causal models are responsible for the primacy effect. However, it is unclear how do the formation of these attention sinks occur and why only in the early tokens."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nq4bRpUHlQ", "forum": "XSHP62BCXN", "replyto": "XSHP62BCXN", "signatures": ["ICLR.cc/2026/Conference/Submission21588/Reviewer_Xfd2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21588/Reviewer_Xfd2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007168813, "cdate": 1762007168813, "tmdate": 1762941845757, "mdate": 1762941845757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the “lost-in-the-middle” effect in LLMs and argues it is an emergent adaptation to information-retrieval (IR) demands during training rather than merely a flaw. The authors train GPT-2 (small/large) and Llama-3.2-1B from scratch on toy but principled human-memory paradigms: Free Recall (uniform long-term demand) and Running Span (end-weighted short-term demand). Training on both yields a U-shaped serial-position curve (primacy + recency), i.e., lost-in-the-middle (Figure 2C; Figure 3G–I). They further: (i) link primacy to autoregressive processing by showing strong primacy in an RNN seq2seq but not in a bidirectional T5 (Figure 4), and (ii) implicate attention sinks by ablating heads/layers flagged by a sink metric, which selectively degrades tasks with long-term retrieval demand and removes primacy (Figure 5D–G). Finally, they replicate the phenomena in a masked sequence completion task that more closely resembles next-token prediction (Figure 6). Formal task definitions and additional ablations are in the Appendix."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: Elegant, minimal task-design lens that unifies primacy/recency and lost-in-the-middle; neat architectural contrast (RNN/T5) and sink-based analysis. (Figures 2–6).\nQuality: Multiple models (GPT-2 S/L, Llama-3.2-1B) trained from scratch; consistent behavioral metrics; ablations that selectively hit long-term retrieval tasks (Figure 5G; Figure 6G).\nClarity: Clear exposition; formal task definitions; helpful visuals of sink heatmaps and SPC/PFR/CRP curves (pp. 6–8, 12–13).\nSignificance: Offers a theoretical framing likely to influence evaluation/mitigation strategies and spur work that tailors training curricula or architectures to desired IR profiles."}, "weaknesses": {"value": "External validity: No natural-data or real long-context benchmarks (QA/book-sum/code) to demonstrate that the mechanism quantitatively explains or predicts behavior in practice.\nAblation method: “Attention-sink dropout” removes entire attention layers flagged as sinks; this may simultaneously remove useful computation. More surgical interventions (head-level masking, key-positional rescaling) or counterfactual re-routing would strengthen causal claims.\nSensitivity & controls: Limited analysis of ε thresholding (0.8), dependence on context length, tokenization, or positional encodings (e.g., RoPE vs absolute). The combined-task loss weighting (FR vs RS) and its effect on the U-shape are not detailed.\nScale: Only up to ~1B parameters; the paper notes reduced U-shape at larger scale—directly charting trend vs scale/length would be valuable."}, "questions": {"value": "Generalization: Can you test the IR-demand hypothesis on natural long-context suites (e.g., re-ordering/context-insertion diagnostics) to predict when mitigations help?\nLoss mixing: How are FR and RS combined—same batch, same loss weight? What happens when you sweep the mixing ratio; does the U-shape vary smoothly?\nSequence length: Does primacy’s dependence on attention sinks persist at much longer contexts (e.g., 4k–32k)? Any qualitative shifts?\nSink metric robustness: Results vs different ε thresholds and identification methods? What about ablating non-sink heads/layers matched for magnitude to isolate the sink-specific effect?\nPositional encodings/architectures: How do RoPE variants, ALiBi, or state-space and hybrid encoder-decoder models affect primacy/recency under the same tasks?\nSurgical ablations: Could you replace sink layers with frozen copies or attention reweighting to rule out capacity loss as the driver?\nData realism: If you train on skewed temporal distributions (e.g., recency-heavy Zipfian windows approximating web/news), does the learned curve match your predicted balance of primacy/recency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bcot0AGrnR", "forum": "XSHP62BCXN", "replyto": "XSHP62BCXN", "signatures": ["ICLR.cc/2026/Conference/Submission21588/Reviewer_Ni46"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21588/Reviewer_Ni46"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762126377894, "cdate": 1762126377894, "tmdate": 1762941845445, "mdate": 1762941845445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}