{"id": "6en51gFQT1", "number": 6304, "cdate": 1757966120838, "mdate": 1759897923415, "content": {"title": "Layer Query Network For Test-Time-Training in Vision-Language-Models", "abstract": "Vision–Language Models (VLMs) struggle to generalize against out-of-distribution (OOD) samples, where conventional fine-tuning is infeasible. \nTest-Time Training (TTT) adapts models to each incoming test sample, yet current methods rely on heavy data augmentation and repeated forward/backward passes through the full VLM, incurring high computational cost.\nWe introduce Layer Query Network (LQN), a lightweight five-layer MLP that adapts a frozen VLM in one forward pass.\nLQN employs Binding to distill randomly sampled intermediate-layer tokens from VLM via 3D positional embeddings, and \nRecirculation to self-supervise spatial invariance for predicting robust spatially consistent features. This design removes the need to fine-tune the entire VLM, achieving faster convergence and strong dense-prediction performance, outperforming the teacher VLM. \nEvaluated across 16 benchmarks spanning natural distribution shifts and cross-dataset generalization, LQN achieves 15\\% faster test-time training on ImageNet-Val compared to the state-of-the-art TPS. In segmentation tasks, LQN surpasses Mask2Former on COCO, Cityscapes, and ADE20K while reducing GFLOPs by up to 11\\%. Our code will be released upon acceptance.", "tldr": "LQN is a lightweight MLP for efficient test-time training and on-the-fly adaptation of OOD dense tasks like segmentation, converges quickly reducing GFLOPs", "keywords": ["Test-Time-Training", "Robustness", "Layer-Locking"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/33fe508a929d57919146ca2229cb23499bb1b6c1.pdf", "supplementary_material": "/attachment/4c8bfd7f41d071589b50242a422716b96d2a8645.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Layer Query Network (LQN), a single-sample test-time training method that avoids the heavy augmentation and compute of TPT/TPS. LQN treats feature positions as queryable addresses and trains a small student MLP per test sample to predict the teacher’s feature at a destination position via Binding and Recirculation. Experiments on 14 classification and 5 segmentation benchmarks show consistent ImageNet-OOD gains over TPT/TPS/Diff-TPT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work explicitly avoids the heavy reliance of TPT/TPS on multi-view augmentations and high compute, and proposes a framework tailored to the single-sample TTT setting.\n\n2. The authors evaluate LQN on 14 classification and 5 segmentation benchmarks, and show consistent gains on ImageNet OOD over TPT/TPS/Diff-TPT.\n\n3. Convincing ablations: The method does not rely on augmentations, adding augmentations hurts LQN/APM, while increasing the number of teacher feature positions distilled is more effective."}, "weaknesses": {"value": "1. The paper does not adhere to the conference’s formatting requirements. For example, table captions should appear above the tables.\n\n2. The pseudocode on page 4 occupies too much space and hurts readability. Consider a more compact presentation (e.g., moving details to the appendix, using algorithm boxes, or summarizing with a flow diagram) to improve clarity."}, "questions": {"value": "1. Have you considered evaluating an LQN variant with history caching / cross-sample accumulation to explore a better time–accuracy trade-off?\n\n2. Would you include comparisons against a broader family of VLMs (e.g., SigLIP, EVA-CLIP, CoCa) to strengthen generality claims?\n\n3. Have you explored alternative position-sampling strategies for (src, src, dest) selection, and how do they affect performance and cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MK3iNZNnVg", "forum": "6en51gFQT1", "replyto": "6en51gFQT1", "signatures": ["ICLR.cc/2026/Conference/Submission6304/Reviewer_83JT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6304/Reviewer_83JT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618835559, "cdate": 1761618835559, "tmdate": 1762918606110, "mdate": 1762918606110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Layer Query Network (LQN), a TTA framework for VLMs designed to handle OOD samples without fine-tuning the entire model. LQN employs a five-layer MLP that extracts randomly sampled intermediate-layer tokens from a frozen VLM using 3D positional embeddings, and learns spatially invariant representations through a self-supervised objective. This design enables single forward-pass adaptation, leading to faster convergence and improved dense prediction performance. Experiments on 16 benchmarks show that LQN outperforms state-of-the-art methods such as TPS and Mask2Former while reducing computation cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to understand, with clear explanations of the method and experimental results."}, "weaknesses": {"value": "- The proposed LQN introduces substantial computational overhead due to multiple iterative updates, yet the performance improvement over more efficient methods (e.g., TDA) is marginal (16 min vs. 47 min, 61.9 vs. 61.35). In addition, the reported efficiency of TDA in Table 4 appears inconsistent with the results presented in its original paper.\n\n- The paper claims that existing single-image TTA methods incur higher computational costs due to data augmentation. However, several lightweight methods such as the training-free MTA [1] and the output-level optimization GS-Bias [2] achieve very low computational overhead, and the manuscript lacks discussion or comparison with these approaches.\n\n- Since positional encoding merely represents geometric locations, it may not maintain a strong monotonic relationship with semantic content. In scenarios with repetitive textures or multiple identical instances, fixed positional coordinates could correspond to semantically similar regions, potentially confusing the localization or adaptation process.\n\n- LQN performs worse than the teacher model on fine-grained datasets such as Pets and Flowers, suggesting that the proposed adaptation may not generalize well to tasks requiring fine-grained visual discrimination.\n\n[1]On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning? CVPR 2024\n\n[2]GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models. ICML 2025"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wj9WYpRNo9", "forum": "6en51gFQT1", "replyto": "6en51gFQT1", "signatures": ["ICLR.cc/2026/Conference/Submission6304/Reviewer_cDhr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6304/Reviewer_cDhr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812494138, "cdate": 1761812494138, "tmdate": 1762918605535, "mdate": 1762918605535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Layer Query Network (LQN): a five-layer MLP that performs layer-wise spatial 3D coordinate querying and distillation for a frozen VLM under single-sample Test-Time Training (TTT). By employing Binding and Recirculation (path consistency) losses, it achieves efficient adaptation without relying on heavy data augmentation or backpropagation through large backbones. The authors report superior performance compared to various TTT/TTPrompt methods on tasks such as natural distribution shifts, cross-dataset classification, and semantic/instance/panoptic segmentation, while also showing better GFLOPs efficiency in several settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe experiments are quite comprehensive, covering both classification tasks and segmentation tasks.\n2.\tThe issue of non-convergence is avoided by using positional encoding."}, "weaknesses": {"value": "1.\tThe writing is not very clear. Does \"At every iteration\" in line 164 refer to each batch rather than every iteration during student training? Figure 2 has not been referenced, which makes it necessary to go back and check when (src, dest) is mentioned in line 215.\n\n2.\tThe motivation is still not clear. Why does positional encoding prevent non-convergence in this problem? And why is the Recirculation process designed to enforce spatial invariance? Can changing the source position maintain spatial consistency?\n\n3.\tThe method section is not detailed enough. How is P specifically implemented?\n\n4.\tThe introduction of innovation is insufficient. Is it just a modification of the APM method by applying multiple layers of distillation? The new challenges faced and the motivation for the proposed solutions are not fully explained."}, "questions": {"value": "1. Sensitivity analysis experiments on the same dataset are necessary. Which dataset is the reported α based on in the paper? Does it vary across different datasets?\n2. If 5% of the samples are randomly selected, is the random selection of parameters important? Will different 5% samples cause significant differences?\n3. Have you tried other VLM models besides CLIP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r6hXi1xnbl", "forum": "6en51gFQT1", "replyto": "6en51gFQT1", "signatures": ["ICLR.cc/2026/Conference/Submission6304/Reviewer_7qYH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6304/Reviewer_7qYH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955714219, "cdate": 1761955714219, "tmdate": 1762918605204, "mdate": 1762918605204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To deal with OOD issue for CLIP-like models, previous solutions are finetuning, Test-Time Adaptation, or Test-Time Training. In this paper, authors explore if we really need finetuning and proposed e Layer Query Network, a lightweight module that can be injected to a forzen CLIP model to improve the generalization ability. Extensive experiments on various benchmarks show great performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Good presentation makes the paper easy to follow.\n2. Extensive experiments show promising performance."}, "weaknesses": {"value": "1. If the 3D positional embedding is just learnable parameters for 2D spatial information and layer index?\n\n2. I have a question about the supervision of intermidiate features. based on empirical experience, Distialltion with guidance on intermidiate features cannot provide significant improvements. If any results that without L_B in Algorithm 1?\n\n3. Im not sure if MSE loss for Eq (3) and Eq (4) is good choice."}, "questions": {"value": "1. It would be a little misleading in the title \"VISION-LANGUAGE-MODELS\", people would consider large multimodal language models.\n\n2. Authors claimed \"Faster convergence\" in Line 84, if any experimental results support that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "bA2CMgemhQ", "forum": "6en51gFQT1", "replyto": "6en51gFQT1", "signatures": ["ICLR.cc/2026/Conference/Submission6304/Reviewer_ujEh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6304/Reviewer_ujEh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986859071, "cdate": 1761986859071, "tmdate": 1762918604844, "mdate": 1762918604844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}