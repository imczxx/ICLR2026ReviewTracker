{"id": "4NeiwxQ2Bp", "number": 14973, "cdate": 1758246390165, "mdate": 1759897338286, "content": {"title": "Spurious Rewards: Rethinking Training Signals in RLVR", "abstract": "We show that reinforcement learning with verifiable rewards (RLVR) can elicit strong mathematical reasoning in certain models even with spurious rewards that have little, no, or even negative correlation with the correct answer. For example, RLVR improves MATH-500\nperformance for Qwen2.5-Math-7B in absolute points by 21.4% (random reward),  13.8% (format reward), 24.1% (incorrect label), 26.0% (1-shot RL), and 27.1% (majority voting)---nearly matching the 29.1% gained with ground truth rewards. However, the spurious rewards that work for Qwen often fail to yield gains with other model families like Llama3 or OLMo2. In particular, we find code reasoning---thinking in code without actual code execution---to be a distinctive Qwen2.5-Math behavior that becomes significantly more frequent after RLVR, from 65% to over 90%, even with spurious rewards. Overall, we hypothesize that, given the lack of useful reward signal, RLVR must somehow be surfacing useful reasoning representations learned during pretraining, although the exact mechanism remains a topic for future work. We suggest that future RLVR research should possibly be validated on diverse models rather than a single de facto choice, as we show that it is easy to get significant performance gains on Qwen models even with completely spurious reward signals.", "tldr": "We show that reinforcement learning with verifiable rewards (RLVR) can elicit strong mathematical reasoning in certain models even with spurious rewards that have little, no, or even negative correlation with the correct answer.", "keywords": ["reinforcement learning", "large language model", "reasoning model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3622bc23476a26328225806f1b86b0182645c94b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper primarily finds that training with spurious rewards in reinforcement learning can also improve the performance of specific models on math reasoning tasks. The authors emphasize that this phenomenon mainly occurs in Qwen series models, while for other series of models such as Llama3.1-8b or Olmo2-7B, this phenomenon does not exist. The authors attempt to explain this discrepancy from whether different models possess the reasoning pattern of code reasoning, and find that code frequency can significantly improve the model's test performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors conducted RLVR experiments on multiple models and with different signals including Ground Truth, Majority Vote, One-Shot RL, Format Reward, Incorrect Label, and Random Reward, and the training process was comprehensively demonstrated."}, "weaknesses": {"value": "I greatly admire the authors' attempt to explain why spurious rewards are only effective on Qwen series models through reasoning behaviors learned during pre-training (such as code reasoning). However, I am not convinced by their experimental design, observations, and corresponding explanations. The phenomena and hypotheses feel quite superficial, and some many observations and conclusions may be contradictory.\n\n\n1. The authors propose in Section 4.2 that \"Performance is correlated with code reasoning frequency,\" but from Figure 5, we can see that this conclusion only holds for Qwen2.5-math. For other models such as Qwen 2.5-7B and OLMo2-7B, we can clearly see that the conclusion is reversed. Although the authors explain that \"for the Bad-Code models, we find that the reduction of code reasoning frequency is highly correlated to performance gains,\" I think this is an empirical observation lacking deep analysis, making it difficult to form a generalizable conclusion.\n\n2. Moreover, the authors' conclusion that \"RLVR with spurious rewards occur on the subset of questions where the model transitions from text-based to code-based reasoning after training\" is arrived at too hastily, lacking rigorous theory or experimental design. It is important to emphasize that many experiments show correlation, which does not imply causation. We still hope to see rigorous theoretical or experimental proof.\n\n3. The authors' statements in Lines 176-180 and Lines 358-362 may contradict each other. The former states that the model has not been taught new reasoning capabilities, but merely triggers latent ones already existing in the base model; the latter states that code reasoning declines and natural language reasoning declines, indicating that the model is learning natural language reasoning through RLVR's high-quality, ground truth rewards.\n\n4. The authors' construction of incorrect rewards seems unfair. They first use majority voting, then filter out incorrect labels for training. However, the authors have not provided a reasonable explanation or conducted ablation studies or comparisons with randomly constructed incorrect rewards. Correspondingly, the conclusions the authors reach in Lines 415-419 seem less credible."}, "questions": {"value": "1. Code reasoning is not the only pattern; other patterns such as reflection are also important reasoning behaviors. Can the authors also monitor these metrics during the training process?\n\n2. Qwen models may have undergone pre-training on a large number of math reasoning tasks, leading to biased conclusions. Maybe authors could extend their conclusions to other tasks such as code, general science, or agentic tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RgpD2fGYrw", "forum": "4NeiwxQ2Bp", "replyto": "4NeiwxQ2Bp", "signatures": ["ICLR.cc/2026/Conference/Submission14973/Reviewer_duSx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14973/Reviewer_duSx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632279054, "cdate": 1761632279054, "tmdate": 1762925305880, "mdate": 1762925305880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that RLVR can significantly improve reasoning performance even with random or incorrect rewards, especially for Qwen2.5-Math models.  More specifically, the authors find that spurious rewards (random, format-based, or wrong answers) yield nearly the same gains as ground-truth rewards on MATH-500 and AMC. The effect, however, does not generalize to other model families like Llama3 or OLMo2. Based on the finding, they trace the improvement to Qwen’s pre-existing “code reasoning” behavior and also analysis the clipping bias in GRPO."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper conducts a very detailed set of experiments across different model families, including Qwen and Llama, which allows it to comprehensively demonstrate the scope of its conclusions, mainly applicable to the Qwen series.\n\n2. In the final part, the paper provides an algorithmic analysis that explains its findings from the optimization perspective."}, "weaknesses": {"value": "1. The experimental results in this paper lack sufficient justification. In Appendix I, the authors show Qwen2.5-Math-7B base model scores under different prompts: the best prompt (from SimpleRL-Zoo) reaches 63.2, while the prompt actually used in the main experiments scores only 49.4. This is a large and critical gap, yet Figure 1, the paper’s most important result, uses the lower 49.4 baseline. The same issue appears for Qwen2.5-7B, whose baseline is also lower than that reported in SimpleRL-Zoo. Such a discrepancy could substantially affect the conclusions. For example, some reward signals might actually decrease performance rather than increase it as shown in Figure 1. The choice of a lower baseline therefore requires a more detailed explanation and justification.\n\n2. In Section 4, the connection to the main storyline is unclear. The paper claims that Qwen models tend to produce Python code while others either do not or produce incorrect code, and attributes this to pretraining data differences. However, this does not explain why spurious rewards benefit Qwen but not other models, nor how these rewards are linked to code reasoning. If, as the paper states, “RLVR with spurious rewards upweights pre-existing reasoning strategies,” then why can’t those strategies be similarly activated and improved in other models?\n\n3. Similarly, in Section 5, the paper argues that the GRPO clipping bias explains why random rewards lead to improvement. Yet the same question remains: if clipping bias is the true cause, why don’t other base models also improve? In theory, all models should benefit from this optimization bias. Therefore, this conclusion may not capture the essential mechanism behind the observed phenomenon."}, "questions": {"value": "1. Why is the “incorrect reward”(Lines 152–155) constructed based on majority voting rather than a simpler and more direct design, such as simply inverting the ground-truth labels (i.e., rewarding wrong answers)? What is the motivation for this choice?\n\n2. In Figure 3, the incorrect reward achieves higher performance than the random reward, which seems counterintuitive. Intuitively, deliberately rewarding wrong answers should be even worse than random noise. Could the authors provide an explanation or hypothesis for why this occurs?\n\n3. Do the findings in this paper, particularly the effectiveness of spurious rewards, extend to other reasoning domains such as code generation? Have the authors conducted or considered experiments outside mathematics to verify this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gb0boqV7Zv", "forum": "4NeiwxQ2Bp", "replyto": "4NeiwxQ2Bp", "signatures": ["ICLR.cc/2026/Conference/Submission14973/Reviewer_K7Z4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14973/Reviewer_K7Z4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667500228, "cdate": 1761667500228, "tmdate": 1762925305575, "mdate": 1762925305575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that RLVR can elicit strong mathematical reasoning improvements even with spurious rewards that have little, no, or negative correlation with correct answers. The authors demonstrate that Qwen2.5-Math-7B achieves 21-27% absolute gains on MATH-500 using completely spurious rewards while other model families like Llama3 and OLMo2 fail to benefit from such spurious signals. They identify \"code reasoning\" as a distinctive Qwen behavior that increases from 65% to over 90% after RLVR, even with spurious rewards. The authors hypothesize that RLVR surfaces useful reasoning representations learned during pretraining rather than relying solely on reward signals. However, the exact mechanism remains unclear. These findings suggest that RLVR research should be validated across diverse model families rather than relying on a single model, as it is surprisingly easy to obtain significant but potentially misleading performance gains with spurious rewards on certain architectures."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "**\\[S1\\] Novel and impactful findings**   \nThe paper presents a striking discovery that RLVR can produce substantial performance improvements even with spurious rewards (Qwen). Furthermore, the insight that RLVR methods should be validated across diverse model families rather than relying on a single architecture is a valuable contribution that will benefit the broader research community in avoiding potentially misleading conclusions.\n\n**\\[S2\\] Rigorous experimental design and analysis**   \nThe experiments and analyses are well-structured and effectively support the authors' claims. The progression from main results to deeper investigation is logical and convincing."}, "weaknesses": {"value": "**\\[W1\\]** While the lack of access to pretraining data makes deeper investigation challenging and understandable, the paper's reliance on phenomenon discovery and analysis through one representative behavior (code reasoning) leaves some mechanistic questions unanswered. Although the authors reasonably acknowledge this as future work, a deeper mechanistic explanation would have strengthened the contribution.\n\n**\\[W2\\]** In Section 5 (Curious case), the authors provide additional analysis that supports their hypothesis on random rewards. However, they lack such analysis for incorrect rewards, only providing a hypothesis without comparable supporting experiments. Given that incorrect rewards actively provide misleading signals (rather than just noise), additional investigation into how RLVR still produces gains in this setting would have been valuable.\n\n**\\[W3\\]** The paper would benefit from analyzing pass@K performance on problems that were unsolvable by the base model (pass@K \\= 0 before RLVR). Examining whether RLVR with spurious rewards enables solving such problems, particularly under large K, would more clearly distinguish genuine reasoning expansion from amplification of existing capabilities.\n\n**\\[W4\\]** The experiments focus exclusively on mathematical reasoning tasks. It remains unclear whether these findings generalize to other domains or reasoning task types. Investigating whether spurious reward effectiveness is a math-specific phenomenon or appears across diverse reasoning tasks (e.g., logical reasoning, commonsense reasoning, code generation) would strengthen the generalizability and impact of the findings."}, "questions": {"value": "**\\[Q1\\]** Could the authors clarify the motivation behind introducing format reward and provide additional analysis? Given that the \\\\boxed{} expression format is already included in Qwen2.5-Math's system prompt (L144), it is unclear why this same design choice was applied uniformly to other model families. Was this intentional to test cross-model behavior, or could different format conventions have been more appropriate for Llama3 and OLMo2?\n\n**\\[Q2\\]** When performing multiple rollouts on the same problem, does the frequency of code reasoning vary across samples, and if so, does this variation correlate with accuracy? \n\n**\\[Q3\\]** The paper notes that Qwen2.5-Math engages in code reasoning even without a code interpreter. Have the authors analyzed the quality or validity of the generated code (e.g., whether it is executable, syntactically correct, or error-free)? Is there a correlation between code quality and problem-solving performance, or does RLVR surface code-like reasoning patterns regardless of actual code validity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OfkyTTdhRM", "forum": "4NeiwxQ2Bp", "replyto": "4NeiwxQ2Bp", "signatures": ["ICLR.cc/2026/Conference/Submission14973/Reviewer_w4Pw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14973/Reviewer_w4Pw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989496790, "cdate": 1761989496790, "tmdate": 1762925305067, "mdate": 1762925305067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}