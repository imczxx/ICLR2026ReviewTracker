{"id": "21j00qo8P4", "number": 22378, "cdate": 1758330306434, "mdate": 1759896869591, "content": {"title": "Partial-Correlation Learning for Large Language Models with Skip-Tuning", "abstract": "Large Language Models (LLMs) require post-training to adapt to specific applications, with Supervised Fine-Tuning (SFT) crucial for injecting emerging or domain-specific knowledge. Conventional SFT using complete sequential text risks causing a distribution shift from pretraining corpora due to large volumes of common-style text, potentially leading to overfitting and catastrophic forgetting. We introduce Skip-Tuning, a novel fine-tuning strategy that utilizes noncontinuous text segments instead. Skip-Tuning performs skipped language modeling on text segments and enables a paradigm of partial-correlation learning, where the model learns from sparse but meaningful text fragments. By excluding common-style texts and using only knowledge-intensive text for fine-tuning, Skip-Tuning demonstrates improvements in fine-tuning effectiveness and generalization in the knowledge editing setting. Furthermore, we demonstrate the effectiveness of partial-correlation learning in a system-prompt following task, which illustrates the broad application of Skip-Tuning across various NLP scenarios.", "tldr": "", "keywords": ["Large Language Models", "Supervised Fine-Tuning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/66ef3b087d64cbabae106b3f61ad502e877b7937.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Skip-Tuning, a novel fine-tuning strategy for Large Language Models (LLMs) that utilizes non-successive text segments instead of complete sequential text. The method enables a paradigm called partial-correlation learning, allowing models to focus on sparse but knowledge-rich fragments while skipping redundant or stylistically common text. The authors validate Skip-Tuning on knowledge editing and system prompt following tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Innovative Concept – The idea of learning from \"partial correlations\" through skipped text segments introduces a fresh perspective on data-efficient fine-tuning for LLMs.\n2. Implementation Simplicity – Skip-Tuning requires only minor changes (adjusting position IDs), making it lightweight and practical to deploy.\n3. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "**Limited Empirical Strength**. The results in Table 2 primarily demonstrate the feasibility rather than the superiority of Skip-Tuning. For CLoRA-based LLaMA-3-8B-Instruct, LoRA-based LLaMA-2-7B-Chat, and Mistral-v0.3-7B-Instruct, Skip-Tuning does not yield consistent improvements across overall metrics (e.g., higher edit effectiveness often comes with greater forgetting). This indicates that while the method works, it is not convincingly better than standard SFT or LoRA. Consequently, the claim of \"improvement in fine-tuning effectiveness and generalization\" appears somewhat overstated given the presented performance.\n\nI value the conceptual novelty of this approach and its promising results in system prompt following. However, its performance on knowledge editing is less convincing. Knowledge editing is one of the most suitable application for Skip-Tuning, since knowledge injection does not strongly depend on input format. The results in this setting suggest that the practical benefit of the method may be limited."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JbwtaDbc2Z", "forum": "21j00qo8P4", "replyto": "21j00qo8P4", "signatures": ["ICLR.cc/2026/Conference/Submission22378/Reviewer_5EJp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22378/Reviewer_5EJp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772708011, "cdate": 1761772708011, "tmdate": 1762942190766, "mdate": 1762942190766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Skip-Tuning, a fine-tuning strategy for LLMs that replaces conventional sequential text inputs with non-successive text segments. The core idea is to perform partial-correlation learning from sparse but meaningful knowledge-intensive fragments, thereby alleviating distribution shift from pretraining corpora and reducing catastrophic forgetting. The effectiveness is evaluated on knowledge editing and system-prompt following tasks, showing comparable performance across multiple models and settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed Skip-Tuning requires almost no modiﬁcation to be adopted in various LLM infrastructure settings.\n\n2. Experiments across knowledge editing and system-prompt tasks demonstrate generality."}, "weaknesses": {"value": "1. The methodology about selecting text segments, determining positional margins, and assigning position identifiers requires more detailed explanation.\n\n2. Some sentences are hard to follow. The description of the method, particularly in Sections 4.1.2 and 4.2, could be more structured for easier understanding.  \n\n3. The impact of the predetermined margin between segments is not explored in depth. The choice of margins (5 for knowledge editing, 100 for prompt following) seems somewhat arbitrary. \n\n4. A deep theoretical explanation or analysis of why and how this method works from partial-correlation learning is missing."}, "questions": {"value": "1. How sensitive are the results to the choice of the position ID \"margin\"? Are there guidelines or principles  for setting this margin? \n\n2. How would the method perform if the segments were chosen arbitrarily?\n\n3. If the tasks are without a predefined structure, how is the margin between segments determined? \n\n4. Under which scenarios or types of tasks do you anticipate Skip-Tuning might perform poorly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E8kS3L5Hc2", "forum": "21j00qo8P4", "replyto": "21j00qo8P4", "signatures": ["ICLR.cc/2026/Conference/Submission22378/Reviewer_SGN4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22378/Reviewer_SGN4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838193293, "cdate": 1761838193293, "tmdate": 1762942190423, "mdate": 1762942190423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Skip-Tuning, a fine-tuning strategy for large language models that uses non-successive text segments rather than full sequential text for supervised fine-tuning. The authors argue that conventional SFT datasets often contain large amounts of “common-style” or formatting-heavy text that may cause distribution shift and increase the risk of catastrophic forgetting. Skip-Tuning instead focuses on knowledge-intensive fragments and assigns non-successive positional IDs to encourage the model to learn partial-correlation patterns rather than full contextual co-occurrences.\n\nThe approach is evaluated in two settings: knowledge editing (using HalluEditBench) and system prompt following (using RealGuardRails and System-IFEval). Across several base models (LLaMA-2, LLaMA-3, Mistral), Skip-Tuning shows improvements in efficacy and generalization, with some evidence of reduced forgetting in stronger base models. In system prompt following, mixing Skip-Tuning samples with standard SFT improves adherence to system instructions, especially when using moderate Skip-Tuning ratios (e.g., 25–50%) and optional position ID perturbation.\n\nOverall, the paper shows some empirical gain without having any analytical insight into why it's the case."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a simple and practical method. Skip-Tuning requires minimal architectural changes and integrates naturally with existing LLM fine-tuning workflows. It is easy to adopt in practice.\n\nThe papers demonstrates broad applicability. The method is tested on both knowledge editing and system prompt adherence, which strengthens the claim that Skip-Tuning is not task-specific. \n\nThe paper shows gains in some key empirical metrics. Improvements in efficacy and generalization (in HalluEditBench) and system prompt compliance (in RealGuardRails/System-IFEval) suggest that partial-correlation learning is meaningful."}, "weaknesses": {"value": "The paper lacks theoretical grounding. While the intuition of “partial-correlation learning” is appealing, the paper does not provide a formal characterization of what correlations are preserved vs. removed, or how this affects the model’s internal representation. The explanation remains qualitative.\n\nThe paper shows mixed forgetting results. While Skip-Tuning improves forgetting metrics for strong models, it underperforms on weaker models and when interacting with CLoRA constraints. This suggests that the method’s robustness is not consistent across architectures.\n\nEvaluation on System Prompt Following Could Be Expanded. The benchmarks used measure rule adherence, but do not evaluate fluency, safety, or stability trade-offs introduced by fragment-based tuning."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VWmKP58Ei6", "forum": "21j00qo8P4", "replyto": "21j00qo8P4", "signatures": ["ICLR.cc/2026/Conference/Submission22378/Reviewer_j1GE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22378/Reviewer_j1GE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953802233, "cdate": 1761953802233, "tmdate": 1762942190186, "mdate": 1762942190186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an input-enhanced training approach that improves the generalization of SFT by fine-tuning LLMs on selected meaningful text fragments from the training data. The experimental results demonstrate improvements in both knowledge editing and system prompt following."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed method is relatively novel.\n* The experiments demonstrate the superiority of the proposed approach on knowledge editing and system prompt following."}, "weaknesses": {"value": "* The proposed method bears similarity to data augmentation techniques for LLMs. However, the paper lacks sufficient comparisons with related baseline methods (e.g., traditional robust training approaches that introduce perturbations in the input space such as synonym substitution), and the corresponding discussion is limited. The absence of such comparison and discussion is my main concern about this paper.\n\n* For instruction-following tasks, the proposed method requires tuning two important hyperparameters, yet the paper only investigates its performance on a single model, which limits the generalizability of the method."}, "questions": {"value": "* Can the proposed method be combined with ICL? Extending it to a broader range of SFT tasks would be valuable.\n\n* How would the performance change if traditional perturbation techniques, such as synonym substitution or random token deletion on non-meaningful text fragments, were applied instead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xwrGPrh6JM", "forum": "21j00qo8P4", "replyto": "21j00qo8P4", "signatures": ["ICLR.cc/2026/Conference/Submission22378/Reviewer_eyaG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22378/Reviewer_eyaG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016359059, "cdate": 1762016359059, "tmdate": 1762942189767, "mdate": 1762942189767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}