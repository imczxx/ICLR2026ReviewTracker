{"id": "zM1JvKOdWT", "number": 3770, "cdate": 1757516439013, "mdate": 1762996159299, "content": {"title": "GraphDenoiser: An Unsupervised Iterative Framework for Node Label Denoising in Graph-Structured Data", "abstract": "Data annotation errors have always been one of the core challenges in the field of supervised learning: such noise not only interferes with the model's effective learning of the patterns of data distribution, but also directly leads to the model's discriminative bias in target tasks, significantly reducing the predictive accuracy of supervised learning systems. For graph-structured data, due to the uniqueness of the associative relationships between data points, traditional denoising methods struggle to adapt to the noise distribution patterns in this scenario. To address this issue, this paper focuses on the denoising problem of node type labels in graph data and proposes a denoising framework based on unsupervised learning called GraphDenoiser. Through multiple rounds of iteration between the node label noise prediction model and the synthetic data generation model, this framework can quantitatively output the noise probability of each node label and accurately locate mislabeled nodes in graph data. This provides a reliable noise diagnosis basis for subsequent label correction and robust graph model training, thereby alleviating the constraints of node mislabeling in graph data on supervised learning performance. Experiments show that under eight different noise injection methods across three datasets, compared with previous methods, the metrics of MCC, and F1 have increased by 22.81%, and 30.51% respectively.", "tldr": "", "keywords": ["Graph", "Unsupervised Learning", "Node Label Denoising", "Iterative Optimization"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3f8fea9bef2a48530fecedbff61f3eecb9816749.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the issue of correcting data annotation errors within graph-structured data. Unlike regular data, annotation errors on graph data can propagate to neighboring nodes through message passing, making the correction process significantly more complex. To tackle this, the authors propose pre-training a model on synthetic annotation errors and subsequently applying the pre-trained model to rectify errors in the target dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper explores the relatively under-researched problem of annotation error correction in graph-structured data, tackling an important and challenging issue. \n* Experimental results demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "* The paper does not clearly highlight the improvements of GraphDenoiser over GraphCleaner, a related method that also utilizes pre-training with synthetic annotation errors. Clarifying the contribution and advancements of GraphDenoiser would strengthen the paper.\n* The method is only validated on synthetic datasets. Incorporating evaluations on real-world datasets with annotation errors would enhance the practical applicability and robustness of the proposed approach.\n* There are inconsistencies in notation, such as the use of y_{NewData} in Algorithm 1 (lines 8 and 10). Ensuring consistent symbols throughout the paper would improve readability and clarity."}, "questions": {"value": "* In alogorithm 1, line 10, should the input to G also include X? \n* Does the noise injection process used during the pre-training stage and test stage follow the same methodology?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kCeyduOoRW", "forum": "zM1JvKOdWT", "replyto": "zM1JvKOdWT", "signatures": ["ICLR.cc/2026/Conference/Submission3770/Reviewer_LrUu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3770/Reviewer_LrUu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3770/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539728434, "cdate": 1761539728434, "tmdate": 1762916980360, "mdate": 1762916980360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "sugDfEtHW4", "forum": "zM1JvKOdWT", "replyto": "zM1JvKOdWT", "signatures": ["ICLR.cc/2026/Conference/Submission3770/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3770/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762996158584, "cdate": 1762996158584, "tmdate": 1762996158584, "mdate": 1762996158584, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GraphDenoiser, an unsupervised iterative framework for denoising node labels in graph-structured data. The framework jointly trains a node label noise prediction model and a graph noise generation model in an iterative manner, supported by a pre-trained autoencoder for stable feature embedding and an experience replay mechanism to improve training robustness. Experiments are conducted on three standard datasets (Cora, CiteSeer, PubMed) under multiple synthetic noise settings, showing performance improvements over existing baselines such as GraphCleaner, CL, and DYB."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed iterative process combining noise generation and noise prediction is conceptually interesting .\n    \n2. The paper is clearly written, with a structured methodology and detailed experimental results."}, "weaknesses": {"value": "1-The paper is primarily heuristic. No theoretical justification or convergence analysis is provided for the iterative denoising process. It remains unclear why or under what conditions the iterative scheme improves noise detection.\n\n2-The key ideas (autoencoder pretraining, experience replay, synthetic noise generation, and iterative training) are borrowed from well-known concepts in fields like generative modeling, denoise included. Their combination for graph denoising is incremental rather than fundamentally novel.\n\n3- Some strong recent baselines for graph robustness or denoising are either not compared or insufficiently discussed.\n\n4- The method involves multiple components (autoencoder, noise generator, replay buffer) but lacks clear intuition about each part’s contribution beyond empirical observation. The ablation study confirms importance but not mechanism.\n\n5-The introduction and methodology sections are verbose and read more like a technical report than a scientific argument.\n\n6-It inherently a threshold method which need set a gap threshold, eg P> 0.5"}, "questions": {"value": "Why is the first-in-first-out strategy adopted instead of alternative approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lRmtthd7hb", "forum": "zM1JvKOdWT", "replyto": "zM1JvKOdWT", "signatures": ["ICLR.cc/2026/Conference/Submission3770/Reviewer_M3Ui"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3770/Reviewer_M3Ui"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3770/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549017758, "cdate": 1761549017758, "tmdate": 1762916979923, "mdate": 1762916979923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GraphDenoiser, an unsupervised, iterative framework to detect mislabeled nodes in graph data. The loop alternates between (i) a node-label noise predictor and (ii) a graph noise generator adapted from GraphCleaner that estimates a label-transition matrix and flips labels accordingly, producing synthetic noisy graphs used to train the predictor. An autoencoder injects Gaussian noise in embedding space for sampling, and an experience-replay buffer stabilizes training. Experiments on Cora, CiteSeer, and PubMed with synthetic symmetric/asymmetric noise at several rates report improvements in MCC and F1 over baselines."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper adopts several sensible metrics (P@T, MCC, F1) and conducts experiments with a comprehensive synthetic noise grid across datasets\n\n- Experimental results show decent empirical gains of the proposed method"}, "weaknesses": {"value": "- The presentation is poor, due to numerous missing logical connections and insufficient explanation of key steps, leaving many implementation and reasoning details unclear.\n\n- The claimed limitations have already been addressed by recent works [1–4], which consider graph structure and reduce dependence on clean labels, making the paper’s motivation and contribution unconvincing.\n\n- Limited method novelty: mostly a recombination of established components (transition-matrix noise simulation, confident/noise-aware training, AE perturbation, replay). The “first-time” claim is unsubstantiated.\n\n- “Unsupervised” is overstated: the generator estimates a mislabel transition matrix using a pretrained GNN and validation data, which contradicts the unsupervised framing and may entangle the evaluation with the same supervision used to synthesize labels.\n\n- Narrow evaluation: only classic citation graphs with synthetic noise; no real noisy-label benchmarks, heterophilous graphs, or larger modern datasets to test scalability and external validity. \n\n- Methodological under-specification: fixed 0.5 threshold for noise decisions is asserted to be “interpretable” but lacks calibration analysis. Sensitivity to AE architecture, σ, and replay capacity is not explored.\n\n- The references and baselines are outdated, with the recent baselines only from 2023, ignoring relevant advances from 2024–2025.\n\n[1] Dong, M., & Kluger, Y. (2023, July). Towards understanding and reducing graph structural noise for GNNs. In International Conference on Machine Learning (pp. 8202-8226). PMLR.\n\n[2] Wu, J., Hu, R., Li, D., Huang, Z., Ren, L., & Zang, Y. (2024). Robust heterophilic graph learning against label noise for anomaly detection. Structure, 4(v5), v6.\n\n[3] Li, K., Sun, J., Lou, J., Feng, Z., Zhou, H., Wu, C., ... & Li, J. Leveraging Peer-Informed Label Consistency for Robust Graph Neural Networks with Noisy Labels.\n\n[4] Wang, Z., Sun, D., Zhou, S., Wang, H., Fan, J., Huang, L., & Bu, J. (2024). Noisygl: A comprehensive benchmark for graph neural networks under label noise. Advances in Neural Information Processing Systems, 37, 38142-38170."}, "questions": {"value": "1. Why is the approach unsupervised if the generator requires a pretrained GNN and a validation set? \n\n2. Why is 0.5 appropriate for all datasets/noise regimes? Did you try calibrating the predictor (e.g., temperature scaling) or optimizing thresholds on held-out data?\n\n3. How does the method handle heterophily and structural noise (edge flips/additions) rather than label noise alone? Any results on heterophilous graphs or edge-noise settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GkDrN4zbGA", "forum": "zM1JvKOdWT", "replyto": "zM1JvKOdWT", "signatures": ["ICLR.cc/2026/Conference/Submission3770/Reviewer_mpV9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3770/Reviewer_mpV9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3770/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712433813, "cdate": 1761712433813, "tmdate": 1762916979601, "mdate": 1762916979601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an unsupervised graph denoising method, GraphDenoiser, which trains the noise prediction model through noise data sampling and iterative training. It claims to outperform multiple baselines on symmetric/asymmetric noise-injected graph datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper discusses an important problem: how to handle noisy data in graph data."}, "weaknesses": {"value": "- Effectiveness of the Method is Doubtful: a) The noise distribution sampled according to the strategy in the paper likely differs significantly from real noise distributions, which raises doubts about the utility of the denoising model. 2) The noise predictor uses an MLP structure, which is likely unsuitable for graph noise structures.\n\n- Experiments use Self-Sampled Noise: The experiments rely on self-sampled noise and do not test with real noisy datasets, such as NoisyGL or BeGIN.\n\n- Lack of Generalization Experiments: There is no discussion of the generalization performance of GraphDenoiser.\n\n- Writing Issues: The paper's writing (including figures) is unclear, and the formalization of equations could be improved."}, "questions": {"value": "- Please explain the specific structure of the AutoEncoder used.\n- Please see the remaining questions in \"Weakness\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nbBX1DlD09", "forum": "zM1JvKOdWT", "replyto": "zM1JvKOdWT", "signatures": ["ICLR.cc/2026/Conference/Submission3770/Reviewer_PJey"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3770/Reviewer_PJey"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3770/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821649038, "cdate": 1761821649038, "tmdate": 1762916979121, "mdate": 1762916979121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}