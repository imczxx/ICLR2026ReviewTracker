{"id": "Vjruxvp1Xd", "number": 15196, "cdate": 1758248896852, "mdate": 1759897322158, "content": {"title": "Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks", "abstract": "Deep reasoning is fundamental for solving complex tasks, especially in vision-centric scenarios that demand sequential, multimodal understanding. However, existing benchmarks typically evaluate agents with fully synthetic, single-turn queries, limited visual modalities, and lack a framework to assess reasoning quality over multiple steps as required in real-world settings. To address this, we introduce Agent-X, a large-scale benchmark for evaluating vision-centric agents’ multistep and deep reasoning capabilities in real-world, multimodal settings. AgentX features 828 agentic tasks with authentic visual contexts, including images, multi-image comparisons, videos, and instructional text. These tasks span six major agentic environments: general visual reasoning, web browsing, security and surveillance, autonomous driving, sports, and math reasoning. Our benchmark requires agents to integrate tool use with explicit, stepwise decision-making in these diverse settings. In addition, we propose a fine-grained, step-level evaluation framework that assesses the correctness and logical coherence of each reasoning step and the effectiveness of tool usage throughout the task. Our results reveal that even the best-performing models, including GPT, Gemini, and Qwen families, struggle to solve multi-step vision tasks, achieving less than 50% full-chain success. These findings highlight key bottlenecks in current LMM reasoning and tool-use capabilities and identify future research directions in vision-centric agentic reasoning models", "tldr": "Agent-X benchmark (828 multimodal tasks) tests deep, stepwise reasoning in vision agents; SOTA models score <50%, revealing major gaps in multimodal reasoning and tool use.", "keywords": ["Vision centric Agents", "Deep Reasoning", "VLMs", "Tool Use Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f758a8ca2e9dbc17d2b55b786622831b9740f106.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Agent-X, a new, large-scale benchmark designed to evaluate the deep reasoning and tool-use capabilities of vision-centric AI agents. The paper argues that existing benchmarks are insufficient because they often rely on synthetic, single-turn queries, have limited visual modalities (mostly static images), and fail to assess the step-by-step quality of an agent's reasoning process.\n\nTo solve this, the paper makes two key contributions:\n* Agent-X provides 828 complex, multi-step tasks across six diverse, real-world environments: general visual reasoning, web browsing, security/surveillance, autonomous driving, sports, and math reasoning. These tasks involve rich multimodal inputs, including images, multi-image comparisons, videos, and instructional text.\n* A fine-grained, step-level evaluation framework. Instead of only checking the final answer, this framework analyzes the entire reasoning chain for:\n  * Step-by-Step correctness (e.g., tool selection and grounding).\n  * Deep Reasoning quality (e.g., logical coherence, factual precision).\n  * Outcome success (e.g., final answer accuracy).\n\nThe authors test several leading large multimodal models (LMMs), including the GPT, Gemini, and Qwen families. Their key observations are as follows:\n\n* No model tested achieves a Goal Accuracy rate above 50%. The best-performing model, OpenAI-4o-mini, only reached 45%, with most open-source models scoring below 30%. This indicates that complex, real-world, tool-augmented tasks remain extremely challenging.\n\n* Models that scored higher on the intermediate \"Deep Reasoning\" metrics (like faithfulness to the task, factual precision, and semantic accuracy) were more likely to produce a correct final answer. This supports the paper's hypothesis that evaluating the reasoning chain is critical.\n\n* Tool Use is a Core Bottleneck: Models struggle significantly with tool invocation and argument prediction. The error analysis shows that even when a model's thought process is logical, it often fails by selecting the wrong tool, hallucinating a tool that doesn't exist, or providing incorrectly formatted arguments.\n\n* Distinct Error Profiles for Top Models:\n  * GPT-4o: Showed strong formatting skills but was overly hesitant, often failing to act or misinterpreting visual content.\n  * Gemini-1.5-Pro: Was aggressive (rarely refused a task) but format-fragile, with a very high rate of JSON formatting errors (44.5%) and visual misinterpretations (34.3%)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The Agent-X benchmark is comprehensive and well-thought-out:\n  * The inclusion of videos, multi-image comparisons, and text alongside single images is a significant step up from most vision benchmarks.\n  * Using six practical domains (like autonomous driving and security) makes the tasks more realistic and relevant than purely synthetic tests.\n  * Queries are designed to be \"tool-agnostic\" (i.e., they don't tell the agent which tool to use), forcing the agent to perform genuine reasoning about tool selection.\n\n* The three-mode evaluation (Step-by-Step, Deep Reasoning, Outcome) provides a powerful diagnostic tool. It allows researchers to move beyond a simple \"pass/fail\" score and understand why and where an agent is failing.\n\n* The paper details a semi-automated pipeline with human-in-the-loop refinement. The examples in the appendix clearly show how this process turns low-quality, ambiguous, or multi-part LMM-generated queries into focused, high-quality, verifiable benchmark tasks."}, "weaknesses": {"value": "* While a clear improvement over single-step tasks, an average of 3-4 steps may not be sufficient to discover breakages in long-horizon planning, memory, and context management.\n* Reliance on model based critique can be brittle (e.g., known to be reward hacked). Perhaps using inference time techniques (like majority scoring) can be used to alleviate such risks.\n* The authors correctly identify the monolingual and potential dataset biases."}, "questions": {"value": "* Have the authors considered maintaining a leaderboard for this benchmark?\n* Are the authors motivated to extend this dataset to address the biases, shortcomings and to harder problems as the frontier models get better at solving the existing problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5tkDMxd4md", "forum": "Vjruxvp1Xd", "replyto": "Vjruxvp1Xd", "signatures": ["ICLR.cc/2026/Conference/Submission15196/Reviewer_KPXX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15196/Reviewer_KPXX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890017072, "cdate": 1761890017072, "tmdate": 1762925499963, "mdate": 1762925499963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Agent-X, a benchmark for evaluating vision-centric AI agents' multimodal reasoning abilities. The benchmark contains 828 tasks spanning six environments (visual reasoning, web browsing, security, autonomous driving, sports, and mathematics) using real-world images, videos, and text inputs. Unlike existing benchmarks that use synthetic queries, Agent-X requires multi-step reasoning with tool usage in authentic scenarios. The authors develop a step-level evaluation framework that measures reasoning correctness, logical coherence, and tool effectiveness. Testing on 10 large multimodal models shows that even top-performing systems achieve less than 50% success on complete reasoning chains. The results indicate significant limitations in current models' tool usage and multi-step reasoning capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors have designed a comprehensive tool subset for the benchmark that covers most tools required for visual tasks. The evaluation criteria are also well-rounded, encompassing multiple dimensions of assessment.\n2. The benchmark demonstrates a significant improvement in sample size compared to previous benchmarks, which represents a notable contribution of this work.\n3. The benchmark proves to be quite challenging for most state-of-the-art open-source and closed-source models. From the results, no model achieves 50% on the Goal Accuracy metric, demonstrating the benchmark's effectiveness in evaluating model capabilities."}, "weaknesses": {"value": "1. The template format used in the paper appears to differ from the official template provided. I am uncertain whether this may violate ICLR's formatting requirements. I recommend that the authors carefully review and address any formatting issues.\n2. The font in Figure 3(b) is quite blurry.\n3. I am not familiar with dataset construction in the agent domain. Could you please explain why JSON-formatted dialogue output is adopted? What is the rationale behind this design choice?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nuDi11ZYwQ", "forum": "Vjruxvp1Xd", "replyto": "Vjruxvp1Xd", "signatures": ["ICLR.cc/2026/Conference/Submission15196/Reviewer_5f8j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15196/Reviewer_5f8j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927520720, "cdate": 1761927520720, "tmdate": 1762925498150, "mdate": 1762925498150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Agent-X, a large-scale benchmark for evaluating deep multimodal reasoning in vision-centric agentic tasks. The benchmark consists of 828 tasks across six environments (general visual reasoning, web browsing, security/surveillance, autonomous driving, sports, and math reasoning). It features multi-step reasoning traces, tool-use sequences, and fine-grained metrics evaluating reasoning quality, coherence, and tool effectiveness.\nThe authors evaluate 12 state-of-the-art large multimodal models (LMMs) — including GPT-4o, Gemini 2.5, and Qwen2.5-VL — showing that even leading models perform below 50% on complex reasoning chains. The paper claims to be the first to combine vision-first multimodal reasoning with stepwise tool-augmented evaluation in real-world settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a timely and relevant problem by filling a clear gap in evaluating deep reasoning and tool use for multimodal agents, an increasingly important topic as LMMs evolve into embodied and interactive systems.\n2. It presents a comprehensive benchmark design that spans six diverse domains and modalities, including images, multi-image comparisons, videos, and text.\n3. The tasks are realistic rather than synthetic, enhancing ecological validity compared to prior benchmarks like GAIA or GTA.\n4. The paper introduces fine-grained, multi-level metrics—covering step-by-step, deep reasoning, and outcome modes—that enable nuanced evaluation beyond final-answer accuracy.\n5. These metrics are particularly useful for analyzing reasoning coherence and the consistency of tool usage.\n6. The experimental analysis is thorough, evaluating both open and closed models, incorporating multiple judges (GPT-4o, Qwen, and humans), and providing detailed error analyses.\n7. It offers valuable insights into common failure modes such as formatting errors, shallow reasoning, and hallucinated tool calls.\n8. Finally, the benchmark is reproducible and openly available, with detailed documentation of the data pipeline, annotation processes, and tool specifications."}, "weaknesses": {"value": "1. The paper primarily integrates existing components i.e. LMM reasoning, multimodal datasets, and tool evaluation, without introducing a fundamentally new evaluation paradigm, making it appear incremental compared to GAIA, GTA, and MLGym.\n2. The evaluation setup relies heavily on GPT-4o and Qwen-based automatic grading, raising bias and circularity concerns since the same model families are both evaluated and used as judges, with no quantitative inter-rater agreement reported.\n3. Despite its claim of real-world grounding, all tasks remain within predefined toolsets, offering limited evidence of generalization to unseen tools or domains, and no transfer or few-shot results are provided.\n4. The semi-automated dataset construction pipeline may inherit biases and linguistic artifacts from the generating LMMs, and the human refinement process lacks quantitative validation or agreement metrics.\n5. The study omits ablation experiments or baseline comparisons using shallow or random reasoning traces, leaving it unclear whether Agent-X truly measures deep reasoning rather than surface pattern matching; key metrics are also GPT-based without formal definitions or validation."}, "questions": {"value": "1. Could you provide quantitative measures of inter-judge agreement (e.g., Cohen’s κ or correlation scores) to establish evaluation consistency among GPT-4o, Qwen-14B, and human annotators?\n2. How do you mitigate the potential circularity bias that arises when GPT-4o, a system under evaluation, is also used as a grader?\n3. Have these metrics been validated through human judgment or ablation studies to confirm that they align with actual reasoning quality?\n4. How do you ensure that the benchmark does not inherit linguistic or reasoning biases from the LMMs used in the task generation process?\n5. Did you measure inter-annotator agreement during human refinement, and how were disagreements resolved?\n6. Why are there no ablations comparing against random or shallow reasoning traces to validate that Agent-X genuinely measures deep reasoning complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HV1YGWcGQA", "forum": "Vjruxvp1Xd", "replyto": "Vjruxvp1Xd", "signatures": ["ICLR.cc/2026/Conference/Submission15196/Reviewer_YEeg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15196/Reviewer_YEeg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993413772, "cdate": 1761993413772, "tmdate": 1762925497698, "mdate": 1762925497698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Agent-X, a benchmark for evaluating deep multimodal reasoning in vision-centric agentic tasks. It assesses how well multimodal models perform multi-step reasoning using images, videos, and tools across six environments — visual reasoning, web browsing, surveillance, driving, sports, and math. The paper includes 828 multimodal tasks, 14 executable tools and evaluated 10 LMMs. Models are evaluated in three modes i.e. Step-by-step, Deep Reasoning Outcome. Also, multiple metrics are reported for each of these modes. The findings suggest that even the best models achieve <50% full-chain success and models struggle with tool usage and reasoning consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Covers six distinct domains and integrates both image and video data\n2) Presents detailed analysis of failure modes of SOTA models\n3) Evaluates not just end answers but intermediate reasoning, tool grounding, and logical coherence\n4) Multiple metrics are reported in three distinct evaluation modes"}, "weaknesses": {"value": "1) Prompts need to be designed for LLM-as-judge. Is there an automated way to do this? Or do the authors need to manually create them every time a new metric is introduced?\n2) Human involvement at multiple stages makes it challenging to scale such a data generation approach. Also this is prone to errors and inconsistency? Any discussion around this would provide clarity.\n3) The dataset is built on top of publicly available datasets, it is hence unclear if this dataset is truely novel apart from some processing done on these datasets.\n4) No mention of variance, significance testing, or confidence intervals in Table 4 results. The results may vary due to use of LLMs in multiple places from task generation, task solving to evaluation.\n5) From the third paragraph in section 3.3, it seems like humans evaluate the correctness of tool calls and its coherence with final answer. Why are humans involved here, can't the tools calls be executed and answer returned to LLM and hence the model proceeds to complete the task.\n6) In the \"Step-by-Step\" process evaluation \"Tool Precision\" seems to be an unnecessary metric as same task can be completed via very different sequences of tool calls. Comparing a tool call sequence at test time to a \"ground truth\" sequence might not be an ideal way if evaluation for several task like web surfing where the environment can be very dynamic and hence change across runs.\n7) Metrics related to hallucination in \"Deep Reasoning Mode\", could be wrongly estimated if the LLM judge itself is prone to hallucination.\n8) Point 6 and 7 render some of the metrics unreliable and their variation needs to be studied across runs.\n9) Average steps per task is 3.4 (from Figure 3a ), which shows that the tasks are rather simple. How does this pipeline scale to tasks with more difficulty and longer trajectories. Also, evaluation of such trajectories seems to be a challenge with the proposed metrics and would incur costs as LLMs are used as judges.\n10) There’s no clear baseline showing how much performance improvement stems from reasoning vs pure perception. A sample experiment would be to take web tasks and evaluation LLMs using the access tree or DOMs instead of the image\n11) Human validation time (~50 hours per annotator) is low given 828 tasks, raising concerns about depth of verification."}, "questions": {"value": "1) All the metrics are derived from LLM-as-judge i.e Qwen14B and GPT-4o. Also results for human evaluations are presented. But this approach is not scalable, as the number of tasks and models increase. Do the authors have a method to scale these evaluations?\n2) How consistent and unbiased is the human refinement process, and how do the authors ensure annotation quality or diversity across annotators?\n3) Were multiple runs conducted for each model to assess reproducibility and statistical significance of reported metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "orNWdru2nR", "forum": "Vjruxvp1Xd", "replyto": "Vjruxvp1Xd", "signatures": ["ICLR.cc/2026/Conference/Submission15196/Reviewer_bQ33"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15196/Reviewer_bQ33"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184311275, "cdate": 1762184311275, "tmdate": 1762925497053, "mdate": 1762925497053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}