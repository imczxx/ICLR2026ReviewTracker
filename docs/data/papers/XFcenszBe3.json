{"id": "XFcenszBe3", "number": 13195, "cdate": 1758214950967, "mdate": 1759897457236, "content": {"title": "Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders", "abstract": "Large language models (LLMs) excel at handling human queries, but they can occasionally generate flawed or unexpected responses. Understanding their internal states is crucial for understanding their successes, diagnosing their failures, and refining their capabilities. Although sparse autoencoders (SAEs) have shown promise for interpreting LLM internal representations, limited research has explored how to better explain SAE features, i.e., understanding the semantic meaning of features learned by SAE. Our theoretical analysis reveals that existing explanation methods suffer from the frequency bias issue, where they emphasize linguistic patterns over semantic concepts, while the latter is more critical to steer LLM behaviors. To address this, we propose using a de-duplicated vocabulary set for feature interpretations and designing a mutual information-based objective, aiming to better capture the semantic meaning behind these features. We further propose two runtime steering strategies that adjust the learned feature activations based on their corresponding explanations. Empirical results show that, compared to baselines, our method provides more discourse-level explanations and effectively steers LLM behaviors to defend against jailbreak attacks. These findings highlight the value of explanations for steering LLM behaviors in downstream applications. We will release our code and data once accepted.", "tldr": "", "keywords": ["Interpretation", "Sparse Autoencoders", "Large Language Models", "Model Steering"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60346ca088794abd8424199bb6cd8cef9958909d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper works around the challenge of interpreting and steering LLMs via SAEs. The authors claim \"frequency bias\" in current methods results in high-frequency linguistic patterns rather than deep semantic/discourse-level concepts. They propose a de-duplicated vocabulary and a mutual information-based objective for selecting explanations of SAE features. They also propose two strategies to steer LLMs using their explanations: amplification and calibration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and organized.\n\nThey clearly identify and support their claim regarding the frequency bias issues within LLMs.\n\nThey advocate their work by answering the following two questions:\nRQ1: Does the proposed method generate more discourse-level explanations than traditional methods? \nRQ2: Whether these discourse-level explanations useful in steering LLM behaviors?\n\nThe experiments are fairly comprehensive w.r.t model selection, datasets, and previous baselines."}, "weaknesses": {"value": "The evaluation heavily relies on an LLM as a judge. It would be better and more reliable to have a human in the loop.\n\nThe assumption that $p(e_C = W_c) \\approx 1$ is not justified. I feel like assuming that $W_c$ represents a unique topic $e_C$ is a strong assumption."}, "questions": {"value": "Have you done any ablation where only one of the de-duplication or the MI-based is considered? I'm wondering if vocabulary pruning is more effective or the new objective?\n\nIs the proposed method generalizable to other applications, rather than jailbreak? For instance, for style control or factuality enhancement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sveYFBVup7", "forum": "XFcenszBe3", "replyto": "XFcenszBe3", "signatures": ["ICLR.cc/2026/Conference/Submission13195/Reviewer_rrmv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13195/Reviewer_rrmv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761469578412, "cdate": 1761469578412, "tmdate": 1762923890176, "mdate": 1762923890176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper points out a common issue when interpreting SAE features, which is that using top-activating tokens from the corpora yields repetitive phrases. They present a new method to extract top tokens that are less repetitive, which can then be used to label SAE features. They do a steering experiment with these new feature descriptions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors show  clear evidence that prior methods to interpret SAE features might be dominated by repetitive text strings in training corpora."}, "weaknesses": {"value": "1. Clarity of some sections could be improved. For example, could the authors more clearly describe the metrics they use to evaluate how good an explanation is - like the Explained Rate (4.2)?\n2. **Steering experiment?** Please clarify the steering experiment, as is it's not really clear how you are comparing your explanation method to the authors in this experiment.\n3. **Tasks beyond steering:** The paper would be stronger if there were a clearer link to how the proposed explanation method will help in downstream tasks besides steering, which recent work finds that SAEs lose to baselines on [4]. For example, does the proposed method improve hypothesis generation [1] or understanding of model biology [2]?\n4. It isn't clear if the proposed explanation method actually yields natural language descriptions that better predict the SAE feature activations compared to baselines. For example, some measure of explanation fidelity or generation scoring [3] would be useful. If this is how the \"Explained Rate\" metric already works, the authors should clarify this.\n\n\n[1] https://arxiv.org/abs/2502.04382  \n[2] https://transformer-circuits.pub/2025/attribution-graphs/biology.html  \n[3] https://blog.eleuther.ai/autointerp/  \n[4] https://arxiv.org/abs/2501.17148"}, "questions": {"value": "1. It's hard to compare this method against the baselines (e.g. in Table 1) because all the methods are interpreting different features. Can we see the *same* features and the top spans that different methods extract?\n2. How does the experiment in 4.3.2 work (details are currently unclear), and how does it show that the authors' proposed explanation method is better?\n3. One simpler approach to mitigate the issue of repetitive phrases is just to use TopAct, except with a diversity filtering step. Did the authors try this? (e.g. sample from the top 10% bin of positive activations, and exclude any examples with very high cosine similarity with others)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jBJSaDXWxt", "forum": "XFcenszBe3", "replyto": "XFcenszBe3", "signatures": ["ICLR.cc/2026/Conference/Submission13195/Reviewer_tgAp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13195/Reviewer_tgAp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761614428276, "cdate": 1761614428276, "tmdate": 1762923889239, "mdate": 1762923889239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies post-hoc explanations for sparse autoencoder (SAE) features in LLMs and argues that prior explainers overfit to frequent linguistic patterns instead of rarer discourse topics (“frequency bias”). It proposes: (1) explaining features with a de-duplicated vocabulary and a mutual-information (MI) objective that normalizes a word’s affinity across all features; and (2) two runtime steering methods—Amplification and Calibration—that adjust selected feature activations. Experiments (Mistral-7B-Instruct; additional Gemma-2 in the appendix) show higher explanation quality (more discourse-level, less frequency bias) compared to TopAct/N2G/LogitLens and improved jailbreak defense (lower ASR on Salad-Bench), with a small, yet helpful, impact on MT-Bench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The topic-model-based analysis motivates why discourse features are rarer and how this induces frequency bias in SAE explanations.\n\n2. Amplification and Calibration are simple residual-space edits, targeting only a small feature subset at runtime.\n\n3. Steering with safety-related features lowers jailbreak ASR (81.6→72.8) with modest MT-Bench degradation (6.5→6.0) and little/no latency overhead compared to prompting or perturbation defenses."}, "weaknesses": {"value": "1. The contribution of the paper is valid but incremental. The overall contribution is to enhance the SAE-based explanation in response to a frequency bias. The paper identifies the bias, provides two methods to mitigate it, and offers implications for safety. This makes the paper an ensemble of many small or trivial contributions, with each contribution being shallow rather than deep. I would suggest the paper be more focused on 1 or 2 points (e.g., deeper analyses on why frequency bias happens) instead of a series of interconnected small points\n\n2. Explanation quality relies on LLM-as-judge summaries/labels; human or task-grounded evaluations would strengthen claims.\n\n3. Core results use Mistral-7B-Instruct with SAEs trained on ~113M tokens from ~711k prompts; broader model scales, layers, and domains are only lightly explored."}, "questions": {"value": "1. Can the paper emphasize/clarify 1 or 2 contributions that are deep?\n\n2. What is the wall-clock and memory overhead of monitoring |S| features for Amplification/Calibration across long contexts and larger batches?\n\n3. How do MI explanations vary with vocabulary size M, feature count C, and layer choice? Is there any principled guidance beyond the 8th-layer focus?\n\n4. Can you include human-rated coherence of explanations and human-rated utility/safety on a subset to corroborate LLM-as-judge results?\n\n5. Beyond Mistral/Gemma, do results hold for instruction-tuned Llama-3 or command-style models, and for non-safety steering (e.g., style or reasoning control)?\n\n6. Since the paper also mentions safety, how robust are MI explanations/steering under distribution shift, adversarial paraphrases, or multilingual prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YGmP4O0kDV", "forum": "XFcenszBe3", "replyto": "XFcenszBe3", "signatures": ["ICLR.cc/2026/Conference/Submission13195/Reviewer_tD4k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13195/Reviewer_tD4k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942225992, "cdate": 1761942225992, "tmdate": 1762923888553, "mdate": 1762923888553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}