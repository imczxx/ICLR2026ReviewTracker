{"id": "YC9O7OyLFK", "number": 19168, "cdate": 1758294062355, "mdate": 1762936316974, "content": {"title": "Gauge Fiber Bundle Geometry of Transformers", "abstract": "We give a geometry-first account of Transformers with GeLU. On a generic regular set of parameters, the head-wise symmetry group acts freely and properly, so the parameter space fibers over a quotient of functionally distinct models—a clean principal-bundle picture with gauge orbits as fibers and function-changing directions as horizontals. Using the empirical Fisher (Fisher-Rao) metric yields a canonical horizontal distribution and clarifies that the natural gradient is the horizontal Riesz representative of the Euclidean gradient (reducing to orthogonal projection only in a special case). Within this framework, attention behaves like a connection with generically nonzero curvature (path-dependent transport), while the feed-forward block is largely fiber-preserving with a dimension-controlled near-orthogonality to attention. We turn these ideas into practical diagnostics—a least-squares, gauge-aware gradient split and a small-loop holonomy estimator—then report Euclidean-proxy consistency checks aligning with the theory; full Fisher-Rao evaluations are presented as algorithms for future work. Architectural choices such as RoPE appear as principled gauge reductions (e.g., per-head Q/K dimension from d_k² to d_k).", "tldr": "Principal-bundle geometry for GeLU Transformers: maximal gauge symmetry (free & proper), Fisher–Rao connection with natural-gradient as horizontal Riesz; attention has nonzero curvature. Empirical checks use Euclidean proxies.", "keywords": ["Principal bundle geometry", "Gauge symmetry of Transformers", "Fisher–Rao natural gradient", "Attention curvature & holonomy", "RoPE commutant reduction"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9e2978c204ca058a5eab08c0f6a26e197860ad10.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to use a gauge theory framework to better understand transformers. It does this by interpreting the weight to function map within a principal bundle framework. In particular, it decomposes the tangent space (with respect to Fisher–Rao) around a generic point in the weight space $\\Theta_0$ into directions that change the realized function and directions that are simply parameter symmetries that don’t change the function. The idea is that understanding this distinction helps us better understand the loss landscape and ultimately the behavior of transformers. To explore this, the paper first presents some results around weight symmetries in attention heads. Then it performs some calculations of curvature for feed forward and attention blocks. Finally, the paper introduces several computational approaches to approximating the constructions it has introduced and runs small-scale experiments."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "**A refreshing injection of ideas that have been powerful in physics:** Fiber bundles, gauge symmetries, holonomy, and other constructions from differential geometry have proven to be immensely valuable in physics. It is an interesting research direction to explore how these can be used in machine learning, especially as machine learning systems scale and new abstractions are needed. \n\n**The premise of decomposing $T\\Theta_0$ is intriguing:** The direction taken by this paper, which focuses on the geometry of the loss landscape, seems reasonable and the reviewer is interested in seeing this further developed. There have been several approaches that have focused on optimization and parameter symmetries and it would be useful to understand how this paper relates to those [1].\n\n[1] Zhao, Bo, et al. \"Symmetry teleportation for accelerated optimization.\" Advances in neural information processing systems 35 (2022): 16679-16690."}, "weaknesses": {"value": "**Who is the audience for this paper?:** It is this reviewer’s impression that this paper is written for an audience of geometry or physics experts. In the introduction, rather than explaining what gauge theory is (at a high-level for those that haven’t heard of it), the motivation for applying it to transformers, and what machine learning-relevant questions the paper aims to answer, the paper simply launches into formalism, expecting the reader to be comfortable with ‘gauge orbits’, ‘principle connections’, and ‘holonomy’. To this reviewer’s knowledge, this will exclude the vast majority of the ICLR community from even understanding at a high level what the paper is about. While technical depth and sophistication should not be a barrier to acceptance at ICLR, some effort should be made to at least make the point of the paper intelligible to those outside of a narrow subdomain. The introduction is a good place to do this. Then, the body of the paper can dive into much greater depth while asking more of the reader.\n\n**Answering the ‘so-what’ for machine learning:** Very little space is spent discussing how the results of the paper shed light on problems in machine learning beyond being able to decompose $T\\Theta_0$. Use of complicated theoretical machinery should be justified by the fact that it enables researchers to access information that would be hard to obtain via simpler tools. For example, what does the notion of holonomy allow us to say about transformers that we could not say before. For the case of curvature, a cryptic remark is made about it measuring ‘context sensitivity’ but this is otherwise left unexplained. \n\n**A lot of notation is not defined:** To take as an example the Introduction:\n- Line 036: $\\mathcal{Q}$ is not defined. It is a function space presumably, but which one? What is its topology?\n- Most of the operators on Line 049 are left undefined. (Presumably $L$ is the loss but $L$ is also used for the depth in Line 079).\n- Line 094: The paper should state what $g$ is. E.g., $g \\in G$.\n- The property of being *Morse* is central to Theorem 6.2, but it is never defined.\n\n**Corollary 2.6:** Generally, it is not the case that the parameter symmetry group for a model as a whole is equal to the product of symmetry groups of each layer (or block). See for example symmetries that come from the computation graph [2] (to this reviewer’s knowledge, this particular question has not been explored for the transformer architecture). As such, this reviewer believes that Corollary 2.6 needs a proof.\n\n**Nitpicks**\n- Line 194: ‘FFN’ is only defined in the appendix\n\n[2] Lim, Derek, et al. \"Graph metanetworks for processing diverse neural architectures.\" arXiv preprint arXiv:2312.04501 (2023)."}, "questions": {"value": "- Why is the acronym for *feed forward block* FFN?\n- Line 284: “path dependence predicted by curvature”. What is the meaning of path dependence in this setting? What is the reason one would care about it? Earlier it was said that this ‘path dependence’ captures the impact on context, can this be expanded on?\n- How would the proposed diagnostics be used in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eKHi6ECKzq", "forum": "YC9O7OyLFK", "replyto": "YC9O7OyLFK", "signatures": ["ICLR.cc/2026/Conference/Submission19168/Reviewer_iiue"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19168/Reviewer_iiue"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761691345038, "cdate": 1761691345038, "tmdate": 1762931175504, "mdate": 1762931175504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "I literally just learned that a version of this paper was accepted by NeurRep workshop on 12/7/25 and will be part of the proceedings. So I will publish this work at that venue. \n\nThanks for the great feedback from the reviewers"}}, "id": "2VlKLt6QiH", "forum": "YC9O7OyLFK", "replyto": "YC9O7OyLFK", "signatures": ["ICLR.cc/2026/Conference/Submission19168/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19168/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762935109534, "cdate": 1762935109534, "tmdate": 1762935109534, "mdate": 1762935109534, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Caveat: I am not an expert in this area, and have tried my best to understand the results in this paper.\n\nThis paper seeks to exactly parametrize the gauge symmetries in a multi-head attention layer, which are the sets of layer parameters that map to the same function represented by the layer. The main result is the quantification of the maximal gauge group (i.e. the largest set of parameter symmetries). Once these symmetries have been identified, the authors define a partition of the tangent space into a (vertical) subspace tangent to the gauge orbit (ie, where the function remains constant) and a (horizontal) orthogonal one (where orthogonality is defined by the Fisher-Rao metric). This allows defining the natural gradient by simply projecting the parameter gradient to the horizontal subspace. The authors construct an algorithm for performing this decomposition up to arbitrary error. Next, they define an estimator for the holonomy of the connection defined by attention (which is essentially a way to measure how curved the parameter space is). With empirical evidence using the Euclidean metric, they show that their gauge symmetries are indeed preserved, and the natural gradient indeed lies along the horizontal (function-changing) subspace."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper formalizes a framework to think about parameter symmetries and natural gradient descent in neural networks in a general way, which will be useful for other architectures. I enjoyed the functional/coordinate-free approach of this paper. The paper is also precise in its scope of the object it is studying."}, "weaknesses": {"value": "The paper in its current form is extremely terse. I believe a lot of technical machinery is introduced in this paper but it is not clear why such machinery is essential (eg. for proving Theorem 2.1). The amount of technical detail makes it not super clear what the main results in this paper are; is it simply the characterization of the maximal gauge group for multi-head attention, and the construction of the holonomy estimator? The authors should try to motivate the significance of their results more. For example, are there limitations in current optimizers for multi-head attention? Can we possibly construct better optimizers for transformers given their results? This would help situate the results of this paper in the broader literature."}, "questions": {"value": "* One piece of terminology confuses me: why do you refer to $\\mathcal{V}$ as the vertical component, and $\\mathcal{H}$ as the horizontal component? The picture I have is that the gauge orbit represents the surface on which the network function doesn't change; but $\\mathcal{H}$ is normal to this surface and hence 'vertical' to it, making $\\mathcal{V}$ the horizontal component. This is just a matter of terminology but I was confused by it.\n\n\n* Several quantities eg. $d_v, d_k, h$ in Line 45 are only defined later (eg. in Definition 2.1). Several quantities (such as induced gauge displacement eg. $∆_{\\epsilon}(u, v)$) should be defined."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "GEySuNNcMM", "forum": "YC9O7OyLFK", "replyto": "YC9O7OyLFK", "signatures": ["ICLR.cc/2026/Conference/Submission19168/Reviewer_19Br"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19168/Reviewer_19Br"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874040417, "cdate": 1761874040417, "tmdate": 1762931175123, "mdate": 1762931175123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper offers a novel, geometry-first perspective on Transformer models utilizing the GeLU activation function. The authors describe the Transformer's parameter space as a principal gauge fiber bundle. The parameter space is decomposed into Fibers and Horizontals. The fibers are composed of gauge orbits of the head-wise symmetry group, representing the set of functionally equivalent models. The horizontals represent the training directions in the parameter space that actually change the model's function. An empirical Fisher information metric (Fisher-Rao metric) is introduced, providing a canonical horizontal distribution for the parameter space."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This work provides a profound and clear mathematical foundation at a theoretical level for the structure and training dynamics of Transformer models. It aids in the understanding of model redundancy (gauge symmetry) and how to geometrically optimize training paths."}, "weaknesses": {"value": "Many advanced concepts are introduced, making the paper technically dense to general readers, although it is well-written.\nThis paper lacks empirical validation and/or practical impact."}, "questions": {"value": "Why is GeLU important? Can this great framework be generalized to other activations?\n\nHow strong are the assumptions, e.g., Def. 2.1, in the real world?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GmtwD4oJii", "forum": "YC9O7OyLFK", "replyto": "YC9O7OyLFK", "signatures": ["ICLR.cc/2026/Conference/Submission19168/Reviewer_dLCE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19168/Reviewer_dLCE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938239433, "cdate": 1761938239433, "tmdate": 1762931174761, "mdate": 1762931174761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}