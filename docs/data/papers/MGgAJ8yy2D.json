{"id": "MGgAJ8yy2D", "number": 7379, "cdate": 1758018735846, "mdate": 1759897856293, "content": {"title": "Self-Guided Low Light Object Detection Framework", "abstract": "Object detection in low-light environments is inherently challenging due to limited contrast and heavy noise, both of which significantly degrade feature representations. In this paper, we propose a novel self-guided low-light object detection framework that effectively addresses these issues without introducing additional parameters or increasing inference time. Our method incorporates a detachable auxiliary pipeline during training, consisting of an image enhancement module and a denoising module, followed by a Fourier-domain fusion block. This pipeline improves the feature representation of the detector's backbone, enhancing its robustness under low-light conditions. Importantly, at inference time, our method incurs no additional computational cost compared to the baseline detector while achieving substantial performance improvements. Extensive experiments on widely used low-light object detection benchmarks, such as DARK FACE and ExDark, demonstrate that our method achieves state-of-the-art performance. Notably, experiments on the nuImages dataset show that our approach can outperform domain adaptation methods—especially when a large domain gap between source and target domains is inevitable in the real-world applications—highlighting its practical effectiveness. Our code will be made publicly available.", "tldr": "Self-guided object detection framework for low light environment", "keywords": ["Low light object detection", "Self-guided training", "No additional inference cost"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3d774a010503ac1d76a3325c9aa40552ac7980da.pdf", "supplementary_material": "/attachment/e19a688b25c03bc44120dbb6b9c4126c45d7977c.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a Self-Guided Low-light Object Detection Framework with a detachable auxiliary pipeline used only during training, which generates a high-quality supervisory target through an image enhancement module, a denoising module, and Fourier-domain fusion. Experimental results show that this method achieves state-of-the-art performance on multiple benchmarks, with a significant improvement on DARK FACE."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The designed auxiliary pipeline is activated only during training and is completely detached at inference time, which allows the model to achieve substantial performance gains while maintaining the exact same inference speed.\n2. The method leverages the properties of the Fourier transform and self-supervised training strategy, which is novel and sound.\n3. The writing is well-organized and easy to follow.\n4. Experiments show significant improvement on multiple benchmarks."}, "weaknesses": {"value": "1. The paper repeatedly emphasizes \"zero inference overhead\", but completely omits any discussion of the cost during training-stage. The introduction of the auxiliary pipeline ($\\mathcal{E}, \\mathcal{D}, \\mathcal{G}$), retraining of $\\mathcal{E}$ and $\\mathcal{D}$ on the target dataset, and multiple Fourier transforms will clearly increase training complexity and time, but this trade-off is not quantified. \n2. The total loss function uses a hyperparameter $\\lambda$ to balance the main and auxiliary pipeline, but the paper lacks a sensitivity analysis for this crucial hyperparameter."}, "questions": {"value": "In Section 3.3, the authors chose the serial strategy $x^{\\mathcal{E}+\\mathcal{D}} = \\mathcal{D}(x^{\\mathcal{E}})$. Why is the current serial strategy the superior choice? Why not choose a parallel fusion strategy (e.g., fusing $\\mathcal{P}(\\mathcal{E}(x))$ and $\\mathcal{A}(\\mathcal{D}(x))$, where $\\mathcal{D}$ operates on the original input $x$)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v8S0FSQOZy", "forum": "MGgAJ8yy2D", "replyto": "MGgAJ8yy2D", "signatures": ["ICLR.cc/2026/Conference/Submission7379/Reviewer_rmxc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7379/Reviewer_rmxc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804297729, "cdate": 1761804297729, "tmdate": 1762919507319, "mdate": 1762919507319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a self-guided low-light object detection framework to improve detection performance under challenging lighting. The method utilizes image enhancement and denoising, and then the outputs are fused in the Fourier domain. The fusion is used to generate a dense pixel-wise supervision signal encouraging the detector's backbone to learn more robust low-light representations. Experiments are performed on DARK FACE, ExDark, and nuImages datasets, along with ablation and qualitative analyses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Fourier-domain fusion: The method’s technical core by combining the amplitude from a denoised image with the phase from an enhanced image using FFT/iFFT is grounded in signal processing principles and attempts to both preserve structure and suppress noise. \n2. Extensive evaluation: The experiments cover multiple datasets (DARK FACE, ExDark, nuImages) and detectors. Ablation studies isolate the contributions of each module, and Figure 4 offers qualitative insights.\n3. No inference overhead: The method is attractive for applications as it adds no complexity or latency at inference."}, "weaknesses": {"value": "1. While the use of Fourier fusion is motivated by separation of amplitude and phase, the theoretical justifications for this separation, particularly in the context of low-light image statistics and deep feature learning, are not well explained and discussed.\n2. The mathematical details for the Fourier fusion are sometimes ambiguous, e.g., the precise computation of bi-level amplitude-phase combination per-channel, whether normalization occurs before/after fusion, whether channel alignment causes artifacts.\n3. The impact of severe boundary artifacts, non-Gaussian noise, or extreme illumination imbalance on the auxiliary fused target and final detector features is not explored."}, "questions": {"value": "1. Did you analyze how signal-dependent noise, common in low-light shots, is distributed between the amplitude and phase components?\n2. Did you investigate whether the fusion process introduces frequency-domain artifacts, and if so, how were they handled?\n3. How does the proposed method perform when significant boundary artifacts are present in the input images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PeMfptcH7m", "forum": "MGgAJ8yy2D", "replyto": "MGgAJ8yy2D", "signatures": ["ICLR.cc/2026/Conference/Submission7379/Reviewer_pa43"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7379/Reviewer_pa43"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919607063, "cdate": 1761919607063, "tmdate": 1762919506961, "mdate": 1762919506961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel framework to improve object detection in low-light conditions. The core idea is to use a detachable auxiliary pipeline during the training phase to provide a self-guided supervisory signal to the main detector's backbone. This pipeline, which is removed at inference time, consists of self-supervised image enhancement and denoising modules whose outputs are combined in the Fourier domain to create a high-quality target image. The backbone is then trained on a multi-task loss, combining the detection loss with a reconstruction loss based on this generated target. The authors claim this approach improves feature representation for low-light scenes, achieving state-of-the-art results on several benchmarks (DARK FACE, ExDark, nuImages) without adding any computational cost during inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The manuscript is well-represented and easy-to-follow.\n\n+ The auxiliary pipeline (enhancer, denoiser, fusion) is completely detached after training. This means the detector is identical in architecture, parameters, and speed to the baseline model, yet it performs significantly better.\n\n+ The effectiveness of the framework is validated across three different datasets and with multiple detector architectures."}, "weaknesses": {"value": "+ The paper claims that performance stems from the framework design itself. However, in Table 5, using simple modules (Gamma Correction, Gaussian Blur) yields a 70.9 mAP, while advanced modules (SCI, SDAP) are required to reach the top performance of 76.6 mAP. This indicates the framework's effectiveness is tied to the quality of the chosen enhancer and denoiser. It would be better to see more variant of enhancer/denoiser pairs and more discussion for this.\n\n+ The model is trained with a multi-task loss combining sparse detection (L_det) and dense pixel-reconstruction (L_self). How is the sensitivity to the weighting hyperparameter $\\lambda$?\n\n+ Figure 4, Retienxformer -> Retinexformer. The authors should carefully check the manuscript to eliminate typos and grammarical errors."}, "questions": {"value": "Please refer to weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WgQkt3rJts", "forum": "MGgAJ8yy2D", "replyto": "MGgAJ8yy2D", "signatures": ["ICLR.cc/2026/Conference/Submission7379/Reviewer_Y1L8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7379/Reviewer_Y1L8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932472613, "cdate": 1761932472613, "tmdate": 1762919506352, "mdate": 1762919506352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}