{"id": "1m8ODIMkax", "number": 21219, "cdate": 1758315054567, "mdate": 1759896933913, "content": {"title": "ProMediate: A Socio-cognitive framework for evaluating proactive agents in multi-party negotiation", "abstract": "While Large Language Models (LLMs) are increasingly used in agentic frameworks to assist individual users, there is a growing need for agents that can proactively manage complex, multi-party collaboration. Systematic evaluation methods for such proactive agents remain scarce, limiting progress in developing AI that can effectively support multiple people together. Negotiation offers a demanding testbed for this challenge, requiring socio-cognitive intelligence to navigate conflicting interests between multiple participants and multiple topics and build consensus. Here, we present ProMediate, the first framework for evaluating proactive AI mediator agents in complex, multi-topic, multi-party negotiations. ProMediate consists of two core components: (i) a simulation testbed based on realistic negotiation cases and theory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and ProMediate-Hard), with a plug-and-play proactive AI mediator grounded in socio-cognitive mediation theories, capable of flexibly deciding when and how to intervene; and (ii) a socio-cognitive evaluation framework with a new suite of metrics to measure consensus changes, intervention latency, mediator effectiveness, and intelligence. Together, these components establish a systematic framework for assessing the socio-cognitive intelligence of proactive AI agents in multi-party settings. Our results show that a socially intelligent mediator agent outperforms a generic baseline, via faster, better-targeted interventions. In the ProMediate-Hard setting, our social mediator increases consensus change by 3.6 percentage points compared to the generic baseline (10.65% vs 7.01%) while being 77% faster in response (15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous, theory-grounded testbed to advance the development of proactive, socially intelligent agents.", "tldr": "We introduce an evaluation framework to measure the social intelligence of mediator agent in human group decision making.", "keywords": ["Social intelligence; evaluation; agent"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54ee3c259480fcbe3a7f0e970df68f585de98a99.pdf", "supplementary_material": "/attachment/dc2e981d7372c81d373f1e244a6e01a2f952f7d5.zip"}, "replies": [{"content": {"summary": {"value": "The paper claims that there is a growing need for agents that proactively manage multi-party collaborations. The authors propose using mediation in negotiations to study such proactive capabilities in AI agents, as negotiations require socio-cognitive intelligence to navigate conflicts. They present a novel framework called “ProMediate” consisting of two components: (1) a simulation testbed based on negotiation cases from Harvard Law with three difficulty levels and an “AI mediator” agent scaffold, and (2) a socio-cognitive evaluation framework equipped with various metrics. The framework uses an LM-as-a-judge approach extensively to extract metrics of interest and evaluate outcomes. The authors use their framework to empirically evaluate three leading language models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "[**significance**] AI agents are increasingly being integrated into society, leading to a growing set of use cases involving AI-AI and Human-AI collaborative interactions. As mediation is an established and proven approach to improve human-human interactions, exploring its effectiveness in interactions involving AI seems a timely and interesting direction.\n\n\n[**clarity**] Overall, the text is fairly clearly written."}, "weaknesses": {"value": "[**quality**]\n- The introduction introduces an example negotiation scenario between two parties, then states that existing benchmarks do adequately address “such complex socio-cognitive dynamics of multi-party interactions” (l87-89). The cited works, e.g., Abdelnabi et al. and Bianchi et al both involve multi-party negotiations. Furthermore, works like Davidson et al. [1] formulate a negotiation setting that allows for arbitrarily complex multi-player negotiations. The framework outlined in section 2.1 strongly resembles [1]. This implies relevant prior work was both misrepresented and missed entirely.\n- The hard-coded consensus change metric appears dependent on the length of the negotiation sequence and is thus confounded both by the type and number of negotiation topics as well as the number of involved parties. Similar hard-coded choices will likely affect the other metrics.\n- The experimental results presented in Table 1 lack confidence intervals and are based on relatively small sample sizes. Similarly, human evaluation lacks basic metrics like confidence intervals and/or inter-grader agreement. The subsequent discussion thus can be entirely based on noise, especially given how close the reported metrics are to each other.\n- Given the strong reliance on GPT 4.1 as an LM judge throughout this framework, a discussion and supporting experimental results are lacking to validate this reliance. For example, appendix F describes that only 60 samples were evaluated by two students each. This hardly justifies the claims of rigor made in the abstract (l33). For example, are extracted metrics robust under multiple scorings of the same model? Are they consistent with metrics produced by other models?\n\n[**significance**] The paper extends known approaches to simulating LM-based agent negotiations using existing scenarios created by Harvard Law School. The paper continues to use LM-as-a-judge to score questionable metrics. Taken together, this reviewer believes the contribution does not pass the bar required for this conference.\n\n\n[1] Davidson et al., Evaluating language model agency through negotiations, ICLR 2024"}, "questions": {"value": "Q1: Section 2.2, line 149: “we provide each simulated human agent” – could you discuss the motivation and setup to have language model-based agents imitate humans?\n\n\nQ2: Section 3.1, lines 185-186 “Negotiation is a collective process, so individual success rates are insufficient”. This seems to entirely depend on the type of issues being negotiated?\n\n\nQ3. Section 3.1, line 198-200, “we use an LLM [...] mental states” - how are the quality and correctness of these turn-based inferred stances evaluated?\n\n\nQ4. Section 3.2, line 239 “Successfully [...] socio-cognitive intelligence” - this is a strong statement that makes assumptions both on the equivalence of human and AI-based mediators, as well between human and AI-based negotiators. Please provide supporting evidence for these assumptions and claim.\n\n\nQ5. Language-model based agents have the unique property of rolling out multiple trajectories from the same starting condition. As such, why not measure the effectiveness of the proposed interventions by comparing continuations of a fixed starting sequence under intervention and no intervention? This would make any causal claims around mediation interventions significantly more plausible.\n\n\nQ6. lines 432-433, “First, humans may ignore [...] of mediator quality” – What evidence supports this claim?\n\n\nSuggestion:\n- [2] Seems relevant related work worth reading"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yhfb3zbnKK", "forum": "1m8ODIMkax", "replyto": "1m8ODIMkax", "signatures": ["ICLR.cc/2026/Conference/Submission21219/Reviewer_TULT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21219/Reviewer_TULT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841595255, "cdate": 1761841595255, "tmdate": 1762941628580, "mdate": 1762941628580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Evaluating AI agents that can proactively manage complex group collaboration is a major challenge. This paper introduces PROMEDIATE, a framework designed to evaluate proactive AI mediator agents in complex, multi-party negotiations. The framework features a simulation testbed with realistic negotiation scenarios set to different difficulty levels (Easy, Medium, and Hard). It also provides a new socio-cognitive evaluation suite with metrics to measure consensus change, mediator effectiveness, and intelligence. Results show that in the Hard setting, a \"Socially Intelligent Mediator\" increased consensus by 3.6 percentage points more than a \"Generic\" baseline (10.65% vs 7.01%)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A Novel End-to-End Framework for proactive mediators in multi-party, multi-topic talks; useful simulation testbed with plug-and-play agents.\n2. Strong Theoretical Grounding: The framework's difficulty levels (Easy, Medium, Hard) are directly based on established socio-cognitive conflict modes like \"Competing\" and \"Accommodating\" . The evaluation metrics are also theory-grounded, assessing the agent's intelligence by its ability to manage perceptual, emotional, cognitive, and communication breakdowns\n3. The authors ran extensive experiments using complex, realistic negotiation scenarios that took 1-3 hours to simulate . A human evaluation study with 12 volunteers confirmed that the generated conversations were natural (4.18/5 score) and correctly reflected the intended conflict modes\n4. In the PROMEDIATE-Hard setting, the Socially Intelligent Mediator achieved a 3.6 percentage point greater increase in consensus than the Generic baseline (10.65% vs 7.01%). This is a very clear result."}, "weaknesses": {"value": "What worries me / questions to address:\n1. How exactly is the “pairwise agreement score” defined, and does it reflect group decisions? The authors use LLM-as-a-judge to rate pair agreement on five socio-cognitive dimensions in [0,1], then average across pairs and topics to get group consensus. Please (a) show the exact prompt and rubric, (b) report inter-judge sensitivity (try another judge model, i.e. not just GPT-4.1), and (c) justify simple averaging: Simply averaging all pairwise scores is a weak method because it ignores complex group dynamics like coalitions and power, and better methods (like graph-based aggregation) exist.Consider graph-aware aggregation (e.g., consensus as network cohesion) or weighting by topic salience.\n\n2. The authors note MI doesn’t neatly track immediate gains (low correlation). The authors should give more intuition with case studies where high-MI interventions surface hidden disagreements (short-term drop, long-term benefit), and where low-MI still helps (lucky timing). This will strengthen construct validity and practical guidance.\n\n3. Is the soft, time-varying consensus compatible with Pareto analysis?\nThe paper's data reveals clear trade-offs between mediator speed and effectiveness, but it's hard to say whether the analysis is formal. For instance, the results in Table 2 show that the fastest model (Claude-Sonnet-4) has the lowest consensus, while the slowest (O4-mini) has the highest. This is a classic multi-objective (Pareto) trade-off, but it is only presented in a table. To properly analyze this, the authors should first define more robust metrics to summarize their time-varying consensus graphs. The current \"Consensus Change\" metric is a good start, I would suggest them to also include metrics for consensus-over-turns or the time-to-reach-a-consensus-threshold (to measure efficiency). And I would like to see the authors then use these metrics to build multi-objective plots (i.e., Pareto frontiers) that visually map the trade-off between \"quality\" (e.g. Consensus Change) and \"cost\" (e.g., Response Latency). This would formalize the paper's findings and provide a much stronger analysis of the different models' strengths.\n\n4.With multiple goals (raise consensus, stay efficient per topic, minimize latency), I would suggest adding Pareto frontier plots comparing methods across {CC, TLE, RL} and do this per difficulty mode to show trade-offs (Hard vs Easy). This would make the comparison more general than single-number ranks."}, "questions": {"value": "1. In figures, can you label who speaks vs when mediator intervenes and map those to metric changes (helps readers read trajectories like Fig. 2)? Now, it's hard to see what exactly happened."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "68EPGzHl8x", "forum": "1m8ODIMkax", "replyto": "1m8ODIMkax", "signatures": ["ICLR.cc/2026/Conference/Submission21219/Reviewer_Kgdd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21219/Reviewer_Kgdd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878087010, "cdate": 1761878087010, "tmdate": 1762941628149, "mdate": 1762941628149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a simulation testbed for realistic multi-party negotiation, featuring scenarios with varying levels of difficulty (from easy to hard). It also introduces a social-cognitive evaluation framework with consensus-based metrics to track the progression of conversations. Experimental results show that incorporating a proactive social mediator into the multi-party simulation improves consensus change by 3.6% compared to general baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The topic of evaluating proactive agents in social and negotiation settings is both important and timely, making this work a valuable contribution to the discussion.\n\n(2) The paper conducts comprehensive human evaluations on both group consensus and mediator intelligence, providing detailed evaluation procedures and clear descriptions, which are commendable.\n\n(3) The general idea of the paper is well-explained and easy to understand."}, "weaknesses": {"value": "(1) Unfair and missing baseline comparison: The baseline comparison for evaluating the effectiveness of the mediator seems potentially unfair. For the NoAgent baselines, the number of conversation turns should be increased, since the mediator’s involvement effectively extends the dialogue length. The observed improvement might therefore be attributed to having more turns (and tokens) rather than the mediator’s reasoning ability. Moreover, it remains unclear whether the decision process for when to interrupt is handled correctly. An important missing baseline would be one where the mediator participates in every turn to better isolate the impact of timing and intervention frequency for both general agents and social mediators.\n\n(2) Limited scope of proactive agent evaluation: The current evaluation focuses solely on mediation, which provides a narrow view of proactive behavior. A truly proactive agent can pursue its own goals, such as supporting one side during a conflict or strategically guiding discussions. By only examining mediation, the framework overlooks broader dimensions of proactivity—particularly the timing of interventions, which is crucial for assessing proactive intelligence. As a result, the evaluation framework may be too limited to comprehensively capture the full spectrum of proactive agent behavior."}, "questions": {"value": "(1) Number of agents in multi-party conversations: When referring to multi-party conversations, how many agents are typically involved? Is the number of agents greater than three, and can the testbed flexibly adjust this number? It would also be helpful to discuss how the mediator’s role changes as the number of participating agents increases or decreases—does the mediator become more or less influential in facilitating consensus under different group sizes? Providing some intuition or analysis on this would strengthen the paper.\n\n(2) Justification for scenario difficulty levels: The rationale for the difficulty levels of different scenarios is unclear. The “easy” settings appear to focus on accommodating or avoiding behaviors, while the “difficult” ones emphasize competition, with the “medium” level being a mix of both. This categorization seems reasonable but somewhat ad hoc. Is there a theoretical or empirical framework supporting this hierarchy? Clarifying whether the difficulty design is theory-driven or heuristic would make the setup more convincing.\n\n(3) Robustness of consensus evaluation: The robustness of the reported consensus results is questionable, given that only 30 conversations were evaluated. The paper should report standard deviations or confidence intervals to demonstrate the stability of the outcomes. With such a limited number of trials, it isn’t easy to assess whether the improvements are statistically reliable.\n\n(4) Directionality of mediator influence: While it is intuitive that introducing a mediator improves consensus, it would be interesting to explore the opposite direction—can a proactive agent intentionally reduce consensus? Studying how a proactive agent could destabilize an agreement would provide a complementary perspective and could enrich the understanding of proactive behavior beyond mediation alone."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PETX5aFpI0", "forum": "1m8ODIMkax", "replyto": "1m8ODIMkax", "signatures": ["ICLR.cc/2026/Conference/Submission21219/Reviewer_52Vi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21219/Reviewer_52Vi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946327088, "cdate": 1761946327088, "tmdate": 1762941627677, "mdate": 1762941627677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors present ProMediate a framework for evaluating how well AI agents can mediate conversation in negotiations. They construct a theory-driven evaluation criteria and then evaluate a couple agents and models at their ability to moderate conversations. They focus on three research questions (1) performance of different providers / agents and find that the social agent outperforms the basic agent in their evaluation. (2) Impact of difficulty of the scenario (3) how well do their proposed metrics reveal construct validity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper adds a complementary angle to traditional LLM negotiation papers that typically focus on an agent acting as the negotiator.\nThe theory-driven approach seems well grounded."}, "weaknesses": {"value": "Overall, I have a couple critiques for this paper. First, this paper is motivated with the need for a mediator in human negotiations [39-45]. But evaluates a mediator for LLM negotiations. This seems like the original claim isn’t reasonable, since LLMs might negotiate in different ways from humans. I would have liked to see a human study where the mediator is involved in real human negotiations to test for distribution shift. I realize that humans were used to evaluate the quality of the transcripts, but the Cohen’s Kappa is 0.63, which feels quite low. \n\nThe main contribution of the paper feels a little weak. The paper wraps what is essentially in LLM-as-a-judge in a series of theory-inspired rules for negotiating. While this is important, I don’t think it makes for an ICLR paper. \n\nThere is a rich set of literature about evaluating LLM ability to negotiate that feels necessary to cite but is missed (see below). There is also some literature on mediation that’s missing.\n[1] Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues https://aclanthology.org/2024.findings-emnlp.310.pdf \n[2]Evaluating Language Model Agency through Negotiations https://arxiv.org/abs/2401.04536 \n[3] Multi-Agent Collaboration Mechanisms: A Survey of LLMs  https://arxiv.org/abs/2501.06322 \n[4] How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis https://arxiv.org/abs/2402.05863  \n[5] Simulating Dispute Mediation with LLM-Based Agents for Legal Research https://arxiv.org/html/2509.06586v1 \n\nThe results that the social agent has improved performance is not at all surprising since there seems to be some leakage between evaluation criteria and the design of the agent.\n\nFinally, the writing in this paper is problematic — especially in the appendix. \n\nMinor points:\n- type [082] ‘neogiating’ and ‘an deadlock’ -> a deadlock.\n- appendix F.2 is poorly written."}, "questions": {"value": "Do the authors plan to release Github package that allow different agent developers to evaluate their models?\n\nDid the authors leave out the other rich negotiation w/ LLMs literature on purpose?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qrNf1RMpJx", "forum": "1m8ODIMkax", "replyto": "1m8ODIMkax", "signatures": ["ICLR.cc/2026/Conference/Submission21219/Reviewer_ESDb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21219/Reviewer_ESDb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060993781, "cdate": 1762060993781, "tmdate": 1762941627237, "mdate": 1762941627237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Standard deviation and internal confidence of Table 1"}, "comment": {"value": "| **Metric**        | **Accommodating (NoAgent)** | **Accommodating (General)** | **Accommodating (Social)** | **Avoiding (NoAgent)** | **Avoiding (General)** | **Avoiding (Social)** | **None (NoAgent)** | **None (General)** | **None (Social)** | **Competing (NoAgent)** | **Competing (General)** | **Competing (Social)** | \n|--------------------|-----------------------------|--------------------------------|-----------------------------|-------------------------|---------------------------|------------------------|---------------------|------------------------|---------------------|---------------------------|-----------------------------|---------------------------| \n| **Consensus Change** | 0.1874 [0.1205–0.2543] (0.1791) | 0.2013 [0.1343–0.2684] (0.1797) | 0.2259 [0.1620–0.2898] (0.1614) | 0.1749 [0.1079–0.2420] (0.1795) | 0.1431 [0.0774–0.2088] (0.1759) | 0.1325 [0.0693–0.1956] (0.1628) | 0.1136 [0.0508–0.1764] (0.1651) | 0.1093 [0.0555–0.1631] (0.1332) | 0.1139 [0.0421–0.1857] (0.1701) | 0.0683 [0.0093–0.1273] (0.1580) | 0.0701 [0.0148–0.1254] (0.1427) | 0.1065 [0.0342–0.1789] (0.1713) | \n| **Topic level Efficiency** | 0.01051 [0.00661–0.01441] (0.01045) | 0.01185 [0.00751–0.01618] (0.01161) | 0.01304 [0.00845–0.01763] (0.01088) | 0.01167 [0.00792–0.01542] (0.01003) | 0.01044 [0.00678–0.01411] (0.00981) | 0.00533 [0.00122–0.00943] (0.00995) | 0.00543 [0.00246–0.00840] (0.00781) | 0.00443 [0.00237–0.00649] (0.00510) | 0.00936 [0.00431–0.01441] (0.01048) | 0.00514 [0.00146–0.00883] (0.00969) | 0.00233 [0.00015–0.00451] (0.00562) | 0.00620 [0.00133–0.01108] (0.0110) | \n| **Intervene Effectiveness** | N/A | -0.0894 [-0.3204–0.1417] (0.3230) | 0.00817 [0.00155–0.01480] (0.01675) | N/A | -0.0651 [-0.2099–0.0797] (0.2614) | 0.00894 [0.00336–0.01451] (0.01437) | N/A | -0.2987 [-0.5590– -0.0384] (0.4885) | 0.00251 [-0.00172–0.00675] (0.01003) | N/A | -0.0360 [-0.1498–0.0778] (0.2361) | 0.00593 [0.00048–0.01138] (0.0129) | \n| **Response Latency** | N/A | 28.9134 [2.2767–55.5501] (21.4524) | 3.2124 [1.5135–4.9113] (3.5248) | N/A | 23.1455 [13.460–32.831] (14.4172) | 4.8176 [3.2063–6.4288] (3.7260) | N/A | 11.2440 [0.4472–22.0408] (16.9929) | 2.1588 [1.0883–3.2293] (2.2211) | N/A | 14.7662 [8.2682–21.2643] (11.2543) | 3.7098 [2.2311–5.1884] (2.8759) | \n| **Mediator Intelligence** | N/A | 0.2750 [-0.6053–1.1553] (2.3574) | 3.5310 [2.7569–4.3050] (1.9567) | N/A | 1.1042 [0.1248–2.0836] (2.6229) | 2.8894 [1.9111–3.8677] (2.5229) | N/A | 0.0176 [-0.842–0.877] (2.1280) | 1.1695 [0.0612–2.2779] (2.6248) | N/A | 1.7991 [0.7669–2.8313] (2.6619) | 3.2103 [2.2673–4.1532] (2.2331) |"}}, "id": "ItTUEyXZST", "forum": "1m8ODIMkax", "replyto": "1m8ODIMkax", "signatures": ["ICLR.cc/2026/Conference/Submission21219/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21219/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission21219/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763714232912, "cdate": 1763714232912, "tmdate": 1763714232912, "mdate": 1763714232912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}