{"id": "fhG2tZgnRJ", "number": 10136, "cdate": 1758161681745, "mdate": 1759897671786, "content": {"title": "Bridging Quantitative Optimization and Qualitative Reasoning: LLM-Enhanced Neural Architecture Search with Synergistic Weights", "abstract": "Differentiable Neural Architecture Search (NAS) has revolutionized deep learning by automating architecture design, but still faces two interdependent limitations: unreliable connection evaluation based solely on local edge weights leads to suboptimal architecture discretization, while static search spaces prevent the discovery of innovative patterns for optimization. Existing approaches treat these as separable problems, lacking either architectural insight or quantitative grounding. To bridge the gap between quantitative optimization and qualitative reasoning directly, we propose SWNAS, which introduces two key innovations: (1) Synergistic Weights that combine edge and node importance for globally-aware architecture evaluation, overcoming myopic local optimization limitations, and (2) Large Language Model (LLM)-guided dynamic search space evolution that enables intelligent topology expansion beyond fixed constraints. Unlike indirect code generation or heuristic rules, SWNAS directly reason with quantitative structural signals to refine discretization and guide strategic node placement, establishing true large-small model collaboration.\nExtensive experiments demonstrate SWNAS's effectiveness: achieving 2.33\\% error rate on CIFAR-10 and 23.9\\% on ImageNet, while maintaining computational efficiency. Our modular design enables seamless integration into existing DARTS-family methods, consistently improving performance by 0.16-0.19\\% across frameworks. Importantly, SWNAS demonstrates robust generalizability across different search spaces and maintains stable performance across multiple LLMs, demonstrating that genuine quantitative-qualitative integration can systematically advance neural architecture discovery.", "tldr": "", "keywords": ["Neural Architecture search(NAS)", "Differentiable architecture search(DARTS)", "Synergistic weights", "Large language models", "LLM reasoning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6bf46da26f54fd4f7e2098a91dfb817e0053e6b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes to combine node-level importance scores with edge-level operation weights to produce a joint “synergistic” importance measure for neural architecture search (NAS). The synergistic weights are then fed (in a structured format) to a large language model (LLM) via prompt engineering to produce discretization and node-expansion suggestions. The method achieves competitive results on CIFAR-10 and ImageNet."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper contains a reasonable set of experiments across multiple benchmarks, and the results on CIFAR-10 and ImageNet appear promissing.\n2. The authors make an effort to improve reproducibility by including prompts and additional implementation details in the appendix."}, "weaknesses": {"value": "1. The fundamental motivation behind simultaneously considering node and edge importance is not sufficiently explained. The paper would be significantly strengthened by a more in-depth, mechanistic explanation or a motivating example of why this combination leads to better performance beyond the intuitive claim of overcoming \"myopic\" optimization.\n2. The proposed method for learning node importance appears to be a straightforward extension of existing edge-importance learning techniques rather than a genuinely novel methodological contribution. \n3. The methodology section lacks clarity and justification for several key design choices. For example, the paper gives the rule “the edge weight α_e is then the maximum A_k” without explaining why taking the maximum is appropriate.\n4. The application of the LLM is primarily based on prompt engineering, leveraging its inherent reasoning capability. This paradigm is not novel. The authors should provide a direct comparison (qualitative or quantitative) with recent and highly relevant methods such as [1], [2], [3], and [4] to better position their contribution and demonstrate any unique advantages.\n\n[1] RZ-NAS: Enhancing LLM-guided Neural Architecture Search via Reflective Zero-Cost Strategy\n[2] Design Principle Transfer in Neural Architecture Search via Large Language Models\n[3] NADER: Neural Architecture Design via Multi-Agent Collaboration\n[4] Computation-friendly Graph Neural Network Design by Accumulating Knowledge on Large Language Models"}, "questions": {"value": "see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "C7WqlwwWY5", "forum": "fhG2tZgnRJ", "replyto": "fhG2tZgnRJ", "signatures": ["ICLR.cc/2026/Conference/Submission10136/Reviewer_8RV1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10136/Reviewer_8RV1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621386773, "cdate": 1761621386773, "tmdate": 1762921510497, "mdate": 1762921510497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SWNAS (Synergistic Weights Neural Architecture Search), a novel NAS framework that bridges the gap between quantitative optimization and qualitative reasoning. The core idea is to enhance differentiable NAS (specifically, the DARTS family of methods) by integrating LLMs as a reasoning component directly within the search process. The authors propose two main contributions: 1) Synergistic Weights, a metric that combines local edge-level importance with global node-level importance to provide a more holistic signal for architecture evaluation, and 2) LLM-guided Dynamic Search Space Evolution, where an LLM uses these synergistic weights and topological information to perform refined architecture discretization and intelligently expand the search space by adding new nodes. Experiments on image classification tasks and an architectural search space expanded from prior works demonstrate that SWNAS achieves state-of-the-art performance while maintaining computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The core mechanism of integrating quantitative weights and qualitative LLM reasoning is novel. \n\nMethodological Quality: The proposed synergistic weights provide a more robust measure of an edge's importance by considering the significance of its source node, addressing the \"myopic optimization\" issue in DARTS. The two-phase search process and the LLM-guided node adaptation are designed to avoid known issues in differentiable NAS and to improve architectural exploration.\n\nClarity: The authors clearly explain the limitations of existing methods and how the newly proposed components are designed to overcome them. The methods are clearly explained overall, aided by visual diagrams and algorithms."}, "weaknesses": {"value": "Limited Scope of the Search Space: The primary experimental validation seems to be performed on a slight expansion of the original DARTS search space. This space is known to be problematic and not representative of many real-world search problems, nor have the resulting architectures been used outside of NAS results reporting. While the use of NAS-Bench-201 (only reported in an appendix) is a good step, this benchmark is also relatively small and limited to the same dataset domains. \n\nIncomparability of Results: The original DARTS search space is expanded in this work, such as by relaxing the constraint of exactly two input edges per node. However, in your main results Tables 2-3, are all of the related frameworks searching in this expanded search space, or only the original one? In the latter case, this is not a fair comparison. \n\nOpaque Computational Cost Analysis: The reported search costs (e.g., 0.16 GPU-days) are misleading as they do not appear to include the computational cost of the LLM inference. The authors mention the LLM process takes \"10 minutes\", but this is not translated into a standardized metric that can be compared to the GPU-days of other methods. High-end LLMs like Gemini 2.5 Pro require substantial computational resources, and a fair comparison would require an estimation of this cost in terms of equivalent GPU-hours on a reference hardware platform. Without this, the claims of computational efficiency are difficult to verify.\n\nInsufficient Justification for Hyperparameters: The method introduces several new, critical hyperparameters whose values are stated but not thoroughly justified: only the timing of introducing node attention (or NWM, see minor edit below) is empirically explored. Having so many hyperparameters suggests the proposed methods require careful tuning, increasing the true costs of application. Wihtouttheir optimality and sensitivity in question."}, "questions": {"value": "Could the authors provide a more comprehensive breakdown of the computational cost? Specifically, can the LLM inference cost be quantified in a more comparable metric, such as converting all costs to dollar estimates?\n\nThe reliance on dated architectural search spaces and tasks is a notable limitation. Have the authors considered how SWNAS might be applied to more modern search spaces? A very relevant search space could be optimizing the organization of different attention-based and state space block types into hybrid architectures for sequence modeling. \n\nThe prompts used to guide the LLM (Appendix D) are quite complex and specific. How robust is the method to variations in prompt phrasing? Was significant prompt engineering required to achieve the stable performance across different LLMs reported in Table 5?\n\nMinor edits:\n* The NWM seems to be referred to with many different names, such as attention module and NAM. Make sure naming is consistent and clear.\n* Use `\\citep` whenever the citation is not directly part of the sentence structure, for example \"...required by reinforcement learning methods (Zoph & Le, 2017) and...\".\n* The text in Tables 2-3 and most Figures is too small to read accessibly. Please ensure all text is legible when printed on standard paper size (Letter/A4)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jTtxQCxhn5", "forum": "fhG2tZgnRJ", "replyto": "fhG2tZgnRJ", "signatures": ["ICLR.cc/2026/Conference/Submission10136/Reviewer_KEzq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10136/Reviewer_KEzq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642099139, "cdate": 1761642099139, "tmdate": 1762921509672, "mdate": 1762921509672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an approach for integrating weight-based optimization NAS with LLM-driven reasoning for dynamic search space evolution, thus overcoming key limitations of differentiable NAS. Results are presented on CIFAR-10 and ImageNet."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel use of LLMs as high-level reasoning engines that directly interpret architecture node weight data derived from differentiable methods.\n- Performance is improved compared to existing methods."}, "weaknesses": {"value": "- Evaluations on Cifar-10 and ImageNet. While the community often uses this benchmarks, I would have liked to see more diverce benchmarks to assess the effectiveness of the approach and the use of an LLM. \n- The approach fundamentally relies on a large-scale pretrained LLM model. This might limit the applicability of the approach.\n- While performance shows improvement, it is not clear to me if the increase in percentage points is enough to justify the additional complexity of using off-the-shelf large-scale LLMs\n- In my opinion the prompt engineering aspect should not be underestimated and should somehow be included in the calculation of the cost. It is a step that would require human oversight."}, "questions": {"value": "- Does the computation include the running of the LLM? Does it run locally or through the cloud? Are the GPU-days reflective of each scenario?\n- How do known issues of LLMs, hallucinations, and context size performance degradation, affect the performance of the architecture search."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f1aisBbGXJ", "forum": "fhG2tZgnRJ", "replyto": "fhG2tZgnRJ", "signatures": ["ICLR.cc/2026/Conference/Submission10136/Reviewer_ESRo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10136/Reviewer_ESRo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905745390, "cdate": 1761905745390, "tmdate": 1762921509291, "mdate": 1762921509291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SWNAS, a Neural Architecture Search (NAS) framework that integrates quantitative optimization with qualitative reasoning by introducing two key innovations: Synergistic Weights, which combine edge and node importance for globally-aware architecture evaluation, and LLM-guided dynamic search space evolution, which enables adaptive topology refinement beyond fixed constraints. The method is evaluated on CIFAR-10 (2.33% error) and ImageNet (23.9% top-1 error) while maintaining computational efficiency, and its modular design allows seamless integration into existing DARTS-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "++ The experimental evaluation is thorough, with strong results on standard benchmarks (CIFAR-10, ImageNet) and ablation studies validating the contribution of each component. The modular integration into multiple DARTS variants demonstrates robustness and practical applicability.\n\n++ The paper introduces a combination of differentiable NAS with LLM-based reasoning. The concept of Synergistic Weights to address local optimization limitations and the direct use of LLMs for architectural discretization and expansion represent a departure from traditional rule-based or purely optimization-driven NAS approaches."}, "weaknesses": {"value": "-- The method heavily depends on LLMs for discretization and node expansion, but it is unclear whether these models possess the necessary architectural or domain-specific knowledge from pre-training. This may lead to suboptimal or uninterpretable structural choices, as LLMs are not inherently trained for neural architecture design. The reasoning examples in the appendix, while illustrative, do not fully justify the generalization of this approach. Moreover, the interpretability of LLM decisions remains limited, which could hinder adoption in critical applications.\n\n-- While the paper argues that LLMs enable \"qualitative reasoning,\" the prompts and strategies used (e.g., avoiding excessive pooling) are heuristic and could be implemented without LLMs. The added value of LLMs over structured algorithms or expert-defined rules is not convincingly demonstrated, and the risk of LLM-generated artifacts or biases is not addressed."}, "questions": {"value": "1. Experiments are primarily conducted on vision tasks (CIFAR-10, ImageNet) and DARTS-based search spaces. The generalizability to non-vision domains or more complex architectures (e.g., transformers) is not explored. Moreover, can we use this method to refine the architecture of LLM?\n\n2. Can this method adapt to other search spaces? Since the interpretability of LLM decisions remains limited, its reasoning may diverge from optimal architectural principles."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0BGjDzjlOL", "forum": "fhG2tZgnRJ", "replyto": "fhG2tZgnRJ", "signatures": ["ICLR.cc/2026/Conference/Submission10136/Reviewer_bLjn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10136/Reviewer_bLjn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992314081, "cdate": 1761992314081, "tmdate": 1762921508524, "mdate": 1762921508524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}