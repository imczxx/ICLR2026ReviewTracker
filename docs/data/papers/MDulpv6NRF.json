{"id": "MDulpv6NRF", "number": 13728, "cdate": 1758221660133, "mdate": 1759897417035, "content": {"title": "Epipolar Geometry Improves Video Generation Models", "abstract": "Video generation models have progressed tremendously through large latent diffusion transformers trained with rectified flow techniques. Yet these models still struggle with geometric inconsistencies, unstable motion, and visual artifacts that break the illusion of realistic 3D scenes. 3D-consistent video generation could significantly impact numerous downstream applications in generation and reconstruction tasks.We explore how epipolar geometry constraints improve modern video diffusion models. Despite massive training data, these models fail to capture fundamental geometric principles underlying visual content. We align diffusion models using pairwise epipolar geometry constraints via preference-based optimization, directly addressing unstable camera trajectories and geometric artifacts through mathematically principled geometric enforcement.\nOur approach efficiently enforces geometric principles without requiring end-to-end differentiability. Evaluation demonstrates that classical geometric constraints provide more stable optimization signals than modern learned metrics, which produce noisy targets that compromise alignment quality. Training on static scenes with dynamic cameras ensures high-quality measurements while the model generalizes effectively to diverse dynamic content. By bridging data-driven deep learning with classical geometric computer vision, we present a practical method for generating spatially consistent videos without compromising visual quality.", "tldr": "Improving Large Video Generation Models with classical computer vision algorithms through Flow-DPO objective", "keywords": ["diffusion models", "epipolar geometry", "latent video diffusion", "video generation", "3d geometry", "3d computer vision", "direct preference optimization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/396f22a0200717e67a3317a7851d5b8935fd5a2d.pdf", "supplementary_material": "/attachment/d65d0f5c8b2b3534bd32c537409ca97a483c5c01.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes improving video diffusion models by introducing epipolar geometry constraints through preference-based optimization. Although large video generation models trained with rectified flow show impressive visual quality, they often suffer from geometric inconsistency, unstable motion, and artifacts. The method enforces classical geometric principles to stabilize camera motion and reduce distortions—without requiring differentiable rendering."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem addressed is highly important — achieving geometrically consistent video generation remains a major challenge today.\n\n2. The proposed method is simple yet effective.\n\n3. The generated results show clear improvements compared to the baseline.\n\n4. The project website is beautifully designed. If possible, could you let me know which GitHub template it is based on?"}, "weaknesses": {"value": "1.\tHow should we evaluate 3D consistency when videos inevitably contain dynamic content? If the presence of motion leads to degraded 3D consistency and such data are consequently filtered out, what would be the impact of suppressing dynamic content generation?\n\n2.\tCurrently, the method is only compared against a single baseline (the original video generation model). However, I think an important baseline is missing — namely, fine-tuning Wan 2.1 with DL3DV and Real10K using LoRA. My reasoning is that works like DimensionX[1] have already demonstrated that, for tasks such as novel view synthesis from a single image, fine-tuning video diffusion models on such datasets alone can yield geometrically consistent generations. Therefore, in this work, it is difficult to determine whether the improvement in core capability truly comes from the proposed epipolar geometry optimization, or simply because the training and test datasets share the same distribution. I also believe that a visual comparison with this baseline would be important. This is my main concern. If fix this problem, I will raise my score.\n[1] DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion\n\n3. My understanding of the data fine-tuning pipeline is as follows: for each prompt, three videos are sampled, their scores are computed, and two of them are selected for DPO fine-tuning. Please correct me if I am mistaken. How long does the scoring process take — is it fast? Why are only three videos sampled instead of more? Why do videos with the same caption show noticeable differences in 3D quality? Can the DPO process use GT videos as the higher-score samples? If GT videos are used as high-score references, how does that differ from directly fine-tuning with GT data?\n\n4. Regarding evaluation metrics, the baseline videos clearly show  geometric distortion, and artifacts. Given the obvious visual differences, why do the metrics in Table 1 — apart from Human Eval — show such limited improvement? Does this imply that the chosen metrics are not appropriate, that the evaluation method has limitations, or that even the optimized results still fail to achieve satisfactory 3D reconstruction quality? How large is the quantitative gap compared to using GT videos directly for reconstruction?\n\n5. Could you elaborate on the setting of Table 5 — for example, what rewards were used and which strategy led to the observed performance differences? I find this comparison particularly interesting.\n \n6. Regarding the proposed large-scale preference dataset, what do you think is its actual value? Since all the data are generated by video models, and given that Wan 2.1 (1.3B) still shows a significant performance gap from real data, why do we need these imperfect datasets?"}, "questions": {"value": "Please refer to the weeknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nSkm7RT9d8", "forum": "MDulpv6NRF", "replyto": "MDulpv6NRF", "signatures": ["ICLR.cc/2026/Conference/Submission13728/Reviewer_dEzy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13728/Reviewer_dEzy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708172377, "cdate": 1761708172377, "tmdate": 1762924271127, "mdate": 1762924271127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to align pretrained video diffusion models with epipolar geometry via preference-based finetuning to enforce the 3D consistency of the generated video. More specifically, it samples multiple videos per prompt, ranks them using a Sampson epipolar error computed from SIFT/RANSAC correspondences, and applies Flow-DPO with a small temporal-variation penalty to avoid degenerated solutions such as frozen scenes. The experiment results shows that the epipolar geometry based preference alignment leads to better 3D consistency of the generated video."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+The idea of rank-based DPO alignment with classical geometry is interesting. The paper applies Direct Preference Optimization (Flow-DPO) so the model learns from pairwise rankings induced by Sampson epipolar error—no absolute, differentiable reward needed. This leverages DPO’s pairwise nature (relative preferences per prompt) and enables efficient latent-space LoRA finetuning without decoding full videos, which is practically useful for video models. \n\n+LoRA-based Flow-DPO with an explicit static-penalty term (Eq. 3) is easy to reproduce and mitigates the “frozen scene” failure mode."}, "weaknesses": {"value": "-Dynamic objects are not handled. The approach assumes static scenes for the reward; moving objects violate a single fundamental matrix and can corrupt the signal.This is important because many real prompts include independent object motion. \n\n-Dynamics–consistency trade-off. Despite the temporal-variation penalty, dynamic degree drops (Table 2), suggesting a residual tendency to damp motion. The paper frames this as an acceptable trade-off but doesn’t systemically tune λ or add complementary rewards to preserve motion amplitude."}, "questions": {"value": "1. Can the methods be applied on training videos with dynamic objects? For those videos, as long as we can extract the camera poses, and segment the dynamic objects to rule out those objects during reward calculation, the proposed method could be applied. This will greatly increase the breadth of the application.\n\n2. Building the offline preference set (≈54k videos; ~1,980 GPU-hrs) is non-trivial; discussion of scalability, deduping, and potential bias in prompt expansions would help practitioners."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VWJuPDnaed", "forum": "MDulpv6NRF", "replyto": "MDulpv6NRF", "signatures": ["ICLR.cc/2026/Conference/Submission13728/Reviewer_67U8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13728/Reviewer_67U8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894426726, "cdate": 1761894426726, "tmdate": 1762924270805, "mdate": 1762924270805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method for enhancing the 3D consistency of video generation models by using pairwise epipolar geometry constraints and preference-based optimization without requiring end-to-end differentiability, and reducing unstable camera trajectories and geometric artifacts through mathematically principled geometric enforcement. The paper demonstrates that using classical geometric constraints, epipolar geometry, provides simple but stable optimization signals, leading to improved video generation quality and enhanced 3D consistency. The method is evaluated across several metrics and performs well, including 3D consistency, motion stability, and generalization across dynamic scenes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The incorporation of classical epipolar geometry into modern video generation models to improve 3D consistency is interesting.\n- The paper presents a thorough evaluation framework that assesses various aspects of video generation quality, which provides a holistic view of the model’s performance and demonstrates the practical benefits of the proposed method. The results show that the use of epipolar geometry reduces geometric artifacts and improves motion smoothness.\n- The method also generalizes effectively to dynamic content, despite being trained primarily on static scenes with dynamic cameras."}, "weaknesses": {"value": "- The performance of the proposed method is limited by the capabilities of the base model itself. The training data is generated by the base model and then sorted. Therefore, the upper bounds of 3D consistency and motion complexity are relatively fixed, even if the input prompts are more complex. Due to the inherent randomness of the base model, generating three videos per caption, I would question the proportion of high-quality samples. I am also curious why this paper does not use labeled real videos as the ground truth to further improve 3D consistency.\n- Epipolar geometry is typically only applicable to static scenes. It will fail to work with dynamic objects or non-rigid bodies, which are common in real-world scenarios, thus causing problems when scaling up the data."}, "questions": {"value": "- What is the impact of the hyperparameter $\\lambda$ in the temporal variation loss on the final video quality and 3D consistency? A more detailed analysis would be beneficial to understand the trade-offs between motion stability and 3D consistency.\n- Theoretically, epipolar geometry constraints cannot effectively constrain dynamic objects, but the results show that even models trained only on static scenes can generalize to dynamic scenes. I'm curious about the deeper insights into how the model acquires this ability.\n- I noticed that most of the dynamic objects in the demo are rigid bodies with relatively small movements. What if we were dealing with more extreme but common real-world scenarios? For example, large motions, non-rigid deformations, or sudden occlusions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jIJ6ZnzgLr", "forum": "MDulpv6NRF", "replyto": "MDulpv6NRF", "signatures": ["ICLR.cc/2026/Conference/Submission13728/Reviewer_zotX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13728/Reviewer_zotX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922537061, "cdate": 1761922537061, "tmdate": 1762924270357, "mdate": 1762924270357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper improves video diffusion models by incorporating epipolar geometry into the finetuning process. Using Sampson distance to rank candidate generations, the method performs DPO-based preference alignment to encourage 3D-consistent video generation. The authors evaluate extensively across geometric accuracy, video quality, reconstruction, and human preference, showing meaningful improvements in temporal and geometric stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Combining classical geometry with generative models is a good idea and helps pushing video diffusion models toward physically grounded behavior.\n- Extensive evaluation protocol makes the empirical results convincing.\n- The paper is well structured, clear, and easy to follow. Also it clearly represents limitations and broader impact discussion in Appendix F.\n- Preparing a large-scale geometry preference dataset is a significant effort, and publicly releasing it will be useful to the community."}, "weaknesses": {"value": "- Dynamic degree decreases, showing that the method improves stability partly by reducing motion amplitude. There is a trade-off between temporal consistency and keeping lively, dynamic motion.\n- Although better than baseline, some videos still show flickering, blur, and occasional scene drift, especially in challenging or texture-heavy regions.\n- The dataset creation pipeline is computationally heavy and slow (as acknowledged in Appendix F), making the method difficult to scale or replicate."}, "questions": {"value": "- Instead of relying only on pairwise epipolar distance, could multi-frame geometric reasoning (e.g., triangulation error, bundle-adjustment residuals, multi-view consistency) provide stronger or more stable supervision?\n- The pipeline is expensive. Are there surrogate metrics, early stopping heuristics, or active-learning strategies that reduce the number of required generations for ranking?\n- Could you present detailed failure examples and categorize them? Understanding failure modes in depth would be helpful for future work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NcJ7T5fGR5", "forum": "MDulpv6NRF", "replyto": "MDulpv6NRF", "signatures": ["ICLR.cc/2026/Conference/Submission13728/Reviewer_QaX5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13728/Reviewer_QaX5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986946849, "cdate": 1761986946849, "tmdate": 1762924269548, "mdate": 1762924269548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}