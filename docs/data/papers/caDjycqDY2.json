{"id": "caDjycqDY2", "number": 5224, "cdate": 1757869772255, "mdate": 1759897987176, "content": {"title": "LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models", "abstract": "Mixture of Experts (MoE) architectures have recently advanced the scalability and adaptability of large language models (LLMs) for continual multimodal learning. However, efficiently extending these models to accommodate sequential tasks remains challenging. As new tasks arrive, naive model expansion leads to rapid parameter growth, while modifying shared routing components often causes catastrophic forgetting, undermining previously learned knowledge.To address these issues, we propose LLaVA-CMoE, a continual learning framework for LLMs that requires no replay data of previous tasks and ensures both parameter efficiency and robust knowledge retention. Our approach introduces a Probe-Guided Knowledge Extension mechanism, which uses probe experts to dynamically determine when and where new experts should be added, enabling adaptive and minimal parameter expansion tailored to task complexity. Furthermore, we present a Probabilistic Task Locator that assigns each task a dedicated, lightweight router. To handle the practical issue that task labels are unknown during inference, we leverage a VAE-based reconstruction strategy to identify the most suitable router by matching input distributions, allowing automatic and accurate expert allocation. This design mitigates routing conflicts and catastrophic forgetting, enabling robust continual learning without explicit task labels.Extensive experiments on the CoIN benchmark, covering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong continual learning performance with a compact model size, significantly reducing forgetting and parameter overhead compared to prior methods. These results showcase the effectiveness and scalability of our approach for parameter-efficient continual learning in large language models. Our code will be open-sourced soon.", "tldr": "", "keywords": ["continual learning", "mixture of experts", "multimodal large language model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8805265cd79300f334b63b5cb6092370c7798a32.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a new continual learning framework, aimed at economical expert expansion during the lifecycle of the system, and with extra mechanisms to mitigate catastrophic forgetting. This consists of basically discarding rarely activate experts and extending routing to the routers themselves. The authors experiment in an established CL benchmark, and show substantial improvements over baselines. The authors also perform some ablation studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Mostly intuitive and simple (in retrospect) solution. It makes sense to check which experts are covered by previous experts to minimize parameter expansion and enable more transfer learning.\n* Each component has several arguments presented for it, making the work well-motivated.\n\nOverall, the work seems good and a step in the right direction."}, "weaknesses": {"value": "* Lack of baselines: seems like little previous work on CL is used to compare with authors.\n* Why are both new experts and new routers added for each task? This requires further elaboration in the paper. In my opinion, this is the one unintuitive part or the proposed approach. Aren't the experts already meant to accommodate new tasks? I understand the need to mitigate forgetting in the router, but this approach seems to kind of muddle the semantics behind what is meant to be an expert and what is meant to be a router. If PTL finds the task, then why use the \"router\" for the task instead of the expert directly? That might be more of an issue with the semantics of experts and routers (and perhaps the authors should consider coining their own terms), as I understand why the router would forget previous tasks.\n* L153: Are any unseen tasks actually evaluated upon in the paper. This direction is very promising, mentioned in the text, but not evaluated at all. While not necessarily a weakness, it leaves something to be desired from the paper."}, "questions": {"value": "* L232: Sub-router?\n* L80: Needs further explanation. Is it something the authors of that work themselves found, or is it your assertion. If it is the latter, please back it up with data in the paper.\n* Why VAE for routing to routers? Were other, simpler approaches tried? Also, was the possibility of a soft router to the routers entertained (assigning weights to routers rather than argmaxing)?\n* How are the routers trained in previous phases catered to the complete set of experts? Or are newly added experts (after the router's task is done) simply ignored? And to ask explicitly, do all the different routers share experts?\n* Eq2: Were other thresholding techniques considered, or was the alpha at least experimented with?\n* Stemming from that, it may be interesting to see the robustness of your method to the choice of hyperparameters.\n* Where does N_s come from? It is unclear why and how many new experts are added with every new task. Could we instead also modulate the \"size\" of the experts (e.r. lora r)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cOca3DgrOP", "forum": "caDjycqDY2", "replyto": "caDjycqDY2", "signatures": ["ICLR.cc/2026/Conference/Submission5224/Reviewer_rkFQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5224/Reviewer_rkFQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761441984534, "cdate": 1761441984534, "tmdate": 1762917959264, "mdate": 1762917959264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LLaVA-CMoE, a continual fine-tuning framework for MLLMs that combines (i) Probe-Guided Knowledge Extension (PGKE) to decide where to add new LoRA-experts, and (ii) a Probabilistic Task Locator (PTL) that picks which router to use at inference without task IDs.  \nPGKE trains temporary probe experts and a probe router on a 10% subset of the new task, then expands only layers whose probe experts show high “activation frequency”. New experts are initialized from the most-frequently used old expert and old experts are frozen thereafter.  Experiments on CoIN report higher “Last” accuracies and markedly better BWT than LoRA, MoELoRA, EWC, and LwF, with ablations on layer expansion, task order, LoRA rank and PTL features."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The motivation is well illustrated\n- PGKE’s two-stage probe and expand workflow is conceptually straightforward.  \n- PTL is a principled attempt to avoid task IDs at test time by modeling task-wise feature distributions"}, "weaknesses": {"value": "1) During “probe locating” all old experts are frozen while probe experts + probe router are trained on $X_i^{\\text{train}}$ with MoE load-balancing loss , then layers are expanded if probe activation exceeds `mean − α·std` . This setup systematically inflates probe usage(fresh experts tuned to the new task vs. frozen ones; load-balancing further equalizes usage), so high activation is not a reliable signal of missing capacity. Moreover, the threshold favors expansion when activations are already broad. No calibration (e.g., against a no-probe baseline per layer) is provided.\n\n2) The text says “if Ns probe experts exceed the threshold, add Ns experts”, yet Algorithm 1 checks only one frequency per layer (`freq ← prob_freq[layer][-1]`) and appends experts without determining Ns. Later (§4.3) the paper also states the number of experts is treated as a hyperparameter, contradicting the thresholding rule. This point confuses me a lot.\n\n3) New experts copy weights from “the most frequently activated expert during probing” (Sec. 3.2.2). When old experts are then frozen and only new ones are trained, this can lead to cloned experts and limited diversity—the opposite of the stated goal of adding genuinely *new* capacity. No analysis of expert diversity (e.g., cosine overlap/Frobenius distance) is provided. \n\n4) PTL uses a frozen representation misaligned with the MoE that determines routing. PTL trains VAEs on a frozen LLaVA last-token feature $F_{\\text{end}}$ (Sec. 3.2.3), while actual task performance depends on MoE-augmented FFNs and task-specific routers. There is no guarantee that the frozen $F_{\\text{end}}$ preserves the discriminative factors the routers rely on. The large drop when swapping PTL features (Table 7, OCR-VQA: 59.44→20.68) suggests feature/locator mismatch is a real issue. \n\n5) Although raw data aren’t stored, PTL maintains a per-task VAE and per-task router; inference evaluates all VAEs to select a router (“z-score normalize under every primitive in B”). The paper reports no inference-time overhead for PTL (Table 14 gives probe/training time only) and does not evaluate scalability beyond 8 tasks. The O(N) locator cost and memory of router/VAEs challenge the “scalable” claim. \n\n6) Task-locator accuracy is modest and uneven.  The confusion matrix in Fig. 5 (App. A.1) shows PTL localization below 80% on TQA, GQA, VQAv2, with significant confusion among similar tasks, yet the main tables report final task accuracy without decomposing errors due to mis-localization vs routing/expert quality. An oracle-router upper bound (using ground-truth task ID) is missing."}, "questions": {"value": "It is recommended that the authors answer the following questions, and I may adjust the assessment:\n\n1. How many experts are actually added per layer? Please reconcile Sec. 3.2.2 (add Ns experts if Ns probes exceed the threshold) with Algorithm 1 (checks one freq per layer) and clarify how Ns is chosen in practice. \n2. During probing, old experts are frozen while probes + router are trained (and L_aux is applied). How do you ensure probe activation isn’t inflated by this asymmetry and load-balancing? \n3. Please report performance with ground-truth task IDs at test time to separate mis-localization from model quality. \n4. After initializing from the most active old expert, how diverse do new experts become"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2LEjNKkvY0", "forum": "caDjycqDY2", "replyto": "caDjycqDY2", "signatures": ["ICLR.cc/2026/Conference/Submission5224/Reviewer_xVKX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5224/Reviewer_xVKX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750536703, "cdate": 1761750536703, "tmdate": 1762917958838, "mdate": 1762917958838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LLaVA-CMoE, a framework for continual learning within large vision-language models (LLMs) using a Mixture of Experts (MoE) architecture. The framework addresses key challenges in continual learning, such as catastrophic forgetting and parameter growth, by introducing two novel components: Probe-Guided Knowledge Extension (PGKE) and Probabilistic Task Locator (PTL). PGKE adaptively expands the model's experts based on task complexity, while PTL dynamically routes tasks to the appropriate expert, mitigating forgetting and maintaining scalability. The approach is demonstrated to significantly reduce forgetting and model expansion compared to previous methods through experiments on the CoIN benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The paper introduces two novel mechanisms (PGKE and PTL) for continual learning within the context of large vision-language models, contributing new insights to Mixture of Experts architectures.\n\nQuality: The paper's methodology is well-thought-out and supported by comprehensive experiments. The performance of LLaVA-CMoE in retaining knowledge and adapting to new tasks is significantly better than prior methods.\n\nClarity: The explanation of the model's components and the experimental setup is clear, though more details on implementation could have been beneficial in some sections.\n\nSignificance: The paper's contribution to continual learning with MoE is significant, especially in improving scalability and efficiency. It also demonstrates substantial improvements in mitigating forgetting, which is a crucial challenge in real-world AI deployment."}, "weaknesses": {"value": "1 Scalability: While the approach is efficient, the model's scalability in environments with significantly more tasks or greater task complexity remains unclear. Future work should address this limitation.\n\n2 Task Labeling in Inference: The use of task-specific routers during inference might still be challenging in certain dynamic scenarios where task labels are ambiguous or unavailable.\n\n3 Real-World Applicability: While the experimental results are promising, further validation in real-world applications outside the benchmark could strengthen the model's practical value."}, "questions": {"value": "1 How does LLaVA-CMoE perform when applied to tasks that require high levels of domain-specific knowledge (e.g., medical or legal tasks)?\n\n2 Can PGKE be further optimized to handle more diverse task types without adding unnecessary parameters?\n\n3 How do the authors plan to address the potential issue of storage overhead as the number of tasks grows significantly in the long term?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9ZtxaCPdkx", "forum": "caDjycqDY2", "replyto": "caDjycqDY2", "signatures": ["ICLR.cc/2026/Conference/Submission5224/Reviewer_xFvE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5224/Reviewer_xFvE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828630313, "cdate": 1761828630313, "tmdate": 1762917958333, "mdate": 1762917958333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose LLaVA-CMoE, a continual learning framework for multimodal large language models using mixture-of-experts. It introduces Probe-Guided Knowledge Extension (PGKE) for adaptive expert expansion and Probabilistic Task Locator (PTL) for router selection without task labels. Evaluated on the CoIN benchmark of eight VQA tasks, it achieves good retention and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and clearly structured. \n- It introduces a novel framework for continual learning in multimodal large language models. The proposed PGKE and PTL modules are designed and effectively address catastrophic forgetting and parameter efficiency."}, "weaknesses": {"value": "- The efficiency analysis focuses mainly on parameter count. From a computational perspective, the number of activated experts in the MoE layers remains fixed. Whereas, the additional probing and selection procedures introduce extra computation. The paper does not provide a quantitative analysis of computational cost (e.g., in FLOPs), which would be necessary to fully substantiate its efficiency claims.\n- The baselines are somewhat out-of-date. Most baselines are before the year of 2018."}, "questions": {"value": "Could the authors provide quantitative results on computational cost (e.g., FLOPs)? The paper mainly reports parameter counts, but the probing and VAE-based steps seem computationally nontrivial."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dj3gmSE1Rm", "forum": "caDjycqDY2", "replyto": "caDjycqDY2", "signatures": ["ICLR.cc/2026/Conference/Submission5224/Reviewer_fo8y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5224/Reviewer_fo8y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983050698, "cdate": 1761983050698, "tmdate": 1762917957793, "mdate": 1762917957793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}