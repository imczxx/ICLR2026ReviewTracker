{"id": "BvgTTL4xJH", "number": 7346, "cdate": 1758017184418, "mdate": 1759897858484, "content": {"title": "Diffusion with Truncated Blocks: Towards Fast and High-Quality Text Generation using Truncated Block Generation", "abstract": "Diffusion-based Large Language Models (dLLMs) are emerging as a powerful alternative to traditional autoregressive models. These models learn to generate text by iteratively denoising masked sequences. In this work, we identify a critical problem in dLLMs that using token-level noise: the model's attention is wastefully expended on uninformative mask tokens, diluting its focus on meaningful context. We term this phenomenon ``attention dilution\". We further show that it is an artifact of token-level noising, whereas models with sentence-level noise does not have such phenomenon. To resolve this problem, we introduce Truncated Block Generation, a novel sampling algorithm that not only mitigates attention dilution but also enables faster inference and flexible-length sequence generation. Extensive experiments validate our analysis and demonstrate the marked effectiveness of our proposed method in enhancing both the performance and efficiency of dLLMs.", "tldr": "", "keywords": ["Diffusion Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b8fbb4295149554918325d3d186e9d52a7409253.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Truncated Block Diffusion, a decoding strategy for token-level diffusion language models (DLMs) that mitigates the attention dilution problem — where excessive masked tokens reduce focus on informative context. The proposed method dynamically truncates and resumes generation in smaller blocks, allowing the model to maintain attention on semantically meaningful regions while supporting flexible-length decoding. The authors present theoretical analysis of how token-level noise leads to attention dilution and propose truncation to maintain contextual density. Experiments on code (HumanEval, MBPP) and math reasoning tasks (GSM8K, MATH) show improved accuracy and efficiency over baselines such as Dream and LLaDA, with results consistent across sequence lengths. While the technical design is well motivated and validated, the paper omits detailed analysis of failure cases or ablations regarding truncation sensitivity. The writing also tends to overemphasize strengths without clarifying limitations in robustness, calibration dependency, or computational trade-offs. Claims of compatibility with other decoding accelerations (e.g., Fast-dLLM) are promising but should be stated more cautiously since direct integration experiments are not reported."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies and analyzes a concrete limitation of token-level diffusion (attention dilution), offering both theoretical reasoning and empirical validation.\n2. The proposed Truncated Block Generation is conceptually simple, implementation-friendly, and does not require retraining, making it broadly applicable to existing DLMs.\n3. Extensive experiments on reasoning and code tasks demonstrate consistent performance and speed improvements under fixed compute budgets.\n4. The theoretical justification for truncation and the visual attention analysis are clear and insightful, helping to connect the intuition of “context density” with measured decoding quality."}, "weaknesses": {"value": "1. The approach is primarily validated on structured domains (code and math) using Dream and LLaDA; generalization to open-ended or natural language tasks remains unclear.\n2. The model’s reliance on token-level confidence for truncation may lead to instability under poorly calibrated confidence distributions.\n3. The paper lacks qualitative or failure-case analysis — e.g., what happens when truncation disrupts a coherent semantic span, leading to invalid continuations.\n4. No adaptive or learnable mechanism is explored for determining truncation boundaries, which could improve robustness but is omitted.\n5. Broader comparisons to models with weaker calibration or longer contexts are missing, leaving uncertainty about the scalability and universality of the method.\n6. The paper provides qualitative claims of acceleration but lacks comprehensive tables comparing latency and total compute under matched budgets."}, "questions": {"value": "* **Baseline coverage and robustness under weaker confidence calibration:** Since the truncation relies on confidence-based masking, how does the approach perform on DLMs with less stable confidence scores? Would the authors include results on other diffusion LMs to verify robustness?\n\n* **Behavior under flat or uniform confidence distributions:** When token confidences are nearly uniform, how does the model decide truncation or continuation? Can the authors provide visualization or analysis showing this failure mode?\n\n\n* **Failure case analysis and adaptive truncation:** In some generations, errors may arise when truncation happens across semantically connected regions, breaking coherence. Have the authors analyzed such cases? Could an *adaptive truncation policy*—where the model learns when to truncate—alleviate this issue?\n\n* **Compute and latency analysis:** Could the authors provide explicit comparisons of generation speed, latency, and total step counts versus fixed-length or block-decoding baselines to better quantify the claimed acceleration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KTF59ZsAcs", "forum": "BvgTTL4xJH", "replyto": "BvgTTL4xJH", "signatures": ["ICLR.cc/2026/Conference/Submission7346/Reviewer_Memz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7346/Reviewer_Memz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792792920, "cdate": 1761792792920, "tmdate": 1762919473247, "mdate": 1762919473247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Truncated Block Generation combats attention dilution in diffusion LMs with token-level noise by decoding in short blocks and truncating before rollover, improving code/math accuracy and throughput under compute parity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Formalizes “attention dilution” under token-level noise with definitions/claims plus intuitive visual evidence.\n2.  Training-free; integrates with existing diffusion LMs without architecture changes.\n3. Truncation removes boundary cues that trigger early <eos>, reducing “half-baked” spans.\n4. Consistent gains on HumanEval/MBPP (and math sets) alongside higher tokens/sec.\n5. Stacks with Fast-dllm; retains more accuracy than using acceleration alone.\n6. Threshold-gated continuation makes length flexible instead of being bound to a single fixed [MASK] tail."}, "weaknesses": {"value": "1.  Benefits concentrate on token-level noise (e.g., Dream); for sequence-level noise (e.g., LLaDA) the lift is muted—generalizability is constrained.\n2. Block length, truncation length, and threshold γ all matter; best settings vary by task, raising tuning costs.\n3. Focuses on code/math; lacks long-form generation, dialogue coherence, factuality, or human evals.\n4. Compared to fixed-length, naive block, and Fast-dllm, but fewer head-to-heads with semi-autoregressive / multi-step remasking and other modern decoding strategies.\n5. The “information value” modeling is stylized; real multi-layer attention/copy dynamics may deviate, so theory–practice gaps can appear on new models/data."}, "questions": {"value": "1. Systematically sweep block length, truncation length, and the threshold γ; report mean ± σ across multiple random seeds and show performance/latency trade-off curves.\n2. Replace raw max-softmax triggers with entropy/energy-based or calibrated-confidence signals; auto-tune γ and truncation length online to reduce hand-tuning and improve robustness OOD.\n3. Evaluate beyond code/math: include long-context reasoning, dialogue coherence, and factual QA; add broader code sets (e.g., DS-1000) plus long-form generation tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6CI47vZdwm", "forum": "BvgTTL4xJH", "replyto": "BvgTTL4xJH", "signatures": ["ICLR.cc/2026/Conference/Submission7346/Reviewer_q85c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7346/Reviewer_q85c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890020629, "cdate": 1761890020629, "tmdate": 1762919472198, "mdate": 1762919472198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Truncated Block Generation (TBG), a decoding method for diffusion-based LLMs trained with token-level noise (e.g., Dream).\nIt argues that such models suffer from attention dilution-where long tails of uninformative mask tokens weaken attention focus.\nTBG generates text in smaller masked blocks and truncates partial outputs iteratively, claiming to improve efficiency and text quality for long sequences.\n\nHowever, the approach closely overlaps with semi-autoregressive or blockwise diffusion decoding already established in SSD-LM and Block Diffusion (BD3-LMs). The only new element-truncation-is a heuristic, not a fundamental algorithmic advance. Baselines are incomplete, and empirical support for both speed and “attention dilution” is weak."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Identifies a plausible failure mode (attention dilution) in token-level noise training.\nSimple heuristic (TBG) that is easy to implement on top of existing diffusion LLMs. However, the approach is not properly compared agianst prior baselines!\nEmpirical results show modest improvements on some reasoning and coding benchmarks, still not compared to prior methods!"}, "weaknesses": {"value": "* The paper lack novelty and fails to compare against prior blockwise diffusion baselines such as SSD-LM and BD3-LMs (https://arxiv.org/pdf/2503.09573) in comparisons, which severely undermines the claim of novelty and contribution.\n\nThe proposed Truncated Block Generation (TBG) is conceptually almost identical to SSD-LM (Han et al., 2023) and Block Diffusion / BD3-LMs (Arriola et al., 2025), which already generate text in sequential diffusion blocks conditioned on previous outputs. However, such strong \nBoth prior works support variable-length text generation, KV caching, and efficient blockwise denoising - the same core benefits claimed here. The only new element is the truncation heuristic, where the generated block is shortened before continuation. However, Looking into appendix this approach comes with heavily tuning the hyper-parameters which questions the practicality of this approach in realworld setting. \n\n\nThe main baseline is “Dream with full-length mask decoding,” which is known to perform poorly on long outputs and serves as a weak strawman.\n\n\n\n* Unconvincing theoretical framing (“attention dilution”)\nThe “attention dilution” argument - that many uninformative MASK tokens from token-level noise distract the attention distribution - is intuitively reasonable but not experimentally validated.\n\n\n* The analysis merely restates the known property that softmax weights are normalized over all keys; it does not establish a causal connection between dilution and degraded text quality.\n\nThe authors argue that truncating uninformative MASK positions (or reducing context length)  provide improvements.\nSince sequence-level noise models (like LLaDA) are unaffected by this problem, a simpler alternative would be to adjust training rather than add decoding heuristics.\n\n* Strong baselines such as SSD-LM, Block Diffusion (BD3-LMs), and LLaDA with standard decoding are not compared, even though they directly address the same limitations. without proper comparison, it is unclear if the approach bring any benefits. It remains unclear whether TBG helps other diffusion LMs, semi-autoregressive LMs, or sequence-level-noise systems that already avoid dilution.\n\n\nNo comparison with autoregressive models on runtime or accuracy, despite the claim of “fast and high-quality generation.”\n\n“Faster inference” is repeatedly claimed but not substantiated: the paper reports no wall-clock time, throughput (tokens/sec), FLOPs, or NFEs (number of function evaluations).\n\nTBG introduces multiple iterative decoding rounds (generate -> truncate -> repeat), each requiring diffusion denoising, which likely increases latency. this is good to clarify this in the paper.\n\n\n* High hyperparameter sensitivity and tuning overhead\n\nTBG depends on several heuristic hyperparameters (block length, truncation length, threshold), which are tuned per dataset . Looking into appendix it looks like they are heavily tuned. this would question practicality of this method.  In contrast, block diffusion and SSD-LM decoding work robustly across datasets without such per-task adjustments. This extensive tuning contradicts the claim of being a “simple, fast decoding algorithm.”\n\nThere is no direct causal experiment showing that truncation specifically restores attention concentration or improves generation quality for the same noise schedule."}, "questions": {"value": "How is TBG different from SSD-LM or Block Diffusion decoding beyond the truncation heuristic?\n\nPaper needs to compare with SSD-LM and BD3-LM as prior baselines. Could you provide the comparisons?\n\nIf attention dilution only arises in token-level noise (Dream), why not just adopt sequence-level noise (LLaDA)?\n\ncould you provide wall-clock or NFE comparisons supporting “faster inference”?\n\nHow sensitive is TBG to hyperparameter tuning (block length, truncation, threshold)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "svFHgb769S", "forum": "BvgTTL4xJH", "replyto": "BvgTTL4xJH", "signatures": ["ICLR.cc/2026/Conference/Submission7346/Reviewer_EtJq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7346/Reviewer_EtJq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020981906, "cdate": 1762020981906, "tmdate": 1762919470191, "mdate": 1762919470191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies and analyzes an “attention dilution” problem that arises in masked/diffusion LLMs under token-level noising, and proposes a simple yet effective sampling strategy called Truncated Block Generation to mitigate this issue—thereby improving generation quality and speeding up inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It pinpoints and rigorously analyzes a previously overlooked “attention dilution” problem in token-level noising dLLMs, supporting the claim with both theoretical arguments and attention visualizations.\n\n2. It proposes a simple, practical sampling algorithm—Truncated Block Generation—that directly mitigates the dilution issue by generating in short blocks and truncating to keep context informative, making the method easy to integrate into existing dLLM pipelines.\n\n3. The approach is empirically validated: experiments and ablations show consistent quality improvements and inference speedups on code and math benchmarks (e.g., MBPP, HumanEval, GSM8K), and the paper demonstrates robustness to key hyperparameters like truncation length and threshold."}, "weaknesses": {"value": "1. There needs to be some baseline comparison, such as adding comparisons using methods like remasking during the inference stage. Currently, there are almost no baselines.\n\n2. I want to see how this method performs on some general tasks such as MMLU and GPQA."}, "questions": {"value": "1. How does truncation interact with long-context reasoning or compositional generation—does repeatedly truncating and regenerating blocks risk losing global coherence or factual consistency over long outputs?\n\n2. I observed that the longer the generated length, the better the performance. What if the generated length is 8k or even 16k? What are the advantages of this method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I3uDlJ5cmO", "forum": "BvgTTL4xJH", "replyto": "BvgTTL4xJH", "signatures": ["ICLR.cc/2026/Conference/Submission7346/Reviewer_niRg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7346/Reviewer_niRg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156222533, "cdate": 1762156222533, "tmdate": 1762919469139, "mdate": 1762919469139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}