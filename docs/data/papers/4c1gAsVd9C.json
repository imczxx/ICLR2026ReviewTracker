{"id": "4c1gAsVd9C", "number": 1588, "cdate": 1756894879233, "mdate": 1759898199521, "content": {"title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning", "abstract": "Unified multimodal models integrate the reasoning capacity of large language models with both image understanding and generation, showing great promise for advanced multimodal intelligence. However, the community still lacks a rigorous reasoning-centric benchmark to systematically evaluate the alignment between understanding and generation, and their generalization potential in complex visual tasks.\nTo this end, we introduce \\textbf{GIR-Bench}, a comprehensive benchmark that evaluates unified models across three complementary perspectives. \nFirstly, we explore whether models can consistently leverage the same knowledge for both understanding and generation (GIR-Bench-Uni). \nSecondly, we investigate whether models can perform reasoning-centric text-to-image generation that requires applying logical constraints and implicit knowledge to generate faithful visual content (GIR-Bench-T2I).\nThirdly, we evaluate whether models can handle multi-step reasoning in editing (GIR-Bench-Edit). \nFor each subset, we carefully design different task-specific evaluation pipelines tailored for each task. This enables fine-grained and interpretable evaluation while mitigating biases from the prevalent MLLM-as-a-Judge paradigm.\nExtensive ablations over various unified models and generation-only systems have shown that: Although unified models are more capable of reasoning-driven visual tasks, they still exhibit a persistent gap between understanding and generation. The data and code for GIR-Bench are available at \n\\url{https://anonymous.4open.science/r/GIR-Bench-7E40}.", "tldr": "", "keywords": ["Evaluation", "Unified Multimodal Model", "Visual Generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f29e314fd15af62c218cdb7acebed471a0ff99b.pdf", "supplementary_material": "/attachment/854601a127b1a3973499268faab1e7d12a98ac0d.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces GIR-Bench, a comprehensive benchmark designed to evaluate reasoning-driven image generation and editing in unified multimodal models—systems that combine visual understanding, reasoning, and generation in a single architecture.\n\nInstead of relying on the subjective and biased “MLLM-as-a-Judge” paradigm, the authors design objective, fine-grained metrics (e.g., object counting, IoU, FID, word-level substring matching) tailored to each task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper demonstrates high originality not by inventing a new model, but by redefining how we evaluate unified multimodal systems. It identifies a critical yet overlooked gap: the misalignment between reasoning (understanding) and visual generation/editing. While prior benchmarks focus on surface-level text-image alignment or isolated reasoning tasks, GIR-Bench is the first to systematically probe reasoning-to-generation fidelity across three carefully designed axes—knowledge consistency, constrained generation, and reasoning-driven editing. The formulation of tasks like implicit-entity generation, logic-constrained layout, and puzzle-based editing represents a creative synthesis of symbolic reasoning, visual semantics, and generative modeling."}, "weaknesses": {"value": "GIR-Bench focuses on deductive and constraint-based reasoning (e.g., arithmetic, spatial ordering, Sudoku), but omits other critical reasoning modalities such as causal, temporal, counterfactual, or commonsense physical reasoning. For instance, generating an image of “a glass falling off a table” requires modeling gravity and motion—reasoning not captured by current tasks.\n\nThe word-level continuous substring score (swc) allows extra text but does not penalize hallucinated or semantically incorrect text that happens to contain the target substring (e.g., generating “Just do it now!” for “Just do it” is acceptable, but “Just do it wrong” would also score fully). More critically, the metric ignores position, font, size, or contextual integration—all crucial for faithful rendering.\n\nGIR-Bench-Edit uses synthetic or highly constrained tasks (jigsaw, Sudoku, green-mask segmentation). These do not reflect practical editing needs like object insertion/removal, attribute modification (“make the car red”), or style transfer guided by reasoning (“render this building in Art Deco style based on its era”).\n\nWhile the paper notes the “reasoning-generation gap,” it does not systematically analyze where the disconnect occurs: Is it in the multimodal alignment layer? The diffusion/autoregressive decoder? The instruction-following interface? Without architectural or ablation insights, the benchmark remains diagnostic but not prescriptive.\n\nAll metrics are automated. While this avoids MLLM-as-a-Judge bias, it misses perceptual or contextual fidelity that humans would notice (e.g., a correctly counted but anatomically distorted animal)."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CbaXWYaPDU", "forum": "4c1gAsVd9C", "replyto": "4c1gAsVd9C", "signatures": ["ICLR.cc/2026/Conference/Submission1588/Reviewer_6bCn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1588/Reviewer_6bCn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760703749191, "cdate": 1760703749191, "tmdate": 1762915827614, "mdate": 1762915827614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GIR-Bench is a reasoning-centric benchmark for unified multimodal models. It tests (1) whether models use the same knowledge consistently across understanding vs. generation, (2) whether they can do reasoning-heavy text-to-image generation, and (3) whether they can edit images via multi-step reasoning. Findings: unified models beat pure generators on reasoning tasks, but there’s a persistent gap between reasoning/understanding and actually rendering the correct visual outputs, especially under implicit prompts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* It introduces three innovative and well-differentiated tracks whose tasks exhibit minimal overlap in the model capabilities they test, ensuring a comprehensive evaluation of reasoning, generation, and editing skills. \n* All selected tasks are designed to be assessed through rule-based evaluation, and the entire framework follows interpretable, deterministic criteria, which greatly enhance the reliability and reproducibility\nof the results.\n* The benchmark covers a wide spectrum of models — both open-source and closed-source — encompassing image generation, image editing, and unified vision-language models, thereby providing a broad and fair comparison across model types."}, "weaknesses": {"value": "* Each subset contains only about 300 entities, which is limited to yield statistically solid or generalizable results. While the authors claim that “overall, unified models outperform generation-only systems on reasoning-centric generation tasks, indicating that joint training across understanding and generation yields tangible benefits,” the paper does not provide detailed data analysis to support this conclusion. \n* The experimental results are relatively sparse, and the accompanying analyses lack the depth and precision needed to validate the authors' claims firmly."}, "questions": {"value": "1. The new swc metric allows extra content without penalty, which makes sense for implicit prompts. However, how does it handle variations like typos, capitalization differences, or partial matches (e.g., \"Just do\" instead of \"Just do it\")? Have the authors tested its robustness against human annotations?\n2. The authors argue that the proposed pipelines avoid biases of the MLLM-as-a-Judge paradigm, but have the authors compared metrics (e.g., swc for text rendering, IoU for reasoning perception) against MLLM-based scores on a subsample? If so, what was the agreement level, and in cases of discrepancy, which better aligned with human preferences?\n3. Are the selected domains (zoology, botany, geography) representative enough to evaluate general reasoning transfer across modalities? The chosen domains seem to be biased towards factual knowledge about nature instead of reasoning chains that apply across general contexts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oZvubmsvbl", "forum": "4c1gAsVd9C", "replyto": "4c1gAsVd9C", "signatures": ["ICLR.cc/2026/Conference/Submission1588/Reviewer_RiLt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1588/Reviewer_RiLt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731381196, "cdate": 1761731381196, "tmdate": 1762915827488, "mdate": 1762915827488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GIR-Bench, a novel and comprehensive benchmark designed to systematically evaluate the reasoning-driven image generation and editing capabilities of unified multimodal models. The authors argue that existing benchmarks fall short in rigorously assessing the alignment between reasoning and generation, often relying on biased MLLM-as-a-Judge paradigms. GIR-Bench addresses this by evaluating models across three complementary perspectives:\n1. GIR-Bench-Uni (Knowledge-to-Generation): Assesses whether models can consistently leverage the same knowledge for both understanding and generating real-world entities from implicit descriptions.\n2. GIR-Bench-T2I (Reasoning-to-Generation): Investigates reasoning-centric text-to-image generation, focusing on numerical reasoning, spatial layout, and text rendering tasks that require logical constraints and implicit knowledge.\n3. GIR-Bench-Edit (Reasoning-to-Editing): Evaluates multi-step reasoning in image editing through visual puzzles, visual logic (Sudoku), and reasoning perception (segmentation based on implicit descriptions).\n\nA key methodological contribution is the design of task-specific evaluation pipelines for each subset, moving beyond the MLLM-as-a-Judge paradigm to provide fine-grained and interpretable assessments. The benchmark was used to evaluate 21 state-of-the-art models, revealing a persistent gap between understanding and generation capabilities in unified models, despite their overall superiority to generation-only systems in reasoning-driven visual tasks. The data and code are made publicly available."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies and rigorously tackles the crucial problem of evaluating the alignment between reasoning and generation in unified multimodal models, a gap not adequately addressed by previous benchmarks. GIR-Bench covers a broad spectrum of reasoning capabilities, from knowledge recall and simple arithmetic to complex spatial arrangement, text rendering from implicit cues, and multi-step image editing.\n2. The shift from subjective MLLM-as-a-Judge evaluations to concrete, task-specific metrics (e.g., object detection for counts/spatial layout, OCR for text, FID for puzzles, IoU for segmentation) is a major strength, leading to more reliable and interpretable results. The evaluation of 21 diverse state-of-the-art models provides a comprehensive overview of the current landscape and validates the benchmark's effectiveness in revealing model limitations.\n3. The paper clearly demonstrates the \"understanding-generation gap\" and the difficulty models face in translating inferred knowledge/reasoning into faithful visual outputs, even when the reasoning process itself is correct (as shown with BAGEL w/ CoT analysis)."}, "weaknesses": {"value": "1. While the paper effectively identifies the reasoning-generation gap, it offers less in terms of a deeper mechanistic analysis of why this gap persists. Is it primarily an architectural limitation, a data scaling issue, a fundamental challenge in aligning discrete symbolic reasoning with continuous pixel generation, or an optimization problem? A more in-depth discussion or hypothesis generation regarding the root causes could provide even more actionable insights for model developers.\n2. The evaluation pipelines rely on external object detection (InternVL3.5-38B) and text recognition (PPOCR v5) models. While these are strong models, their performance and potential failure modes on diverse, potentially synthetic or \"hallucinated\" images generated by the tested models are not thoroughly discussed. The robustness of these evaluators to the specific characteristics of generated content could impact the final scores.\n3. The top-performing models (GPT-Image-1, Gemini-2.5-Flash-Image) are proprietary. While this is an accurate reflection of the current SOTA, it means the most successful approaches are not open for inspection or direct improvement by the research community, which limits the actionable insights for open-source development.\n4. While comprehensive within its defined scope, the benchmark primarily focuses on specific types of reasoning (numerical, spatial, textual, knowledge recall). Other forms of reasoning, such as causal reasoning, temporal reasoning, or abstract concept manipulation, are not explicitly covered. This is more of a future expansion opportunity than a weakness of the current work."}, "questions": {"value": "1. Could the authors elaborate further on their hypotheses regarding the underlying causes of the reasoning-generation misalignment? For instance, do they believe it's more of a representational issue (how reasoning is encoded), an architectural bottleneck (how reasoning modules interact with generation modules), or a training data challenge (lack of sufficiently diverse and complex reasoning-generation pairs)?\n2. Given that the evaluation relies on external object detection and text recognition models, did the authors perform any analysis or sanity checks on the performance of these tools when applied to the generated images, which might differ significantly from their typical training data? Are there cases where the evaluators themselves might misinterpret generated content, leading to skewed scores?\n3. For the text rendering task, the \"word-level continuous substring score\" is a custom metric. Could a few more illustrative examples be provided, perhaps in the appendix, to clearly demonstrate its calculation, especially for edge cases where the generated text might have extra words, missing words, or partial matches?\n4. While Table 1 and 2 present results, a more explicit \"Ablation Study\" section could be beneficial. For example, the comparison of BAGEL with and without CoT is a good start. Are there other controlled experiments that could shed light on specific design choices or components of unified models?\n5. Are there plans to extend GIR-Bench to include more complex forms of reasoning (e.g., counterfactual, moral, scientific hypothesis generation) or to incorporate other modalities beyond visual generation (e.g., video, 3D)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7u3y1qdtfL", "forum": "4c1gAsVd9C", "replyto": "4c1gAsVd9C", "signatures": ["ICLR.cc/2026/Conference/Submission1588/Reviewer_R2PX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1588/Reviewer_R2PX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926667817, "cdate": 1761926667817, "tmdate": 1762915827207, "mdate": 1762915827207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GIR-Bench, a novel benchmark designed to evaluate the reasoning capabilities of unified multimodal models in image generation and editing tasks. It provides a suite of challenging tasks that require models to combine visual understanding with logical reasoning, spanning three components: aligning understanding vs generation for the same concept, reasoning-centric text-to-image generation, and reasoning-driven image editing. The authors also develop specialized evaluation metrics for each task to obtain fine-grained, interpretable assessments instead of relying on subjective large-model judgments. Experiments on 21 state-of-the-art models demonstrate the benchmark’s effectiveness, revealing a significant gap between what current models can understand and what they can faithfully generate under complex reasoning constraints."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper fills an important void by focusing on reasoning-driven image generation, an aspect where prior image generation benchmarks were limited or shallow. The benchmark is thoroughly validated by testing it on 21 state-of-the-art models, including both unified multimodal models and traditional image generation models.\n2. A notable strength is the introduction of task-specific evaluation pipelines, replacing the common practice of using a multimodal LLM as a subjective judge."}, "weaknesses": {"value": "1. Some tasks in the benchmark, though creative, are somewhat niche or artificial (e.g., asking an image generation model to solve a Sudoku puzzle or reassemble a jigsaw). These scenarios are not typical use cases for image generation models; thus, poor performance might reflect the models’ lack of exposure to such tasks rather than a general reasoning failure, raising questions about the real-world relevance of these evaluations.\n2. The paper could offer more in-depth analysis of the observed performance gaps and the reasoning task scope. It identifies a gap between understanding and generation, but does not deeply explore why certain models fail (e.g., is it due to knowledge retrieval limits, reasoning logic, or image-generation fidelity?) and stops short of suggesting how future models might overcome these shortcomings."}, "questions": {"value": "1. Could the authors clarify how the ground-truth outputs for the complex editing tasks (e.g., solving a Sudoku puzzle or rearranging a jigsaw image) were obtained? Understanding whether these reference outputs were generated by an algorithm, manually created, or produced by another model would help in assessing the objectivity and fairness of the evaluation for those tasks.\n2. How robust are the automated evaluation metrics (such as object detection for counting and OCR for text) to variations in the model outputs? It would be useful to know if the authors observed any cases where these tools misjudged a correct output, and what steps were taken to mitigate potential evaluation errors in such situations.\n3. Why were the particular reasoning tasks and domains chosen for inclusion in GIR-Bench, and do the authors believe these cover the most critical reasoning skills needed for image generation? A discussion of the task selection rationale — and whether there are plans to extend the benchmark with other reasoning scenarios (e.g., causal reasoning or commonsense reasoning in images) — would help clarify the benchmark’s comprehensiveness and how it might evolve.\n4. The results indicate a significant gap between models’ understanding capabilities and their generation performance under reasoning constraints; have the authors investigated the root causes of this gap or potential ways to reduce it? Including more analysis or hypotheses (for example, whether the limitation stems from model architecture, training data, or reasoning strategy) and suggesting directions for improving reasoning in image generation would strengthen the paper’s conclusions and usefulness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2F02cvSK9q", "forum": "4c1gAsVd9C", "replyto": "4c1gAsVd9C", "signatures": ["ICLR.cc/2026/Conference/Submission1588/Reviewer_9Tai"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1588/Reviewer_9Tai"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976075815, "cdate": 1761976075815, "tmdate": 1762915826938, "mdate": 1762915826938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}