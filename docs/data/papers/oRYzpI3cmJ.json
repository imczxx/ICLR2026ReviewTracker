{"id": "oRYzpI3cmJ", "number": 20993, "cdate": 1758312479818, "mdate": 1759896947881, "content": {"title": "Command-V: Training-Free Representation Finetuning Transfer", "abstract": "Retrofitting large language models (LLMs) with new behaviors typically requires full finetuning or distillation—costly steps that must be repeated for every architecture. In this work, we introduce ⌘V (Command-V), a backpropagation-free behavior transfer method that copies an existing residual representation adapter from a donor model and pastes its effect into an architecturally different recipient model. ⌘V profiles layer activations on a small prompt set, derives linear converters between corresponding layers, and applies the donor intervention in the recipient’s activation space. This process does not require access to the original training data and needs minimal compute. In three case studies—safety-refusal enhancement, jailbreak facilitation, and automatic chain-of-thought reasoning—⌘V matches the performance of direct finetuning while using orders of magnitude less resources.", "tldr": "", "keywords": ["Model Editing", "Steering", "Activation", "Interpretability"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3ce86fd460e134d28eaf67cd72d2ed9acf20be4b.pdf", "supplementary_material": "/attachment/1b2346e128f2852db515d730f9a68bd8b806897f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Command-V, a backpropagation free method which transfers fine tuned behaviors between models by copying a donor model’s residual adapter and pasting its activation level effect into a different recipient model. It builds activation profiles from a prompt set to align layers across architectures and derives simple linear converters that map one model’s hidden space onto another’s. Experiments across model families show that Command-V transfers behaviors such as safety refusal, jailbreak susceptibility, and CoT reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Command-V achieves effective behavior transfer without any backpropagation, training data, or gradient updates, making it cheaper and faster than conventional fine tuning.\n2. Across diverse tasks, including safety refusal, jailbreak facilitation, and CoT reasoning, the method matches or closely approaches the performance of direct fine tuning.\n3. The paper provides a clear mechanism using activation profiling and linear converters, grounding the approach in a mathematically simple yet powerful framework."}, "weaknesses": {"value": "1. The method’s linear converters struggle to bridge major model gaps, e.g., from llama to qwen. So the method strongly depends on strategic model pairing. \n2. I think the method might be better for transferring from large model to small model, as in figure 3, qwen and llama from large version to small version have much better performance than from small version to large version. So, it could be difficult to build high-performing generalists by pasting from small specialist, which is a future direction proposed in the discussion.\n3. Model D' mentioned on line 138 is not used in the rest parts."}, "questions": {"value": "1. I want to be more precise, does authors only use the representation of the last token of the prompt, without involving any representations from the model’s generated content at all. \n2. If we apply this method to transfer different abilities into the same model using multiple converters, would that lead to improvements across all abilities, or could there be conflicts between them? Perhaps it might be worth exploring whether these converters can be merged."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IqgjoODUS7", "forum": "oRYzpI3cmJ", "replyto": "oRYzpI3cmJ", "signatures": ["ICLR.cc/2026/Conference/Submission20993/Reviewer_inhB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20993/Reviewer_inhB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761377090903, "cdate": 1761377090903, "tmdate": 1762940000525, "mdate": 1762940000525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of transferring a representation adapter from a donor LLM to a different recipient LLM without backprop or access to task data. The paper proposes Command-V. Command-V builds a linear converter between matched layers using pseudoinverses. During inference, Command-V applies the donor’s adapter in the recipient’s activation space by first converting the representations to the donor’s space, applying the intervention and then applying them back. Since the converters can be built beforehand, this process process does not require training the recipient model.\nThe paper evaluates three behaviors, including safety refusal, refusal suppression, and chain-of-thought reasoning. The experimental results show the proposed approach can successfully transfer the representation adapter."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies an interesting problem.\n\nThe proposed approach is a straightforward and neat solution for this. And empirically it shows some success.\n\nThe experiments are rather thorough (regarding models and tasks), covering multiple behaviors and models.\n\nThe paper is well-written and easy to follow."}, "weaknesses": {"value": "While the proposed approach is conceptually interesting, I have some concerns on its utility in practical scenarios:\n\n* There’s some trade-offs between performance and efficiency. In many scenarios the Command-V approach underperforms learning and editing on the recipient itself.\n* While the paper argues improved efficiency. It is arguable that this representation editing approach is already very efficient (updating very few parameters, and not requiring tons of data). It is unsure how practitioners will be willing to trade such performance time for efficiency only required at train time.\n\n\nThe experiments only consider one type of representation adaptor. It is unsure how robust the technique is to other editing techniques (like LoFiT (Yin et al., 2025), JoLA (Lai et al., 2025)."}, "questions": {"value": "How sensitive is Command-V’s performance to the choice of representation adapter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EiFE7rAdkT", "forum": "oRYzpI3cmJ", "replyto": "oRYzpI3cmJ", "signatures": ["ICLR.cc/2026/Conference/Submission20993/Reviewer_67Xf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20993/Reviewer_67Xf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852504259, "cdate": 1761852504259, "tmdate": 1762939997244, "mdate": 1762939997244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a technique for learning a mapping from one model's representation space to another's that doesn't require training (instead it can be computed analytically with the Moore-Penrose pseudoinverse on a dataset of paired activation samples), and then uses this technique to transfer a learned adapter from one model to another. This approach is compared with learned parameter-efficient finetuning on the target model and seems to perform as well as or even outperform it in some cases on a wide set of tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The method is quite elegant, not requiring any learning but instead computing a transfer matrix analytically with some paired activation data.\n- Analyses in Figure 3 and Figure 4 are extremely interesting; it's a novel result in the literature so far that extremely low parameter count PEFTs can learn long-form reasoning, e.g. the ReFT paper showed worse performance on GSM8K in non-reasoning models. It's also interesting how model families don't seem to really capture how well refusal training transfers, particular Qwen models transfer better out-of-family in many cases?\n- I really appreciate the analysis of how similar different models' representation spaces are, e.g. Figure 5 in the appendix; these might even be main-text-worthy contributions. There might be applications to understanding model provenance/distillation that go beyond this immediate application to transfer."}, "weaknesses": {"value": "- Some nice connections that might be worth looking at (not really weakness of the paper, just interesting!): There must be some relation to the Platonic Representation Hypothesis ([Huh et al. (2024)](https://arxiv.org/abs/2405.07987)) which will help with framing. Also compare Appendix E.3 of [Wu et al. (2025)](https://arxiv.org/abs/2501.17148) which has some related experiments on affine transfer of steering vectors across models."}, "questions": {"value": "- Why use DiReFT for the adapter as opposed to the other ReFT variants (e.g. LoReFT which outperforms DiReFT)? Is it purely to reduce cost or other performance-related considerations come into play?\n- For the ReFT baselines, was LoReFT used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jeHj6IAXpn", "forum": "oRYzpI3cmJ", "replyto": "oRYzpI3cmJ", "signatures": ["ICLR.cc/2026/Conference/Submission20993/Reviewer_R7Es"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20993/Reviewer_R7Es"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762293331866, "cdate": 1762293331866, "tmdate": 1762939989613, "mdate": 1762939989613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Command-V (⌘V), a training-free approach for transferring fine-tuned behaviors between large language models of varying architectures and sizes.\n⌘V aligns the representation spaces of a donor and a recipient model through activation profiles and linear converters, enabling behavioral transfer without access to the original training data and with minimal computational overhead.\nEmpirical studies across three case scenarios demonstrate that ⌘V achieves performance comparable to direct fine-tuning while consuming significantly fewer computational resources."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method avoids the high computational and memory costs typically associated with fine-tuning or distillation.\n\n2. ⌘V requires no access to the task-specific data used to train the donor adapters, making it especially practical in settings where data sharing is restricted or unavailable.\n\n3. Across three distinct case studies, ⌘V has demonstrated competitive performance relative to baseline fine-tuning, with substantially reduced compute requirements."}, "weaknesses": {"value": "1. The approach implicitly assumes a linear correspondence between activation spaces. While computationally efficient, the paper provides limited theoretical or empirical evidence supporting this assumption.\n\n2. ⌘V inherits the strengths and weaknesses of the donor adapter. When the donor yields only modest gains, the transfer conveys little benefit. Moreover, smaller recipient models occasionally experience output collapse or incoherent text generation after transfer.\n\n3. The paper also mentions that predicting good model pairs and task performance transferability remains an open question, posing a challenge for practical deployment."}, "questions": {"value": "1. Could the authors provide stronger theoretical or empirical evidence supporting the validity of the linear converter assumption? In particular, what justifies expecting linear mappings to adequately capture the relationships between donor and recipient activation spaces?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NaMDNLM48M", "forum": "oRYzpI3cmJ", "replyto": "oRYzpI3cmJ", "signatures": ["ICLR.cc/2026/Conference/Submission20993/Reviewer_Tbum"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20993/Reviewer_Tbum"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762520944650, "cdate": 1762520944650, "tmdate": 1762939945213, "mdate": 1762939945213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}