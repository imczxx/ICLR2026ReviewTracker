{"id": "JojyT9niJL", "number": 11697, "cdate": 1758203146932, "mdate": 1759897560219, "content": {"title": "P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark", "abstract": "Academic posters are vital for scholarly communication, yet their manual creation is time-consuming. However, automated academic poster generation faces significant challenges in preserving intricate scientific details and achieving effective visual-textual integration.\nExisting approaches often struggle with semantic richness, structural nuances, and lack standardized benchmarks for evaluating generated academic posters comprehensively. \nTo address these limitations, we introduce P2P, the first flexible, LLM-based multi-agent framework that generates high-quality, HTML-rendered academic posters directly from research papers. P2P employs three specialized agents—for visual element processing, content generation, and final poster assembly—each integrated with dedicated checker modules to enable iterative refinement and ensure output quality. \nTo foster advancements and rigorous evaluation in this domain, we argue that generated posters must be assessed from two complementary perspectives: objective fidelity and subjective quality. \nSo we establish P2Peval, a comprehensive benchmark featuring 1738 checklist items and a dual evaluation methodology (Fine-Grained and Universal). \nOur Fine-Grained Evaluation uses human-annotated checklists to objectively measure the faithful preservation of verifiable content from the source paper. Concurrently, our Universal Evaluation captures subjective, holistic quality by training a model to align with human aesthetic preferences across key design principles. We evaluate a total of 35 models.\nTo power these advancements, we also release P2Pinstruct, the first large-scale instruction dataset comprising over 30,000 high-quality examples tailored for the academic paper-to-poster generation task. Furthermore,  our contributions aim to streamline research dissemination while offering a principled blueprint for evaluating complex, creative AI-generated artifacts. \nThe code is on the anonymous GitHub, https://anonymous.4open.science/r/c-163Canonymous/README.md.", "tldr": "We present P2P, a LLM-based multi-agent framework that turns research papers into polished HTML posters, backed by a 30k-example instruction dataset and establish a fine-grained benchmark for rigorous evaluation.", "keywords": ["Poster Generate", "LLM-as-a-Judge", "Multi Agent"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eb2bac98e25a97d69df9ae6a2383e594d3b92f3d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a comprehensive and well-executed study on the automated generation of academic posters from research papers. The work is timely, addressing a clear need in the academic community. The proposed P2P framework, the P2PINSTRUCT dataset, and the P2PEVAL benchmark constitute a significant contribution to the field of document AI and scientific communication. The paper is generally well-written and the experimental evaluation is extensive. However, some aspects of the methodology and presentation could be strengthened to improve clarity and impact."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.   This is one of the first works to systematically tackle end-to-end academic poster generation using an LLM-based multi-agent framework. The problem is relevant and underexplored. The creation of P2PINSTRUCT and P2PEVAL fills a critical gap, providing essential resources for future research in this domain.\n2.   The dual-evaluation framework of P2PEVAL is a major strength. The argument for decoupling objective fidelity (Fine-Grained) from subjective quality (Universal) is well-motivated. The use of human-annotated checklists and a learned model for holistic scoring is sophisticated and convincing. The high R² score (0.92) for the Universal Evaluation model adds credibility.\n3.  The paper evaluates a vast number of models (35), including both closed-source and open-source LLMs/MLLMs. The ablation studies effectively demonstrate the contribution of each component (multi-agent, figure describer, reflection). The human preference evaluation and analysis of output formats (HTML vs. SVG/LaTeX) provide valuable practical insights.\n4.   The authors commit to releasing code, datasets (P2PINSTRUCT, P2PEVAL), and detailed prompts, which will greatly facilitate reproducibility and future work. The ethical considerations and resource reporting are appropriate."}, "weaknesses": {"value": "1.  While the paper extensively compares different LLM backbones, it lacks a direct comparison to prior dedicated poster-generation systems (e.g., Qiang et al., 2019; Jaisankar et al., 2024 - Postdoc). A qualitative or quantitative comparison against such established, non-LLM-based methods would better situate the performance of P2P and highlight its advancements. The current comparison feels more like a model benchmark than a system-level comparison.\n2.  The relationship between the P2P *framework* and the specific LLMs used within it (e.g., Claude-3.7-Sonnet) needs to be clarified. Is P2P best viewed as a prompting/orchestration strategy that can be deployed on top of any powerful LLM? The results in Table 1 show \"P2P\" achieving high scores, but this seems to be the result of using the P2P *pipeline* with Claude-3.7-Sonnet as the core LLM. The fine-tuned models (Qwen3-P2P) are a separate contribution. This distinction should be made more explicit to avoid confusion.\n3.   The paper convincingly argues for HTML's advantages (flexibility, interactivity). However, the limitation regarding the prevalence of LaTeX/PPT in academia is buried in the appendix (Section M). This is a significant practical constraint that deserves more prominent discussion in the main text. How might P2P be adapted or extended to output these more traditional formats? Are there plans for a PDF/LaTeX export feature?\n4.   The multi-agent framework with iterative reflection loops is computationally expensive. While resource consumption is mentioned in Appendix G, a more detailed discussion in the main limitations section about the inference-time cost and latency of the full P2P system would be valuable for potential users."}, "questions": {"value": "1.  The P2PINSTRUCT dataset is generated by the P2P framework itself. Could there be a risk of a \"closed loop\" where the dataset inherits and potentially amplifies any systematic biases or error modes of the P2P pipeline? How was the quality of this synthetic data validated?\n2.  The Universal Evaluation uses an XGBoost model to map LLM-generated criterion scores to a human holistic score. Was there any exploration into using a fine-tuned LLM as the holistic judge directly? What was the rationale for choosing a simpler model like XGBoost over an LLM for this final aggregation step? Furthermore, how does the performance of this XGBoost model generalize to posters generated by future, potentially very different, methods not seen during its training?\n3.  The checker modules are a critical component for iterative refinement. Could you provide more concrete examples of typical failure cases that the checkers identified and how the reflection loops successfully corrected them? Conversely, are there systematic errors or types of source paper content that the checkers consistently fail to catch, leading to persistent issues in the final poster? \n4.  The P2PEVAL benchmark is constructed from 121 paper-poster pairs. Could you provide more detail on the stratification and diversity of this benchmark? For instance, how are papers from different scientific fields (e.g., computer science, NLP, CV) and types (e.g., methodological, theoretical, survey) distributed? Is there a risk that the benchmark over-represents certain fields, and how might this affect the fair evaluation of a model's general capabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4cb4UdlaqD", "forum": "JojyT9niJL", "replyto": "JojyT9niJL", "signatures": ["ICLR.cc/2026/Conference/Submission11697/Reviewer_4iW7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11697/Reviewer_4iW7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755777085, "cdate": 1761755777085, "tmdate": 1762922747339, "mdate": 1762922747339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the time-consuming task of creating academic posters from research papers. The authors present a comprehensive, three-part contribution:\n\n1. **P2P:** A flexible, LLM-based multi-agent framework that generates HTML-rendered posters. It employs three specialized agents (Figure Agent, Section Agent, Orchestrate Agent) and a novel \"checker-reflection\" mechanism to iteratively refine the output.\n2. **P2PEVAL:** A new, comprehensive benchmark for evaluating generated posters. Its core innovation is a dual-evaluation methodology, assessing both **objective fidelity** (via a \"Fine-Grained\" score based on 1738 human-annotated checklist items) and **subjective quality** (via a \"Universal\" score from an XGBoost model trained to emulate human aesthetic preferences).\n3. **P2PINSTRUCT:** The first large-scale (30,000+ examples) instruction dataset for the paper-to-poster task, derived from the intermediate outputs of the P2P pipeline.\n\nExperiments show the P2P system, particularly when powered by Claude-3.7-Sonnet, outperforms 35 other models. Notably, human evaluations found the P2P-generated posters to be competitive with, and in many cases preferred to, the original posters created by the authors themselves."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors have built an end-to-end solution, from the generation framework (P2P) to the large-scale data for training (P2PINSTRUCT) and a novel benchmark for evaluation (P2PEVAL). This is a significant and impressive amount of work.\n2. The authors conducted comprehensive experiments, including evaluation on 35 models and a human preference study. The P2P framework is shown to be effective via an ablation study."}, "weaknesses": {"value": "1. There is a lack of analysis for the effectiveness of the P2PINSTRUCT dataset. \n2. The methodology of this metric measures fidelity to a specific human instance, which itself may be suboptimal (as shown by human preference research). The authors should discuss this tension between \"imitation\" and \"fidelity\" more openly.\n3. The two-step, MLLM-Featurizer-plus-XGBoost-Regressor methodology for the universal score is not justified over simpler, more direct MLLM-as-a-Judge approaches. This adds a \"Rube Goldberg\"-like complexity without a clear benefit."}, "questions": {"value": "1. In Table 1, models fine-tuned on P2PINSTRUCT (like Qwen3-P2P) achieve the absolute best scores on ROUGE and BERTScore. However, the improvement on the paper's own flagship metrics, Fine-Grained and Universal, is limited. Does this mean there is a bias in the way this dataset was constructed?\n2. Could the authors more clearly state in the paper that the purpose of the \"Fine-Grained\" metric is to measure fidelity to \"the key content selected by the authors\" rather than imitation of \"the layout design of the authors\"? \n3. The \"checker-reflection\" mechanism is interesting. Can you provide a specific example? For example, when the Poster Checker detects a \"spacing imbalance,\" what kind of \"reflection\" instructions does it send to the Orchestrate Agent to correct it? How many iterations does this process typically take?\n4. Could the authors provide an ablation experiment or justification to demonstrate that the MLLM-XGBoost process is superior to the simpler, end-to-end MLLM-as-a-Judge baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NIKxlA3xvK", "forum": "JojyT9niJL", "replyto": "JojyT9niJL", "signatures": ["ICLR.cc/2026/Conference/Submission11697/Reviewer_HwQu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11697/Reviewer_HwQu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832680931, "cdate": 1761832680931, "tmdate": 1762922746759, "mdate": 1762922746759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a foundational ecosystem for automated academic poster generation, addressing key limitations in prior work, such as the failure to capture semantic richness and the lack of standardized benchmarks. The work is built on three main contributions:\n\n1.  **P2P**: A flexible, LLM-based multi-agent framework designed to generate high-quality, HTML-rendered academic posters directly from research papers.\n2.  **P2PINSTRUCT**: The first large-scale instruction dataset (30,000+ examples) specifically tailored for the paper-to-poster generation task.\n3.  **P2PEVAL**: A comprehensive benchmark featuring 1738 checklist items and a novel dual evaluation methodology (Fine-Grained and Universal) to assess poster quality.\n\nUnlike prior methods that often struggle with semantic nuances and lack systematic evaluation, this paper offers a complete system from generation to evaluation, providing a principled blueprint for assessing complex, creative AI-generated artifacts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.  **Innovative P2P Framework**\n    The P2P multi-agent architecture is a novel contribution to complex document transformation tasks. The inclusion of a **checker-reflection mechanism** is particularly strong, as it mimics the human design process of drafting and revision. This iterative approach helps ensure both scientific accuracy and structural integrity in the final output.\n\n2.  **Valuable Dataset (P2PINSTRUCT)**\n    The paper introduces P2PINSTRUCT, the first large-scale (30K+) instruction dataset specifically designed to train models for the complete paper-to-poster workflow. This resource could be highly valuable for the community and spur further research in this domain.\n\n3.  **Comprehensive Benchmark (P2PEVAL)**\n    The authors have clearly put significant effort into creating P2PEVAL, a new and comprehensive benchmark (1738 checklist items, 121 pairs). The **dual-evaluation framework** is a key strength, thoughtfully separating objective fidelity (via human-annotated checklists) from subjective quality (via a predictive scoring model). This provides a robust and multifaceted analysis of poster quality."}, "weaknesses": {"value": "### 1. Lack of Detail on P2P Checker Mechanisms\nThe paper states, \"Each agent operates in conjunction with a dedicated checker module that triggers a reflection loop if its output fails to meet quality standards\". However, the manuscript provides no concrete details on how these critical checker modules are implemented.\n* What is the core component of each checker? Is it an LLM-as-a-judge, a set of programmatic rules, or a trained classifier?\n* **Figure Checker**: The paper mentions \"an initial confidence threshold\". What is this threshold value, and how was it determined?\n* **Section Checker**: This checker reportedly evaluates four complex metrics: coherence, completeness, faithfulness, and correct referencing. How are these metrics automatically and objectively measured?\n* **Poster Checker**: How is this checker implemented to evaluate \"layout aesthetics and structural integrity\"?\n\n### 2. Missing Implementation Details for P2P Agents\nThe paper describes the multi-agent architecture but omits all implementation details about the agents themselves.\n* What specific models are used for the main agents ($\\mathcal{A}_{Fig}$, $\\mathcal{A}_{Sec}$, $\\mathcal{A}_{Orch}$)?\n* What models are used for their generative sub-components (e.g., Figure Describer, Section Generator, Content Generator, HTML Generator)? This is crucial for reproducibility.\n\n### 3. Contradictory Description of Fine-Grained Poster Evaluation\nI am very confused about how the Fine-Grained Poster Evaluation is actually performed, as the paper seems to contradict itself.\n* In one section, the paper states it uses \"LLM-as-a-Judge... to score generated posters against detailed, human-annotated checklists\".\n* However, in Section 3.2.1, it explicitly states, \"This is a **deterministic scoring methodology, not a trained model**\".\n* These two statements are mutually exclusive. This core evaluation method must be clarified."}, "questions": {"value": "### 1. Unclear Notations\nSome technical notations are ambiguous and hinder a clear understanding of the model's inputs and outputs.\n* L134: \"$v_i$ denotes the visual\". Does \"visual\" refer to the raw image file (e.g., a PNG) or a high-dimensional feature vector extracted from the image?\n* L142: \"a detailed structural schema ($S$)\". What is the data type of $S$? Is it a textual representation of the poster's structure (e.g., a JSON object) or a numerical feature vector?\n\n### 2. Lack of Detail on the P2PINSTRUCT Dataset\nThe paper introduces P2PINSTRUCT as a \"high-quality\" large-scale dataset, but more details are needed to fully assess its scope and quality. Section 2.2 mentions it's derived from the P2P framework's intermediate outputs, including prompting Claude for figure descriptions. The authors should provide more details on:\n\n* **Generation Process**: Beyond the brief mention of Claude, what were the specific prompts or processes used to create the 13,612 instruction-response pairs for the Section, Content, and HTML generators?\n* **Quality Validation**: Given its synthetic origin and the \"high-quality\" claim, what steps (if any) were taken to manually or automatically verify the factual accuracy, coherence, and overall quality of these generated pairs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hfscNm2Bcg", "forum": "JojyT9niJL", "replyto": "JojyT9niJL", "signatures": ["ICLR.cc/2026/Conference/Submission11697/Reviewer_Fp4t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11697/Reviewer_Fp4t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882831113, "cdate": 1761882831113, "tmdate": 1762922746297, "mdate": 1762922746297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a technically sound framework for automating academic poster generation, supported by a novel dataset and a well-designed evaluation benchmark. The paper is well-organized and addresses a meaningful problem in academic communication. However, several methodological details require further clarification to ensure the robustness and generalizability of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes three core contributions:  A multi-agent system that decomposes poster generation into specialized sub-tasks (figure extraction, content summarization, and layout assembly) with integrated reflection mechanisms for iterative improvement.  A large-scale instruction dataset containing over 30,000 examples, designed to support training and fine-tuning of models for the poster generation task.  A benchmark featuring fine-grained checklists and a universal evaluation metric, combining objective fidelity checks with learned subjective quality assessment.  \n\n- The modular design of the P2P framework demonstrates a clear understanding of the complexities involved in translating academic papers into visual summaries.  \n- The release of P2PINSTRUCT and P2PEVAL represents a valuable resource for the community, enabling reproducible research and direct comparison of future methods.  \n- The dual evaluation strategy (fine-grained and universal) effectively captures both factual accuracy and aesthetic quality, addressing a critical challenge in evaluating generative tasks."}, "weaknesses": {"value": "1. The checker-reflection paradigm represents a significant architectural innovation, but its failure boundaries remain unclear. Could you provide a typology of errors that persist despite reflection cycles? Specifically, we're interested in cases where the system's compositional reasoning breaks down - for instance, when reconciling complex multi-panel figures with nuanced methodological descriptions. Understanding these limitations would help define the theoretical ceiling of this approach.\n2. The improvements from P2PINSTRUCT fine-tuning are clear, but we should examine whether this comes at the cost of creative diversity. Are fine-tuned models converging toward a \"P2P house style\" that prioritizes template adherence over adaptive design? Quantitative analysis of layout diversity and qualitative assessment of creative risk-taking in generated posters would help address concerns about potential over-standardization of academic communication.\n3. The recursive nature of P2PINSTRUCT generation raises fundamental questions about knowledge distillation in synthetic datasets. Beyond potential error propagation, we should examine whether this approach creates an \"imprinting bias\" where the dataset becomes increasingly optimized for the P2P framework's specific architectural assumptions. Could you provide analysis showing how the statistical distribution of generated examples evolves through this process and whether it converges toward a local optimum that might limit future model innovation?\n4.  While P2PEVAL's scale is commendable, the selection methodology for the 121 paper-poster pairs warrants deeper scrutiny. More importantly, have you investigated whether the benchmark exhibits structural biases toward certain paper formats (e.g., NLP papers with standardized experimental sections) that might disadvantage models trained on more diverse corpora? The field would benefit from understanding how benchmark composition affects comparative model performance beyond aggregate scores.\n5. The choice of XGBoost for universal scoring, while pragmatic, raises questions about the conceptual framework for evaluating creative artifacts. Have you conducted cross-architecture validation to test whether your learned metric captures fundamental design principles versus simply recognizing patterns characteristic of P2P-generated content? The community needs assurance that this evaluation approach generalizes to novel generation paradigms."}, "questions": {"value": "Some suggestions for improvement\n- Include statistical significance testing for key comparisons in Table 1 to strengthen claims of improvement.  \n- Expand the failure analysis section to provide qualitative examples of error correction via reflection loops.  \n- Discuss the computational overhead of the full P2P pipeline (with reflection) to help users assess practical deployability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5PGWJtqoNL", "forum": "JojyT9niJL", "replyto": "JojyT9niJL", "signatures": ["ICLR.cc/2026/Conference/Submission11697/Reviewer_c6H3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11697/Reviewer_c6H3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985978792, "cdate": 1761985978792, "tmdate": 1762922745433, "mdate": 1762922745433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}