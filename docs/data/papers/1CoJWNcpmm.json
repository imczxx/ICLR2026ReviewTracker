{"id": "1CoJWNcpmm", "number": 24293, "cdate": 1758355035193, "mdate": 1759896772456, "content": {"title": "You Do Not Fully Utilize Transformer's Representation Capacity", "abstract": "In contrast to RNNs, which compress their history into a single hidden state, Transformers can attend to all past tokens directly. However, standard Transformers rely solely on the hidden state from the previous layer to represent the entire context. We show that this design choice induces representation collapse and degrades performance. To address this issue, we introduce Layer-Integrated Memory (LIMe), a lightweight extension that leverages existing key–value buffers and learns per-head, per-layer routing weights to integrate representations from all previous layers with negligible overhead. Through extensive experiments—including language modeling, synthetic reasoning benchmarks, and very deep architectures—LIMe consistently achieves faster convergence, lower perplexity per FLOP, and substantial accuracy improvements on synthetic tasks while preserving higher value–vector entropy and improved token separability. Finally, our analysis of the learned routing weights reveals systematic reuse of both local and long-distance features, demonstrating how LIMe mitigates collapse, unlocks richer representations without increasing hidden-state size, and points to promising directions for future research.", "tldr": "", "keywords": ["Transformers", "Attention", "Representation Collapse", "Residual Stream"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9554a105183406dde59b11872c56fa3ad57611e4.pdf", "supplementary_material": "/attachment/86507d5505a2bd6fa37a3ad1e302051d2a37b43a.zip"}, "replies": [{"content": {"summary": {"value": "This paper identifies \"representation collapse\" as a key weakness in standard Transformer decoders, where the reliance on a single residual stream from the immediately preceding layer forces the model to compress all prior information, leading to a loss of feature diversity in deeper layers. To address this, the authors propose LIMe, a lightweight architectural modification. LIMe allows each attention head at every layer to compute its KV representations by routing and mixing the KV buffers from all preceding layers, not just the current one. This is achieved by learning a per-head, per-layer routing matrix that weights the contributions of past layers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The primary strength of LIMe is its elegance and low overhead. By reusing existing KV buffers, it adds multi-layer information flow with almost no additional memory and a negligible computational cost (especially when GQA is used). This makes it a very practical and \"drop-in\" friendly modification.\n\n-  The paper does an excellent job of clearly identifying a specific problem (representation collapse) and proposing a solution (LIMe) that directly targets it."}, "weaknesses": {"value": "- The authors correctly identify in the limitations that the vanilla implementation of the router has an $\\mathcal{O}(L^2)$ asymptotic complexity (where $L$ is the number of layers), as each layer's router must process keys from all $L-1$ previous layers. This is fine for the 16-layer models in the main paper, but it will become a significant computational bottleneck for scaling to very deep models (e.g., $L=100+$). The heuristic ablations in Appendix F (e.g., last-j or first-j) all show worse performance, suggesting a difficult trade-off between performance and scalability.\n- The method's core idea, accessing all previous KV caches, creates a practical implementation challenge for large-scale training. In a standard pipeline parallel setup, this would require significant communication across pipeline stages (GPUs), as later layers would need to fetch KV caches from all earlier GPUs. The authors acknowledge this and their preliminary test shows a ~7.8% latency overhead. This practical hurdle might deter adoption for training SOTA-scale models, as it requires \"non-trivial engineering effort\" to optimize."}, "questions": {"value": "Please refer to my weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GVt58opRIr", "forum": "1CoJWNcpmm", "replyto": "1CoJWNcpmm", "signatures": ["ICLR.cc/2026/Conference/Submission24293/Reviewer_8E6q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24293/Reviewer_8E6q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848197754, "cdate": 1761848197754, "tmdate": 1762943032608, "mdate": 1762943032608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper starts from the observation that in standard transformer networks, there is a single residual stream, meaning that the representations from all the previous layers are compressed into a single hidden state. This single hidden state is then used as the input of the next layer. This can lead to *representation collapse*, which is the phenomenon where different tokens become undistinguishable. Hence, this paper propose a new mechanism to address this issue, called LIMe. The idea is that each layer can attend to the representations of *all* previous layers, instead of just the immediante previous one. In practice, this is done by modifying the way keys and values are computed. Instead of just using the keys and values computed from the input of the current layer, the keys and values from all previous layers are linearly combined, using trainable weights. Said otherwise, the keys and values of used in the attention of layer L are obtained by doing a linear combination of the keys and values of all the heads of the previous layers. The weight of this linear combination are fixed trainable parameters.\n\nThe proposed method is then empirically evaluated on different language modeling tasks. First, a LLaMa like model, with 1B parameters is trained on 50B tokens, and evaluated on downstream NLP tasks such as QNLI, WiC or ARC (easy/challenge). Here the experiments show that LIMe obtain better performance than the standard transformer architecture, as well as other approaches such as DenseFormer or HyperConnections. Then the model is compared to the standard transfomer on GSM8k or synthetic tasks such as arithmetic expression evaluation, again showing that LIMe performs better than the baseline. There are also ablations studying the *representation collapse* showing that LIMe is less prone to representation collapse than standard transformers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I am a bit of the fence regarding this paper.\n\nIn terms of strengths, I believe that the proposed idea in the paper is simple and elegant. The paper is clearly written and easy to follow. The experimental evaluations are convincing."}, "weaknesses": {"value": "My main concern with the paper is its relation to previous work, and especially its significance with respect to these.\n\nFirst, I believe that the paper does not make a great job discussing the difference with previous work such as DenseFormer, or Value Residual Learning. More precisely, I think that the idea of combining the representations from multiple previous layers instead of just using the representation from the previous layer is not new. The contributions of the paper are thus mostly about details of how this idea is implemented in practice, and the paper could do a better job at discussing these. Moreover, I believe that the baseline considered in the paper (DenseFormer, HyperConnection) have multiple variant considered in the original papers, and the details of which one is used are missing. Finally, I am a bit surprised that the baseline (such as DenseFormer) does not seem to improve compared to the standard transformer, which goes against the claim of the original paper.\n\nAnother minor concern is the additional runtime required by the method, as it needs to read significantly more activations from memory compared to the standard transformer. \n\n**Additional references**\n\n*MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections.* Da Xiao, Qingye Meng, Shengping Li, Xingyuan Yuan. 2025.\n\n*Value Residual Learning.* Zhanchao Zhou, Tianyi Wu, Zhiyun Jiang, Fares Obeid, Zhenzhong Lan. 2024\n\n*LAUREL: Learned Augmented Residual Layer.* Gaurav Menghani, Ravi Kumar, Sanjiv Kumar. 2024\n\n*DeepCrossAttention: Supercharging Transformer Residual Connections.* Mike Heddes, Adel Javanmard, Kyriakos Axiotis, Gang Fu, MohammadHossein Bateni, Vahab Mirrokni. 2025"}, "questions": {"value": "Which variant of DenseFormer and HyperConnection did you use?\n\nDid you re-implement the baselines yourself or use existing code?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SupanSNIuT", "forum": "1CoJWNcpmm", "replyto": "1CoJWNcpmm", "signatures": ["ICLR.cc/2026/Conference/Submission24293/Reviewer_nGJ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24293/Reviewer_nGJ6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923685771, "cdate": 1761923685771, "tmdate": 1762943032296, "mdate": 1762943032296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Layer-Integrated Memory (LIMe), which allows each attention head to access Key-Value representations from all previous layers through learned routing weights."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Comprehensive experimental design: The evaluation spans multiple dimensions: language modeling perplexity, mathematical reasoning on GSM8K, and synthetic tasks with controlled difficulty levels. The representation collapse analysis combines entropy measurements, linear separability tests, and grammatical probing to validate the core hypothesis from different angles. The routing weight analysis provides interpretability by revealing which layer representations the model prefers to access. This is the most lovely part of this paper."}, "weaknesses": {"value": "1. Limited novelty over prior work. The core mechanism of using learned weights to aggregate multi-layer representations appears in Transparent Attention (Bapna et al., EMNLP 2018), which uses trainable softmax-normalized weights to combine encoder layer outputs in NMT decoder cross-attention. The mathematical formulation resembles that prior work, with the main difference being application to decoder-only self-attention. More recently, Hyper-Connections (Zhu et al., Sept 2024) addresses representation collapse through multi-stream connections with learned routing, sharing similar motivation. The paper does not clearly articulate what architectural insight LIMe provides beyond adapting these known techniques to decoder-only models with efficient KV buffer reuse.\n2. Unclear computational cost analysis. The paper claims \"negligible overhead\" yet mentions O(L**2) routing complexity in limitations. For a 64-layer model, each layer must route over 64 previous layer KV pairs, but the paper does not provide memory bandwidth analysis for this case. The pipeline parallelism overhead of 7.8% contradicts the \"negligible\" claim for production scenarios."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0XAZPOMqvX", "forum": "1CoJWNcpmm", "replyto": "1CoJWNcpmm", "signatures": ["ICLR.cc/2026/Conference/Submission24293/Reviewer_vzV7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24293/Reviewer_vzV7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155233863, "cdate": 1762155233863, "tmdate": 1762943032091, "mdate": 1762943032091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper suggests adding a weighted average after the standard key projection. The average is taken over all the key representations of the current token in the current layer and head as well as the previous ones (over $i * h$ vectors in the $i$-th layer with a model having $h$ kv heads). The same is done for the values (but not the queries). The coefficient of weighted average is shared between keys and values. Results show improvement over baseline (as well as DenseFormer and HyperConnections) on downstream tasks. Particularly, there is a signficant boost in accuracy over Arithmetic Expression Task which is attributed to the ability to store more information needed for reasoning. Additionally the authors show that the representation remains linearly separable even in later layers which is not true about the baseline."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "While the method shares similarity with existing methods such as DenseFormer, the correct placement of weighted averages is important and in addition to superior performance on the experiments, yields side-benefits such as the ability to re-use the KV cache. The authors report additional investigative results such as the analysis done on the learned router weights."}, "weaknesses": {"value": "In Section 5.1, it would be very helpful to have the random baseline for each task. In particular, that results that are reported for several of tasks seem near-chance (e.g. WiC). There is also no confidence intervals reported which makes it very hard to determine the significance of the improvements. Overall this makes me question the efficacy of the method in general language modeling.\n\nIt is confusing to refer to LLaMA in Table 1. Based on my understanding, this is only a model with the same base architecture as LLaMA models where as a LLaMa baseline suggests the pre-trained models. I strongly suggest to make this clear since based on my understanding you are training everything from scratch.\n\nI have asked additional questions below. Overall, I am uncertain about the intepretation of the provided results and whether they can currently clearly establish the effectiveness of the proposed method."}, "questions": {"value": "1. When doing value classification (e.g. in Fig. 2b) is the rest of the model frozen?\n\n2. Did you consider using a per-dimension (instead of per-head) weighted average? Was there any difference in performance? Alternatively, is it important to average across heads or is it enough to average over the same head across different layers? \n\n3. DenseFormer does a similar mixing as the proposed method. Still, the results for DenseFormer are sometimes even worse than the baseline. Also, Denseformer paper reports reasonable improvements over the baseline. Why similar consistent improvements are not observed in these new set of experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2BlHNIBl7g", "forum": "1CoJWNcpmm", "replyto": "1CoJWNcpmm", "signatures": ["ICLR.cc/2026/Conference/Submission24293/Reviewer_Ax1G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24293/Reviewer_Ax1G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762216911680, "cdate": 1762216911680, "tmdate": 1762943031829, "mdate": 1762943031829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}