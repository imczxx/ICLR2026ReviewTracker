{"id": "udTDFAshNM", "number": 1780, "cdate": 1756919905485, "mdate": 1759898187272, "content": {"title": "Breaking Gradient Temporal Collinearity for Robust Spiking Neural Networks", "abstract": "Spiking Neural Networks (SNNs) have emerged as an efficient neuromorphic computing paradigm, offering low energy consumption and strong representational capacity through binary spike-based information processing. However, their performance is heavily shaped by the input encoding method. While direct encoding has gained traction for its efficiency and accuracy, it proves less robust than traditional rate encoding. To illuminate this issue, we introduce Gradient Temporal Collinearity (GTC), a principled measure that quantifies the directional alignment of gradient components across time steps, and we show—both empirically and theoretically—that elevated GTC in direct encoding undermines robustness. Guided by this insight, we propose Structured Temporal Orthogonal Decorrelation (STOD), which integrates parametric orthogonal kernels with structured constraints into the input layer of direct encoding to diversify temporal features and effectively reduce GTC. Extensive experiments on visual classification benchmarks, show that STOD consistently outperforms state-of-the-art methods in robustness, highlighting its potential to drive SNNs toward safer and more reliable deployment.", "tldr": "", "keywords": ["Neuromophic computing", "Spiking neural networks", "robustness"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6d1462f79b25b4c960773dcc836cfc32fe493df.pdf", "supplementary_material": "/attachment/e7bad25ce1d6b752e2867523263bb49cf5823e4b.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to improve the robustness of spiking neural networks (SNNs) under direct coding. The authors first proposes a metric called Gradient Temporal Collinearity (GTC) to measure robustness by how similar gradients are across time. Then the authors propose Structured Temporal Orthogonal Decorrelation (STOD), which applies orthogonal transformations to input features at each time step via Patchwise Feature Diversification and Global Orthogonal Regularization. Experiments on CIFAR-10, CIFAR-100, and ImageNet show that STOD achieves better robustness against adversarial attacks than baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a mathematical analysis linking GTC to the spectral radius of the Hessian, connecting gradient structure directly to network robustness.\n\n2. The proposed STOD framework improves robustness by decorrelating temporal features through orthogonal transformations. It delivers strong robustness gains on CIFAR-10, CIFAR-100, and ImageNet.\n\n3. The paper offers clear Interpretability through visualization."}, "weaknesses": {"value": "1. The link between the proposed GTC metric and the STOD method appears somewhat indirect. The paper does not clearly articulate how insights from GTC specifically motivate the design of STOD, nor does it provide sufficient theoretical or empirical evidence showing that STOD directly reduces GTC in a quantifiable manner.\n\n2. The paper omits a detailed analysis of the computational overhead introduced by STOD during both training and inference. Without explicit comparisons of time or resource consumption against baseline methods, it is difficult to assess the practical efficiency and scalability of the proposed approach. \n\n3. The method introduces several hyperparameters (e.g., patch size, regularization strength), yet the paper does not investigate whether the optimal settings generalize across different architectures and datasets. This raises concerns about the generalizability of the approach in broader applications.\n\n4. The evaluation focuses primarily on static image datasets. However, SNNs are naturally well-suited for event-based or sequential data. The paper does not explore or discuss how STOD could be applied or adapted to event-based or sequential datasets, which limits the scope and generalizability of the proposed method."}, "questions": {"value": "1. The robustness improvements of STOD appear smaller on ImageNet compared to CIFAR datasets. Could the authors explain the underlying reasons for this difference? \n\n2. The experiments on black-box attacks and rate gradient approximation attacks did not include baseline comparisons. Could the authors clarify why these comparisons were omitted and whether STOD maintains its advantages when such baselines are included?\n\n3. What are the key differences between the proposed STOD and the baseline methods? While it is intuitive that STOD improves over vanilla SNNs, it remains unclear why it also outperforms other robust approaches.\n\n4. Could the authors empirically compare the robustness of direct coding with that of rate coding? Is the proposed STOD method still effective when applied to rate coding or other spike encoding schemes? \n\n5. In Figure 1, GTC gradually decreases under direct coding. What reasons contribute to this trend? \n\n6. How exactly is the gradient term G[i] in Eq. (3) computed? Does it aggregate gradients from all network parameters, or only from specific layers or subsets of weights?\n\n7. Why is improving robustness particularly crucial for SNNs compared to ANNs? The authors might consider emphasizing the significance and potential benefits of robustness for SNNs in the Introduction.\n\n8. Could the authors compare STOD with simpler baselines that inject random noise into features or gradients? \n\n9. Why are adversarial attacks not applied at every time step? Would the proposed method remain effective under this more challenging attack setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WC0aNSKhOM", "forum": "udTDFAshNM", "replyto": "udTDFAshNM", "signatures": ["ICLR.cc/2026/Conference/Submission1780/Reviewer_6hhX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1780/Reviewer_6hhX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761669883850, "cdate": 1761669883850, "tmdate": 1762915887933, "mdate": 1762915887933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the resistance of spiking neural networks (SNNs) to adversarial attacks (white box and black box).\n\nThey argue that direct input coding, which outperforms Poisson rate coding in terms of accuracy, makes the SNNs less resistant because of strong correlations across timesteps.\n\nThey propose a new method to decorrelate timesteps, and thus increase resistance to adversarial attacks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "As far as I know, the method is new, and it outperforms other approaches for white box attacks (Table 1)."}, "weaknesses": {"value": "I am not convinced, because a number of comparisons are missing:\n\n* Comparison to SOTA (Table 1) is limited to white box attacks. How about for black box attacks?\n\n* The authors should also compare their method to Poisson rate coding.\n\n* How about using direct input coding, but adding white noise? This should also decorrelate time steps (similar to Poisson rate coding). One could vary the noise amplitudes. Maybe there is a sweet spot in terms of amplitude, the authors should investigate.\n\n* An alternative to direct input coding has been proposed in\nhttps://dl.acm.org/doi/10.5555/3692070.3692856\nthe idea is simple:\na simple 2d conv is applied to the input image with:\nCin = 3 (RGB channels)\nCout = T\nthe output is [T, W, H] then at each t timestep t we they feed the image [t, :, :] to the SNN.\nThis could also decorrelate timesteps.\nIt would be interesting to compare the proposed approach to this previous one, in terms of accuracy and resistance to attacks."}, "questions": {"value": "The size of the orthogonal kernels is d in the text, but c x p x p on Fig 1. So you choose d = c x p x p?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AhZHL9AwTD", "forum": "udTDFAshNM", "replyto": "udTDFAshNM", "signatures": ["ICLR.cc/2026/Conference/Submission1780/Reviewer_ZQzg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1780/Reviewer_ZQzg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733068311, "cdate": 1761733068311, "tmdate": 1762915887718, "mdate": 1762915887718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Structured Temporal Orthogonal Decorrelation (STOD), a framework designed to mitigate Gradient Temporal Collinearity (GTC) - the tendency of gradients in Spiking Neural Networks to align their directions across time steps, where high GTC in direct encoding results in vulnerability to adversarial perturbations. \nSTOO combines two complementary modules: Patchwise Feature Diversification (PFD), which enforces local orthogonality in kernel transformations to preserve intra-step feature independence; and Global Orthogonal Regularization (GOR), which introduces a feature-level angular diversity loss — a cosine-based soft constraint (Eq. 8) — to penalize similarity between normalized transformed features across time steps. This combination jointly ensures orthogonal kernel dynamics and temporally diverse representations, leading to robust and decorrelated spatio-temporal learning. \nEmpirical evaluations on CIFAR-10/100 and ImageNet under multiple adversarial attack scenarios demonstrate consistent improvements without additional inference overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clearly identifies GTC as a key cause of robustness degradation in SNNs and provides an intuitive explanation of its geometric implications, supported by experiments (in Fig.1).\n- Integrates local (PFD) and global (GOR) components to address temporal alignment of gradients from complementary kernel- and feature-level perspectives, offering a coherent and theoretically consistent framework.\n- Evaluates robustness under diverse attack settings (e.g., FGSM, PGD and both white- and black-box attacks) and across multiple datasets, presenting consistent gains over baselines.\n- Effectively connects temporal gradient decorrelation to robustness improvement while maintaining no inference overhead due to removal of orthogonal kernels at test time."}, "weaknesses": {"value": "- Some mathematical formulations are ambiguous: in Eq.(7), the index $j$ range in the Householder reflection is unclear ($j \\in [1,T]$ vs. $j \\in [1,d]$), limiting reproducibility and interpretability of the initialization process. \n- Lacks ablation studies analyzing the independent contributions of PFD and GOR, making it difficult to assess which component drives the main performance gains in STOD.\n- Although the motivation contrasts direct and rate encoding, there is no quantitative evidence that STOD-enhanced direct encoding exhibits rate-like decorrelation characteristics. Fig.4 includes comparisons, but those differences are visually small and not statistically analyzed.\n- Experiments are limited to static image benchmarks, which aligns with the focus on input encoding but leaves open whether the proposed decorrelation mechanism generalizes to other domains of natural inputs (such as audio, time-series, etc.)."}, "questions": {"value": "1. Could you provide ablation results isolating the impact of PFD and GOR individually? It is difficult to evaluate their relative contributions without such analysis. \n2. In Fig. 4, the comparison between direct encoding and STDO shows only marginal visual differences, and the transformed features via PFD across time steps also appear similar. Could you provide quantitative evidence or clearer visualizations demonstrating improved gradient diversity? \n3. Eq.(7) defines the canonical basis $e_1, …, e_d$, yet the index range for $j$ ($[1,T]$ vs. $[1,d]$) remains ambiguous. How is this handled to ensure valid kernel initialization?\n4. The inference phase introduces no additional overhead because learned orthogonal kernels are discarded. Can you clarify whether inference without these kernels still maintains the same level of temporal decorrelation? \n5. The paper focuses exclusively on static natural-image datasets. While this choice aligns with the motivation around direct encoding, have you considered extending STOD to other natural input domains (e.g., sensor time-series, audio, etc.), where gradient temporal collinearity may also naturally arise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wOA7xUsA7V", "forum": "udTDFAshNM", "replyto": "udTDFAshNM", "signatures": ["ICLR.cc/2026/Conference/Submission1780/Reviewer_PFkG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1780/Reviewer_PFkG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920299043, "cdate": 1761920299043, "tmdate": 1762915887538, "mdate": 1762915887538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates robustness issues in Spiking Neural Networks (SNNs) under direct encoding, a promising alternative to rate encoding due to its efficiency. The authors introduce Gradient Temporal Collinearity (GTC), a metric that quantifies temporal gradient alignment, and theoretically show that high GTC contributes to reduced robustness via analysis linked to the Hessian spectral radius. To mitigate this, the paper proposes Structured Temporal Orthogonal Decorrelation (STOD), which applies parametric orthogonal kernels with structured constraints at the input stage to decorrelate temporal features. STOD is applied only during training and can be removed at inference, incurring no runtime cost. Experiments on visual classification benchmarks show improved robustness over state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes GTC and provides a theoretical analysis that connects the encoding strategy with robustness through the derivation of the Hessian spectral radius, demonstrating strong theoretical depth.\n2. STOD performs orthogonal transformation only during the input stage of training; after training it can be removed, resulting in low deployment cost and no impact on inference efficiency."}, "weaknesses": {"value": "1. In the definition of GTC, several important formulations should be presented in the main text.\n2. The direct connection between GTC and robustness should be further validated experimentally under other encoding schemes such as temporal or latency encoding.\n3. It remains unexplained why the GTC curve of Rate Coding does not decrease during training, whereas that of Direct Coding gradually becomes smoother and decreases.\n4. Although gradient reshaping methods are known to reduce accuracy on clean-dataset, the paper does not analyze why its performance on clean-dataset is lower than other methods or why the degradation is larger.\n5. Since STOD orthogonal transformation is removed during validation, it is unclear whether this might cause overfitting during training."}, "questions": {"value": "see weaknesses 2-5"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BkZ0BkUBST", "forum": "udTDFAshNM", "replyto": "udTDFAshNM", "signatures": ["ICLR.cc/2026/Conference/Submission1780/Reviewer_Tj2T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1780/Reviewer_Tj2T"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101397140, "cdate": 1762101397140, "tmdate": 1762915887431, "mdate": 1762915887431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}