{"id": "v8C2Cd0lAh", "number": 23142, "cdate": 1758340156250, "mdate": 1759896830599, "content": {"title": "Hallucination-aware Intermediate Representation Editing in Large Vision-Lanugage Models", "abstract": "Large Vision-Language Models have demonstrated exceptional performance in multimodal reasoning and complex scene understanding. However, these models still face significant hallucination issues, where outputs contradict visual facts. Recent research on hallucination mitigation has focused on retraining methods and Contrastive Decoding (CD) methods. While both methods perform well, retraining methods require substantial training resources, and CD methods introduce dual inference overhead. These factors hinder their practical applicability. To address the above issue, we propose a framework for dynamically detecting hallucination representations and performing hallucination-eliminating edits on these representations. With minimal additional computational cost, we achieve state-of-the-art performance on existing benchmarks. Extensive experiments demonstrate the effectiveness of our approach, highlighting its efficient and robust hallucination elimination capability and its powerful controllability over hallucinations. Our code will be released.", "tldr": "", "keywords": ["mitigating hallucination", "feature editing", "LVLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d4af9a4b96c64dd898dbe690545471dc466d71c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel approach to mitigating hallucinations in language models by modifying intermediate representations, avoiding the need for retraining or doubling inference costs. The proposed framework, HIRE, includes 3 parts - Editor, Router, and Regulator, and dynamically detects and edits parts of the model’s internal states that are prone to hallucination, while allowing users to control the degree of editing to meet different requirements. The experiments demonstrate the effectiveness of HIRE, achieving state-of-the-art performance across three benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents convincing results, demonstrating the effectiveness of the proposed method across multiple benchmarks and settings.\n\n* The writing is clear and well-structured, making the ideas and methodology easy to follow.\n\n* The method is novel, introducing a new paradigm for hallucination mitigation by dynamically editing intermediate representations, which is both conceptually interesting and practically useful. The regulator is a nice addition that I haven't seen in other approaches.\n\n* The analysis in Figure 5 effectively illustrates the key points, providing insight into how the editing of intermediate representations reduces hallucinations and supports the overall claims of the paper."}, "weaknesses": {"value": "* Baseline comparison: As far as I understand, the baselines presented in the paper were trained on different datasets (or dataset sizes), making direct comparisons potentially misleading. A more appropriate comparison would include a reinforcement-learning baseline trained with LoRA on the same positive and negative examples that were used here, matching the number of trained parameters or FLOPs, to ensure a fair evaluation.\n\n* Missing related work: The paper overlooks prior research on hallucination mitigation at the representation level (e.g., [1]). While most existing methods rely on retraining or contrastive decoding, some works directly edit representations and could serve as meaningful comparisons.\n\n[1] Jiang et al., Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations, ICLR 2025"}, "questions": {"value": "The model appears to scale well with increasing training data, as shown in Figure 6. This raises a couple of important questions:\n\n* How does this scaling compare to other finetuning-based methods? Is it possible that the proposed approach is more data-efficient, extracting more value per training example?\n\n* The curve does not appear to have saturated. At what data size would the model’s performance plateau, and what are the implications for larger-scale training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ybGADWhLRr", "forum": "v8C2Cd0lAh", "replyto": "v8C2Cd0lAh", "signatures": ["ICLR.cc/2026/Conference/Submission23142/Reviewer_cpPf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23142/Reviewer_cpPf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761442707784, "cdate": 1761442707784, "tmdate": 1762942530881, "mdate": 1762942530881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new framework called HIRE for mitigating hallucinations in LVLMs without requiring model retraining or doubling inference costs. HIRE modifies intermediate representations by incorporating an Editor and a Router, which combine contrastive learning and DPO. By controlling the degree of hallucinations, the method adapts to different user requirements. Extensive evaluations show that the proposed approach achieves state-of-the-art performance on three benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to follow, with clear visualizations.\n- The proposed HIRE framework is effective, lightweight, and applicable to most models.\n- Detailed evaluations are conducted to demonstrate the effectiveness of HIRE."}, "weaknesses": {"value": "- In Line 474, \"..edited representations shift toward the non-hallucinated cluster and begin to merge, confirming that our editing effectively reduces hallucination.\" However, in Figure 5 (right), the entire green cluster appears to move closer to the separation line. How this observation can be explained should be clarified.\n- The HIRE framework reduces hallucinations at the token level by enhancing/suppressing corresponding tokens based on the original/perturbed image. In the COCO dataset shown in Figure 9, the YES/NO tokens may not be as directly influenced as those in Figure 7, where the response directly contains instances. A deeper analysis on the effectiveness of HIRE in YES/NO questions could be constructed."}, "questions": {"value": "- How does the centroid of the entire green cluster change relative to the separation line in Figure 5? Is it closer to the separation line after being edited?\n- More analysis on the effectiveness of HIRE in YES/NO questions could be constructed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "evVERA3W8W", "forum": "v8C2Cd0lAh", "replyto": "v8C2Cd0lAh", "signatures": ["ICLR.cc/2026/Conference/Submission23142/Reviewer_z4GY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23142/Reviewer_z4GY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891823597, "cdate": 1761891823597, "tmdate": 1762942530478, "mdate": 1762942530478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method to reduce object hallucination. Prior approaches, including training-based methods and contrastive decoding, often require substantial training resources or increase inference costs. To address these limitations, the authors propose HIRE, which is composed of an Editor and a Router. The Editor comprises two encoders, an attention module, and a decoder, and is used to compute a steering direction. The Router then determines whether to apply this steering direction. The proposed method is evaluated on three benchmark datasets: CHAIR, POPE, and AMBER, demonstrating its effectiveness in mitigating hallucinations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Lower Inference Cost Compared to Contrastive Decoding.** Despite the increased number of parameters and computational overhead in HIRE, the overall inference cost remains lower than that of contrastive decoding methods, making it more efficient in practice.\n- **Reasonable Design Choice.** The proposed router architecture is inspired by the Mixture of Experts (MoE) approach, a widely adopted and validated method in LVLMs and LLMs. By selectively activating Editor, the router enables adaptive and effective processing."}, "weaknesses": {"value": "**W1. Lack of Comparison with Training Method.** The proposed method falls within the training approach. The detailed comparison with existing training approaches validate the effectiveness of the proposed method.\n- How efficient is the proposed method from a training resource perspective compared to other training methods? Also, the performance comparison with training methods (e.g.,  HACL) is required than contrastive decoding method.\n- Is M3ID equivalent to M3ID + DPO? If not, a direct comparison with M3ID + DPO should be provided.\n\n**W2. Lack of Comparison with Steering Methods.** HIRE intervenes the latent representation by computing the direction of non-hallucination. The comparison with steering methods [R1, R2] is needed.\n\n[R1] Le Yang et al., Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection, CVPR 2025\n\n[R2] Sheng Liu et al., Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering, ICLR 2025\n\n**W3. Performance of General Task Capabilities.** LVLMs can perform various visual tasks in a zero-shot manner. After training, are these capabilities maintained? \n\n**W4. Reproducibility.** HIRE has hyperparameters, including learning rates, schedulers, the scaling factor, and the editing strength. However, the paper does not provide a detailed justification for the selection of these hyperparameters. How were these values chosen?"}, "questions": {"value": "**Q1. Degree of Hallucination.** (Line 76) I think that most existing methods can control the degree of the hallucination. Both Contrastive Decoding (CD) and steering methods can effectively control the degree of hallucinations. Steering methods utilize a scaling factor for the steering direction, enabling control over the generated outputs. Similarly, CD has hyperparameters that influence the decoding process, allowing for the adjustment of the model’s logits. \n\n**Q2. Missing Reference.** I was wondering whether the proposed method differs from MOEs; the paper does not cite existing work on MOEs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3z4LhYJFum", "forum": "v8C2Cd0lAh", "replyto": "v8C2Cd0lAh", "signatures": ["ICLR.cc/2026/Conference/Submission23142/Reviewer_AKoZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23142/Reviewer_AKoZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954773929, "cdate": 1761954773929, "tmdate": 1762942530085, "mdate": 1762942530085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HIRE, a lightweight plug-in module for large vision-language models that reduces hallucinations without retraining the base model or doing multi-pass decoding. Instead of modifying output logits, HIRE edits the model’s intermediate hidden representations during inference. It learns to disentangle each token’s “semantic content” from its “hallucination tendency,” computes an edit direction that shifts the token’s representation toward image-grounded truth, and applies this edit only when a learned router predicts the token is likely hallucinated. A single global coefficient controls the edit strength and direction, enabling continuous, user-adjustable hallucination suppression. Experiments on standard hallucination benchmarks (e.g., CHAIR, POPE, AMBER) show that HIRE significantly lowers hallucination rates while preserving fluency and with minimal compute overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work performs hallucination control by directly editing intermediate representations, rather than retraining the whole LVLM or running contrastive decoding with multiple forward passes. This keeps compute cost low.\n2. Authors disentangle “semantic content” vs. “hallucination tendency” in the hidden space and edits only the hallucination component, which preserves fluency and factual content instead of bluntly suppressing all tokens.\n3. This work improves standard hallucination benchmarks (CHAIR, POPE, AMBER) while keeping language natural, showing that hallucination mitigation does not have to trade off descriptiveness."}, "weaknesses": {"value": "1. Training images come from MSCOCO, and some evaluation benchmarks (e.g., POPE) also draw from MSCOCO. The method should be tested on images from other datasets to rule out dataset bias and show robustness.\n2. The approach is only demonstrated on older LVLM backbones （LLaVA 1.5 & InstructBLIP. It should also be tested on the latest multimodal LLMs (e.g., the Qwen 2.5 VL series) to further validate generality and effectiveness.\n3. The paper does not report the number of additional training parameters introduced by the added modules."}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OhUu8w3Lwe", "forum": "v8C2Cd0lAh", "replyto": "v8C2Cd0lAh", "signatures": ["ICLR.cc/2026/Conference/Submission23142/Reviewer_GT68"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23142/Reviewer_GT68"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982861270, "cdate": 1761982861270, "tmdate": 1762942529395, "mdate": 1762942529395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}