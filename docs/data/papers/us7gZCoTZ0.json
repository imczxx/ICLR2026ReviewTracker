{"id": "us7gZCoTZ0", "number": 15620, "cdate": 1758253263092, "mdate": 1759897294692, "content": {"title": "DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization", "abstract": "Prompt Optimization has emerged as a crucial approach due to \nits capabilities in steering Large Language Models to solve \nvarious tasks. However, current works mainly rely on the random \nrewriting ability of LLMs, and the optimization process generally \nfocus on specific influencing factors, which makes it easy to fall into local optimum. \nBesides, the performance of the optimized prompt is often unstable,\nwhich limits its transferability in different tasks. \nTo address the above challenges, we propose $\\textbf{DelvePO}$ \n($\\textbf{D}$irection-Guid$\\textbf{e}$d Se$\\textbf{l}$f-E$\\textbf{v}$olving\nFramework for Fl$\\textbf{e}$xible $\\textbf{P}$rompt $\\textbf{O}$ptimization), \na task-agnostic framework to optimize prompts \nin self-evolve manner. In our framework, we decouple \nprompts into different components that can be used to explore \nthe impact that different factors may have on various tasks. \nOn this basis, we introduce working memory, through which \nLLMs can alleviate the deficiencies caused by their own uncertainties \nand further obtain key insights to guide the generation of \nnew prompts. Extensive experiments conducted on different \ntasks covering various domains for both open- and \nclosed-source LLMs, including DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct and GPT-4o-mini. Experimental results show that \nDelvePO consistently outperforms previous SOTA methods \nunder identical experimental settings, demonstrating \nits effectiveness and transferability across different tasks.", "tldr": "This paper presents a task-agnostic, component-customizable and flexible framework to optimize the prompts in a self-evolve manner. To the best of our knowledge, this is the first work that introduces memory mechanism to PO.", "keywords": ["Prompt Optimization", "Prompt Engineering", "Evolutionary Algorithm", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35d67293bfeac783620310d71df779a23d0efe32.pdf", "supplementary_material": "/attachment/7fe9971c40ed1acf4d3d0618f6c427072e076456.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces DelvePO (Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization). DelvePO decomposes prompts into explicit components to systematically probe how different factors affect performance, and augments the process with a working memory that helps the LLM mitigate its own uncertainty, extract key insights, and use those insights to guide the next round of prompt generation. Across diverse tasks and domains, and on both open- and closed-source models (e.g., DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct, GPT-4o-mini), DelvePO consistently outperforms prior state-of-the-art under identical settings, demonstrating improved effectiveness and transferability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Decouples prompts into components and uses direction-guided self-evolution, reducing reliance on random rewrites and avoiding local optima while offering clearer interpretability of what matters in a prompt.\n\n2. Introduces a working memory to capture insights across iterations, mitigating LLM uncertainty and making prompt updates more principled and consistent.\n\n3. Demonstrates consistent improvements across diverse tasks and both open/closed models."}, "weaknesses": {"value": "1. Novelty vs structured prompt methods is under-specified. The paper claims a key contribution in decomposing prompts into components, but similar ideas exist (e.g., Task Facet Learning: A Structured Approach To Prompt Optimization). It will be better to add into related works.\n\n2. Missing closer, up-to-date baselines. Need to add some recent and more related prompt-optimization methods as baseline (Task Facet Learning is one example).\n\n3. The transferability claim need stronger evidence. Currently, it is unclear how the transferability is demonstrated in the experiments, which is claimed in the abstract."}, "questions": {"value": "May be better to show some cases with optimized prompt."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BJeQabxiuD", "forum": "us7gZCoTZ0", "replyto": "us7gZCoTZ0", "signatures": ["ICLR.cc/2026/Conference/Submission15620/Reviewer_akfT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15620/Reviewer_akfT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761211415349, "cdate": 1761211415349, "tmdate": 1762925887130, "mdate": 1762925887130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DelvePO, a direction-guided and memory-augmented prompt optimization framework. It first decomposes prompts into functional components and then evolves them using two working memories: Component Memory, which records beneficial component edits, and Prompt Memory, which stores high-performing prompt combinations. This design reduces the randomness of LLM-based prompt mutation, prevents the loss of important components, and improves transfer across tasks and models. Experiments on multiple datasets and LLMs show consistent gains over human prompts and prior prompt optimization methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a clear component-level prompt representation together with two explicit working memories (Component Memory and Prompt Memory), which turns otherwise highly stochastic LLM-based prompt mutation into a more controllable and reusable optimization process."}, "weaknesses": {"value": "1.\tThe paper proposes a task-agnostic framework, but the initial component pool is manually collected and constructed from a wide range of related literature (line 116). This raises a question about the motivation of the method: does DelvePO truly make no strong task-specific assumptions and generalize to different tasks because of the framework design itself, or is the observed generality mainly due to the fact that a very comprehensive task component pool has been pre-collected and constructed?\n2.\tThe ablation in Table 3 provides only qualitative evidence that the proposed direction signal, instantiated via both the component-level memory and the prompt-level memory, indeed contributes to the final performance: removing either memory degrades the results, and removing both effectively collapses the framework back to a largely stochastic evolution regime. This supports the authors’ claim that “direction-guided” optimization is beneficial. However, earlier in the paper, the authors explicitly state that direction-guided evolution “can reduce the time required for evolutionary operations” (line 87), i.e., that guidance does not merely improve the final score but also makes the optimization more efficient. To substantiate this stronger claim, the paper should report performance vs.–budget curves (e.g., performance as a function of iteration/time, number of LLM calls, or total input tokens) under a fixed budget, and compare them against a purely stochastic/mutation-only variant. Otherwise, the observed gains can still be explained by “doing more (or better-informed) LLM calls” rather than by genuinely improving the sample efficiency of the evolutionary process.\n3.\tThe components used in DelvePO are extracted and constructed from related literature and can be further generated by an LLM. However, there is no experimental evidence showing whether the method remains effective when the target task domain deviates substantially from the initial component set. For domain-specific tasks, you may refer to the test sets used in Table 2 of PROMPTAGENT [1].\n\n\n[1] Wang, Xinyuan, et al. \"Promptagent: Strategic planning with language models enables expert-level prompt optimization.\" arXiv preprint arXiv:2310.16427 (2023)."}, "questions": {"value": "1. The method adopts a multi-stage pipeline whose complexity is relatively high, and the usage cost will be noticeably higher for closed-source LLMs. It is recommended to provide a clearer breakdown of the computational and token cost, and to specify in the method section which stages can be parallelized and which stages can be cached in advance.\n\n2. The authors significantly increase token consumption by writing a large amount of memory back into the prompt, but the experimental section does not systematically report the relationship between “memory context length vs. performance vs. cost.” As a result, it is unclear whether the performance gains are achieved primarily by spending a large number of tokens. \n\nTypos:\n1. The double quotation marks around “role” in line 56 are not formatted correctly, and the same issue appears with other quotation marks in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TLZfhVMj65", "forum": "us7gZCoTZ0", "replyto": "us7gZCoTZ0", "signatures": ["ICLR.cc/2026/Conference/Submission15620/Reviewer_URXt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15620/Reviewer_URXt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880717388, "cdate": 1761880717388, "tmdate": 1762925886760, "mdate": 1762925886760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Previous works about prompt optimization focuses on limited specific factors, making local optimum inevitable. This work proposes DelvePO with memory to recognize deficiencies and thus guide new generation of prompt. DelvePO consistently outperforms previous SOTA methods on classical NLP tasks, QA, etc. across several models, showing the effectiveness of DelvePO. Specifically, DelvePO decouples the prompt into several factors, task-evolution, solution-evolution, memory-evolution, to guide the evolutionary process. The integration of multiple modules could also improve the interpretability of the optimization process, making it easier to interact with the  system."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* DelvePO achieves better performance compared with previous baselines and ablation study shows the effectiveness of each component in the method.\n* This paper is well-written and the motivation is clear, meaningful."}, "weaknesses": {"value": "* Datasets and tasks selected are classical, relatively easy tasks for LLMs and these are not difficult for current strong LLMs anymore. I'm curious about the performance of DelvePO on more challenging and difficult tasks in LLM-era, like GSM8k, BBH, more reasoning tasks and so on. \n* This paper introduces memory and in essence, memory appears as concluding insights from last-generation prompts, which is a little far-fetched. OPRO[1] gives previous good-performing prompts and worse prompts to guide generation, APO[2] gives bad cases to guide optimization, all of which can be explained as \"memory\". I disagree with the statement \"DelvePO is the first one to introduce memory\", instead, it seems that DelvePO first explains such optimization as \"memory\".\n* Though the motivation is meaningful, I don't think DelvePO solves the problem pointed out, i.e. local optimum. Jumping out of local optimum has not been proved quantitively, I'm not convinced of the claim.\n## references \n[1] Yang, Chengrun, et al. \"Large language models as optimizers.\" The Twelfth International Conference on Learning Representations. 2023.\n[2] Pryzant, Reid, et al. \"Automatic Prompt Optimization with\" Gradient Descent\" and Beam Search.\" The 2023 Conference on Empirical Methods in Natural Language Processing."}, "questions": {"value": "* Could the authors provide ad detailed example of $M_{components}$\n* In table 6, the cost of DelvePO is relatively high. The experimental proformances compared with baselines under same costs should be investigated further.\n* See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BPVuBnWk2S", "forum": "us7gZCoTZ0", "replyto": "us7gZCoTZ0", "signatures": ["ICLR.cc/2026/Conference/Submission15620/Reviewer_y3qT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15620/Reviewer_y3qT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999165936, "cdate": 1761999165936, "tmdate": 1762925886364, "mdate": 1762925886364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DelvePO, a framework for automatic prompt optimization that decouples prompts into modular components (analogous to genetic loci and alleles) and uses a working memory mechanism to guide evolutionary operations. The framework consists of three main modules: Task-Evolution (determining which components to evolve), Solution-Evolution (performing mutation/crossover operations), and Memory-Evolution (updating component and prompt memories). Experiments are conducted on 11 datasets across 3 LLMs (DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct, GPT-4o-mini), showing improvements over baselines including APE, PromptBreeder, and EvoPrompt."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Decomposing prompts into interpretable components is valuable for understanding what makes prompts effective\n- Testing across multiple LLMs and domains demonstrates effort to validate generalizability\n- Detailed appendices with all prompts used enhance transparency\n- The working memory design that stores both component-level and prompt-level information is sensible"}, "weaknesses": {"value": "- The core contributions are incremental improvements over existing evolutionary prompt optimization methods\n- Lack of significance testing and inconsistent use of random seeds weakens confidence in reported improvements\n- The framework requires extensive prompt engineering (Sub-tasks I-II, Sub-solutions I-II, multiple scenarios) that may limit adoption\n- Practical Limitations:\n\n1. Higher computational costs than baselines\n2. Requires predefined component types that may not transfer across domains\n3. The \"case study\" reveals manual steps that contradict automation claims"}, "questions": {"value": "- Can you provide statistical significance tests (e.g., paired t-tests) comparing DelvePO against baselines across random seeds?\n- Table 4 shows concerning instability with different numbers of component values. How do you recommend practitioners set this hyperparameter?\n- The case study (Appendix I) involves manual interaction with DeepSeek Chat. How does this square with claims of a fully automated framework?\n- Can you provide ablations showing the contribution of individual components to overall performance? Which components are most important?\n- How does performance scale with the number of component types? What happens if users define 10+ components?\n- The discrete vs. continuous prompt memory distinction is unclear. Can you provide empirical comparison of these two approaches?\n- Why were different subsets of datasets used for different LLMs? This makes it difficult to draw conclusions about model-specific behaviors.\n- How sensitive is the method to the quality of initial component value generation (Figure 4)?\n- Can you provide analysis of failure modes? When does DelvePO underperform simpler baselines?\n- The framework requires many carefully crafted meta-prompts (Figures 8-14). How much prompt engineering effort went into developing these, and how transferable are they?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Rs2fzmZCTO", "forum": "us7gZCoTZ0", "replyto": "us7gZCoTZ0", "signatures": ["ICLR.cc/2026/Conference/Submission15620/Reviewer_ga5y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15620/Reviewer_ga5y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015256937, "cdate": 1762015256937, "tmdate": 1762925886044, "mdate": 1762925886044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DelvePO, a framework for automatic prompt optimization that decouples prompts into modular components and uses a working memory mechanism to guide evolutionary operations. The framework consists of three main modules: (1) Task-Evolution, determining which components to evolve;  (2) Solution-Evolution, performing mutation/crossover operations, and (3) Memory-Evolution, updating component and prompt memories. Experiments are conducted on multiple datasets across various LLMs, showing improvements over baselines including APE, PromptBreeder, and EvoPrompt."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Decomposing prompts into interpretable components is valuable for understanding what makes prompts effective\n- Testing across multiple LLMs and domains demonstrates effort to validate generalizability\n- Detailed appendices with all prompts used enhance transparency and reproducibility \n- The working memory design that stores both component-level and prompt-level information is sensible"}, "weaknesses": {"value": "- The core contributions are incremental improvements over existing evolutionary prompt optimization methods\n- Lack of significance testing and inconsistent use of random seeds weakens confidence in reported improvements, e.g., a single random seed for baseline, PromptBreeder, how do you pick up the random seed? \n- The framework requires extensive prompt engineering that may limit adoption.\n- Practical Limitations: (1) Higher computational costs than baselines; (2) Requires predefined component types that may not transfer across domains; (3) The case study involves manual interaction with DeepSeek Chat, is that against the claims of a fully automated framework?"}, "questions": {"value": "- Can you provide statistical significance tests (e.g., paired t-tests) comparing DelvePO against baselines across random seeds?\n- Table 4 shows concerning instability with different numbers of component values. How do you recommend practitioners set this hyperparameter?\n- Can you provide ablations showing the contribution of individual components to overall performance? Which components are most important? How does performance scale with the number of component types? What happens if users define 10+ components?\n- The discrete vs. continuous prompt memory distinction is unclear. Can you provide empirical comparison of these two approaches?\n- Why were different subsets of datasets used for different LLMs? This makes it difficult to draw conclusions about model-specific behaviors.\n- How sensitive is the method to the quality of initial component value generation (Figure 4)?\n- Can you provide analysis of failure modes? When and why does DelvePO underperform simpler baselines?\n- The framework requires many carefully crafted meta-prompts (Figures 8-14). How much prompt engineering effort went into developing these, and how transferable are they?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Rs2fzmZCTO", "forum": "us7gZCoTZ0", "replyto": "us7gZCoTZ0", "signatures": ["ICLR.cc/2026/Conference/Submission15620/Reviewer_ga5y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15620/Reviewer_ga5y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015256937, "cdate": 1762015256937, "tmdate": 1763669750165, "mdate": 1763669750165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}