{"id": "LVvBvvqoL3", "number": 11590, "cdate": 1758202213410, "mdate": 1759897565943, "content": {"title": "Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning", "abstract": "Estimating the quality of register transfer level (RTL) designs is crucial in the electronic design automation (EDA) workflow, as it enables instant feedback on key performance metrics like area and delay without the need for time-consuming logic synthesis. While recent approaches have leveraged large language models (LLMs) to derive embeddings from RTL code and achieved promising results, they overlook the structural semantics essential for accurate quality estimation. In contrast, the control data flow graph (CDFG) view exposes the design's structural characteristics more explicitly, offering richer cues for representation learning. In this work, we introduce StructRTL, a novel structure-aware graph self-supervised learning framework for improved RTL design quality estimation. By learning structure-informed representations from CDFGs, StructRTL significantly outperforms prior art on various quality estimation tasks. To further boost performance, we incorporate a knowledge distillation strategy that transfers low-level insights from post-mapping netlists into the CDFG-based predictor. Experimental results demonstrate that StructRTL establishes new state-of-the-art results, highlighting the effectiveness of combining structural learning with cross-stage supervision.", "tldr": "", "keywords": ["RTL Quality Estimation", "Graph Self-Supervised Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cddaf54c0fa4a42e4f69154bd08123c964691072.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces StructRTL, a structure-aware graph self-supervised learning framework. Unlike token-based LLM embeddings, StructRTL operates on Control Data Flow Graphs (CDFGs) to explicitly capture structural semantics. It employs two self-supervised tasks, structure-aware masked node modeling and edge prediction, and further incorporates knowledge distillation from post-mapping netlists to enhance RTL quality estimation. Experimental results demonstrate notable improvements over both LLM-based and graph-based baselines in predicting area and delay."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work presents a successful demonstration of tailoring architectural design and pretraining strategies for the RTL domain by leveraging CDFG inputs, a Transformer backbone, and dedicated pretraining tasks.\n\n2. It conducts comprehensive experiments, including ablation studies and knowledge distillation analyses, showing consistent improvements over prior methods.\n\n3. The open-source release of the complete pipeline and the constructed dataset could be a useful contribution to the research community."}, "weaknesses": {"value": "1. The work demonstrates limited technical novelty, as it primarily integrates existing techniques such as graph masked autoencoding and knowledge distillation into the RTL quality estimation context without introducing fundamentally new learning principles.\n\n2. It remains unclear whether the proposed framework is inherently specific to the RTL domain or if its structural learning principles can generalize to broader code or graph representation learning tasks. The paper lacks a discussion of transferable insights or contributions to the general ML community.\n\n3. The ablation study is basic and does not adequately explain why each pretraining component contributes to the observed improvements, nor how the learned structural representations differ qualitatively from token-based embeddings.\n\n4. The experiments focus solely on area and delay prediction, without exploring other hardware-related metrics or evaluating the method’s generalization to unseen hardware designs.\n\n5. The contributions are largely application-driven within the EDA field, offering limited theoretical or methodological advances that would engage the broader representation learning or ML research community."}, "questions": {"value": "My questions have been included in the weakness section.  I'm willing to adjust my scores if my concerns are properly addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ojkGSYq3br", "forum": "LVvBvvqoL3", "replyto": "LVvBvvqoL3", "signatures": ["ICLR.cc/2026/Conference/Submission11590/Reviewer_bNQe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11590/Reviewer_bNQe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989830810, "cdate": 1761989830810, "tmdate": 1762922673816, "mdate": 1762922673816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on representation learning of the RTL (register transfer level) representation of digital electronic design with the goal of design quality estimation. This StructRTL approach utilizes the control data flow graph (CDFG) of the RTL code which is obtainable from compiler tools. It uses a graph neural network (GNN) + transformer encoder. The three pretraining tasks that make this approach work by creating strong representations is 1) masked node modeling 2) edge predictions 3) knowledge distillation of fine-grained circuit synthesis insights."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Very strong experimental results.\n* Pretraining tasks are sounds and essential for representation learning, leading to the strong downstream task result of overall quality estimation."}, "weaknesses": {"value": "* This work implements various ideas well but is ultimately heavily inspired by prior work. Graph representation (GraphMAE, MaskGAE) and fine-grained knowledge distillation (VeriDistill)."}, "questions": {"value": "* What is your insight on data scale. In the software domain, when trained on vast amounts of data, LLMs seem to perform strongly with just token representation of code. If there was large scale data available for RTL code, would the gap between token based representation and StructRTL be slimmer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cTNsvFtETx", "forum": "LVvBvvqoL3", "replyto": "LVvBvvqoL3", "signatures": ["ICLR.cc/2026/Conference/Submission11590/Reviewer_UDHU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11590/Reviewer_UDHU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000002916, "cdate": 1762000002916, "tmdate": 1762922673244, "mdate": 1762922673244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "StructRTL is a  framework for predicting RTL hardware design quality (area/delay) without requiring slow logic synthesis. The method uses Control Data Flow Graphs (CDFGs) with graph neural networks and incorporates knowledge distillation from post-synthesis netlists. It combines GNN processing with Transformer encoding and employs two self-supervised pretraining tasks. The approach achieves state-of-the-art results on the OpenABC-D dataset, significantly outperforming existing methods including traditional ML models, graph-based approaches, and LLM-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "a. State-of-the-Art Performance: Clear and substantial outperformance across multiple quality metrics (area R²=0.8676, delay R²=0.8872) on a large, modern dataset, providing strong evidence for the structure-aware approach's superiority.\n\nb. Rigorous Experimental Validation: Comprehensive ablation studies systematically validate each architectural component (GNN backbone, pretraining tasks, positional embeddings), proving their individual contributions are critical and non-redundant.\n\nc. Effective Technical Integration: Successfully combines advanced techniques (graph learning, self-supervision, knowledge distillation) into a cohesive pipeline that meaningfully incorporates low-level physical design insights.\n\nd. Practical Impact: Demonstrates massive speedup over traditional synthesis-based methods while maintaining accuracy, showing clear real-world applicability for accelerating hardware design workflows."}, "weaknesses": {"value": "a. Limited Generalization Assessment: All experiments confined to OpenABC-D dataset; no evaluation on other RTL benchmarks or real-world proprietary designs raises questions about performance on diverse circuits outside the training distribution.\n\nb. Insufficient Knowledge Distillation Analysis: The source of the teacher model's superiority isn't deeply explored; missing ablation comparing netlist-trained vs CDFG-trained teachers to validate claims about \"low-level insights.\"\n\nc. Indirect LLM Comparison: Post-hoc comparison to LLM methods like VeriDistill; would benefit from more direct comparison such as fine-tuning the same LLM on CDFG representations for stronger evidence.\n\nd. Pipeline Complexity Concerns: Multi-stage pipeline (CDFG construction, GNN processing, Transformer encoding) lacks discussion of computational costs and training complexity, potentially creating adoption barriers compared to simpler approaches."}, "questions": {"value": "Q1.\tGeneralization: How does StructRTL perform on other RTL benchmarks beyond OpenABC-D? Have you tested on proprietary or more complex designs?\n\nQ2. \tKnowledge Distillation Deep Dive: Can you provide ablations comparing different teacher model configurations (netlist-based vs CDFG-based) to better isolate the value of low-level insights?\n\nQ3.\tDirect LLM Comparison: Would fine-tuning existing LLMs on CDFG representations provide a more direct comparison to validate the structural approach's superiority?\n\nQ4.\tComputational Analysis: What are the training time, memory requirements, and computational costs of the full pipeline compared to simpler baseline methods?\n\nQ5\tScalability: How does performance scale with larger, more complex designs? Are there practical limits to the CDFG approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8K5eRpOEI2", "forum": "LVvBvvqoL3", "replyto": "LVvBvvqoL3", "signatures": ["ICLR.cc/2026/Conference/Submission11590/Reviewer_GoD5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11590/Reviewer_GoD5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762176246609, "cdate": 1762176246609, "tmdate": 1762922672555, "mdate": 1762922672555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes StructRTL, a structure-aware graph self-supervised learning framework for estimating quality metrics (area and delay) of Register Transfer Level (RTL) hardware designs. The method operates on control data flow graphs (CDFGs) and uses two pretraining tasks namely structure-aware masked node modeling and edge prediction to train a model for RTL quality prediction. Additionally, knowledge distillation from post-mapping netlists is incorporated to improve performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Correctly predicting design metrics at an earlier stage in the design cycle is very useful since it can reduce the overall design cycle time.\n2. Using CDFGs is well motivated.\n3. The paper is generally well-written and easy to follow."}, "weaknesses": {"value": "1. while larger than some prior work, however the current dataset size is still small.\n2. 80% of the designs have less than 600 nodes which raises concerns about whether the method scales to real life designs and whether the current dataset truly represents real-world complexity.\n3. Proposed knowledge distillation requires running synthesis to get the netlists. Scalability will become an issue for big designs."}, "questions": {"value": "1. What kind of designs did you use for creating the dataset? Please specify the design types.\n2. How does the technique perform for combinational circuits and sequential circuits? How much difference do you see in the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "46AIbuF2HL", "forum": "LVvBvvqoL3", "replyto": "LVvBvvqoL3", "signatures": ["ICLR.cc/2026/Conference/Submission11590/Reviewer_Nnbc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11590/Reviewer_Nnbc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762589139226, "cdate": 1762589139226, "tmdate": 1762922672205, "mdate": 1762922672205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}