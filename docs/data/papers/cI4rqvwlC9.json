{"id": "cI4rqvwlC9", "number": 2407, "cdate": 1757075228090, "mdate": 1763749690243, "content": {"title": "Incremental Learning of Sparse Attention Patterns in Transformers", "abstract": "This paper studies simple transformers on a high-order Markov chain, where the model must incorporate knowledge from multiple past positions, each with different statistical importance.\nWe show that transformers learn the task incrementally, with each stage induced by the acquisition or copying of information from a subset of positions via a sparse attention pattern.\nNotably, the learning dynamics transition from competitive, where all heads focus on the statistically most important attention pattern, to cooperative, where different heads specialize in different patterns.\nWe explain these dynamics using a set of simplified differential equations, which characterize the stage-wise learning process and analyze the training trajectories.\nAs transformers progress through these stages, they climb a complexity ladder defined via simpler misspecified hypothesis classes until reaching the full model class.\nOverall, our work provides theoretical explanations for how transformers learn in stages even without an explicit curriculum and provides insights into the emergence of complex behaviors and generalization, with relevance to applications such as natural language processing and algorithmic reasoning.", "tldr": "", "keywords": ["incremental learning", "transformers", "optimization", "dynamics", "generalization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/283fb41f9fd2c416fe2c81a68ca02db1c74d2b08.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper analyzes how simple Transformers learn on a high-order Markov-chain task where the next token depends on multiple past positions with different statistical importance. The authors observe stage-wise learning: attention first concentrates on the statistically most important positions, then progressively adds less important ones as training proceeds. The dynamics transition from a competitive phase, where all heads chase the same dominant pattern, to a cooperative phase, where different heads specialize in distinct sparse attention patterns. They formalize these behaviors with simplified differential equations for a minimal single-layer multi-head architecture and connect the analysis to tensor factorization. They also study a regression variant and derive gradient-flow dynamics that reproduce the incremental stages. Finally, they report generalization effects tied to dataset size and early stopping, arguing that trajectory-induced misspecification can be beneficial in low-data regimes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### Clear minimal setting that isolates sparse attention\n\nBy crafting a high-order Markov process with block-structured lags and importance weights, the paper cleanly isolates when sparse attention should emerge. The construction $I(1),\\dots,I(h)$ partitions the past and the norm ordering $\\lVert A_1^\\star\\rVert \\ge \\cdots \\ge \\lVert A_h^\\star\\rVert$ creates a natural “importance ladder,” making stage boundaries interpretable and reproducible. This design sharpens causal attributions between task structure and learned attention patterns. \n\n### Mechanistic account of competitive to cooperative dynamics\n\nA notable empirical finding is that heads initially compete for the same dominant position, then shift to cooperation as specialization emerges across heads. The paper documents this transition with attention maps and with a KL-divergence probe that tracks how many required patterns the model has acquired at any time. This mechanistic narrative clarifies how multi-head attention distributes roles across heads as training advances.\n\n### Simplified differential-equation modeling\n\nThe authors complement experiments with a simplified ODE analysis of a minimal single-layer, multi-head model. The resulting coupled dynamics explain the competitive phase under symmetric initialization and illuminate how the cooperative phase arises. This pairing of gradient-flow equations with observations provides a compact theoretical handle on stage-wise emergence.\n\n### Connection to factorization viewpoints\n\nBy reducing the problem to structured predictors and aligning the updates with tensor or matrix factorization intuitions, the paper relates attention-pattern acquisition to well-studied decomposition problems. This link helps interpret why heads specialize on different subsets and why importance ordering drives the sequence of stages.\n\n### Generalization insights via training-trajectory regularization\n\nThe study on dataset size reports that smaller datasets yield fewer learned stages, effectively selecting shorter effective context and sparser copying behaviors. The authors interpret this as trajectory-induced regularization and show that early stopping can choose a beneficial misspecification level in low-data regimes. This gives a practical takeaway about tuning training length and data scale to trade off bias and variance."}, "weaknesses": {"value": "### Synthetic scope and external validity\n\nAll main results are on synthetic high-order Markov data with one-hot vocabularies, blocky lag sets $I(k)$, and fixed feature matrices $A_k^\\star$. While this isolates mechanisms, it leaves open how strongly the conclusions transfer to natural language or other real data with heterogeneous, long-range and overlapping dependencies. The paper gestures at relevance, yet concrete tests beyond the synthetic regime are limited.\n\n\n### Strong simplifying assumptions in the theory\n\nThe regression analysis relies on assumptions such as orthogonality of normalized features and model minimality, which streamline the ODEs but narrow applicability. It is unclear how robust the predicted stages and transition timings are when these assumptions are relaxed, for example with correlated features or deeper stacks.\n\n\n### Limited architectural and optimization diversity\n\nMost analysis targets a single-layer, decoder-style, multi-head setup with symmetric initialization. The paper does not deeply explore how depth, alternative positional encodings, optimizer choices, or regularizers perturb the competitive to cooperative transition. This reduces guidance for practitioners calibrating real training runs.\n\n\n### Relation to Zucchet et al., (2025) [1] is unclear\n\nI think Zucchet et al., (2025) [1] is a relevant paper, but the relation is not sufficiently discussed in the paper, although the authors cite it in Section 3.2.\n\n\n[1] Zucchet, Nicolas, et al. \"The emergence of sparse attention: impact of data distribution and benefits of repetition.\" arXiv preprint arXiv:2505.17863 (2025)."}, "questions": {"value": "- In your data-generation process $x_t \\sim \\mathrm{softmax} \\Big( \\sum_{k=1}^{h} A_k^\\star \\sum_{i\\in I(k)} \\alpha_i, x_{t-i}\\Big)$ with disjoint lag blocks $I(k)$, how sensitive are the stage boundaries to relaxing the disjointness assumption, for example when $I(k)$ overlap or when importance weights $\\alpha_i$ are heavily skewed or noisy?\n\n- Please clarify the relation to Zucchet et al., (2025) [1]."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yhhaCvgXXB", "forum": "cI4rqvwlC9", "replyto": "cI4rqvwlC9", "signatures": ["ICLR.cc/2026/Conference/Submission2407/Reviewer_ShLm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2407/Reviewer_ShLm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809774712, "cdate": 1761809774712, "tmdate": 1762916224658, "mdate": 1762916224658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the learning dynamics of single-block, decoder-only Transformers, focusing on the emergence of sparse attention circuits as network learns to predict sequence tokens that are influenced by a mixture of other token chunks in history. The authors empirically observe that attention heads initially converge on the most statistically dominant pattern and, over time, progressively specialize in less frequent ones. \nThe paper develops a theoretical framework to explain this phenomenon. Starting from a simplified regression setup, it derives differential equations characterizing the gradient dynamics of multi-head attention learning. The analysis reveals that the process can be viewed as a low-rank tensor regression problem, where the model effectively factorizes a target tensor (the ground truth) into rank-1 components. The dynamics of the network explain that all heads compete for the same dominant pattern during training; later, the learning dynamics transition from competitive to cooperative, leading to functional specialization across heads."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The figures are thoughtfully designed and effectively convey the key results.\n* The theoretical development is clear and satisfying: it provides a principled dynamical systems view of an empirically observed phenomenon.\n* The connection between optimization dynamics and low-rank tensor factorization is elegant and offers intuitive insight into head specialization."}, "weaknesses": {"value": "1. Missing Discussion of Generalization.  The abstract mentions “generalization,” yet this topic is not revisited in the main text.\n2. Limited Scope of Contribution. The first stated contribution—analyzing a simplified, single-layer model—should be reframed as a limitation rather than a contribution. It remains unclear how the conclusions would extend to deeper or larger-scale architectures or more generalized type of sequences. \n3. Unclear Cognitive Relevance. The setup—modeling the next token as a weighted superposition of prior chunks of tokens—deviates substantially from linguistic or cognitive formulation of hierarchical structures in sequences. Language is not well approximated by a linear summation of token embeddings.\n    * How canonical is this formulation for studying sequences in general?\n    * How does it compare to more structured formulations of sequences (e.g., Wu et al., NeurIPS 2022; Wu et al., ICLR 2025)?"}, "questions": {"value": "1. Copying vs. Reuse.  Lines 38–41 mention compositionality and the reuse of sub-components. Please clarify the distinction between copying, reusing, and re-binding (cf. Wu 2022, Wu 2024 ICLR) and how these mechanisms relate to the learning dynamics observed here.\n2. Conceptual Connection to Prior Work.  The relation to recent work such as Zucchet et al. on compositional knowledge formation should be discussed. How does the present theory complement or differ from these accounts?\n3. Interpretation of Results. In Fig. 3, why does the KL divergence for A A1∗ increase after the first training stage, given that it measures the divergence between the ground truth and a transformer with unrestricted context length? \n\n\nReference:\n \n_Zucchet, N., D’Angelo, F., Lampinen, A. K., & Chan, S. C. Y. (2025). The emergence of sparse attention: impact of data distribution and benefits of repetition. arXiv preprint arXiv:2505.17863._\n\n_Wu, S., Thalmann, M., Dayan, P., Akata, Z., & Schulz, E. (2025). Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences. The Thirteenth International Conference on Learning Representations (ICLR 2025)._\n\n_Wu, S., Élteto, N.,Dasgupta, I., & Schulz, E. (2022). Learning Structure from the Ground-up—Hierarchical Representation Learning by Chunking. 36th Conference on Neural Information Processing Systems (NeurIPS 2022)._"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4c1zTxFy0K", "forum": "cI4rqvwlC9", "replyto": "cI4rqvwlC9", "signatures": ["ICLR.cc/2026/Conference/Submission2407/Reviewer_dRyx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2407/Reviewer_dRyx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944989574, "cdate": 1761944989574, "tmdate": 1762916224497, "mdate": 1762916224497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how a single-layer transformer learns sparse attention patterns for next-token prediction in a synthetic task where tokens depend on non-overlapping blocks of past tokens with varying importance. Through experiments, the authors observe an incremental learning dynamic: the model first enters a competitive phase where all attention heads focus on the highest-norm block, followed by a cooperative phase where heads gradually specialize to attend to lower-norm blocks. They support this finding with a theoretical analysis that reduces the training dynamics to a tensor factorization problem, proving convergence properties for the competitive phase. Additional experiments explore the impact of dataset size, initialization scale, and block importance hierarchy on generalization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The synthetic task cleanly isolates how transformers learn hierarchical sparse dependencies, providing a tractable testbed for analyzing training dynamics.\n\n2. The theoretical analysis offers a principled explanation for the competitive phase, complementing empirical observations.\n\n3. Experiments on dataset size reveal that limited data induces implicit regularization(learning fewer blocks), deepening understanding of generalization in data-scarce regimes."}, "weaknesses": {"value": "1. While previous works like [1] have studied how transformers learn causal structure, this paper provides a more detailed analysis of the training dynamics; however, most of the analysis remains experimental, and the finding that the model first learns to attend to the most important tokens and then refines the pattern seems intuitive and not surprising.\n\n2. The theoretical analysis lacks a clear explanation; for example, V(t) and s(t) are not defined, making it hard to grasp the main statement of each theorem.\n\n3. Conclusions are derived from an idealized setup (single-layer transformer, synthetic Markov data). It remains unclear if dynamics hold in deeper architectures, with natural data, or under standard techniques like layer normalization.\n\n[1] Nichani, Eshaan, Alex Damian, and Jason D. Lee. \"How transformers learn causal structure with gradient descent.\" arXiv preprint arXiv:2402.14735 (2024)."}, "questions": {"value": "Please refer to the weakness part. If there are any misunderstandings on my part, please point them out, and I will reconsider my evaluation of this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3eyQRq5FqY", "forum": "cI4rqvwlC9", "replyto": "cI4rqvwlC9", "signatures": ["ICLR.cc/2026/Conference/Submission2407/Reviewer_4xLi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2407/Reviewer_4xLi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984626416, "cdate": 1761984626416, "tmdate": 1762916224172, "mdate": 1762916224172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a synthetic task to study how transformers incrementally learn features of varying importance during training. The task is inspired by n-gram or Markov-chain settings, but the relative importance of previous tokens is explicitly controlled through feature/weight matrices with different norms.\n\nEmpirically, the authors train transformers on this task and show that the models learn in stages: first capturing the most important features or positions, then gradually incorporating less important ones. During this process, attention heads initially all compete to represent the most important features, but later each specializes in distinct subsets of positions according to their importance. The paper also includes ablation studies analyzing the effects of initialization scale, the feature scale, and dataset size.\n\nTheoretically, the authors analyze a regression variant of the task trained with MSE loss with a simplified architecture. They study the gradient flow dynamics and show that, under symmetric initialization, all heads focus on the most important positions, whereas small deviations from symmetry lead different heads to take different paths."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-motivated, and the problem setup is clearly presented. The task design is well-suited for studying learning dynamics in a simplified and controlled testbed that also lends itself to theoretical analysis (though the theoretical part later relies on additional simplifications).\n\nFor the proposed task, the empirical study effectively reveals and visualizes the inner mechanisms and learning dynamics, offering nice insight into how attention heads specialize during training.\n\nThe overall presentation is clear and well-structured, with some exceptions discussed later."}, "weaknesses": {"value": "- The theoretical analysis is conducted on a much simpler setup than the synthetic task used in the empirical study. This level of simplification is not inherently problematic for a theoretical treatment, provided the simplified model replicates the key behaviors and offers a tractable framework for analysis. However, the theory presented here is only partial: it focuses solely on the initial stage of training, where all heads learn the same pattern under specific initialization assumptions. In addition, the presentation of the theory section could be improved for clarity (see questions below).\n\n- The paper could more clearly situate its findings in relation to prior work. For instance, the Markov-chain setup of Edelman et al. (2024) can be viewed as a special case of the proposed task (e.g., with $h=1$ and all $\\alpha$ values equal), where all positions have the same importance. Yet even in that setting, stage-wise learning progression is observed. Here, however, the stage-wise progression appears mainly due to $h>1, m>1$ (the difference in positional importance), and the positions with the same importance, say all positions in $I(1)$, seem to be learned simultaneously and not stage-wise anymore.\n\n    There are also two more papers that study similar synthetic task studies for training dynamics, which are relevant to cite. \n\n    [1] Zhou et al. “Transformers Learn Variable-Order Markov Chains In-Context.”\n\n    [2] Deora et al. “In-Context Occam’s Razor: How Transformers Prefer Simpler Hypotheses on the Fly.”"}, "questions": {"value": "1. In all experiments, the $\\alpha$ values are chosen uniformly within each group $I(k)$. What would be the impact of using non-uniform $\\alpha$ on the training dynamics? Would that also contribute to the stage-wise learning behavior?\n\n2. In Figure 4 (left), what is the baseline curve?\n\n3. The notation in Section 3 is a bit confusing. (i) Is $d_i$ (line 378) defined? (ii) Since $V_k$ and $s_k$ are trainable parameters, shouldn’t they carry a time index $t$ in Theorem 2 (first equation) and Theorem 3? \n\n4. Could you clarify Figure 10? What do the three plotted features and positions represent?\n\n5. In Theorem 1, does the fact that $m_1*$ is the largest (specifying the most important position) play a role in the proof, or is the alignment of the fixed point with $V_1^*$ solely a result of the initialization assumption (6)?\n\n6. Regarding Section 3.3, how does the theoretical setup demonstrate the cooperation phase? Theorem 3 still shows that all heads remain close to each other for a finite time, but there doesn’t seem to be a result describing what happens afterward, is that correct?\n\n*Minor:*\n- In Section 2.2, it would be helpful to mention that $w=12$ in the main text (currently stated only in the Appendix) so that the context lengths $c$ in paragraph 186 are clearer.\n- In line 195, should it say **un**restricted context length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kAGeke1yHt", "forum": "cI4rqvwlC9", "replyto": "cI4rqvwlC9", "signatures": ["ICLR.cc/2026/Conference/Submission2407/Reviewer_yNiz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2407/Reviewer_yNiz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993625444, "cdate": 1761993625444, "tmdate": 1762916223657, "mdate": 1762916223657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}