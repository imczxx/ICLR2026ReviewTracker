{"id": "7n0at94O2H", "number": 10659, "cdate": 1758178861298, "mdate": 1763061766197, "content": {"title": "HSImul3R: Reconstructing Simulation-Ready Human-Scene-Interaction from Sparse Views", "abstract": "We present the first framework for simulation-ready 3D reconstruction of human–scene interactions (HSI) from sparse-view images. Prior approaches to 3D reconstruction are typically fragmented, focusing either on scene geometry or human motion, and rarely model their interactions. There are also recent attempts that reconstruct both jointly. However, they remain constrained by limited datasets or neglect the physical plausibility of interactions, and therefore fail to remain stable when deployed in simulators, which is a critical requirement for embodied AI. To address these challenges, we propose **HSImul3R** with three key contributions. Specifically, firstly, we introduce **contact-aware interaction modeling** to enforce realistic human-scene coupling within the unified 3D world coordinate system by aligning generative 3D priors with reconstructed geometry. Secondly, we propose a **scene-targeted reinforcement learning** which learns to stabilize interactions in simulation through dual supervision on motion fidelity and object proximity. To further improve the stability of this HSI simulation, we design **direct simulation reward optimization (DSRO)**, a reward-driven fine-tuning scheme that improves scene reconstructions by assessing stability under both gravity and interactions. To support training and evaluation, we further collect **HSIBench**, a new dataset featuring diverse objects, human motions, and interaction scenarios. Extensive experiments demonstrate that HSImul3R achieves the first stable, simulation-ready HSI reconstructions and substantially outperforms existing methods.", "tldr": "A method for reconstructing simulation-ready human-scene-interactions from uncalibrated sparse-view inputs", "keywords": ["3D reconstruction", "human-scene-interaction", "simulation", "embodied AI"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/5c1fb2b1e692114b2257f082baa554229fc13600.pdf", "supplementary_material": "/attachment/d83e153a71f1d69e6412ac2fc180fef40ce51037.zip"}, "replies": [{"content": {"summary": {"value": "HISmul3R is proposed to reconstruct simulation-read 3D human-scene interactions from sparse-view images. Built upon HSfM, explicit 3D structural priors from image-to-3D models and contact constraints between humans and scenes are injected. Then, an RL-based refining procedure is introduced to optimize the human pose. Finally, DSRO is designed to optimize the object geometry."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed pipeline is reasonable and effective, especially the DSRO part. Comprehensive experiments are conducted to validate the effectiveness of different modules. \n\n- The collected HSIBench could be a valuable data contribution.\n\n- The performance of the proposed pipeline is impressive."}, "weaknesses": {"value": "- Some technical details are missing, like the difficulty classification criteria and the training of scene-targeted RL. \n\n- In Sec. 3.4, the three criteria of DSRO are still ambiguous, especially (2) and (3). \n\n- According to Figure 2, (Sec. 3.2 + Sec. 3.4) appear to be conducted simultaneously with Sec. 3.3. Will the object reconstruction be influenced by the inaccurate and unstable human pose before simulation? \n\n- The pipeline description is ambiguous or lacking support for certain scenarios. As I understand, the object is not put into the simulator in the Section. 3.3. If so, how would the lying-in-couch case in Figure 7 be processed? If I misunderstood and the object was put into the simulator, does it mean that Sec. 3.3 and Sec. 3.4 were conducted jointly? Also, the physics material parameters and densities of the object could be critical. \n\n- Quantitative analyses of object poses are missing.\n\n- The evaluation is unclear. In Table 1, since the human poses are different, why would the penetration not change for V2-V4?\n\n- It might be better to replace V1-4 in Table 1 with abbreviations of the corresponding variances for clarity. Adding simple explanations in the table caption would also be helpful."}, "questions": {"value": "- How long does it take to reconstruct a case?\n\n- Is Eq. 5 further formulated as an extra reward of PHC? If so, the reward formulation should also be clarified.\n\n- How many steps does the scene-targeted RL take? Will the RL stop after a certain number of steps or after some criteria are met? Is it trained from scratch or from an off-the-shelf PHC checkpoint? \n\n- In the video, some shakes could be observed in the human. Will this also happen for scene-targeted RL? If it happened, what would be treated as the output human pose at this stage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DU7BJBBBro", "forum": "7n0at94O2H", "replyto": "7n0at94O2H", "signatures": ["ICLR.cc/2026/Conference/Submission10659/Reviewer_q9gg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10659/Reviewer_q9gg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832480538, "cdate": 1761832480538, "tmdate": 1762921912671, "mdate": 1762921912671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "9zEuS2auyY", "forum": "7n0at94O2H", "replyto": "7n0at94O2H", "signatures": ["ICLR.cc/2026/Conference/Submission10659/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10659/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763061765361, "cdate": 1763061765361, "tmdate": 1763061765361, "mdate": 1763061765361, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents HSImul3R, a framework for simulation-ready 3D reconstruction of human–scene interactions (HSI) from sparse-view RGB images. It builds upon HSfM (Human-Scene Structure from Motion) and introduces three new components to improve physical plausibility.\nFirst, a Contact-Aware Interaction Modeling module integrates the image-to-3D generative model MIDI and enforces geometric consistency between human and scene via contact and non-contact losses that reduce penetration and encourage realistic coupling.\nSecond, the framework adapts the Perpetual Humanoid Control (PHC) controller to refine reconstructed static poses in simulation, introducing an additional scene-proximity loss that aligns human contact points with nearby object surfaces for stable contact under gravity.\nThird, a Direct Simulation Reward Optimization (DSRO) stage fine-tunes the generative model using feedback from physics simulation: scenes that remain stable under contact are rewarded, while unstable ones are penalized, thus biasing generation toward physically valid geometries.\nThe paper also introduces HSIBench, a small benchmark dataset of 300 multi-view recordings (3 subjects, 19 household objects) for evaluating physically consistent human–scene reconstruction. Experiments show that HSImul3R achieves substantially higher physical stability and lower penetration compared to HSfM and other baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper tackles a promising direction of making 3D human–scene reconstruction simulation-ready, bridging perception and physics.\n\n2. The idea of incorporating simulation feedback into the reconstruction loop is potentially impactful.\n\n3. The introduction of the HSIBench dataset provides a useful benchmark for evaluating physically consistent human–scene interactions."}, "weaknesses": {"value": "1. DSRO is heuristic and weakly grounded. The proposed simulation reward optimization simply flips the diffusion loss sign based on a binary “stable/unstable” label from simulation, producing noisy, non-directional gradients that push samples away from unstable regions without indicating how to achieve stability. There is no theoretical justification or ablation showing that this mechanism learns a meaningful physical prior.\n\n2. Static-interaction assumption and questionable use of RL. All experiments focus on static resting poses (sitting, leaning, standing), where full physics simulation is unnecessary and simple geometric consistency checks could suffice. Forcing contact in every case often results in unnatural over-contacted configurations. Moreover, using reinforcement learning to refine a single static frame in simulation is conceptually misplaced. The supplementary videos also show noticeable jittering and unnatural ostures, suggesting limited realism and stability.\n\n3. Pipeline over-integration with limited novelty. The overall framework stitches together existing components (DUSt3R, HSfM, MIDI, PHC) with additional losses, but lacks a clear algorithmic innovation.\n\n4. Missing discussion of relevant prior work. The paper overlooks several closely related studies in human–scene reconstruction and contact modeling, such as Resolving 3D Human Pose Ambiguities with 3D Scene Constraints (ICCV 2019), Learning Motion Priors for 4D Human Body Capture in 3D Scenes (ICCV 2021), Generating 3D People in Scenes without People (CVPR 2020), and Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes (CVPR 2021). Similarly, the use of RL for motion refinement has been explored in SFV: Reinforcement Learning of Physical Skills from Videos (SIGGRAPH ASIA 2019) and VideoMimic (CoRL 2025), which are relevant but not discussed.\n\n5. Insufficient quantitative analysis. The paper reports limited numerical evaluation, lacking comparisons with recent human–scene estimation methods and missing standard comparisons such as human pose error and scene reconstruction error. The evaluation focuses narrowly on stability percentages, leaving uncertainty about the method’s actual reconstruction accuracy and generalization capability."}, "questions": {"value": "Why not to extend to 4D reconstruction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1i1Z9NUcL3", "forum": "7n0at94O2H", "replyto": "7n0at94O2H", "signatures": ["ICLR.cc/2026/Conference/Submission10659/Reviewer_oiro"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10659/Reviewer_oiro"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858391588, "cdate": 1761858391588, "tmdate": 1762921912264, "mdate": 1762921912264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for obtaining simulation-ready 3D human-object reconstructions. The idea is interesting and the paper is overall complete, with clear figures and tables, and demonstrates the method’s effectiveness on one dataset. However, several technical details are missing, and the scope of evaluation are limited."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a good idea to produce simulation-ready reconstructions that bridge human-scene estimation and physical simulation.\n\n2. The paper is complete and reasonably well organized; figures and tables are clear and easy to follow.\n\n3. Experiments on one dataset demonstrate the method’s potential effectiveness."}, "weaknesses": {"value": "1. The paper mentions “While PHC was designed for dynamic motions, we adapt it to static poses by replicating each pose across the temporal dimension to fit the network.” This adaptation seems inefficient and might not be the best way to handle static inputs.\n\n2. Figure 2 appears to contain labeling errors: loss “non-contrast” and “contrast” seem to correspond to Eq. (3) and Eq. (4), respectively, which should be \"contact\"?\n\n3. The paper lacks clarity and important implementation details.\n\n   - For example, in Eq. (2), when the scene contains $O$ objects, it is unclear how the contact-related losses are computed. Presumably, one needs to first determine whether each object has contact with the human, but this process is not described.\n\n   - Related work on similar datasets is insufficient, and experiments are conducted on only one dataset. It would be beneficial to test on additional datasets such as CHAIRS (ICCV’23) or CORE4D (CVPR’25).\n\n   - Some of the claimed contributions appear to be incremental, eg, adding additional loss terms to existing techniques. While this leads to some improvement, the conceptual contribution seems limited."}, "questions": {"value": "In the demo video, is the shaking humanoid supposed to show the optimization process within the simulation environment? Without narration or explanation, it is a bit difficult to understand."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DjDYFrvKaj", "forum": "7n0at94O2H", "replyto": "7n0at94O2H", "signatures": ["ICLR.cc/2026/Conference/Submission10659/Reviewer_YWhf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10659/Reviewer_YWhf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893919601, "cdate": 1761893919601, "tmdate": 1762921911933, "mdate": 1762921911933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper recovers simulation-ready 3D reconstruction of human-scene interactions from 4 uncalibrated sparse-view images. First, the 3D scene and human pose are reconstructed via HSfM. The objects in the scene are segmented and replaced with generated image-to-3D meshes produced by MIDI. A contact objective is used to enforce non-interpenetration while encouraging that the human body part nearest to the object is in contact. Finally, the human pose is retargeted to IsaacGym simulation via PHC. To improve the stability of the reconstructed 3D meshes, DSRO is used to fine-tune MIDI to produce more stable 3D meshes (which remain upright and stable while experiencing interaction throughout the simulation)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem of stable human-scene reconstruction from 4 sparse-view images is an important problem.\n- The proposed innovations (3d mesh reconstruction, contact optimization, and simulation reward fine-tuning of MIDI) are well-motivated and effectively help improve the stability of the human-scene reconstruction."}, "weaknesses": {"value": "- Although the pipeline works, it does not have a very high success rate (especially on medium and hard interaction scenarios).\n- The presentation of the qualitative results could be improved (e.g. explaining what the demo video is showing, and removing the \"Visual Studio Code is not responding\" screens from the demo video)\n- The collected HSIBench dataset seems to be missing important details in the paper (e.g. how are the ground-truth geometry and human motions obtained for the evaluation in Tab. 1 and Tab. 2? What modalities are collected - e.g. RGB or RGBD? )\n- Although the paper shows results on the self-captured HSIBench dataset, qualitative video comparisons of the proposed method compared to baselines (such as HSfM) would help illustrate the performance of the method."}, "questions": {"value": "- What is the supplementary demo video showing? Why is the humanoid moving in the supplementary demo video when the reconstruction is of a static scene? Is it showing the retargeting or contact optimization over time?\n- How is the Stability-HSI metric defined? Is it the same as the DSRO optimization objective? If so, it is expected that fine-tuning on DSRO directly would improve Stability-HSI metric."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O5DEthNAvh", "forum": "7n0at94O2H", "replyto": "7n0at94O2H", "signatures": ["ICLR.cc/2026/Conference/Submission10659/Reviewer_G5vk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10659/Reviewer_G5vk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952039136, "cdate": 1761952039136, "tmdate": 1762921911620, "mdate": 1762921911620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}