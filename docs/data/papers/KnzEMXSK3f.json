{"id": "KnzEMXSK3f", "number": 14043, "cdate": 1758227530124, "mdate": 1759897393934, "content": {"title": "Regression Language Models for Code", "abstract": "We study **code-to-metric regression**: predicting numeric outcomes of code executions, a challenging task due to the open-ended nature of programming languages. While prior methods have resorted to heavy and domain-specific feature engineering, we show that a single unified Regression Language Model (RLM) can simultaneously predict directly from text, (i) the memory footprint of code across multiple high-level languages such as Python and C++, (ii) the latency of Triton GPU kernels, and (iii) the accuracy and speed of trained neural networks represented in ONNX. In particular, a relatively small 300M parameter RLM initialized from T5Gemma, obtains $>$0.9 Spearman-rank on competitive programming submissions from APPS, and a single unified model achieves $>$0.5 average Spearman-rank across 17 separate languages from CodeNet. Furthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five classic NAS design spaces previously dominated by graph neural networks, and simultaneously predict architecture latencies on numerous hardware platforms.", "tldr": "We introduce a regression language model that can predict, from text descriptions, code execution memory and latencies, in addition to neural network accuracies.", "keywords": ["LLM", "Decoding-based regression", "Code", "Regression", "LLMs For Code"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a4b3c13ff7c4ad0b57822993af219de1169fbb85.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Regression Language Models (RLMs) that treat code-to-metric prediction as text-to-text regression with an encoder–decoder LM (initialized from T5Gemma). The same model predicts (i) peak memory for Python/C/C++ programs (APPS, CodeNet), (ii) Triton kernel latency, and (iii) NAS metrics (accuracy and multi-hardware latency) from ONNX text dumps, using a custom numeric tokenizer (P10) and constrained decoding. On APPS memory the model achieves ρ≈0.93, on CodeNet multiple languages ρ≈0.35–0.75, Triton latency ρ≈0.52, and on NAS Kendall-τ it slightly edges a strong FLAN baseline on average. The paper also shows multi-objective decoding (accuracy→latency chains), and ablations on pretraining, tokenization, head choice (decoder vs. regression head), and context length."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "One small-ish (~300M) RLM covers diverse inputs (source code, ONNX graphs) and outputs (accuracy, memory, latency), reducing feature engineering and task-specific heads.\n\nStrong APPS memory (ρ≈0.93), respectable CodeNet across 24 languages, non-trivial Triton latency, and average Kendall-τ on five NAS spaces matching/exceeding FLAN without zero-cost proxies."}, "weaknesses": {"value": "Hardware coverage is narrow for Triton (single A6000); latency predictors can be highly hardware-sensitive. Broader devices or cross-device generalization would strengthen claims.\n\nCodeNet regime mixes train/test questions (few-shot per problem). Authors acknowledge this limits zero-shot difficulty; results may over-estimate real-world generalization to new problems with unseen inputs."}, "questions": {"value": "How many samples per input are used at inference, and how are means/medians chosen? Did you evaluate calibration or UQ quality (CRPS/NLL) when using the density view? \n\nThe ONNX graphs can be long; what fraction of graphs truncate at 1k/2k/4k tokens, and how does that affect τ/ρ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2qZ1bxLtPn", "forum": "KnzEMXSK3f", "replyto": "KnzEMXSK3f", "signatures": ["ICLR.cc/2026/Conference/Submission14043/Reviewer_NB5z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14043/Reviewer_NB5z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491302829, "cdate": 1761491302829, "tmdate": 1762924531806, "mdate": 1762924531806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Regarding ICL with flagship models"}, "comment": {"value": "We thank all of the reviewers for their time spent on reading our paper in detail and the good questions raised. Below, we address a common question, which is the comparison against in-context learning (ICL).\n\nWe used the largest GPT5 variant (GPT-5 thinking) which allows a maximum 400K token context limit, and performed in-context regression by adding random examples (x,y) into the context as well as appropriate ICL-prompting. We scaled the number of examples up to the maximum token limit (~50 examples), which we term “MAX-Shot ICL”. Below is a comparison to the RLM, when performance is measured by Spearman rank:\n\n| Method/Dataset | RLM   | **GPT5-Thinking**                                      |       |        |        |                      |\n|----------------|-------|---------------------------------------------------------|-------|--------|--------|----------------------|\n|                |       | 0-Shot | 3-Shot ICL | 5-Shot ICL | 20-Shot ICL | MAX-Shot ICL (~50) |\n| Triton Kernels (Code)    | **0.562** | 0.230  | 0.216      | 0.432      | 0.465       | 0.456              |\n| APPS (Code)    | **0.900** | -0.379 | 0.119      | 0.422      | 0.633       | 0.596              |\n| DARTS (NAS)    | **0.672** | 0.074  | 0.310      | 0.430      | 0.470       | 0.544              |\n| Amoeba (NAS)   | **0.751** | -0.017 | 0.263      | 0.340      | 0.344       | 0.413              |\n\nAs you can see, despite using a much larger and presumably more capable base model (GPT5), ICL performs consistently worse than in-weight training of the RLM due to the massive reduction in contextual examples used. Furthermore, the cost to train the much smaller and more customizable RLM (40 dollars) is less than the cost to just perform inference with GPT5 (80 dollars), demonstrating our method’s cost-effectiveness.\n\nTo further elaborate on the associated costs, please find below the average and maximum tokens required for the results above. As presented, for just one inference at MAX-Shot ICL, it can cost around $0.3125 per query (excluding thinking and output token costs). For comparison, the RLM can run on a single A6000 rented at roughly the same cost for an hour, serving ~12.5 queries per second (45 thousand queries at the same cost), all while significantly out-performing ICL with flagship models.\n\n\n| Method/Dataset        | RLM (avg / max)  | 0-Shot ICL (avg / max) | 3-Shot ICL (avg / max)   | 5-Shot ICL (avg / max)    | 20-Shot ICL (avg / max)    | MAX-Shot ICL (~50) (avg / max) |\n|-----------------------|------------------|------------------------|--------------------------|---------------------------|----------------------------|---------------------------------|\n| Triton Kernels (Code) | 3223.5 / 8251    | 3223.5 / 8251          | 13986.8 / 30112          | 52787.0 / 141911          | 72719.5 / 102817           | ~272000 / ~272000              |\n| APPS    (Code)              | 1338.0 / 1521    | 1338.0 / 1521          | 4998.5 / 5255            | 7734.8 / 8876             | 26785.8 / 29854            | ~272000 / ~272000              |\n| DARTS (NAS)           | 4234.0 / 4234    | 4234.0 / 4234          | 16612.0 / 16612          | 24864.0 / 24864           | 86764.5 / 86765            | ~272000 / ~272000              |\n| Amoeba (NAS)          | 4234.0 / 4234    | 4234.0 / 4234          | 16611.8 / 16612          | 24863.8 / 24864           | 86764.5 / 86765            | ~272000 / ~272000              |"}}, "id": "vNWKu8m6vP", "forum": "KnzEMXSK3f", "replyto": "KnzEMXSK3f", "signatures": ["ICLR.cc/2026/Conference/Submission14043/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14043/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14043/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763230737761, "cdate": 1763230737761, "tmdate": 1763230737761, "mdate": 1763230737761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Regression Language Models that frame code-to-metric regression as a text-to-text generation task. Instead of relying on domain-specific feature engineering, the authors propose using a unified encoder-decoder model that reads the raw text of code or intermediate representations and autoregressively decodes the numerical metric using a special P10 tokenizer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's core idea, i.e., treating all code regression as a text-generation problem, is original and simplifies a process that traditionally requires heavy, domain-specific feature engineering.\n2. A key strength is the demonstration of a unified model successfully handling regression tasks across highly disparate inputs and predicting different metrics.\n3. The methods outperform or remain competitive with baselines."}, "weaknesses": {"value": "1. The paper's central choice to formulate regression as an autoregressive token-generation problem (using P10) instead of using a standard regression head is not convincingly justified. Have you tried using a log transform for the output within the MLP? It's also not clear if the conditional (auto-regressive) modeling provides any real benefit over a simpler multi-head approach that predicts all metrics independently from the encoder embedding.\n2. The paper motivates its approach by contrasting it with feature-engineering methods. However, it only compares against deep learning feature engineering. For tasks like memory and latency prediction, the true established baselines are often traditional static analysis tools or logic-based methods from the programming languages and compiler communities. The paper provides no comparison to these methods.\n3. The study relies exclusively on T5Gemma. This is not a common base model in modern LLM research (compared to Llama, Qwen, etc.).\n- The authors should include results based on more standard base models.\n- The paper should also report the performance of the base T5Gemma model without RLM fine-tuning to properly isolate the gains from the proposed training method.\n4. Since the task is reformulated as text-to-text generation, the most obvious baselines are missing: the performance of modern, general-purpose LLMs. How well do state-of-the-art models (e.g., GPT, Gemini, DeepSeek) perform on this task under different settings? This comparison is essential to understand if the specialized RLM is necessary or if the capability is already present in general-purpose models."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kfjb04Cfdn", "forum": "KnzEMXSK3f", "replyto": "KnzEMXSK3f", "signatures": ["ICLR.cc/2026/Conference/Submission14043/Reviewer_sjYR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14043/Reviewer_sjYR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761564231389, "cdate": 1761564231389, "tmdate": 1762924531324, "mdate": 1762924531324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Regression Language Models for code. It introduces encoder-decoder language models based on T5Gemma, fine-tuned to perform numeric regression on code-related tasks. They have discussed predicting memory footprint, latency, or model accuracy from textual code or ONNX representations. The authors claim that a single unified model can regress across diverse programming languages and the created NAS spaces, outperforming specialized graph-based regression models (e.g., GNNs, FLAN)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It is a very interesting direction for code applications: predicting performance metrics directly from code.\n2. The idea of using a unified single model across code, kernel, and graph inputs is conceptually appealing.\n3. The presentation of the paper is organized with supportive tables and illustrative examples."}, "weaknesses": {"value": "1. Novelty: I can hardly agree that the major concept is new. It uses text-to-text regression and autoregressive regression with trivial modifications.\n2. Missing details of the experiments.\na. The multi-task regression setup mixes heterogeneous tasks without normalization or loss balancing.\nb. No ablation on negative transfer or cross-domain interference.\nThe results thus cannot support the central claim that “a unified RLM generalizes across tasks.”\n3. Evaluation\na. The >0.9 Spearman on APPS is due to the dataset’s extremely small label variance.\nb. The gains in table 4 are statistically insignificant.\n4. In general, the authors are using inflated language such as “massively simplifying graph regression” and \"general-purpose universal regressor,” with no mechanistic or theoretical support."}, "questions": {"value": "1. Appendix A.3 shows that fine-tuning reduces performance due to catastrophic forgetting, and A.2 shows that multilingual pretraining hurts zero-shot languages. Do they directly contradict the paper’s claims of generality?\n2. How does the model handle tasks with vastly different output scales (e.g., bytes vs milliseconds vs accuracy)?\n3.Why compare to FLAN without describing its training details?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YBZESGdfmC", "forum": "KnzEMXSK3f", "replyto": "KnzEMXSK3f", "signatures": ["ICLR.cc/2026/Conference/Submission14043/Reviewer_e7RV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14043/Reviewer_e7RV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988014609, "cdate": 1761988014609, "tmdate": 1762924530566, "mdate": 1762924530566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unified framework called Regression Language Models (RLM) for \"code-to-metric\" regression. The core idea is to predict various numerical metrics directly from source code and other textual representations of programs, eliminating the need for manual feature engineering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Elimination of Feature Engineering: A significant advantage of the RLM is its ability to learn directly from raw text representations of code. This simplifies the prediction pipeline and makes it more adaptable to new programming languages, hardware, or model architectures.\n- Strong Empirical Results: The RLM achieves impressive performance, with a Spearman's rank correlation greater than 0.9 on predicting the memory usage of competitive programming submissions. It also performs well on predicting Triton kernel latency and achieves state-of-the-art results in Neural Architecture Search benchmarks, outperforming graph-based methods.[1][2][3][4]\nMulti-Objective Prediction: The autoregressive nature of the decoder allows for the conditional prediction of multiple metrics. This is a powerful feature for tasks like hardware co-design and compiler optimization, where trade-offs between different performance aspects need to be considered.\n- Effective Use of Pretraining: The work demonstrates the significant benefit of initializing the RLM from a pretrained language model (T5Gemma), which leads to faster convergence and better overall performance."}, "weaknesses": {"value": "- Limited Exploration of Larger Models: While the paper shows promising results with a 300M and a 600M parameter model, a more extensive analysis of how performance scales with model size would be beneficial. A marjor concern is that: modern SOTA models excel at generalizing from just a handful of examples provided in a prompt, without any weight updates. This is a far more flexible and cost-effective approach than fine-tuning. A frontier model like a hypothetical GPT-5 or Claude-4.5 could potentially become a powerful code-to-metric regressor on the fly, adapted to novel metrics or hardware simply by being shown a few examples. The paper misses the opportunity to investigate whether its proposed task can be solved \"in-context,\" which is a crucial question for practical applicability.\n\n- Dependence on Textual Representation: The model's performance is contingent on the quality and completeness of the textual representation of the code or computation graph. For complex structures, serializing them into a linear text format might lead to a loss of information."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "k2abU0KnJF", "forum": "KnzEMXSK3f", "replyto": "KnzEMXSK3f", "signatures": ["ICLR.cc/2026/Conference/Submission14043/Reviewer_AHw1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14043/Reviewer_AHw1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001156680, "cdate": 1762001156680, "tmdate": 1762924529920, "mdate": 1762924529920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}