{"id": "1orI9Cczug", "number": 4749, "cdate": 1757756824051, "mdate": 1763636893986, "content": {"title": "FUSE: Fast Semi-Supervised Node Embedding Learning via Structural and Label-Aware Optimization", "abstract": "Graph-based learning is a cornerstone for analyzing structured data, with node classification as a central task. However, in many real-world graphs, nodes lack informative feature vectors, leaving only neighborhood connectivity and class labels as available signals. In such cases, effective classification hinges on learning node embeddings that capture structural roles and topological context. We introduce a fast semi-supervised embedding framework that jointly optimizes three complementary objectives: (i) unsupervised structure preservation via scalable modularity approximation, (ii) supervised regularization to minimize intra-class variance among labeled nodes, and (iii) semi-supervised propagation that refines unlabeled nodes through random-walk-based label spreading with attention-weighted similarity. These components are unified into a single iterative optimization scheme, yielding high-quality node embeddings. On standard benchmarks, our method consistently achieves classification accuracy on par with or superior to state-of-the-art approaches, while requiring significantly less computational cost.", "tldr": "When node attributes are unavailable, node embeddings for classification can be generated in a computationally efficient manner by including modularity in the objective function.", "keywords": ["network representation learning", "node classification", "linear modularity", "label propagation"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fcc3de5828353467ccbabbe09be5274bdd9637c5.pdf", "supplementary_material": "/attachment/437bdc2332d004124cad41883e2dd72aeeb96864.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes FUSE, a fast semi-supervised node embedding framework that jointly optimizes three complementary objectives to learn informative node representations on graphs with missing node features. The learned embeddings can then be subsequently applied to downstream classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed linear-time approximation of modularity gradient optimization achieves comparable or slightly superior performance to baseline methods on most datasets while substantially reducing computational cost.\n\n2. The paper presents extensive experiments, including ablation studies, hyperparameter sensitivity analysis, scalability evaluation, and label-missingness analysis, providing a well-rounded empirical assessment of the proposed framework.\n\n3. The authors address the class imbalance among unlabeled nodes observed in real-world graphs, which enhances the adaptability of FUSE in practical deployment scenarios."}, "weaknesses": {"value": "1. It remains unclear how embeddings generated by FUSE perform on other downstream graph tasks, such as link prediction (which requires multi-hop reasoning) or community detection. Is FUSE inherently tailored to node classification, or can it generalize effectively to other objectives?\n\n2. Since annotation requires a certain cost, an important question is whether the proportion of known labels affects the quality of embeddings generated by FUSE. What is the relationship between the proportion of labeled data and the overall embedding quality?\n\n3. How transferable are the node embeddings generated by FUSE?"}, "questions": {"value": "See the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lgyd205R6U", "forum": "1orI9Cczug", "replyto": "1orI9Cczug", "signatures": ["ICLR.cc/2026/Conference/Submission4749/Reviewer_3uSk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4749/Reviewer_3uSk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967287359, "cdate": 1761967287359, "tmdate": 1762917554060, "mdate": 1762917554060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FUSE, a lightweight framework for fast semi-supervised node embedding on graphs. It aims to bridge the gap between classical shallow embedding techniques (like DeepWalk and Node2Vec) and graph neural networks (GNNs), which, while powerful, are computationally expensive. FUSE proposes a linear fusion approach that efficiently integrates unsupervised structural information with partial label information, generating high-quality embeddings in a fraction of the time required by GNNs. The key insight is that semi-supervised node representations can be derived directly through algebraic fusion of unsupervised embeddings and label propagation signals, without iterative message passing or gradient-based training. This yields an interpretable and efficient alternative to deep GNN models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Unified, label-aware embedding objective with linear-time modularity: \nThe method jointly optimizes (i) an unsupervised modularity term, (ii) a supervised compactness term that pulls labeled nodes of the same class together, and (iii) a semi-supervised label-propagation term with attention—combined in one iterative scheme. This is a clean, well-motivated formulation that directly fuses structural and label signals. Value: principled objective that leverages labels without requiring node features or heavy GNN stacks.\n\n2. Scalable gradient approximation with explicit complexity gains:\nThe paper proposes a linearized modularity gradient and shows the overall complexity is , explicitly contrasting it with spectral methods. Value: strong algorithmic contribution that explains why the method is fast on large graphs. \n\n3. Clear and sizable runtime advantages, validated empirically:\nThe paper reports FUSE is ~5× faster on average than Node2Vec/DeepWalk and >7× faster on ArXiv-scale data, while maintaining comparable accuracy. Value: practical efficiency that matters for large or time-sensitive workloads."}, "weaknesses": {"value": "1. Presentation and proofreading quality undermine the submission's professionalism:\nThe manuscript contains numerous typographical errors, inconsistent mathematical notation, and formatting issues that detract from its clarity and reproducibility—issues that should have been caught prior to submission to a top-tier venue like ICLR. Examples include:\n- Spelling errors: \"yelding\" (p. 1), \"formative\" instead of \"informative\" (p. 1).\n- Mathematical notation inconsistencies: transpose symbol alternates between ⊺, ^⊺, and ^T; vectors are inconsistently bolded (d, 1, S); subscripts use both Si,: and S_{i,:}; summation indices lack bounds (e.g., ∑_{i,j}).\n\n2. Complete lack of visualizations in the main paper severely limits interpretability and empirical transparency:\nDespite strong claims of superior classification accuracy, runtime efficiency, and scalability, the main body contains zero figures or plots — no accuracy vs. label rate curves, no runtime scaling plots, no t-SNE/UMAP embeddings, no ablation bar charts, and no comparison tables. All quantitative results are presumably deferred to appendices or not shown at all in the provided excerpt.\n\n3. Overstated generality:\n\"the algorithm uses labels if available, but can be adapted to scenarios where labels are completely unavailable with some compromise in performance.\" overstated. Setting  =  = 0 reduces FUSE to modularity-only optimization, which is not evaluated in unsupervised mode. No ablation shows performance \"with some compromise\" vs. Node2Vec/DGI. Claiming adaptability without evidence is speculative.\n\n4. No theoretical guarantees for the linearized modularity gradient approximation:\nThe method replaces the exact degree-weighted term with an unweighted global mean for “stability and efficiency,” but provides no error bounds, convergence analysis, or conditions under which the approximation preserves modularity-driven structure. This limits the conceptual strength of the contribution beyond empirical performance. A theorem or approximation-error analysis (and a small-graph comparison to the exact gradient) would materially strengthen the paper."}, "questions": {"value": "Q1. Approximation guarantees:\nThe linearized modularity gradient replaces the degree-weighted term with an unweighted global mean “for stability and efficiency.” Can you provide any error bounds, convergence guarantees, or conditions under which this approximation preserves modularity structure?\n\nQ2. Scope of “state-of-the-art” claim:\nThe abstract says FUSE is “at par with or superior to state-of-the-art,” yet the baseline set is mostly classic shallow/self-supervised methods plus GraFN/ReVAR. Do you consider recent strong semi-supervised GNNs/graph transformers out of scope, or can you expand comparisons to support the SOTA claim?\n\nQ3. Biased vs. uniform walks:\nYou state that label-biased random walks “preferentially visit labeled nodes,” but Algorithm 2 samples the next node uniformly from neighbors. Is the walk actually biased, and if so, where is the bias implemented? Please reconcile the text and the pseudocode."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eXlwuKftB0", "forum": "1orI9Cczug", "replyto": "1orI9Cczug", "signatures": ["ICLR.cc/2026/Conference/Submission4749/Reviewer_sNXf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4749/Reviewer_sNXf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985518812, "cdate": 1761985518812, "tmdate": 1762917553839, "mdate": 1762917553839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a node embedding generation algorithm called FUSE (Fast Unified Semi-supervised Node Embedding Learning from Scratch), whose objective function integrates an unsupervised, a semi-supervised and a supervised component. The experimental results show the effectiveness of the proposed FUSE algorithm."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. FUSE has three components and does not require predefined features. \n\nS2. The experimental results demonstrate the effectiveness of the FUSE algorithm."}, "weaknesses": {"value": "W1. The applicability of the proposed FUSE seems limited. For example, as presented in the last section (DISCUSSION AND CONCLUSION) of the paper, the dynamic graphs are not applicable for the current FUSE algorithm. In addition, the size of the used ArXiV dataset is not large enough to show the scalability of the proposed methods. Also, how about the proposed FUSE algorithm for complex graph datasets, e.g., directed graphs and social networks? Last but not least, how about the proposed FUSE algorithm for other downstream tasks, e.g., link prediction and node clustering?\n\nW2. It seems that the experimental results are not convincing. For instance, Table 1 shows the classification accuracy and F1-score across embedding methods and three classifiers for all the datasets (except ArXiV). Why not present the results for each dataset respectively? The authors should provide the reasons.\n\nW3. The statements of the paper should be reasonable. For example, this paper claims that \"FUSE achieves the best or second-best performance across classifiers.\" However, as shown in Figure 1, DeepWalk, Node2Vec, and VGAE clearly outperform FUSE for 30-70 train-test split with SAGE.\n\nW4. The authors should carefully polish the submission to avoid typos and grammar mistakes, such as \"Tables 1 summarize\" in the first sentence of the first paragraph of Section 4.3.1."}, "questions": {"value": "W1-W4"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eAWP1eRam5", "forum": "1orI9Cczug", "replyto": "1orI9Cczug", "signatures": ["ICLR.cc/2026/Conference/Submission4749/Reviewer_2cEk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4749/Reviewer_2cEk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762174726234, "cdate": 1762174726234, "tmdate": 1762917553459, "mdate": 1762917553459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main contribution of the paper is FUSE, a fast semi-supervised embedding framework for node classification when the node features are not available. It jointly optimizes for three objectives, an unsupervised objective based on a linearized modularity surrogate, a supervised compactness objective that clusters labeled nodes and a semi supervised propagation term using random walk label spreading with attention weighted similarity, with normalization of the emebddings. The authors show the gradient approximation and argue that it is numerically stable. Experimental results are provided on 6 graph datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The setting that the authors look at, of no node features where the label information needs to be integrated is interesting and practically useful for the graph learning community. \n\n\n2. The algorithm and update proposed by the authors is straightforward and should be easy to implement. \n\n\n3. The authors provide runtime and robustness analysis in their results, which is appreciated."}, "weaknesses": {"value": "1. The paper has a heavy focus on scalability as one of the advantages, however the graph datasets used in the paper are all small scale datasets, with the largest dataset Arxiv (from OGB) only being of the order of a hundred thousand nodes which is not enough to demonstrate real scalability in practical settings. There are multiple other large scale datasets from the OGB suite itself, of the order of a million nodes which can be used to demonstrate scalability \n\n\n2. Some of the details around the method and experimentation are unclear.  The paper mentions that the random walk “preferentially visit labeled nodes” (L223), however the algorithm 2 uniformly samples the neighbors without biasing the probabilities, which is different from how the method is described. \n\n3. The complexity details mentioned by the authors does not include the cost of orthonormalization, which is not cheap and the cost can be high when the graphs are sparse. \n\n\n4. Several baselines used by the authors require features and the authors inject random features while keeping the architecture fixed, which is known to show much worse performance than, say using identity features (which is not included in the paper). This would not be a fair comparison and these baselines will show significantly lower results than what they actually are. \n\n\n5. The empirical results of FUSE are close or within the error range of DeepWalk/Node2vec and hence the margin of improvement is small"}, "questions": {"value": "See weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YFbd8AGd2v", "forum": "1orI9Cczug", "replyto": "1orI9Cczug", "signatures": ["ICLR.cc/2026/Conference/Submission4749/Reviewer_b6i3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4749/Reviewer_b6i3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762886501625, "cdate": 1762886501625, "tmdate": 1762917553222, "mdate": 1762917553222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}