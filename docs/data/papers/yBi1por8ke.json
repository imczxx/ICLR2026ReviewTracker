{"id": "yBi1por8ke", "number": 17424, "cdate": 1758275879890, "mdate": 1763101785700, "content": {"title": "Efficient Evaluation of Large Language Models via Collaborative Filtering", "abstract": "With the development of Large Language Models~(LLMs), numerous benchmarks have been proposed to measure and compare the capabilities of different LLMs. However, evaluating LLMs is costly due to the large number of test instances and their slow inference speed. \nIn this paper, we propose a collaborative filtering–inspired method that estimates model performance on a benchmark using only a small subset of test instances. \nSpecifically, we treat \"LLM–instance\" interactions as \"user-item\" interactions and design a two-stage approach. \nOur method first selects a small set of representative instances for a given task and then predicts the overall task-level performance from the model’s results on these selected instances.\nThese two stages correspond to the cold-start problem and the rating prediction problem in recommendation systems, respectively. \nExperiments on multiple LLMs and benchmarks demonstrate that our method achieves performance estimation 3\\% error using 10\\% of test data, reducing evaluation cost by an order of magnitude while maintaining high accuracy.", "tldr": "", "keywords": ["efficient evaluation", "recommendation systems", "large language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1beb0ca930565ed92fc5c954e2327abd4bb1e375.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a method to efficiently evaluate large language models (LLMs) using collaborative filtering. The proposed approach consists of two stages: instance sampling and performance prediction. Experiments conducted on multiple LLMs and benchmarks demonstrate that the method performs well."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated. How to evaluate llm efficiently is very valuable. \n2. The Experiments on multiple LLMs and benchmarks all shows the method works well.\n3. This paper is readable."}, "weaknesses": {"value": "1. The method still relies on **B early LLMs** that have already been evaluated on the entire benchmark. However, in real-world scenarios, it may be difficult to directly obtain the outputs of these B models on the benchmark. Therefore, we may first need to **generate** the responses of these B models on the whole benchmark. From this perspective, should we also **take the cost of these B models into account**?\n2. New models are continuously emerging and being updated, but should the **B early LLMs** also be updated accordingly? If not, the selected samples may lose relevance for newer models; however, if they do need to be updated, each updated model would have to generate responses for the entire benchmark again, which would undermine the efficiency of the proposed method."}, "questions": {"value": "1. Could you describe in detail the experimental setup shown in Figure 2?\n2. Section 3.3 presents a simple yet effective evaluation method, but how is it related to the approach proposed in Section 4?\n3. Figure 3 shows that the *Random Selection 0.1* setting took the least amount of time (0.0023 s). Based on this estimation, would testing all the data take less than 0.023 s? Is this calculation correct? Could you provide the actual time required to evaluate all the data?\n4. One of the key aspects of an evaluation method is having a unified standard. However, this method does not specify certain parameters, such as the sample ratio or **B** (the number of early models), which may lead to inconsistencies or confusion when comparing results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8wbgUyHPcf", "forum": "yBi1por8ke", "replyto": "yBi1por8ke", "signatures": ["ICLR.cc/2026/Conference/Submission17424/Reviewer_ddJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17424/Reviewer_ddJ9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727586023, "cdate": 1761727586023, "tmdate": 1762927318279, "mdate": 1762927318279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an efficient LLM evaluation method by reframing the task as a collaborative filtering problem. Specifically, each model is treated as a user, and each test example as an item. The model performance is regarded as a rating. By selecting a small amount of high-variance subset of samples (i.e., Probe Set) and using optimal-transport–based alignment to predict results on unseen samples, the method achieves nearly full-set accuracy with much lower evaluation cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Novel application of collaborative filtering for model evaluation.**\nThe paper introduces a new perspective by framing LLM evaluation as a collaborative filtering problem. Instead of introducing a new metric or dataset, it contributes a new perspective for efficient model evaluation. This conceptual shift opens a promising direction for accelerating LLM evaluation efficiently.\n\n**Simplicity and practicality.**\nThe approach is straightforward to implement. It only requires existing evaluation records for instance selection. No additional model training or annotation is needed, making it easily applicable in real-world evaluation pipelines.\n\n**Clarity and readability.**\nThe paper is well-written and easy to understand. The motivation, methodology, and experimental setup are easy to follow. I appreciate the informative illustrations, such as Figure 2, Figure 3, and Figure 4, making the background and method clearer to me."}, "weaknesses": {"value": "The method assumes that new models share similar capability structures with historical ones. Performance may degrade on out-of-distribution models. For example, there are several expert models, which may excel in one specific domain such as math. In this case, the performance of this model is not uniformly distributed like general models. It is not clear whether the proposed method also holds for such domain-specific models."}, "questions": {"value": "- What is abab-6.5? It seems that this model only occurs once in the introduction without any description of it.\n\n- I am curious about the implementation details of the \"historical\" baseline in Table 1. It seems that details about it are missing in Section 3.3.\n\n- What is the model sim distribution in step 2 of stage 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iKPL030dMX", "forum": "yBi1por8ke", "replyto": "yBi1por8ke", "signatures": ["ICLR.cc/2026/Conference/Submission17424/Reviewer_aKUQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17424/Reviewer_aKUQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833436902, "cdate": 1761833436902, "tmdate": 1762927317645, "mdate": 1762927317645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This research studies the efficient evaluation problem for large language models. In particular, the researchers consider the evaluations of new LLMs by using the evaluation results from older ones. The authors propose to study this problem by considering the collaborative filtering framework, where LLMs are users and samples are items. The empirical results on the real-world datasets demonstrate that the proposed method can more precisely predict model performance than other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The research question of evaluating LLMs' performance with a limited inference budget is interesting.\n2. The proposed idea of considering it as a collaborative filtering problem is reasonable. \n3. The ablation study on running time makes the experiment more rigorous."}, "weaknesses": {"value": "I'm happy to see that you consider the model-sample matrix as the user-item matrix. I also agree with providing some seed samples to testify that the model can solve the \"cold-start\" problem. But I have these two questions:\n1. After collecting the performance on those \"cold-start\" samples, why can we not simply apply matrix factorization methods (the simplest approach for recommendation under the collaborative filtering setting) to directly reconstruct the missing scores from this new model in the matrix?\n2. The old LLMs have provided full predictions on all test samples, which is different from the recommendation settings, where even activated users cannot provide feedback to all items. So, I don't think the current method leverages this unique property.\n\nIn the experimental setting, why do we split the candidate models by release date? What impact will happen if we don't have that information?\n\nMinors:\n1. Line 185: The cite format is improper."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M0r5ySBflG", "forum": "yBi1por8ke", "replyto": "yBi1por8ke", "signatures": ["ICLR.cc/2026/Conference/Submission17424/Reviewer_TJ7U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17424/Reviewer_TJ7U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935434203, "cdate": 1761935434203, "tmdate": 1762927316981, "mdate": 1762927316981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the efficient evaluation of LLMs, an important and practical problem in model assessment. The authors identify a limitation of existing efficient evaluation methods, namely their inability to generalize to task-specific scenarios. To address this, they propose a two-stage evaluation framework inspired by collaborative filtering, offering a new perspective on sample selection for efficient evaluation. Empirical results demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a timely and practical problem in LLM evaluation.\n\n2. The use of collaborative filtering to address sample selection and performance prediction is novel and provides an interesting perspective.\n\n3. Extensive experiments are conducted to validate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The motivation is insufficiently developed. The paper does not clearly explain how existing efficient evaluation methods fail to capture task-specific performance and model ranking.\n\n2. The discussion of related work is limited and lacks depth. At least clear descriptions of how prior works select samples should be discussed and compared in the paper.\n\n3. Certain design choices in the proposed method appear questionable (see Questions for details)."}, "questions": {"value": "1. In the proposed data selection process, both easy and difficult samples are excluded. While removing easy samples is reasonable, excluding difficult samples seems questionable, as they often reveal the performance gap among models. If initial models are used to estimate difficulty, the estimation may become inaccurate as the models evolve. Could the authors discuss this potential issue?\n\n2. The baseline design relies on two assumptions (lines 196–202) that may not always hold. The first assumes that models produce consistent responses for semantically similar samples. However, semantically similar questions can differ significantly in difficulty. The second assumes that models behave consistently within the same cluster. Empirical evidence supporting these assumptions would strengthen the paper.\n\n3. The idea of selecting similar models is reasonable, but the instance selection based on “variance” in model responses is unclear. It is not evident how this aligns with the collaborative filtering principle that similar users have similar interactions. Could the authors elaborate on the rationale for choosing samples with high variance among similar models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PxeijevBPs", "forum": "yBi1por8ke", "replyto": "yBi1por8ke", "signatures": ["ICLR.cc/2026/Conference/Submission17424/Reviewer_fDsg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17424/Reviewer_fDsg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985427747, "cdate": 1761985427747, "tmdate": 1762927316385, "mdate": 1762927316385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}