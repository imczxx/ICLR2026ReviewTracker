{"id": "fWhRslwYri", "number": 6398, "cdate": 1757979971346, "mdate": 1763745709921, "content": {"title": "Scalable Random Wavelet Features: Efficient Non-Stationary Kernel Approximation with Convergence Guarantees", "abstract": "Modeling non-stationary processes, where statistical properties vary across the input domain, is a critical challenge in machine learning; yet most scalable methods rely on a simplifying assumption of stationarity. This forces a difficult trade-off: use expressive but computationally demanding models like Deep Gaussian Processes, or scalable but limited methods like Random Fourier Features (RFF). We close this gap by introducing Random Wavelet Features (RWF), a framework that constructs scalable, non-stationary kernel approximations by sampling from wavelet families. By harnessing the inherent localization and multi-resolution structure of wavelets, RWF generates an explicit feature map that captures complex, input-dependent patterns. Our framework provides a principled way to generalize RFF to the non-stationary setting and comes with a comprehensive theoretical analysis, including positive definiteness, unbiasedness, and uniform convergence guarantees. We demonstrate empirically on a range of challenging synthetic and real-world datasets that RWF outperforms stationary random features and offers a compelling accuracy-efficiency trade-off against more complex models, unlocking scalable and expressive kernel methods for a broad class of real-world non-stationary problems.", "tldr": "", "keywords": ["Random features", "Non-stationary kernels", "Wavelet features", "Gaussian Process", "Kernel approximation"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03b4d148d0b7846c1c0e79e8e9f0ee74792ef89a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The manuscript proposes random wavelet features to close the gap between expressibility and computational cost of non-stationary kernels for Gaussian Processes. The objective and approach are laudable, but critical details are missing in the results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Broad and impactful scope.\n- Understandable writing.\n- Strong theoretical results.\n- Good diversity of experimental tests."}, "weaknesses": {"value": "- RMSE only in experiments. At a minimum, CRPS should be added.\n- Unclear setup of the competing methods (see questions). \n- No truly large GP in experiments (> 100,000 data points): There are GPs now that can run on several million points; 45000 max is a little disappointing. \n- Writing has to be polished a bit (missing articles and small typos)\n- Missing comparison to more traditional but powerful ways to encode non-stationarity, for instance, \"Paciorek, Christopher, and Mark Schervish. \"Nonstationary covariance functions for Gaussian process regression.\" Advances in neural information processing systems 16 (2003).\""}, "questions": {"value": "(1) It is not immediately clear why \"The RFF-GP framework is thus a scalable approximation for stationary kernels.\" The kernel z(x) z(x') is non-stationary. Please explain the discrepancy.\n(2) \"Random Fourier-based kernel approximation methods exploit Bochner’s theorem (Rahimi & Recht,\n2007) to yield scalable approximations for stationary kernels, but they struggle to capture non-\nstationarity.\" Are they stationary kernels, or do they struggle with non-stationarity? Stationary kernels don't struggle with non-stationarity; they simply cannot model it. Please explain. \n(3) It was difficult for me to decipher the setup of the competing methods. For example, the exact GP in the examples performs pretty poorly, but should not. Given the same setup, it should beat approximate methods. So a stationary kernel in an exact GP should perform better than SVGP with the same kernel. What was the exact setup of the competing methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HOTFTTdtnA", "forum": "fWhRslwYri", "replyto": "fWhRslwYri", "signatures": ["ICLR.cc/2026/Conference/Submission6398/Reviewer_28um"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6398/Reviewer_28um"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761584621058, "cdate": 1761584621058, "tmdate": 1762918803036, "mdate": 1762918803036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an approach to build scalable kernel models using wavelet-based decompositions. The method takes a similar approach to random Fourier features (RFFs), but uses wavelet bases, instead of the cosine bases. The result provides a way to build finite-dimensional feature maps whose dot products approximate (possibly non-stationary) positive-definite kernels/covariance functions. Theoretical results are presented on the positive-definiteness of the resulting kernel and on the approximation error due to Monte Carlo approximations. Experiments are presented comparing the proposed RWF against traditional RFFs, sparse variational Gaussian processes, deep GPs, and other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well written and organised in a way that makes the presentation easy to follow.\n* The proposed idea is intuitive and the theoretical analysis follows a similar approach to the analysis of RFF methods.\n* The experimental evaluations compare against a wide range of baselines revealing mostly strong performance improvements.\n* Interesting ablation studies are included, comparing for example training time and memory usage, the latter of which is usually rare to find."}, "weaknesses": {"value": "* My main concern is that the related work discussion fails to cover previous wavelet-based kernel methods and their approximations. A quick literature search reveals wavelet support vector machines (Zhang et al., 2004) and other methods also apparently using wavelet-based kernel decompositions (Guo et al., 2004; Yger, 2011), and potentially others that I'd have missed. Even if these methods are not directly solving the same modelling problem, it'd be important to contrast this paper's approach with them (or at least a few key representatives), to better contextualise and assess the significance of this paper's contribution.\n* In the background on wavelets, a few concrete examples of mother wavelet functions could help to motivate the unfamiliar reader and also demonstrate the capabilities of this modelling approach where, e.g., RFFs would fail.\n\nReferences:\n* Guo, W., Zhang, X., Jiang, B., Kong, L., & Hu, Y. (2024). Wavelet-based Bayesian approximate kernel method for high-dimensional data analysis. Computational Statistics, 39(4), 2323-2341.\n* Yger, F., & Rakotomamonjy, A. (2011). Wavelet kernel learning. Pattern Recognition, 44(10-11), 2614-2629.\n* Zhang, L., Zhou, W., & Jiao, L. (2004). Wavelet support vector machine. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 34(1), 34-39."}, "questions": {"value": "What methods in the literature of wavelet-based kernel machines would be the closest to the approach in this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vEcgUqVxhu", "forum": "fWhRslwYri", "replyto": "fWhRslwYri", "signatures": ["ICLR.cc/2026/Conference/Submission6398/Reviewer_daFv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6398/Reviewer_daFv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761787397318, "cdate": 1761787397318, "tmdate": 1762918802360, "mdate": 1762918802360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes   a scalable method, RWF, for modeling non-stationary processes by sampling from wavelet families. RWF extends RFFs to capture localized, input-dependent patterns, offering theoretical guarantees and strong empirical performance, achieving a superior accuracy-efficiency trade-off for large-scale, non-stationary kernel learning tasks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1 It provides a rigorous theoretical analysis of Random Wavelet Features (RWF), establishing positive definiteness, unbiasedness, variance bounds, and uniform convergence with explicit sample complexity.\n\n2 RWF achieves O(ND²) training complexity, maintaining the scalability of random feature methods while effectively encoding non-stationarity via wavelet localization.\n\n3 Extensive empirical evaluations on synthetic, speech, and large-scale regression datasets demonstrate that RWF consistently outperforms stationary random features"}, "weaknesses": {"value": "1 While the proposed RWF framework is clearly presented and supported by solid theoretical analysis, I have concerns regarding its novelty. The core formulation of RWF (Eqs. 3.1–3.3) and the sampling procedure (Algorithm 1) appear conceptually similar to existing RWF methods, which also construct kernel approximations using randomly sampled wavelet bases. The authors should clarify what specific differences or innovations distinguish RWF from  earlier approaches such as L. Sun et al., “Wavelet-based Bayesian Approximate Kernel Method for High-Dimensional Data Analysis,” 2023, as well as earlier studies on non-stationary kernel approximations (e.g., Remes et al., “Non-stationary Spectral Kernels,” NeurIPS 2017; Samo & Roberts, “Stochastic Process Regression with Non-stationary Spectral Kernels,” 2015).\n\n2 The manuscript would benefit from better organization. For example, I am very confusing that why so much space is devoted to introducing SVGP in the Preliminaries section, while the discussion on the computational complexity of the proposed method (Section 3.3) is relatively brief and should be expanded.\n\nI am not an expert of GP. I will adjust my score according to the authors' feedback and other reviewer's comments."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HfPvksme9L", "forum": "fWhRslwYri", "replyto": "fWhRslwYri", "signatures": ["ICLR.cc/2026/Conference/Submission6398/Reviewer_QoaL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6398/Reviewer_QoaL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088553867, "cdate": 1762088553867, "tmdate": 1762918801437, "mdate": 1762918801437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Random Wavelet Features (RWF) as a Monte-Carlo feature map for Gaussian-process (GP) regression intended to capture non-stationarity while retaining the scalability of random features, and is posed as a generalization of the RFF approach to the case of non-stationary kernels. Concretely, the authors define a kernel by averaging outer products of wavelet atoms over random scale $s>0$ and shift $t \\in \\mathbb{R}^d$ :\n$$\nk(x, y)=\\int_{(s, t)} \\psi_{s, t}(x) \\psi_{s, t}(y) p(s, t) d s d t, \\quad \\psi_{s, t}(x)=s^{-d / 2} \\psi\\left(\\frac{x-t}{s}\\right)\n$$\nand approximate it with $D$ Monte-Carlo samples, yielding an explicit feature map $z(x) \\in \\mathbb{R}^D$ and a linear-in-data GP regression algorithm for large sample sizes. The theory section claims (i) positive definiteness of $k$, (ii) unbiasedness/variance bounds for the Monte-Carlo estimator, and (iii) a uniform approximation bound $\\sup _{x, y \\in \\mathcal{M}}\\left|z(x)^{\\top} z(y)-k(x, y)\\right|$ via covering arguments. Experiments on synthetic data, TIMIT, several UCI datasets, and Protein report lower RMSE and similar or lower training time than RFF-GP, SVGP, deep GPs, spectral mixtures, and adaptive RKHS features."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper derives potentially non-stationary kernels using wavelet based dictionaries together with the random features machinery. Experimental results show slightly better RMSE values for GP regression with slightly reduced training cost compared to baselines on most experiments."}, "weaknesses": {"value": "Theoretical novelty of the work is extremely limited. The positive-definiteness result (Thm. 4.1) is a direct application of the classic “integral of feature products” construction; nothing wavelet‑specific is used. The unbiasedness and uniform‑convergence proofs follow the standard random‑features and Monte-Carlo estimator analysis tools. Also, the claim about non-stationarity is violated if the weighting probabality distribution $p(s,t)$ in Eqaution 3.3 is taken to be of the form $p_s(s) p_t(t)$ with $p_t$ being translation invariant (For example Lebesgue measure over $\\mathbb{R}^{d}$) The idea of using wavelets in place of Fourier features for scalable kernel approximations also appears in the literature prior to this, including works that explicitly propose random wavelet features for approximate kernels [1]. Closely related Monte‑Carlo wavelet frame constructions exist in learning theory as well [2]. \n\nReferences:\n[1] Guo, W., Zhang, X., Jiang, B. et al. Wavelet-based Bayesian approximate kernel method for high-dimensional data analysis. Comput Stat 39, 2323–2341 (2024). https://doi.org/10.1007/s00180-023-01438-1\n[2] Z. Kereta, S. Vigogna, V. Naumova, L. Rosasco and E. De Vito, \"Monte Carlo wavelets: a randomized approach to frame discretization,\" 2019 13th International conference on Sampling Theory and Applications (SampTA), Bordeaux, France, 2019, pp. 1-5, doi: 10.1109/SampTA45681.2019.9030825."}, "questions": {"value": "1. Is there anything specific to the wavelet dictionary (other than choices of constants) that can be used to improve the theoeretical results in Sections 4.1 to 4.3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QYt5oOgLlc", "forum": "fWhRslwYri", "replyto": "fWhRslwYri", "signatures": ["ICLR.cc/2026/Conference/Submission6398/Reviewer_Rq9f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6398/Reviewer_Rq9f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762248686364, "cdate": 1762248686364, "tmdate": 1762918795802, "mdate": 1762918795802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}