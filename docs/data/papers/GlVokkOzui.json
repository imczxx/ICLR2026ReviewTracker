{"id": "GlVokkOzui", "number": 14867, "cdate": 1758244874400, "mdate": 1759897344535, "content": {"title": "Post-Processing Approach for Distributive Fairness in Multi-Class Federated Learning", "abstract": "Distributive fairness is a critical concern in the application of Federated Learning\n(FL) to decision making. Three concepts of distributive fairness are recently con\nsidered important in FL: global, local group and client fairness. Global fairness\naddresses disparities among legally protected groups across the entire population.\nLocal group fairness addresses disparities between protected groups within indi\nvidual clients. Client fairness focuses on disparities across clients. These concepts\nof distributive fairness coexist in FL and achieving one does not guarantee the\nothers. Most FL studies focus on only a single concept. In real-world applications,\nhowever, different stakeholders often require fairness from different perspectives\nsimultaneously. Enforcing those fairness concepts inherently incurs an accuracy\ncost. This paper investigates that, for a given FL setup, the maximum achievable\naccuracy under various combinations of distributive fairness, i.e., all three, any two,\nor just one, depending on the application. We propose a post-processing algorithm\nthat returns a model with the near-optimal accuracy while satisfying pre-specified\nfairness constraints. Experimental results show that our algorithm outperforms\nthe current state of the art (SOTA) in terms of the fairness–accuracy tradeoff,\ncomputational and communication efficiency. Code is available on Github.", "tldr": "", "keywords": ["Fairness", "Federated Learning", "Post-processing"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3ff4f5fc6e2592503dccdc6108a2f863a4df2b34.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a framework for multi-class FL that balances various combinations of distributive fairness, which is a pioneer work to that seeks to optimize global group, local group, and individual fairness collectively."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The theoretical analysis of this paper is solid.\n\n2. This work is the first to propose a unified framework that seeks to optimize global group, local group, and individual fairness collectively."}, "weaknesses": {"value": "1. There are only 2 clients for the experiments on Adult, and 5 clients for the experiments on HM10000. Note that in real-world FL settings, the number of clients could range from 100 to 10000. Although the paper uses 50 clients for the ACS dataset, this is only one special case not a common setting throughout the experimental evaluation.\n\n2. The paper lacks evaluation on real-world large-scale vision/text datasets.\n\n3. The assumption that each client has access to all sensitive groups is not very practical. Also, there may be incompatibility between client fairness, local fairness, and global group fairness."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U7olrIO05V", "forum": "GlVokkOzui", "replyto": "GlVokkOzui", "signatures": ["ICLR.cc/2026/Conference/Submission14867/Reviewer_tAYg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14867/Reviewer_tAYg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761409194148, "cdate": 1761409194148, "tmdate": 1762925218357, "mdate": 1762925218357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of fairness in federated learning, considering three definitions: global group fairness, local group fairness, and client fairness. The question is what is the maximum achievable accuracy under various combinations of fairness, i.e., all three, any two, or just one. The paper also proposes a post-processing algorithm to obtain a model with near-optimal accuracy while satisfying pre-specified fairness constraints. Experimental results are included on Adult, ACSPublicCoverage and HM10000 datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-- New formulation considering three types of fairness simultaneously. While local and global fairness have been looked at, including their tradeoffs via a convex optimization, the introduction of client fairness into the formulation is quite interesting.\n\n-- Leads to a nice convex optimization to find the optimal accuracy under constraints on the different fairness criteria.\n\n-- Proposes a post-processing algorithm that further tries to achieve the optimal accuracy. The algorithm describes the role of each client in the execution of the strategy, showing how it will be implemented in a distributed setting.\n\n-- Experiments are provided on 3 datasets, and multiple baselines have been considered. Adult has 2 clients while PublicCoverage has 50 clients."}, "weaknesses": {"value": "-- Figures could be better for comparing the tradeoffs rather than tables. There are some figures in the Appendix, but the captions/legends were not very clear to understand how it outperforms SOTA.\n\n-- It is a bit confusing to say it outperforms SOTA. It seems they achieve better fairness by paying a cost in accuracy (unless I am misunderstanding the table)? The benefit will be clearer in a tradeoff plot. (communication/computation benefit is acknowledged)\n\n-- Another option is to consider a radar chart to better understand the performance comparison.\n\n-- Some grammar issues and typos were noted. \n\n125. (1) We formally defines\n129. We defines\n\nA few others as well."}, "questions": {"value": "Q1. Could you explain how and in what aspect these algorithms outperform SOTA (other than communication/computation benefit)? Or, is there a better way to visualize the accuracy-fairness tradeoff if there is one?\n\nQ2. The paper has discussed extensions to some other group fairness metrics in the Appendix. I was wondering if it is possible to define a general class of fairness metrics over which this kind of a convex optimization formulation can be extended? \n\nQ3. This is optimal for post-processing techniques, but is it possible that there may exist other in-processing/pre-processing techniques that could lead to an even better tradeoff?\n\nQ4. Are there any assumptions that the initial FedAvg solution needs to satisfy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UBowIP0Lcz", "forum": "GlVokkOzui", "replyto": "GlVokkOzui", "signatures": ["ICLR.cc/2026/Conference/Submission14867/Reviewer_Un6i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14867/Reviewer_Un6i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865622427, "cdate": 1761865622427, "tmdate": 1762925217844, "mdate": 1762925217844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a post-processing framework for federated learning that enforces three distributive fairness notions—global group fairness, local group fairness, and client fairness—while selecting an outcome predictor that maximizes accuracy under explicit constraints.   It extends ROC-based post-processing from binary to multi-class settings by defining a multi-class ROC surface, characterizing the region under this surface via supporting hyperplanes, and proving that this region is convex and contains all achievable true-positive vectors, which yields a convex program for optimal accuracy under fairness constraints. Experimental result on three datasets reduces global/local/client disparity with competitive accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a timely challenge in federated learning, reducing global, local, and client disparity with competitive accuracy.\n\n- Clear formalization of three fairness notions in multi-class FL with precise constraints for global, local, and client fairness.\n\n- Empirical efficiency: consistent reductions in three disparities with fewer communication rounds compared to baselines.\n\n- The paper is well organized and easy to read."}, "weaknesses": {"value": "- The theoretical framework presented in this paper is nearly identical to that proposed in [1], substantially diminishing its technical contribution and novelty. I would be willing to raise my score if the authors can clearly differentiate their approach and address this concern.\n\n- Local statistics may reveal important properties of the local datasets in high-stakes scenario, even without containing any per-user information. Differential privacy alone is insufficient to prevent such leakage. The authors ought to consider encrypted computation on the server side.\n\n- Accuracy cost can be substantial, especially when enforcing all three notions on some datasets. The Pareto frontier is referenced but not visualized in the main text.\n\n[1] The Cost of Local and Global Fairness in Federated Learning, AISTATS 2025."}, "questions": {"value": "1. Why does the theoretical approach in this paper appear overly similar to that of [1]? Is this essentially the same method applied to a different problem setting?\n\n2. How sensitive are results to Bayesian score miscalibration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dsdA32vqGJ", "forum": "GlVokkOzui", "replyto": "GlVokkOzui", "signatures": ["ICLR.cc/2026/Conference/Submission14867/Reviewer_Cory"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14867/Reviewer_Cory"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911538071, "cdate": 1761911538071, "tmdate": 1762925217153, "mdate": 1762925217153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}