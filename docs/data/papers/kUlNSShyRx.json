{"id": "kUlNSShyRx", "number": 14831, "cdate": 1758244469428, "mdate": 1759897346875, "content": {"title": "Conformal Correction for Efficiency May be at Odds with Entropy", "abstract": "Conformal prediction (CP) provides a comprehensive framework to produce statistically rigorous uncertainty sets for black-box machine learning models. To further improve the efficiency of CP, conformal correction is proposed to fine-tune or wrap the base model with an extra module using a conformal-aware inefficiency loss. In this work, we empirically and theoretically identify a trade-off between the CP efficiency and the entropy of model prediction. We then propose an entropy-constrained conformal correction method, exploring a better Pareto optimum between efficiency and entropy. Extensive experimental results on both computer vision and graph datasets demonstrate the efficacy of the proposed method. For instance, it can significantly improve the efficiency of state-of-the-art CP methods by up to 34.4\\%, given an entropy threshold.", "tldr": "", "keywords": ["Conformal Prediction; Conformal Correction; Efficiency-Entropy Trade-off; Pareto Optimality"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6ee9a4d134556a4c85c3f306771a63aaef10b2f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper shows a fundamental trade-off in conformal prediction between efficiency (smaller sets) and prediction entropy (more decisive probabilities), and reframes conformal correction as multi-objective optimization on this Pareto frontier. It introduces EC3, a plug-in adapter that takes base probabilities and is trained with a conformal-aware inefficiency loss while constraining entropy via focal loss and temperature scaling, shrinking sets without inflating uncertainty. A theoretical link between APS expected set size and entropy explains why improving one typically harms the other. Experiments on vision and graph benchmarks indicate EC3 consistently yields better efficiency–entropy trade-offs and improves conditional coverage while preserving target marginal coverage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper clearly identifies and formalizes a trade-off between CP efficiency and prediction entropy, with supporting analysis (e.g., APS-based bounds) that explains why the objectives can conflict.\n2. This paper persuasively recasts conformal correction as a multi-objective problem with entropy as a first-class constraint, making the practical value obvious.\n3. The paper is well organized; figures (especially Pareto plots) effectively illustrate key ideas and make the theory easy to follow."}, "weaknesses": {"value": "1. The contribution is hard to assess because similar ideas have been explored previously. Please expand the related-work discussion to delineate what is new here and add the relevant prior methods as baselines to the empirical evaluation.\n2. This work does not establish formal conformal coverage guarantees, in contrast to standard CP.\n\n[1] Xi H, Huang J, Liu K, et al. Does confidence calibration improve conformal prediction.    \n[2] Dabah L, Tirer T. On Temperature Scaling and Conformal Prediction of Deep Classifiers."}, "questions": {"value": "1. See weaknesses. Please discuss more about related works and experimental baselines to outline the contribution.\n2. Please provide more training details: loss trajectories and how the two objectives (efficiency vs. entropy) evolve and interact during optimization.\n3. The evaluation shows that, at matched entropy, your method yields smaller prediction sets. Why is this desirable for downstream decision making?\n4. Hyperparameters appear important; please expand discussion and provide practical selection guidelines.\n5. How does the method perform under distribution shift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oDPr3garnO", "forum": "kUlNSShyRx", "replyto": "kUlNSShyRx", "signatures": ["ICLR.cc/2026/Conference/Submission14831/Reviewer_wJCq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14831/Reviewer_wJCq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761498372118, "cdate": 1761498372118, "tmdate": 1762925182815, "mdate": 1762925182815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Entropy-Constrained Conformal Correction (EC$^3$), an approach to improve the efficiency of non-conformity score “adapters” for downstream conformal prediction. The authors facilitate this by augmenting the inefficiency loss used to train these adapters with focal loss. Lastly, they provide theoretical grounding for adding focal loss and empirical results using EC$^3$ on image classification, node classification, and Q&A tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper includes a strong empirical evaluation, presenting results across several datasets with varying characteristics (i.e., number of classes). \n- The paper provides a clear justification for using **focal loss** in training adapters for Adaptive Prediction Sets (APS) non-conformity scores. It also provides a good theoretical connection between minimizing focal loss and maximizing entropy, leading to smaller prediction set sizes (**Theorem 3**, under the assumption $\\mu \\geq 0.5$).  \n- The paper also provides a **class-conditional version of EC³** to address imbalanced class coverage."}, "weaknesses": {"value": "- The main weakness of the paper is the lack of comparison with APS using randomization, as introduced in [1]. The authors employ APS **without randomization** in their experiments. In other words, their implementation omits the second red and bolded term in the randomized non-conformity score:\n$$\nV(x, y; u) = \\sum_{i=1}^{y} \\hat{\\pi}_ {(i)}(x)~\\textcolor{red}{\\mathbf{- u \\hat{\\pi}_{(y)}(x) }}\n$$\nfor \n$u \\sim U([0,1])$. Theoretically and empirically, randomized APS has been shown to **reduce set size**  (**improve efficiency**) compared to non-randomized APS [2]; thus, it is imperative to include randomized APS when comparing with SOTA methods.  \n\n   - For instance, Cora-ML is reported to have efficiencies of around 4 and 1.85 for the baseline CP and CF-GNN methods in Table 2, respectively, using non-randomized APS. However, Figure 5 in [2] shows efficiencies closer to 1.5 when using the randomized APS approach for both baseline CP and CF-GNN — indicating that randomized APS performs similarly to EC³. (Similar comparisons are available in [2] for the remaining datasets). For this reason, the perceived gains of **EC³** may not be as significant as claimed.\n\n- The paper is missing an efficiency plot/table for the class-conditional $EC^3$. It is important to quantify the efficiency and SSCV trade-off between the two methods.\n\n- The paper references $EC^{3}-1$ and $EC^{3}-2$, but it is not clear what those are referencing.\n\n- Missing definitions for SSCV and WSC, either in the main body or appendix.\n\n\nReferences\n\n[1] Y. Romano et al. Classification with valid and adaptive coverage [NeurIPS 202]\n\n[2] P. Maneriker et al. Conformal Prediction: A Theoretical Note and Benchmarking Transductive Node Classification in Graphs [TMLR 2025]"}, "questions": {"value": "See weaknesses. The main weakness/question is a lack of comparison with APS with randomization. If that can be addressed by the authors and the $EC^3$ still provides quality results, I will be happy to adjust my score accordingly. \n\nL743: \"inequation\" -> \"inequality\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vA4YgfqWXI", "forum": "kUlNSShyRx", "replyto": "kUlNSShyRx", "signatures": ["ICLR.cc/2026/Conference/Submission14831/Reviewer_s5ab"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14831/Reviewer_s5ab"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761602047440, "cdate": 1761602047440, "tmdate": 1762925182204, "mdate": 1762925182204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the relationship between efficiency (small size of conformal prediction (CP) sets) and entropy (uncertainty in model predictions) in conformal prediction frameworks. The authors show empirically and theoretically that these two quantities are often in conflict: increasing CP efficiency typically raises prediction entropy, reducing confidence in predictions.\n\nTo address this, the authors introduce $EC^3$ (Entropy-Constrained Conformal Correction), a new conformal correction method that adds an entropy control term (via focal loss and temperature scaling) to balance efficiency and entropy. They formalize this trade-off as a Pareto frontier and use $EC^3$ to search for better optima.\n\nExperiments on vision (CIFAR-10/100) and graph datasets (Cora-ML, CS, Photos) show that $EC^3$ achieves up to 34% improvement in efficiency while maintaining marginal coverage and improving conditional coverage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a first rigorous analysis linking CP inefficiency and prediction entropy (Propositions 1-2, Theorem 3).\n- It identifies a real tension between compact prediction sets and calibrated uncertainty, which has been largely ignored by prior conformal training work.\n- The $EC^3$ objective combines focal loss and inefficiency regularization with entropy control; temperature scaling provides a simple yet effective Pareto traversal mechanism.\n- Extensive experiments across multiple architectures and domains; results are consistent and show significant practical gains.\n- The paper is clearly written, with good visualizations (e.g., Pareto frontiers) and an accessible discussion of theoretical results."}, "weaknesses": {"value": "- The analysis focuses on adaptive conformal prediction (APS); extension to other CP variants (e.g., regression or non-adaptive scores) is not discussed.\n- The entropy parameter $\\gamma$ and the temperature $T$ are hyperparameters tuned via grid search; no principled guidance for choosing them is provided.\n- While acknowledged in the Limitations section, empirical degradation in base model accuracy is not quantified or analyzed.\n- Some proofs (especially Proposition 2 and Theorem 3) rely on simplifying assumptions (exchangeability, bounded calibration errors) that might not hold in practical non-i.i.d. data settings.\n- Baselines are limited to conformal-training-based methods; recent calibration or information-theoretic conformal approaches could strengthen evaluation."}, "questions": {"value": "- How sensitive is $EC^3$ to the choice of $\\gamma$ (entropy weight) and $\\beta$ (inefficiency weight)? Can adaptive schedules mitigate tuning difficulty?\n- Does $EC^3$ preserve or degrade conditional coverage guarantees beyond empirical results? Are there any theoretical bounds?\n- How does the method perform on non-classification tasks (e.g., regression CP or language model calibration)?\n- Could temperature scaling alone (without $EC^3$) achieve comparable Pareto improvements with careful tuning?\n- Are there insights into how entropy affects human interpretability of conformal sets - e.g., does lower entropy align with better human decision support?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jeZpXtp1pa", "forum": "kUlNSShyRx", "replyto": "kUlNSShyRx", "signatures": ["ICLR.cc/2026/Conference/Submission14831/Reviewer_G9pT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14831/Reviewer_G9pT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983340155, "cdate": 1761983340155, "tmdate": 1762925179288, "mdate": 1762925179288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper empirically highlights a tradeoff between the entropy of the model predictions and the set size ( efficiency ) of CP. The\n authors provide theoretical justifications of this trade-off for a specific score function and propose a new conformal training algorithms that achieves a more favorable balance between entropy and efficiency. The method, which fine-tunes pertained predictors using the proposed conformal adaptor, rather than, scratch, demonstrates improved efficiency in terms of prediction set size compared to existing conformal training methods such as ConfTr ( Stutz et al. )"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The investigation of the entropy and CP set size in this particular setting of conformal training is novel to the best of my knowledge. the framing is new and can be valuable for future research. \n\n2. The theoretical results, though specific to a single score function, are well-motivated and provide interesting intuition.\n\n3. The empirical results show promise in terms of prediction set size minimization relative to prior work. \n\n4. The authors had valuable practical considerations in mind. Instead of retraining the entire model, the paper utilizes the idea of conformal wrappers to finetune, which reduces the computational cost and improves practicality. this design choice suggests that the authors are mindful of real-world applicability."}, "weaknesses": {"value": "1. **Limited scope of score functions**: the authors focus exclusively on the APS-type score functions. it remains unclear whether the observed entropy-efficiency tradeoff generalizes to other commonly used conformal scores such as 1-p(y|x), which is standard in split-conformal methods. At minimum, empirical evidence across multiple score functions would significantly strengthen the claims. \n\n2. Theoretical results are again presented only considering the APS score. While generalizing the analysis to other score functions may be challenging, it would be valuable to at least empirically test whether similar behaviors are observed with alternatives. \n\n3. I found the evidence insufficient for the observed claimed trade-off. More explanations are needed. Particularly the explanation around lines 070-090 and Figure 1 does not convincingly establish the existence of the proposed tradeoff. It would be important to (i) evaluate whether the same trend holds under full model retraining, (ii) demonstrate the empirical tradeoff using other score functions. Moreover in this paragraph I found the explanation confusing for figure 1. does this figure correspond with only fine-tuning using L_class ? if only utilizing the L_class, then there is no term balancing conformal set size and thus its just standard fine-tuning using cross entropy loss ? I would appreciate clarification from the authors. \n\n4. the alignment between text and figure is sometimes unclear, making it difficult to interpret how empirical results support the theoretical claims."}, "questions": {"value": "1. **Line 158**: the authors mentions jointly optimizing L_class and L_efficiency. The figure only corresponds to L_class. ( which is standard cross entropy loss). It is possible to train ( even from scratch) using only L_efficiency to obtain smaller sets at comparable accuracy ( thats what conformal training does.) Can you show the results and figure when only using L_efficiency ? \n\n\n2.  Figure 1a vs Line 160: In figure 1a, efficiency initially decreases while accuracy increases, yet the text suggests both increase together. Please clarify this discrepancy or update the figure/text for consistency. \n\n3. Line 071 (CIFAR-100 results): It is counterintuitive that prediction set size increases after applying conformal correction. Could you clarify this behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uXkfZeaxNa", "forum": "kUlNSShyRx", "replyto": "kUlNSShyRx", "signatures": ["ICLR.cc/2026/Conference/Submission14831/Reviewer_3wJp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14831/Reviewer_3wJp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762372700368, "cdate": 1762372700368, "tmdate": 1762925177508, "mdate": 1762925177508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}