{"id": "PywcMGMg4f", "number": 20814, "cdate": 1758310502888, "mdate": 1759896957362, "content": {"title": "Sparse Feature Routing for Tabular Learning", "abstract": "The landscape of high-performance tabular learning is defined by a difficult compromise between the opaque ensembles of gradient-boosted trees and deep models that rely on elaborate pre-training to adapt ill-suited, monolithic backbones. We argue this compromise stems from a fundamental architectural mismatch. We propose a more principled path forward with a decomposed architecture that performs instance-wise selection over independent feature experts. Our model, the Sparse Feature Routing Network (SFR Net), assigns a small expert to each feature and uses a sparse router to dynamically compose expert results into an instance-specific representation, while a low-rank module captures higher-order interactions. This design yields native instance-level attributions and remains computationally efficient. A comprehensive empirical study validates these advantages. Across diverse benchmarks, SFR Net consistently outperforms strong specialized baselines, including Transformer-based models. Furthermore, it remains highly competitive with powerful self-supervised learning methods, despite being trained end-to-end without the pre-training step. Our ablation studies rigorously quantify the contribution of each architectural module, proving that the performance gains stem from the principled decomposition and dynamic routing, not brute-force capacity. These results position Sparse Feature Routing as a transparent, efficient, and powerful foundation for deep tabular learning.", "tldr": "We show that decomposing tabular learning by learning a sparse, instance-wise selection over independent, per-feature experts is a simple and powerful foundation for building accurate and transparent models.", "keywords": ["Sparsity", "feature experts", "tabular representation learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da513b74f53f455c37a8b5b6c4605846d62f04ce.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Sparse Feature Routing (SFR Net): per-feature “experts” (tiny MLPs / embeddings) produce vectors that are softmax-gated by an “instance-wise sparse router” with an entropy penalty; a low-rank interaction head (factorization-style) models pairwise effects; a final MLP predicts the target. On four datasets (two classification: AD, JA; two regression: HE, CA) SFR Net reportedly outperforms several deep tabular baselines (MLP, DCNv2, AutoInt, FT-Transformer) and is competitive with XGBoost/CatBoost; ablations on Adult attribute gains to the feature-wise decomposition, routing, and the entropy regularizer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•\tClear decomposition (feature-wise experts + mixer) with simple end-to-end training.  \n\t•\tReadable method section and schematic; ablation table on Adult is helpful.  \n\t•\tAttempts to provide per-instance attributions via router weights"}, "weaknesses": {"value": "The design closely parallels NAMs/GA2M (feature-wise functions) plus factorization-style interaction (FM/DCNv2) and feature selection ideas from TabNet / conditional computation. The router uses standard softmax + entropy (not sparsemax/top-k gating), so claimed “sparsity” is largely peaky-but-dense attention. The paper positions this as a novel inductive bias, but the components and motivation feel incremental.  \n\nResults rely on four datasets; yet the abstract and discussion claim “across diverse benchmarks” and competitiveness with strong SSL methods. Modern, decisive baselines for tabular learning—TabM, TabPFN(v2), TabICL, CatBoost with careful HPO—are absent (only FT-Transformer/TabNet/etc. are included). With such limited scope, the paper cannot substantiate “transparent, efficient, and powerful foundation” claims.  \n\nUsing entropy-regularized softmax does not yield true zeros, and attention weights are not guaranteed faithful attributions under feature correlation. There’s no faithfulness check (remove top-k features and measure Δ performance), stability across seeds, or agreement with perturbation/SHAP. Claims of “native interpretability” therefore remain speculative.  \n\nThe paper asserts CPU-friendliness and linear scaling in feature count, but provides no runtime, FLOPs, or energy comparisons vs baselines (including tabular FMs/DCNv2/GBDTs). Ablations mention fewer epochs on Adult, but wall-clock comparisons are absent.  \n\nCritical knobs—entropy weight λ, rank K, expert width/depth, and router temperature—lack robustness sweeps. The “sparsity” benefit over dense routing is tiny in Table 3; without broader ablations, it’s unclear that entropy-sparsification is consistently helpful.  \n\nHow ResNet backbones are adapted to tables (reshaping? tokenization?) isn’t specified; the comparison risks being apples-to-oranges and distracts from missing state-of-the-art tabular baselines."}, "questions": {"value": "Replace softmax+entropy with sparsemax/entmax or top-k gating and compare—does true sparsity help?  \nProvide faithfulness tests (deletion/keeping-k, knockoffs) and stability checks for router attributions.  \nAdd modern baselines (TabM, TabPFN/TabICL) and a larger benchmark (OpenMLCC18 or comparable), with paired tests and critical-difference diagrams.  \nReport compute (GPU/CPU time, params, memory) vs DCNv2/FT-Transformer/GBDTs; include cost-normalized leaderboards.  \nRun robustness to irrelevant/correlated/noisy features and missing-value handling; test scaling to high-dimensional tables."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jzrjdwG0Wc", "forum": "PywcMGMg4f", "replyto": "PywcMGMg4f", "signatures": ["ICLR.cc/2026/Conference/Submission20814/Reviewer_8cBT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20814/Reviewer_8cBT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760727815202, "cdate": 1760727815202, "tmdate": 1762999996667, "mdate": 1762999996667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new tabular deep learning model, SFR-Net, which employs feature-wise independent expert networks and uses instance-dependent routing to combine feature-wise representations. The proposed approach is compared with three deep learning baselines across four datasets, and the authors claim superior performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* Interesting idea on MoE-style model for tabular data\n* Important ablations in Section 4\n* Overall, paper is clearly written"}, "weaknesses": {"value": "See questions for details. \n\nSummary of weaknesses:\n* Limited number of datasets and baselines\n* No details on experimental setup\n* Missing comparison with NAM and numerical embeddings\n* Poor empirical performance"}, "questions": {"value": "My main concern is that the results are not convincing. Full list of questions:\n\n1. Modern tabular DL papers typically use benchmarks with dozens of datasets, whereas this paper uses only four but still claims a “comprehensive empirical study” (L19). [1, 2, 3]\n2. What was the motivation for choosing the baselines? All selected models are relatively old, while many modern and stronger DL baselines exist. [1, 2, 3]\n3. What is the training protocol? How HPO was performed? What values of hyperparameters are used?\n4. The paper mentions NAM in the related work but does not compare against it. I believe this is an essential baseline since your method is closely related to NAM.\n5. Embeddings for numerical features [4] are also very relevant since numerical embeddings essentially are feature-wise independent neural networks but they are concatenated in one big representation. While NAMs are more closely related to your approach, incorporating this method might improve your results.\n6. An ablation study analyzing the benefits of higher-order interactions would be insightful.\n7. Analysis on weights $\\alpha$ would help reveal whether there is any sparsity.. Additionally, it is unclear why the method is referred to as “sparse.” The entropy loss on $\\alpha$ does not necessarily imply that many weights will be sparse.\n8. The authors claim effective training and inference (L74), but no supporting experiments or results are provided. On large datasets, a standard MLP is likely to be more efficient.\n9. The ablations in Section 4.3 are valuable but are conducted on a single dataset, which may make the conclusions data-dependent.\n10. Authors provide a performance of GBDTs \"for reference\" but modern DL architectures generally outperform  GBDTs.\n11. Please, explain motivation for comparing with SSL methods while there is no comparison with strong tabular DL architectures?\n\n\n[1]: Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular Data. David Holzmüller, Léo Grinsztajn, Ingo Steinwart. 2024.  \n[2]: TabM: Advancing tabular deep learning with parameter-efficient ensembling. Yury Gorishniy, Akim Kotelnikov, Artem Babenko . 2025.  \n[3]: Accurate predictions on small data with a tabular foundation model. Hollmann et al. 2025.  \n[4]: On Embeddings for Numerical Features in Tabular Deep Learning. Yury Gorishniy, Ivan Rubachev, Artem Babenko. 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NiEbtDWbbF", "forum": "PywcMGMg4f", "replyto": "PywcMGMg4f", "signatures": ["ICLR.cc/2026/Conference/Submission20814/Reviewer_hfF6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20814/Reviewer_hfF6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660113641, "cdate": 1761660113641, "tmdate": 1762935899446, "mdate": 1762935899446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **SFR Net**, a decomposed architecture for tabular learning that routes each instance over **independent, per-feature experts** via a **sparse router**, then mixes selected features with a **low-rank interaction module**. \n\nConcretely, each feature $x_j$ is processed by its expert $E_j$ to produce $h_j \\in \\mathbb{R}^D$. A shared scoring MLP outputs scores $s_j$; the router produces instance-wise weights\n\n$$\n\\alpha_j=\\frac{\\exp(s_j)}{\\sum_{k=1}^{F}\\exp(s_k)},\\qquad\nH(\\alpha)=-\\sum_{j=1}^{F}\\alpha_j\\log\\alpha_j,\n$$\n\nand adds an entropy penalty $\\lambda,H(\\alpha)$ to encourage sparsity. First-order effects are aggregated as\n\n$$\nr^{(1)}=\\sum_{j=1}^{F}\\alpha_j,h_j,\n$$\nwhile higher-order interactions use shared low-rank projections $W_K,W_V\\in\\mathbb{R}^{D\\times K}$ with $K\\ll D$:\n$$\nk_j=h_j^\\top W_K,\\quad v_j=h_j^\\top W_V,\\quad\nr^{(2)}=\\sum_{j=1}^{F}\\alpha_j,(k_j \\odot v_j),\\quad\nr_{\\mathrm{final}}=[,r^{(1)}; r^{(2)},].\n$$\n\nA final MLP maps $r_{\\mathrm{final}}$ to predictions, trained with the task loss plus $\\lambda,H(\\alpha)$. The design aims to yield **native instance-level attributions** (the router’s $\\alpha$) and computational efficiency. Experiments on several benchmarks indicate that SFR Net outperforms strong Transformer-based tabular baselines and is competitive with GBDTs; it also compares favorably to self-supervised pretraining approaches despite **no pretraining**."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Principled decomposition:** One expert per feature + instance-wise sparse routing provides a transparent, task-aligned inductive bias for tabular heterogeneity.\n* **Low-rank interaction head:** Captures higher-order effects efficiently under a tunable rank budget $K$.\n* **Native attributions:** Router weights $\\alpha$ offer per-instance explanations without post-hoc methods.\n* **Competitive results without SSL:** Outperforms strong neural baselines and remains competitive with GBDTs; compares well to SSL backbones **without** pretraining cost.\n* **Ablations:** Sensible ablations indicate gains stem from decomposition, routing, and sparsity rather than brute-force capacity."}, "weaknesses": {"value": "* **Dataset breadth:** The evaluation spans only a small number of datasets; lacks a larger, standardized suite (e.g., 10–20 public tabular benchmarks) with **average-rank** analyses and significance tests.\n* **Sparsity mechanism:** Entropy regularization yields soft sparsity; comparisons to **hard top-$k$** or **entmax/sparsemax** would clarify sparsity–accuracy–efficiency trade-offs. Reporting the **average selected feature count** would help.\n* **Complexity accounting:** No explicit wall-clock or memory comparisons vs. FT-Transformer/GBDTs/SSL; empirical scaling in $F,D,K$ is not quantified.\n* **Interaction coverage:** The low-rank mixer may undercapture very high-order/non-linear dependencies unless $K$ grows; guidance on choosing $K$ is limited.\n* **Robustness:** Systematic evaluations under missingness, extreme categorical cardinality, and distribution shift are not reported."}, "questions": {"value": "1. **Router sparsity:** What is the **average number of selected features per instance** as a function of $\\lambda$? Have you tried **hard top-$k$** or **entmax/sparsemax** routing, and how do accuracy/efficiency/attributions change?\n2. **Complexity & scaling:** Please report wall-clock (train/infer) and peak memory vs. $F,D,K$, and provide Pareto curves (accuracy vs. time/memory) against FT-Transformer, GBDTs, and SSL baselines.\n3. **Low-rank sensitivity:** How sensitive are results to $K$? On heavily interacting datasets, does increasing $K$ help, or would an additional cross-network/FM-style term improve performance?\n4. **Attribution fidelity:** Do router weights correlate with SHAP/Integrated Gradients? Any randomization or sanity checks to validate attribution robustness?\n5. **Robustness:** How does the router behave under **missing values**, extreme categorical cardinality, and covariate/label shift? Does sparsity sharpen or degrade under noise?\n6. **Benchmark breadth:** Can you expand to a larger public suite (OpenML/UCI) and report **average ranks** and **statistical tests** to bolster generality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EuhtuYJvL3", "forum": "PywcMGMg4f", "replyto": "PywcMGMg4f", "signatures": ["ICLR.cc/2026/Conference/Submission20814/Reviewer_RK1z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20814/Reviewer_RK1z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933276348, "cdate": 1761933276348, "tmdate": 1762935858657, "mdate": 1762935858657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel neural architecture for supervised learning on tabular data, SFR Net, which consists of a per-feature component and a low-rank component. The paper compares the proposed architecture against some other supervised deep learning architectures, self-supervised tabular models, and standard tree-based models on four datasets, and finds that SFR Net outperforms the supervised models and ranks among the top unsupervised models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a classical problem of deep supervised learning on tabular data.\n- The paper proposes a relatively simple architecture with an eye on interpretability and understandability of the architecture and model."}, "weaknesses": {"value": "- Proposing a new architecture for a common tasks rests on the empirical evaluation of the model. Using four datasets is insufficient. Please use the TabArena benchmark, which consists of 51 datasets, or at least another recent benchmark suite like CC-18/CTR, TabZilla or Talent.\n\n- The paper does not compare against any recent deep architectures such as TabM or RealMLP, and completely disregards current state-of-the-art foundational models such as TabPFN V2, TabICL, LimiX and TabDPT. Taking into account these models, many of the claims in the introduction are false, such as claiming \"ill-suited, monolithic backbones\". This seems not appropriate for any of these foundational models. In particular these use per-feature embeddings (all foundational tabular models except for TabPFN V1 do afaik).\n\n- The appeal to interpretability of the models is interesting and a good motivation, but there is no experiments on interpretability, and it's unclear how the low-rank components could be interpreted."}, "questions": {"value": "- How is the feature-wise experts different from a NAM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IieRM3h5LE", "forum": "PywcMGMg4f", "replyto": "PywcMGMg4f", "signatures": ["ICLR.cc/2026/Conference/Submission20814/Reviewer_xRPF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20814/Reviewer_xRPF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975504491, "cdate": 1761975504491, "tmdate": 1762935817104, "mdate": 1762935817104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}