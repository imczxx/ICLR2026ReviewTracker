{"id": "KBqEwFZEgz", "number": 9927, "cdate": 1758149534229, "mdate": 1759897684931, "content": {"title": "HoRA: Cross-Head Low-Rank Adaptation with Joint Hypernetworks", "abstract": "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) technique that adapts large pre-trained models by adding low-rank matrices to their weight updates. However, in the context of fine-tuning multi-head self-attention (MHA), LoRA has been employed to adapt each attention head separately, thereby overlooking potential synergies across different heads. To mitigate this issue, we propose a novel Hyper-shared Low-Rank Adaptation (HoRA) method, which utilizes joint hypernetworks to generate low-rank matrices across attention heads. By coupling their adaptation through a shared generator, HoRA encourages cross-head information sharing, and thus directly addresses the aforementioned limitation of LoRA. By comparing LoRA and HoRA through the lens of hierarchical mixture of experts, our theoretical findings reveal that the latter achieves superior sample efficiency to the former. Furthermore, through extensive experiments across diverse language and vision benchmarks, we demonstrate that HoRA outperforms LoRA and other PEFT methods while requiring only a marginal increase in the number of trainable parameters.", "tldr": "", "keywords": ["Low-rank Adaptation", "Multi-head Self-attention", "Mixture of Experts", "Hypernetworks"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54d8fd46c95a609520f1bbc58c8630277df75db7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces HoRA, a parameter-efficient fine-tuning method designed to overcome LoRA’s limitation of independently adapting each attention head. The authors reinterpret multi-head LoRA through the lens of Hierarchical Mixture of Experts and prove that this shared structure improves sample efficiency from exponential to polynomial rates. Empirical evaluations show consistent improvements over LoRA, DoRA, and adapters with ~0.1% additional parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper is well written and provides a conceptually sound theoretical grounding for parameter sharing mechanisms. The insight that attention heads often exhibit redundancy is well known, and formalising this through a shared hypernetwork is an intuitive and promising approach. Cross-domain validation demonstrates flexibility across modalities."}, "weaknesses": {"value": "W1. Some assumptions used for theoretical proofs may not be realistic in practical situations. For example, independent per-head subspaces are assumed to simplify the HMoE formulation, where each head corresponds to a separate expert with no shared latent factors. This is necessary for deriving Theorem 1, which provides the lower bound on sample efficiency under the non-shared setting. While this is not an inconsistency in the theory, attention heads in practice are coupled through shared normalisation layers, residuals, and joint gradients. Does this meaningfully impact the realism of the theory?\n\nW2. Recent works like ReLoRA, AdaLoRA, Compacter, and HyperPEFT are either omitted or cited only tangentially. Since HoRA is a hypernetwork-based PEFT variant, stronger baselines with shared generators would improve the paper. Runtime and FLOPS analyses could also be included to strengthen efficiency claims.\n\nW3. The formal analysis is mathematically rigorous but slightly disconnected from the empirical evaluation: the theorems rely on simplified mixture-model and Gaussian-noise assumptions, whereas experiments involve large-scale transformer fine-tuning. The paper could more clearly explain how the theoretical insights translate into architectural choices (e.g., activation functions, normalisation strategy, or hypernetwork structure)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ewP1ctQ3Zr", "forum": "KBqEwFZEgz", "replyto": "KBqEwFZEgz", "signatures": ["ICLR.cc/2026/Conference/Submission9927/Reviewer_zvwU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9927/Reviewer_zvwU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634947272, "cdate": 1761634947272, "tmdate": 1762921381738, "mdate": 1762921381738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HoRA (Cross-Head Low-Rank Adaptation), a new PEFT method designed to enhance the adaptability of large pre-trained models by introducing cross-head low-rank interactions. Unlike conventional LoRA-based approaches that apply low-rank updates independently to each attention head, HoRA allows cross-head sharing and interactions to better capture inter-head dependencies while maintaining efficiency. The authors provide a detailed theoretical analysis that establishes the expressiveness and rank properties of HoRA, supported by formal proofs and ablation-based empirical validation. Extensive experiments across both natural language processing and vision benchmarks demonstrate consistent performance improvements over baselines such as LoRA, DoRA, and HiRA under similar parameter budgets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper offers rigorous and clearly structured mathematical analysis, including rank-related theorems and proofs that ground the proposed cross-head low-rank formulation in solid linear algebraic reasoning. This theoretical depth enhances the credibility of the method and distinguishes it from many empirical-only PEFT studies.\n\n- The authors conduct extensive experiments across NLP and vision tasks, showing HoRA‘s performance surpasses or matches strong baselines while maintaining comparable parameter efficiency. The evaluation includes detailed ablation and convergence studies that support the claimed benefits.\n\n\n- The paper is well-organized and easy to follow. Figures and tables effectively complement the text. The proofs are self-contained and pedagogically written."}, "weaknesses": {"value": "(1) **Limited experimental diversity and overemphasis on sample efficiency.** While the paper provides compelling evidence that HoRA improves sample efficiency in few-shot and low-data settings, the experimental validation remains narrowly focused on this aspect. It is unclear whether the cross-head hypernetwork design continues to yield benefits under large-scale or high-resource regimes, or whether the gains vanish when data is abundant. Broader experiments on full-data fine-tuning, convergence behavior, and computational trade-offs would significantly strengthen the empirical foundation.\n\n(2) **Limited Analysis of Rank and Correlation Dynamics.**\nThe theoretical section claims improved expressiveness due to cross-head mixing, yet the experiments do not empirically examine the effective rank or correlation between head updates after fine-tuning. Providing SVD-based or cosine-similarity analysis would substantiate the claim that HoRA genuinely induces more diverse and high-rank adaptations rather than introducing redundant cross-talk."}, "questions": {"value": "- **Experimental scope appears narrow, focusing mainly on few-shot sample efficiency.**\nSpecifically, I suggest including convergence curves, runtime or FLOPs comparisons, and GPU memory usage statistics versus LoRA/HiRA. These would clarify whether HoRA maintains its efficiency and performance benefits when data and computational resources are not constrained.\n\n- **Scaling experiments to larger foundation models.** Have the authors tested HoRA on larger foundation models (e.g., LLaMA-13B or ViT-L)? It would be valuable to see whether the cross-head benefits persist as model size grows, or if the method’s advantage diminishes with increased intrinsic capacity.\n\n- **Practical Applicability.** Since HoRA introduces inter-head coupling, does it affect the modularity or mergeability of adapters (e.g., combining task-specific adapters as in LoRA-Merge)? Discussing compatibility with existing PEFT deployment frameworks would clarify its real-world usability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rYnGLs3eXz", "forum": "KBqEwFZEgz", "replyto": "KBqEwFZEgz", "signatures": ["ICLR.cc/2026/Conference/Submission9927/Reviewer_f5c6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9927/Reviewer_f5c6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835337413, "cdate": 1761835337413, "tmdate": 1762921381348, "mdate": 1762921381348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to work with hypernetworks, which generate low-rank matrices across attention heads and layers.\nTo achieve this goal the paper establishes a theoretical connection between low rank adaptation methods, multi-head self attention and hierarchical mixture of experts.\nThe paper's theoretical considerations suggest that the mechanism improves sample efficiency from an exponential to polynomial rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Very well written background section\n- Innovative idea\n- Theoretically and experimentally sound."}, "weaknesses": {"value": "- The experimental section does not compare to tensor decomposition based finetuning work.\n- The paper does not tell us much about its computational cost."}, "questions": {"value": "- The paper approaches the multi-head tuning problem from a hyper-network perspective, which is refreshing. \n- Related work employs tensor decompositions i.e. Tucker\nor Tensor-Train to capture correlation along the head dimension (see https://arxiv.org/pdf/2212.03145 for example). Since this is similar in spirit, perhaps it's worth comparing to this and similar approaches?\n- Similarly, this paper's results for VTAB-1k are better than what the NAS-Based NOAH paper reported (https://arxiv.org/pdf/2206.04673). Using the numbers from the NOAH paper\n``` python\nnp.mean([69.6, 92.7, 70.2, 99.1, 90.4, 86.1, 53.7, 84.4, 95.4, 83.9, 75.8, 82.8, 68.9, 49.9, 81.7, 81.8, 48.3, 32.8, 44.2])\n```\nyields $73.24$, HoRA is better. Unfortunately, NOAH reports a group wise mean which is misleading. This paper could be an opportunity to set the record straight. Perhaps, the authors are willing to do it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CmWRsaSiHK", "forum": "KBqEwFZEgz", "replyto": "KBqEwFZEgz", "signatures": ["ICLR.cc/2026/Conference/Submission9927/Reviewer_SKLj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9927/Reviewer_SKLj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762022448947, "cdate": 1762022448947, "tmdate": 1762921381081, "mdate": 1762921381081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}