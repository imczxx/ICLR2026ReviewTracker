{"id": "Q78kLO9rfB", "number": 17986, "cdate": 1758282683432, "mdate": 1759897141014, "content": {"title": "CoCoPIF: Benchmarking Conversational Coding and Programmatic Instruction Following", "abstract": "Code generation with large language models (LLMs) has become popular to software development, yet existing benchmarks like HumanEval and LBPP focus on single-turn task completion. In real-world scenarios, users often engage in multi-turn interactions, iteratively refining code through instruction-following feedback to meet complex requirements or constraints. Current benchmarks fail to capture the dynamic, instruction-driven nature of such workflows. To address this, we introduce CoCoPIF, a new evaluation pipeline for evaluating LLMs in multi-turn, instruction-following code generation, by emulating real-world interaction data from ShareGPT and problems from LiveCodeBench. Our framework dynamically transforms code problems into multi-turn tasks with verifiable instructions. It features an evaluation protocol that mirrors user-LLM interaction by iteratively refining model outputs through targeted feedback. Furthermore, our assessment approach evaluates both instruction adherence and functional correctness, delivering a reliable measurement of model performance. CoCoPIF reflects practical coding scenarios, providing a tool to assess LLMs in realistic, interactive programming contexts.", "tldr": "", "keywords": ["Large language models (LLMs)", "Multi-turn Benchmark", "Instruction-following", "Code generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/601dd8e41752f89d08beb0cc0186cb20a628b8a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a benchmark for conversational coding as multi-turn, instruction-following tasks. \nBoth functional correctness and programmatic instruction adherence are evaluated to reflect multi-turn development workflows."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "*Realistic target scenario: multi-turn code refinement and instruction-following reflects realistic code generation scenarios. \n*Evaluation Criteria: Functional correctness and adherence reflect the needs of target scenario\n* Dynamic evaluation: This setting reduces the risk of data contamination/memorization."}, "weaknesses": {"value": "* Important benchmarks: Simulating users providing iterative feedback to help model complete coding tasks across multiple turns (Wang et al.; Han et al., Laban et al., 2025) need to be cited/discussed\n* Evaluation cost: time/infra cost for evaluation needs to be analyzed/justified.\n* Dependence: Sensitivity to the quality and clarity of the dynamically generated programmatic instructions need to be analyzed/justified.\nXingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. In The Twelfth International Conference on Learning Representations.\nHojae Han, Rajhans Samdani, Yuxiong He, et al. Convcodeworld: Benchmarking conversational code generation in reproducible feedback environments. In The Thirteenth International Conference on Learning Representations.\nPhilippe Laban, Hiroaki Hayashi, Yingbo Zhou, and Jennifer Neville. Llms get lost in multi-turn conversation. arXiv preprint arXiv:2505.06120, 2025."}, "questions": {"value": "Please address questions in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VXrG1hiYCr", "forum": "Q78kLO9rfB", "replyto": "Q78kLO9rfB", "signatures": ["ICLR.cc/2026/Conference/Submission17986/Reviewer_rNrz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17986/Reviewer_rNrz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761455309659, "cdate": 1761455309659, "tmdate": 1762927783111, "mdate": 1762927783111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CoCoPIF, a benchmark designed to evaluate LLMs in multi-turn, instruction-following code generation. It integrates programming problems from LiveCodeBench and instruction data distilled from ShareGPT, forming multi-turn interactions where models iteratively refine code under verifiable constraints. The benchmark aims to measure both instruction adherence and functional correctness while maintaining contamination-free evaluation. The authors evaluate several proprietary and open-source LLMs and report interesting findings on instruction-forgetting and difficulty sensitivity across models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The discussion on \"instruction forgetting\" is interesting and could inspire follow-up analyses."}, "weaknesses": {"value": "- Missing Core Related Work: Table 1 omits several directly comparable multi-turn or interaction-based benchmarks such as [1,2,3,4]. Particularly, MINT [2] is cited in the Related Work section yet omitted in Table 1, which is questionable. \n\n> (lines 183-185) Our work provides a new perspective, evaluating model efficiency in multi-turn scenarios by measuring the number of turns required to complete a code problem, taking into account the unique characteristics of code-related tasks.\n\n- Novelty Concern: Contrary to the claim above, similar efficiency-based evaluation has already been proposed in ConvCodeWorld [3], which quantifies turn efficiency using a Mean Reciprocal Rank (MRR) formulation—i.e., scoring each task by the inverse of the turn index at which the problem is first solved. \n\n- Weak Distinction from Prior Benchmarks: While the paper provides extensive analysis, the conceptual distinction from prior works [1,2,3,4] is under-articulated. The evaluation pipeline largely combines existing components—LiveCodeBench for contamination-free problems and ShareGPT for user-like instructions—without introducing a fundamentally new evaluation paradigm. \n\n\n[1] John Yang, Akshara Prabhakar, Karthik R Narasimhan, and Shunyu Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=fvKaLF1ns8.\n\n[2] Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. MINT: Evaluating LLMs in multi-turn interaction with tools and language feedback. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=jp3gWrMuIZ.\n\n[3] Hojae Han, seung-won hwang, Rajhans Samdani, and Yuxiong He. ConvCodeWorld: Benchmarking Conversational Code Generation in Reproducible Feedback Environments. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=rpouyo09V0.\n\n[4] Zimu Lu, Yunqiao Yang, Houxing Ren, Haotian Hou, Han Xiao, Ke Wang, Weikang Shi, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Webgen-bench: Evaluating llms on generating interactive and functional websites from scratch, 2025b. URL https://arxiv.org/abs/2505.03733."}, "questions": {"value": "Suggestion: \n- Figure 3 (instruction-type distribution) is descriptive but not central to the paper’s main claims; it would fit better in the Appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EUWwNR5Mlb", "forum": "Q78kLO9rfB", "replyto": "Q78kLO9rfB", "signatures": ["ICLR.cc/2026/Conference/Submission17986/Reviewer_YBjb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17986/Reviewer_YBjb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835398735, "cdate": 1761835398735, "tmdate": 1762927782547, "mdate": 1762927782547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CoCoPIF addresses an important and growing problem in the code editing space which is **realistic** and **multi-turn** interactions. This area is still under-explored and the paper tackles an important first step in attempting to develop a new benchmark for the task. Unfortunately, the paper emphasizes realistic code, yet uses unrealistic source benchmarks and does not have a clear takeaway."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem is original and novel\n- the presentation of the work, for the most part, is clear and easy to follow. \n- the benchmark contains over 800+ problems which is substantial."}, "weaknesses": {"value": "The paper emphasizes the importance of **realistic** instructions and code, but it uses code from livecodebench which is unrealistic by nature. Additionally, the instructions aren't real user instructions, but instructions deconstructed from real user instructions and then generated via an LLM. A better, integrated source of user instructions would make for a stronger case.\n- It's unclear what the main takeaway of this paper is. Is it just that models can't do multi-turn as well due to instruction forgetting? Clearer takeaways would provide for a stronger impact.\n- Additionally, if there's a focus on real users instructions it seems remiss to not include any references to Arena style work such as:\n\t- Chatbot Arena (WL Chiang, et. al.) (General Chat with Instructions / Multi-turn)\n\t- Copilot Arena (W Chi, et. al.) (Code Edits with in-the-wild Instructions)\n- Lastly, this might be a personal preference, but I think the notation is unnecessary. It's fairly easily describable in words and not complex enough to need notation. Also, notations is not re-used."}, "questions": {"value": "I do not fully understand how the ShareGPT instructions are merged with the livecodebench problems. My understanding is that the ShareGPT instructions are just broad templates that a model (deepseek/gpt-4o-mini?) then use to format to livecodebench problems. Is this correct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oQGW6Og7b9", "forum": "Q78kLO9rfB", "replyto": "Q78kLO9rfB", "signatures": ["ICLR.cc/2026/Conference/Submission17986/Reviewer_NftL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17986/Reviewer_NftL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857017953, "cdate": 1761857017953, "tmdate": 1762927782093, "mdate": 1762927782093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-turn coding capability evaluation based on LiveCodeBench. Motivated by the use of LLMs for coding in multi-turn environments, the authors create a pipeline to transform algorithmic coding problems into multi-turn variants. Specifically, they first analyze the existing coding chat logs out of ShareGPT to obtain a taxonomy of user query topics. Then they run GPT-4o-mini for each full problems to obtain initial solutions, which helps filter the kind of multi-turn instructions to use. Finally, sampled instructions are paraphrased by Deepseek-v3.\nEvaluation on this derived dataset shows that smaller / open-source models particularly struggle to follow instructions after the first turn. They also analyzed that certain types of instructions that require accurate tracking of variables are extra difficult for all models. Additionally they observe various instruction forgetting cases across models. Interestingly there doesn't seem to be correlation between models' capability and the forgetting rate, but rather related to the recency of models (v3 vs v3.1, 3.7-sonnet vs sonnet-4)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* A great diagnostic dataset for measuring fine-grained multi-turn instruction following capability in the code domain, enabled by data construction based on carefully analyzed real-life multi-turn code usecases.\n* Interesting analyses on completion rate per instruction type. This motivates more targeted reasoning / perhaps other architectures that enable better state tracking.\n* The authors tackle a highly practical problem. Focuses on multi-turn coding is well-aligned with community interest."}, "weaknesses": {"value": "* Somewhat unclear description of transformation pipeline itself, which is one of the main contributions.\n  * If my summary above is correct, do you have a fixed set of multi-turn instructions per problem, regardless of what models to evaluate against? \n  * Obtaining the instructions based on gpt-4o-mini-generated solutions mean that the instruction set is accordingly biased. Is that true?\n  * How do you determine what instruction to show in turn i?\n* The set up to show the entire problem upfront is not fully convincing. The authors mention that this setup \"mimick(ing) real user interactions\", but I don't see evidence for that. Specifically, how often do users in ShareGPT stick to the same problem but introduce new information / constraints? Something like \"Oh by the way I forgot that you need to output in int, not float\".\n* While LCB is a straightforward resource for contamination-free dataset with verifiable tests, it's not clear if the sampled ShareGPT data to draw the instruction taxonomy is aligned with LCB-style questions. What kind of \"keyword-based screening\" have authors performed to ensure the data is code-related AND in the style of LCB?"}, "questions": {"value": "Please see above. \n\nAdditionally, I would be curious to see the performance on more recent models, because the analysis suggest that some recently updated models overcame instruction forgetting.\n\nInstruction forgetting is also observed by [1], which also evaluated multi-turn version of LCB but in an incremental problem specification. I'd suggest including in the references.\n\n[1]: Laban, P., Hayashi, H., Zhou, Y., & Neville, J. (2025). LLMs get lost in multi-turn conversation. arXiv preprint arXiv:2505.06120."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1qFLvB6iHn", "forum": "Q78kLO9rfB", "replyto": "Q78kLO9rfB", "signatures": ["ICLR.cc/2026/Conference/Submission17986/Reviewer_UeBu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17986/Reviewer_UeBu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939916704, "cdate": 1761939916704, "tmdate": 1762927781552, "mdate": 1762927781552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}