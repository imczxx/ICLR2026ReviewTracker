{"id": "Q524K1mgCc", "number": 7519, "cdate": 1758025710251, "mdate": 1759897848315, "content": {"title": "LATTS: LAtent space Test Time Scaling for diffusion language models", "abstract": "Test-time scaling (TTS) improves the performance of autoregressive (AR) large language models by adding computation at inference.\nWhile the prominent sequential TTS enhances accuracy by inducing models to generate longer chain-of-thought (CoT) reasoning, its computational overhead emerges as a drawback.\nMeanwhile, diffusion large language models (DLLMs) have emerged as a promising alternative that offers parallel decoding and self-correction capabilities.\nHowever, existing sequential TTS methods are incompatible with modern masked DLLMs.\nThis incompatibility arises from two fundamental constraints:\n(1) DLLMs operate holistically on fixed-length sequences, preventing the dynamic token-level expansion required for CoT,\nand (2) DLLMs' number of refinement (i.e., denoising) steps is capped by the sequence length, precluding further quality improvements via additional computation.\nWe introduce LATTS, a novel sequential TTS method for DLLMs that overcomes the above challenges by operating in the latent embedding space.\nLATTS reframes CoT reasoning from a spatial process of extending sequence length to a temporal process that uses additional computation to extend the iterative self-refinement steps over the entire sequence's latent representation.\nOur evaluation on the LLaDA-Instruct model shows that LATTS achieves notable improvements over SFT baselines on reasoning and code generation benchmarks\nwith gains of +4.1\\% on GSM8K, +4.8\\% on MATH, +3.2\\% on MBPP, and an average of +4.6\\% on commonsense reasoning tasks with minimal additional inference computation.\nThese results establish sequential TTS as a promising technique for optimizing DLLMs.", "tldr": "We introduce LATTS, a latent space test time scaling method to enhance the performance of masked diffusion language models.", "keywords": ["Diffusion Large Language Model", "Test Time Scaling", "Latent Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/788d868ce91d36f533e86c54f228e51492eb5aa6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces LATTS, a method for test-time scaling in masked diffusion language models such as LLaDA. LATTS works by initializing and operating in the latent embedding space, enabling iterative self-refinement over a fixed-length sequence. This approach leads to notable gains over standard supervised fine-tuning baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental setup for training, and comparing LATTS and SFT is fair, the authors use the same training duration and hyperparameters. \n2. On all reported tasks, LATTS improves the performance over regular SFT."}, "weaknesses": {"value": "### Major weaknesses\n1. The sampling procedure after initialization with LATTS is unclear. Are you using standard ancestral sampling starting from the LATTS embeddings? Do you decode intermediate tokens, or do you operate entirely in latent space without explicit decoding? Please clarify the exact sampling steps after initialization.\n2. **Framing**: LATTS is described as a test-time scaling (TTS) method, but has two fundamental differences with regular test-time scaling approaches.\n    - The authors *do not* increase the computational resources allocated to sampling, in order to measure test-time scaling. Instead, the authors argue that LATTS is as expensive as standard ancestral sampling. The \"Temporal CoT\" metaphor is also misleading: LATTS does not add extra denoising or refinement steps to answer generation, unlike regular CoT reasoning in AR models.\n    - Secondly, unlike standard TTS methods, which use a fixed base model and require no fine-tuning, LATTS involves additional fine-tuning. This distinction is unclear in the abstract and introduction and may mislead readers about the actual contribution. \n- The paper makes several claims that are not adequately supported by experimental evidence. For example:\n    - Lines 195-196: The authors seek embeddings that are *\"semantically rich and sufficiently abstract\"*, but provide no evidence or analysis to support this.\n    - On lines 82-83, the authors claim that LATTS *\"mitigates the error propagation in spatial CoT and fully leverages the self-correction capabilities of DLLMs.\"*. I did not find evidence of self-correction or mitigation of error propagation in this submission.\n- The paper contains several inconsistent statements:\n    - The LATTS sampling process, is self-contradictory. First, LATTS produces $n$ draft tokens and aggregates their embeddings using a fixed operation like a mean, into a single vector, with the stated goal of avoiding starting sampling too close to a low-energy region (lines 204-205). Then, on lines 223-224, this vector is copied $K$ times (for $K$ mask tokens) to form the initial matrix, and the authors claim this shifts the starting point closer to a low-energy region. This is confusing: are you trying to avoid or approach the low-energy region in initialization? Please clarify the intended design and the reasoning.\n    - Relatedly, lines 232-233 claim that the initialized embeddings *\"implicitly encode token-wise similarities.\"* However, since lines 239-240 state that all embeddings are created by duplicating the same vector, how can they meaningfully represent token-wise similarities? This needs clarification.\n\n### Other weaknesses\n1. The abstract inaccurately claims dLLMs require fixed-length generation. Prior work, such as Llada supports variable and semi-autoregressive generation, *which is investigated in this paper in Table 8*.\n2. **Limited selection of downstream tasks**. Thanks to LLM evaluation frameworks, such as lm-eval-harness, it should be straightforward to compare LATTS and regular ancestral sampling on multiple downstream tasks, while you only report Arc-challenge and HellaSwag. This is not sufficient and could raise concerns whether the results of LATTS were poor on other tasks.\n\n- The claim that the number of refinement steps is limited by sequence length (abstract) only applies to ancestral sampling. Remasking methods like ReMDM [1] allow more sampling steps than tokens by reintroducing masks.\n\n- Key dLLM works are missing from the citations: D3PM, MDLM, MD4, and RADD.\n\n- Lines 237-238: Please avoid calling your strategy \"optimal\" without evidence; you have not shown there is no better alternative.\n\n[1] Remasking Discrete Diffusion Models with Inference-Time Scaling, Wang et al."}, "questions": {"value": "1. While you report the hyper-parameters for fine-tuning with LATTS and regular SFT, you do not compare the actuall wall-time duration, and throughput of LATTS training and SFT. \n2. Similarly, what is the impact of LATTS on sampling speed? As stated in the weaknesses, it is not clear to me from the paper how sampling is implement, besides the initialization of tokens. The caption of Figure 3 states *\"estimated inference time computation (lower better)\"*, however it is not clear what units of measurement is used\n3. On lines (274-275), you state the the length of the draft is always 50% of the total sequence length. Why this choice? What happens if you use 10%, 25%, 75% of the total tokens as draft?\n4. On line 281, you say that you use the standard lm_eval framework. Are you using lm-eval-harness? If so, can you cite it? Note that lm-eval-harness was designed for autoregressive models, in which case, how are you adapting it for diffusion language models?\n5. The \"Temporal Remask\" baseline performs poorly, which serves as a key justification for LATTS's more complex initialization strategy. However, this baseline remasks tokens randomly. How would LATTS compare to a stronger baseline that remasks tokens based on the lowest confidence scores from an initial generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nb0XKXBLoR", "forum": "Q524K1mgCc", "replyto": "Q524K1mgCc", "signatures": ["ICLR.cc/2026/Conference/Submission7519/Reviewer_bbqK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7519/Reviewer_bbqK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761157105, "cdate": 1761761157105, "tmdate": 1762919624234, "mdate": 1762919624234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LATTS, a test time scaling method for masked diffusion language models that reframes chain of thought from a spatial process, adding tokens, to a temporal process, adding refinement steps in latent space. Concretely, the model first self-samples a short draft answer, aggregates the draft’s token embeddings into a small set of semantic latent vectors, expands these vectors to the target length, mixes them with the mask embedding, and uses this as the new initial latent state for the standard denoise–remask procedure. The intuition is that a semantically informed starting point steers iterative refinement toward better basins in the embedding landscape. The method is simple to implement and requires only a brief post-training stage. Experiments show modest gains on LLaDA-8B across GSM8K, MATH500, MBPP, ARC-Challenge, and HellaSwag relative to supervised fine tuning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The temporal chain of thought view is a clean conceptual bridge for diffusion LMs, and it is well integrated with the masked diffusion decoding loop. The paper clearly formalizes the remask rule and places the method in the latent optimization picture.\n2. The two core components, aggregation of draft embeddings and blockwise expansion with mask mixing, are straightforward and easy to implement.\n3. Timely contribution. Extending TTS to Diffusion LLMs isn’t a trivial task."}, "weaknesses": {"value": "1. The reported improvements over supervised fine-tuning are in the three to five percent range, depending on task and length, which is modest at best.\n2. All core results use LLaDA-8B Base and Instruct. There is no replication on an alternative diffusion LM. This makes it difficult to judge generality.\n3. The potential energy narrative in Figure 2 is insightful but remains qualitative. There is no formal argument connecting the initialization to objective improvement or basin selection under the remask schedule.\n4. The paper argues that naive temporal remasking changes very few tokens post-convergence, which hints at a basin “stickiness” phenomenon, but it does not quantify it across methods or give a more direct look at failure modes.\n5. The illustrative figures are largely uninformative and fail to convey new insight beyond the text"}, "questions": {"value": "1. Can the authors move beyond the current heuristic chunking approach and explore sentence-aware or adaptive segmentation strategies that better align with semantic boundaries? Fixed-size windowing may misrepresent reasoning units within latent space. Recent work demonstrates that boundary-aware chunking substantially enhances retrieval and reasoning, including [1,2,3,4] have also established the benefits of linguistically informed segmentation. Citing and empirically engaging with these works would situate LATTS within the broader literature on semantic chunking and help demonstrate that its latent-space aggregation leverages meaningful linguistic structure rather than relying purely on heuristic pooling.\n2. The paper’s generality claim would be much stronger if replicated on a second masked diffusion LM and at least one additional model size. Even a small scale experiment on another eight billion model, or a four billion model, would help. \n3. The scaling plot shows diminishing returns as extra steps increase. Please extend the curve farther for at least one task and length to find the true plateau, and report the best operating point across lengths.\n4. You sample drafts with half as many denoising steps as the draft length. How sensitive are results to the draft schedule and to the temperature used for the draft? Please include a table that varies draft length, draft steps, and sampling temperature while keeping total compute fixed, to see whether the best practice is to spend compute on a better draft or on more final refinement.\n\n[1] Zhao, Jihao; Ji, Zhiyuan; Feng, Yuchen; Qi, Pengnian; Niu, Simin; Tang, Bo; Xiong, Feiyu; Li, Zhiyu. *Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception.* arXiv preprint arXiv:2410.12788 (2024). \n[2] Zhu, Yuxuan; Falahati, Ali; Yang, David H.; Mohammadi Amiri, Mohammad. *SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV Caching.* arXiv preprint arXiv:2504.00970 (2025). \n[3] Duarte, André V.; Marques, João D. S.; Graça, Miguel; Freire, Miguel; Li, Lei; Oliveira, Arlindo L. *LumberChunker: Long-Form Narrative Document Segmentation.* Findings of EMNLP 2024 (2024). \n[4] Sheng, Boheng; Yao, Jiacheng; Zhang, Meicong; He, Guoxiu. *Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models.* arXiv preprint arXiv:2506.00773 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QoDkagD5R4", "forum": "Q524K1mgCc", "replyto": "Q524K1mgCc", "signatures": ["ICLR.cc/2026/Conference/Submission7519/Reviewer_8UsB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7519/Reviewer_8UsB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920787139, "cdate": 1761920787139, "tmdate": 1762919623719, "mdate": 1762919623719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LATTS (Latent space Test-Time Scaling), a novel method for improving the inference-time performance of diffusion large language models (DLLMs). The authors identify that conventional sequential Test-Time Scaling (TTS) methods, such as Chain-of-Thought (CoT), are incompatible with modern masked DLLMs. This incompatibility stems from two key constraints of DLLMs: they operate on fixed-length sequences and have a limited number of refinement steps, which prevents the dynamic token-level expansion required for CoT.\n\nThe core idea of LATTS is to reframe CoT from a \"spatial\" process (extending sequence length) to a \"temporal\" one (extending iterative refinement in the latent space). The method works in two stages:\n\n1.\tFirst, it uses the model to generate a short \"draft answer\".\n2.\tThe embeddings of this draft are then aggregated (e.g., via average pooling) into a compact set of \"semantic latent vectors\".\n3.\tThese vectors are then used to create a new, semantically-informed initial latent state for the final answer, replacing the standard all-mask starting point.\n\nThe model is briefly post-trained to learn how to effectively refine from this new starting point. Experiments on the LLaDA-Instruct model show that LATTS achieves notable gains on reasoning and code benchmarks (e.g., +4.1% on GSM8K, +4.8% on MATH, +3.2% on MBPP) with minimal additional computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  Sound Solution: The paper's key conceptual contribution, the \"Temporal Chain of Thought,\" is a novel, sound, and well-motivated solution to the problem of applying sequential reasoning to non-autoregressive, fixed-length DLLMs. The paper makes a solid and valuable contribution to the field by cleverly bypassing the architectural constraints of full-attention DLLMs.\n\n2. Strong Empirical Results: The central claims are well-supported by the evidence. LATTS demonstrates consistent and significant performance gains over strong SFT baselines across multiple challenging benchmarks, including mathematical reasoning (GSM8K +4.1%, MATH +4.8%) and code generation (MBPP +3.2%).\n\n3. Excellent Clarity and Presentation: The paper is very well-written, and the core concepts are explained with exceptional clarity. The introduction of \"Spatial CoT\" versus \"Temporal CoT\" is an intuitive and effective framing. The figures are high-quality and add significant value, and the paper structure is logical.\n\n4. Thorough Experimental Setup: The experimental setup is generally sound. The method is tested on a range of reasoning, code, and commonsense tasks. The paper includes a strong comparative analysis against five other sequential TTS baselines (e.g., Static Extend, Concat Guidance, Temporal Remask) and shows that LATTS provides a Pareto-optimal improvement. The ablation studies on hyperparameters are also thorough."}, "weaknesses": {"value": "1. Missing Latency Benchmarks: The paper's primary weakness is the complete omission of practical, end-to-end latency measurements. The analysis relies solely on theoretical computational estimates. This is insufficient for a method explicitly designed for test-time scaling, as it introduces an entire extra generation step (sampling the draft answer) whose real-world latency cost is never measured.\n2. Limited Exploration of Decoding Strategies: The experiments appear confined to the default LLaDA decoding setup: a semi-AR, fixed-step process where the number of denoising steps is tied to the answer length. The paper fails to explore how LATTS interacts with the parallel decoding proposed by Fast-dLLM, which allows the model to generate the answer via variable denoising steps. As far as I can see, by providing a semantically rich starting point, LATTS could reduce the number of refinement steps needed to converge, potentially making the model faster end-to-end. It would be better if the authors could provide extended experiments on this issue.\n3. Limited Scope (Full-Attention DLLMs): The paper's experiments and analysis are focused exclusively on LLaDA, a DLLM that uses full attention over the entire sequence at each denoising step. The authors do not discuss the applicability or potential modifications needed for LATTS to work with newer, more efficient block-diffusion models (e.g., D2F, SDAR, Fast-dLLM-v2), which are a growing trend in the field. This limits the paper's significance.\n4. Questionable Baseline Implementation: The \"Dynamic Extend\" baseline results in Figure 3 are surprisingly strong. Many practitioners find that LLaDA suffers severe performance drops with this method, as they tend to terminate generation prematurely. The paper does not provide sufficient hyperparameter detail to explain its decent reported performance, raising minor concerns about the baseline's implementation.\n5. Superficial Related Work on CoT: The paper positions LATTS against a \"Spatial CoT\" strawman (simple, linear generation). The \"Related Work\" section fails to discuss or compare against more advanced, non-linear CoT methods that also address efficiency and parallel generation, such as \"Skeleton-of-Thought\".\n6. Lack of Qualitative Analysis of Latent Vectors: While Appendix C provides a case study, it only compares the final text outputs. The paper would be much stronger if it included a qualitative analysis of the semantic latent vectors ($a_j$) that form the core of the method, to provide intuition for why they are a good starting point."}, "questions": {"value": "1.\tCould the authors please provide end-to-end, wall-clock latency benchmarks (e.g., time per sequence) for the SFT baseline vs. LATTS? This is essential for understanding the practical performance-latency trade-off.\n2.\tHave the authors considered experimenting with the parallel-decoding strategy proposed by Fast-dLLM? It seems plausible that the \"better starting point\" provided by LATTS could allow the model to reach a high-quality answer in fewer denoising steps than the fixed number used in the experiments.\n3.\tThe \"Dynamic Extend\" baseline results in Figure 3 are surprisingly strong. Given that LLaDA models often suffer from premature termination with this strategy, could the authors please elaborate on the exact hyperparameters and prompting used to achieve these decent baseline results?\n4.\tHow does the \"Temporal CoT\" paradigm conceptually compare to other advanced CoT methods for DLLMs such as \"Skeleton-of-Thought\"?\n5.\tThe LATTS method seems intrinsically tied to the full-attention, holistic refinement of LLaDA. How could this method be adapted to work with newer block-diffusion DLLMs (like D2F or SDAR)?\n6.\tCould the authors provide a more in-depth, qualitative analysis of the semantic latent vectors ($a_j$) themselves? A visualization, for example, could provide valuable intuition as to how these aggregated draft embeddings create a more effective starting point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6xKx0zY2Bo", "forum": "Q524K1mgCc", "replyto": "Q524K1mgCc", "signatures": ["ICLR.cc/2026/Conference/Submission7519/Reviewer_JXwA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7519/Reviewer_JXwA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984310041, "cdate": 1761984310041, "tmdate": 1762919623361, "mdate": 1762919623361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Test-time scaling (TTS) enhances autoregressive models by allowing additional computation during inference. However, existing sequential TTS approaches do not directly apply to diffusion models because:  \n(1) diffusion models operate on fixed-length sequences, and  \n(2) the number of iterative refinement steps is capped by the sequence length.  \n\nThis paper addresses these challenges by reframing chain-of-thought reasoning from a *spatial* process (extending sequence length) to a *temporal* process (adding refinement steps during denoising). Experimental results show an average improvement of **4.6%** on GSM8K, MATH, and MBPP benchmarks, with minimal additional inference cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed sequential test-time scaling for diffusion large language models improves reasoning performance over base diffusion models. Importantly, the performance after applying TTS becomes comparable to methods trained with DPO variants, while requiring **99% less training data**.  \n\n2. As shown in **Table 1**, LATTS outperforms both the base and SFT models while using significantly less computation.  \n\n3. The ablation study in **Figure 3 (left)** clearly demonstrates the advantage of LATTS over dynamically extending the sequence during semi-autoregressive decoding, which represents the strongest baseline among the compared methods."}, "weaknesses": {"value": "1. **Lines 18 & 64–66: “refinement steps capped by sequence length”:**  \n   This claim is not accurate. Prior DLLMs (e.g., ReMDM: https://openreview.net/pdf?id=xNwZ8kDC7T, ADLM: https://openreview.net/pdf?id=E8adS5srds) show that increasing denoising steps **beyond** the sequence length improves generation quality. ADLM further demonstrates that with sufficiently many steps, DLLMs can surpass strong AR models and even larger DLLMs. These works should be discussed in Related Work, and the statement should be revised.\n\n2. Is optimizing **only the initial embeddings** sufficient before running the standard reverse diffusion? What is the wall-clock cost of this optimization, and how does it scale with model size? Could this optimization be applied (or updated) at **every time step**, and if not, why?\n\n3. The method appears to average draft word embeddings and then proceed with standard denoising. Even if random remasking provides little benefit once tokens are unmasked (Fig. 3 left; Lines 368–369), why can’t **LATTS be applied at the end** (i.e., after full unmasking) as a final refinement?\n\n4. The paper claims to address (1) fixed-length sequences and (2) capped iterative refinement. However, the algorithm still assumes a **fixed answer length** and a **fixed cutoff** (e.g., 2048) augmented with averaged draft embeddings. In what precise sense does LATTS relax the fixed-length constraint, beyond increasing effective refinement steps via initialization?"}, "questions": {"value": "Please see the weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IoDFxe1j0M", "forum": "Q524K1mgCc", "replyto": "Q524K1mgCc", "signatures": ["ICLR.cc/2026/Conference/Submission7519/Reviewer_6aRU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7519/Reviewer_6aRU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135725729, "cdate": 1762135725729, "tmdate": 1762919622943, "mdate": 1762919622943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}