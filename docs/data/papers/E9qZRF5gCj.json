{"id": "E9qZRF5gCj", "number": 23003, "cdate": 1758337959411, "mdate": 1759896836643, "content": {"title": "What really matters in matrix whitening optimizers?", "abstract": "A range of recent optimizers have emerged that approximate the same \"matrix-whitening\" transformation in various ways. In this work, we systematically deconstruct such optimizers, aiming to disentangle the key components that explain performance. Under tuned hyperparameters across the board, all flavors of matrix-whitening methods reliably outperform their elementwise counterparts, such as Adam. Matrix-whitening is often related to spectral descent -- however, metrics reveal that performance gains are *not explained solely by accurate spectral normalization* -- particularly, SOAP displays the largest per-step gain, even though Muon more accurately descends along the steepest spectral descent direction. Instead, we argue that matrix-whitening serves *two* purposes, and the variance-adaptation component of matrix-whitening is the overlooked ingredient explaining this performance gap. Experiments show that variance-adapted versions of optimizers consistently outperform their sign-descent counterparts, including an adaptive version of Muon. We further ablate variance adaptation strategies, finding that while \"lookahead\" style approximations are not as effective, low-rank variance estimators can reduce memory costs without a performance loss.", "tldr": "", "keywords": ["optimization", "whitening", "shampoo", "muon"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9aefffe0195e26d68c9c573c89dc056cc5f352d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper systematically deconstructs and analyzes the factors contributing to the performance improvements of modern matrix-whitening optimizers such as Shampoo, SOAP, and Muon. The authors argue that the success of these optimizers is not solely attributed to 'spectral normalization', but that 'variance adaptation'—an often-overlooked component—is critically important. Experiments demonstrated that optimizers incorporating variance adaptation (Adam, SOAP, AdaMuon) consistently outperformed those without it (Signum, SPlus, Muon). This suggests that the two components of a matrix-whitening optimizer can be applied in a decoupled manner."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe analytical approach itself, which deconstructs optimizers along the two axes of 'spectral normalization' and 'variance adaptation', is original.\n\n2.\tAlthough some experiments were 'best-effort' , the fact that the other version optimizers underwent thorough tuning of four key hyperparameters —which was transparently disclosed in the appendix—shows an effort to adhere to scientific procedures."}, "weaknesses": {"value": "1.\tLack of generalization: All conclusions rely solely on a single model (GPT-2 Base) , a single task (Language Modeling) , and a single dataset (OpenWebText).\n\n2.\tInsufficient evaluation metrics: The sole measure of final performance is 'Validation Loss'. The paper fails to demonstrate whether faster convergence or a slight loss improvement translates to actual performance gains on downstream tasks. \n\n3.\tBaseline failure: The analysis completely lacks an explanation for why the key baseline, Shampoo-100, failed to converge, which lowers the experiment's reliability.\n\n4.\tAmbiguity: The use of a 'simplified' AdaMuon and the 'best-effort' tuning for N=10 versions are factors that compromise the fairness of the comparisons."}, "questions": {"value": "1.\tIt would be difficult to generalize findings based on only a single model and dataset . Can you provide evidence that the paper's claims hold true for other transformer-based models and datasets?\n\n2.\tCan you present evidence that the marginal improvements in validation loss lead to statistically significant performance gains on practical downstream tasks for LLMs, such as the GLUE benchmark?\n\n3.\tPlease provide a clear explanation for the convergence failure of Shampoo-100."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "W8x4llMdRo", "forum": "E9qZRF5gCj", "replyto": "E9qZRF5gCj", "signatures": ["ICLR.cc/2026/Conference/Submission23003/Reviewer_veBi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23003/Reviewer_veBi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876383417, "cdate": 1761876383417, "tmdate": 1762942473190, "mdate": 1762942473190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study compares various optimizers based on the *whitening metric* by reporting the minimal validation loss achieved after training a GPT-2 transformer for next-token prediction on the OpenWebText dataset. The training consists of 10,000 updates with cosine learning rates following an initial warmup. The hyperparameter sweep includes four parameters: learning rate, weight decay, momentum EMA coefficient, and variance EMA coefficient. Other hyperparameters, such as Adam's epsilon (also used in other optimizers for regularizing inverses), are held constant.\n\nThe first key result confirms that all carefully tuned optimizers employing update rotation outperform the diagonal Adam optimizer.\n\nAdditionally, the study argues that Shampoo-style optimizers facilitate more accurate estimation of steepest spectral descent, based on a metric that I was not able to fully understand. Importantly, this improved spectral descent is not the sole factor behind the enhanced performance: a key takeaway is that, when comparing \"variance-adaptation\" and \"sign\" versions of equivalent optimizers side by side, the variance-adaptation variants consistently perform better."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The empirical comparison is of interest by itself, even though I think a much greater effort should be pursued across the whole community to provide larger scale controlled benchmarks.\n\nThe paper is careful not to overstate its findings. The limitations are clearly acknowledged, emphasizing that the results are based on a single experimental setup."}, "weaknesses": {"value": "Surveying different optimization methods is inherently challenging due to the large number of hyperparameters, the multitude of optimizers proposed by the community, and the variety of available datasets—even for the same task of next-token prediction. While the current work is undoubtedly of interest, I am uncertain whether its findings are sufficiently general to warrant publication at ICLR. For example, using a different learning rate scheduler (such as cosine annealing) might lead to different conclusions.\n\nAdditionally, some design choices could be questioned. For instance, I believe that Adam's epsilon, which is arguably the second most important hyperparameter after the learning rate, should have been included in the hyperparameter sweep.\n\nAnother point of concern is that it is not clear what is being plotted in the left panel of Figure 3, nor is the corresponding discussion sufficiently explanatory."}, "questions": {"value": "1. A clarification question: what is the \"singular value of updates\" (l.305, plotted in figure 3 ?), where the \"update\" is just a step in parameter space, i.e. a vector of all parameters of the LLM, for which I don't get the concept of \"singular value\".\n2. A question on the relevant of publishing such work to ICLR: Do you think your findings would generalize to other setups, or are they specific to this architecture/dataset/training setup ? If so, how can you convince your readers ?\n\nThen, I have some more minor questions/comments:\n\n3. How are the hyperparameters of the \"nonstandard\" Adam optimizer (line 199) chosen?\n4. It would be helpful to include a summary table of the update rules for all the benchmarked alternatives (lines 212–262).\n5. Why limit the comparison to optimizers based on \\sqrt{var} and sign only? Since implementing natural gradient-like optimizers (such as KFAC/EKFAC) involves not much additional effort, including them could provide a more theoretically grounded comparison. Do you have any thoughts on this?\n6. Could you clarify whether you are using weight decay or L2 regularization? This distinction might matter, as discussed in \"Three Mechanisms of Weight Decay Regularization\" (ICLR 2019).\n7. What about the training loss? Isn't that the quantity that the optimizers are directly minimizing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "09u5pcpeDH", "forum": "E9qZRF5gCj", "replyto": "E9qZRF5gCj", "signatures": ["ICLR.cc/2026/Conference/Submission23003/Reviewer_HjvJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23003/Reviewer_HjvJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930626954, "cdate": 1761930626954, "tmdate": 1762942472772, "mdate": 1762942472772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a systematic decomposition and empirical analysis of matrix whitening optimizers. The authors find that both spectral normalization and variance adaptation are indispensable components. Further ablation studies show that the lookahead approximation is less effective than the low-rank approximation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of this paper lies in its thorough empirical analysis of recently popular matrix whitening optimizers, including Shampoo, SOAP, and Muon. Through carefully controlled experiments, the authors disentangle two key factors, spectral normalization and variance adaptation, and find that both lead to significant performance improvements compared with their element-wise and non-adaptive counterparts. The experimental results are comprehensive and well support their findings, and the presentation is generally clear."}, "weaknesses": {"value": "Although this paper presents detailed empirical analyses, it does not appear to offer important new findings. The authors only verify the effectiveness of spectral normalization and variance adaptation under a single setting, results that are already well known. Specifically, Adam corresponds to variance adaptation, while spectral normalization is employed in methods such as Shampoo and Muon. Numerous prior works have already demonstrated that these techniques improve performance across a wide range of tasks. Moreover, combining the two is not new either; the SOAP and AdaMuon papers have extensively shown that integrating spectral normalization with variance adaptation is highly effective. Therefore, it is unclear what new insights the authors provide. If the contribution is merely a systematic validation of previously established findings under a single setting, it would be difficult for such a study to be accepted at a venue like ICLR."}, "questions": {"value": "The paper’s main conclusion, that both spectral normalization and variance adaptation contribute to the success of matrix-whitening optimizers, appears consistent with prior findings from works such as SOAP and AdaMuon. Could the authors clarify what new understanding or insight their analysis brings beyond these established results?\n\nCould the authors provide more empirical evidence to explain how matrix-whitening optimizers work, rather than only reporting validation losses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oRNNkmaoIG", "forum": "E9qZRF5gCj", "replyto": "E9qZRF5gCj", "signatures": ["ICLR.cc/2026/Conference/Submission23003/Reviewer_kdfp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23003/Reviewer_kdfp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968708662, "cdate": 1761968708662, "tmdate": 1762942472461, "mdate": 1762942472461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic deconstruction of modern \"matrix-whitening\" optimizers. The authors' central thesis is that the success of these methods relies on two key, decoupled components: (1) spectral normalization and (2) variance adaptation.\n\nThrough a series of controlled experiments on a 162M parameter GPT-2 model, the paper argues that while spectral normalization (the \"whitening\" part) is beneficial, variance adaptation (the \"Adam-like\" part) is an \"overlooked\" and \"crucial ingredient\" that is \"roughly as important as the spectral-normalizing aspect\". The paper's primary conclusion is that variance-adapted optimizers (like Adam, SOAP, and AdaMuon) consistently and significantly outperform their sign-descent counterparts (like Signum, SPlus, and Muon)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper's primary strength lies in its attempt to create a controlled and rigorous experimental setup. The methodology isolates the optimizers to only the dense parameters of the Transformer and relies on a thorough, independent hyperparameter sweep for each method, which the authors use to argue against improper tuning as a confounding factor."}, "weaknesses": {"value": "1. A Questionable Premise: The paper groups historically distinct optimization strategies—namely adaptive regularization methods (Adam, Shampoo) and spectral descent methods (Muon)—into the same \"matrix-whitening\" bucket. This re-interpretation, which frames all methods as approximations of a single \"whitening metric\" defined in Equation (2), is a non-standard premise that is not sufficiently justified or defended. Equation (2) computes a value different from adapative regularization methods which use gradient accumulation over optimization steps.\n\n2. Unconvincing Baseline Performance (Shampoo): The paper's central argument for the importance of variance adaptation is severely weakened by its own experimental results for Shampoo. If variance adaptation is a \"crucial ingredient,\" one would expect Shampoo (a variance-adapted method) to significantly outperform Muon (which the paper frames as a sign-descent method). However, the paper's own results show Shampoo-10 (Val Loss 2.963) is only negligibly better than Muon (Val Loss 2.964), and Shampoo-100 \"fails to converge\".\nThis strongly suggests the Shampoo baseline was not properly tuned. The authors' admission that they \"disregard auxiliary design choices in each algorithm (e.g. learning rate grafting...)\" all but confirms this. While this decision was made to isolate \"core\" behavior, it appears to have crippled a key baseline.\n\n3. Confusing Experimental Comparisons: The analysis in Section 5, which compares SOAP to Muon, is confusing. A more natural and direct comparison to isolate the effect of variance adaptation would have been between SOAP and SPlus, as both operate on the same rotated eigenbasis. The paper's choice to instead compare SOAP (explicit eigendecomposition) with Muon (implicit Newton-Schulz) to critique the accuracy of spectral normalization obscures the main argument.\n\n\n4. Insufficient Explanation for the Central Claim: The paper's primary takeaway—that variance-adapted optimizers (Adam, SOAP) perform better than their sign-descent counterparts (Signum, SPlus)—is presented as a major finding. However, this is a not so surprising observation. The major concern is that while the paper shows variance adaptation is important, it fails to provide a deep, novel explanation for why. \n\n5. Minor Errors: The abstract states that matrix-whitening serves \"two purposes\" but does not clearly enumerate them in the following text, Validation loss 0.4 --> 0.04 in discussion section."}, "questions": {"value": "Did you try grafting the Frobenius norm of Muon to Shampoo?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qEV5rcyx0S", "forum": "E9qZRF5gCj", "replyto": "E9qZRF5gCj", "signatures": ["ICLR.cc/2026/Conference/Submission23003/Reviewer_JNF4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23003/Reviewer_JNF4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982686920, "cdate": 1761982686920, "tmdate": 1762942472083, "mdate": 1762942472083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}