{"id": "s1H22o72kx", "number": 20634, "cdate": 1758308414685, "mdate": 1763162204957, "content": {"title": "PED-X-Bench: A Benchmark of Adult-to-Pediatric Extrapolation Decisions in FDA Drug Labels", "abstract": "Pediatric clinical trials are often ethically complex, expensive, and infeasible, leading the U.S. FDA to extrapolate adult efficacy and safety data when justified. However, no public resource systematically documents these regulatory decisions. We present PED-X-Bench, the first dataset and benchmark that encodes FDA pediatric extrapolation outcomes as a four-class classification task (Full, Partial, None, Unlabeled). PED-X-Bench comprises 737 drug-label sections (~1M words) from 2007–2024 across diverse therapeutic areas. A two-stage o3-mini prompting pipeline extracted evidence directly from FDA labels, and nine domain experts adjudicated a stratified sample of 135 records (κ = 0.72, macro-F1 = 0.63). For each drug, we provide the gold-standard extrapolation label, concise efficacy and PK/safety summaries, and harmonized study metadata. We benchmark a range of models from metadata-only classifiers to domain-adapted transformers and show that significant headroom remains, underscoring the task’s complexity. Beyond benchmarking, PED-X-Bench enables AI-assisted regulatory decision-support systems and safety-focused applications aimed at accelerating pediatric drug development and reducing off-label use. The dataset card, code, and annotations are attached in the supplementary material and will be released publicly upon acceptance.", "tldr": "We release PED-X-Bench: a dataset of 737 FDA drug-labels tagged as full, partial, or no adult-to-child extrapolation, each paired with concise efficacy/safety snippets and harmonized pediatric-study metadata.", "keywords": ["Biomedical NLP", "clinical pharmacology", "pediatric drug development", "healthcare AI", "drug safety"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/608b40fbd4cd11cef2247e469e5cd36e0f922182.pdf", "supplementary_material": "/attachment/47d15a232e5236511de1d39e4c3bb52a2d9ea142.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a benchmark dataset, PED-X-Bench, that captures adult-to-pediatric extrapolation decisions from the FDA. Each instance in the dataset corresponds to a regulatory decision and contains (1) the FDA drug labelling texts, (2) a categorical label on the final FDA extrapolation decision, (3) a summary of the pediatric efficacy and (4) meta-data. The stated goal is to design a benchmark dataset that can be used to design ML models to predict and analyze these decisions from the text, thereby enabling the development of decision support tools in regulatory contexts."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The problem domain of adult-to-pediatric extrapolation is interesting and societally relevant. It would for instance be interesting to study if (and under which assumptions) the outcome of a pediatric trial can actually be modeled based on the results of a trial on adults. \n\n- The paper makes an attempt to structure and categorize FDA regulatory decisions which are typically complex and difficult to analyze systematically."}, "weaknesses": {"value": "The paper introduces a benchmark dataset on FDA decision-making regarding adult-to-pediatric extrapolation. While this is novel, the motivation for the dataset is lacking and no new methods are presented, making the contributions limited.  \nThe introduction clearly describes the problem of adult-to-pediatric extrapolation, but it fails to describe the regulatory process that deals with this problem (FDA decisions pipeline, structure of the drug labels, ...). This leaves the reader guessing on the context and makes it hard to understand what is actually being modelled. \nThere is also no compelling motivation on why modelling FDA extrapolation decisions would be meaningful. In the conclusion it is mentioned that this can reduce the need for costly pediatric efficacy trials, but the models are trained on texts that describe the presence or absence, and the results of such trials.  \nAdditionally, the related work section is very brief and misses several key lines of work. For instance, there are no references of LLM-assisted labelling or benchmarking with generated labels / annotations. \n\nIn summary, I identified the following weaknesses:\n- (1) the paper proposes labels that are conceptually ambiguous, \n- (2) these labels are assigned based on prompting LLMs instead of clear criteria, \n- (3) the expert validation of the label quality is weak, \n- (4) the inclusion and exclusion criteria of the paper are unclear and hard to judge, and \n- (5) additional motivation and a review of existing work is also needed."}, "questions": {"value": "- If the goal is to reduce the amount of pediatric trials, wouldn’t it be more meaningful to predict the outcome of such trails instead of the FDA extrapolation decisions? \n- Would it be possible to translate the continuum of extrapolation decisions to label categories based on clear, measurable criteria?\n- Since LLMs are used to assign the labels, and similar transformer-based models are used to predict these labels, how do you avoid circularity (and hence, bias) in the evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bPFnm7KzPS", "forum": "s1H22o72kx", "replyto": "s1H22o72kx", "signatures": ["ICLR.cc/2026/Conference/Submission20634/Reviewer_hZ4z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20634/Reviewer_hZ4z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922293777, "cdate": 1761922293777, "tmdate": 1762934032680, "mdate": 1762934032680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "cOjeHJ4Q1h", "forum": "s1H22o72kx", "replyto": "s1H22o72kx", "signatures": ["ICLR.cc/2026/Conference/Submission20634/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20634/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763162204197, "cdate": 1763162204197, "tmdate": 1763162204197, "mdate": 1763162204197, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PED-X-Bench, a benchmark dataset which contains 737 drug-labels of FDA adult-to-pediatric extrapolation outcomes as a four-way classification task. The authors employ a two-stage LLM labeling pipeline (with human expert review) and perform benchmark evaluations using this dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Overall this paper is clearly written and easy to understand. \n\n2. The dataset is relatively novel and generated by both LLM labeling and expert review."}, "weaknesses": {"value": "1. The dataset size of 737 drug-labels is still small, since there are many more clinical trials conducted for pediatric conditions from 2007 to 2024. \n\n2. Although the dataset is relatively novel, it may not be of significant interest to researchers outside this area due to its small data size, scarce positive labels, and specialized focus on adult-to-pediatric extrapolation by the US FDA. \n\n3. The authors did not introduce novel techniques in their data curation process.   \n\n4. Related to above, the authors should clearly demonstrate that this benchmark dataset contains a representative sample of clinical trials and discuss any potential biases in the data curation process."}, "questions": {"value": "1. What new techniques are introduced in the data curation process that are both novel and applicable to similar datasets? \n\n2. Does this dataset contain a representative sample of adult-to-pediatric extrapolations? \n\n3. Is the dataset quality comparable or better than manual curation by human experts in this domain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TNSRuaAykD", "forum": "s1H22o72kx", "replyto": "s1H22o72kx", "signatures": ["ICLR.cc/2026/Conference/Submission20634/Reviewer_5sEZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20634/Reviewer_5sEZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976676401, "cdate": 1761976676401, "tmdate": 1762934032235, "mdate": 1762934032235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PED-X-Bench, the first public, machine-readable corpus that systematizes how the U.S. FDA has decided, over 2007–2024, whether adult efficacy/safety/PK data can be extrapolated to children. The authors collect 737 pediatric labeling sections from FDA pediatric labeling changes and convert them into a four-way classification task: Full, Partial, None, and Unlabeled. They do this with a two-stage LLM pipeline (LLM to extract evidence → LLM to assign the extrapolation category + rationale) and validate a stratified subset of 135 examples with nine domain experts to establish that LLM-assisted labeling is close to human reliability (κ≈0.72–0.79). They then benchmark a range of baselines from metadata-only models to domain clinical transformers, showing that current models only reach moderate macro-F1, so there is real headroom. The stated goal is to enable AI-assisted regulatory decision support and to make FDA’s historically opaque pediatric extrapolation reasoning computationally analyzable."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Timely and non-trivial problem framing. Pediatric trials are expensive, slow, and ethically constrained; in practice regulators do extrapolate adult data, but this has been hard to study at scale because decisions are buried in PDF labels. Turning this into a structured prediction task is, in itself, a contribution.\n\n- First public, structuring of FDA pediatric extrapolation. To my knowledge, there isn’t an open dataset that (i) spans 2007–2024, (ii) pairs each case with evidence summaries, and (iii) normalizes decisions to FDA’s own threeish buckets (full/partial/none) + “unlabeled.” That immediately creates opportunities for retrieval, RAG, policy analysis, and longitudinal studies."}, "weaknesses": {"value": "- Goldness of the labels is partly circular and LLM-mediated.\nThe central object — the four-way label — is largely produced by an LLM reading FDA labels, then checked on a relatively small expert subset (135 / 737 ≈ 18%). That’s reasonable for a first release, but it also means: downstream models will be learning to mimic the LLM+guideline interpretation of FDA text, not necessarily the actual (sometimes more nuanced) FDA review logic. The paper acknowledges ambiguity (21% disagreement), but it could be clearer about which parts are unavoidably model-generated vs. truly human-confirmed. A fully human-annotated core (say 250–300 items) would make the benchmark more solid.\n\n- Source-of-truth is the public label, not the full regulatory dossier.\nFDA labels are the tip of the iceberg: nuanced reasoning, modeling assumptions (e.g., disease similarity, exposure–response modeling), and even pediatric plan negotiation often live in reviews, not labels. Training a model on labels risks learning “how FDA writes labels about extrapolation” rather than “how FDA decided to extrapolate.” The authors hint at this but don’t quantify potential information loss. This is important because the stated use case is “regulatory decision support,” which ideally wants pre-label information."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "p6m2W0oHp5", "forum": "s1H22o72kx", "replyto": "s1H22o72kx", "signatures": ["ICLR.cc/2026/Conference/Submission20634/Reviewer_AU25"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20634/Reviewer_AU25"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979423691, "cdate": 1761979423691, "tmdate": 1762934031775, "mdate": 1762934031775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}