{"id": "9jdQLmPUHW", "number": 3102, "cdate": 1757334486325, "mdate": 1759898109146, "content": {"title": "Compactness and Consistency: A Conjoint Framework for Deep Graph Clustering", "abstract": "Graph clustering is a fundamental task in data analysis, aiming at grouping nodes with similar characteristics in the graph into clusters. This problem has been widely explored using graph neural networks (GNNs) due to their ability to leverage node attributes and graph topology for effective cluster assignments. However, representations learned through GNNs typically struggle to capture global relationships between nodes via local message-passing mechanisms. Moreover, the redundancy and noise inherently present in the graph data may easily result in node representations lacking compactness and robustness. To address the aforementioned issues, we propose a conjoint framework called CoCo, which captures compactness and consistency in the learned node representations for deep graph clustering. Technically, our CoCo leverages graph convolutional filters to learn robust node representations from both local and global views, and then encodes them into low-rank compact embeddings, thus effectively removing the redundancy and noise as well as uncovering the intrinsic underlying structure. To further enrich the node semantics, we develop a consistency learning strategy based on compact embeddings to facilitate knowledge transfer from the two perspectives. Our experimental findings indicate that our proposed CoCo outperforms state-of-the-art counterparts on various benchmark datasets.", "tldr": "", "keywords": ["Graph Neural Networks", "Graph Clustering", "Representation Learning", "Consistency Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cea8895a48815b340782b3394309bd6ec39f3d21.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CoCo, a conjoint framework for deep graph clustering. Addressing limitations of Graph Neural Networks (GNNs) in capturing global relationships and handling redundancy and noise in graph data, CoCo aims to learn node representations endowed with compactness and consistency. The method first leverages graph convolutional filters to extract node features from both local and global views. These representations are then encoded into low-rank compact embeddings using a Gaussian Mixture Model (GMM) to eliminate redundancy and noise. Finally, a consistency learning strategy is developed to facilitate knowledge transfer between the compact embeddings from the two perspectives, thereby enriching node semantics. Experimental results demonstrate that CoCo outperforms state-of-the-art methods on various benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly identifies existing GNN limitations in capturing global relationships and handling redundancy/noise, proposing targeted solutions. The proposed CoCo framework innovatively combines graph diffusion (for global view), disentangled graph convolutional filters (for feature extraction), GMM-based low-rank compactness learning, and cross-view consistency learning, with these components synergistically improving clustering performance."}, "weaknesses": {"value": "The paper states that GMM aims to maximize p(Z|\\lambda) and that prior probabilities and covariance matrices are fixed. A more detailed explanation is needed for this simplification, such as why these parameters are fixed and how this impacts the model's generality. The limitations of the existing methods lack support from newer literature. Related work does not correspond to the limitations of the existing works. Theorem 2 requires further explanation. The information loss or over-smoothing risk that low-pass filtering and low-rank approximation may bring during denoising may reduce the practical application of this paper."}, "questions": {"value": "Theorem 2 seems to be insufficiently proven.\n\nIs the setting of noise only beneficial to the author's method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2ykQ0Nalje", "forum": "9jdQLmPUHW", "replyto": "9jdQLmPUHW", "signatures": ["ICLR.cc/2026/Conference/Submission3102/Reviewer_3Q5j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3102/Reviewer_3Q5j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535541420, "cdate": 1761535541420, "tmdate": 1762916551712, "mdate": 1762916551712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for deep graph clustering that aims to learn node representations which are both compact and consistent. The model addresses two key limitations in existing methods: the inability of standard GNNs to capture global node relationships and the sensitivity to noise and redundancy in graph data. The methodology first extracts node features from both local (original adjacency matrix) and global (graph diffusion matrix) views using disentangled graph convolutional filters. These features are then encoded into a low-rank subspace via a Gaussian mixture model to eliminate redundancy and noise, producing compact embeddings. Finally, a consistency learning strategy aligns the similarity distributions of nodes between the two views using a symmetric KL-divergence loss, enriching the semantics without needing negative samples. Extensive experiments on benchmark datasets show that CoCo outperforms state-of-the-art methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The model effectively integrates both local and global structural information to form comprehensive node representations\n- The low-rank compactness learning effectively reduces noise and redundancy, leading to more robust embeddings.\n- The authors provide a detailed theoretical analysis that justifies the rationale behind the proposed technique.\n- It demonstrates strong generalization capability by achieving superior performance across diverse tasks, including both node clustering and classification.\n- The approach is computationally efficient and scalable to large graphs, owing to its linear complexity."}, "weaknesses": {"value": "- The GMM-based low-rank learning adds model complexity and training steps.\n- The sensitivity analysis of hyperparameters is relatively limited."}, "questions": {"value": "- In Equation 1, what is the impact of different values of $k$ on model performance? Please provide further experimental explanation.\n- The author's original intention to propose the use of graph convolutional filters is to disentangle the filters and the weight matrix. How to prove its effectiveness?\n- In Section 2.2, why is GMM used for low-rank subspace training? Why is the original embedding representation $\\mathbf{Z}$ added in Equation 5?\n- Since the EM algorithm is independent of the model training, how does its initialization affect the model? What is the additional training cost required?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t11c4Bs3xJ", "forum": "9jdQLmPUHW", "replyto": "9jdQLmPUHW", "signatures": ["ICLR.cc/2026/Conference/Submission3102/Reviewer_4B3S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3102/Reviewer_4B3S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712056023, "cdate": 1761712056023, "tmdate": 1762916551363, "mdate": 1762916551363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of graph clustering, particularly their inability to capture global node relations and their susceptibility to redundancy and noise in learned representations. The authors propose a conjoint framework, CoCo, which learns robust representations from local and global views through graph convolutional filters, encodes them into low-rank compact embeddings to eliminate redundancy and noise, and further introduces a consistency learning strategy to transfer knowledge across views. Extensive experiments on benchmark datasets demonstrate that CoCo consistently outperforms state-of-the-art methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The research topic is important and meaningful in the graph learning field, and appeals to a broad audience.\n- The paper is well-written and easy to follow. Its cutting-edge, rational technical solution addresses long-distance dependence, redundancy, and noise in feature representation, which are often overlooked in the current graph clustering field.\n- The theoretical analysis further reinforces the soundness and validity of the proposed scheme.\n- Extensive experiments and analyses across multiple downstream tasks and over ten baseline methods fully demonstrate the proposed method’s superiority, and effectively validate the study’s stated motivations."}, "weaknesses": {"value": "- Neither the Introduction section nor the methods selected for comparative experiments have incorporated studies published in 2025.  \n- The main research results lack statistical significance tests."}, "questions": {"value": "- For the graph convolutional filter adopted in Eq. 1, how well would it perform if this filter were replaced with GCN?\n- The authors proposed GMM-based compactness learning; what impact would using only PCA dimensionality reduction have?\n- In Eq. 7, the authors only use sum averaging for the semantic representation of the two different branches. Would performance improve if attention weighting were used instead?\n- For the node classification evaluation, clarification is needed on whether the authors applied a classification head to the representation and trained via a supervised loss, or trained with an unsupervised loss before supervised fine-tuning.\n- The calculations of $\\mathbf{p}$ and $\\mathbf{q}$ are based on cosine similarity. Why was this similarity metric chosen instead of other methods? How would the results compare with those from other methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ghau5CGfV1", "forum": "9jdQLmPUHW", "replyto": "9jdQLmPUHW", "signatures": ["ICLR.cc/2026/Conference/Submission3102/Reviewer_y7YE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3102/Reviewer_y7YE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724949622, "cdate": 1761724949622, "tmdate": 1762916551119, "mdate": 1762916551119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a conjoint framework named CoCo for deep graph clustering, which captures compactness and consistency in learned node representations by leveraging graph convolutional filters for local-global view feature extraction, GMM-based low-rank embedding for redundancy elimination, and consistency learning for semantic enhancement. Experimental results on multiple benchmark datasets show CoCo outperforms state-of-the-art methods in graph clustering and demonstrates good scalability and robustness."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1.The paper is clearly written and logically rigorous, presenting the proposed method in a clear way.\n\n2.The novelty and technical solution of the paper is sound and reasonable, long-range dependence and redundancy/noise are indeed the points that are easily ignored in graph clustering.\n\n3.The paper provides rigorous theoretical analysis and well-defined theoretical framework, Theorems 1-3 provide the optimal filtering parameters and the convergence guarantee for the dimension reduction, respectively.\n\n4.The authors conduct extensive experiments for the proposed framework, showing the superiority of the overall results. The ablation study also verifies the effectiveness of each component, and the analysis on different tasks and different scenarios also illustrates the good generalization of the framework."}, "weaknesses": {"value": "1.The details regarding compactness learning and the EM algorithm are not elaborated or analyzed with sufficient clarity.\n\n2.The motivation behind compact learning for redundancy reduction is not sufficiently clear.\n\n3.The fusion of representations from different branches may lack flexibility."}, "questions": {"value": "1.Although this paper claims a linear time complexity, could the iteration count of the EM algorithm impact the overall training efficiency?\n\n2.Why is EM-based subspace learning used?\n\n3.Why compact learning can eliminate redundancy and mitigate the impact of noise requires further clarification.\n\n4.When fusing local and global representations, equal weights are used. Why not adopt adaptive weights (e.g., learned via attention) to prioritize more informative views?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Grn5aeRxy8", "forum": "9jdQLmPUHW", "replyto": "9jdQLmPUHW", "signatures": ["ICLR.cc/2026/Conference/Submission3102/Reviewer_wkWL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3102/Reviewer_wkWL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819629489, "cdate": 1761819629489, "tmdate": 1762916550902, "mdate": 1762916550902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a compactness-and-consistency framework designed for deep graph clustering. The contributions are three-fold as follows:\n1.The method jointly captures local and global graph structures via disentangled low-pass filters, effectively alleviating the over-smoothing problem.\n2.Low-rank GMM reconstruction removes redundancy/noise and yields compact embeddings while maintaining linear complexity.\n3.Cross-view consistency learning transfers knowledge at the distribution level, enabling mutual enrichment between different semantic spaces.\nExtensive comparisons with numerous baselines and across diverse data scenarios demonstrate the superiority and robustness of the proposed framework."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.**Theoretically sound**: The proposed framework provides a theoretical analysis of the effective parameter settings for graph filters and the ability of feature reconstruction to preserve semantic information.\n2.**Broad Applicability**: Superior and stable state-of-the-art performance on homophilic, heterophilic, nosiy and large-scale graphs.\n3.**Low Training Cost**: Both theoretically and experimentally, the proposed approach is verified to deliver low runtime and low memory costs."}, "weaknesses": {"value": "1.How to guarantee the low-rank property?\n2.The effectiveness of consistency learning is insufficiently justified.\n3.Hyper-parameter analysis is inadequate."}, "questions": {"value": "1.Perform a sensitivity analysis on the parameter $\\alpha$ in the graph diffusion matrix, since different settings of $\\alpha$ correspond to different graph structures.\n2.What role does low-rank learning play in graph clustering, and what specific benefits does it provide in this paper?\n3.How can guarantee that the node representations obtained by feature reconstruction possess the low-rank property?\n4.What are the advantages of consistency learning over contrastive learning? Provide both theoretical and experimental analyses.\nIf the author addresses my concerns and doubts well, I am willing to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6fLZCWuwc6", "forum": "9jdQLmPUHW", "replyto": "9jdQLmPUHW", "signatures": ["ICLR.cc/2026/Conference/Submission3102/Reviewer_bn3i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3102/Reviewer_bn3i"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878694838, "cdate": 1761878694838, "tmdate": 1762916550647, "mdate": 1762916550647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}