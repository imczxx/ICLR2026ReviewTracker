{"id": "WoMMSVZHfP", "number": 21581, "cdate": 1758319271059, "mdate": 1759896913901, "content": {"title": "NeuCLIP: Efficient Large-Scale CLIP Training with Neural Normalizer Optimization", "abstract": "Accurately estimating the normalization term (also known as the partition function) in the contrastive loss is a central challenge for training Contrastive Language-Image Pre-training (CLIP) models. Conventional methods rely on large batches for approximation, demanding substantial computational resources. To mitigate this issue, prior works introduced per-sample normalizer estimators, updated at each epoch in a blockwise coordinate manner to keep track of updated encoders. However, this scheme incurs an optimization error that scales with the ratio of dataset size to batch size, limiting effectiveness for large datasets or small batches.  To overcome this limitation, we propose NeuCLIP, a novel and elegant optimization framework based on two key ideas: (i) **reformulating** the contrastive loss for each sample **via convex analysis** into a minimization problem with an auxiliary variable representing its log-normalizer; and (ii) **transforming** the resulting minimization over $n$ auxiliary variables (where $n$ is the dataset size) via **variational analysis** into  the minimization over a compact neural network that predicts the log-normalizers. We design an alternating optimization algorithm that jointly trains the CLIP model and the auxiliary network. By employing a tailored architecture and acceleration techniques for the auxiliary network, NeuCLIP achieves more accurate normalizer estimation, leading to improved performance compared with previous methods. Extensive experiments on large-scale CLIP training, spanning datasets from millions to billions of samples, demonstrate that NeuCLIP outperforms previous methods.", "tldr": "", "keywords": ["Representation Learning", "Contrastive Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a4028e07690192fb899d542e43d8fdac2ad11107.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper looks into a fundamental issue in CLIP model training and its dependence on large batch size for good performance. It looks into existing techniques that are less dependent on large batch size but their normalization approximation for log-partition estimation has room for improvement. The authors proposes a normalizer prediction network (NPN). The training objective jointly learns the encoder and a small network that predicts the log-normalizer for image and text"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper has novel contribution and addresses an important issue in CLIP training. This is helpful for the research community specially when they are looking to train on a smaller budget\n2. The method beats others in Datacomp. Although in some cases (table 9, 10) the improvement in marginal"}, "weaknesses": {"value": "1. Probably needs some more in related work. For example https://arxiv.org/abs/2409.13079 and https://openreview.net/pdf?id=3i13Gev2hV and how this work is tackling a different aspect of image-text pretraining\n2. The paper has novel contribution but it is not clear how those improved results number actually help in retrieval. Probably some quantitative examples will be helpful\n3. Minor: typo in line 58, 60. SigCLIP should be SigLIP\n4. Having insight on the limitation of the proposed approach and how it can be improved further will be helpful"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "13x51shboZ", "forum": "WoMMSVZHfP", "replyto": "WoMMSVZHfP", "signatures": ["ICLR.cc/2026/Conference/Submission21581/Reviewer_TYsU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21581/Reviewer_TYsU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761165019534, "cdate": 1761165019534, "tmdate": 1762941842971, "mdate": 1762941842971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes NeuCLIP, a training framework for CLIP that replaces per-sample moving-average normalizer estimates with a neural normalizer–prediction network (NPN) trained jointly with the encoders. The authors (1) use convex conjugate analysis to rewrite each per-sample contrastive loss as a minimization over an auxiliary log-normalizer variable, and (2) use a variational argument to replace the set of per-sample variables by a function class, then parameterize that function with compact networks operating on encoder embeddings. Training alternates between several fast NPN updates (with periodic proto-reinitialization) and encoder/temperature updates. Experiments on CC3M, CC12M, and DFN at 14M/192M/1B scales report higher Datacomp averages than OpenCLIP, FastCLIP, SigLIP, and AmorLIP under the authors’ setups; ablations examine unified vs. separate objectives, NPN architecture, restart frequency, and NPN update count, and profiling suggests single-digit percent overhead for the NPN step."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The loss is reformulated via a Fenchel conjugate into a per-sample minimization whose optimizer equals the log-normalizer; a variational theorem is then invoked to justify searching over functions and learning an NPN. The paper provides the derivations and the induced FastCLIP update as a special case. \n\n\n2. Results span multiple datasets (millions→billions of pairs), report the Datacomp average plus subset scores, include ablations of hyperparameters (e.g., restart frequency and update count), and list batch sizes, samples processed, and optimizer choices. \n\n\n3. A profiler table reports the NPN adds about 6–9% per-iteration time across representative encoders, do"}, "weaknesses": {"value": "1. The paper notes dataset download discrepancies vs. AmorLIP (different numbers of successfully fetched samples) yet still compares scores; large-scale web datasets are sensitive to crawl state, which can materially shift results. The largest-scale runs report single numbers without variability, and some baselines rely on third-party code with possible configuration drift—together reducing the strength of “method X > method Y” claims. \n\n\n2. The estimation-error plots rely on a “true normalizer”; for global losses that sum over all negatives, computing (or approximating) this quantity exactly is costly and may itself introduce bias. The procedure for obtaining this target at scale is not fully detailed, which affects how to interpret the error curves. \n\n\n3. While block-coordinate convergence is cited for idealized alternating optimization, the implemented procedure adds multiple NPN steps per batch and periodic re-initializations. There is no formal convergence rate or stability guarantee for this specific schedule, making the method’s robustness to optimizer/hyperparameter choices theoretically unclear. \n\n\n4. Performance depends on the number of prototypes m, restart frequency, and update count; the paper shows drops when these deviate (e.g., too frequent restarts or too many NPN updates leading to batch-overfit), but provides limited guidance for transfer to different hardware/batch regimes. This can be a practical failure mode in low-resource or non-H100 settings. \n\n\n5. All main experiments use 8×H100 GPUs and large global batches (e.g., 4096–5120). Claims about efficiency versus large-batch training would be stronger with results under constrained hardware and strict parity of training budgets and data filters across methods; otherwise, improved scores might partly reflect schedule choices rather than the optimization change alone."}, "questions": {"value": "1. How is the “true” (log) normalizer computed for the error analyses at scale, and what bias/variance does that estimator have relative to the exact full-dataset partition values?\n2. What are the peak memory and wall-clock contributions of the NPN as a function of prototype count m and embedding dimension, and how do these scale for larger backbones (e.g., ViT-L/H)? Please include per-step FLOPs and activation memory. \n3. Does the alternating schedule with Tu>1 and periodic re-initialization admit any convergence guarantee (e.g., to a stationary point) under realistic stochasticity, or can it cycle?** Empirical stability diagnostics would help. \n4. How sensitive are results to m and to the prototype initialization strategy? Would K-means/K-medoids over a streaming buffer of embeddings outperform random re-seeding? \n5. Please report Datacomp and normalizer-error curves to validate the claimed robustness to small batches. \n6. Does the NPN inadvertently learn to absorb temperature effects, and if so, how do you regularize to prevent degeneracy between τ and α(·)? \n7. Can you provide ablations isolating the two accelerations (multiple NPN updates vs. periodic restart) across datasets, and quantify which contributes most under fixed compute?\n8. Since dataset discrepancies materially affect AmorLIP/FastCLIP comparisons, will you release the exact URL lists and filtering scripts for each run so the community can reproduce your numbers byte-for-byte? \n9. Does the NPN introduce a biased gradient for the encoder parameters relative to the true global objective? If unbiasedness does not hold, can you bound or characterize the induced bias and its dependence on m, Tr, and Tu? \n10. How portable is the approach to multilingual CLIP, video-text, or retrieval-only training? Are any parts of the derivation tied to cosine similarity or image–text symmetry that would need adjustment? \n\n**If you address my concerns, I will consider raising my score.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iby8hnms3z", "forum": "WoMMSVZHfP", "replyto": "WoMMSVZHfP", "signatures": ["ICLR.cc/2026/Conference/Submission21581/Reviewer_fUVb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21581/Reviewer_fUVb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761423705983, "cdate": 1761423705983, "tmdate": 1762941842685, "mdate": 1762941842685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes NeuCLIP, a training framework for CLIP that replaces per-sample moving-average normalizer estimates with a neural normalizer–prediction network (NPN) trained jointly with the encoders. The authors (1) use convex conjugate analysis to rewrite each per-sample contrastive loss as a minimization over an auxiliary log-normalizer variable, and (2) use a variational argument to replace the set of per-sample variables by a function class, then parameterize that function with compact networks operating on encoder embeddings. Training alternates between several fast NPN updates (with periodic proto-reinitialization) and encoder/temperature updates. Experiments on CC3M, CC12M, and DFN at 14M/192M/1B scales report higher Datacomp averages than OpenCLIP, FastCLIP, SigLIP, and AmorLIP under the authors’ setups; ablations examine unified vs. separate objectives, NPN architecture, restart frequency, and NPN update count, and profiling suggests single-digit percent overhead for the NPN step."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The loss is reformulated via a Fenchel conjugate into a per-sample minimization whose optimizer equals the log-normalizer; a variational theorem is then invoked to justify searching over functions and learning an NPN. The paper provides the derivations and the induced FastCLIP update as a special case. \n\n2. Results span multiple datasets, report the Datacomp average plus subset scores, include ablations of hyperparameters, and list batch sizes, samples processed, and optimizer choices. \n\n3. A profiler table reports the NPN adds about 6–9% per-iteration time across representative encoders"}, "weaknesses": {"value": "1. The paper notes dataset download discrepancies vs. AmorLIP (different numbers of successfully fetched samples) yet still compares scores; large-scale web datasets are sensitive to crawl state, which can materially shift results. The largest-scale runs report single numbers without variability, and some baselines rely on third-party code with possible configuration drift—together reducing the strength of method X > method Y claims. \n\n\n2. The estimation-error plots rely on a “true normalizer”; for global losses that sum over all negatives, computing (or approximating) this quantity exactly is costly and may itself introduce bias. The procedure for obtaining this target at scale is not fully detailed, which affects how to interpret the error curves. \n\n\n3. While block-coordinate convergence is cited for idealized alternating optimization, the implemented procedure adds multiple NPN steps per batch and periodic re-initializations. There is no formal convergence rate or stability guarantee for this specific schedule, making the method’s robustness to optimizer/hyperparameter choices theoretically unclear. \n\n\n4. Performance depends on the number of prototypes m, restart frequency, and update count; the paper shows drops when these deviate, but provides limited guidance for transfer to different hardware/batch regimes. This can be a practical failure mode in low-resource or non-H100 settings. \n\n\n5. All main experiments use 8×H100 GPUs and large global batches. Claims about efficiency versus large-batch training would be stronger with results under constrained hardware and strict parity of training budgets and data filters across methods; otherwise, improved scores might partly reflect schedule choices rather than the optimization change alone."}, "questions": {"value": "### **Major Questions**\n\nQ1. How is the “true” (log) normalizer computed for the error analyses at scale, and what bias/variance does that estimator have relative to the exact full-dataset partition values?\n\nQ2. What are the peak memory and wall-clock contributions of the NPN as a function of prototype count m and embedding dimension, and how do these scale for larger backbones? Please include per-step FLOPs and activation memory. \n\nQ3. Does the alternating schedule with Tu>1 and periodic re-initialization admit any convergence guarantee under realistic stochasticity, or can it cycle?\n\nQ4. How sensitive are results to m and to the prototype initialization strategy? Would K-means/K-medoids over a streaming buffer of embeddings outperform random re-seeding? \n\nQ5. Please report Datacomp and normalizer-error curves to validate the claimed robustness to small batches. \n\nQ6. Does the NPN inadvertently learn to absorb temperature effects, and if so, how do you regularize to prevent degeneracy between τ and α(·)? \n\n### **Minor Questions**\n\nQ7. Can you provide ablations isolating the two accelerations across datasets, and quantify which contributes most under fixed compute?\n\nQ8. Since dataset discrepancies materially affect AmorLIP/FastCLIP comparisons, will you release the exact URL lists and filtering scripts for each run so the community can reproduce your numbers byte-for-byte? \n\nQ9. Does the NPN introduce a biased gradient for the encoder parameters relative to the true global objective? If unbiasedness does not hold, can you bound or characterize the induced bias and its dependence on m, Tr, and Tu? \n\nQ10. How portable is the approach to multilingual CLIP, video-text, or retrieval-only training? Are any parts of the derivation tied to cosine similarity or image–text symmetry that would need adjustment? \n\n**If you address my concerns, I will consider raising my score.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iby8hnms3z", "forum": "WoMMSVZHfP", "replyto": "WoMMSVZHfP", "signatures": ["ICLR.cc/2026/Conference/Submission21581/Reviewer_fUVb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21581/Reviewer_fUVb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761423705983, "cdate": 1761423705983, "tmdate": 1763651209056, "mdate": 1763651209056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes NeuCLIP, a training framework for CLIP that replaces per-sample moving-average normalizer estimates with a neural normalizer–prediction network (NPN) trained jointly with the encoders. The authors (1) use convex conjugate analysis to rewrite each per-sample contrastive loss as a minimization over an auxiliary log-normalizer variable, and (2) use a variational argument to replace the set of per-sample variables by a function class, then parameterize that function with compact networks operating on encoder embeddings. Training alternates between several fast NPN updates (with periodic proto-reinitialization) and encoder/temperature updates. Experiments on CC3M, CC12M, and DFN at 14M/192M/1B scales report higher Datacomp averages than OpenCLIP, FastCLIP, SigLIP, and AmorLIP under the authors’ setups; ablations examine unified vs. separate objectives, NPN architecture, restart frequency, and NPN update count, and profiling suggests single-digit percent overhead for the NPN step."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The loss is reformulated via a Fenchel conjugate into a per-sample minimization whose optimizer equals the log-normalizer; a variational theorem is then invoked to justify searching over functions and learning an NPN. The paper provides the derivations and the induced FastCLIP update as a special case. \n\n2. Results span multiple datasets, report the Datacomp average plus subset scores, include ablations of hyperparameters, and list batch sizes, samples processed, and optimizer choices. \n\n3. A profiler table reports the NPN adds about 6–9% per-iteration time across representative encoders"}, "weaknesses": {"value": "1. The paper notes dataset download discrepancies vs. AmorLIP (different numbers of successfully fetched samples) yet still compares scores; large-scale web datasets are sensitive to crawl state, which can materially shift results. The largest-scale runs report single numbers without variability, and some baselines rely on third-party code with possible configuration drift—together reducing the strength of method X > method Y claims. \n\n\n2. The estimation-error plots rely on a “true normalizer”; for global losses that sum over all negatives, computing (or approximating) this quantity exactly is costly and may itself introduce bias. The procedure for obtaining this target at scale is not fully detailed, which affects how to interpret the error curves. \n\n\n3. While block-coordinate convergence is cited for idealized alternating optimization, the implemented procedure adds multiple NPN steps per batch and periodic re-initializations. There is no formal convergence rate or stability guarantee for this specific schedule, making the method’s robustness to optimizer/hyperparameter choices theoretically unclear. \n\n\n4. Performance depends on the number of prototypes m, restart frequency, and update count; the paper shows drops when these deviate, but provides limited guidance for transfer to different hardware/batch regimes. This can be a practical failure mode in low-resource or non-H100 settings. \n\n\n5. All main experiments use 8×H100 GPUs and large global batches. Claims about efficiency versus large-batch training would be stronger with results under constrained hardware and strict parity of training budgets and data filters across methods; otherwise, improved scores might partly reflect schedule choices rather than the optimization change alone."}, "questions": {"value": "### **Major Questions**\n\nQ1. How is the true (log) normalizer computed for the error analyses at scale, and what bias/variance does that estimator have relative to the exact full-dataset partition values?\n\nQ2. What are the peak memory and wall-clock contributions of the NPN as a function of prototype count m and embedding dimension, and how do these scale for larger backbones? Please include per-step FLOPs and activation memory. \n\nQ3. Does the alternating schedule with Tu>1 and periodic re-initialization admit any convergence guarantee under realistic stochasticity, or can it cycle?\n\nQ4. How sensitive are results to m and to the prototype initialization strategy? Would K-means/K-medoids over a streaming buffer of embeddings outperform random re-seeding? \n\nQ5. Please report Datacomp and normalizer-error curves to validate the claimed robustness to small batches. \n\nQ6. Does the NPN inadvertently learn to absorb temperature effects, and if so, how do you regularize to prevent degeneracy between τ and α(·)? \n\n### **Minor Questions**\n\nQ7. Can you provide ablations isolating the two accelerations across datasets, and quantify which contributes most under fixed compute?\n\nQ8. Since dataset discrepancies materially affect AmorLIP/FastCLIP comparisons, will you release the exact URL lists and filtering scripts for each run so the community can reproduce your numbers byte-for-byte? \n\nQ9. Does the NPN introduce a biased gradient for the encoder parameters relative to the true global objective? If unbiasedness does not hold, can you bound or characterize the induced bias and its dependence on m, Tr, and Tu? \n\nQ10. How portable is the approach to multilingual CLIP, video-text, or retrieval-only training? Are any parts of the derivation tied to cosine similarity or image–text symmetry that would need adjustment? \n\n**If you address my concerns, I will consider raising my score.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iby8hnms3z", "forum": "WoMMSVZHfP", "replyto": "WoMMSVZHfP", "signatures": ["ICLR.cc/2026/Conference/Submission21581/Reviewer_fUVb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21581/Reviewer_fUVb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761423705983, "cdate": 1761423705983, "tmdate": 1763671584773, "mdate": 1763671584773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the difficulty of estimating the partition function (normalizer) in CLIP’s global contrastive loss without relying on very large batch sizes. The authors (1) use convex conjugacy to rewrite each per-sample log-normalizer as the solution of a simple one-dimensional minimization, and (2) replace the n per-sample variables by a compact “normalizer-prediction network” (NPN) learned jointly with the CLIP encoders through a single objective. They propose an alternating optimization routine with two accelerators: several NPN steps per encoder update and periodic re-initialization of NPN “prototypes.” On several CLIP training scales (millions to a billion samples), NeuCLIP reports better Datacomp averages than OpenCLIP, FastCLIP, SigLIP, and AmorLIP under fixed compute budgets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The convex-variational reformulation removes the reciprocal-of-estimator bias from mini-batch CLIP and avoids per-sample moving averages in FastCLIP. The unified loss couples encoder and NPN training without needing a separate consistency target for the normalizer.\n\n2. Alternating updates with multiple quick NPN steps and periodic NPN restarts is easy to implement and, per their appendix, gives better stability than simultaneous updates."}, "weaknesses": {"value": "1 **Limited accounting of compute and wall-clock.** Results are reported “under the same budget” and with “8 × H100,” but the paper does not provide thorough wall-clock and energy numbers for NeuCLIP vs. strong baselines at equal accuracy.\n\n2. **Breadth of baselines and settings.**  SigLIP is included, but the study would benefit from (i) larger-batch SigLIP/OpenCLIP points at matched compute, and (ii) comparisons under stronger data filtering or with modern data recipes, since normalizer accuracy can interact with data quality. The paper notes some dataset mismatch with AmorLIP but does not fully normalize the comparison."}, "questions": {"value": "1. **Compute and speed**. At matched Datacomp accuracy, what is the wall-clock time and GPU-hours for NeuCLIP vs. FastCLIP and SigLIP?\n\n2. **Scaling to 1B+.** DFN-1B shows smaller gains than DFN-192M under the stated budget. Is this due to fewer total seen samples (1.0B vs. 1.3B) or to NPN capacity limits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fikIaJvg2v", "forum": "WoMMSVZHfP", "replyto": "WoMMSVZHfP", "signatures": ["ICLR.cc/2026/Conference/Submission21581/Reviewer_erJT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21581/Reviewer_erJT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781424834, "cdate": 1761781424834, "tmdate": 1762941842382, "mdate": 1762941842382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Proposes NeuCLIP, a framework for efficient CLIP training by learning a neural normalizer that predicts log-normalization terms in contrastive loss.\n\n- Reformulates the contrastive loss via convex and variational analysis, turning per-sample normalizer estimation into learning a compact neural network.\n\n- Introduces an alternating optimization algorithm that jointly updates CLIP encoders and the normalizer-prediction network (NPN).\n\n- Demonstrates consistent improvements over OpenCLIP, FastCLIP, SigLIP, and AmorLIP on datasets from 3M to 1B pairs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is based on an elegant theoretical foundation combining convex conjugate and variational principles to remove per-sample normalizer tracking.\n\n- We see strong empirical validation - NeuCLIP consistently outperforms baselines on multiple datasets and scales favorably to billion-sample training."}, "weaknesses": {"value": "- Some hyperparameters (restart frequency, number of prototypes m) seem tuned per dataset; robustness to such choices is not discussed.\n\n- It might be worth discussing the computational overhead of the extra NPN updates, restarts, and parameter sync cost to give practitioners more insights to use in reality."}, "questions": {"value": "- What is the per-step wall-clock and memory overhead compared to FastCLIP and OpenCLIP?\n\n- How sensitive are results to NPN hyperparameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dzhqz77f3B", "forum": "WoMMSVZHfP", "replyto": "WoMMSVZHfP", "signatures": ["ICLR.cc/2026/Conference/Submission21581/Reviewer_LKoG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21581/Reviewer_LKoG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762216947746, "cdate": 1762216947746, "tmdate": 1762941842130, "mdate": 1762941842130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}