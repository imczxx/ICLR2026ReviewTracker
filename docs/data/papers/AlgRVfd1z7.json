{"id": "AlgRVfd1z7", "number": 13251, "cdate": 1758215658687, "mdate": 1759897452582, "content": {"title": "Referring Layer Decomposition", "abstract": "Precise, object-aware control over visual content is essential for advanced image editing and compositional generation. Yet, most existing approaches operate on entire images holistically, limiting the ability to isolate and manipulate individual scene elements. In contrast, layered representations, where scenes are explicitly separated into objects, environmental context, and visual effects, provide a more intuitive and structured framework for interpreting and editing visual content. To bridge this gap and enable both compositional understanding and controllable editing, we introduce the Referring Layer Decomposition (RLD) task, which predicts complete RGBA layers from a single RGB image, conditioned on flexible user prompts, such as spatial inputs (e.g., points, boxes, masks), natural language descriptions, or combinations thereof. At the core is the RefLade, a large-scale dataset comprising 1.11M image–layer–prompt triplets produced by our scalable data engine, along with 100K manually curated, high-fidelity layers. Coupled with a perceptually grounded, human-preference-aligned automatic evaluation protocol, RefLade establishes RLD as a well-defined and benchmarkable research task. Building on this foundation, we present RefLayer, a simple baseline designed for prompt-conditioned layer decomposition, achieving high visual fidelity and semantic alignment. Extensive experiments show our approach enables effective training, reliable evaluation, and high-quality image decomposition, while exhibiting strong zero-shot generalization capabilities. We will release our dataset, evaluation tools, and model for future research.", "tldr": "", "keywords": ["Dataset", "Benchmark", "Layer Decomposition"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/094ecec8f0c548098496e5b4bb7c1c4f3dd77f33.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new task that extracts a target RGBA layer guided by a user-provided prompt. To support this task, the authors present RefLade, a dataset comprising 1.11 million image–layer–prompt triplets. Building on RefLade, they establish an evaluation protocol along three key dimensions: preservation, completion, and faithfulness. Furthermore, the paper proposes RefLayer as a baseline model for this task. RefLayer encodes spatial prompts using color-coded maps integrated into the latent space and employs a parallel alpha decoder to generate complete object RGBA layers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-written and easy to understand.\n\n2. This paper defines a novel task and presents a comprehensive framework to address it, including a dedicated dataset, evaluation metrics, and a baseline model."}, "weaknesses": {"value": "1. In Figure 1, the flag extracted exhibits a noticeable color discrepancy (or color cast) when compared to the original image. The color shift is quite apparent and detracts from the quality of the result. The authors should investigate the cause of this artifact.\n\n2. The paper's qualitative evaluation is currently a weak point. First, the paper lacks direct qualitative comparisons of different methods and ablation study. Without side-by-side visual comparisons, it is difficult for the reader to verify the claimed advantages of the proposed model over existing work. Second, the number of generated results shown is limited. The authors should include a more comprehensive and diverse set of examples (either in the main paper or, more extensively, in the supplementary material) to better demonstrate the model's capabilities, robustness, and potential failure cases."}, "questions": {"value": "1. The authors employed a third-party, closed-source LLM to construct a large-scale dataset. Have the authors considered making this dataset publicly available? Since the primary contribution of this work lies in the dataset’s construction, the overall impact of the paper would be considerably limited if the dataset remains closed-source.\n\n2. In the constructed dataset, approximately 25% of the training data contain errors. As mentioned in the appendix, these issues primarily result from failed restoration and segmentation inaccuracies. Have the authors conducted an analysis of these erroneous samples? Furthermore, do these problematic data limit the potential applications of the dataset? Do the authors plan to address these issues in future work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "31yzCxz7xu", "forum": "AlgRVfd1z7", "replyto": "AlgRVfd1z7", "signatures": ["ICLR.cc/2026/Conference/Submission13251/Reviewer_wYda"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13251/Reviewer_wYda"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727721342, "cdate": 1761727721342, "tmdate": 1762923930898, "mdate": 1762923930898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce the Referring Layer Decomposition (RLD) task, a new task which predicts\ncomplete RGBA layers from a single RGB image, conditioned on flexible\nuser prompts.  a large-scale\ndataset (RefLade) comprising 1.11M image–layer–prompt triplets  is constructed.  In addition, RefLayer is proposed as a simple baseline for\nprompt-conditioned layer decomposition."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces Referring Layer Decomposition (RLD), the pioneering task that explores layer decomposition guided by multi-modal referring inputs.\n\n2. The authors introduce RefLade, a large-scale dataset of 1.11 million image-layer-prompt triplets built using a scalable data engine. With its human-curated splits for tuning and testing and a well-defined evaluation protocol, RefLade facilitates and paves the way for future RLD studies. RefLayer is also desigend as a simple baseline."}, "weaknesses": {"value": "1. More details to ensure the correctness of the image–layer–prompt triplets should be given. In scene understanding, the availabel models for object detection and instance-segmentation can not perform well in all situations, especially for small or occlude objects. How doauthors deal with these cases?\n\n2. It would have been better to show the image distribution with respect to styles, e.g., real images, cartoon， posters and so on, and discuss the model performance in different styles."}, "questions": {"value": "What are the computational costs in terms of both human effort and GPU resources  to construct  training and testing datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7kyaolRWNm", "forum": "AlgRVfd1z7", "replyto": "AlgRVfd1z7", "signatures": ["ICLR.cc/2026/Conference/Submission13251/Reviewer_rYGP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13251/Reviewer_rYGP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993677368, "cdate": 1761993677368, "tmdate": 1762923930525, "mdate": 1762923930525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new task called Referring Layer Decomposition (RLD). RLD is conceptually similar to referring image segmentation, whose inputs can be text or spatial cues of the target object. The output of the task is the layer representation of the target object (RGBA, completed object + alpha mask). Since the most critical factor for generative AI models is the data, this work scales up the RGBA data generation pipeline and creates the RefLade dataset which is significantly larger than prior real-world RGBA datasets. Then, authors design an evaluation metric, which is shown to be aligned with human preference. Finally, a simple baseline model, RefLayer, is designed and fine-tuned on the proposed dataset. Despite its simple design, it achieves significantly better performance than prior works."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to follow. The figures and charts help understand the pipeline and the statistics of the dataset very well.\n- We all know that data is key to the advance of GenAI. This paper proposes a large-scale, real-world dataset for an interesting task. The data generation pipeline involves several SOTA models which guarantee the high quality of the dataset.\n- The analysis on data scale, data quality (different subsets), and pre-trained models is comprehensive.\n- I love how the proposed metric is aligned with human preference. This is critical for a useful metric to monitor real model progress."}, "weaknesses": {"value": "I do not see any major weaknesses in the paper. As a paper that defines a new task and proposes a new dataset, every aspect of it is executed very well. Maybe one concern is having more baselines: the paper proposes its own simple baseline which is great, but would it be possible to adapt some prior work's models to this task and benchmark against your proposed method?\n\nAnother weakness is, can you show some downstream application of the dataset, similar to Sec.4.4 of the MULAN paper. For example, is it possible to fine-tune LayerDiffuse on the single-object subset of RefLade, and see if that improves performance? Or fine-tune InstructPix2Pix to perform object removal & insertion tasks? These results will definitely make the paper stronger, though I think the current form is already enough for acceptance."}, "questions": {"value": "- The HPA scores are low when using text prompts. Can you provide some qualitative failure case analysis? One issue might be there are multiple similar objects (e.g. belong to the same category) in an image, so text prompts along cannot disambiguate between them.\n- What are model 1-9 in Fig.4? Are they the same model trained on different steps / amount of data?\n- Typo in Fig.1 \"Linguistic Prompting\" part: “The Brown and white house” should be \"horse” not \"house\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gub6L4giEo", "forum": "AlgRVfd1z7", "replyto": "AlgRVfd1z7", "signatures": ["ICLR.cc/2026/Conference/Submission13251/Reviewer_HNvP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13251/Reviewer_HNvP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762793932032, "cdate": 1762793932032, "tmdate": 1762923930120, "mdate": 1762923930120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the task of Referring Layer Decomposition (RLD), which aims to recover a complete RGBA layer corresponding to a user-provided prompt, such as spatial maps or text. To support this task, the authors construct RefLade, a large-scale dataset of image-layer-prompt triplets produced through a combination of automatic generation and human curation, along with an evaluation protocol assessing preservation, completion, and faithfulness. A diffusion-based baseline, RefLayer, is proposed and evaluated extensively. Overall, the paper presents a well-motivated and clearly defined formulation, a high-quality dataset, and a comprehensive baseline, offering a valuable foundation for future research on image layer decomposition."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed dataset constitutes a significant improvement over existing resources for this problem domain, both in scale and in the level of curation. The combination of automated and manual verification enhances its overall quality and reliability.\n- The paper provides thorough evaluations, including analyses of design choices, as well as assessments of the alignment between the proposed metrics and human judgments.\n- The paper is clearly written, with a well-motivated problem statement and sufficient technical and implementation details."}, "weaknesses": {"value": "The paper is overall good, and I didn't find major weaknesses. One minor:\n\nFor the completion metric, what is the rationale for defining it as the difference between CLIP embeddings, $f(g_\\text{rgb}) - f(g_\\text{rgb} * g_v)$, rather than directly using the CLIP embedding of the non-visible region, $f(g_\\text{rgb} * (1 - g_v))$? \nThe motivation for this specific formulation should be clarified."}, "questions": {"value": "No questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N6i7HRrqwW", "forum": "AlgRVfd1z7", "replyto": "AlgRVfd1z7", "signatures": ["ICLR.cc/2026/Conference/Submission13251/Reviewer_kTb4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13251/Reviewer_kTb4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762815300406, "cdate": 1762815300406, "tmdate": 1762923929612, "mdate": 1762923929612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}