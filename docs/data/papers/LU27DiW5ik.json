{"id": "LU27DiW5ik", "number": 7826, "cdate": 1758037812076, "mdate": 1759897829680, "content": {"title": "EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models", "abstract": "CUDA kernel optimization has become a critical bottleneck for AI performance, as deep learning training and inference efficiency directly depends on highly optimized GPU kernels.\n   Despite the promise of Large Language Models (LLMs) for automating kernel optimization, this field suffers from a fragmented ecosystem of isolated and incomparable approaches with unclear problem formulations.\n   Furthermore, general-purpose LLM code evolution methods cannot meet strict correctness requirements of CUDA kernel optimization.\n   We address these fundamental challenges by first formalizing CUDA kernel optimization as a code optimization task with clear objectives, constraints, and evaluation metrics.\n   We then establish the first systematic LLM-based code evolution framework, EvoEngineer, that provides guidance for designing and adapting optimization strategies to achieve a balance between performance and correctness.\n   Finally, we implement a kernel optimization system based on this framework and conduct extensive experiments on 91 real-world CUDA kernels.\n   Our results demonstrate that EvoEngineer achieves a principled balance between performance and correctness, with the highest averaged median speedup of \\textbf{2.72}$\\times$ over baseline CUDA kernels and a code validity rate of \\textbf{69.8}\\%, outperforming existing methods on both dimensions.\n   Our method achieves a maximum speedup of \\textbf{36.75}$\\times$ among all operations over PyTorch kernels and delivers the highest speedup on \\textbf{25} (\\textbf{59.5\\%}) of 42 operations that achieve over \\textbf{2$\\times$} acceleration. Our code is available at \\url{https://anonymous.4open.science/r/EvoEngineer_open-FD4E/ }.", "tldr": "", "keywords": ["Kernel Engineering", "Evolutionary Algorithm", "Large Language Model"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/771b7a4cdaf17e35b0367cdb98b491906c9a7dac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes EvoEngineer, a systematic framework that utilizes large language models to automatically optimize CUDA kernel code. It classifies prompts to explore the impact of different prompt types on the results. The framework also supports the verification of syntactic correctness and performance testing, feeding the results back to the large language model for iteration. Finally, a population management component selects the high-quality solutions provided by the LLM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method proposed in the paper performs well and has been verified on multiple LLMs.\n\n2. Significant effort was invested in replicating the key baseline, AI CUDA Engineer, which greatly enhances the credibility of the comparison results."}, "weaknesses": {"value": "1.The experiments rely entirely on expensive large-scale models (GPT-4.1, DeepSeek-V3.1, Claude-Sonnet-4), without evaluating low-cost LLMs such as Qwen3-32B or Qwen3-Next (or even Qwen-7B), thus limiting insights into the scalability of the framework in budget-constrained settings.\n\n2.The lack of detailed prompt examples limits the reader’s understanding of the proposed prompt engineering process.\n\n3.The experimental evaluation, though extensive, has several notable limitations. All experiments were conducted on a single RTX 4090 GPU, raising concerns about cross-platform generalizability. Performance measurements suffer from inherent stochasticity without statistical significance testing. Moreover, only Level-1 kernels from KernelBench were evaluated, excluding more complex fusion and model-level tasks.\n\n4.The paper claims to propose an “LLM-driven evolutionary optimization” framework, but in practice it still relies on a static heuristic evolutionary algorithm without incorporating learning signals, reward modeling, or adaptive search.\nAs a result, the whole system behaves more like a stack of prompts with manually designed strategies rather than a genuinely intelligent evolutionary process."}, "questions": {"value": "1.How does the proposed method perform on small-scale LLMs?\n\n2.Although the tasks are not exactly the same, some common code optimization metrics[1] may be of reference.\n\n[1]Shypula A, Madaan A, Zeng Y, et al. Learning performance-improving code edits[J]. arXiv preprint arXiv:2302.07867, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Mz8jC1vxt4", "forum": "LU27DiW5ik", "replyto": "LU27DiW5ik", "signatures": ["ICLR.cc/2026/Conference/Submission7826/Reviewer_M7M3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7826/Reviewer_M7M3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760963385580, "cdate": 1760963385580, "tmdate": 1762919872195, "mdate": 1762919872195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EvoEngineer, a framework for automated CUDA kernel code optimization leveraging large language models (LLMs). The authors formalize kernel optimization as a constrained code evolution problem, and propose a modular framework decomposing LLM-driven code evolution into two components: traverse techniques (including two sub-modules solution guiding and prompt engineering) and population management. The framework is instantiated into three concrete strategies based on the information the agent receive. The experiment are conducted on 91 real-world CUDA kernels from Kernel Bench, where EvoEngineer demonstrates best trade-offs between kernel speedup and code correctness compared to existing LLM-based and evolutionary code optimization methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-  By splitting LLM-based code optimization into \"traverse techniques\" and \"population management,\" and further into a two-layer model, the method enables independent analysis of information flows and optimization effectiveness.\n- The paper provides a constrained optimization formulation for CUDA kernel code evolution (Section 3.1, Equation 1).\n- The study offers practical guidelines for evaluating code optimization methods by exploring the trade-offs among token/computational usage, speedup, and validity."}, "weaknesses": {"value": "- While the two-layer \"traverse technique\" decomposition is justified (Section 4.1.1), the paper tends to overgeneralize the implications. Specifically, the paper asserts that all \"operator-based\" prior works conflate strategy with implementation, without a sufficiently granular breakdown or ablation study against competing operator definitions in, e.g., FunSearch.\n- The mathematical formalization is restricted mainly to the high-level constrained optimization form (Equation 1), with little follow-through on more formal properties (e.g., landscape characterization, convergence of solution populations, or theoretical limitations of LLM-guided mutation/search in this discrete code space). The formal optimization statement is sound but does not translate into deeper theoretical results or predictive analysis of code search behavior.\n- While Figures 4, 6, 7 present token-resource curves, the study's resource trade-off analysis is mostly empirical. There is little theoretical or algorithmic reasoning about the sensitivity (e.g. for different environments) or scaling (e.g., for different hardware settings)."}, "questions": {"value": "- The provided framework is described at an abstract level. What would be a concrete example?\n- The evaluation of kernel correctness and speedup is performed primarily using median and count metrics. Could the authors provide further analysis of error and failure cases?\n- Could the authors provide more insight into which operation categories benefit most and least from each EvoEngineer configuration, and explain why?\n- During the search process, which optimization techniques (such as memory coalescing and occupancy optimization) are employed to accelerate the model? Which techniques are utilized most frequently?\n- How sensitive is EvoEngineer to the selection and diversity of its \"historical solutions\" and \"optimization insights\" components? Have any ablation studies or diagnostic analyses been conducted to assess potential overfitting or instability arising from these information sources?\n- How does the design of the prompt engineering layer affect code optimization performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xowJVWWNd6", "forum": "LU27DiW5ik", "replyto": "LU27DiW5ik", "signatures": ["ICLR.cc/2026/Conference/Submission7826/Reviewer_C1R6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7826/Reviewer_C1R6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761348265190, "cdate": 1761348265190, "tmdate": 1762919871622, "mdate": 1762919871622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes EvoEngineer, a system for automatic cuda code generation using LLMs and iterative refinement."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "I believe the correctness results are not (or only marginally, re. reward hacking) impacted by my criticism of the evaluation below, and these show general improvements over previous methods."}, "weaknesses": {"value": "unfortunately, the paper is hamstrung by the fact that I don't think any of the evaluations are meaningful.\nwhile the paper itself is sparse in details, it generally defers to KernelBench, which (possibly unbeknownst to the authors) is critically flawed.\nJust taking the first example from the linked repository:\n> \"org_py_code\": \"import torch\\nimport torch.nn as nn\\n\\nclass Model(nn.Module):\\n    \\\"\\\"\\\"\\n    A model that computes Hinge Loss for binary classification tasks.\\n\\n    Parameters:\\n        None\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        super(Model, self).__init__()\\n\\n    def forward(self, predictions, targets):\\n        return torch.mean(torch.clamp(1 - predictions * targets, min=0))\\n\\nbatch_size = 128\\ninput_shape = (1,)\\ndim = 1\\n\\ndef get_inputs():\\n    return [torch.randn(batch_size, *input_shape), torch.randint(0, 2, (batch_size, 1)).float() * 2 - 1]\\n\\ndef get_init_inputs():\\n    return []\",\n\nThe input shape is `128 x 1`; there is no meaningful work to be done in this case, its just launch overhead. \n\nIn the past year, we've seen multiple claims of \"high-performance\" LLM-written cuda kernels followed by retractions because the evals were broken or the LLM was reward-hacking. Note that, e.g., the citation https://pub.sakana.ai/ai-cuda-engineer/leaderboard given in the paper 404s nowadays.\n\nTo  gain confidence that the results your seeing are real, you  need to\n* ensure the inputs are large enough to justify launching a cuda kernel\n* determine the speed-of-light of the kernels in question, i.e., calculate what the fastest possible execution of the kernel is based on  the GPUs memory bandwidth and flops; if the llm is faster than the speed-of-light, you know something is broken\n* validate that you use proper baselines (e.g., torch.compile,  enabling tensor cores, ...); again,  speed-of-light can help to determine whether the baseline is unexpectedly weak\n* carefully analyse individual cases with large speed-ups. Anything over 2x is highly suspicious, for matmul-like operations and other things that are used extensively in typical pytorch workloads, I'd consider anything over 10% suspicious, too."}, "questions": {"value": "* what are the speed-ups on meaningful input shapes\n* what are the numbers1-6 in Table 4"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CBiV5Ugi97", "forum": "LU27DiW5ik", "replyto": "LU27DiW5ik", "signatures": ["ICLR.cc/2026/Conference/Submission7826/Reviewer_35Jg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7826/Reviewer_35Jg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960647147, "cdate": 1761960647147, "tmdate": 1762919869727, "mdate": 1762919869727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes EvoEngineer, a systematic framework for automated CUDA kernel optimization using LLMs. This work formalizes kernel optimization as a constrained code evolution problem and decomposes LLM-based code evolution into two orthogonal components: (1) traverse techniques, with a two-layer design of “solution guiding” and “prompt engineering”, and (2) population management for solution maintenance. Based on the evaluation of 91 CUDA kernels, the authors claim up to 2.72× median speedup and 69.8% code validity, outperforming AI CUDA Engineer, FunSearch, and EoH."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The automation of CUDA kernel optimization is an important and timely topic, especially for AI system efficiency.\n2. The paper is well organized and visually clear, with consistent formatting and informative tables.\n3. The authors make an effort to discuss a taxonomy of existing LLM-based code optimization methods.\n4. The experiment covers 91 CUDA kernels, demonstrating substantial engineering effort."}, "weaknesses": {"value": "1. novelty: The claimed “systematic framework” introduces no genuine algorithmic innovation; the proposed two-layer design is largely conceptual and repackages standard evolutionary-search principles without introducing new operators or optimization rules.\n2. The work lacks formalism and analytical grounding. Equation (1) remains a generic placeholder, with no derivation or justification for why the proposed decomposition improves search efficiency or correctness. \n3. Experimental methodology is insufficiently detailed: key factors such as kernel size, difficulty level, GPU utilization, batch size, and warm-up policy are omitted, and no ablation is provided to isolate the contribution of each framework component. As a result, the empirical evidence does not convincingly support the claimed advantages of the approach."}, "questions": {"value": "1. What is the key difference between your work and the current pipelines when they also do structured multi-stage optimization and correctness validation, especially when you are claiming as the first or unique?\n2. your repo has only the data and results, making it hard to test the performance. Especially when the results are remarkble, such as up to 36X max speedup. How is the “two-layer traverse” implemented in code? Is there any measurable effect from separating prompt construction from solution guidance?\n3. Why are no ablation studies provided to isolate the effects of (a) traverse technique design and (b) population strategy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t1LVy0lpRg", "forum": "LU27DiW5ik", "replyto": "LU27DiW5ik", "signatures": ["ICLR.cc/2026/Conference/Submission7826/Reviewer_sPg4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7826/Reviewer_sPg4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979960458, "cdate": 1761979960458, "tmdate": 1762919868586, "mdate": 1762919868586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}