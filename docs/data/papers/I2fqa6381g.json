{"id": "I2fqa6381g", "number": 16491, "cdate": 1758265125639, "mdate": 1759897237550, "content": {"title": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models", "abstract": "Training stability is a critical challenge in the pre-training of large language models (LLMs), particularly for architectures like Post-Norm Transformers prone to gradient explosion and dissipation. In this paper, we introduce Scale-Distribution Decoupling (SDD), a novel approach designed to enhance training stability by explicitly decoupling the scale and distribution of the weight matrix within fully-connected layers. SDD employs a normalization mechanism to regulate activation magnitudes and a learnable scaling vector to maintain well-conditioned gradients, thereby effectively preventing gradient explosion and dissipation and ensuring stable gradient propagation. This principled separation improves optimization efficiency, especially in deep networks. Extensive experiments across various LLM architectures (dense and MoE) demonstrate that SDD consistently achieves faster convergence and superior performance compared to existing normalization techniques. Furthermore, SDD is lightweight and seamlessly compatible with current frameworks, offering a practical and effective solution for robust LLM training.", "tldr": "", "keywords": ["Large Language Models", "Training Stability", "Pre-Training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a248414bd94c3980e04d36000048a19d1b4c835b.pdf", "supplementary_material": "/attachment/00e155b2a16f42492acedc282d98bbd4a3d43736.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Scale-Distribution Decoupling (SDD), a reparameterization of fully connected layers that separates the learning of “distribution” and “scale.” Concretely, it replaces $y=Wx$ with $y = \\alpha \\bigodot norm \\left( Vx \\right)$, where $norm(⋅)$ denotes RMS normalization and $\\alpha$ is a learnable vector controlling the output magnitude. The motivation is that coupling scale and distribution in the weight matrix leads to unstable gradients in deep models, particularly in Post-Norm Transformers. The authors claim that SDD improves gradient conditioning, accelerates convergence, and enables more stable training, with negligible computational and parameter overhead. Experiments on 1B and 7B dense models, as well as an MoE variant, show lower training loss, faster convergence (up to 2.1x), and modest downstream accuracy improvements compared to normalization baselines such as Pre-Norm, Post-Norm, DeepNorm, MixLN, and nGPT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The idea of embedding normalization inside FC layers is simple to integrate and practically appealing.\n * The empirical results are extensive, covering both dense and MoE settings, with consistent gains in convergence stability.\n * The paper provides some theoretical intuition and gradient analyses suggesting bounded updates and stable gradient norms."}, "weaknesses": {"value": "* Algebraically, the SDD formulation is identical to an FC layer followed by RMSNorm with a learned scale, raising doubts about whether it introduces a fundamentally new mechanism. If this equivalence holds, the proposed method substantially overlaps with prior formulations such as QK LayerNorm and PeriLN. The authors are encouraged to clarify this point and provide rebuttals in their response.\n * The claimed expressiveness equivalence relies on Gaussian assumptions and normalization approximations; the proofs do not establish true functional equivalence or new representational capacity.\n * Reported efficiency gains (2.1x faster) are not clearly separated from possible implementation or hardware factors. (Figure 1 and Section 4.2) It is important to distinguish between compute and step efficiencies.\n * Comparisons focus mainly on OLMo-based setups; broader baselines (e.g., with existing Post-Norm + RMSNorm or WeightNorm variants) are missing.\n * The paper’s contribution may thus reduce to a restatement of existing normalization practices, which needs stronger empirical or theoretical justification to be convincing."}, "questions": {"value": "* Have you compared SDD against existing per-layer or per-projection normalization schemes (e.g., WeightNorm, ScaleNorm, MixLN) under controlled setups to isolate its unique effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bEWTd1m6Ya", "forum": "I2fqa6381g", "replyto": "I2fqa6381g", "signatures": ["ICLR.cc/2026/Conference/Submission16491/Reviewer_nbiS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16491/Reviewer_nbiS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16491/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892217702, "cdate": 1761892217702, "tmdate": 1762926590910, "mdate": 1762926590910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Scale-Distribution Decoupling (SDD), a new formulation of fully connected layers for large language models (LLMs) that explicitly separates the scale and distribution of weight matrices. By applying RMS normalization to the transformed activations and introducing a learnable per-channel scaling vector α, SDD aims to stabilize gradient propagation and prevent explosion or vanishing, especially in Post-Norm Transformers. The paper provides approximate theoretical analysis and extensive experiments on dense (1B/7B) and MoE models trained up to 4T tokens, reporting faster convergence, better downstream performance, and improved robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Reinterprets Transformer instability as arising from weight scale-distribution coupling and proposes a structural decoupling mechanism.\n- Large-scale evaluations (1B/7B/MoE, up to 4T tokens) consistently show improved stability, convergence speed, and downstream accuracy.\n- SDD requires minimal architectural change (one scaling vector per layer) and adds negligible overhead (<0.3%).\n- The paper is accessible and addresses a core bottleneck in LLM optimization."}, "weaknesses": {"value": "- The analysis is illustrative and dependent on simplified assumptions (Gaussian activations, RMS~1). No formal bound or proof ensures stability in realistic, non-ideal conditions.\n- The paper omits conceptually related techniques that also decouple or normalize weight magnitude and direction (e.g., Weight Normalization, Weight Standardization). Including such baselines would clarify whether SDD’s advantage truly comes from explicit decoupling.\n- Some figures mix different training budgets (e.g., 200B vs. 2T tokens), and no variance across random seeds is reported, which limits the statistical reliability of the claims.\n- All experiments are confined to the OLMo/OLMoE framework and datasets; it remains unclear how SDD generalizes to other architectures or pretraining corpora."}, "questions": {"value": "- Can the authors include comparisons with weight reparameterization or normalization methods (e.g., Weight Normalization, Weight Standardization) to verify that SDD’s gains indeed stem from scale-distribution decoupling rather than normalization placement?\n- How robust is SDD’s theoretical argument when the Gaussian or independence assumptions are violated (as in real Transformer activations)?\n- Could the authors provide matched-budget comparisons (same token count) and report variance across multiple seeds to improve fairness and statistical confidence?\n- Would SDD maintain its benefits under different pretraining corpora, tokenization schemes, or architectures outside the OLMo family?\n- As an ablation, what happens if α is fixed or reduced to a scalar? Does the benefit persist?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V09aFy7p0I", "forum": "I2fqa6381g", "replyto": "I2fqa6381g", "signatures": ["ICLR.cc/2026/Conference/Submission16491/Reviewer_57A3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16491/Reviewer_57A3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16491/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894120319, "cdate": 1761894120319, "tmdate": 1762926590312, "mdate": 1762926590312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors introduce Scale-Distribution Decoupling (SDD), a novel approach designed to enhance training stability by\nexplicitly decoupling the scale and distribution of the weight matrix within fullyconnected layers.\nKey idea is to replace fullyconnected layers by \"y = α ⊙ norm(V x)\", where α is learnable vector (learns magnitude) and V is weight primary transformation.\nThey show that their method consistently improves both convergence speed and training efficiency, illustrated by achieving similar training\nloss levels approximately 2.1× faster in 7B models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Authors propose Scale-Distribution Decoupling (SDD), a novel approach designed to enhance training stability by\nexplicitly decoupling the scale and distribution of the weight matrix within fullyconnected layers:\"y = α ⊙ norm(V x)\", where α is learnable vector (learns magnitude) and V is weight primary transformation. Previous work did decoupling of weight magnitude vs weight learnable distribution as in \"σREPARAM: STABLE TRANSFORMER TRAINING WITH SPECTRAL REPARAMETRIZATION\", and applied layer scale before every residual connection as in \"Going deeper with Image Transformers\". This work applies layer scale after every fully connected layer combined with normalization of the output of fully connected layer.\n\nThis method consistently improves both convergence speed and training efficiency, illustrated by achieving similar training loss levels approximately 2.1× faster in 7B models.\n\nExtensive experiments on both dense and MoE models.\n\nThorough ablation studies."}, "weaknesses": {"value": "it would be interesting to see model accuracy with layer scale only, to see the impact of every component in the proposed method."}, "questions": {"value": "Authors evaluated model training stability with higher learning rate, have you tried this method on quantized training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AKqdUXpNf7", "forum": "I2fqa6381g", "replyto": "I2fqa6381g", "signatures": ["ICLR.cc/2026/Conference/Submission16491/Reviewer_H8Yk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16491/Reviewer_H8Yk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16491/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952512350, "cdate": 1761952512350, "tmdate": 1762926589253, "mdate": 1762926589253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Scale‑Distribution Decoupling (SDD), a simple reparameterization of fully‑connected layers that decouples the magnitude and the directional/distributional component of the linear transform. SDD is tested on dense 1B and 7B models and a 588M‑active MoE built on OLMo/OLMoE backbones. The experiment indicates faster convergence compared to a strong baseline. The authors note that any small activation-memory increase during training can be largely mitigated with gradient checkpointing, making SDD a low-friction, drop-in change for practical LLM training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1, SDD is an appealingly simple and practical twist: normalize activations of the linear map and introduce a per‑feature scaling vector .\n\n2 If widely reproducible, the stability under Post‑Norm can be impactful.\n\n3 Consistent gains on both dense and MoE transformer variants."}, "weaknesses": {"value": "Please see the questions."}, "questions": {"value": "1, The caption states in Figure 9, “SDD‑1B achieves the highest inter‑layer similarity”, but the text below claims “SDD‑1B exhibits the lowest similarity, indicating reduced feature redundancy.” These statements point in opposite directions and conflict with the earlier motivation that feature collapse corresponds to higher similarity.\n\n2, I was wondering does “SDD” specifically means SDD (Post-Norm) in this paper?\n\n3, One of my concerns is about the memory usage. The paper states, \" SDD’s additional memory cost during training can be effectively eliminated through gradient checkpointing.\" Could you report the approximate memory usage with and without checkpointing (Baseline vs. SDD) to illustrate this effect?\n\n4, The paper reports 2.1× faster convergence; I assume this is measured in tokens-to-target-loss. Could you also provide a brief wall-clock indication (same hardware/parallelism)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lX44nXoP1z", "forum": "I2fqa6381g", "replyto": "I2fqa6381g", "signatures": ["ICLR.cc/2026/Conference/Submission16491/Reviewer_rsXb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16491/Reviewer_rsXb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16491/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762036219335, "cdate": 1762036219335, "tmdate": 1762926588592, "mdate": 1762926588592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Scale-Distribution Decoupling (SDD), a method designed to improve training stability in large Transformer and Mixture-of-Experts models. The key idea is to separate the optimization of weight scale and distribution by normalizing the linear transformation output and introducing a learnable scaling vector for each layer. This design aims to prevent gradient explosion and vanishing, thereby simplifying optimization and accelerating convergence. Experiments on dense and MoE architectures show that SDD achieves faster convergence and modest improvements in downstream accuracy compared to existing normalization techniques."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Easy to follow.\n- Clear motivation and coherent formulation.\n- Implementation simplicity and minimal overhead."}, "weaknesses": {"value": "* **The theoretical novelty is limited.**\n  The proposed formulation closely parallels prior normalization and reparameterization methods (e.g. *ScaleNorm / RMSNorm* variants) which already decouple magnitude and directional components of weights. SDD mainly relocates the learnable scale to the output side, making it more of a reinterpretation than a fundamentally new principle.\n\n* **This should be directly compared to σ-reparameterization.**\n  σ-reparameterization [1] explicitly addresses gradient explosion through *spectral normalization* of Transformer weights. Both SDD and σ-reparam normalize every linear transformation, but σ-reparam constrains the Lipschitz constant to a fixed value *c*, providing a stricter bound on gradient growth. Since SDD also targets exponential variance–driven gradient explosion, a head-to-head comparison is necessary to establish whether it offers genuine advantages.\n\n* **Analytical claims are not empirically substantiated.**\n  The paper claims gradient-norm preservation and improved conditioning but does not provide quantitative diagnostics such as singular-value spectra, variance propagation, or layer-wise gradient statistics to support these claims.\n\n* **Related-work coverage is incomplete.** Given that the central contribution of this paper is to explicitly control and decouple scale, a broader discussion of prior work addressing scale dynamics in deep networks is expected. Numerous studies—spanning normalization, initialization, and parameterization—have explored scale stability (e.g., weight normalization, residual scaling, spectral constraints, and depth-scaled initialization). However, the paper limits its comparison mostly to LayerNorm variants, without situating SDD within this larger body of “scale-centric” literature. As a result, the connection to existing understanding of scale propagation and stability remains underdeveloped, and the claimed novelty feels overstated.\n\n\n---\n\n**[1]** *Stabilizing Transformer Training by Preventing Attention Entropy Collapse*, ICML 2023"}, "questions": {"value": "I recommend tracking gradient variance rather than gradient norm, since the mean gradient drifts from zero during training and early-stage stability is tightly correlated with variance [2]. \n\n \n[2] On the Variance of the Adaptive Learning Rate and Beyond, ICLR 2020"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jhRBOjmf3w", "forum": "I2fqa6381g", "replyto": "I2fqa6381g", "signatures": ["ICLR.cc/2026/Conference/Submission16491/Reviewer_P1kD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16491/Reviewer_P1kD"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16491/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762519001381, "cdate": 1762519001381, "tmdate": 1762926588129, "mdate": 1762926588129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Clarification and Summary Regarding the Rebuttal Response"}, "comment": {"value": "Dear Reviewers, ACs, SACs, and PCs,\n\nWe sincerely thank all reviewers and ACs for their time, constructive comments, and detailed evaluations. Across the reviews, our work was acknowledged for its clarity, practicality, and consistent improvements on both dense and MoE models. In our rebuttal, we clarified that SDD is not a normalization-operator variant, but a structural reparameterization applied inside every fully connected layer, derived from an SVD-based perspective that decouples scale and distribution. This distinguishes it from RMSNorm/ScaleNorm, QK LayerNorm, PeriLN, and $\\sigma$-Reparam. We further showed that SDD outperforms strong baselines, including Post-Norm, DeepNorm, Mix-LN, and nGPT, under matched training budgets, and added an additional comparison to ScaleNorm for completeness.\n\nWe also addressed theoretical concerns (Gaussian assumptions, expressiveness equivalence), efficiency questions (2.1× token-to-target-loss speedup with negligible FLOPs/parameter overhead), and implementation issues such as memory usage and the effect of gradient checkpointing. To resolve fairness concerns, we clarified that all controlled comparisons use identical budgets, while only one critical comparative experiment was extended to 2T tokens for long-horizon analysis. We also explained why random-seed variance becomes negligible in 4T-token from-scratch training. Regarding generality, we emphasized that the OLMo architecture matches widely used Transformer backbones such as Qwen, and our experiments already include diverse architectural variants (Post-Norm, DeepNorm, Mix-LN, nGPT, dense, and MoE), demonstrating broad applicability.\n\nWe appreciate the reviewers’ thoughtful feedback and will integrate all clarifications, additions, and improvements into the final version. Thank you again for your time and effort in evaluating our submission.\n\nBest regards,\n\nAll Authors"}}, "id": "f805ezsBzk", "forum": "I2fqa6381g", "replyto": "I2fqa6381g", "signatures": ["ICLR.cc/2026/Conference/Submission16491/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16491/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission16491/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763731358948, "cdate": 1763731358948, "tmdate": 1763731358948, "mdate": 1763731358948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}