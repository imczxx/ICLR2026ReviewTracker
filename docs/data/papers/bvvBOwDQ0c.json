{"id": "bvvBOwDQ0c", "number": 14393, "cdate": 1758234390551, "mdate": 1759897372889, "content": {"title": "A Revisit of Total Correlation in Disentangled Variational Auto-Encoder with Partial Disentanglement", "abstract": "A fully disentangled variational auto-encoder (VAE) aims to identify disentangled latent components from observations. However, enforcing full independence between all latent components may be too strict for certain datasets. In some cases, multiple factors may be entangled together in a non-separable manner, or a single independent semantic meaning could be represented by multiple latent components within a higher-dimensional manifold. To address such scenarios with greater flexibility, we develop the Partially Disentangled VAE (PDisVAE), which generalizes the total correlation (TC) term in fully disentangled VAEs to a partial correlation (PC) term. This framework can handle group-wise independence and can naturally reduce to either the standard VAE or the fully disentangled VAE. Validation through three synthetic experiments demonstrates the correctness and practicality of PDisVAE. When applied to real-world datasets, PDisVAE discovers valuable information that is difficult to uncover with fully disentangled VAEs, implying its versatility and effectiveness.", "tldr": "We propose the Partially Disentangled VAE, which relaxes full independence into group-wise independence via a partial correlation term, enabling more flexible disentanglement and yielding valuable insights on both synthetic and real-world datasets.", "keywords": ["disentangled variational auto-encoder", "partial disentanglement", "group-wise independence"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a7f0a17da746d483e6c840976b1a03a6c267bbb6.pdf", "supplementary_material": "/attachment/a1097e79dfaeb05c229e7eb204dc65dfaf3267dc.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces an approach that tries to relax the full independence assumption in traditional disentangled VAEs to accommodate group-wise independence - a more realistic scenario where semantically meaningful factors my be inherently entangled within groups but independent across groups. The key innovation is the generalization of the total correlation (TC) penalty used in fully disentangled VAEs to a partial correlation (PC) penalty, which explicitly enforces independence between groups while flexibly permitting within-groups entanglement, allowing in principle the within-group disentanglement rank to vary. \n\nThe authors present an argument that attempting to disentangle multiple semantically meaningful factors may be a difficult endeavor, and that jointly encoding such entangled factors independently from another group of entangled semantically meaningful factors constitutes a more principle approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The work tackles an important representation learning problem by allowing statistically independent factors to be encoded as vector-valued rather than scalar variables, thereby enabling more expressive and efficient latent representations.\n\nThe theoretical argument supporting importance sampling, as compared to mini-batch weighted sampling and mini-batch stratified sampling, appears technically sound and adequately justified.\n\nThe proposed method may help to identify the underlying relationship between the entangled factors encoded in a single group."}, "weaknesses": {"value": "Even when the objective is to encode statistically independent groups of factors as vectors instead of disentangling on the basis of semantics, additional inductive biases are still required, since nonlinear ICA is fundamentally unidentifiable under the independence assumption alone. \n\nThe synthetic datasets employed for validation are limited to linear mappings from the representation space to the data space, which substantially constrains the generality and scope of the validation. This is evident from the weaker results on the nonlinear mapping, partial dsprites dataset.\n\nWhile the method aims to encode statistically independent factor groups, the learned representations seem to violate the group-wise independence (especially for the partial dsprites dataset) by capturing information from multiple factor groups simultaneously, substantially reducing interpretability.\n\nThe partial dsprites and the synthetic validation on the \"flexibly reduce to the fully independent case\" is not convincing (ficure 4c group 2) and it seems that the rank does not reduce to the true number of components, which greatly reduces the interpretability of the method.\n\nIt is very hard to interpret the results for Celeb-A and to know which groups of semantic meanings are encoded by which latent variables (and vice versa).\n\nA logcosh-priored VAE does not correspond to an equivalent nonlinear ICA problem since independence is insufficient for source recovery under nonlinear mixing."}, "questions": {"value": "What is the R2 alignment matrix for the partial dsprites dataset?\n\nProvide ablation studies with nonlinear mixing datasets\n\nWhat are the values of the disentanglement metrics, specifically the ones which measure independence (DCI, MIG, modularity, ...) on the partial dsprites dataset (appropriately adjusted to account for group independence)\n\nClarify the calculation of PC on the true latent (ground truth?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eSWm8BYf3A", "forum": "bvvBOwDQ0c", "replyto": "bvvBOwDQ0c", "signatures": ["ICLR.cc/2026/Conference/Submission14393/Reviewer_tj5C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14393/Reviewer_tj5C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761235962941, "cdate": 1761235962941, "tmdate": 1762924807096, "mdate": 1762924807096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PDisVAE, a framework for partial disentanglement in Variational Autoencoders (VAEs). The core idea is to generalize the Total Correlation (TC) penalty used in fully disentangled VAEs to a Partial Correlation (PC) penalty, which enforces independence betweengroups of latent variables while allowing for entanglement withineach group. The authors provide a theoretical analysis of batch approximation methods, arguing for the superiority of Importance Sampling (IS), and validate their method on synthetic and real-world datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper correctly identifies that enforcing full independence among all latent factors can be an overly strong and unrealistic inductive bias. Exploring partial, group-wise disentanglement is a meaningful direction for the field.\n2. The claim that the proposed framework can reduce to a standard VAE (G=1) or a fully disentangled VAE (G=K) is a theoretically appealing property, suggesting a generalized approach."}, "weaknesses": {"value": "1. The introduction spends too much time on well-known background and could be restructured to more sharply highlight the gap in the literature (the lack of flexible, penalty-based methods for partial disentanglement) and the paper's specific contributions.\n2. The explanation of the PC term is abrupt. A more intuitive explanation of how the PC penalty operates during training would be helpful. How does it practically encourage the formation of groups?\n3. The comparison to ISA-VAE is a critical point but is not fully leveraged. The paper correctly states that ISA-VAE uses a rigid prior (Lp-nested), while PDisVAE offers flexibility. However, the experiments, while showing PDisVAE's superiority, do not deeply analyze whyISA-VAE performs poorly on the synthetic tasks. Is it solely the prior mismatch? A more detailed discussion contrasting the prior-based vs. penalty-based approaches would strengthen the paper's contribution. \n4. A significant practical challenge is choosing the number of groups G. The paper mentions this as a limitation in the conclusion but defers it to future work. While it's a valid and difficult problem, providing some practical guidance or even a simple heuristic (e.g., based on the PC value on a validation set) would greatly increase the practical utility of the method."}, "questions": {"value": "1. How does the computational cost (training time, memory) of PDisVAE scale with the number of groups G and the chosen batch approximation method compared to a standard β-TCVAE?\n2. Could the performance gap between PDisVAE and ISA-VAE on your synthetic datasets be closed by using a more flexible family of priors within the ISA-VAE framework, or is the penalty-based approach fundamentally more adaptable?\n3. In a real-world scenario with no ground truth, how should a practitioner choose the number of groups (G) and the per-group dimensionality? Did you perform any sensitivity analysis on these hyperparameters for the real-world experiments?\n4. Can you demonstrate the utility of the partially disentangled representations learned by PDisVAE on a downstream task? For example, on CelebA, could you show that your method allows for more precise control and better generalization in attribute manipulation compared to a fully disentangled VAE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GD1yKBjPFB", "forum": "bvvBOwDQ0c", "replyto": "bvvBOwDQ0c", "signatures": ["ICLR.cc/2026/Conference/Submission14393/Reviewer_VLKy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14393/Reviewer_VLKy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917812389, "cdate": 1761917812389, "tmdate": 1762924806640, "mdate": 1762924806640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PDisVAE replaces the TC penalty with a PC term to enforce group-wise independence while allowing within-group entanglement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method proposed in this paper can degenerate into the standard VAE and fully disentangled VAEs.\n2. The paper not only presents theoretical results but also experimental results."}, "weaknesses": {"value": "1. From Eq 3 the paper defines the PC term. However, the paper does not explain how this quantity is practically estimated. It jumps directly to Eq 4, which proposes a batch approximation to the density q(z), and then states Theorem 3.1 claiming that IS is an unbiased estimator for q(z) with lower variance than MWS/MSS. This is presented as a key contribution. But there might be two issues here:\n\n(a) Theorem 3.1 is about the density level (estimating q(z)). It does not directly imply anything about the final objective, the PC term, which is a function of log-densities. Even if $q̂(z)$ is unbiased for q(z), the estimator $log q̂(z)$ is generally not unbiased for log q(z) because of Jensen’s inequality (equality holds only in degenerate cases). Hence, unbiasedness does not transfer to the PC term.\nFor variance, one might hope that smaller $Var[q̂(z)]$ imply a smaller variance of $\\hat{PC}$. But this derivation is not trivial. It need further analysis. Since the paper does not provide this analysis, Theorem 3.1 gives limited support for statistical claims about the PC estimator (core proposal of the paper).\n\n(b) Readers can not see any explicit formula or introductions for the estimator used for the PC term. For estimating PC term, there might be two ways: one is like FactorVAE, directly estimating density-ratio through additional network. One is first estimate qz then take logs and .... I guess authors chose the latter one. Therefore it would be better to show a comprison (theoretical or empirical) between these two ways focusing on their estimation performance with respect to the PC term rather than qz.\n\n\n\n2. For Fig 3a, it seems it is a synthetic experiment, thus we expect PC term to be zero. Could you discuss why the result is mean:0.332 (std: 0.006), which is far away from 0?\n\n3. line 437, 'RMSE= 0.47', but in fig 5 RMSE are around 0.046 \n\n4. A minor comment:This paper replaces the TC term with a new term to enforce group-wise independence, and names this term \"partial correlation\". However, this choice of terminology is problematic, because \"partial correlation\" is already a well-established technical term in statistics and machine learning. Maybe use another word if possible, or state their difference."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IjoasvENrq", "forum": "bvvBOwDQ0c", "replyto": "bvvBOwDQ0c", "signatures": ["ICLR.cc/2026/Conference/Submission14393/Reviewer_3jNW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14393/Reviewer_3jNW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971756231, "cdate": 1761971756231, "tmdate": 1762924806132, "mdate": 1762924806132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}