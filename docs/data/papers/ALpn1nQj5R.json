{"id": "ALpn1nQj5R", "number": 19872, "cdate": 1758300124915, "mdate": 1759897014925, "content": {"title": "Dual-Path Condition Alignment for Diffusion Transformers", "abstract": "Denoising-based generative models have been significantly advanced by representation-alignment (REPA) loss, which leverages pre-trained visual encoders to guide intermediate network features. However, REPA's reliance on external visual encoders introduces two critical challenges: potential \\textit{distribution mismatches} between the encoder's training data and the generation target, and the high \\textit{computational costs} of pre-training. Inspired by the observation that REPA primarily aids early layers in capturing robust semantics, we propose an unsupervised alternative that avoids external visual encoder and the assumption of consistent data distribution. We introduce \\textit{\\textbf{DU}al-\\textbf{P}ath condition \\textbf{A}lignment} (\\textbf{DUPA}), a novel self-alignment framework, which independently noises an image multiple times and processes these noisy latents through decoupled diffusion transformer, then aligns the derived conditions\\textemdash low-frequency semantic features extracted from each path. Experiments demonstrate that DUPA achieves FID$=$1.46 on ImageNet 256$\\times$256 with only 400 training epochs, outperforming all methods that do not rely on external supervision. Critically, DUPA accelerates training of its base model by 5$\\times$ and inference by 10$\\times$. DUPA is also model-agnostic and can be readily applied to any denoising-based generative model, showcasing its excellent scalability and generalizability.", "tldr": "", "keywords": ["Diffusion Transformer", "Self-Supervised Learning", "Representation Learning."], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d3d7f066a2bdfdbd7bd460505ecaa72201b7363.pdf", "supplementary_material": "/attachment/923abb9d0018585f1211fafd4136c6c0e24b9426.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to accelerate the diffusion transformer training without an external pre-trained encoder by aligning latents from multiple noisy images through decoupled diffusion transformers. They show the proposed method, DUPA, can perform better than REPA without any external guidance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-motivated and easy-to-follow.\n\n2. The proposed method, DUPA, consistently improves the baselines, and even perform better than REPA.\n\n3. Extensive analysis demonstrates the effectiveness of each suggested component."}, "weaknesses": {"value": "1. The authors point out the weakness of REPA in practical scenarios (i.e., beyond the ImageNet), e.g., out-of-distribution problem can arise, and additional pretraining is required. However, the authors do not handle such scenarios in the main experiments: They only conducted ImageNet generation results, which makes it difficult to argue whether the proposed method indeed addresses the problem of REPA. I think such a problem of REPA does not arise in the image generation problem, as (1) recent pretrained visual representations (e.g., DINOv3 and SigLIPv2) are trained on extremely large-scale datasets, and (2) we can easily use open-source image encoders.\n\n2. The authors fixed the batch size at 256 for their experiments, but I think that increasing the sampling times K can have a similar effect to enlarging the batch size. In fact, a previous study [1] has shown that applying augmentations to the same batch can yield better performance with fewer iterations. Therefore, it is needed to verify how much of the improvement in the proposed method comes from this effect, e.g., by training SiT with a batch size of 512, or using sampling times K with only a flow matching loss.\n\n[1] Hoffer et al., Augment Your Batch: Improving Generalization Through Instance Repetition, CVPR 2020"}, "questions": {"value": "Please answer the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nvl6Sv5ggs", "forum": "ALpn1nQj5R", "replyto": "ALpn1nQj5R", "signatures": ["ICLR.cc/2026/Conference/Submission19872/Reviewer_A859"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19872/Reviewer_A859"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936143647, "cdate": 1761936143647, "tmdate": 1762932039523, "mdate": 1762932039523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work (DUPA) first points out the internal issues brought by REPA: \n(1) Out of distribution. and (2) Huge additional computational costs. \nThen inspired by the idea of DDT, SD-DIT and Contrastive FM works, the authors propose DUPA to directly align  two noisy views of a single image without external supervision. And such self-alignment (unsupervised alignment)  can significantly accelerate the convergence of SiT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed DUPA is simple but effective, without the reliance of external ViTs, like dinov2.\n2. The author’s writing is very straightforward and concise, without storytelling or beating around the bush.\n3. Clear convergence of SiT is brought by DUPA.\n4. Sufficient experiments ."}, "weaknesses": {"value": "1. More discussion and analysis about: why such self-supervised alignment could work for the convergence of SiT?\nfor example in SD-DiT[2], the choice of t=min (mostly close to pure image) is the most effective acceleration technique. And how about DUPA?\n2.  Recently there are some works [1][2] like DUPA, focusing on the self-alignment DiT, please claim the difference/advantage/difsussion compared with DUPA  and add the corresponding reference.\n\n[1] Jiang, Dengyang, et al. \"No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves.\" arXiv preprint arXiv:2505.02831 (2025).\n[2] Zhu, Rui, et al. \"Sd-dit: Unleashing the power of self-supervised discrimination in diffusion transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."}, "questions": {"value": "See weakness.\n\nI am very willing to raise my ratings if you can provide sufficient discussions mentioned in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "K2mvKhmRfq", "forum": "ALpn1nQj5R", "replyto": "ALpn1nQj5R", "signatures": ["ICLR.cc/2026/Conference/Submission19872/Reviewer_7oUe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19872/Reviewer_7oUe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991249896, "cdate": 1761991249896, "tmdate": 1762932039083, "mdate": 1762932039083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper present DUPA, a self supervised approach for representation alignment for boosting the convergence speed of diffusion transformers. The core idea is to utilize representations from a parallel branch which has the input of the same image but at a different noise level Such a mode of training would force the model to learn noise robust features faster leading to faster convergence. Experiments show an improvement of training speed by 5x and the model achieving similar performance to REPA."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of utilizing augmented versions of the same image as input and aligning their representations is a clever approach for representation alignment. Such a mode of training may scale better for text to image training when compared to a vision encoder that introduces an inductive bias\n2. The idea is novel and solid and the authors have performed extensive experiments to find the suitable layers and hyperparameters for DUPA. \n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. Is there some contraints on the value of the independently sampled timesteps needed for better performance? As an example, assume that one timestep is sampled with maximum noise and the other at minimum noise, aligning their representations might be a case where training with DUPA loss might not leading to a meaningful solution. \n2. Would performing DUPA on multiple layers at the same time lead to a better performance? \n3. I think the claim regarding 10x inference speed may be a bit misleading. Usually diffusion models are utilized to obtain a few samples. I believe with the current setup, DUPA will portray similar sampling speeds to REPA. I would advise the authors to correct this wording."}, "questions": {"value": "1. Aside from the cosine similarity loss similar to REPA, would a simple MSE loss work for DUPA? In the case of REPA the cosine similarity loss may make sense. But is it the same case here, since the features of the same network are aligned?\n2. Could the authors provide a comparison between REPA and DUPA for text to image generation ? \n3. I'm rating the paper as borderline accept now, mainly because this approach seems scalable for text to image generation on the first look. ,but I'm willing to improve my rating if the authors can address the questions in a satisfactory way."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TsZQUOHBi6", "forum": "ALpn1nQj5R", "replyto": "ALpn1nQj5R", "signatures": ["ICLR.cc/2026/Conference/Submission19872/Reviewer_bsUa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19872/Reviewer_bsUa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762237047049, "cdate": 1762237047049, "tmdate": 1762932038637, "mdate": 1762932038637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DUPA (Dual-Path Condition Alignment), an unsupervised representation-alignment framework for training diffusion transformers. Building upon the decoupled diffusion transformer (DDT), DUPA introduces a condition alignment loss that aligns features of multiple noisy versions of the same image, effectively mimicking the self-supervised contrastive learning.\nExperimental results on ImageNet 256×256 demonstrate that DUPA outperforms the reproduced DDT baseline both in terms of training and sampling efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed DUPA not only achieves better FID than DDT (reproduced) with the same number of training steps but also reduces the number of denoising steps for sampling. \n- The design of DUPA is simple and easy to integrate."}, "weaknesses": {"value": "- The experiments are limited to ImageNet 256×256.\n- While the paper is clear in structure, the prose is dense and overly formal in places, which introduces unnecessary friction. For example, in lines 192–194:\n\t- there are too many dependent clauses. \"thereby generating... to be denoised\". \n\t- Unnecessary formality. \"conduct multiple samplings to get different...\" is verbose. \"We sample multiple noises..\" will be more natural. \n\t- Grammar issues: \"independent sampling times\" -> \"independent samples\""}, "questions": {"value": "- What's the main difference between DDT and the dual-path sampling in Table 4? Aren't they the same without DUPAlign loss? \n- Why does DUPA not integrate the architectural improvements of DDT? Would DUPA still retain its advantage if those were included?\n- How does DUPA work on higher resolution like ImageNet512?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9ezcP3pnvq", "forum": "ALpn1nQj5R", "replyto": "ALpn1nQj5R", "signatures": ["ICLR.cc/2026/Conference/Submission19872/Reviewer_tRWZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19872/Reviewer_tRWZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762244587997, "cdate": 1762244587997, "tmdate": 1762932038223, "mdate": 1762932038223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}