{"id": "5nVrMIl7vf", "number": 22855, "cdate": 1758336368373, "mdate": 1763720424375, "content": {"title": "Continual Fine-Tuning with Provably Accurate and Parameter-Free Task Retrieval", "abstract": "Continual fine-tuning aims to adapt a pre-trained backbone to new tasks sequentially while preserving performance on earlier tasks whose data are no longer available. Existing approaches fall into two categories which include input- and parameter-adaptation. Input-adaptation methods rely on retrieving the most relevant prompts at test time, but require continuously learning a retrieval function that is prone to forgetting. Parameter-adaptation methods instead use a fixed input embedding function to enable retrieval-free prediction and avoid forgetting, but sacrifice representation adaptability. To combine their best strengths, we propose a new parameter-adaptation method that enables adaptive use of input embeddings during test time with parameter-free retrieval. We derive task-retrieval error bounds for a clustering-based, parameter-free paradigm, providing theoretical guarantees that link low retrieval error to structural properties of task-specific representation clusters, revealing a fresh insight into how well-organized clustering structure will enable reliable retrieval. Motivated by this insight, our method is designed with two key components: (i) an adaptive module composition strategy that learns informative task-specific updates to preserve and complement prior knowledge, and (ii) a clustering-based retrieval mechanism that captures distinct representation signatures for each task, enabling adaptive representation use at test time. Extensive experiments show that these components work synergistically to improve retrieval and predictive performance under large shifts in task semantics.", "tldr": "", "keywords": ["Continual Fine-Tuning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e9dcfda111897183f0a323a0d92e7419804a5c3f.pdf", "supplementary_material": "/attachment/024d46ee693474cd7fec0eb8e637be21096d0f4c.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces PROTEUS, a continual fine-tuning framework that achieves provably accurate, parameter-free task retrieval. It bridges the gap between input-adaptation and parameter-adaptation methods by combining adaptability in test-time representation with retrieval-free prediction. The method derives theoretical bounds linking retrieval accuracy to the clustering structure of task-specific representations, ensuring low retrieval error through well-separated clusters. PROTEUS incorporates an adaptive fine-tuning mechanism that promotes orthogonal and informative updates across tasks, preserving prior knowledge while improving discrimination among task embeddings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a parameter-adaptation method that uniquely combines adaptive use of input embeddings with parameter-free task retrieval, bridging the gap between input-adaptation and parameter-adaptation paradigms.\n\n2. The authors provide a theoretical analysis that formally connects retrieval accuracy to structural properties of task-specific representation clusters. This foundation is concretely realized through the adaptive module composition strategy, which enforces orthogonality and selective knowledge transfer, ensuring both preservation and complementarity of prior knowledge.\n\n3. The clustering-based retrieval mechanism offers a clear, interpretable, and scalable solution for adaptive test-time inference, improving retrieval stability and reducing forgetting."}, "weaknesses": {"value": "1. As noted, the method generates a distinct LoRA update for each task, which can become memory-intensive when scaling to long sequences or high-dimensional backbones. Although the authors argue that LoRA units are lightweight, the cumulative storage of adaptation modules and their associated signature distributions may still pose practical challenges.\n\n2. The Gaussian clustering assumption is a somewhat strong theoretical simplification in the paper. In reality, feature distributions might be non-Gaussian or overlap between tasks, violating the conditions under which PROTEUS can provably bound the retrieval error."}, "questions": {"value": "1. The paper presents results primarily on 10-task sequences (e.g., Split CIFAR-100, ImageNet-R, and VTAB benchmarks), but continual fine-tuning is especially challenging when the number of tasks grows larger. It would be helpful if the authors could provide or discuss results for shorter (e.g., 5) and longer (e.g., 20 or 50) task sequences, to better understand how PROTEUS scales in performance, retrieval accuracy, and memory usage as the task sequence length increases.\n\n2. Could the authors consider introducing a fixed memory budget and examining how performance degrades under such constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vEDkVkjZDL", "forum": "5nVrMIl7vf", "replyto": "5nVrMIl7vf", "signatures": ["ICLR.cc/2026/Conference/Submission22855/Reviewer_JdHy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22855/Reviewer_JdHy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750464557, "cdate": 1761750464557, "tmdate": 1762942416054, "mdate": 1762942416054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of catastrophic forgetting in continual fine-tuning (CFT), specifically tackling the limitations of existing input-adaptation and parameter-adaptation methods. The authors argue that input-adaptation methods (e.g., prompt retrieval) suffer from \"retriever forgetting\" , while parameter-adaptation methods (e.g., RanPAC) lack representation adaptability at test time"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Clearly identifies the problem of retriever forgetting in prompt/parameter-pool based CFT methods and proposes a novel parameter-free retrieval mechanism as a direct solution. \n\n2. Theoretical Foundation: Provides a non-trivial theoretical analysis linking the retrieval error rate to geometric properties (cluster separation factor $\\delta$) of the learned representation signatures. This offers valuable insight and principled guidance for the algorithmic design. Theorem 3.4 and 3.5 are significant.\n\n3. Achieves SOTA performance on several challenging CL benchmarks, including those with large semantic gaps like VTAB , significantly outperforming both prompt-based and other LoRA-based CL methods."}, "weaknesses": {"value": "1. High System Complexity: The overall PROTEUS framework is quite intricate, involving adaptive LoRA with orthogonality constraints, non-parametric GMM fitting (DP-GMM) for potentially many components per task, storing these GMM parameters as signatures, computing likelihoods against all signatures during retrieval, and finally performing LDA prediction. This complexity raises concerns about implementation difficulty, computational overhead (especially GMM fitting and retrieval), and potential fragility.\n\n2. Insufficient Ablation of Design Choices: While key components (retrieval, adaptive FT) are ablated, the paper could benefit from more fine-grained ablations. For example, why is DP-GMM necessary? Would simpler clustering (e.g., k-means for centroids) or simpler signatures (e.g., just means) suffice? Is the combination of $l_1/l_2$ regularization 95and orthogonality 96 both necessary in the LoRA update? Justifying the necessity of each complex piece versus simpler alternatives would strengthen the paper.\n\n3. Reliance on Distributional Assumptions: The method relies on the assumption that task-specific embeddings $h_k(x)$ can be well-modeled by GMMs. The theoretical bounds (Assumption A.1) also depend on this. While GMMs are flexible, the quality of fit and the resulting cluster separation might degrade if the true embedding distributions are highly non-Gaussian or have complex structures, potentially impacting retrieval accuracy and invalidating the theoretical guarantees"}, "questions": {"value": "* Can the authors comment on the practical computational overhead of the DP-GMM fitting process during training and, more importantly, the signature matching process during inference, especially as the number of tasks $m$ grows large (e.g., hundreds or thousands)?\n\n* How robust is the GMM fitting and retrieval performance if the underlying embedding distributions deviate significantly from Gaussian mixtures? Have the authors explored alternative, potentially more robust, signature representations?\n\n* The adaptive LoRA update combines knowledge transfer ($S_\\tau$) and orthogonal new directions ($\\Delta\\omega_{k+1}^\\perp$). Could the authors provide ablation studies isolating the effects of just the orthogonality constraint versus just the knowledge transfer component (compared to the baseline without either)?\n\n* Theorem 3.5 suggests error can be made arbitrarily low if $\\delta$ is large enough. How large does $\\delta$ typically become in practice with the proposed adaptive fine-tuning? Does it continue to grow sufficiently as $m$ increases, or does it saturate, potentially limiting performance on very long sequences? (Figure 2 provides some insight for CIFAR-100, but more analysis would be helpful)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ceAVheihWQ", "forum": "5nVrMIl7vf", "replyto": "5nVrMIl7vf", "signatures": ["ICLR.cc/2026/Conference/Submission22855/Reviewer_as1a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22855/Reviewer_as1a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865689867, "cdate": 1761865689867, "tmdate": 1762942415812, "mdate": 1762942415812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PROTEUS, a method for parameter-based continual fine-tuning (CFT) that improves parameter-free retrieval. The method has two synergistic parts. First, an \"Adaptive Knowledge Composition\" strategy trains new LoRA adapters to be a selective combination of past adapters plus a new, orthogonal component for task-specific knowledge. Second, this orthogonal training creates highly separated representation clusters, which are captured by a \"Parameter-Free Retrieval\" mechanism (a DP-GMM). At test time, an input is assigned to the task with the highest likelihood—a non-learnable lookup. The paper provides theoretical bounds linking this cluster separation to low retrieval error and shows SOTA empirical results, demonstrating superior retrieval and predictive performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a novel application of Gaussian Mlixutre Models on CF. Improving prior state of the art in CFT with LoRA-based adapters. \n2. The paper provides a solid theoretical backing for its approach.\n3. The paper provides empirical evidence of the consistently higher performance of their method compared to previous state of the art across a diverse set of datasets."}, "weaknesses": {"value": "1. The paper claims to alleviate key issues with lack of representation adaptability and forgetting in retrieval-based methods. However, the latter is a prominent problem in prompt-based methods, not in parametric CFT methods (as PROTEUS) and previous parametric CFT approaches (RanPAC, InfLoRA, SD-LoRA) already address this with parameter-free retrieval methods."}, "questions": {"value": "1. \"Provably Accurate\" is a term consistently mentioned in the paper but not properly introduced. Adding reference when mentioned for the first time would strengthen the readability of the paper.\n2. I understand that at test time the input embedding for unseen samples has to go through each of the LoRA-augmented architectures in contrast to previous methods that used the first or last adaptation. How does the inference latency scale as we increase the number of tasks compared to the other parametric CFT baselines ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fjlmpvaYe4", "forum": "5nVrMIl7vf", "replyto": "5nVrMIl7vf", "signatures": ["ICLR.cc/2026/Conference/Submission22855/Reviewer_yqcf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22855/Reviewer_yqcf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905155584, "cdate": 1761905155584, "tmdate": 1762942415471, "mdate": 1762942415471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors examine the strengths and weaknesses of input-adaptation (prompt-based) and parameter-adaptation (LoRA-based) approaches for continual fine-tuning of pre-trained models. Input-adaptation enables flexible test-time behavior but suffers from retriever forgetting, while parameter-adaptation is stable but lacks adaptability. To bridge this gap, they propose PROTEUS, a parameter-free, provably accurate task-retrieval framework that learns orthogonal low-rank adapters and retrieves the most relevant one via statistical matching of task-specific signature distributions. Theoretical analysis provides error bounds linking retrieval accuracy to cluster separation, and experiments on benchmarks such as CIFAR-100, ImageNet-R/A, and VTAB-5T show that PROTEUS achieves state-of-the-art performance with improved adaptability and minimal forgetting."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The authors try to mathematically justify the proposed approach\n- The proposed method outperforms the baselines"}, "weaknesses": {"value": "- The paper is poorly written and difficult to follow, even after multiple readings. It would benefit from substantial restructuring, rewriting, and clearer explanations throughout.\n\n\t- The distinction between input-adaptation and parameter-adaptation is unclear. From the paper, it seems that input-adaptation corresponds to prompt-tuning and parameter-adaptation to LoRA-based fine-tuning, but these categories are only briefly mentioned in the abstract and never clearly defined or elaborated upon in the main text.\n\t- The authors state that input-adaptation relies on retrieving relevant prompts at test time, whereas parameter-adaptation uses a fixed embedding function, is retrieval-free, and avoids forgetting. However, it is not well explained why these approaches necessarily have these characteristics. It seems the intended distinction is that one requires identifying task-specific parameters at test time while the other does not. If that is the case, it would be clearer to use standard terminology such as task ID and task-specific parameters instead.\n\t- Moreover, the claim that only input-adaptation requires test-time task retrieval while parameter-adaptation does not is too rigid. Whether retrieval is required depends on how task-specific parameters are constructed, not inherently on whether the method is input- or parameter-based.\n\t- The paper’s terminology is often confusing and inconsistent, making it hard to understand the authors’ intent. For example, the phrase \"Solution Vision\" in the \"Existing Literature\" section is ambiguous. It appears to mean \"proposed solution.\" Similarly, terms like \"signature pattern,\" \"nearest signature,\" and \"signature distribution\" are introduced without clear definitions. Such vague or unconventional terminology significantly reduces the readability and clarity of the paper."}, "questions": {"value": "Refer to my comments in weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "G1Zr6akK3u", "forum": "5nVrMIl7vf", "replyto": "5nVrMIl7vf", "signatures": ["ICLR.cc/2026/Conference/Submission22855/Reviewer_4Hxf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22855/Reviewer_4Hxf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762845639137, "cdate": 1762845639137, "tmdate": 1762942415248, "mdate": 1762942415248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal Summary"}, "comment": {"value": "We thank the reviewers for their thoughtful comments and for recognizing the strengths of our work:\n\n- **Clarity of presentation:** reviewer **as1a**: “clearly identifies the problem”, reviewer **JdHy**: “clear, interpretable, and scalable solution”.\n\n- **Principled theoretical analysis:** reviewer **as1a**: “non-trivial theoretical analysis linking the retrieval error rate to geometric properties”,  reviewer **yqcf**: “provides a solid theoretical backing for its approach”, reviewer **JdHy**: “formally connects retrieval accuracy to structural properties of task-specific representation clusters”.\n\n- **Strong empirical performance**: reviewer **4Hxf**: “the proposed method outperforms the baselines”,  reviewer **as1a**: “Achieves SOTA performance on several challenging CL benchmarks”.\n\nAcross the four reviews, the main requests for clarification center on: \n\n- **Elaborating key terminologies**: input- and parameter-adaptation continual fine-tuning (CFT), signature pattern and distribution, and how these methods approach representation retrieval during test time (reviewer **4Hxf** and **yqcf**).\n\n- **Explaining design choices**: system complexity, additional ablation studies, and assumptions in the theoretical analysis (reviewer **as1a** and **JdHy**).\n\n- **Evaluating scalability**: stress test our approach in cases with longer task sequences (reviewer **yqcf** and **JdHy**).\n\nWe have addressed these points as follows:\n\n**Elaborating key terminologies.**\n\n- We explained **input adaptation, parameter adaptation**, as well as their distinction that leads to the conception of our framework. See our response to reviewer **4Hxf**.\n\n- We clarified the terminologies **signature pattern, signature distribution**, and **nearest signature** in our response to reviewer **4Hxf**.\n\n**Explaining design choices.**\n\n- We performed an additional experiment to demonstrate the non-triviality of integrating a retrieval mechanism into a LoRA-based CFT workflow in our response to reviewer **4Hxf**.\n\n- We discussed and ran additional ablation studies on our key design choices, including DP-GMM clustering, orthogonal LoRA updates, and parameter-free retrieval in our response to reviewer **as1a** and reviewer **JdHy**.\n\n**Evaluating scalability.**\n\nWe conducted additional experiments to demonstrate the scalability of our method (i.e., inference latency, computational overhead) in scenarios with significantly longer task sequences (in response to Reviewers **yqcf, as1a**). We further demonstrate the robust performance of our work in memory-constrained scenarios with long task sequences (in response to Reviewer **JdHy**). \n\n- Overall, our empirical results consistently show that PROTEUS remains robust and scalable with affordable cost under complex, real-world feature distributions, supporting the validity and practical strength of our design.  \n\n**Summary of our approach and contributions:**\nOur method, **PROTEUS**, is a continual fine-tuning approach that combines the strengths of two existing CFT paradigms: input adaptation and parameter adaptation. It overcomes the limitations of both paradigms by introducing the **first parameter-adaptation framework with parameter-free retrieval**, enabling adaptive test-time behavior without being prone to forgetting. \n\nThis is achieved with learning task-specific LoRA adapters via solving Eq. (11), computing their induced input embeddings, and partitioning these embeddings into clusters. The representations of these clusters form the basis for our parameter-free retrieval mechanism and its theoretical guarantee on high retrieval accuracy during test time. This delivers a significant theoretical contribution as we are the first to provide provable retrieval-accuracy guarantee for parameter-adaptation in continual fine-tuning with explicit retrieval accuracy bound (see Theorems 3.4 and 3.5). Our rigorous analysis thus offers principled guidance to design a novel parameter-free retrieval mechanism that remains robust and highly and consistently effective in numerous experiment scenarios with large semantic drift across tasks, long task sequences, and even memory-constrained scenarios.\n\nWe thank the reviewers once again for their careful evaluations and look forward to engaging in discussion. We have also uploaded a revised manuscript."}}, "id": "ZgNRxZXYDc", "forum": "5nVrMIl7vf", "replyto": "5nVrMIl7vf", "signatures": ["ICLR.cc/2026/Conference/Submission22855/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22855/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission22855/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763722927280, "cdate": 1763722927280, "tmdate": 1763722927280, "mdate": 1763722927280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}