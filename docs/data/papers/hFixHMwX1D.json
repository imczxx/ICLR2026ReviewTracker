{"id": "hFixHMwX1D", "number": 22712, "cdate": 1758334647373, "mdate": 1759896851097, "content": {"title": "BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs", "abstract": "Coreference resolution in biomedical texts presents unique challenges due to complex domain-specific terminology, high ambiguity in mention forms, and long-distance dependencies between coreferring expressions. In this work, we present a comprehensive evaluation of generative large language models (LLMs) for coreference resolution in the biomedical domain. Using the CRAFT corpus as our benchmark, we assess the LLMs' performance with four prompting experiments that vary in their use of local, contextual enrichment, and domain-specific cues such as abbreviations and entity dictionaries. We benchmark these approaches against a discriminative span-based encoder, SpanBERT, to compare the efficacy of generative versus discriminative methods. Our results demonstrate that while LLMs exhibit strong surface-level coreference capabilities, especially when supplemented with domain-grounding prompts, their performance remains sensitive to long-range context and mentions ambiguity. Notably, the LLaMA 8B and 17B models show superior precision and F1 scores under entity-augmented prompting, highlighting the potential of lightweight prompt engineering for enhancing LLM utility in biomedical NLP tasks.", "tldr": "", "keywords": ["Coreference resolution", "Large language models (LLMs)", "Benchmarking and evaluation", "Prompt engineering", "CRAFT corpus"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e2b0186ace8fa8a5d68fb1e28566e9904215d318.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a benchmarking study of large language models (LLMs) for biomedical coreference resolution. Using the CRAFT corpus, the authors evaluate many LLaMA variants under four prompting strategies: local-only, reference-context, abbreviation-aware, and entity-aware, and compare them with a SpanBERT baseline. The experiments aim to understand how prompt design and model scale affect LLM performance in resolving coreference links across biomedical texts. Results indicate that smaller models (e.g., LLaMA-8B and 17B) sometimes outperform larger ones, and that domain-specific auxiliary inputs (abbreviation/entity lists) can slightly improve recall and F1 scores."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1.\tProvides a comprehensive empirical benchmark comparing generative and discriminative approaches for biomedical coreference resolution.\n2.\tThe evaluation is thorough and well-documented, covering multiple prompt types and coreference categories.\n3.\tThe use of the CRAFT corpus is appropriate for biomedical text evaluation and ensures reproducibility."}, "weaknesses": {"value": "1.\tThe paper lacks methodological innovation. It does not propose new models, training strategies, or prompting frameworks; it mainly reports comparative results.\n2.\tThe claimed challenges in biomedical coreference (domain-specific terminology, long-range dependency, ambiguity) are described but not explicitly addressed with any novel solution.\n3.\tThe prompt designs are rather straightforward and incremental (local vs. entity-aware), offering limited scientific insight beyond descriptive benchmarking.\n4.\tFigures and visualizations are poorly designed and do not effectively convey the key findings.\n5.\tThe analysis is shallow: while results are reported in detail, there is minimal discussion on why certain trends occur or what they imply for future LLM-based biomedical NLP research."}, "questions": {"value": "1.\tBeyond benchmarking, what is the main research contribution of this work? How does it advance understanding or methodology in coreference resolution?\n2.\tCan the authors clarify whether the prompting templates or evaluation procedures introduce biases or artifacts (e.g., by segmenting text into 200-word chunks)?\n3.\tGiven that the motivation centers on long-range dependencies, why not explore context-extension or retrieval-based prompting methods to explicitly address this challenge?\n4.\tHow do these results compare to fine-tuned domain-specific models or hybrid approaches (e.g., combining extraction and generation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RqQ99zbRnV", "forum": "hFixHMwX1D", "replyto": "hFixHMwX1D", "signatures": ["ICLR.cc/2026/Conference/Submission22712/Reviewer_ZZKj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22712/Reviewer_ZZKj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887869730, "cdate": 1761887869730, "tmdate": 1762942353185, "mdate": 1762942353185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary:\nAuthors explore four different prompting methods and asses their performance on biomedical coreference resolution."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n- Cleanly written paper. Easy to read and understand."}, "weaknesses": {"value": "Weaknesses\n- Incomplete Metrics\n  - Only coreference-level precision/recall reported; mention-level metrics missing.\n  - SpanBERT baseline lacks precision/recall breakdown.\n  - If these are already included, clear definitions are needed.\n\n- Insufficient Dataset Statistics\n  - No details on average document length, chunk size, or total chunks per document.\n  - Missing counts of mentions and coreference links detected by each model.\n- Limited Dataset Coverage\n  - Evaluation restricted to CRAFT; other biomedical datasets (e.g., MedMentions, BioNLP Coref) not tested.\n  - No cross-domain validation to assess robustness.\n- Weak Baselines and Comparisons\n  - Uses outdated SpanBERT coreference model. Many other stronger coreference models have come out after that.\n  - 4 methods have been compared msotly focussed on smaller paragraphs. Why not use larger context windows given this is useful in coreference.\n\n- Impact of Coreference Distance\n  - CLaims made under this not justified. Lot of coreference links may span across 500 token boundaries.\n\n- Narrow Contribution and Analysis\n  - Main finding (“LLMs > SpanBERT on one dataset”) is limited in novelty. Also this was already shown in https://aclanthology.org/2024.lrec-main.145.pdf\n  - Little explanation for smaller models outperforming larger ones."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oEvQHqxbrb", "forum": "hFixHMwX1D", "replyto": "hFixHMwX1D", "signatures": ["ICLR.cc/2026/Conference/Submission22712/Reviewer_NNwE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22712/Reviewer_NNwE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945968374, "cdate": 1761945968374, "tmdate": 1762942352792, "mdate": 1762942352792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This research benchmarks recent LLMs performance on zero-shot co-reference resolution in the biomedical domain. The focus of the paper is revisiting a natural language processing task that posed significant challenges to public-weight models prior to the LLM expansion around the time of GPT3.5's launch. They explore several settings of co-reference resolution in which models are provided different contexts that contain different levels of information. The four experimental settings are as follows: Local-only resolution (baseline), resolution with local and reference context, abbreviation-aware resolution using LLM-extracted dictionaries, and entity-aware resolution using LLM-extracted dictionaries. The paper compares three models from the LLama family, consisting of 8B, 17B and 70B parameters with SpanBert (340 million parameters). The authors find that SpanBERT achieves zero-shot performance of 13.22% (although it is ambiguous which setting this refers to), while the 8B, 17B and 70B Llama models achieve mixed results with the 8B/17B models outperforming the 70B models in all settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper takes on an interesting idea in benchmarking zero-shot LLMs on NLP tasks that previously required bespoke finetuning. While not wholly original, the idea has merit in showing capabilities of LLMs on domain-specific tasks that are not used to traditionally benchmark performance (ex. logic/mathematical reasoning). The paper very clearly defines each of the four experimental settings and presents the model performances in a digestible format."}, "weaknesses": {"value": "The paper requires more justification of each of the task settings. A one-sentence justification for the purpose of each of the experimental settings that specifies what aspect of co-reference resolution each setting is evaluating would help to improve clarity. \n\nIn addition, the authors frame the contributions of the paper as benchmarking LLMs against previous approaches by comparing several sizes of Llama against a 340-million parameter model that is not provided training. It is unclear what the comparison attempts to show, as it seems intuitive that more recent models that are orders of magnitude larger would outperform a 340-million parameter model in a zero-shot setting. I believe this work would greatly benefit from including other model families, especially those that regularly outperform Llama models on other benchmarks (ex. Qwen), as well as additional datasets to provide more context to the results.\n\nThe data analysis section is a little unclear and would benefit from being fleshed out. The authors claim that the unintuitively poor performance of the 70B Llama model is \"One likely explanation is that smaller models generalize more conservatively and make fewer overconfident errors, whereas larger models despite stronger generative capacity may be more susceptible to prompt misalignment and semantic overreach.\" This needs to either be justified experimentally or via a citation at the very least, as it is surprising that the 70B model performs so poorly.\n\nIt is unclear why the SpanBERT model only has a single reported number."}, "questions": {"value": "It would be good to explain the purpose of each of the task settings and provide a stronger justification for the lack of performance of the 70B parameter model via data analysis.\n\nIt would also be good to get the SpanBERT performances for the other tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LZhLY0VVjF", "forum": "hFixHMwX1D", "replyto": "hFixHMwX1D", "signatures": ["ICLR.cc/2026/Conference/Submission22712/Reviewer_fMxq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22712/Reviewer_fMxq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975494658, "cdate": 1761975494658, "tmdate": 1762942352321, "mdate": 1762942352321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive evaluation of generative large language models (LLMs) for coreference resolution in biomedical texts. The authors use the CRAFT corpus as their primary benchmark and evaluate LLMs through four distinct prompting strategies that incorporate different types of contextual information: local context, contextual enrichment, domain-specific cues (abbreviations), and entity dictionaries. The study compares these generative approaches against SpanBERT, a discriminative span-based encoder, to assess the relative merits of generative versus discriminative methods for biomedical coreference resolution."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Timely and Relevant Research Question: The evaluation of LLMs on biomedical coreference resolution addresses a critical gap as these models become increasingly important in healthcare and life sciences applications.\nComprehensive Prompting Strategy Evaluation: The four-tier evaluation framework (local context, contextual enrichment, domain cues, entity augmentation) provides systematic insights into how different types of information affect LLM performance in specialized domains.\nPractical Utility: The finding that entity-augmented prompting significantly improves performance offers immediately applicable insights for practitioners, potentially improving real-world biomedical NLP systems.\nDomain-Specific Focus: The paper demonstrates clear understanding of biomedical text challenges, including complex terminology, high ambiguity, and long-distance coreference relationships.\nModel Diversity: Testing both LLaMA 8B and 17B variants provides insights into how model scale affects domain-specific performance.\nClear Performance Analysis: The identification of specific LLM limitations (long-range dependencies, mention ambiguity) provides valuable insights for future research directions."}, "weaknesses": {"value": "w1: Limited Evaluation Scope\nThe paper's evaluation is restricted to a single corpus (CRAFT), which significantly undermines the generalizability of findings. While CRAFT provides rich annotations for 67 full-text biomedical articles, this narrow scope fails to capture the diversity of biomedical text types, writing styles, and domain-specific challenges across different subfields (clinical notes, molecular biology, pharmacology). The lack of cross-corpus validation makes it impossible to determine whether the proposed prompting strategies are robust across different biomedical contexts or merely optimized for CRAFT's specific characteristics. This limitation severely restricts the practical applicability of the research findings and prevents meaningful comparison with other studies that may use different evaluation datasets.\n\nw2: Insufficient Baseline Coverage\nThe comparison framework is inadequately narrow, relying solely on SpanBERT as a discriminative baseline. This approach overlooks critical comparisons with recent state-of-the-art coreference resolution systems, including newer transformer-based models, domain-adapted variants like BioBERT and ClinicalBERT, and hybrid architectures that combine neural and rule-based approaches. The absence of these comparisons makes it impossible to accurately position LLM performance relative to current best practices in biomedical coreference resolution. Additionally, SpanBERT's reported F1 score of 0.1322 appears unusually low, raising questions about experimental setup validity and making the LLM improvements potentially misleading without proper context.\n\nw3: Shallow Error Analysis\nWhile the paper identifies that LLMs struggle with long-range dependencies and mention ambiguity, it provides insufficient systematic analysis of failure modes. The absence of detailed error categorization, entity-type-specific performance breakdowns, and concrete examples of successful versus failed coreference resolution cases limits the actionable insights for model improvement. Without understanding which specific mention types, entity categories, or linguistic phenomena cause failures, practitioners cannot effectively apply or adapt the proposed approaches. The lack of error propagation analysis in coreference chains further reduces the practical value of the findings.\n\nw4: Missing Technical Rigor\nThe paper lacks essential statistical validation and methodological transparency required for rigorous scientific evaluation. Critical missing elements include statistical significance testing of reported improvements, confidence intervals or variance analysis, detailed hyperparameter specifications, and systematic prompt design validation. The evaluation protocol using partial character overlap (≥2 characters) for mention matching may be too lenient and needs justification. Without these technical details, the results cannot be properly validated, reproduced, or trusted by the research community, significantly undermining the paper's scientific contribution.\n\nw5: Limited Model Coverage\nThe evaluation focuses exclusively on LLaMA variants (8B, 17B, 70B), missing comparison with other prominent LLMs that dominate current NLP applications. The absence of GPT models, Claude, and other leading systems prevents comprehensive assessment of LLM capabilities for biomedical coreference resolution. This narrow model selection may lead to biased conclusions about LLM performance that don't generalize to the broader ecosystem. Additionally, testing only autoregressive transformers ignores architectural diversity and specialized models that might be better suited for structured tasks like coreference resolution, limiting the study's relevance to real-world deployment scenarios."}, "questions": {"value": "Evaluation Methodology:\nWhat specific evaluation metrics were used beyond precision and F1? Were standard coreference metrics reported?\nHow were coreference chains evaluated - link-based or mention-based scoring?\nWere statistical significance tests performed on the reported improvements?\n\nExperimental Design:\nHow were the four prompting strategies designed and validated? Was there human evaluation of prompt quality?\nWhat was the process for selecting entity dictionaries and abbreviation lists? How comprehensive were these resources?\nWere hyperparameters optimized for each model, or were default settings used throughout?\n\nError Analysis:\nCan you provide specific examples of coreference relationships that LLMs handle well vs. poorly?\nHow does performance vary across different biomedical entity types (genes, proteins, diseases, etc.)?\nWhat is the distribution of errors across different coreference chain lengths?\n\nGeneralizability:\nHave you tested these findings on other biomedical corpora or clinical texts?\nHow do results vary across different biomedical subdomains (molecular biology, clinical medicine, etc.)?\nWould these prompting strategies transfer to other specialized domains?\n\nComputational Analysis:\nWhat are the computational costs and inference times for different models and prompting strategies?\nHow do memory requirements scale with document length and context size?\nAre there practical limitations for processing long biomedical documents?\n\nModel Comparison:\nWhy were other prominent LLMs (GPT models, Claude, etc.) not included in the evaluation?\nHow does performance scale with model size beyond the 8B/17B comparison?\nHave the benchmark tested instruction-tuned or chat-optimized variants?\n\nDataset Considerations:\nWhat are the specific characteristics of the CRAFT corpus that might affect generalizability?\nHow does annotation quality and inter-annotator agreement affect the reliability of your results?\nAre there known biases in CRAFT that might influence LLM performance assessment?\n\nFuture Directions:\nWhat specific improvements would you recommend for better LLM performance on biomedical coreference?\nHow might fine-tuning or domain adaptation compare to prompt engineering approaches?\nWhat role could few-shot learning play in this task?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)", "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"]}, "details_of_ethics_concerns": {"value": "This paper addresses a timely and practically important research question by conducting the first systematic evaluation of large language models on biomedical coreference resolution, which represents a valuable contribution to the intersection of biomedical NLP and LLM evaluation. The empirical insights regarding the effectiveness of different prompting strategies, particularly the superior performance of entity-augmented approaches with LLaMA models, offer immediate practical value to practitioners working on biomedical text processing systems. The systematic four-tier evaluation framework comparing local context, contextual enrichment, domain-specific cues, and entity dictionaries provides a methodical approach to understanding how different types of information affect LLM performance in specialized domains, while the comparison with SpanBERT offers useful perspective on generative versus discriminative approaches.\n\nHowever, despite these contributions, several fundamental limitations prevent the work from meeting the acceptance threshold for a top-tier venue like ICLR. The most critical concern is the severely limited evaluation scope, with the study confined to a single corpus (CRAFT) that cannot adequately demonstrate the generalizability of findings across the diverse landscape of biomedical texts, including clinical notes, molecular biology literature, and other specialized subdomains. This narrow scope is compounded by insufficient technical rigor, as the paper lacks essential statistical significance testing, confidence interval analysis, and detailed methodological transparency that would allow for proper validation and reproduction of results. The baseline comparison framework is inadequately narrow, relying solely on SpanBERT while omitting comparisons with recent state-of-the-art coreference resolution systems, domain-adapted models like BioBERT, and other neural architectures that would provide proper context for evaluating LLM performance. Furthermore, the error analysis remains superficial, identifying general limitations like sensitivity to long-range dependencies without providing the systematic failure mode analysis, concrete examples, or entity-type-specific breakdowns that would offer actionable insights for model improvement.\n\nThe evaluation also suffers from limited model coverage, focusing exclusively on LLaMA variants while excluding other prominent LLMs such as GPT models and Claude that dominate current NLP applications, potentially leading to biased conclusions about LLM capabilities that may not generalize to the broader ecosystem. These methodological gaps, combined with missing computational efficiency analysis and insufficient detail about prompt design and validation processes, significantly undermine the work's scientific rigor and limit its impact. While the research tackles an important problem and provides useful preliminary insights into prompt engineering for biomedical coreference resolution, the limited scope, shallow technical analysis, and methodological shortcomings fall short of the comprehensive evaluation and rigorous methodology expected at ICLR. The work would require substantial expansion across multiple corpora, comprehensive baseline comparisons, systematic error analysis, and enhanced technical rigor before it could meet publication standards for a venue of this caliber."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mj71L3mPvG", "forum": "hFixHMwX1D", "replyto": "hFixHMwX1D", "signatures": ["ICLR.cc/2026/Conference/Submission22712/Reviewer_3ytB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22712/Reviewer_3ytB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134048438, "cdate": 1762134048438, "tmdate": 1762942351845, "mdate": 1762942351845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}