{"id": "ddveHyUe5U", "number": 11814, "cdate": 1758204011464, "mdate": 1763110699624, "content": {"title": "Stay Unique, Stay Efficient: Preserving Model Personality in Multi-Task Merging", "abstract": "Model merging has emerged as a promising approach for enabling multi-task capabilities without additional training.\nHowever, existing methods often suffer from substantial performance degradation compared to individual models, even on similar tasks, highlighting the importance of preserving task-specific information.\nThis paper introduces an approximation-based personalized merging method, Decomposition, Thresholding, and Scaling (DTS), which retains task-specific information with minimal storage overhead.\nDTS first performs singular value decomposition on the task-specific information and preserves only a small subset of singular values and vectors.\nIt then applies a novel thresholding strategy to group the elements within each singular vector and computes a scaling factor for each group.\nTo further support generalization to unseen tasks, this paper extends DTS with a variant that leverages the semantic similarity of task characteristics to merge task-specific information in a data-free manner.\nExtensive experiments demonstrate that DTS consistently outperforms state-of-the-art baselines, delivering superior performance with just 1% extra storage per task.\nFurthermore, experiments on unseen tasks show that the DTS variant achieves significantly better generalization performance.\nOur code is available in the supplementary materials.", "tldr": "", "keywords": ["Model Merging", "Multi-task Learning", "Efficiency", "Personalization"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7053c72aba99cc9c355bc29ff1014c3942dee86a.pdf", "supplementary_material": "/attachment/387e7911dfd362467a635b2b902bfe999a7319eb.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies task-vector-based model merging, providing a multi-task model by aggregating the differences between task-specific finetunings and their common base model (task vectors). In particular, the work proposes DTS, a 3-step pipeline consisting of i) “Decomposition”, a layer-wise low-rank approximation of the differences, ii) “Thresholding”, a bit-masking step followed by a partitioning in 4 groups to reduce the storage requirements, and iii) a “Scaling” step that ensures the new computed singular vectors are re-scaled to the original unitary norm. The approach is tested over ViT-B/16 and ViT-L/14 on a common 8-task vision benchmark, and using RoBERTa and GPT2 on the GLUE benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The singular-vector-level thresholding is methodologically novel and might be the main strength of the method. The 1% extra storage requirement is impressive, and is obtained by leveraging both low-rank approximation and quantization.\n- The method is data-free: there is no training, tuning or test-time adaptation involved. While these are often observed in the related literature, I strongly advise against using task-specific data to prevent limiting the practical adoption of the method."}, "weaknesses": {"value": "- The method requires task information at inference time which is not obtained by routing. This makes the comparison with standard merging techniques unfair, as the model is expecting not only the classification head to be known, but also which task-specific parameters to use. While this assumption is present in some of the literature, it severely restricts the usefulness of the approach, and is often unrealistic in practice.\n- Lack of motivation. The performance degradation on similar tasks is not a surprising phenomenon per se, and it is not clear to me why it would motivate the proposed approach. The choice of SVHN as a dataset over which to compute the similarity wrt the other tasks is also arbitrary.\n- Outdated baselines. The paper does not compare against the more recent structured merging methods such as Iso-C [1], TSV [2] and KNoTs [4].\n- Lack of novelty. The paper presents limited methodological novelty: its first and main step, taking the SVD of a task vector, is already widely explored in the field [1, 2, 3, 4, 5]; these papers are also not properly cited and discussed. The subsequent steps seem incremental and should be compared with existing literature: while different from a methological point of view, the thresholding step has a similar goal as the quantization performed in [6], and it is not clear whether it should be preferred to this one. Considering the “difference vector” with respect to the merged model seems arbitrary and provides very marginal gains (ranging from +0.04% to 0.08%). Overall, the paper presents no novel insights.\n- Incomplete empirical evidence. The work does not consider the commonly used 14- and 20-tasks benchmark proposed in [7] and commonly used in most subsequent works [1, 2, 3, 4], with the considered 8-tasks benchmark being fairly saturated. Also lacking the results for the ViT-B/16 baseline.\n\nWhile lacking a solid motivation is not by itself reason enough to reject the paper, the overall lack of novelty and shaky experimental evidence makes the work incremental and its contribution uncertain. Having carefully considered both the strengths and weaknesses, I am inclined to reject the paper at this time. \n\n[1] Gargiulo, Antonio Andrea, et al. \"Task singular vectors: Reducing task interference in model merging.\" *Proceedings of the Computer Vision and Pattern Recognition Conference*. 2025.\n\n[2] Marczak, Daniel, et al. \"No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces.\" *Forty-second International Conference on Machine Learning*.\n\n[3] Choi, Jiho, et al. \"Revisiting weight averaging for model merging.\" *arXiv preprint arXiv:2412.12153* (2024).\n\n[4] Stoica, George, et al. \"Model merging with SVD to tie the Knots.\" *The Thirteenth International Conference on Learning Representations*.\n\n[5] Lu, Zhenyi, et al. \"Twin-merging: Dynamic integration of modular expertise in model merging.\" *Advances in Neural Information Processing Systems* 37 (2024): 78905-78935.\n\n[6] Kim, Youngeun, et al. \"Task Vector Quantization for Memory-Efficient Model Merging.\" *CoRR* (2025).\n\n[7] Wang, Ke, et al. \"Localizing task information for improved model merging and compression.\" *Proceedings of the 41st International Conference on Machine Learning*. 2024."}, "questions": {"value": "- The method requires reconstructing the merged model on the fly for each task. Assuming that each sample has a different task, what is the required added latency?\n- It is not entirely clear to me how to derive the 1% extra storage cost. Is this dependent on the model size? depth? number of tasks?\n- The 4 in the 4-group quantization seems arbitrary. How was this chosen? were 2 and 8 groups tried?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hrngAzyG9Z", "forum": "ddveHyUe5U", "replyto": "ddveHyUe5U", "signatures": ["ICLR.cc/2026/Conference/Submission11814/Reviewer_irAB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11814/Reviewer_irAB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761308867597, "cdate": 1761308867597, "tmdate": 1762922836159, "mdate": 1762922836159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "AbkWFxcNMb", "forum": "ddveHyUe5U", "replyto": "ddveHyUe5U", "signatures": ["ICLR.cc/2026/Conference/Submission11814/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11814/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763110698403, "cdate": 1763110698403, "tmdate": 1763110698403, "mdate": 1763110698403, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a DTS method for multi-task model merging. The proposed approach performs low-rank decomposition on task vectors or difference vectors and defines a mask matrix using a threshold. Finally, it reconstructs the expert model parameters through the low-rank components and the mask matrix. However, this approach seems inconsistent with the original goal of model merging, which is to enable a single model to perform all tasks without requiring task identifiers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes DTS for multi-task model merging.\n- The method is validated across multiple architectures and domains.\n- The paper is well-structured, and the method is easy to follow."}, "weaknesses": {"value": "- The proposed method restores expert models by adding low-rank and sparse components to the pretrained model. However, this approach seems somewhat redundant-one could directly perform low-rank decomposition or sparsification on the expert model itself and store those parameters, eliminating the need to retain the pretrained model, thus saving storage. What are the advantages of the proposed approach compared with directly saving compact expert models?\n- In Equation (7), the method only utilizes information from the pretrained model and a single expert model, without leveraging cross-task knowledge transfer, which seems inconsistent with the fundamental goals of multi-task learning or model merging.\n- The proposed method requires the task ID during inference, which significantly limits its practical usability."}, "questions": {"value": "Refer to the Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ndp0tfDQYE", "forum": "ddveHyUe5U", "replyto": "ddveHyUe5U", "signatures": ["ICLR.cc/2026/Conference/Submission11814/Reviewer_zWHb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11814/Reviewer_zWHb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730759065, "cdate": 1761730759065, "tmdate": 1762922835413, "mdate": 1762922835413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Decomposition, Thresholding, and Scaling (DTS), a personalized model merging method designed to improve storage efficiency. The DTS method first applies Singular Value Decomposition (SVD) to a task's parameter matrix. Then, it further compresses by separating left or right singular matrix (U/V) into four groups of weights: large-positive, small-positive, large-negative, and small-negative, based on signs and the median values. Each of these four groups is then represented by a binary mask and a scaling factor. This allows for a reconstruction of the task-specific vector from minimal storage (binary masks + scalars). Experiments across vision and NLP benchmarks show that DTS achieves a significantly better performance/storage trade-off than the baselines"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Offer better trade-off between performance and storage efficiency than the baselines.\n2. The presentation of the methods is clear and easy to follow.\n3. Introduce the difference vector which make the method practical for the case when the pretrained is inaccessible.\n4. Investigation on unseen tasks is appreciated especially for real world uses."}, "weaknesses": {"value": "1. The authors do not evaluate computational overhead at inference time. Basic merging methods produce a single model, while personalized methods like DTS require a reconstruction of each task. Therefore, DTS should have higher computational overhead than basic merging and it would be valuable to evaluate this aspect too.\n2. Some missing details (see questions)"}, "questions": {"value": "1. How is the decomposition (Eq. 2) applied to the layers that have more than 2 dimensions such as a convolutional layer?\n2. In an ideal case, the Eq. 8 would be equivalent to the weight averaging of the finetuned models. But is it possible to use other basic model merging at this step such as TIES-Merging or DARE?\n3. The thresholding strategy splits weights into 4 groups (pos/neg, large/small). Is it possible to apply this recursively? For example, could the $g_m^+$ group be further split into two sub-groups (e.g., $g_m^{++}$ and $g_m^{+-}$) to create 8 total groups? Would this provide a better trade-off point, or do the storage costs (more masks and scaling factors) grow too quickly for the marginal performance gain?\n4. The details regarding the text encoder for the unseen task setup seems to be missing. How sensitive is the generalization performance (Section 4.2) to the choice of the text encoder?\n5. The difference vector $d_n = \\theta_n - \\theta_m$ (Appendix D.5) is proposed as a solution for when the pretrained weight is inaccessible, using $\\theta_m$ as an average of fine-tuned models. This seems to allow us to use the fine-tuned models that derive from different pretrained weights. Would this be possible?\n\n\nWriting\n1. The $\\odot$ symbol in Equation 5 is never explicitly defined. While it can be inferred as element-wise multiplication (Hadamard product), this should be stated for clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z4Lxxm5LGQ", "forum": "ddveHyUe5U", "replyto": "ddveHyUe5U", "signatures": ["ICLR.cc/2026/Conference/Submission11814/Reviewer_VamT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11814/Reviewer_VamT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971996563, "cdate": 1761971996563, "tmdate": 1762922834348, "mdate": 1762922834348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the performance degradation commonly observed in model merging, especially when combining models fine-tuned on similar tasks. The authors propose DTS (Decomposition, Thresholding, and Scaling)—a personalized, approximation-based merging method designed to preserve task-specific information with minimal storage overhead. DTS performs singular value decomposition (SVD) on task-specific information (e.g., task or difference vectors), retains top-r singular components, and applies a novel thresholding and scaling strategy to compress and reconstruct parameters efficiently. A data-free variant further generalizes to unseen tasks by weighting stored components according to semantic similarity between task characteristics. Experiments across visual, language understanding, and generation tasks demonstrate that DTS outperforms strong baselines such as T-Switch and EMR-Merging while requiring ≈ 1% additional storage per task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1)Comprehensive empirical validation: The paper conducts large-scale experiments across multiple domains and backbones, including ViT-B/32, RoBERTa, GPT-2, and Qwen-14B (Sec. 4.1). Results consistently show that DTS outperforms both basic and personalized baselines while maintaining very low storage overhead. For example, DTS-D achieves 90.40% average accuracy on vision tasks with only 3.68% extra memory (Table 1), while achieving near-individual model performance on RoBERTa with 0.88% overhead (Table 2). This extensive coverage and consistent trend strongly validate the proposed method’s generality and robustness.\n\n2)Novel and effective compression mechanism: The decomposition thresholding scaling pipeline (Sec. 3.2; Fig. 3) is a distinctive and technically sound design that effectively preserves important task information under extreme compression. The use of four-value thresholding with learned scaling factors improves upon prior binary masking approaches. The ablation results (Table 12) show that removing scaling or thresholding leads to 2–5% accuracy drops, demonstrating that each step contributes meaningfully to preserving task personality.\n\n3)Clarity, structure, and reproducibility: The paper is clearly written, logically structured, and easy to follow. The motivation in Fig. 1 and Sec. 1 is convincing, the method section (Sec. 3) is self-contained, and reproducibility is explicitly discussed (Sec. 7). The inclusion of code in the supplementary material further supports transparency and practical usability."}, "weaknesses": {"value": "1)Lack of theoretical grounding for the method: Although DTS shows strong empirical performance, there is no theoretical analysis of its approximation quality or why the proposed thresholding and scaling preserve task identity. The manuscript does not provide error bounds for SVD truncation (Eq. 2) or theoretical justification for using four quantization groups (Sec. 3.2).\n\n2)Inference cost ignored: Reconstructing $\\hat{\\theta}_u$ at every forward pass (Eq. (7)) adds matrix multiplications whose FLOPs and latency are neither analysed nor benchmarked (Sec. 3.2).\n\n3)Hyper-parameter selection opaque: The claim “r is adaptively adjusted to keep storage <1 %” lacks an algorithm.\n\n4)Incomplete fairness and statistical reporting: the experiments appear single-run with no mention of variance or confidence intervals."}, "questions": {"value": "1) Figure 1 is drawn unclearly, and the overall explanation is ambiguous."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T1TKuQyYya", "forum": "ddveHyUe5U", "replyto": "ddveHyUe5U", "signatures": ["ICLR.cc/2026/Conference/Submission11814/Reviewer_BKB6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11814/Reviewer_BKB6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983687485, "cdate": 1761983687485, "tmdate": 1762922833896, "mdate": 1762922833896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}