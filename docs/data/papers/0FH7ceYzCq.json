{"id": "0FH7ceYzCq", "number": 4298, "cdate": 1757658463010, "mdate": 1763715676352, "content": {"title": "Sequence-agnostic Continual Multi-modal Clustering", "abstract": "Continual multi-modal clustering (CMC) aims to address the challenges posed by the continuous arrival of multi-modal data streams, enabling models to progressively update cluster assignments while avoiding catastrophic forgetting.\nCMC closely aligns with the requirements of real-world scenarios and has attracted significant attention from researchers.\nHowever, existing CMC methods face two limitations.\n(1) They fail to reliably model the relationship between historical and new information, leading to redundancy in the shared representation and weakened discriminative power of clustering.\n(2) They are highly sensitive to modality sequence, as early high-quality modalities are gradually forgotten, making the results dependent on the input order.\nTo address these limitations, we propose a novel Sequence-agnostic Continual Multi-modal Clustering (SCMC) method that achieves reliable continual learning and is insensitive to the modality arrival sequence. Specifically, SCMC employs a residual fusion network to suppress the update bias introduced by the newly arrived modalities. It then leverages a cross-temporal knowledge collaboration mechanism to bidirectionally filter information between the historical information and the new modalities, thereby maximizing the preservation of task-relevant information and ensuring reliable continual learning.\nTo eliminate the high sequence sensitivity, we design a sequence-agnostic anti-forgetting strategy, which aligns the current features and cluster distribution with the previous step through cross-temporal consistency transfer, and then prioritizes retaining high-value modality information based on modality importance scores.\nExtensive experiments demonstrate that SCMC outperforms existing SOTA methods, exhibiting sequence insensitivity and strong anti-forgetting capabilities. To the best of our knowledge, SCMC is the first approach to explicitly address the sequence sensitivity problem in CMC.", "tldr": "We propose a novel sequence-agnostic continual multi-modal clustering method.", "keywords": ["Multi-modal Clustering", "Continual Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/209c2ab5e805c63521a005458e58d3aec204732d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a sequence-agnostic method for continuous multi-modal clustering, Sequence-Agnostic Continual Multi-Modal Clustering (SCMC). It aims to address two core issues in existing continuous multi-modal clustering: the unreliable fusion of historical and new modal information, and the strong dependence of clustering performance on the order of modal input. The paper first analyzes the shortcomings of existing methods in fusing noise and forgetting high-quality modalities, and constructs a reliable continuous information propagation framework. Through a residual fusion network and a cross-temporal knowledge collaboration mechanism, it achieves bidirectional information filtering between new and old modalities, thereby enhancing complementarity and suppressing redundancy. Subsequently, the authors designed a sequence-agnostic anti-forgetting strategy, including cross-temporal consistency transfer and quality-aware history integration, enabling the model to retain the discriminative knowledge of early high-value modalities when new modalities arrive."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.This method is highly innovative and original. The paper is rigorously structured, logically coherent, and systematically comprehensive, with well-reasoned theoretical and experimental demonstrations. It represents a significant contribution to the research of multi-modal clustering.\n\n2.SCMC explicitly solved the problem of \"modal sequence sensitivity\" in continuous multi-modal clustering for the first time and proposed a \"sequence-independent\" learning mechanism, which has important theoretical and significant value.\n\n3.The proposed residual fusion network structurally retains the previously learned discriminative subspace and achieves smooth update of new and old modalities through up-down sampling residual blocks, suppressing update bias and improving feature stability.\n\n4.The cross-temporal knowledge collaboration module introduces matrix cross-entropy into the continuous learning scenario, forming interpretable mutual information maximization and redundancy minimization goals; its theoretical proof (Propositions 1 and 2) is relatively complete, providing a mathematical basis for the reliability of information flow."}, "weaknesses": {"value": "1.The description of the residual fusion network in the paper is not clear enough, and its specific role in information preservation and fusion has not been fully revealed.\n\n2.The relationship between MCE loss and mutual information is not fully explained in the manuscript.\n\n3.The authors are advised to further analyze and elaborate on the differences between the proposed method and existing continuous multi-modal methods.\n\n4.Minor issues：The authors are advised to systematically proofread the some formatting issues in the paper. For example, the abbreviation \"residual fusion network (RFN)\" is inconsistently formatted in some sections. Furthermore, as one of the core loss terms, \"Matrix Cross-Entropy (MCE)\" is recommended to provide a complete and clear formula definition upon its first appearance."}, "questions": {"value": "1.The paper's description of the Residual Fusion Network (RFN) is not clear enough, and its role in information preservation and fusion is not fully revealed. The authors are advised to further clarify how the RFN preserves the learned discriminative subspace through the residual channel and forms a high-rank global basis in the fusion process.\n\n2.The authors are advised to further clarify in the Methods section how modal sequence-agnostic is achieved through the synergy between the model structure and the optimization objective.\n\n3.The \"Matrix Cross-Entropy (MCE)\" loss is introduced in Equation (5), but the main text does not provide an intuitive understanding of its equivalence with mutual information (MI), and only provides a mathematical derivation in the Appendix. This arrangement makes it difficult for readers to directly understand the core motivation of the MCE loss and its information-theoretic significance when reading the main text. The authors are advised to add an intuitive explanation in the main text to explain how MCE plays a role in bridging feature uniformity and mutual information maximization in the optimization process."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BVR6XWMEs2", "forum": "0FH7ceYzCq", "replyto": "0FH7ceYzCq", "signatures": ["ICLR.cc/2026/Conference/Submission4298/Reviewer_vdaj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4298/Reviewer_vdaj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663816432, "cdate": 1761663816432, "tmdate": 1762917284023, "mdate": 1762917284023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submissions introduces a framework for Continual Multi-Modal Clustering that explicitly addresses the challenge of sequence sensitivity in modality arrival. The method combines a Residual Fusion Network for reliable continual information propagation, a Cross-Temporal Knowledge Collaboration mechanism to filter redundant information between historical and new modalities, and a Sequence-Agnostic Anti-Forgetting Strategy that uses cross-temporal consistency transfer and quality-aware consolidation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1/ This submission addresses the sequence sensitivity problem in continual multi-modal clustering, an under explored but practically important challenge in streaming, multi-view data environments.\n\n2/ The residual fusion and cross-temporal information filtering are well motivated and supported by information-theoretic derivations that enhance the paper’s credibility.\n\n3/ The proposed SCMC integrates residual fusion with information-theoretic regularization and cross-temporal consistency, showing careful theoretical grounding.\n\n4/The method is evaluated against a wide range of baselines, including both traditional clustering and recent continual multi-modal methods, with significant improvements."}, "weaknesses": {"value": "1/ While the framework is technically sound, the presentation suffers from heavy notation and dense exposition. A clearer pseudocode and a concise diagram mapping each loss term to modules in Figure 2 would make the methodology easier to follow.\n\n2/ The experiments rely on relatively small datasets. It would strengthen the claims to test SCMC on larger-scale or real-world streaming datasets, e.g., MM-IMDB, CMU-MOSI, or visual-linguistic datasets. Scalability analyses (time and memory cost vs. number of modalities) are also missing.\n\n3/ The current ablations focus mainly on removing modules. However, the interaction between RCIP and SAAS could be explored, e.g., does the anti-forgetting still work without residual fusion? Additionally, since a pretraining step is used, it is unclear how much improvement arises from pretraining itself versus continual adaptation.\n\n4/ The use of both MCE-based cross-temporal mutual information and contrastive QHC losses may lead to overlapping optimization effects. Clarifying their distinct roles or showing complementarity empirically would enhance methodological clarity."}, "questions": {"value": "Refer to the weakness session."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HP2QUgzoWi", "forum": "0FH7ceYzCq", "replyto": "0FH7ceYzCq", "signatures": ["ICLR.cc/2026/Conference/Submission4298/Reviewer_j6TL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4298/Reviewer_j6TL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894091453, "cdate": 1761894091453, "tmdate": 1762917283629, "mdate": 1762917283629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SCMC (Sequence-agnostic Continual Multi-modal Clustering), a framework for continual clustering over streaming modalities that aims to be insensitive to modality arrival order. The method (i) fuses historical and new features via a Residual Fusion Network (RFN), (ii) performs Cross-Temporal Knowledge Collaboration (CTKC) using a matrix cross-entropy regularizer to maximize cross-temporal mutual information, and (iii) applies a Sequence-Agnostic Anti-Forgetting Strategy (SAAS) composed of consistency transfer (CCT) and quality-aware consolidation (QHC) with MMD-based modality weights."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Clear problem framing: the paper isolates sequence sensitivity as a concrete failure mode for CMC and motivates it with a simple schematic (Fig. 1).\n2.FN→CTKC→SAAS is easy to follow; losses and objectives are given explicitly (MCE, CCT, QHC with MMD-based weights). This makes implementation straightforward."}, "weaknesses": {"value": "1.Novelty is incremental and several choices are heuristic. RFN is a standard residual bottleneck; CTKC is essentially an information-uniformity penalty applied across time; SAAS blends L2/KL consistency with contrastive losses and entropy maximization, while “quality” is a monotone transform of MMD to the fused representation. The components are sensible but not particularly original, and the design space (e.g., why MCE vs. CCA/InfoNCE variants, why this MMD→softmax mapping) isn’t justified\n2. Sequence-agnostic claim is under-stress-tested. The sequence analysis (Fig. 5) bins permutations into coarse groups; we don’t see exhaustive or adversarial ordering (e.g., worst-first or quality-descending sequences defined by measurable criteria). The t-tests in Fig. 6 compare against two baselines only and focus on p-values rather than effect sizes; there’s no correction for multiple comparisons"}, "questions": {"value": "1.Why MCE specifically? Give a principled comparison of CTKC’s MCE objective against alternatives (e.g., InfoNCE between history/new, CCA-style alignment, Barlow Twins-type redundancy reduction). Under what (non-Gaussian) conditions does your Proposition-1/2 reasoning still hold?\n2.Since the method claims redundancy suppression, evaluate under heavy noise injection or partial-modality dropout at different steps, and report whether sequence-agnostic gains persist."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "As mentioned above."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ykYFxK0W4k", "forum": "0FH7ceYzCq", "replyto": "0FH7ceYzCq", "signatures": ["ICLR.cc/2026/Conference/Submission4298/Reviewer_TUhU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4298/Reviewer_TUhU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762271148925, "cdate": 1762271148925, "tmdate": 1762917283231, "mdate": 1762917283231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles Continual Multi-modal Clustering and argues that existing methods are both (i) unreliable when fusing historical and newly arriving modalities and (ii) sequence-sensitive—early, high-quality modalities get forgotten as more (potentially noisy) modalities arrive. The proposed method, SCMC, has three components: (1) a Residual Fusion Network (RFN) that keeps a stable high-rank historical basis while adding a new modality; (2) Cross-Temporal Knowledge Collaboration, which links the fused representation with both historical and current modality features using a matrix cross-entropy objective; and (3) a Sequence-Agnostic Anti-Forgetting Strategy combining Cross-Temporal Consistency Transfer and Quality-aware Historical Consolidation with MMD-based modality importance weights. Experiments on five datasets show strong improvements over baselines and reduced sensitivity to modality order, supported by ablations and sequence-stability plots."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear problem framing and motivation around order sensitivity in streaming modalities; the paper operationalizes this with concrete evaluation of different arrival permutations and statistical tests.\n- CTKC’s MCE links the fused representation with both historical and current features and is supported by propositions connecting it to mutual information and redundancy control.\n- The experiments are comprehensive with consistent empirical gains. Table 1 and ablations show considerable ACC/NMI/PUR improvements; removing SAAS hurts most, underlining the importance of sequence-agnostic design."}, "weaknesses": {"value": "- The method is a combination of standard continual-learning techniques. Each component is individually familiar; what’s new is mostly the combination. As a result, the contribution risks looking like an over-engineered pipeline rather than a principled advance that falls short of ICLR’s bar.\n- I personally feel that \"sequence-agnostic\" is over-claimed in the paper. In continual learning, catastrophic forgetting is expected when training sequentially; numerous CL methods mitigate—but rarely eliminate—this effect, and older knowledge typically decays more than recent. The paper should either temper the claim or provide stronger evidence of genuine order invariance across settings."}, "questions": {"value": "- It’s unclear whether baselines were tuned comparably under continual multi-modal settings. Please detail search ranges and whether per-method tuning was allowed and conduct significance tests.\n- Please report params/FLOPs, wall-clock per step, and memory for each ablation and baseline. The log-det term can be cubic; what is d here and how is numerical stability handled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q0ibukrVIS", "forum": "0FH7ceYzCq", "replyto": "0FH7ceYzCq", "signatures": ["ICLR.cc/2026/Conference/Submission4298/Reviewer_LZ8w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4298/Reviewer_LZ8w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762275306137, "cdate": 1762275306137, "tmdate": 1762917282995, "mdate": 1762917282995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We appreciate the constructive feedback from the reviewers. The revised manuscript has been uploaded, with all revisions marked in blue."}}, "id": "65SKFmWTFY", "forum": "0FH7ceYzCq", "replyto": "0FH7ceYzCq", "signatures": ["ICLR.cc/2026/Conference/Submission4298/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4298/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission4298/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763633030944, "cdate": 1763633030944, "tmdate": 1763633030944, "mdate": 1763633030944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}