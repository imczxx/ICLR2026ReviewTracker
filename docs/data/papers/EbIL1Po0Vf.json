{"id": "EbIL1Po0Vf", "number": 17276, "cdate": 1758274153300, "mdate": 1759897185709, "content": {"title": "Deep Low Rank Projector for KV Cache Compression", "abstract": "Large Language Models (LLMs) have become integral to a wide range of natural language processing tasks. A key component enabling fast autoregressive inference in LLMs is the Key-Value~(KV) cache, which stores hidden states across decoding steps. However, the KV cache imposes substantial memory overhead, especially in long-context generation. While recent studies have proposed various compression techniques to mitigate this issue, they largely overlook the interaction between the techniques Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA, under which models are significantly more sensitive to KV cache compression. To address this issue, we propose the Deep Low-Rank Projector (DLRP), a novel adapter that compresses the KV cache along the head dimension while preserving downstream performance in PEFT-adapted models. We introduce the Deep Linear Projector (DLP), which is realized leveraging a Deep Linear Network (DLN). We also propose a novel regularizer that approximates the nuclear norm of the DLP, thereby promoting low-rank structure in the learned projection. After training with the proposed regularizer, we inspect the singular‑value spectrum and select the minimum rank satisfying a predefined energy threshold, yielding a compact head dimension that balances compression and accuracy. Based on this rank, we construct the DLRP, fine-tune it on the target task, and merge its factorized layers into a single linear operator for efficient inference. Empirical evaluation confirms that DLRP achieves substantial KV cache compression while maintaining strong performance across diverse LLM benchmarks, offering a practical solution for deploying PEFT-adapted models in memory-constrained settings.", "tldr": "", "keywords": ["Large Language Model", "KV Cache Compression", "Deep Linear Network"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f11140fbdbc279a8534fad491d34bed87701a853.pdf", "supplementary_material": "/attachment/0df15aad8c3c1755f3ea821c65f8339cbd911df1.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a Deep Low-Rank Projector (DLRP) to compress KV caches when models are fine-tuned with LoRA. It trains deep linear adapters with a Frobenius-norm surrogate to promote low rank, selects rank via energy thresholds, and folds the projector for inference. Experiments on Qwen and Mistral report better accuracy than applying token-eviction compression baselines on fine-tuned at similar compression ratios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Clear, practical pipeline: train DLRP, energy-based rank selection, fold for inference.\n+ Broad evaluation across models (Qwen/Mistral) and tasks with multiple compression rates."}, "weaknesses": {"value": "+ **Novelty & positioning**: Trained low-rank projectors for KV compression have appeared recently (e.g., MatryoshkaKV); overlap and distinctions are not sufficiently discussed, weakening the originality claim.\n\n+ **Baseline selection**: DLRP targets head-dimension redundancy, yet most baselines are token-eviction methods. Missing like-for-like baselines (e.g., EigenAttn [7] or Palu [4] with LoRA finetuning and other low-rank/train-to-compress methods) make it unclear whether gains stem from DLRP or from weaknesses of eviction on these tasks.\n\n+ **Related work gaps**: Recent training-based KV compression exploring head-dimension redundancy (e.g., MHA2MLA [3], TransMLA[2]) are not adequately discussed, despite clear relevance to the paper’s “KV-compress + fine-tune” scope.\n\n+ **Task choice**: Benchmarks are mostly short-context (e.g., PIQA, HellaSwag). Long-context stress tests (e.g., RULER [5], SCBench [6]) are lacking, leaving the real-world robustness under memory pressure unclear.\n\n[1]. MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection\n\n[2]. TransMLA: Multi-Head Latent Attention Is All You Need\n\n[3]. Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs\n\n[4]. Palu: Compressing KV-Cache with Low-Rank Projection\n\n[5]. RULER: What's the Real Context Size of Your Long-Context Language Models?\n\n[6] SCBench: A KV Cache-Centric Analysis of Long-Context Methods\n\n[7]. Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2osXcayolx", "forum": "EbIL1Po0Vf", "replyto": "EbIL1Po0Vf", "signatures": ["ICLR.cc/2026/Conference/Submission17276/Reviewer_9119"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17276/Reviewer_9119"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945050606, "cdate": 1761945050606, "tmdate": 1762927221197, "mdate": 1762927221197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies that PEFT-adapted LLMs (e.g., LoRA fine-tuned models) are significantly more sensitive to KV cache compression than base models, and proposes the Deep Low-Rank Projector (DLRP) to address this issue. Authors propose Deep Linear Networks as trainable adapters with a novel regularizer that approximates the nuclear norm to learn task-adaptive low-rank projections of the KV cache head dimension, automatically discovering appropriate compression levels through energy-based rank selection. Experiments across multiple models (Qwen3-4B/8B, Mistral-7B) and benchmarks show that the method substantially outperforms five baseline methods, especially at high compression rates (e.g., 59.4% vs 26.5% on GSM8K at 50% compression), while revealing interpretable patterns where reasoning tasks preserve higher ranks and summarization tasks adopt more aggressive compression."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Evaluations are performed across multiple model sizes and different datasets with multiple compression rates.\n- Practical and deployment friendly design. The linear matrices can be folded to have little to no overhead during deployment.\n- Demonstrates interpretable behavior by automatically discovering task-appropriate compression levels: reasoning tasks preserve higher ranks while redundant long-context tasks (CNN/DM) achieve lower ranks"}, "weaknesses": {"value": "**Potential Error in the derivation of Theorem 3.1**\n- In Equation (20), (21) the denominator should be N^N instead of N. While the stated theorem is still true as \\frac{1}{N^N} <= \\frac{1}{N}, the claimed bound becomes **exponentially loose** as depth increases.\n- This did not seem to affect the results much, because authors only tried N=1,2,3 (from table3). However there is no theoretical backing anymore to use the said regularization loss. Once you accept arbitrarily loose bounds, the specific choice of \\Sigma||D_n||_F becomes **unprincipled**. The authors need to argue why *their* loose bound is the right one to use. \n\n\n**Motivation for DLN is lacking**\n- What training dynamics of DLN motivates their use here? (line 202), “..induce highly non-convex training objectives…and exhibit an implicit bias towards low-rank solutions..”.  How do these help the task? If low-rankedness is required, why not use a projection and explicitly enforce it with a loss instead of an indirect method?\n- With N=2, (table 3) the method essentially reduces to “finding the lowest ranked update on W and then fine tuning it with that rank”.\n  - Determining that different tasks require updates of different ranks is not conceptually novel.\n- Multiple LoRA methods exist that optimize the singular values of the weights ([e.g.](https://arxiv.org/abs/2405.19597)) which need to be discussed and compared with.\n- If alpha=0 works, was the regularization even needed? Does having higher alpha, reduce the selected rank?\n\n**Unfair Evaluation Setup**\n- Line 316, “..first finetuned with LoRA (Hu et al (2022)) (rank 32)..” are all models finetuned with rank 32 irrespective of compression?\n- Line 323, “..Ranks are selected using energy thresholds that correspond approximately to 10%, 25%, 50%..”. With different ranks to LoRA and the DLRP, the proposed method uses more task-dependent parameters for each dataset. Which makes the comparison unfair."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "01VgqIxjJ4", "forum": "EbIL1Po0Vf", "replyto": "EbIL1Po0Vf", "signatures": ["ICLR.cc/2026/Conference/Submission17276/Reviewer_i3sK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17276/Reviewer_i3sK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968262274, "cdate": 1761968262274, "tmdate": 1762927220936, "mdate": 1762927220936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Deep Low-Rank Projector (DLRP), an adapter module designed to compress the Key-Value (KV) cache of large language models (LLMs), particularly in parameter-efficient fine-tuning (PEFT) settings such as LoRA. The key innovation lies in leveraging Deep Linear Networks (DLNs) to model the projection process and introducing a nuclear-norm-inspired regularizer to promote low-rank structure without performing expensive SVD. After training, the method selects the smallest energy-preserving rank, constructs the DLRP, and fine-tunes it on downstream tasks. Experiments on Qwen3-4B, Qwen3-8B, and Mistral-7B show that DLRP significantly outperforms existing methods such as SnapKV, StreamingLLM, PyramidKV, and KeyDiff, especially in PEFT settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel formulation:\nThe use of Deep Linear Networks for cache compression is novel and theoretically grounded. The proposed regularizer offers an elegant and computationally efficient surrogate to the nuclear norm.\n2. Comprehensive experiments:\nEvaluation across multiple benchmarks (GSM8K, PIQA, HellaSwag, XSum, CNN/DM) and models (Qwen, Mistral) demonstrates strong empirical robustness.\n3. Clear motivation:\nThe paper clearly identifies and addresses the overlooked interaction between KV-cache compression and PEFT."}, "weaknesses": {"value": "1. Generalization beyond a single task or dataset:\nIt appears that the DLRP needs to be trained separately for each downstream task, since the rank-selection step is based on task-specific energy thresholds and the adapter is fine-tuned afterward.\nQuestion: Does this imply that if we want to apply DLRP to another downstream task, we must re-train the projector from scratch?\nIf so, this may limit scalability and increase computational overhead. I suggest the authors explore task-agnostic training or transferability across related domains.\n2. Inference-time efficiency vs. compression ratio:\nWhile the paper demonstrates strong accuracy retention, it does not explicitly quantify inference latency reduction.\nQuestion: As the compression ratio increases, does the actual inference time decrease proportionally?\nGiven that the DLRP folds into a single linear map, it would be useful to report speedups or GPU memory usage during real-time decoding.\n3. Integration with other compression paradigms:\nDLRP currently focuses on the head-dimension compression only. The authors acknowledge this in the Limitations section.\n→ Suggestion: It would be interesting to discuss or experiment with combining DLRP with sequence-length compression methods (e.g., StreamingLLM, H2O) or quantization-based approaches (e.g., bit-level cache compression). Such hybrid methods may yield higher compression without retraining."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rPvrt4b7mD", "forum": "EbIL1Po0Vf", "replyto": "EbIL1Po0Vf", "signatures": ["ICLR.cc/2026/Conference/Submission17276/Reviewer_mc12"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17276/Reviewer_mc12"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992173466, "cdate": 1761992173466, "tmdate": 1762927220410, "mdate": 1762927220410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DLRP is a PEFT-friendly KV-cache compressor that reduces the head dimension via trained Deep Linear Projectors with an SVD-free low-rank regularizer. After selecting an energy-preserving rank, the adapters are fine-tuned and folded into single linear maps for inference, consistently beating prior methods (SnapKV, StreamingLLM, Knorm, PyramidKV, KeyDiff) at 10/25/50% compression on Qwen3-4B/8B and Mistral-7B, with task-/layer-adaptive ranks that keep reasoning accuracy while exploiting redundancy in long-context summarization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- First KV-cache compression method explicitly tailored for LoRA- or PEFT-adapted models, addressing their higher sensitivity to compression.\n- Introduces Deep Low-Rank Projector (DLRP) built on deep linear networks, enabling effective compression along the head dimension.\n- Proposes an SVD-free nuclear-norm surrogate (Frobenius-norm–based) that promotes low-rank structure with much lower computational cost."}, "weaknesses": {"value": "- DLRP only compresses the head dimension, leaving other major axes (layers, sequence length, number of heads) untouched—limiting overall memory savings.\n- The two-stage process (regularized DLP training + DLRP fine-tuning) increases training cost and implementation overhead compared to simpler post-hoc compression methods.\n- The paper focuses on accuracy retention but omits decoding speed or latency improvements, which are critical for practical deployment\n- Performance depends on choices like the energy threshold and regularization strength (α), yet guidelines for tuning are minimal."}, "questions": {"value": "How well does DLRP perform on non-instruction-tuned or multilingual models? Since the experiments focus on English datasets, it would be useful to evaluate whether the learned low-rank subspace transfers across domains and languages."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MLljkfnxbc", "forum": "EbIL1Po0Vf", "replyto": "EbIL1Po0Vf", "signatures": ["ICLR.cc/2026/Conference/Submission17276/Reviewer_ravY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17276/Reviewer_ravY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997953487, "cdate": 1761997953487, "tmdate": 1762927219961, "mdate": 1762927219961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}