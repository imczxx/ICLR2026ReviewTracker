{"id": "rQM3oU9cyg", "number": 3043, "cdate": 1757320485080, "mdate": 1759898112272, "content": {"title": "Accelerating Discrete Diffusion Decoding with Parallel Scan", "abstract": "Diffusion-based language models provide strong controllability and parallel generation capabilities, but suffer from prohibitively high decoding cost. Block diffusion, a semi-autoregressive approach, alleviates this issue by reducing diffusion steps and enabling KVCache utilization, yet it restricts parallel decoding strictly within blocks, preventing inter-block parallelism. In this work, we identify a class of \\emph{associative states} in diffusion models: \\textbf{blocks that can be independently sampled without conditioning on the prefix, and later refined once the prefix becomes available, effectively performing a form of self-refinement}. Leveraging this property and inspired by classic parallel algorithms, we propose a novel \\textbf{parallel-scan based decoding} framework. Our method incorporates two key techniques---\\emph{local remasking} and \\emph{global aggregation} ---to enhance stability and efficiency. We further introduce a systematic design space of prefix-network topologies, cache strategies, and parameter configurations, and conduct a preliminary exploration of this search space. Empirically, our training-free approach achieves up to \\textbf{68 tokens/s} throughput and \\textbf{60.7\\% accuracy on GSM8K}, and we validate the effectiveness of the proposed techniques through ablations. Compared to mainstream semi-autoregressive methods, our results demonstrate that parallel decoding with structured parallel patterns remains a promising and underexplored direction for efficient inference in diffusion-based LLMs.", "tldr": "", "keywords": ["Efficient AI System", "Discrete Diffusion Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/41fdd1c813980216cb180815dadd31f3553670d1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper targets decoding inefficiency in discrete diffusion language models (dLLMs). It observes that many token positions behave as \"associative states\"---they can be decoded speculatively and later revised with minimal interference---then exploits this with a prefix-scan (parallel-scan) decoding framework. Two mechanisms stabilize the scheme: local remasking (selectively re-mask low-confidence tokens between levels) and global aggregation (periodic full-KV reconciliation). A design space over prefix-network topologies, cache strategies, and hyperparameters is explored. On LLaDA-8B-Instruct model, the method reports higher throughput (up to 4.25×) and competitive or improved accuracy on reasoning tasks (e.g., GSM8K)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper targets at a timely and relevant problem: Efficient inference for diffusion language models is important.\n\nThe paper is also well written with helpful graphical demonstrations.\n\nThe empirical results look promising to increase the throughput but also improve the accuracy."}, "weaknesses": {"value": "Although I am familiar with diffusion models and its discrete counterpart, accelerating inference is not my main research direction. Therefore, I don't have high confidence of the following potential weaknesses.\n\nThe concept of \"associative states\" is purely motivated by a probing experiment, which shows that most token pairs are non-interfering. Further analysis---either more compelling experiments or theoretical justification---would strengthen the concept.\n\nAs mentioned, the baseline LLaDA is run in full-block diffusion (not its standard semi-autoregressive/latency-optimized setting). This leaves the comparison with the best configuration unclear."}, "questions": {"value": "Please see weaknesses. The questions are mainly how to further support associative states and compare baselines in the best reported setting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NytYfMtGdK", "forum": "rQM3oU9cyg", "replyto": "rQM3oU9cyg", "signatures": ["ICLR.cc/2026/Conference/Submission3043/Reviewer_mnqi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3043/Reviewer_mnqi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967868285, "cdate": 1761967868285, "tmdate": 1762916523192, "mdate": 1762916523192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a scan-based diffusion language model decoding method that can accelerate inference of existing DLLMs without requiring additional training, while maintaining comparable performance. The proposed algorithm achieves block-level parallelism through self-reflection and self-speculation mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written, with a clear motivation and an algorithm specifically designed for the observed phenomenon.\n\n2. The proposed method requires no additional training and can be quickly adapted to existing models."}, "weaknesses": {"value": "1. The paper lacks comparisons with similar methods; it only presents some comparisons with the naive LLADA. Comparing with other block-level training-free methods could better highlight the advantages of the proposed approach.\n\n2. The generalizability of the method is a concern. Unlike the nearly lossless performance observed in math tasks, the proposed method shows a notable performance drop in code tasks, which cannot be ignored."}, "questions": {"value": "1. The advantages over block-diffusion need to be further demonstrated experimentally. Currently, the acceleration ratio seems somewhat lower than block-diffusion, and the task performance shows no significant improvement.\n\n2. Typo on line 247: change c to d."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3sCoxyOg37", "forum": "rQM3oU9cyg", "replyto": "rQM3oU9cyg", "signatures": ["ICLR.cc/2026/Conference/Submission3043/Reviewer_7eEr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3043/Reviewer_7eEr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990641349, "cdate": 1761990641349, "tmdate": 1762916522986, "mdate": 1762916522986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a parallel-scan-based decoding framework to accelerate discrete dLLMs. Traditional diffusion models offer full-sequence parallelism but suffer from sequential denoising and high per-step cost, while semi-autoregressive methods such as Block Diffusion reduce steps but limit inter-block parallelism. The authors restructure decoding into parallel-scan stages inspired by classical parallel algorithms. The method integrates two mechanisms: local remasking for uncertainty-driven refinement and global aggregation for stable convergence. It further explores a design space involving prefix-network topologies (Brent–Kung, Kogge–Stone, Sklansky), cache strategies, and remasking parameters. Experiments show significant throughput gains (up to 4.2×) and improved accuracy on reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces a novel application of parallel-scan algorithms to diffusion decoding, uncovering a previously underexplored form of structured parallelism in dLLMs.\n\n2. This paper presents well-motivated mechanisms, and the design of associative states, local remasking, and global aggregation collectively improve both stability and speed.\n\n3. Comprehensive empirical analysis exhibits throughput gains and clear insight into decoding dynamics."}, "weaknesses": {"value": "1. The associative state assumption is only validate on math and coding datasets. The authors should test on more diverse dataset types to validate its effectiveness on more diverse domains.\n\n2. The experiments lack comparisons with other parallel decoding methods like Fast-dLLM. More comparison with baselines is needed."}, "questions": {"value": "Please refer to weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MLk5j8auDU", "forum": "rQM3oU9cyg", "replyto": "rQM3oU9cyg", "signatures": ["ICLR.cc/2026/Conference/Submission3043/Reviewer_WjmL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3043/Reviewer_WjmL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762072131254, "cdate": 1762072131254, "tmdate": 1762916522775, "mdate": 1762916522775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes parallel-scan based decoding for DLMs. Current DLMs suffer from expensive inference due to sequential denoising and bidirectional attention costs. The key insight is identifying associative states: token blocks that can be decoded speculatively without prefix conditioning, then refined once prefix context arrives. Drawing from classic parallel algorithms, the authors propose a prefix-network framework with two mechanisms: local remasking (revisiting low-confidence tokens) and global aggregation (using full key-value cache for stability). The training-free method explores three network topologies (Brent-Kung, Kogge-Stone, Sklansky) and reports up to 68 tokens/s throughput and 60.7 percent accuracy on GSM8K with 2-4x average speedup over block diffusion."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Novel associativity insight: Figure 2 empirically shows most token pairs are order-independent, justifying parallel-scan over pure semi-autoregressive methods. This differentiates the work.\n- Comprehensive design space: Systematic framework spanning topologies, caching modes, and parameters provides a clear overview. Figure 7 effectively shows Brent-Kung dominates the Pareto frontier.\n- Strong reasoning results: GSM8K shows substantial gains (+32.9 percent on 512-token generation) with Brent-Kung topology, and Table 3 demonstrates 2-4x throughput improvement.\n- Training-free applicability: Works with pretrained DLMs without modification, making deployment straightforward."}, "weaknesses": {"value": "- Limited scope and generalization: Only LLaDA-8B evaluated; unclear whether findings transfer to other DLMs, scales, or domains. Code tasks show significant degradation, yet the paper lacks characterization of when associativity holds. Does it depend on training procedure, model size, or task type?\n- Incomplete comparisons: Missing head-to-head comparisons with recent concurrent methods. Fast-dLLM, dKV-Cache, and dInfer achieve 8-45x speedups using different KV cache strategies. D2F enables pipeline parallelism across blocks. AdaBlock-dLLM uses adaptive block sizing. How does parallel-scan compare quantitatively on identical benchmarks and generation lengths?\n- Unclear theoretical foundation: Associative states are empirically observed but lack formal definition. When do they exist? Why do code tasks show sparser associativity? The claim assumes perfect parallelism but ignores memory bandwidth and synchronization overhead in actual hardware.\n- Ad-hoc design space exploration: The search methodology (600 random configurations sampled on 80 examples) lacks principled justification. No ablation on search strategy, sensitivity to sample size, or comparison with Bayesian optimization or evolutionary algorithms. The unified score combining latency and pass rate lacks weighting justification."}, "questions": {"value": "- Generalization across architectures: Does parallel-scan work for Dream, Mercury, or other recent DLMs? Can you characterize when associative states emerge as a property of model training (masked vs. unmasked pretraining), architecture (e.g., linear attention variants), or task type (reasoning-heavy vs. code)? Evaluating at additional DLMs would strengthen the paper.\n- Direct comparison with concurrent cache methods: Provide head-to-head experiments on GSM8K and HumanEval (same generation lengths, same prompting) comparing your method to Fast-dLLM, dKV-Cache, and dInfer. Can parallel-scan be combined with those caching strategies? What is the actual memory overhead of maintaining intermediate KV states across prefix-network levels?\n- Formal characterization of associativity: Can you define associativity rigorously? Provide a metric (mutual information, causal influence, attention entropy) predicting when token pairs are associative? Analyze which model layers exhibit higher associativity? Explain why code exhibits sparser long-range associativity.\n- Design space search methodology: How sensitive are results to the random search procedure? How about Bayesian optimization or evolutionary algorithms? Provide ablations showing Brent-Kung is robust across multiple seeds and sample sizes. Which parameters (topology, cache, remask ratio, confidence threshold) most impact performance? Can you learn heuristics for selecting configurations based on task characteristics?\n- Extensions and scalability: How does performance scale with more aggressive parallelism (larger block sizes, deeper trees)? Can variable-length generation be supported beyond fixed 256/512 tokens? Can this be combined with block-autoregressive training (D2F style)? What memory-throughput tradeoffs exist at different sequence lengths?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zi0RBMhGfx", "forum": "rQM3oU9cyg", "replyto": "rQM3oU9cyg", "signatures": ["ICLR.cc/2026/Conference/Submission3043/Reviewer_HFGa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3043/Reviewer_HFGa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123604090, "cdate": 1762123604090, "tmdate": 1762916522115, "mdate": 1762916522115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper claims that while current block-wise diffusion models enables better efficiency due to a current block's decoding only depending on the previous block (instead of the full sequence in traditional masked diffusion), the block dependency reduces parallel decoding ability. To improve upon this shortcoming, the authors decide to utilize a parallel-scan-based framework.\n\nThey show through their probing experiments that many states within diffusion models are associative, meaning that the decoding order does not change the final decoded token outcomes. Utilizing this finding, they build a framework that enables the parallel decoding and refinement of multiple groups of blocks.\n\nThe authors explore several different prefix network topologies within their parallel-scan framework and validate their framework on four tasks with the LLaDA-8B-Instruct model. They find their framework improves upon the baseline by large margins for longer generations on reasoning tasks, but programming tasks suffer under their framework due to lack of associativeity. Compared to traditional autoregressive blockwise diffusion, they find a 2x gain in throughput while using their parallel-scan framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem they are trying to tackle is very important as efficiency in modeling is highly impactful and timely given the increased interest in block diffusion modeling.\n\nThe paper has an interesting finding that many states in block diffusion models are associative, and their framework does achieve significant throughput gains."}, "weaknesses": {"value": "It is unclear how the probing experiment is conducted. The authors make no mention of the dataset, model, or other experimental setup (number of samples, etc). Also it seems like the probing experiment is done on a token level while the actual framework operates on a block-level, leading to some mismatch in settings.\n\nIn addition, it is unclear whether their probing experimental findings (and by extension, framework) generalize to other diffusion LLMs as they only test on LLaDA.\n\nAlso, it seems like perforamnce is highly dependent on the topology of the parallel-scan. It is unclear how well Brent_kung may extrapolate to other settings. \n\nThe LLaDA baseline results seem substantially lower than what is reported in their preprint [1]. Also, there is no comparison to a block-diffusion model, e.g., [2].\n\n\n[1] https://arxiv.org/abs/2502.09992\n[2] https://arxiv.org/abs/2503.09573"}, "questions": {"value": "How does this method compare to Block Diffusion?\n\nWhat might be attributing to the lower scores of LLaDA on GSM8K compared to the original paper?\n\nWhy does performance seem to vary across certain tasks when generation length is increased? E.g., increasing generation length hurts GSM8K but seems to improve some models on programming?\n\nHow do the authors modify the vocabulary ID?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Uo0MwKzFOi", "forum": "rQM3oU9cyg", "replyto": "rQM3oU9cyg", "signatures": ["ICLR.cc/2026/Conference/Submission3043/Reviewer_Jvmc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3043/Reviewer_Jvmc"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762719495410, "cdate": 1762719495410, "tmdate": 1762916521397, "mdate": 1762916521397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}