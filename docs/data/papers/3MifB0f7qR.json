{"id": "3MifB0f7qR", "number": 4093, "cdate": 1757599225430, "mdate": 1763745808858, "content": {"title": "QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation", "abstract": "Reinforcement learning (RL) has emerged as a central paradigm for training large language models (LLMs) in reasoning tasks. Yet recent studies question RL’s ability to incentivize reasoning capacity beyond the base model. This raises a key challenge: how can RL be adapted to solve harder reasoning problems more effectively?\nTo address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. \nOur method, QuestA, when applied during RL training on math reasoning tasks,  not only improves pass@1 but also pass@k—particularly on problems where standard RL struggles to make progress. \nThis enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 72.50\\% (+10.73\\%) on AIME24, 62.29\\% (+12.79\\%) on AIME25, and 41.67\\% (+10.11\\%) on HMMT25. Code, data and model are available at https://anonymous.4open.science/r/questa932.", "tldr": "By introducing partial solutions to make hard problems easier during reinforcement learning, this method significantly boosts the mathematical reasoning capabilities of language models.", "keywords": ["Large Language Model; Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e4289aad1761237293be613ac7fbe2a0ac0763c5.pdf", "supplementary_material": "/attachment/a6b85af841542472cf8721bb30bda2e1e156380e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces QuestA, a data-centric augmentation technique designed to enhance reasoning performance during RLVR by injecting partial solution hints into training prompts. The approach is conceptually simple yet theoretically grounded, with analyses explaining why partial-solution augmentation is effective. Experimental results demonstrate its strong performance and incorporating partial hints not only accelerates learning but also mitigates the entropy collapse."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is simple yet effective, and achieves new SOTA pass@1 results on challenging math benchmarks for 1.5B-parameter models.\n2. The method is thoroughly evaluated through comprehensive ablation studies and extensive analyses across multiple datasets, model architectures, and training curricula, consistently demonstrating performance improvements across diverse settings."}, "weaknesses": {"value": "1. The method’s reliance on high-quality, step-wise solutions for augmentation raises concerns about scalability to domains lacking such curated data. For instance, can QuestA generalize to real-world science Q&A or open-domain reasoning tasks where solution steps are unavailable?  \n2.  The paper would benefit from a qualitative error analysis. There is limited discussion of why the “Partial-0” (no hints) setting yields no improvement, or why certain tasks exhibit smaller gains.\n3.   In my personal opinion, although the paper offers a theoretical perspective on why partial-solution augmentation improves RL efficiency, this section feels somewhat unnecessary and potentially confusing. The core idea is already straightforward and intuitive, and adding theoretical formalism may detract from its practical clarity—especially since the method can be naturally interpreted as a form of prompt optimization."}, "questions": {"value": "1. Could the authors elaborate on practical limitations—such as applicability to problems without step-wise gold solutions or the impact of poor-quality hints? Is there empirical evidence of robustness to incorrect or misleading hints?  \n2. How sensitive is QuestA to the form of hints used (e.g., solution block, chain-of-thought, or intermediate step)?  \n3. How does the choice of $p$ trade off between training speed and final performance? Can the method adaptively tune this during RL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "irU2Io1Dhw", "forum": "3MifB0f7qR", "replyto": "3MifB0f7qR", "signatures": ["ICLR.cc/2026/Conference/Submission4093/Reviewer_2jPJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4093/Reviewer_2jPJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751084918, "cdate": 1761751084918, "tmdate": 1762917174544, "mdate": 1762917174544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores improving reasoning in large language models (LLMs) using reinforcement learning (RL). It highlights limitations of RL in enhancing reasoning beyond the base model and proposes a novel strategy called Question Augmentation (QuestA). QuestA introduces partial solutions during training to simplify problems and provide better learning signals. Applied to math reasoning tasks, QuestA significantly boosts performance metrics like pass@1 and pass@k, especially on challenging problems. The method achieves state-of-the-art results on math benchmarks using 1.5B-parameter models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1,The authors demonstrate QuestA's effectiveness through rigorous experiments on math reasoning benchmarks, achieving state-of-the-art results with notable performance gains on metrics like pass@1 and pass@k.\n2. The open resources makes it a valuable contribution to the field of LLMs, particularly in reasoning-intensive domains like mathematics."}, "weaknesses": {"value": "1. The experiments are conducted on 1.5B-parameter models, which may not generalize to larger models. The scalability and adaptability of QuestA across different model sizes remain unclear.\n2. The relationship between data difficulty and performance was mentioned in earlier papers on mathematics long ago, such as [1]. However, this paper does not discuss. And, in this subproblem, it is also an obvious insight, somewhat lacking innovation.\n\n[1] WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"}, "questions": {"value": "refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ji96My11xP", "forum": "3MifB0f7qR", "replyto": "3MifB0f7qR", "signatures": ["ICLR.cc/2026/Conference/Submission4093/Reviewer_4ed7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4093/Reviewer_4ed7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907642935, "cdate": 1761907642935, "tmdate": 1762917174321, "mdate": 1762917174321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A simple yet effective strategy via Question Augmentation is introduced for  partial solutions during training to reduce problem difficulty and provide more informative learning signals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Improves pass@1 but also pass@k—particularly on problems where standard RL struggles to make progress.\n2.Achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 72.50% (+10.73%) on AIME24, 62.29% (+12.79%) on AIME25, and 41.67% (+10.11%) on HMMT25.\n3.There is reasonable proof for the the proposed theory ."}, "weaknesses": {"value": "1.The benchmark is better if code dataset is evaluated.\n2.The algorithm of RL is not advanced."}, "questions": {"value": "1.It's a good data augmentation for advance the reasoning during RL training.\n2.It is more experiments on other RL algorithm."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aAyhyxwBQ9", "forum": "3MifB0f7qR", "replyto": "3MifB0f7qR", "signatures": ["ICLR.cc/2026/Conference/Submission4093/Reviewer_iEgk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4093/Reviewer_iEgk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919078907, "cdate": 1761919078907, "tmdate": 1762917174081, "mdate": 1762917174081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces QUESTA, a simple yet effective data-centric strategy to enhance the reasoning capabilities of LLMs through reinforcement learning. The paper identifies a critical trade-off in RL training for reasoning: while easy problems can cause a decline in reasoning diversity (pass@k), training on hard problems is inefficient due to sparse reward signals. The proposed method is introduced to navigate this trade-off, aiming to mitigate this inefficiency and effectively expand the model's reasoning capacity. Experiments show the method enables 1.5B-parameter models to achieve better results on challenging math benchmarks, in some cases surpassing a much larger 32B model. \n\nA key limitation is the method's reliance on an empirically-tuned curriculum. This hand-crafted 50-25 schedule presents challenges for the method's transferability to new settings. Furthermore, its effectiveness on larger-scale models remains unverified, as such models may not benefit from this specific scaffolding strategy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper clearly identifies and demonstrates the critical trade-off between training on easy versus hard prompts in RL, providing a strong motivation for the proposed method.\n\n- The paper introduces an elegant approach that does not require complex changes to the underlying model architecture or RL algorithm. The idea of using partial solutions as hints is intuitive and proves to be effective.\n\n- The method yields consistent performance improvements. The 1.5B model trained with QUESTA shows a clear gain over the standard RL baseline and, on the AIME25 benchmark, even surpasses a model over 20 times larger, highlighting the efficiency of the approach.\n\n- The paper includes a theoretical analysis that formalizes why augmenting questions with partial solutions can improve the sample efficiency of RL, adding depth and rigor to the empirical findings."}, "weaknesses": {"value": "- The experiments are limited to 1.5B models. It is unclear if the method would provide similar gains on larger models that already possess stronger reasoning capabilities.\n\n- The curriculum for providing hints (50-25) appears to be a key component of the method's success. The ablation study only compares this strategy against a fixed 50% hint, which is insufficient to understand the sensitivity of the model's performance to other potential curriculum designs or hyperparameter choices.\n\n- \"We apply augmentation using the solution block rather than the reasoning chain-of-thought\"; this is a design choice presented without any rationale or comparative experiments to justify this decision. It is plausible that hints derived from the CoT could be more effective."}, "questions": {"value": "The curriculum for the hint percentage (from 50% to 25%) is a key component. Could you elaborate on how this schedule was chosen and have you considered alternatives, such as a more gradual decay or an adaptive schedule based on model performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VzQ7ZMwT5g", "forum": "3MifB0f7qR", "replyto": "3MifB0f7qR", "signatures": ["ICLR.cc/2026/Conference/Submission4093/Reviewer_oEsW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4093/Reviewer_oEsW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088044168, "cdate": 1762088044168, "tmdate": 1762917173837, "mdate": 1762917173837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}