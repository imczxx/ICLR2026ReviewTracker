{"id": "GYJFJz9Dy5", "number": 20874, "cdate": 1758311282144, "mdate": 1759896954204, "content": {"title": "RefineBench: Evaluating Refinement Capability in Language Models", "abstract": "Can language models (LMs) self-refine their own responses? This question is increasingly relevant as more than 10% of real-world user interactions involve refinement requests (see Appendix F). Yet prior studies have largely tested LMs on verifiable tasks such as competition math or symbolic reasoning with simplified scaffolds, whereas users often pose open-ended queries and provide varying degrees of feedback about what went wrong. The recent advent of reasoning models that exhibit self-reflection patterns in their chain-of-thought further motivates this question. To address it, we introduce RefineBench, a benchmark of 1,002 challenging problems across 11 domains paired with a checklist-based evaluation framework. We evaluate two refinement modes: (1) guided refinement, where an LM is provided natural language feedback, and (2) self-refinement, where LMs attempt to improve without guidance. In the self-refinement setting, even frontier LMs such as Gemini 2.5 Pro and GPT-4.1 achieve modest baseline scores of 31.1 and 23.4, respectively, and most models fail to consistently improve across iterations (e.g., Gemini-2.5-Pro gains only +1.8%, while DeepSeek-R1 declines by –0.2%). By contrast, in guided refinement, both proprietary LMs and large open-weight LMs (>70B) can leverage targeted feedback to refine responses to near-perfect levels within five turns. These findings suggest that frontier LMs require breakthroughs to self-refine effectively when their initial responses are incorrect, and that RefineBench provides a valuable testbed for tracking progress.", "tldr": "We propose RefineBench, a benchmark containing 1002 challenging problems across 11 domains that uses a controlled checklist-based evaluation framework. Our benchmark supports two main refinement settings: self-refinement and guided refinement.", "keywords": ["Refinement", "Large Language Model", "Checklist"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a6fd6073c7747d64a394eb7eadeb62c1538b091.pdf", "supplementary_material": "/attachment/13bc7c7bb04e664dd7ca0806b0674e282f9c9916.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces RefineBench, a new benchmark with 1,002 problems across 11 domains designed to evaluate the refinement capabilities of Large Language Models. It uses a novel checklist-based framework to test two modes:\n1. Self-Refinement (no feedback, $f_t = \\emptyset$)\n2. Guided Refinement (with feedback $f_t$)\n\nThe primary contribution is the finding that even frontier LMs like Gemini 2.5 Pro struggle significantly with self-refinement, showing minimal gains (e.g., +1.8%) across iterations. However, in guided refinement, these same models effectively use targeted feedback to achieve near-perfect scores. This suggests LMs possess refinement abilities but lack the direction of what to fix."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces RefineBench, a high-quality benchmark for evaluating refinement in complex, non-verifiable domains like law and humanities, moving beyond simple math problems.\n2. The quality of this benchmark is very high, using real-world problems and a novel checklist-based evaluation framework that was rigorously validated by Ph.D. domain experts (96.1% appropriateness).\n3. The authors identify a key bottleneck: LMs can improve with feedback but are fundamentally poor at self-refining because they cannot identify their own errors."}, "weaknesses": {"value": "1. The \"self-refinement\" failure mode is not precisely identified. The paper concludes models \"lack direction on what to fix\". However, the evidence (e.g., Figure 6) suggests the model failed to identify that a problem existed at all. This is a failure of self-verification or error-detection. Models aren't necessarily unable to fix errors, but rather they incorrectly conclude their initial answer is already \"complete and correct\"  and thus stop trying to refine.\n2. The \"Guided Refinement\" setting likely overstates true refinement capability by effectively testing instruction-following. The feedback provided is not a realistic, high-level critique. Instead, it's a list of explicit, atomized commands derived directly from the failed checklist items (e.g., \"The response should accurately...\" in Appendix K), which is a much simpler task for LMs."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BPk0XwEWr0", "forum": "GYJFJz9Dy5", "replyto": "GYJFJz9Dy5", "signatures": ["ICLR.cc/2026/Conference/Submission20874/Reviewer_8xJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20874/Reviewer_8xJ9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650044959, "cdate": 1761650044959, "tmdate": 1762937466140, "mdate": 1762937466140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents RefineBench, a benchmark of 1002 problems across 11 domains. Each problem includes a checklist to help evaluators assess LLM responses consistently and accurately. The dataset combines verifiable STEM tasks with non-verifiable, free-form tasks. Experiments show that guided refinement enables most LLMs to reach correct answers after multiple turns, whereas self-refinement does not achieve comparable gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a new benchmark with a relatively large problem set and clear per-problem checklists, enabling more reliable evaluation of LLMs’ reasoning abilities.\n\n- The analyses are clear and highlight that self-refinement remains challenging, particularly due to LLMs’ difficulty in identifying specific errors and determining how to adjust initial answers."}, "weaknesses": {"value": "- The study uses GPT-4.1 as the sole evaluator, which may introduce bias. Incorporating a second independent LLM-as-judge or human auditing would strengthen the evaluation.\n\n- For problems that originally include images, textual descriptions may omit important details. Expanding the benchmark to a multimodal setting would address this limitation."}, "questions": {"value": "1. The paper finds a key bottleneck is that LLMs struggle to pinpoint detailed issues or the direction of correction from the initial response. Do the authors have insights or proposals on how to improve this?\n\n2. The paper notes that many existing benchmarks focus on math or symbolic reasoning rather than open-ended questions, yet RefineBench still contains a moderate share of math and math-like tasks. How are truly open-ended tasks represented, and could their proportion be increased?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SAT0nck0Ki", "forum": "GYJFJz9Dy5", "replyto": "GYJFJz9Dy5", "signatures": ["ICLR.cc/2026/Conference/Submission20874/Reviewer_KPFp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20874/Reviewer_KPFp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924008315, "cdate": 1761924008315, "tmdate": 1762937464949, "mdate": 1762937464949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RefineBench, a benchmark aimed at probing whether LLMs can perform self-refinement, either independently or with external guidance. In this context, external guidance refers to evaluation checklist items that the model previously failed on. The authors evaluate 37 LLMs and highlight several insights: (1) LLMs generally struggle to refine their own responses, though thinking models show slightly better self-refinement; (2) LLMs can refine themselves when given failed checklist items, but still fail to address issues that are not explicitly pointed out in partially guided setups."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark is well-curated, covering a wide range of topics and domains. The manual quality control process also seems solid.\n2. The evaluation spans a large number of models, showing a commendable level of comprehensiveness.\n3. The findings are interesting - especially the comparison between thinking models and standard ones. As the paper notes, whether refinement itself is beneficial has been extensively studied and debated in prior work, but revisiting this question in the context of reasoning models is valuable."}, "weaknesses": {"value": "1. I have concerns about using the same checklist for both external guidance and evaluation. Could this create potential leakage, where models optimize for missing checklist items instead of genuinely improving quality? It's unclear whether the provided guidance leads to real improvement or just better checklist completion.\n2. Discussion of related work is strangely organized. The CriticBench line of work seems most relevant and should probably be introduced earlier in Section 2. In contrast, the part on multi-turn benchmarks feels less directly connected and can be toned down."}, "questions": {"value": "The analysis on whether test-time scaling helps refinement is intriguing. However, it's limited to Gemini-2.5-Pro in Figure 5. It would be great to expand the analysis and see whether similar trends hold across other reasoning models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xapjVOWxnr", "forum": "GYJFJz9Dy5", "replyto": "GYJFJz9Dy5", "signatures": ["ICLR.cc/2026/Conference/Submission20874/Reviewer_tMKC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20874/Reviewer_tMKC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977067938, "cdate": 1761977067938, "tmdate": 1762937460692, "mdate": 1762937460692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This dataset presents a multi-topic refinement benchmark with university-level problems and evaluation checklists. The paper presents the dataset curation process and evaluation process, as well as extensive experiments with LLMS. This is a fantastic paper, with one significant let down -- there is no human evaluation/verification of the evaluation pipeline -- which really weakens the results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Very interesting problem.\n- The dataset is a significant contribution. \n- Human evaluation of checklist generation.\n- Summary statistics of the dataset and comparison to other benchmarks are included and well-presented.\n- Extensive experiments."}, "weaknesses": {"value": "I really only found one letdown in this paper, but it is a big one --  there is no human evaluation/verification of the evaluation pipeline. To be strong, there needs to be a human verification of a sample of the end-to-end evaluation process. How do we know how good the LLMs are at comparing the answer to the checklist and providing good feedback? This is instrumental to understanding the results. \n\nI also think that a good baseline would have been to compare the success with human feedback (desirable)."}, "questions": {"value": "(line 190 -- step 3) -- If this was manually reviewed by the authors, why did you need LLMs to create the checklists from reference answers? \n\n-- Why did you not have human verification of the eval pipeline, and can it be reasonably added?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DiWekc3Y5z", "forum": "GYJFJz9Dy5", "replyto": "GYJFJz9Dy5", "signatures": ["ICLR.cc/2026/Conference/Submission20874/Reviewer_XS27"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20874/Reviewer_XS27"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002212665, "cdate": 1762002212665, "tmdate": 1762937452556, "mdate": 1762937452556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}