{"id": "0r1KU3dlps", "number": 13108, "cdate": 1758213659886, "mdate": 1759897464443, "content": {"title": "Distributionally Robust Linear Regression with Block Lewis Weights", "abstract": "We present an algorithm for the empirical group distributionally robust (GDR) least squares problem. Given $m$ groups, a parameter vector in $\\mathbb{R}^d$, and stacked design matrices and responses $\\mathbf{A}$ and $\\bf{b}$, our algorithm obtains a $(1+\\varepsilon)$-multiplicative optimal solution using $\\widetilde{O}(\\min\\{\\mathsf{rank}(\\mathbf{A}),m\\}^{1/3}\\varepsilon^{-2/3})$ linear-system-solves of matrices of the form $\\mathbf{A}^{\\top}\\mathbf{B}\\mathbf{A}$ for block-diagonal $\\mathbf{B}$. Our technical methods follow a recent technique that relates the empirical GDR problem to a carefully chosen least squares problem and an application of accelerated proximal methods. Our algorithm improves over known interior point methods for moderate accuracy regimes and matches the state-of-the-art guarantees for the special case of $\\ell_{\\infty}$ regression. We also give algorithms that smoothly interpolate between minimizing the average least squares loss and the distributionally robust loss.", "tldr": "We give algorithms for optimizing a distributionally robust/multidistributional loss for least squares linear regression.", "keywords": ["distributionally robust optimization", "linear regression", "acceleration", "convex geometry"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f02dcc4b0bd8559fc815fcec3f118c525314a9f6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes two algorithms for solving the “egalitarian”/“group DRO objective” of multi-group regression: one algorithm for the “robust” version of it—i.e., the maximum over each group’s squared prediction error—and one which interpolates from the “robust” version to the fully “nonrobust” version—i.e., the average squared prediction error across groups. The contribution is to obtain, in terms of linear-system-solves of some specific matrices of the problem, approximation guarantees that: (i) holds for more families of objective functions than currently in the literature, and (ii) can match best-known error accuracies in the literature for particular cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of the paper are:\n- It is rigorous and the authors have tried to explain the layout of their proof in the main paper, making explicit connections with previously published results that they use, expand, or improve on.\n- The contribution of the paper is clear and the problem they study can benefit diverse communities in machine learning, statistics, and signal processing. \n- Results are clearly compared with the existing literature, e.g., see Table 1."}, "weaknesses": {"value": "I have a series of things to point out about the paper.\n\n**>>About clarity and results:**\n- Line 107 mentions that, compared to all other methods in Table 1, the paper’s results do not have “geometry-dependent terms”. Can the authors pinpoint what those “geometry-dependent terms” are in the other methods from Table 1? A definition of what this phrase means will be useful.\n- In line 151 inside Remark 1.2: the term “morally equivalent” does not sound technically correct. What do the authors mean by this? Perhaps another phrase could be used. \n- Theorem 2 is also valid for $p=2$, which is also the setting of Theorem 1, though the complexity of linear-system-solved are different between both theorems. Can the authors comment on the differences between the complexity of these two approximation results? Can the authors also comment on the main differences between Algorithms 3 and 5 and how these reflect on the approximation/complexity results?\n- Lines 225-226 states that “our results suggest that optimizing (...) between non-robust and robust objectives may be *computationally easier* than optimizing for the robust objective alone”. Could you indicate how this is suggested by simply looking at Theorems 1 and 2, or is there some missing information?\n- Equation (6), which is the decomposition of equation (2) or (4) into subproblems, is introduced (i) without explanation of how it was obtained and (ii) without a formal justification for it. Can the authors explain this? Also, the authors claim that this decomposition is a common procedure in convex optimization—see line 285—however, no citation is added.\n- I am trying to understand the paragraph that starts in line 356. I am sure that the second sentence is, using the notation of Definition 2.1, referring to $f(\\cdot)=\\sqrt{\\delta^2+||\\cdot - b\\_{s\\_i}||\\_2^2}-\\delta$ and the norm $||\\cdot||\\_2$. Can the authors confirm this? For some reason the authors are introducing the term “$A_{S_i}x$” as an argument, which is confusing.\n- When finishing Section 2.1.1, I don’t see any indication on how to choose the radius $r_q$ in the proof outlined so far. Is this something which is missing or am I missing something? This is important since line 366 of the next Section 2.1.2 seems to indicate that $r_q$ has already been “explicitly” constrained in Section 2.1.1.\n- Line 407: it is mentioned that the “diameter of our problem will have decreased by a constant factor”. However, what does “decreased by a constant factor” mean here? Is it an *additive* constant factor or a *multiplicative* one? I suppose it is not an additive factor, unless it somehow shrinks due to some parameter.\n\n**>> Paper organization:**\n- It is important to include Algorithm 3 in the main paper (if possible, I would also suggest including Algorithm 5, but I understand that this could be more complicated since it uses Algorithm 4 and other results that may be difficult to fully include). This will make things more self-contained. Also, Algorithm 3 shows mathematical constructions and references mentioned in Section 2, which can help the reader to better follow the paper.\n- Move Table 1 earlier in the paper, close to the first text where it is referred to: right after Theorem 1.\n\n**>>About the title:**\n- The “Block Lewis Weights” are only mentioned as part of a technique for the papers’ proofs: the reader has to wait **until the last page of the paper**, where Theorem 2.3 is, to know what they are. It is strange that nowhere earlier in the paper, **including the abstract itself**, an idea of what “BLock Lewis Weights” are provided, even though the phrase is in the title itself. Thus, I strongly suggest two things:\n  - Change the title to something like “Solving Distributionally Robust Linear Regression Using Block Lewis Weights”.\n  - Mention earlier in the paper, perhaps Section 1.1 when introducing the results, what Block Lewis Weights are and how they are used in the proof.\n\n\n**>>Other things:**\n- There is a typo in equation (4): the exponent “$2$” of the $l_2$-norm of the prediction error should be “$p$” instead.\n- In the related literature mentioned in Section 1.2, I suggest mentioning works that have studied the connection between distributionally robust optimization (DRO) and the effect of regularization on regression problems and other estimation problems. For example, two works come into mind:\n  - “Robust Wasserstein profile inference and applications to machine learning” by Blanchet et al., 2019 studies this problem in the context of logistic regression and linear regression (including lasso).\n  - “Distributionally Robust Formulation and Model Selection for the Graphical Lasso” by Cisneros-Velarde et al., 2020, studies this problem in the context of precision matrix estimation.\n- The last sentence of the first paragraph (line 029) needs a citation.\n- I suggest using parentheses every time an equation is referenced. E.g., instead of “equation 1” use “equation (1)”.\n- The paper mentions references where equation (2) has been used; however, no references are provided where equation (1) has also been used.\n- Line 145: instead of the word “promised”, a more appropriate term would be “guaranteed”. \n- Line 201: I suggest indicating in the paper that “DRO” stands for “Distributionally Robust Optimization”.\n- Line 294: when mentioning the “three questions” that arise when dealing with the subproblems, I would suggest adding the fact that these three questions correspond to three *consecutive* subproblems/steps of the main proof.\n- Title of Lemma D.3.: it may be better to use “$||\\cdot||_2^p$” instead of “$||y||_2^p$”\n- Lines 411-422: I suggest replacing the expression “It would be nice to obtain this” by “It was difficult to obtain this”.\n- Line 436: the word “existentially” does not seem appropriate, perhaps replace it with “further”."}, "questions": {"value": "Please, see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qZ4rxtdwuG", "forum": "0r1KU3dlps", "replyto": "0r1KU3dlps", "signatures": ["ICLR.cc/2026/Conference/Submission13108/Reviewer_kYbK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13108/Reviewer_kYbK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714428291, "cdate": 1761714428291, "tmdate": 1762923836835, "mdate": 1762923836835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the author proposes a novel second-order algorithm based on accelerated proximal methods to solve the empirical group distributionally robust (GDR) least-squares problem, aiming to improve group-level fairness in linear regression.\n\nThe proposed algorithm achieves a (1+ε)-multiplicative optimal solution through a series of linear-system solves. Moreover, the author introduces a continuous interpolation between robust and non-robust optimization, which elegantly bridges the utilitarian and egalitarian objectives. Theoretical analyses are provided for both the accuracy bounds and the iteration complexity, allowing readers to compare the proposed method against existing optimization approaches.\n\nFrom my perspective, the writing of this paper is clear, professional, and well-organized. The author explains the intuition and derivations of the algorithm in detail, and systematically contrasts it with classical methods. The theoretical sections are rigorous, with definitions and notation clearly presented. Additionally, the discussion of parameter roles and practical implications is thoughtful and accessible, making this paper almost textbook-level for readers who may not be deeply familiar with theoretical optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.\tElegant writing and well-structured mathematical presentation.\n2.\tExtensive and up-to-date literature review that situates the work within the broader DRO and optimization landscape.\n3.\tCareful and coherent technical overview that guides readers through the proofs.\n4.\tThe paper clearly acknowledges limitations and points out meaningful future directions."}, "weaknesses": {"value": "1.\tAlthough the algorithm avoids geometry-dependent convergence and relies only on linear-system solves, its numerical accuracy remains lower than that of interior-point methods (IPM) in high-precision regimes.\n2.\tGiven the complex form of the theoretical bounds, it would be valuable to include numerical experiments that empirically illustrate the relationship between ε and runtime or accuracy, and directly compare with IPM and Lewis.\n3.\tMany recent DRO frameworks adopt a regret-minimization perspective, which also leads to convex formulations with desirable properties. It would strengthen the paper if the author could discuss or contrast their approach with regret-based DRO methods."}, "questions": {"value": "1. In some DRO frameworks, researchers may add more assumption on different groups, e.g. the weighted addition or Wasserstein distance. What will happen if we extend the algorithm to this area?\n2. In real world, the number of group m is rarely greater than the number of features, which influences the Rank of A. So, will the gap between proposed method and standard log-barrier IPM be not so significant in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nubcUzEWOW", "forum": "0r1KU3dlps", "replyto": "0r1KU3dlps", "signatures": ["ICLR.cc/2026/Conference/Submission13108/Reviewer_Q7n9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13108/Reviewer_Q7n9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966419429, "cdate": 1761966419429, "tmdate": 1762923834207, "mdate": 1762923834207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents condition-number-free algorithms for empirical group distributionally robust (GDR) least-squares regression and for an $\\ell_p$-style interpolation between average and worst-group loss.  \nThe main theoretical claims are:\n\n1. An algorithm achieving a $(1+\\varepsilon)$-approximate solution to  \n   $\\displaystyle \\min_x \\max_i \\frac{\\|A_{S_i}x-b_{S_i}\\|_2}{\\sqrt{n_i}}$  \n   using $\\tilde O(\\min\\{\\mathrm{rank}(A),m\\}^{1/3}\\varepsilon^{-2/3})$ linear-system solves of the form $A^\\top B A$ for block-diagonal $B$.\n\n2. An extension to $\\ell_p$-type interpolants between the average and robust objectives with iteration complexity $\\tilde O(\\mathrm{poly}(p)\\min\\{\\mathrm{rank}(A),m\\}^{(p-2)/(3p-2)})$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides an algorithm that can solve empirical group distributionally robust least-squares regression with STOA complexities."}, "weaknesses": {"value": "Honestly, I am not an expert in optimization theory. However, I think this paper may be more suitable for a journal such as *Mathematical Programming* rather than ICLR. Moreover, the paper does not provide any numerical results.\n\nThe complexity analysis depends on the oracle for solving linear systems, which may not be a fair comparison to the gradient evaluation oracle."}, "questions": {"value": "na"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uvukCFJzkH", "forum": "0r1KU3dlps", "replyto": "0r1KU3dlps", "signatures": ["ICLR.cc/2026/Conference/Submission13108/Reviewer_caGj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13108/Reviewer_caGj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989391106, "cdate": 1761989391106, "tmdate": 1762923833777, "mdate": 1762923833777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops an algorithm for 'distributionally robust linear regression'. Namely, it assumes m datasets for linear regression to be given, corresponding (for instance) to m distinct data sources. Instead of aggregating the data and fitting a linear model via least squares, the distributionally robust optimization (DRO) approach minimizes the maximum ell-2 error across the m datasets.\nThe paper also considers an objective that interpolates between the aggregate ell-2 error and the maximum of ell-2 error by averaging the p-th powers of ell-2 errors. \nThe new algorithm follows a general scheme from Carmon et. al (2020), and proceeds by solving a sequence of proximal problems defined on suitably constructed ellipsoidal trust regions. \nThe main technical innovations are a smoothening of the objective, and a new construction of the trust regions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem is quite fundamental, and this paper improves over the state of the art.\nThe results appear to be sound and technically novel."}, "weaknesses": {"value": "- The new method only improves over interior point methods (Lee ans Sidford, 2019) for large m\n- It is unclear how practical is the algorithm, given the nested structure\n- The authors apply a general framework from earlier work, and innovations is rather in specific aspects of the problem."}, "questions": {"value": "1. A general suggestion. I think that the technical overview is the most interesting and important part of the paper. I would suggest to make it more precise and detailed, eventually stating auxiliary lemmas. In contrast, it would be sufficient to focus on the p=\\infty case. \n\n2. Are the complexity of the inner calls in IPM and in the present method comparable? They appear to require the solution of linear systems with somewhat different structure.\n\n3. It would be useful to have an overall presentation of the algorithm. The definition is now dispersed across multiple sections.\n\nMinor:\n-I do not think the adjective \"existential\" is used correctly at the top of p 9"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C9nfxnfkDP", "forum": "0r1KU3dlps", "replyto": "0r1KU3dlps", "signatures": ["ICLR.cc/2026/Conference/Submission13108/Reviewer_fntP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13108/Reviewer_fntP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762151181115, "cdate": 1762151181115, "tmdate": 1762923833500, "mdate": 1762923833500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}