{"id": "qzgro4i3sg", "number": 5809, "cdate": 1757936636127, "mdate": 1759897952018, "content": {"title": "Efficient numeracy in language models through single-token number embeddings", "abstract": "To drive progress in science and engineering, large language models (LLMs) must be able to process large amounts of numerical data and solve long calculations efficiently. This is currently only possible through the use of external tools or extensive reasoning chains, either limiting the numerical intuition of LLMs or limiting the length of problems they can solve. We show that frontier LLMs require excessive amounts of reasoning tokens to solve even basic calculations, which is exacerbated by their tokenization strategies that split single numbers into multiple tokens. This motivates the need for efficient and effective single-token number encodings. We introduce a set of desiderata for such encodings and show that existing approaches fail to fulfill them. To address these shortcomings, we propose BitTokens, a novel tokenization strategy that embeds any number into a single token using its IEEE 754 binary floating-point representation. Through extensive experiments we show that our BitTokens allow even small language models to learn algorithms that solve basic arithmetic operations nearly perfectly. This newly gained efficiency could expand the length and complexity of problems language models can solve.", "tldr": "We propose BitTokens, a novel tokenization strategy for LLMs that embeds numbers using their IEEE 754 binary floating-point representation, which allows for efficient numeracy in language models", "keywords": ["language model", "LLM", "arithmetic", "numeracy", "benchmark", "single-token number embedding", "tokenization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d0aaaebcdf6076500c32b5aeca07d9195d4d8a08.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors in this paper first propose a benchmark to test the numeracy of LLMs, and nine desiderata of the number tokenizer. They analyze two classic previous work xVal and FoNE and find their drawbacks. To solve the problem, they propose their BitTokens which can satisfy all the nine desiderata and can also represent a very large range of numbers. The experiments show that their tokenizer performs better than other tokenizers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Meaningful improvement**: The method detailed analyze the previous mehtods like xVal and FoNE, and propose meaningful improvements over them.\n2. Their experiments show that their method outperforms the previous methods on various datasets.\n3. The FineWeb dataset is a practical text dataset that can support its effectiveness on real-world tasks."}, "weaknesses": {"value": "1. **Fair comparison between related work**: the related work NumberCookbook contains four different representation of numbers including integer, float, fraction and scientific notation. The float and scientific notation contain large range of numbers. Lines 140-147 incorrectly state about the related works' contribution.\n2. **Contribution of the benchmark**: it is not clear what the benchmark adds to the existing related work. It is also not clear that the relation between the results in section 2 and their BitTokens.\n3. **The necessary to further justify the nine desiderata**: although the nine desiderata seem to be reasonable, some of them are not well justified. \n   1. For example, why \"a single token\" is required? (D1) A long-standing issue regarding the one-token representation lies in its practical effectiveness. Despite the common emphasis on the computational overhead associated with single-digit tokenizers in certain scenarios, I have yet to see any evidence from efficiency comparisons in real-world tasks that sufficiently demonstrate the importance of this method (including in this article). Considering that, even in fields like physics and finance, the number of tokens for words, spaces, or other formatting elements (such as tables in markdown) will far exceed that of the numbers themselves, with shorter numbers still predominating, the actual benefits of this method remain questionable. \n   2. For D3: we need experiment evidence to show that structured representation leads to better performance, where some indirect evidence is insufficient. I am aware that past work on interpretability in smaller models seems to suggest that the model tends to learn this structural form, but this may be strongly correlated with insufficient model size.\n   3. For D6: in most practical scenarios,both the token embedding and the activation value will not use low-precision representation, even the model parameters have heavily quantized. Therefore, it is not clear why low-precision representation is necessary.\n4. **Reasoning**: Some text-number reasoning tasks are required to further validate the effectiveness of the proposed method. For example, GSM8K, SVAMP, and other benchmarks that require numerical reasoning in addition to basic understanding. The FineWeb dataset can only validate the fitting ability. Whether the token is suitable for math reasoning is not clear."}, "questions": {"value": "1. What is the contribution on the benchmark?\n2. Why do you believe each of the nine desiderata is necessary? Is there any experiment evidence?\n3. Is there enough motivation to use a special designed tokenizer to represent number, especially when the experiments show that one-digit tokenizer performs also well on most of the tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X8RT4oGCZF", "forum": "qzgro4i3sg", "replyto": "qzgro4i3sg", "signatures": ["ICLR.cc/2026/Conference/Submission5809/Reviewer_THn1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5809/Reviewer_THn1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761190334997, "cdate": 1761190334997, "tmdate": 1762918275121, "mdate": 1762918275121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, Efficient Numeracy in Language Models through Single-Token Number Embeddings, investigates why even frontier LLMs struggle with basic arithmetic despite strong reasoning capabilities. The authors argue that the root cause lies in inefficient number tokenization—existing models split numbers into multiple tokens, forcing them to use long reasoning chains or external tools for simple computations.\nTo address this, the paper introduces BitTokens, a new single-token number encoding scheme based on the IEEE 754 binary floating-point representation. Each number is encoded as a 64-dimensional binary vector representing its sign, exponent, and significand, concatenated with its reciprocal to improve division learning. This design satisfies a set of nine desiderata for efficient, trainable, and numerically stable encodings.\nComprehensive experiments across nine numeracy tasks show that BitTokens enable even small GPT-style models to perform addition, multiplication, and division with near-perfect accuracy—surpassing prior single-token methods such as xVal (value-scaled embeddings) and FoNE (Fourier Number Embeddings)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\n The paper introduces a novel conceptual and technical framework for enhancing numeracy in LLMs through single-token number embeddings, addressing a long-standing inefficiency in numerical reasoning. The proposed BitTokens represent a creative synthesis of ideas from numerical computing (IEEE 754 floating-point representation) and modern tokenization strategies for LLMs. The formalization of nine desiderata for single-token number encodings is conceptually fresh and provides a principled foundation for evaluating future approaches.\n\n\nTechnical quality:\n The work demonstrates strong theoretical and empirical rigor. The authors analyze prior methods (xVal and FoNE) through formal proofs—e.g., showing the additive homomorphism of sinusoidal encodings and their inability to support efficient multiplication—and motivate BitTokens as a solution grounded in established numerical theory. Experimental methodology is solid: the benchmark includes nine carefully controlled numeracy tasks, diverse number ranges, and rigorous evaluation metrics (e.g., log-sMAPE). Results are robust, replicable, and consistently show clear improvements across models and tasks.\n\n\nClarity and presentation:\n The paper is very clearly written and well structured. Each section flows logically—from motivation, desiderata, and theoretical analysis to implementation and results. Visualizations (e.g., Figures 1–4) effectively communicate both the inefficiency of reasoning-based numeracy and the improvements achieved by BitTokens. Mathematical formulations (e.g., Lemma 4.2, Proposition 4.3) are carefully explained and accessible to readers with standard ML background.\n\n\nSignificance and impact:\n The contribution is potentially highly significant for the development of numerically capable LLMs. By providing a deterministic, stable, and efficient encoding for numbers, BitTokens could reduce reasoning-token overhead and unlock more efficient arithmetic computation inside general-purpose LLMs. This directly impacts scientific and engineering applications of LLMs and contributes to the broader goal of building models with intrinsic numerical understanding rather than reliance on external tools."}, "weaknesses": {"value": "The method cannot generalize to unseen or longer-digit numbers across tasks, since each number is represented as a unique token rather than a compositional encoding of digits or bits.\n\n\nThe paper does not explain how the model reproduces exact numeric strings (e.g., 1.000, 000101) where format, not value, is important.\n\n\nThe comparative setup with prior work (FoNE, Neural Number Representations) lacks fairness and direct equivalence in optimization and data sampling settings."}, "questions": {"value": "Generalization to longer or unseen numbers:\n\n Since each number is represented by its own token, the model cannot share parameters across digits or magnitudes. This means if the training data contains only 3-digit numbers, any 6-digit number in an unseen task will have an untrained token embedding. In practice, you cannot train every number on every task. Could you evaluate how BitTokens handle such out-of-distribution magnitudes — for example, models trained on ≤3-digit numbers but tested on ≥6-digit ones for addition, multiplication, or exponentiation?\n\n---\n\nExact string prediction:\n\n How does your pipeline preserve numeric formatting when the target string must match exactly (e.g., 1.000, 000101)? Does the [NUM] decoding step reproduce such surface forms, or does it always canonicalize to a float value (e.g., 1 or 101)?\n\n---\n\nComparison to related methods:\n\n Please clarify how BitTokens differ conceptually and empirically from Improving LLM Numerical Reasoning with Neural Number Representations (arXiv:2405.17399), which also explores specialized numeric embeddings for arithmetic reasoning.\n\n---\n\nExperimental fairness:\n\n In your FoNE reproduction, you employ different optimization and data sampling strategies from the original paper. The FoNE work reports ~97% accuracy on 60-digit addition, suggesting that performance differences may arise from optimizer choice, curriculum design, or sampling distribution rather than representational limits. Could you clarify whether you controlled for these factors? Before large-scale training, it would be informative to compare all methods under identical, simple settings to isolate the effects of different training strategy and training data distribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z51uAhCMZi", "forum": "qzgro4i3sg", "replyto": "qzgro4i3sg", "signatures": ["ICLR.cc/2026/Conference/Submission5809/Reviewer_M71x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5809/Reviewer_M71x"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761344792814, "cdate": 1761344792814, "tmdate": 1762918274660, "mdate": 1762918274660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BitTokens, a single token number embedding based on IEEE-754 floating-point structure. Each number is encoded into a [NUM] token augmented with a 64-dimensional vector that corresponds to the sign bit, exponent bits, and significand bits. Decoding uses a small number head with a sigmoid, trained with bit-wise BCE. The authors first motivate the need for efficient numeracy by showing frontier LLMs still require very large reasoning traces for basic arithmetic. They then lay out nine desiderata for single token numeric encodings and argue that prior approaches such as xVal and FoNE violate key desiderata for arithmetic, especially multiplication in sinusoidal space. Experiments on small GPT-2 style models trained from scratch compare BitTokens to subword, single-digit, xVal, and FoNE across seven single-step tasks in a multi-task setting, plus three harder solo tasks. BitTokens achieve near-perfect accuracy on comparison and single-step arithmetic, with notably strong multiplication and division, and competitive language modeling perplexity on FineWeb. The paper also introduces a curriculum for numeric tasks and dynamic multi-task sampling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Recasting numeric representation as IEEE-754 bit planes inside a single token is a clean idea that aligns with hardware arithmetic and Boolean operations. The formal desiderata are a helpful framework for comparing encodings.\n\nThe paper diagnoses why sinusoidal number tokens struggle with multiplication, showing that any learned operator must effectively decode, convolve, propagate carries, then re-encode. This is a sound argument that matches the empirical results.\n\nOn multi-task training with text, BitTokens outperform FoNE and xVal on multiplication and division, and are competitive or better than digit and subword baselines on many tasks. With the proposed setup, BitTokens achieve the best FineWeb perplexity among the compared tokenizers in the multi-task configuration. \n\nThe design stays numerically stable and LayerNorm-friendly by scaling bits to unit RMS and using a simple number head, which is attractive for integration."}, "weaknesses": {"value": "1. **Training distribution fairness**. The curriculum introduces an **extra training set** that uniformly samples bit precision to balance difficulty for BitTokens, while evaluation retains decimal difficulty for all methods. This is a non-trivial distribution tweak that seems tailored to BitTokens and is not obviously mirrored for baselines. After investigating the provided codes, I find this obvious in the configs: BitTokens have more config training sets compared to other baselines. The paper should either remove this asymmetry or construct equivalently fair curricula for each tokenizer.\n\n2. **Scope of multi-task justification**. The paper argues that exponentiation, mean, and standard deviation are hard and thus are removed from the multi-task mix and trained as solo tasks, which complicates comparisons and the claim that BitTokens deliver broadly better numeracy under realistic pretraining mixes. A stronger justification and matched compute budgets across tasks are needed.\n\n3. **Precision choice not explored.** The method hard-codes float64, yet many deployments run in float32, bfloat16, or even fp8. The advantages of 64-bit mantissa bits vs smaller formats are not quantified, and the desideratum about low-precision robustness is asserted rather than carefully tested across precisions.\n\n4. **Ablation coverage is narrow.** Ablations cover token combination strategies, base-10 vs base-2, reciprocal concatenation, and curriculum. Missing are precision width, number head variants, normalization treatments, noise robustness, and data mixing ratios. Also these methods are trained with Muon optimizer and whether it gives BitTokens unfair favor is unknown. I would like to see standard AdamW results. \n\n5. **Generalization and downstreams.** Results are on small models trained from scratch. It is still unclear how BitTokens interact with large-scale pretraining and math word problems. The discussion acknowledges this but it limits the strength of the contribution for ICLR.\n\n6. **Language modeling tradeoff depends on setup.** In multi task training with text, BitTokens win FineWeb perplexity. In solo task training, FoNE has the best FineWeb perplexity and BitTokens are slightly worse, which weakens the claim that BitTokens are a drop in replacement for BPE or FoNE in general LLM pretraining."}, "questions": {"value": "1. **FineWeb perplexity comparison.** In Table 4 BitTokens have the best perplexity among baselines in the multi-task setup. Can the authors clarify whether FoNE ever wins on perplexity under any controlled setting, for example when numeracy data is removed or curriculum is disabled, and with matched token budgets and seeds? A small ablation in Table 11 hints at shifts when curriculum is off. Please add a controlled study. Also in Table 8, it shows FoNE achieves the best perplexity on solo tasks trained on FinewebText, does that imply the distribution needed for BitTokens to work are so different from natural number distributions?\n\n2. **Distributional fairness of curricula.** The bit-precision-balanced auxiliary training set seems specific to BitTokens. What is the effect size of this design choice on final arithmetic and perplexity metrics? Please either remove this asymmetry or provide equivalent difficulty balancing for decimal-centric tokenizers. A paired ablation would help. I think this is *very important* for scientific studies. \n\n3. **Why multi-task excludes multi-step tasks.** Excluding exponentiation and std from the shared mix weakens the general claim. Could you include them with a capped sampling ratio and report end-to-end training dynamics and final tradeoffs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l1rzD4wlAD", "forum": "qzgro4i3sg", "replyto": "qzgro4i3sg", "signatures": ["ICLR.cc/2026/Conference/Submission5809/Reviewer_QH8n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5809/Reviewer_QH8n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943065836, "cdate": 1761943065836, "tmdate": 1762918274179, "mdate": 1762918274179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript addresses the problem of LLM performance on arithmetical operations over numerical expressions. It identifies shortcomings of current implementations supported by empirical evidence, critically surveys alternative solutions available in the state of the art, lists a series of desiderata, and proposes a novel alternative accompanied by experimental results. In my view, the manuscript exhibits clear strengths but also important weaknesses, suggesting significant room for improvement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem of arithmetical content processing in distributional models, and LLMs in particular, is an important and timely one\n2. The manuscript is well-written and well-structured\n3. It provides a reasonable account of the literature relevant to the solution proposed, even if some relevant work is missing (cf. weaknesses)\n4. Good and informative empirical evaluation on frontier LLMs\n5. Very interesting analysis of addition and multiplication over sinusoidal encoding of numerical expressions\n6. Empirical and formal results are sound as far as I can judge (disclosure: I'm not a mathematician by training, so, despite my best efforts, it is not impossible that I have overlooked some technical details)"}, "weaknesses": {"value": "(in order of importance)\n1. The motivation for this work is ill-formed, and therefore, the solution proposed can look unjustified\n2. The experimental results obtained are relatively limited\n3. Desiderata look arbitrary\n4. Using prompting as a method is inadequate\n5. Relevant work is not considered\n6. Confusion between tokenization and embedding\n7. Lack of clarity in formal statements\n\nFurther details on weaknesses:\n1. The problem of the arithmetical performance of LLMs is presented in terms of \"numeracy\" of LLMs, defined as \"the ability to understand and work with numbers.\" As such, this problem is conceived as a cognitive task. But the problem of arithmetical operations in LLMs, no less than in an elementary pocket calculator, or any abstract or concrete computational model for that matter, is not a cognitive but a formal one. My calculator---or, say, the lambda calculus---doesn't have \"the ability to understand and work with numbers\", while one can arguably say that my neighbor does. Certainly, we have some good ideas of why my calculator performs arithmetical operations, while my neighbor's cognitive capabilities are more obscure. But the cognitive character of the latter doesn't come from that obscurity. Therefore, the fact that the operations of a computational model like an LLM are obscure to us is not a legitimate reason to assume that whatever numerical calculation they perform is to be addressed in cognitive terms as \"numeracy\". These remarks are not purely speculative. They point to the fact that attempting to provide an LLM with \"intrinsic numeracy skills\" is an ill-defined task at best (if not belonging to magic altogether). With respect to LLMs, the problem of numerical calculation is either a descriptive one (i.e., understanding the formal mechanisms explaining the possibilities and limitations of distributional models of computation to perform arithmetical calculations) or a normative one (i.e., what alternative mechanisms can we imagine for correct calculations). This manuscript provides some interesting insights on the former by formally analyzing alternative solutions (in particular, sinusoidal encoding, which, however, is not distributional). But if the problem is how to enhance distributional models with formal methods for correct calculation, it is not clear why not simply outsource calculations to an elementary calculator, which would be, without any doubt, more efficient and more effective. The manuscript claims that this \"prevents the model from building an intuition for numbers and the results of calculations, which is required to interpret and contextualize information from complex domains\", which again, I take for highly unscientific, since \"intuition\", \"interpretation\", and \"contextualization\" are not computational concepts. One could maybe claim that outsourcing numerical calculations affects the processing of non-numerical (eg. linguistic) expressions in LLMs, but this would require evidence that neither is present in this manuscript, nor seems to be its intention to provide. One could also claim that outsourcing calculations would interfere with the current tendency of constructing end-to-end models, but the proposed solution is no better in this sense, because it comes down to introducing symbolically engineered components foreign to the end-to-end distributional approach, and yet are less efficient and perform worse than a simple numerical calculator. Another way to put it is: why should numerical calculations be trainable in an LLM, once acknowledged that, one one side, training methods are highly inefficient and ineffective, and on the other, we have simple, well-understood, very efficient, and 100% correct formal methods for numerical calculation available in case we are ready to enhance a trained model with something else? Without an answer to this question, there is a risk of not addressing the problem at hand (numerical calculation in LLMs) with the right tools, introducing spurious concerns, while neglecting important dimensions. I believe that, due to the questionable motivation, the manuscript suffers from both (see other points below).\n2. Even if one disregards the motivations, one could claim that the results of the proposed method are relatively modest, with significant improvements with respect to the leading baseline only for multiplication and division, while performing significantly worse on the computation of the mean, and slightly worse on 3 other of the 8 tasks. This wouldn't be a problem *per se*, if these results were mobilized for descriptive purposes, giving solid insights on the mechanisms responsible for the different behaviors, instead of evidence for a normative goal (i.e., making the models better). However, the manuscript only reports the performance of varying representation strategies (Appendix D.3) without analysis of the possible reasons behind the difference in performance. And for the interesting case of the mean, it attributes the advantage to other methods, without evidence, to \"the fact that multi-token methods generate answers over multiple forward passes, which effectively enables a form of “reasoning”\", once again hiding behind cognitive metaphors the lack of understanding of the corresponding formal mechanisms.\n3. The desiderata advanced in section 3, supposed to justify the proposed methods, are introduced without sufficient discussion and, therefore, appear as a more or less arbitrary list of properties matching the solution (and not the other way round) instead of a coherent system of independent conditions (as in an axiomatic system), especially if one can raise doubts about the overall motivation of the paper, as discussed in the point 1 above. As an example, one could claim, unlike D1, that representing numbers with as many tokens as digits for some positional representation in some base (eg. representing the number 123 with the 3 consecutive tokens \"1\" \"2\" \"3\") is the most efficient way of representing numbers, making algorithmic properties readily available to computation (hence the importance of positional numerical systems since the Babylonians). Or that making numbers independent of geometry, unlike D3, actually frees arithmetic from geometric limitations, as centuries of abstract algebra have shown. All this can be debated, and depends on what is the point of computing arithmetical properties in one way or another, but the point is that there is nothing obvious in the desiderata proposed, which would then require further justification.\n4. A rigorous approach to understanding how LLMs perform arithmetical computation should not accept prompting (e.g., \"you are an expert in numeracy... do not explain...\") as a valid method, as none of this has a rigorous computational/formal status. I know this is widespread practice in the field, but I don't think that's a legitimate argument. I also know that not all models are open source, and there's no other way to explore them than prompting. But that's not a reason to accept prompting as a scientific methods, but a reason not to use those models for scientific purposes. I'm reviewing a scientific paper, not a commercial product. Can you be sure that a model didn't use a calculator tool just because you asked it to please not do it? If that can't be guaranteed formally, then any result coming from such an obscure procedure falls necessarily outside the domain of computer science and belongs to other areas of scientific knowledge, such as anthropology or psychology, or non-scientific, such as religion, or magic. Not having a clear motivation, as pointed in 1., hides the problem with this methodology.\n5. Since a central aspect of this paper has to do with the representation of numerical expressions and their mathematical processing in the framework of transformer models, it is surprising not to see any discussion of the work done by Charton on this topic ([eg1](https://arxiv.org/pdf/2308.15594), [eg2](https://arxiv.org/pdf/2306.15400),[eg3](https://arxiv.org/pdf/2211.00170), [eg4](https://arxiv.org/pdf/2112.01898)). Discussing his views and adopting some of his solutions could be precious for the work presented in this manuscript (Disclosure: I am neither Charton, nor any one of his co-authors).\n6. The manuscript presents the problem as a tokenization problem (\"We hypothesize that addressing this problem requires rethinking the way LLMs tokenize numbers.\"; and BitTokens is presented as \"a novel tokenization strategy\"), but the problem is clearly not one of tokenization, but of embedding. From a tokenization perspective, the solution proposed is trivial: every numerical expression is mapped to the same [NUM] token (which, incidentally, exposes the LLM to the risk of statistical inconsistency, cf. [Gastaldi et al., 2025](https://arxiv.org/abs/2407.11606)). It was not until page 3 or 4 that I understood that tokenization was not the issue. I suggest to remove any substantial reference to tokenization and frame the paper in terms of encodings, vector representations, or embeddings.\n7. Proposition 4.3 is expressed in rather informal terms (\"numbers\", \"states\" \"outputs\", \"read\", etc), in a way that it is difficult to follow the correctness of the proof proposed. For instance, it's not clear where the contradiction lies (because nowhere was explicitly claimed that the operator was injective), let alone the fact that a proof by contradiction might not be needed at all here, since an explicit bound is being found (e.g., a direct proof could claim that below that bound the operator is not injective, or sthg of the sort). The proof of the computational complexity is even less formal, so, while I think I understand the argument, I'm not sure I can follow the correctness of a proof. To avoid misunderstandings, I suggest either providing a more formal statement and proof, or presenting this more like an argument than like a formal result."}, "questions": {"value": "- Would you be ready to change the motivation of the paper to avoid appealing to obscure cognitive properties? If you were not allowed to appeal to cognitive metaphors to justify your work, how would you justify the use of your method over outsourcing numerical computation to a simple numerical algorithm? Could you provide formal or empirical evidence for whatever that justification would be?\n- Would you be ready to remove any claim about tokenization and frame this work exclusively in terms of encoding or embedding?\n- What is your justification for considering it a rigorous scientific method to politely ask LLMs to do something? What alternative methods can you imagine to make your work more rigorous?\n- What is the unified perspective that justifies the desiderata?\n- It is unclear to me what the number sampling is supposed to reflect. Is it actual distributions on real-life corpora? Cognitive numeracy? Formal principles of learnability? How is the \"increased difficulty\" that justifies oversampling operands with similar exponents judged? Difficulty is usually a function of the algorithmic implementation, which is largely unknown in the case of DNNs. It could also be that by learning easy cases, a model generalizes better, and this is being artificially prevented by the sampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "350r2qrCTi", "forum": "qzgro4i3sg", "replyto": "qzgro4i3sg", "signatures": ["ICLR.cc/2026/Conference/Submission5809/Reviewer_bJvs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5809/Reviewer_bJvs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994373467, "cdate": 1761994373467, "tmdate": 1762918273923, "mdate": 1762918273923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}