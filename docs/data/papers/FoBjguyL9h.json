{"id": "FoBjguyL9h", "number": 16505, "cdate": 1758265293321, "mdate": 1759897236632, "content": {"title": "Understanding Tool-Integrated Reasoning", "abstract": "We study why Tool-Integrated Reasoning (TIR) makes  Large Language Models (LLMs) more capable. While LLMs integrated with tools like Python code interpreters show great promise, a principled theory explaining why this paradigm is effective has been missing. \nThis work provides the first formal proof that TIR fundamentally expands an LLM's capabilities. We demonstrate that tools enable a strict expansion of the model's empirical and feasible support, breaking the capability ceiling of pure-text models by unlocking problem-solving strategies that are otherwise impossible or intractably verbose. To guide model behavior without compromising training stability and performance, we also introduce Advantage Shaping Policy Optimization (ASPO), a novel algorithm that directly modifies the advantage function to guide the policy behavior. We conduct comprehensive experiments on challenging mathematical benchmarks, leveraging a Python interpreter as the external tool. Our results show that the TIR model decisively outperforms its pure-text counterpart on the pass@$k$ metric. Crucially, this advantage is not confined to computationally-intensive problems but extends to those requiring significant abstract insight. We further identify the emergent cognitive patterns that illustrate how models learn to *think with tools*. Finally, we report improved tool usage behavior with early code invocation and much more interactive turns with ASPO. Overall, our work provides the first principled explanation for TIR's success, shifting the focus from the mere fact *that* tools work to *why* and *how* they enable more powerful reasoning.", "tldr": "", "keywords": ["Large Language Models", "Tool-Integrated Reasoning", "Reinforcement Learning", "Advantage Shaping Policy Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/06467dedde8358e3734aa40c9eb2000029c0c67d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper examines tool-integrated reasoning (TIR) for LLMs and introduces ASPO, a training tweak that adds a clipped/normalized term to the advantage to prefer earlier and shorter tool calls (e.g., code execution, verifier). The theoretical sections frame tools as expanding both the support set (what solutions are reachable at all) and the feasible support set under finite token budgets, arguing that external computation can compress long textual derivations into short calls, thereby enlarging reachable solutions without increasing sequence length. The algorithmic claim is that naive reward-level shaping is unstable in GRPO-like pipelines, whereas advantage-level shaping (ASPO) is simpler and more stable in practice. Empirically, the paper reports pass@k gains on math benchmarks (AIME24 AIME 25, Omni-MATH), alongside qualitative evidence that policies learn to invoke tools earlier."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Understanding when/why tools help LLM reasoning is important and practically relevant.\n2. The support set / feasible support set expansion lens offers an intuitive way to talk about why tools could enlarge reachable solutions under token budgets.\n3. Improvements are reported across several math-reasoning benchmarks, suggesting the approach is not dataset-specific."}, "weaknesses": {"value": "1. Although the paper positions its theoretical proofs as a primary contribution (as stated in the Contributions), the core results largely repackage a self-evident intuition: if a deterministic tool can collapse a long derivation into a one-step call, its reachable set will strictly dominate text-only generation. The theorems read more like formal wrapping of an intuition than analysis that meaningfully narrows uncertainty about real systems.\n2.  ASPO modifies the advantage with a clipping/normalization term tied to “earliness” and tool length. Unlike potential-based shaping (which preserves optimal policies), this construction can change the fixed point of policy optimization. Will it incentivize spurious early tool calls in tasks that do not need tools?"}, "questions": {"value": "See my comments on weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wiKFA56zUg", "forum": "FoBjguyL9h", "replyto": "FoBjguyL9h", "signatures": ["ICLR.cc/2026/Conference/Submission16505/Reviewer_zSw2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16505/Reviewer_zSw2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632004603, "cdate": 1761632004603, "tmdate": 1762926598698, "mdate": 1762926598698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper promotes tool usage (specifically, a Python interpreter) as a general framework for LLM reasoning. It first establishes two theoretical results: (1) Tool-Integrated Reasoning (TIR) can solve a strictly larger class of problems than text-only reasoning, and (2) TIR achieves greater token efficiency in reasoning. On the implementation side, the paper introduces Advantage Shaping Policy Optimization (ASPO), a GRPO-derived reinforcement learning algorithm designed to stabilize TIR training. ASPO incorporates an auxiliary reward that encourages early tool usage by modulating the advantage rather than directly modifying the reward. Experimentally, TIR significantly outperforms text-only RL across standard mathematical benchmarks beyond programming-related tasks. Notably, these gains are not limited to improved arithmetic accuracy, as similar improvements appear in abstract, non-computational problems. Qualitative analysis further reveals distinct reasoning behaviors induced by TIR, including insight-to-computation transformation, exploration and verification through code, and offloading of complex calculations."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tThe theoretical analysis in Section 3, while with concerns on practicality as discussed in the Weaknesses, paves motivation to consider TIR as general reasoning framework, especially considering token efficiency.\n-\tThe proposed RL method (ASPO) not only enhances downstream performance but also elicits cognitive patterns essential for establishing TIR as a general reasoning framework rather than a mere computational aid.\n-\tThe experiments are also thoroughly design and conducted not only to verify benefit of TIR and ASPO, but also to rule out null hypotheses via in depth quantitative and qualitative analysis.\n-\tPersonally, I find the algorithmic friendliness wise scoring in Section 4.2 and qualitative study in Section 4.3 revealing various cognitive patterns persuading enough to consider TIR as a general reasoning framework beyond mere calculator assistance."}, "weaknesses": {"value": "-\tIn Section 3.1.2, the proof of strictness (suggesting that tool augmentation can handle a strictly larger class of problems than text-only LLMs) relies solely on random oracle problems. However, random oracles are not representative of the kinds of problems that LLM-based reasoners are intended to solve. Consequently, it remains uncertain whether the identified strictness region meaningfully extends to practical reasoning or real-world computational tasks.\n-\tIn Section 3.2.2, the proof of strictness (claiming that tool augmentation improves the token efficiency of text-only LLMs for any non-trivial algorithmic problem) assumes that program literals—i.e., code representations—are inherently more concise than the execution traces a text-only LLM must produce to simulate them. While this assumption generally holds, it overlooks the computational cost of executing the tool itself. The reasoning may be reasonable for lightweight tools such as Python interpreters, but it becomes less convincing when the tool involves any resource-intensive process. Because the proof treats all tools equivalently, its notion of token efficiency becomes less meaningful in realistic scenarios.\n-\tIn both Section 3.1.2 and 3.2.2, the proof of inclusion follows from that a tool-augmented model can always choose to not use it. However, that is only true in theory; by incorporating another interface (tool), the model should decide when and not to use it. Without proper selection capacity, the tool-augmented model can and often underperform models without it in certain domains.\n-\tMinor point: the first and second paragraphs of Section 2 do not connect well; the first covers RL + tool use, while the second explains RL for LLM without tool use."}, "questions": {"value": "-\tI am a bit confused on the baseline for experiment in Section 4.1: Is the Qwen3-8B baseline also fine-tuned with the same dataset as TIR? If then, it should be notified otherwise (e.g. Text-Only RL) to differentiate it from vanilla Qwen3-8B.\n-\tIs there a validation process for Gemini’s aptness as classifier for algorithmic friendliness in Section 4.2? Without validation, we do not know how correct the friendliness score label Gemini generated is."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wNubnjDLID", "forum": "FoBjguyL9h", "replyto": "FoBjguyL9h", "signatures": ["ICLR.cc/2026/Conference/Submission16505/Reviewer_mLPx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16505/Reviewer_mLPx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637125685, "cdate": 1761637125685, "tmdate": 1762926598275, "mdate": 1762926598275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper study how tool use can improve large language model capabilities, both from a theoretical perspective and an empirical one. First, the paper introduces two theorems to characterize how tool-use enhance what LLMs can generate. The first theorem shows that, in general, the support of the distribution of the tool augmented LLM is strictly larger than the one of the text-only LLM (the support of the distribution is defined as the sequences of text with a probability higher than epsilon). The proof rely on using a random oracle as tool, stating that the pure text LLM cannot model the mapping of the random oracle. Then, the authors state a second theorem, showing the superiority of tool augmented LLM under token constraint. This theorem states that under a constraint budget B, and for a non trivial class of problems, there exists a problem size N such that the support of the tool augmented LLM contains one solution of for each problem, while this is not the case for the text only model.\n\nThen, the authors perform an empirical evaluation of tool augmented LLM in the context of reasoning, showing that adding a python interpreter enhance the capabilities of the model for solving math problems. More specifically, the authors performed reinforcement learning algorithm, using the Qwen3-8B model, with and without the use of a python interpreter, on a subset of the DAPO dataset (which comprises math problems). Then, the two models are compared the following benchmarks: AIME24, AIME25 and a subset of Omni-MATH. These experiments show that having access to a python interpreter leads to significantly better results these benchmarks, hinting that it improves the reasoning abilities of the model. Then, the authors explore whether tool augmentation only helps for computation intensive problems or not. To do so, the problems of the Omni-MATH benchmark are splitted into five buckets, based on how much solving these problems rely on computation vs. abstract reasoning. It shows that tool-use improve for all class of problems, and not only for the more computational ones. The last contribution of the paper is a modification of the GRPO algorithm, called ASPO, to elicit the use of tool early in the reasoning. Instead of modifying the reward, which leads to reward hacking, the authors proposed to modify the algorithm itself. They show that with this variant of GRPO, the LLM tends to use the tool more, and earlier in the reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, I am a bit ambivalent about this paper. I believe that the experimental section is strong, and in particular, the results showing that using a python interpreter strongly improve the reasoning abilities of an LLM a interesting (with the small caveat here, that I believe that all the problems considered in the benchmarks have a numerical answer, hence being probably more prone to reasoning with interpreter than problems that have a symbolic answer. In particular, I am wondering whether the tool augmented LLM can guess the answer more often, instead of proving that the answer is correct). I also liked the ablation where authors investigate if tool-use also work for more \"abstract\" problems vs. computational one. Finally, I think that the modification of the GRPO algorithm is interesting compared to changing the reward, although I am not convinced this is really useful to \"force\" the model to use the tool more in the setting considered in the article."}, "weaknesses": {"value": "On the other hand, I am really not convinced by the theoretical results, and I believe that they are a big weakness of the paper. Here are my main concerns with these results.\n\nFirst, I do not believe that the theorems and proofs are \"formal\" results. For example, some terms are not defined properly, making the theorems vague (eg, what are \"non trivial algorithmic problems\"?). While I understand the general ideas behind these theorems, I do not believe that they are correct under the current assumptions. For example, to prove that the support of the distribution of the text model in included in the support of the distribution of the tool model, the authors state that\n\n> The tool-integrated model pTIR can generate this same trajectory by adopting a policy of never invoking the external oracle\n\nHowever, this is not generally true for all tool-augmented models. Some models might actually use the tool every time, depending on how these were trained. More generally, since the distribution is normalized, if some sequences have a higher probability under the tool-augmented model, this means that other sequences have a smaller probability, which likely means that the support of the text only distribution is not included. In particular, it is likely that the probability of \"incorrect answers\" for certain problems is smaller for the tool augmented LLM, and thus that there is not an inclusion of supp(p_text) in supp(p_tool).\n\nSecond, the proof of the first theorem relies on a \"random oracle\", which is an idealized object that does not exist in practice. So basically, the theorem is only true for a \"tool\" that actually cannot be instatiated in practice. I think that adding some assumptions on the tool would make the theorem more interesting (but also probably much harder to formalize).\n\nFinally, I also have some concern regarding the significance of the second theorem (about superiority of tool LLM wrt. to text LLM): as currently stated, the theorem assume that calling the tool is computationally free, which is not the case in practice. In particular, it is true that there exists problems for which the solution would require a linear or quadratic solution in terms of number of tokens. But this is also true for the computational complexity of the tool. Thus, I do not believe that only considering the length of the query of the tool is a good way to compare tool augmented LLM with text only ones.\n\nOverall, I do not believe that the theoretical sections add much value to the paper. In particular, I think that the arguments made are informal at best, some of them probably being incorrect as they are currently stated."}, "questions": {"value": "Are all problems considered in the paper have a numerical answer?\n\nDo you have an idea whether sometimes the model \"guess\" then answer thanks to the interpreter, without actually proving that this is the correct answer?\n\nIn Figure 1, both models are trained with RL in the same way, starting from Qwen 3, right? Are there different prompts to elicit tool use for the tool augmented model?\n\nIn section 4.4, what is the baseline? Is it GRPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KPxURN3JQP", "forum": "FoBjguyL9h", "replyto": "FoBjguyL9h", "signatures": ["ICLR.cc/2026/Conference/Submission16505/Reviewer_ZcNa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16505/Reviewer_ZcNa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909831720, "cdate": 1761909831720, "tmdate": 1762926597799, "mdate": 1762926597799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides the first formal proof that Tool-Integrated Reasoning (TIR) strictly expands a Large Language Model's (LLM) capabilities, breaking the constraints of pure-text models. The authors prove that tools expand both the empirical support (unlocking trajectories with previously negligible or zero probability) and the feasible support (making complex algorithms practically achievable via \"token efficiency,\" which would be intractably verbose in natural language). \n\nThe paper also introduces Advantage Shaping Policy Optimization (ASPO), a novel algorithm that stably guides model behavior by directly modifying the advantage function, overcoming the instability of traditional reward-based methods. Using the pass@k metric on mathematical benchmarks, the TIR model's performance improvement is evaluated against a pure-text variant. Further deep dives show TIR's benefits are universal, extending even to abstract problems , and identify emergent cognitive patterns of tool usage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "ASPO, a new approach is proposed to handle the practical limitations of existing methods for promoting tool usage in the model.\n\nProvides the first formal proof explaining why Tool-Integrated Reasoning (TIR) works, demonstrating it strictly expands an LLM's \"empirical\" and \"feasible\" support to overcome the \"invisible leash\" of pure-text models.\n\nAdvocates for a paradigm shift in viewing LLMs: not as monolithic problem-solvers, but as core reasoning engines that intelligently delegate computational tasks to efficient, specialized tools.\n\nCategorizing the problems and demonstrating minimal 'Capability Shrinkage' certainly improves the confidence in the results and a higher performance of the TIR model for problems with high friendliness makes this a very motivating solution. Demonstrating the changes in the model's behaviour as a result of ASPO under sec 4.3 puts the approach into perspective."}, "weaknesses": {"value": "The paper's experiments, while strong, are confined to a single tool (Python interpreter), a single problem domain (mathematical reasoning), and a single base model (Qwen3-8B). This limited scope means the conclusions about the universal benefits of TIR, its generalizability to other tools (like search engines) or domains, and the robustness of the ASPO algorithm across different model families and scales are not fully demonstrated. Authors may want to explicitly state about this limitation, and how they argue that their conclusion is still has external validity of the results. \n\nThe paper's own analysis reveals a 1.8% \"Capability Shrinkage,\" where the pure-text model solved problems that the TIR model could not (Figure 2). Will be really good if the authors can provide examples or qualitative analysis of why the TIR model failed on these problems. This will help understand the new failure modes better and appreciate the results better. \n\nSec L in the appendix indicates that the improvement in the performance is much higher for higher friendliness indicating that the support expansion may not necessarily translate into higher performance. Authors may want to explain this in the draft."}, "questions": {"value": "Can we explore how the 'temperature' of the model affects the improvement in performance? Can authors mention something about this in the draft?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lt8qdRnGmz", "forum": "FoBjguyL9h", "replyto": "FoBjguyL9h", "signatures": ["ICLR.cc/2026/Conference/Submission16505/Reviewer_nNhn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16505/Reviewer_nNhn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957215377, "cdate": 1761957215377, "tmdate": 1762926597145, "mdate": 1762926597145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}