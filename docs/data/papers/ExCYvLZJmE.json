{"id": "ExCYvLZJmE", "number": 20233, "cdate": 1758303943904, "mdate": 1763704472604, "content": {"title": "When Case Gets Rare: A Retrieval Benchmark for Off-Guideline Medical Question Answering", "abstract": "Across medical specialties, clinical practice is anchored in evidence-based guidelines that codify best studied diagnostic and treatment pathways. These pathways work well for the majority of patients but routinely fall short for the long tail of real-world care not covered by guidelines. Most medical large language models (LLMs), however, are trained to encode common, guideline-focused medical knowledge in their parameters. Current evaluations test models primarily on recalling and reasoning with this memorized content, often in multiple-choice settings. Given the fundamental importance of evidence-based reasoning in medicine, it is neither feasible nor reliable to depend on such memorization in practice. To address this gap, we introduce OGCaReBench, a long-form retrieval-focused benchmark aimed at evaluating LLMs at answering clinical questions that require going beyond typical guidelines. Extracted from published medical case reports and validated by medical professionals, OGCaReBench contains long-form clinical questions requiring free-text answers, providing a systematic framework for assessing open-ended medical reasoning in rare, case-based scenarios. Our experiments reveal that even the best-performing baseline (GPT-o3-mini) correctly answers only 51% of our benchmark with open-source models only reaching 36%. Augmenting the models with retrieved medical articles improves this performance to up to 75% (using GPT-5) highlighting the importance of evidence-grounding for real-world medical reasoning tasks. OGCaReBench thus establishes a foundation for benchmarking and advancing both general-purpose and medical language models to produce reliable answers in challenging clinical contexts.", "tldr": "We present OGCaReBench, a rare-case based physician-validated medical QA benchmark. RAG significantly improves performance upon retrieval quality, context processing, and reasoning.", "keywords": ["medical LLMs", "retrieval augmentation", "benchmarking"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36a96d70050e084131cf0dc4a8b20e29c90041a7.pdf", "supplementary_material": "/attachment/e2049453f31b13c44c4be58d10d0e7bf8db847d0.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a new benchmark and dataset derived from published case reports. The benchmark is filtered by physicians to ensure validity and relevance to the task defined by the authors. The resulting evaluation is then used to obtain a baseline for a variety of models including closed and open models. The authors then augment the models by using RAG over the case reports dataset. Different embedding models are used and compared. Overall, this work shows that adding RAG improves the performance on the OGCAREBENCH evaluation compared to the models without RAG."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The dataset curated by the authors has merit and the question generation pipeline is robust. The experimental results are clear and demonstrate the relevance of RAG for improving performance on under represented data such as rare medical cases. Including multiple embedding models supports the need for adapted models and suggest that focusing on improving RAG may be a more viable approach than training LLMs for domain specialization."}, "weaknesses": {"value": "The assumptions made by the authors on what the goals of care are and how clinicians practice medicine are questionable. Rare cases are by definition rare and often have non-specific presentations shared with more common diseases [1]. The assumption that the best next step for a known rare case is the best next step in a real-world encounter is incorrect. For example, if a 20 year old male presents to the ER with shortness of breath and imaging shows a pneumothorax, the optimal diagnostic step is not to look for FLCN genetic mutations whereas that would be the case if in a Birt Hogg Dube case report [2].\n\nThis misalignment raises questions about the applicability of a system focusing on rare cases. For rare kidney diseases, a biopsy is the gold standard to make the diagnosis [3], would using this system in real world settings suggest a biopsy for every patient presenting with proteinuria or altered kidney function? Does the system rely on the clinician's intuition of what is or isn't a rare case? What are the cost and outcome implications of a system proposing advanced interventions too quickly?\n\nThis misalignment adds to the existing disconnect between lab research and the reality of clinical practice. While the experience is interesting, I do not find the method, system, and results to be novel enough to compensate the lack of real-world relevance and insufficient framing as is.\n\nConsidering the baseline and comparison, having a baseline of classic inference is necessary but insufficient, to justify using this RAG approach, it should be compared to other RAG systems, GPT-5 with web search for example, deepsearch, and OpenEvidence would be more accurate comparisons.\n\nThe inclusion of OpenBioLLM is a major issue for me [4]. A model released without any information, paper or data description making bold claims of achieving SOTA should not be included in any scientific work especially given the results reported in this paper.\n\n# References\n\n[1] Rare inherited kidney diseases: challenges, opportunities, and perspectives (Devuyst et al. 2014)\n\n[2] Birt-Hogg-Dube Syndrome (Crane et al. 2023)\n\n[3] The Kidney, (Brenner., and Rector. 2019)\n\n[4] aaditya/Llama3-OpenBioLLM-70B (2024)"}, "questions": {"value": "# Suggestions/Questions\n\n1) Include in the baseline the performance of base models with web search usage (GPT-5 Search, Claude with web use, deepsearch).\n\n2) I recommend removing OpenBioLLM from the paper and in general to avoid models from unknown sources without data/technical report.\n\n3) Evaluate the performance of the system on common cases, does it make rare suggestions? Could you quantify the incidence of the cases in the evaluation dataset and quantify the pre- and post-test likelihood of the correct answer? In addition, I would like to see a benefit/risk assessment beyond accuracy. For example, missing causa equina syndrome has more impact than missing a birt hogg dube diagnosis. Likewise, performing a kidney biopsy on a healthy patient is more risky than a blood sample. Finding the optimal benefit/risk ratio is the primary objective in clinical practice, not accuracy [1].\n\n4) An expert baseline on the benchmark would help put into perspective the performance of the system.\n\n5) Was any method used to ensure the absence of contamination between the evaluation cases and the cases included in the RAG dataset?\n\n# References\n\n[1]  Comparing diagnostic tests on benefit-risk (Pennello et al., 2016)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "95mTYpCC3X", "forum": "ExCYvLZJmE", "replyto": "ExCYvLZJmE", "signatures": ["ICLR.cc/2026/Conference/Submission20233/Reviewer_ASoa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20233/Reviewer_ASoa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761344739671, "cdate": 1761344739671, "tmdate": 1762933729260, "mdate": 1762933729260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OGCAREBENCH, a benchmark designed to evaluate large language models (LLMs) on rare or off-guideline medical cases. The benchmark is constructed from over 53,000 open-access medical case reports and includes 235 curated question–answer pairs validated by physicians. The dataset focuses on long-form, retrieval-based question answering where guideline-based reasoning is insufficient. Experiments compare several general-purpose and medical LLMs—both with and without retrieval augmentation (RAG)—demonstrating that even advanced models like GPT-5 struggle without retrieval (≈45–50% accuracy) but achieve substantial gains (up to 75%) when augmented with relevant case report retrieval. The paper argues that reliable clinical LLMs must move beyond memorized knowledge and toward retrieval-grounded reasoning to handle real-world, rare patient scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Novel focus on off-guideline cases**:\nThe paper highlights an important but underexplored problem—how LLMs perform when faced with rare, atypical medical scenarios not covered by standard clinical guidelines. This makes the benchmark highly relevant to the deployment of LLMs in clinical support settings.\n\n**Robust dataset design and validation**:\nThe authors construct OGCAREBENCH using a clear multi-step pipeline involving filtering, controlled question modifications, and expert validation. The attention to medical plausibility and domain fidelity adds credibility to the dataset’s reliability.\n\n**Comprehensive experimental evaluation**:\nThe paper thoroughly evaluates multiple general-purpose and domain-specific models under both baseline and retrieval-augmented conditions. The inclusion of 15 retrieval models, from BM25 to biomedical-specific retrievers, offers a valuable empirical contribution to RAG research in healthcare.\n\n**Clear demonstration of retrieval importance**:\nResults convincingly show that RAG significantly enhances reasoning accuracy for rare medical cases, underscoring a key insight: parametric memory alone is insufficient for safe and effective medical reasoning."}, "weaknesses": {"value": "**Limited methodological novelty**:\nThe work primarily focuses on dataset creation and empirical benchmarking rather than proposing a new retrieval or reasoning framework. This makes it somewhat engineering-heavy and evaluation-oriented, which may not align well with ICLR’s focus on algorithmic or representational innovation.\n\n**Scale and representativeness concerns**:\nDespite using over 50,000 case reports as the retrieval corpus, the final benchmark contains only 235 validated instances. This relatively small size raises questions about statistical robustness and whether the benchmark adequately covers the diversity of real-world rare cases.\n\n**Evaluation dependency on GPT-based judging**:\nThe reliance on GPT-4o as an automatic evaluator introduces potential bias and inconsistency in clinical correctness judgments. The limited physician cross-validation (45 samples) may not be sufficient to confirm reliability across all cases.\n\n**Shallow analysis of model failure modes**:\nWhile quantitative results are extensive, the paper lacks qualitative insights into why models fail on certain rare cases—whether due to retrieval errors, reasoning gaps, or hallucinated procedures. This limits the interpretability of the findings."}, "questions": {"value": "**Venue suitability (ICLR relevance)**:\nThe contribution centers on dataset construction and empirical evaluation, not on learning mechanisms or model training. It may better fit NLP or medical informatics venues (e.g., ACL, EMNLP) rather than ICLR, which emphasizes theoretical and representational advances.\n\n**Benchmark longevity and updateability**:\nSince medical knowledge evolves rapidly, the benchmark may require frequent updates to remain relevant. The paper does not discuss mechanisms for versioning, reannotation, or handling outdated clinical knowledge.\n\n**Clinical validation and real-world utility**:\nWhile the dataset is well-validated at construction time (Section 3.1 Step 1~4), it remains unclear how the benchmark correlates with actual clinical decision-making outcomes. Without external validation in real medical workflows, practical impact remains speculative. Furthermore, the authors did not provide any information about the annotators (i.e., three physicians)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QE3oGGaQDH", "forum": "ExCYvLZJmE", "replyto": "ExCYvLZJmE", "signatures": ["ICLR.cc/2026/Conference/Submission20233/Reviewer_uPgu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20233/Reviewer_uPgu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567844297, "cdate": 1761567844297, "tmdate": 1762933728447, "mdate": 1762933728447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Problem & Motivation:**\nThe paper identifies a gap between the training of most medical LLMs and the demands of real-world clinical practice. Current medical LLMs are primarily trained on and evaluated against common, \"guideline-focused\" medical knowledge, often using multiple-choice question formats. This reliance on parametric memorization is insufficient for the \"long tail\" of clinical care, where physicians encounter rare or \"off-guideline\" cases not covered by standard pathways. In these scenarios, evidence-based reasoning, which requires dynamically consulting external sources (like case reports), is essential. The authors argue that current benchmarks fail to test this critical skill, rarely evaluating whether models can generate expert-level, long-form answers grounded in retrieved evidence for complex, rare cases.\n\n**Method:**\nTo address this gap, the paper introduces OGCAREBENCH, a long-form, retrieval-focused benchmark designed to evaluate LLMs on realistic, \"off-guideline\" clinical questions derived from medical case reports. The dataset's creation involves a four-step, semi-automatic process:\n- *Corpus & Case Filtering:* A retrieval corpus of 53,617 case reports was collected from PubMed Central. From this, a subset of 28,219 reports was filtered by excluding older reports (published ≤ 2022) and those with high citation counts (indicating the case may have become standard knowledge).\n- *Q&A Extraction:* An LLM (GPT-4o) was used on a 1,100-report subset to extract a timeline of the case and its significant contribution (e.g., a novel diagnosis, rare treatment). A question-answer pair was then generated, with the question detailing the case up to the decision point and the answer being the contribution (the novel step taken).\n- *Question Modification:* To simulate a realistic clinical scenario where a new patient resembles but is not identical to a published case, the extracted questions were modified by another LLM (Claude 4 Opus). This involved adding \"distractors\" like altered demographics, comorbidities, or synonymous medical terms, making the question distinct from the source text.\n- *Physician Validation:* The final modified Q&A pairs were validated by physicians, who rated them on medical alignment and difficulty. Only pairs requiring expert-level knowledge (rated 4 or 5 out of 5) were retained, resulting in the final benchmark of 235 validated cases.\n\n**Experimental Setup:**\nThe benchmark is evaluated in two settings:\n- Baseline (Memorization): Models answer questions using only their parametric knowledge.\n- RAG (Retrieval-Augmented): Models are provided with relevant case reports retrieved from the 53k corpus to ground their answers.\nA mix of general-purpose (e.g., GPT-5, GPT-03-mini, Llama 3.3) and medical-specific (e.g., MedGemma, Llama 3-Med42) models were tested.\n\n**Results & Findings:**\nThe results demonstrate the benchmark's effectiveness in highlighting the limitations of memorization.\nWithout retrieval, even the best-performing model (GPT-03-mini) answered only 51.5% of questions correctly. Open-source models were lower, with MedGemma at 36.2%. This confirms that current models have not memorized this long-tail, rare-case knowledge.\nWhen augmented with retrieved case reports, performance increased significantly. The top-performing model (GPT-5 with RAG) achieved 75.3% accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The task of answering rare, off-guideline clinical questions is highly relevant.\n- Timeline extraction and question reformulation by presenting all procedures preceding the decision point.\n- A broad spectrum of models—domain-specialized and not—is benchmarked, including 8 LLMs, 14 semantic retrievers, and BM25."}, "weaknesses": {"value": "- *Limited methodological novelty.* The primary contribution is the use of case reports as a source, not the development of a novel benchmark methodology. The resource is constructed by filtering and sampling (with poor control) an existing PubMed Central corpus, applying simple semi-automatic LLM-based extraction, and performing (incomplete) manual verification.\n- *Superficial comparison to existing work.* For a resource-centric paper, the comparison to the existing literature is insufficient. It lacks a detailed, quantitative comparison table positioning OGCAREBENCH against other benchmarks (especially those already using case reports) across key dimensions (e.g., scale, task, validation rigor). This makes it difficult to assess the true novelty or \"delta\" provided by this work.\n- *Questionable sampling and unbalanced dataset.*\n   - Sampling. A \"pure random sampling\" of 1,100 reports (from which only 235 are finalized) is inadequate. Stratified sampling (e.g., by specialty, contribution type) would have been more rigorous.\n   - Scale. The final expert-verified dataset of 235 questions is extremely small.\n   - Balance. The dataset is highly unbalanced, with 70% of cases coming from only two specialties, a distribution that does not reflect the source corpus. The authors also fail to quantify the distribution of \"contribution types\" (diagnosis, treatment, etc.), a dimension they claim is a key part of their novelty.\n- *Critically insufficient expert validation.* The validation process, particularly for a sensitive medical domain, is inadequate and falls well below standard scientific practice.\n   - The utility of case reports is justified anecdotally via \"informal interviews with 10 physicians,\" lacking any structured evidence or detailed findings.\n   - The \"distractor\" modifications are a delicate process where errors could invalidate the answer. The complete list of modification types is not reported in the main paper, only two examples are reported. The authors state this stage was verified by three physicians for only an unspecified subset of questions, not the entire dataset. The subset size, selection criteria, provided instructions, verification results, and inter-annotator agreement are not provided.\n   - Some details about instructions and annotations criteria are provided only for step 4 but, again, they appear incomplete and poorly designed. It seems that each report has been evaluated by one physician only. One quality dimension only (realism). As for the 1-5 Likert scale, the authors only provide definitions for 1 (unrealistic) and 5 (realistic), leaving intermediate values (2, 3, 4) to subjective interpretation.\n- *Over-reliance on LLM-as-a-Judge.* The primary evaluation metric is an \"LLM-as-a-Judge\" (GPT-4o) for equivalence. Physician validation of this judge was performed on a very small subsample (45/235). While the 93% agreement is noted, the process, again, lacks detail.\n- *Predictable RAG findings due to circular experimental design.* A primary conclusion is that RAG enhances performance. Assuming a successful retrieval and considering the nature of the applied modifications in QA generation, this finding is mostly an anticipated consequence of the experimental setup, where the benchmark's questions were derived directly from the retrieval corpus.\n- *Questionable novelty of QA format.* The paper criticizes multiple-choice question benchmarks (e.g., MedQA, MedMCQA, PubMedQA) while advocating for its long-form questions and open-ended answers. However, the \"open-ended\" answers are exceptionally short (29 tokens on average), resembling simple verbalizations of a multiple-choice correct option. This raises doubts about whether this format truly evaluates the \"open-ended reasoning\" the authors claim is necessary. As other researchers have already created open-ended versions of benchmarks like MedQA by verbalizing correct answers with and without LLMs, the QA format, as implemented, does not appear to be a significant novel identity factor for this resource.\n- *Insufficient depth in model and retriever analysis.* A deeper breakdown of error modes (e.g., where RAG fails despite correct retrieval, or which case types defeat open-source models) would be valuable.\n- *Missing technical details.* No prompt or context engineering discussion, LLM decoding strategies information (crucial for results interpretation), and any statistical significance tests to validate the results.\n- *Presentation quality.* The manuscript's quality is limited, featuring low-resolution, non-vectorial figures with default Draw.io colors, poorly organized tables, inconsistent notation (e.g., use of commas for thousands), and repeated acronym definitions.\n- *Missing license.* The authors state the dataset and code will be publicly released but do not specify the license, which is a critical omission for a resource paper.\n- *Domain drift and dataset maintenance.* In Appendix, the authors recognize that the benchmark's relevance may erode as some rare cases become standardized, yet there is limited technical prescription for maintaining dataset relevance or tracking drift over time."}, "questions": {"value": "- How robust is the LLM-based evaluation metric to changes in prompting, underlying judge model, or model drift over time?\n- Can the authors expand the analysis of error modes—for example, cases where retrieval finds the correct document but LLMs misinterpret, or vice versa?\n- What is the observed impact of distractor additions on model/retriever confusion rates? Can the authors share specific ablations or examples where distractors led to errors, to clarify how realistic modifications challenge current systems?\n- What steps could be taken to dynamically update or maintain the relevance and challenge of rare-case benchmarks as some cases become incorporated into guidelines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1ARoLnHLZ6", "forum": "ExCYvLZJmE", "replyto": "ExCYvLZJmE", "signatures": ["ICLR.cc/2026/Conference/Submission20233/Reviewer_wxnk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20233/Reviewer_wxnk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992175965, "cdate": 1761992175965, "tmdate": 1762933727420, "mdate": 1762933727420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OGCaReBench, i.e. a benchmark to evaluate LLM and RAG performance on questions derived from rare clinical case studies. To this end, a dataset is constructed semi-automatically by mining publicly available case reports, converting them into QA format and altering non-relevant parts of the report. A suite of LLMs and retrievers is evaluated, suggesting that LLMs alone struggle with the benchmark, as evaluated by a (validated) judge LLM, but retrieval improves performance - with access to original documents which the QA pair is derived from, accuracy is very high, suggesting that the problem is mostly that of retrieval."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written, i think overall the research is largely well executed. I appreciate the validation of LLM results with human annotators."}, "weaknesses": {"value": "I find no big weaknesses with the execution of the research, only a few remarks:\n\n- it would be great to have more details regarding the protocols used for human validation (both the validity of the QA pairs as well as the LLM as a judge).\n\n- It seems like the problem is mostly with retrieval - given that the larger LLMs have very big context windows, I'm not sure why the RAG experiments reported in table 7 stop at 5 documents. It would be good to see results if the context window is maxxed out for each LLM.\n\n- Given the rather small dataset size, it would be good to have statistical significance reporting, in form of statistical significance tests as well as confidence intervals, to contextualise the amount of statistical uncertainty \n\nThat being said, I do question the motivation and therefore the overall contribution of the paper a bit. \n\nThe use case appears to be very applied, therefore it should be of interest for domain experts who could use such technology (i.e. physicians). But then, QA is only a proxy of the actual task at hand, that is helping to find the appropriate course of action for a patient, which shouldn't be evaluated in QA format but rather as the actual task, where physicians are (or aren't) supported by technology (such as LLMs) to inform their action. Probably, the findings would be of more interest for audiences of medical (informatics) journals, who could also more rigorously judge the significance and validity of the research, rather than an AI conference. \n\nLooking at the QA formulation, the findings are somewhat plain - LLMs struggle to answer questions about rare cases and using RAG (and retrieving very similar cases) improves performance. I don't find this finding particularly exciting. In order to tease interest for more AI/CS relevant audiences, it would be good to see analyses investigating the root causes, failure modes and potential avenues for or actual demonstrated methods of improvement."}, "questions": {"value": "Please address my minor remarks stated in the weaknesses.\n\nAlso, did you tamper with the submission template? The margins are smaller compared to the official template."}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "no report how annotators were compensated"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zOFhTvbaKh", "forum": "ExCYvLZJmE", "replyto": "ExCYvLZJmE", "signatures": ["ICLR.cc/2026/Conference/Submission20233/Reviewer_hJvx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20233/Reviewer_hJvx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762234154964, "cdate": 1762234154964, "tmdate": 1762933726939, "mdate": 1762933726939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comments"}, "comment": {"value": "**1. Statistical analysis for dataset size**\n\nWe used bootstrap analysis, which is a resampling method of repeatedly drawing samples with replacement to approximate the sampling distribution, to confirm that our benchmark shows stable results. 1000 bootstrap sampling of both the baseline and BGE retriever with 3 reports as context confirms that our benchmark has a stable performance estimate. The bootstrap mean is nearly identical to the reported accuracy. This shows that our clinically rich benchmark of 235 questions is sufficient for model comparison. \n\n**Baseline**\n| Model | Reported Accuracy | Bootstrap Mean | 95% CI | \n|-------|------|------|--------|\n| GPT-5 | 0.447 | 0.447 | 0.383–0.515 |\n| GPT-o3-mini | 0.515 | 0.512 | 0.447–0.574 |\n| MedGemma-27b-text-it | 0.362 | 0.361 | 0.302–0.421 |\n| Llama3-Med42-70B | 0.421 | 0.421 | 0.357–0.485 |\n\n**Retriever: BGE, Context Length: 3 reports**\n| Model | Reported Accuracy | Bootstrap Mean | 95% CI | \n|-------|------|------|--------|\n| GPT-5 | 0.728 | 0.728 | 0.672–0.783 |\n| GPT-o3-mini | 0.719 | 0.720 | 0.664–0.779 |\n| MedGemma-27b-text-it | 0.630 | 0.629 | 0.570–0.694 |\n| Llama 3-Med42-70B  | 0.562 | 0.562 | 0.498–0.626 |\n\n\n**2. LLM-as-a-judge validation and instructions for the annotators**\n\nFollowing is the instruction for the human annotators for the LLM-as-a-judge quality validation. \n\nThis was the instruction to the model: \n\n>- Consider them equivalent if they refer to the same kind of medical action, even if wording differs. If the main medical procedure is similar and other details somewhat aligns, two texts are equivalent. \n>- Mark as mismatch if the response suggests a different type or intent of medical procedure. If two texts include similar medical procedure but their importance differs greatly, or their main medical procedure differs, mark as mismatch. If one suggests a broad method and the other specifically mention one of the methods involved in the broad method, mark as mismatch. \n>- Some texts have reasons or rationale explaining their main content. Do not use this part to determine equivalence or mismatch.\n>\n>Please comment on the annotation column if this content is labeled correctly according to the description above. It will be a binary classification of yes/no. \n\nUsing the Wilson confidence interval, the 95% confidence interval is 82.1%-97.7%. This surpasses the agreement levels in MT-Bench [1], which has human-human agreement of approximately 81% and LLM-human agreement of above 80%. \n\n\n**3. Contingency table of retrieval and RAG**\n\nWe conducted further analysis by jointly examining retrieval success and RAG answer correctness. This is the result for BGE retriever and 3 context documents, and we are planning to add results and analysis for the full set.\n| Model | Total | RAG Accuracy | Ret. Succ. + RAG Correct | Ret. Fail + RAG Correct | Ret. Succ + RAG Incorrect | Ret. Fail + RAG Incorrect |\n|-------|-------|---------|------------|------------|------------|------------|\n| GPT-5 | 235 | 72.8 | 144 | 27 | 32 | 32 |\n| GPT-o3-mini | 235 | 71.9 | 137 | 32 | 39 | 27 |\n| Llama 3.3 70B Instruct | 235 | 66.0 | 131 | 24 | 45 | 35 |\n| Claude 4 Sonnet | 235 | 67.7 | 135 | 24 | 41 | 35 |\n| Thinking Claude 4 Sonnet | 235 | 73.6 | 144 | 29 | 32 | 30 |\n| MedGemma-27b-text-it | 235 | 63.0 | 122 | 26 | 54 | 33 |\n| Llama 3-Med42-70B | 235 | 56.2 | 111 | 21 | 65 | 38 |\n| OpenBioLLM-Llama 3-70B | 235 | 48.1 | 94 | 19 | 82 | 40 |\n\nThis is a part of failure analysis using retrieval success and RAG success. In general, <Retrieval Success + RAG Incorrect> is greater than <Retrieval Fail + RAG Correct> or <Retrieval Fail + RAG Incorrect>, suggesting that medical reasoning is a challenging task even when a correct document is provided. Models with good performance (GPT-5, Thinking Claude 4 Sonnet) have low <Retrieval Success + RAG Incorrect>, implying that pointing to the correct document often results in an accurate answer if the model’s performance is good. Their high <Retrieval Fail + RAG Correct> also emphasizes the necessity of reasoning ability in answering challenging medical questions, which aligns with our findings in the paper. \n\n**References**\n\n[1] Zheng, Lianmin, et al. \"Judging llm-as-a-judge with mt-bench and chatbot arena.\" Advances in neural information processing systems 36 (2023): 46595-46623."}}, "id": "m7yhRYmGE2", "forum": "ExCYvLZJmE", "replyto": "ExCYvLZJmE", "signatures": ["ICLR.cc/2026/Conference/Submission20233/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20233/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20233/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763705419778, "cdate": 1763705419778, "tmdate": 1763705419778, "mdate": 1763705419778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}