{"id": "kswX9NfAlo", "number": 15589, "cdate": 1758252950912, "mdate": 1763704158565, "content": {"title": "CityLens: Evaluating Large Vision-Language Models for Urban Socioeconomic Sensing", "abstract": "Understanding urban socioeconomic conditions through visual data is a challenging yet essential task for sustainable urban development and policy planning. In this work, we introduce CityLens, a comprehensive benchmark designed to evaluate the capabilities of Large Vision-Language Models (LVLMs) in predicting socioeconomic indicators from satellite and street view imagery. We construct a multi-modal dataset covering a total of 17 globally distributed cities, spanning 6 key domains: economy, education, crime, transport, health, and environment, reflecting the multifaceted nature of urban life. Based on this dataset, we define 11 prediction tasks and utilize 3 evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression. We benchmark 17 state-of-the-art LVLMs across these tasks. These make CityLens the most extensive socioeconomic benchmark to date in terms of geographic coverage, indicator diversity, and model scale. Our results reveal that while LVLMs demonstrate promising perceptual and reasoning capabilities, they still exhibit limitations in predicting urban socioeconomic indicators. CityLens provides a unified framework for diagnosing these limitations and guiding future efforts in using LVLMs to understand and predict urban socioeconomic patterns.", "tldr": "We propose a global scale benchmark to evaluate the performance of large language-vision models for urban imagery-based socioeconomic prediction", "keywords": ["Multi-modal Large Language Model", "Socioeconomic Prediction", "Urban Imagery", "Urban Science", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0a67cfcf5a78928850277e6e849b57b7839ffb5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CityLens, which is a benchmark for evaluating the capabilities of LVLM in urban socioeconomic deduction. The benchmark is comprehensive acorss 11 prediction tasks across 6 domains e.g., economy and health. The authors evaluate 17 SOTA LVLMs using three evaluation protocols. The primary findings are that current LVLMs struggle significantly with these tasks and that the feature regression method outperforms the direct and zero-shot predictions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The primary strength of this paper is the creation of a large-scale dataset for this urban tasks. The effort in collecting, aligning, and processing satellite and street view images from multiple sources, and 11 distinct socioeconomic indicators is non-trivial. The dataset could be of use for other research efforts.\nThe reproducibility is very good. The authors provide an alternative version of the dataset using only publicly accessible Mapillary images (CityLens-Mapillary), which is a commendable step towards transparency and reproducibility."}, "weaknesses": {"value": "1.This paper's central contribution is a new benchmark rather than a new method. The models and evaluation techniques are nothing new. This is not in line with the scope of ICLR\n\n2The main takeaway is that current LVLMs perform not good on this complex task with many R2 scores near 0. I admit \"negative results\" are valuable, the paper offers limited deep insight into the reasons behindwhy these representations fail.\n\n3The finding that COT is task-dependent is already a well-established consensus in the broader LLM community. The claim that specific vision encoders CLIP outperform others is also expected.\n\n4.The comparison is not fair. Feature based methods have a LASSO regressor while other evaluation protocols do not have.\n\n5.The finding that satellite imagery has \"minimal impact\" is based on an unbalanced comparison of 1 satellite image vs. 10 street view images. It is unsurprising that the information from 10 ground-level images dominates the information from one overhead image."}, "questions": {"value": "What would happen if you only use 1 satellite image and compare it with 10 street ones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wB8iQ9Pvnn", "forum": "kswX9NfAlo", "replyto": "kswX9NfAlo", "signatures": ["ICLR.cc/2026/Conference/Submission15589/Reviewer_Zxk9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15589/Reviewer_Zxk9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760982696660, "cdate": 1760982696660, "tmdate": 1762925862705, "mdate": 1762925862705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CityLens, a comprehensive benchmark dataset designed to evaluate the capabilities of Large Vision-Language Models (LVLMs) in predicting urban socioeconomic indicators from satellite and street view imagery. Specifically, this multimodal dataset covers 17 cities across 6 continents, 6 socioeconomic domains (economy, education, crime, transport, health, environment), and 11 specific prediction tasks. Building upon this, the authors systematically evaluate 17 state-of-the-art LVLMs using three distinct paradigms: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression. The key findings reveal that while LVLMs show promise, they generally struggle with accurate and generalizable socioeconomic prediction, with performance varying significantly across tasks, models, and geographic locations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The curated CityLens dataset is a major contribution to the community.\n2. The benchmarking results are comprehensive and have a large coverage of state-of-the-art LVLMs.\n3. The use of three evaluation paradigms (Direct, Normalized, Feature-Based) is technically sound.\n4. The paper is well organized and easy to follow."}, "weaknesses": {"value": "1. The evaluation is primarily zero-shot or few-shot. A natural question is how much these models could improve if fine-tuned on the CityLens dataset. While the authors mention this as a future direction, a preliminary fine-tuning experiment on one or two models would have strengthened the paper by establishing a potential performance upper bound.\n2. While the paper diagnoses what models struggle with (e.g., mental health, life expectancy), a more detailed qualitative analysis of why could be beneficial. For instance, providing examples of specific visual cues that models misinterpret or fail to utilize for the most challenging tasks would offer deeper mechanistic insights."}, "questions": {"value": "1. In the Feature-Based Regression paradigm, the authors show that CLIP-based encoders perform best. Was the satellite image used as an input to the LVLM during this feature extraction step, or was this based purely on the street view images? Could the superior performance of CLIP be attributed to its training on web-scale image-text pairs, which might include more diverse urban scenes?\n2. Did you try nonlinear regressors for the 13-attribute vectors (RF/GBM/MLP)? If not, is the conclusion “LVLM features aren’t sufficient” or “the linear readout is underpowered”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GyMjpkGlVY", "forum": "kswX9NfAlo", "replyto": "kswX9NfAlo", "signatures": ["ICLR.cc/2026/Conference/Submission15589/Reviewer_gfN8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15589/Reviewer_gfN8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875318136, "cdate": 1761875318136, "tmdate": 1762925862181, "mdate": 1762925862181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents CityLens, a comprehensive benchmark for evaluating Large Vision-Language Models (LVLMs) on urban socioeconomic sensing using satellite and street-view imagery. Covering 17 cities, 11 indicators, and six domains, CityLens defines three evaluation paradigms and benchmarks 17 LVLMs. Results show that while LVLMs capture some visually grounded socioeconomic patterns, they perform poorly on abstract or weakly visual indicators, highlighting key limitations and opportunities for future research in multimodal urban analytics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes CityLens, a large-scale and multi-domain benchmark designed to evaluate the performance of LVLMs on urban socioeconomic indicator prediction tasks. It represents a pioneering attempt in this research area.\n2. The dataset covers 17 cities, 11 socioeconomic indicators, and 17 LVLMs, with a large experimental scale and carefully designed data collection, mapping, and preprocessing pipelines. The paper introduces three distinct evaluation paradigms—Direct Metric Prediction, Normalized Estimation, and Feature-Based Regression—providing a multi-perspective analytical framework. Both data and code are publicly available, ensuring strong reproducibility and potential academic impact.\n3. Beyond comparing model performance, the paper also investigates the effects of geographic variation, input modality (satellite vs. street view), the number of visual features, and Chain-of-Thought (CoT) reasoning strategies."}, "weaknesses": {"value": "1. The results in the Direct Metric Prediction and Normalized Estimation sections (with most tasks showing R² below 0.2 or even negative) clearly demonstrate the severe limitations of current LVLMs in numerical prediction tasks. From a methodological perspective, relying on prompting to have LVLMs directly output numerical values may be inherently unstable. Such a mechanism is fundamentally ill-suited for precise regression tasks.\n2. The current experiments primarily focus on comparing multiple general-purpose LVLMs (e.g., GPT-4o, Gemini, Qwen2.5VL), which are not specifically optimized for urban or geospatial applications. As a result, the findings mainly reflect the deficiencies of generic LVLMs on urban tasks rather than revealing the key factors that could improve model performance.\n3. Although the paper presents extensive experimental results, it lacks deeper mechanistic analysis explaining why models perform poorly and which visual or linguistic factors contribute to these errors. For instance, indicators with strong visual correlations such as Building Height and GDP perform relatively well, whereas tasks like Life Expectancy and Mental Health perform extremely poorly—an observation that deserves more in-depth discussion."}, "questions": {"value": "Has the author considered including domain-specific models such as UrbanVLP, UrbanCLIP, or UrbanGPT, as well as other fine-tuned LVLMs, as part of the baselines? These models incorporate spatial alignment or social semantic injection mechanisms during training, which may better align with the research objectives of this paper. Analyzing how their training paradigms affect the results would provide deeper insights and elevate the work from a mere model evaluation study to a training paradigm analysis at the methodological level."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4PAt2yp8jK", "forum": "kswX9NfAlo", "replyto": "kswX9NfAlo", "signatures": ["ICLR.cc/2026/Conference/Submission15589/Reviewer_pPnJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15589/Reviewer_pPnJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925522230, "cdate": 1761925522230, "tmdate": 1762925861769, "mdate": 1762925861769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CityLens—a comprehensive benchmark designed to evaluate the ability of Large Vision-Language Models (LVLMs) to predict socioeconomic indicators from satellite and street-view imagery. The benchmark covers 17 cities worldwide, spans six key domains—economy, education, crime, transportation, health, and environment—and defines 11 prediction tasks, assessed under three evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression. The study systematically evaluates 17 state-of-the-art LVLMs, making CityLens the most extensive benchmark to date in terms of geographic coverage, indicator diversity, and model scale. Results show that while LVLMs demonstrate promising perceptual and reasoning capabilities, they still face significant limitations in accurately predicting real-world socioeconomic indicators. CityLens provides a unified framework for diagnosing these challenges and advancing future research in urban intelligence and multimodal socioeconomic analysis."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. CityLens introduces the most extensive benchmark to date for evaluating LVLMs on urban socioeconomic prediction, with unprecedented coverage across 17 globally distributed cities, 11 diverse tasks, and 6 critical socioeconomic domains. This scale and diversity significantly advance beyond prior urban vision or geospatial AI benchmarks.\n\n2. Beyond reporting performance numbers, the paper offers thoughtful discussion on why LVLMs struggle—such as lack of numerical grounding, sensitivity to visual ambiguity, and collapse to city-level averages—thereby guiding future model design and training strategies for urban perception tasks."}, "weaknesses": {"value": "Please refer to questions."}, "questions": {"value": "1. I am quite puzzled about the basic motivation of this paper: why choose to predict socioeconomic indicators by feeding street-view or satellite images into large vision–language models (LVLMs)? The existing literature already offers numerous methods specifically designed for modeling urban regions (e.g., region representation learning). I suspect such specialized models may substantially outperform general-purpose multimodal large models in predictive accuracy. Could the authors further clarify the theoretical or practical rationale for choosing LVLMs over traditional urban computing models?\n\n2. I find it difficult to understand how an LVLM can effectively infer complex socioeconomic indicators solely from street-view images. LVLMs were originally designed for semantic understanding of image content (e.g., object recognition, scene description), and their training objectives are not explicitly tied to socioeconomic variables. Thus, establishing a reliable mapping from visual features to abstract socioeconomic indicators seems to lack sufficient mechanistic support. Could the authors explain by what internal mechanisms (e.g., implicit knowledge, vision–language alignment, etc.) the LVLM accomplishes this cross-modal reasoning?\n\n3. Street-view imagery may suffer from serious representational limitations. For example, two areas with vastly different levels of economic development may exhibit highly similar architectural styles or street layouts, causing the model to output similar predictions despite substantial differences in the ground-truth indicators; conversely, areas with similar socioeconomic conditions may yield inconsistent predictions if the street-view sampling locations differ (e.g., arterial roads vs. back alleys). Would the noise and bias introduced by the randomness of image sampling severely undermine predictive reliability? Is reliance on street-view images alone sufficient to support robust socioeconomic sensing?\n\n4. In the “Region” column of Figure 3b, the label “sat” appears. Does this denote satellite imagery? If so, how do the authors extract socioeconomic information from region-level satellite images? Compared with street views, what complementary or more critical visual cues do satellite images provide?\n\n5. I am very interested in the results of Figures 5b and 5c. According to recent work such as FlexiReg: Flexible Urban Region Representation Learning, satellite imagery is often more discriminative than street-view imagery for urban representation. Do the results in this paper support that conclusion as well? Could you further explain why satellite imagery might be more effective than street-view imagery for predicting socioeconomic indicators? What differences in spatial scale or semantic information underlie this discrepancy?\n\n6. I consider the “Prompt Design and Case Analysis” section to be one of the most insightful parts of the paper, as it directly reveals how the LVLM is guided to perform structured scoring. Given the methodological importance of these details, I recommend moving this content into the main text rather than the appendix, so that readers can better understand the coupling between model behavior and task design.\n\n7. I am intrigued by the ablation studies on vision foundation models such as DINO and SigLIP presented at the end of the paper. Could the authors elaborate on the functional differences between these pure vision encoders and end-to-end multimodal large models (e.g., LLaVA, Qwen-VL) for this task? Which class of models contributes more to the final predictive performance? Is it reasonable to conclude that high-quality visual representations are the key to success, while the language model primarily serves as a “scoring interface”?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fioE350a6v", "forum": "kswX9NfAlo", "replyto": "kswX9NfAlo", "signatures": ["ICLR.cc/2026/Conference/Submission15589/Reviewer_DkC7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15589/Reviewer_DkC7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996177011, "cdate": 1761996177011, "tmdate": 1762925861390, "mdate": 1762925861390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}