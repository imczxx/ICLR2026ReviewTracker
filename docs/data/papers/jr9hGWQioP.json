{"id": "jr9hGWQioP", "number": 10334, "cdate": 1758167396029, "mdate": 1763499865049, "content": {"title": "Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning", "abstract": "Reasoning about failures is crucial for building reliable and trustworthy robotic systems. Prior approaches either treat failure reasoning as a closed-set classification problem or assume access to ample human annotations. Failures in the real world are typically subtle, combinatorial, and difficult to enumerate, whereas rich reasoning labels are expensive to acquire. We address this problem by introducing ARMOR: Adaptive Round-based Multi-task mOdel for Robotic failure detection and reasoning. We formulate detection and reasoning as a multi-task self-refinement process, where the model iteratively predicts detection outcomes and natural language reasoning conditioned on past outputs. During training, ARMOR learns from heterogeneous supervision - large-scale sparse binary labels and small-scale rich reasoning annotations - optimized via a combination of offline and online imitation learning. At inference time, ARMOR generates multiple refinement trajectories and selects the most confident prediction via a self-certainty metric. \nExperiments across diverse environments show that ARMOR achieves state-of-the-art performance by improving over the previous approaches by up to 30\\% on failure detection rate and up to 100\\% in reasoning measured through LLM fuzzy match score, demonstrating robustness to heterogeneous supervision and open-ended reasoning beyond predefined failure modes.", "tldr": "", "keywords": ["Failure Reasoning", "Robotics", "Foundation Models"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/163dd7c24ef709799e36723d4488ad6041678c85.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduce ARMOR, an approach that jointly detects whether a robot execution succeed or failed and then provide explains and building on top of prior work like AHA, and they achieve this through iteratively refines both outputs over several rounds. It is designed for the realistic case where you have many binary success/failure labels but few natural‑language explanations. Training mixes offline imitation (warm‑up + conditioning on ground‑truth previous outputs) with online refinement rollouts; inference runs several refinement trajectories and selects the output with lowest entropy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strength of the paper:\n-> The method demonstrated to be effiective and efficient, as on RLBench‑Fail & ManiSkill‑Fail (sim), Sparrow‑Fail & ARMBench (real). Detection is measured by Binary Success Rate; reasoning by LLM Fuzzy Match (judge: Claude‑3.7‑Sonnet) and ROUGE‑L; the fuzzy score is computed only on the reasoning text to avoid format bias.\n-> The work also demonstrated that the explicitly target s the common regime with a lot of binary label data, and only a few explains which still able to achieve good performance.\n->Gains hold from simulation to real warehouses and under transfer (R→M, S→A), where naïvely mixing sparse+dense (SFT‑S+D) often collapses reasoning."}, "weaknesses": {"value": "-> Unreported runtime or inference time latency, as the inference does multiple rounds and samples, so it is good to know the wall-clock or throughput for this, cause it matters for robot deployment.\n\n-> There are many runs on benchmarks from AHA, however there was not any demonstrate of how such VLM model could be used to help with downstream robotics manipulation tasks. \n\n-> How well does this transfer to cross-embodiment to different single arm robot."}, "questions": {"value": "Refer to the weakness for the questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ahOGVIZltJ", "forum": "jr9hGWQioP", "replyto": "jr9hGWQioP", "signatures": ["ICLR.cc/2026/Conference/Submission10334/Reviewer_2SV2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10334/Reviewer_2SV2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761280095601, "cdate": 1761280095601, "tmdate": 1762921668470, "mdate": 1762921668470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses failure detection and reasoning in robotic systems, which is indeed an important capability for enhancing the reliability of robotic systems. Prior work either treats this as closed-set classification or relies heavily on fully annotated reasoning labels. The authors propose ARMOR, a VLM that performs multi-round self-refinement to jointly predict binary failure detection and open-ended natural-language reasoning.\n\nARMOR is trained under heterogeneous supervision, combining large-scale sparse binary labels and a small number of dense reasoning annotations. The framework employs two task-specific heads (classification and text generation) trained via a combination of offline imitation and online refinement, allowing it to iteratively improve predictions conditioned on previous outputs. At inference time, multiple refinement trajectories are generated and scored using an entropy-based self-certainty metric, with the most confident prediction selected as output.\n\nExperiments across four domains (RLBench, Maniskill, Sparrow, and ARMBench) show state-of-the-art performance, with up to 30% improvement in detection accuracy and 100% in reasoning quality (LLM fuzzy match) compared to prior baselines such as AHA and SFT variants. Ablations demonstrate that both multi-task design and iterative refinement contribute to performance gains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an under-explored but crucial problem, robotic failure detection and reasoning. \n\n- The proposed multi-round adaptive refinement allows the model to iteratively improve its predictions, similar to human introspection, and provides more coherent reasoning explanations.\n\n- The authors performed extensive experiments on four diverse robotic datasets (RLBench, ManiSkill, Sparrow, ARMBench), showing clear and consistent gains over strong baselines in both detection accuracy and reasoning quality.\n\n- The authors conduct detailed ablation studies on the refinement rounds, task weighting, and uncertainty metrics, providing convincing evidence of each design component’s impact."}, "weaknesses": {"value": "- How does the multi-round refinement introduce additional inference cost proportional to the number of rounds? It can be helpful to also include some latency or runtime statistics. In real-time robotic applications, providing timely feedback can be challenging without interrupting task execution. Similarly, the dependency on large pre-trained language models raises deployment challenges for real-time execution. \n\n- For the failure detection of these safety-critical tasks, using these large models with even video inputs can be overkill compared to specialized settings. Since these manipulation tasks are not performed/validated in the wild, why can one not use more overfitting metrics instead of an open-set VLM if one cares more about the binary value of success or not than the detailed reasoning? Including some discussions would further improve the soundness of the paper.\n\n- While the proposed multi-round self-refinement mechanism is well-motivated, the architectural backbone and training strategy primarily build upon the existing vision–language and instruction-tuning framework. The main novelty lies in integrating iterative refinement and multi-task learning, which, though valuable, represents an incremental advance rather than a new paradigm."}, "questions": {"value": "See the questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GlI0jt9X06", "forum": "jr9hGWQioP", "replyto": "jr9hGWQioP", "signatures": ["ICLR.cc/2026/Conference/Submission10334/Reviewer_zLbK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10334/Reviewer_zLbK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728949480, "cdate": 1761728949480, "tmdate": 1762921668030, "mdate": 1762921668030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ARMOR, a VLM trained to detect failures in robotic scenarios. ARMOR introduces a two-task training pipeline separating binary failure detection and reasoning, which are iteratively optimized. A self-refinement objective further enhances reasoning ability. The authors evaluate their approach on existing failure reasoning benchmarks and show that their approach outperforms the selected baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clear and the addressed issue is very relevant for robotics.\n- The multi-task training and conditional supervision on type of available ground truth is well justified for the robotics domain."}, "weaknesses": {"value": "### The clarity and methodology of the proposed method present several concerns\n\n- The second stage of offline imitation seems to just reproduce the textual input. If I understand correctly, the model is provided with the ground truth labels in textual format and then learns to reproduce these outputs. This could possibly lead to the model ignoring the visual modality, making the initial training stage obsolete. \n- The ablation “Multitask Prediction” also indicates this. The model almost performs on par with ARMOR, whereas reasoning is significantly worse. However, the authors don't provide the training details for this baseline, which makes it hard to tell if it's a fair comparison. The difference could be due to shorter training or absence of prompt p_t.\n- It is not clear what supervision signal the model uses during online refinement. Similar methods create a refinement dataset through policy rollouts and ensure each subsequent response is better through a teacher model or reward. It seems like ARMOR solely relies on the supervision from the ground truth target reasoning without any feedback or step-by-step datasets.  This raises the question whether the iterative refinement actually results in better reasoning with multiple iterations.\n- The results provided by the authors regarding performance with multiple model iterations are not convincing. Without any conditioning (step 0), the model reasons poorly, which makes sense, since the model is mainly trained on predicting reasoning conditioned on previous reasonings, and further iterations result in almost no improvements.\n\n### The experimental section is unclear and insufficient to support the main claims\n- Missing comparison with AHA, despite it being evaluated on the same benchmarks\n- Including experiments showcasing the effectiveness of the method under different dense/sparse distributions (e.g., reducing the number of dense annotations further) would support the claim that the method works in regimes where data is annotated with low detail.\n- Experiments showcasing reasoning transferability of reasoning from one domain to another would be more interesting to strengthen the claims of the paper (e.g. Maniskill ( M→R))\n- Motivation and downstream applications (e.g., failure recovery policies, recovery reasoning…) for why correct reasoning even matters in this scenario are missing.\n\n\n### Additional Issues:\n- Usage of the term “two-heads” is misleading. Clarifying that the second “head” is actually just the language model head or the standard VLM would make it easier to grasp the method.\n- Line 240 and subsequent:  Misleading use of “encoder-decoder” terminology for a decoder-only VLM.\n- Missing model sizes\n- No justification/ablation for the CLS token\n- Line 322: Lowest confidence is misleading. Shouldn’t it be the lowest entropy and the highest confidence?\n\nThe paper addresses an important problem and reports promising results, but it remains unclear why the method works. The training pipeline and iterative refinement lack clear intuition and justification, and the experiments do not convincingly support the main claims. I suggest that the authors clarify the method and explain the intuition behind the iterative refinement without actually ensuring that the refinement steps result in incrementally better responses during training."}, "questions": {"value": "- Why not use RL fine-tuning for reasoning refinement?\n- How does Multitask Prediction differ from Offline Imitation?\n- What model size is used, and how does performance scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rcpz45s5mZ", "forum": "jr9hGWQioP", "replyto": "jr9hGWQioP", "signatures": ["ICLR.cc/2026/Conference/Submission10334/Reviewer_MSS9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10334/Reviewer_MSS9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744843527, "cdate": 1761744843527, "tmdate": 1762921667632, "mdate": 1762921667632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ARMOR, a VLM trained to detect failures in robotic scenarios. ARMOR introduces a two-task training pipeline separating binary failure detection and reasoning, which are iteratively optimized. A self-refinement objective further enhances reasoning ability. The authors evaluate their approach on existing failure reasoning benchmarks and show that their approach outperforms the selected baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clear and the addressed issue is very relevant for robotics.\n- The multi-task training and conditional supervision on type of available ground truth is well justified for the robotics domain."}, "weaknesses": {"value": "### The clarity and methodology of the proposed method present several concerns\n\n- The second stage of offline imitation seems to just reproduce the textual input. If I understand correctly, the model is provided with the ground truth labels in textual format and then learns to reproduce these outputs. This could possibly lead to the model ignoring the visual modality, making the initial training stage obsolete. \n- The ablation “Multitask Prediction” also indicates this. The model almost performs on par with ARMOR, whereas reasoning is significantly worse. However, the authors don't provide the training details for this baseline, which makes it hard to tell if it's a fair comparison. The difference could be due to shorter training or absence of prompt p_t.\n- It is not clear what supervision signal the model uses during online refinement. Similar methods create a refinement dataset through policy rollouts and ensure each subsequent response is better through a teacher model or reward. It seems like ARMOR solely relies on the supervision from the ground truth target reasoning without any feedback or step-by-step datasets.  This raises the question whether the iterative refinement actually results in better reasoning with multiple iterations.\n- The results provided by the authors regarding performance with multiple model iterations are not convincing. Without any conditioning (step 0), the model reasons poorly, which makes sense, since the model is mainly trained on predicting reasoning conditioned on previous reasonings, and further iterations result in almost no improvements.\n\n### The experimental section is unclear and insufficient to support the main claims\n- Missing comparison with AHA, despite it being evaluated on the same benchmarks\n- Including experiments showcasing the effectiveness of the method under different dense/sparse distributions (e.g., reducing the number of dense annotations further) would support the claim that the method works in regimes where data is annotated with low detail.\n- Experiments showcasing reasoning transferability of reasoning from one domain to another would be more interesting to strengthen the claims of the paper (e.g. Maniskill ( M→R))\n- Motivation and downstream applications (e.g., failure recovery policies, recovery reasoning…) for why correct reasoning even matters in this scenario are missing.\n\n\n### Additional Issues:\n- Usage of the term “two-heads” is misleading. Clarifying that the second “head” is actually just the language model head or the standard VLM would make it easier to grasp the method.\n- Line 240 and subsequent:  Misleading use of “encoder-decoder” terminology for a decoder-only VLM.\n- Missing model sizes\n- No justification/ablation for the CLS token\n- Line 322: Lowest confidence is misleading. Shouldn’t it be the lowest entropy and the highest confidence?\n\nThe paper addresses an important problem and reports promising results, but it remains unclear why the method works. The training pipeline and iterative refinement lack clear intuition and justification, and the experiments do not convincingly support the main claims. I suggest that the authors clarify the method and explain the intuition behind the iterative refinement without actually ensuring that the refinement steps result in incrementally better responses during training."}, "questions": {"value": "- Why not use RL fine-tuning for reasoning refinement?\n- How does Multitask Prediction differ from Offline Imitation?\n- What model size is used, and how does performance scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rcpz45s5mZ", "forum": "jr9hGWQioP", "replyto": "jr9hGWQioP", "signatures": ["ICLR.cc/2026/Conference/Submission10334/Reviewer_MSS9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10334/Reviewer_MSS9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744843527, "cdate": 1761744843527, "tmdate": 1763395060175, "mdate": 1763395060175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ARMOR which is used for robotic failure detection and reasoning. The authors cast binary success/failure detection and natural-language reasoning as a multi-task self-refinement process. ARMOR iteratively predicts both tasks, conditions on its prior outputs, and refines over several rounds. Training combines offline imitation (expert-conditioned supervision) and online refinement (rollouts with heterogeneous supervision: large-scale sparse binary labels + small-scale dense reasoning labels).\n\nAt inference, multiple refinement trajectories are sampled and the most confident one is selected via an entropy-based self-certainty metric. Across four datasets (RLBench-Fail, Maniskill-Fail, Sparrow-Fail, ARMBench), ARMOR reports state-of-the-art detection and reasoning performance. The work targets the important challenge of scalable, explainable robotic monitoring under limited annotations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. ARMOR reframes robotic failure detection and reasoning as a multi-round, multi-task self-refinement problem. Instead of predicting both tasks jointly in one forward pass, it trains the model to condition on its own past predictions (for both detection and reasoning) and iteratively improve them, which I believe is a interesting approach.\n\n2. The proposed refinement framework is not limited to failure detection. Its iterative conditioning mechanism and multi-task training setup could naturally extend to other vision-language domains such as task planning, human-robot interaction, or embodied reasoning areas where explanation quality and temporal consistency are often more important than raw classification accuracy. This gives the method strong potential impact beyond the specific application studied."}, "weaknesses": {"value": "1. All reported numbers appear to be single-run result, no mention of multiple seeds, error bars, or statistical significance. Given that inference involves stochastic sampling of trajectories (M=3), performance could vary substantially between runs. Without variance or confidence intervals, large reported gains (up to +100% in reasoning) cannot be verified as statistically meaningful.\n\n2. The paper only compares to generative VLMs (Qwen2.5-VL, Cosmos-Reasoning, LLaVA-NeXT, Claude-3.7). It omits discriminative baselines (like CLIP or SigLIP fine-tuned on detection labels), which are often stronger for classification tasks.\n\n3. The model chooses the best refinement trajectory using entropy (sec 4.3 eq 2) and entropy is used as a proxy for confidence. The paper assumes entropy correlates with correctness, but never tests this hypothesis. There’s no analysis of correlation between low entropy and high accuracy, nor justification for the λ=0.1 choice.\n\n4. Fine-tuning Qwen2.5-VL on 8×H100 GPUs and performing multiple inference rollouts is computationally heavy. The paper does not report inference time, memory usage, or cost, which limits practical value for robotic systems requiring near-real-time feedback.\n\n5. Although not major the ablation study in Table 2 is informative but remains limited in scope. It explores only a few coarse variants of the method and omits key hyperparameter sensitivities that are central to understanding ARMOR’s behavior. In particular, the paper does not analyze how performance varies with the number of refinement rounds (T), the number of sampled trajectories (M), or the masking probability used during offline conditional training. Similarly, the impact of balancing the detection (BCE) and reasoning (NTP) losses is not reported, which makes it difficult to gauge how each objective contributes to the final performance."}, "questions": {"value": "1. The paper uses LLM Fuzzy Match and ROUGE-L to measure reasoning quality, but both metrics can be biased or insensitive to factual correctness. Did you conduct any human evaluation, factual consistency analysis, or use alternative metrics (e.g., BLEU, GPT-4-based scoring) to confirm that the improvements reflect genuinely better reasoning rather than surface-level similarity?\n\n2. Since heterogeneous supervision is central to your claim, is it possible to provide evidence that the method’s improvements are not driven by cross-domain artifacts in Table 4 (e.g., using sparse data from one environment and dense data from another)? How robust is ARMOR to changes in this supervision distribution?\n\n3. Since the detection component is a binary classification task, did you consider comparing against smaller vision-only models (e.g., ResNet, ViT, EfficientNet) fine-tuned for failure detection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "i6md7x3wsQ", "forum": "jr9hGWQioP", "replyto": "jr9hGWQioP", "signatures": ["ICLR.cc/2026/Conference/Submission10334/Reviewer_VQku"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10334/Reviewer_VQku"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967855111, "cdate": 1761967855111, "tmdate": 1762921667101, "mdate": 1762921667101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Paper revision"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful feedback and constructive suggestions. We have updated the manuscript to address the reviewers' concerns and improve the paper accordingly. The changes are marked in blue, and below is a list of the changes made to the main paper and appendix:\n* Added discussion on applying failure reasoning to downstream applications in the introduction\n* Added discussion to motivate the usage of foundation model in the introduction\n* Clarified the definition of two heads in Section 4.1\n* Justified the usage of CLS token in Section 4.1\n* Clarified the masking procedure in Section 4.2\n* Changed \"lowest confidence score\" to \"lowest entropy score\" in Section 4.3\n* Added the new ablation table with each component clearly marked\n* Specifed model sizes in experiments\n* Added quanitative results on performance v.s. refinement rounds with multiple seeds in Section 5.2\n* Added inference cost of the model in Section 5.4\n* Added broader impact of the method in the conclusion\n* Added sampling hyperparameters in Appendix A.2\n* Added experiments testing sparse-dense data ratio in Appendix C.1\n* Added experiments testing model scaling in Appendix C.1"}}, "id": "Q7kElvnveA", "forum": "jr9hGWQioP", "replyto": "jr9hGWQioP", "signatures": ["ICLR.cc/2026/Conference/Submission10334/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10334/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission10334/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763500575352, "cdate": 1763500575352, "tmdate": 1763500575352, "mdate": 1763500575352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}