{"id": "U1xQiqTnBp", "number": 9881, "cdate": 1758146176053, "mdate": 1759897689583, "content": {"title": "Positional Encoding via Token-Aware Phase Attention", "abstract": "We prove under practical assumptions that Rotary Positional Embedding (RoPE) introduces an intrinsic distance-dependent bias in attention scores that limits RoPE's ability to model long-context. RoPE extension methods may alleviate this issue, but they typically require post-hoc adjustments after pretraining, such as rescaling or hyperparameters retuning. This paper introduces Token-Aware Phase Attention (TAPA), a new positional encoding method that incorporates a learnable phase function into the attention mechanism. TAPA preserves token interactions over long range, extends to longer contexts with direct and light fine-tuning, extrapolates to unseen lengths, and attains significantly lower perplexity on long-context than RoPE families.", "tldr": "We TAPA—a new positional encoding based on learnable phase—that provably and empirically achieves better long context ability than RoPE families", "keywords": ["Positional Encoding", "Transformers", "Long Context Modeling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4251030ad011945d67b9e573cc44b1bbd2a1c8a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Token-Aware Phase Attention (TAPA), a novel positional encoding method to deal with the limitations of Rotary Positional Embedding (RoPE) in long-context language modeling. The authors first provide a theoretical proof that RoPE has an intrinsic distance-dependent bias, which limits its ability to long-range modeling. TAPA overcomes this by incorporating a learnable phase function into the attention mechanism, which eliminates this bias. The paper demonstrates theoretically that TAPA's distance bias vanishes as context grows while maintaining non-degenerate attention. Empirically, the authors pretrain a LLaMA3 7B model and show that TAPA significantly outperforms RoPE and its variants (like PI and YaRN) on long-context perplexity benchmarks, especially when extrapolating to unseen context lengths. When pretrained on 8k context without finetuning, TAPA remains effective up to 32k context length, while other methods see a collapse in performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents a mathematical proof of the inherent distance-dependent bias in RoPE. It also provides theoretical guarantees for TAPA, proving its decaying distance bias and non-degeneracy for long contexts.\n\n* TAPA is a well-motivated idea to the prevalent RoPE-based methods. Unlike many RoPE extensions that rely on heuristic adjustments such as position rescaling, TAPA is presented as a more fundamental solution that can be extended to longer contexts with minimal fine-tuning and without manual hyperparameter changes.\n\n* The head-to-head comparison with RoPE, PI, and YaRN on a LLaMA3 7B architecture demonstrates TAPA's superior performance in long-context scenarios. In particular, under a 8k context pretrain setting, TAPA remains stable up to 32k context length while the other methods collapse.The zero-shot evaluation also highlights TAPA's generalization ability."}, "weaknesses": {"value": "* TAPA's formulation requires two QK dot products, which is computationally more expensive than the standard attention mechanism. While they mention the feasibility of a custom implementation, the practical overhead in terms of training and inference speed compared to highly optimized RoPE implementations is a potential limitation.\n\n* The experiments are conducted on a 7B LLaMA3 architecture. Demonstrating the effectiveness of TAPA on a wider range of model sizes and architectures would strengthen the generalizability of the claims.\n\n* The evaluation is focused on perplexity. Although this is a standard metric, it may not fully capture all aspects of long-context understanding, such as reasoning over long documents or complex instruction following."}, "questions": {"value": "* Could you elaborate on the practical implications of the increased computational cost of TAPA? Have you performed any analysis on the slowdown compared to a flash-attention-based RoPE implementation during training and inference?\n\n* The paper focuses on a quadratic phase function for TAPA. While the ablation shows it outperforms a linear phase, have you explored other families of functions for the learnable phase?\n\n* Do you have insights into how pre-training with TAPA from scratch might differ from pre-training with RoPE in terms of training dynamics or final model capabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dvKUIRrgeI", "forum": "U1xQiqTnBp", "replyto": "U1xQiqTnBp", "signatures": ["ICLR.cc/2026/Conference/Submission9881/Reviewer_8R3a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9881/Reviewer_8R3a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761182162542, "cdate": 1761182162542, "tmdate": 1762921349495, "mdate": 1762921349495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article re examines the shortcomings of remote attenuation in Rotary Positional Embedding (RoPE) and proposes a new position encoding called s Token-Aware Phase Attention (TAPA) based on this, which is beneficial for length extrapolation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  Sufficient theoretical analysis. There is theoretical analysis on the weaknesses of ROPE's distance-dependent bias (Theorem 2.1, 2.2) and the relative advantages of TAPA (Theorem 3.2) in this regard.\n\n2. Innovative solutions. A new solution (TAPA) for length extrapolation has been proposed and be verify in Section 4.3."}, "weaknesses": {"value": "1.**Evaluation is limited**, without a long window benchmark, such as  a needle in a haystack [1] or ruler [2].\n\n2.The assumption 2.1 **has not been verified**.  For example, the assumption of 2.1 may not be satisfied for any $d$.  As shown in [3][4], there is significant anisotropy between different dimensions. \n\n3.Lack of comparison. Perhaps we can compare alibi (with different slopes) and sliding window attention (with different window size). There are other methods that do not rely on RoPEs and can be extrapolated, e.g. CAPE [5], antmax[6]. \n\n\n[1] Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation. https://aclanthology.org/2023.eacl-main.75/\n\n[2] RULER: What’s the Real Context Size of Your Long-Context Language Models? https://openreview.net/forum?id=kIoBbc76Sy\n\n[3]The Rotary Position Embedding May Cause Dimension Inefficiency in Attention Heads for Long-Distance Retrieval https://aclanthology.org/2025.findings-acl.697.pdf\n\n[4]On the token distance modeling ability of higher RoPE attention dimension https://aclanthology.org/2024.findings-emnlp.338.pdf\n\n[5]CAPE: Context-Adaptive Positional Encoding for Length Extrapolation https://arxiv.org/html/2405.14722v1\n\n[6] Long-Context Generalization with Sparse Attention https://arxiv.org/pdf/2506.16640"}, "questions": {"value": "1.In my opinion, there is a **logical incompleteness** in this article, namely the unclear correlation between intrinsic distance dependent bias in attention scores and length extrapolation. In fact, when there is strong decay bias, the model can actually achieve better extrapolation on PPL, such as using swa [5], or even the a larger $\\theta$ in RoPE [6].  **However**, this will present a superficial contextual ability, where the model's PPL can extrapolate, but in reality, it cannot perform tasks such as finding a needle in a haystack. \n\n2.At the same time, even the a larger $\\theta$ in RoPE [6] can lead to better length extrapolation. But in line 130 “Fortunately, the next Theorem says that one can reduce such gap between any given position-pairs by further decreasing RoPE’s $\\theta_0$\".  Can the contradiction presented by these two be explained using the theorem presented in this article?\n\n3.There is no code. This article does not provide code for implementing TAPA, especially for efficiently utilizing the Triton/Telelang. I am currently unclear about its speed difference compared to RoPE.\n\n4.Will partial ROPE[10], as a recently popular setting [11,12], affect the conclusions of this article?\n\n5.In [13], the author of RoPE get context length $L$ scale with the $2 \\theta_0^{-1}$  by using $Ci(x)$ under a sufficiently large $d$. This is consistent with line 126's statement that 'context length is $O(\\theta_0^{-1})$ Is there a connection？\n\n[7] Dissecting transformer length extrapolation via the lens of receptive field analysis. https://aclanthology.org/2023.acl-long.756.pdf\n\n[8] Scaling laws of rope-based extrapolation. https://openreview.net/forum?id=JO7k0SJ5V6\n\n[9] Base of RoPE Bounds Context Length. https://openreview.net/pdf?id=EiIelh2t7S\n\n[10] https://github.com/lucidrains/x-transformers/issues/40\n\n[11] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model https://arxiv.org/pdf/2405.04434\n\n[12] Kimi K2: Open Agentic Intelligence https://arxiv.org/pdf/2507.20534\n\n[13] https://kexue.fm/archives/10122"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J5BKwqPYMK", "forum": "U1xQiqTnBp", "replyto": "U1xQiqTnBp", "signatures": ["ICLR.cc/2026/Conference/Submission9881/Reviewer_6Cy2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9881/Reviewer_6Cy2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629335588, "cdate": 1761629335588, "tmdate": 1762921349021, "mdate": 1762921349021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides an analysis of the shortcomings of Rotary Position Embedding (RoPE), demonstrating the distance bias of RoPE, and further presents a new method, namely Token-Aware Phase Attention, which utilizes the inner product of the query and key to determine a phase function that encodes position information. The paper presents theoretical analyses and empirical evidence demonstrating that the new method outperforms RoPE in scenarios with long context lengths."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed method seems novel, which addresses a vital problem of distance-dependent bias that exists on RoPE.\n\n* Overall, the analyses on the distance bias of RoPE and how the proposed method would improve it are clear and interesting.\n\n* The experiments demonstrate that models with the proposed method are more stable when the context length increases, compared to models with RoPE."}, "weaknesses": {"value": "* The proposed method splits the query and the key of each token into two parts and uses part of the query and key for the learnable phase function, which may restrict the ability of the model. As demonstrated in the experiment, under regular context lengths, the model with TAPA demonstrates worse performance than the model with RoPE.\n\n* While the proposed method adopts a learnable phase function where $\\cos (2\\pi |m-n|^{\\alpha} \\phi(q,k))$ affects the attention score, the function $\\phi(q, k)$ has an unstable impact on the attention score. For example, at some distances, larger $\\phi(q,k)$ leads to a larger attention score, while at other distances, larger $\\phi(q,k)$ leads to a smaller attention score. It also raises concerns about the expressivity and training stability of models with the proposed TAPA.\n\n* When $\\alpha<1$ (e.g., the $\\alpha$ is set to $0.1$ in the experiments), $|m-n|^{\\alpha}$ rarely changes at long distance, leaving the phase function (approximately) solely determined by the phase function $\\phi(q,k)$, which is similar to a scenario where no position embedding is used. The community has witnessed methods like Multi-Head Latent Attention (MLA) that only adopt RoPE for part of the query and key. However, no experiments have been conducted to compare the proposed method with these previous methods.\n\n* Lack of an ablation study on the hyperparameter setting. For example, how would $\\alpha$ and $\\theta$ and the choice of $\\mathcal{N}$ affect the performance of the proposed method?"}, "questions": {"value": "As mentioned above in the weakness section, I have concerns about how the proposed method will affect the expressivity and training stability of the model, as well as the relationship between the proposed method and previous methods, such as MLA. In the following, I have further questions about the proposed method:\n* While attention has a quadratic complexity regarding the input length, we have witnessed techniques like sliding window attention that try to reduce the computational complexity. Is it really necessary to pursue the ability to process extremely long text while sacrificing the ability in scenarios of regular length?\n\n* Is the $\\mathcal{N}$ in Eq. 10 a learnable parameter or initialized and fixed for the entire training? If it is learnable, it appears to introduce a large number of parameters.  If it is fixed, would the initialization have a dramatic influence on the model's performance? A more detailed ablation study would be appreciated.\n\nGenerally, I appreciate the theoretical analyses in the paper and think it would be an excellent paper with further justification on the proposed method and more empirical results. However, with the concerns I mentioned above, I am leaning towards rejection and look forward to the authors' replies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7A2UnudiN1", "forum": "U1xQiqTnBp", "replyto": "U1xQiqTnBp", "signatures": ["ICLR.cc/2026/Conference/Submission9881/Reviewer_VwhP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9881/Reviewer_VwhP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901803590, "cdate": 1761901803590, "tmdate": 1762921348211, "mdate": 1762921348211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical analysis of Rotary Positional Embedding (RoPE), proving it suffers from an intrinsic distance-dependent bias that hinders long-context modeling. To address this, it introduces Token-Aware Phase Attention (TAPA), a novel positional encoding that incorporates a learnable, token-dependent phase into the attention mechanism. The core contributions are: (1) A rigorous proof of RoPE's long-context instability and an explanation for the effectiveness of extension methods like PI. (2) The proposal of TAPA, which theoretically eliminates this bias and maintains long-range interactions. (3) Strong empirical results on a LLaMA3-7B scale model, where TAPA significantly outperforms RoPE variants at context lengths up to 64k, remaining stable where others collapse."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a rare and rigorous theoretical dissection of RoPE's failure modes in long-context scenarios. The proofs regarding distance bias (Theorems 2.1, 2.2) are insightful and fill a crucial gap in community understanding.\n2。 TAPA is a fundamentally new approach to positional encoding. Moving the positional dependency into a token-aware phase is an elegant idea, backed by sound theoretical motivations from harmonic analysis (e.g., decaying bias in Thm 3.2).\n3. The experiments are large-scale, well-designed, and convincing. TAPA's ability to remain stable and performant at 49k-64k context lengths, where strong baselines like YaRN completely fail, is a very strong result."}, "weaknesses": {"value": "1. The proposed TAPA formulation (Eq. 12) requires two separate `QK^T` dot products (`q_A^T k_A` and `q_P^T k_P`), effectively doubling the FLOPs of the most expensive part of the attention mechanism. The paper acknowledges this but understates the practical implications for training and deployment at scale.\n2. The theoretical analysis of RoPE heavily relies on `Assumption 2.1`, which posits that expectations of certain bilinear forms of token embeddings are constant. This is a strong simplification that may not hold in practice, where representations are highly context-dependent. The paper does not discuss the gap between this assumption and the reality of trained models.\n3. The token-aware phase term `q_P^T k_P` appears in the denominator of the argument to a cosine function. If this term approaches zero for certain token pairs, it could cause extremely high-frequency oscillations and numerical instability. The paper does not address this potential failure mode or discuss any regularization techniques (e.g., adding an epsilon) to prevent it.\n4.  **Insufficient Analysis of Hyperparameters (Insufficient Analysis):** The method introduces new, critical hyperparameters (`θ` and `α`) 4. The model's performance dependency on the amplitude/phase split (`θ`) and the phase scaling (`α`) remains unexplored, hindering reproducibility and practical application.\n5. The specific implementation of TAPA uses a hard, disjoint split of embedding dimensions for amplitude and phase. This design is somewhat ad-hoc. It is unclear why this is preferable to a \"soft\" approach where all dimensions could potentially contribute to both, for example, through separate linear projections.\n6. While mathematically motivated, the paper offers no insight into what the learned phase `q_P^T k_P` captures. Is it learning syntactic relationships, semantic distance, or something else? Without this analysis, the \"token-aware\" aspect remains a black box."}, "questions": {"value": "1.  Regarding computational cost: Could you provide wall-clock time benchmarks for training/inference throughput of TAPA versus a standard (and ideally, fused) RoPE implementation? How significant is the slowdown in practice on modern hardware?\n2.  Regarding numerical stability: Did you observe instances of `q_P^T k_P` becoming very small during training, and did it cause any instability? Have you considered adding a small constant `ε` to the denominator as a safeguard, and if so, how does it affect performance?\n3.  Regarding the TAPA formulation: Have you experimented with alternative formulations that don't rely on a hard split of dimensions? For instance, one could use the full query/key vectors and project them differently for the amplitude and phase components (e.g., `(qW_A)^T(kW_A)` and `(qW_P)^T(kW_P)`).\n4.  Regarding hyperparameters: How was the phase scaling factor `α=0.1` chosen? This term seems to control the \"sensitivity\" of the attention score to the phase. How does performance change as `α` varies, for instance, from `1.0` down to `0.01`?\n5.  Regarding the theoretical assumptions: How well does `Assumption 2.1` hold in your trained models? Could you empirically measure the statistics of the `A_d` and `B_d` terms from Eq. (4) to show how much they vary from a constant `µ0` and `ν0` across different positions and head dimensions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TYqMo7n27k", "forum": "U1xQiqTnBp", "replyto": "U1xQiqTnBp", "signatures": ["ICLR.cc/2026/Conference/Submission9881/Reviewer_Wzan"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9881/Reviewer_Wzan"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762181074361, "cdate": 1762181074361, "tmdate": 1762921347861, "mdate": 1762921347861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}