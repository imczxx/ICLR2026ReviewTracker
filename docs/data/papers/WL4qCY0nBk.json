{"id": "WL4qCY0nBk", "number": 378, "cdate": 1756736987505, "mdate": 1762958364868, "content": {"title": "ERC-SVD: Error-Controlled SVD for Large Language Model Compression", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in a wide range of downstream natural language processing tasks. \nNevertheless, their considerable sizes and memory demands hinder practical deployment, underscoring the importance of developing efficient compression strategies. \nSingular value decomposition (SVD) decomposes a matrix into orthogonal components, enabling efficient low-rank approximation. This is particularly suitable for LLM compression, where weight matrices often exhibit significant redundancy.\nHowever, current SVD-based methods neglect the residual matrix from truncation, resulting in significant truncation loss. \nAdditionally, compressing all layers of the model results in severe error propagation. \nTo overcome these limitations, we propose ERC-SVD, a new post-training SVD-based LLM compression method from an error-controlled perspective. \nSpecifically, we leverage the residual matrix generated during the truncation process to reduce truncation loss. \nMoreover, under a fixed overall compression ratio, we selectively compress the last few layers of the model, which mitigates error propagation and improves compressed model performance.\nComprehensive evaluations on diverse LLM families and multiple benchmark datasets indicate that ERC-SVD consistently achieves superior performance over existing counterpart methods, demonstrating its practical effectiveness.", "tldr": "", "keywords": ["Model Compression", "SVD", "Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9f2ce4e886f059da2f0c11a51f564e137ab5e564.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "ERC-SVD proposes splitting a fixed target rank into an intermediate rank (SVD on the original weight matrix) plus a small residual rank (SVD on the residual) and combining them, together with selecting only the last K layers for compression. The method is simple and empirically often improves on several baselines across many setups, but the paper falls short on novelty, theoretical justification, arbitrary experimental setting — therefore I recommend rejection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Good experiment results"}, "weaknesses": {"value": "1. Limited novelty. The core contribution is simply applying SVD twice—first to the weight matrix, then to the residual. This is not a novel algorithmic insight.\n2. Lack of  theoretical justification. The authors didn’t explain why two-stage SVD should outperform single-stage decomposition given the same rank budget. There's no analysis of what properties of the residual matrix that make separate decomposition beneficial, no approximation error bounds, and no indicators of when the method works or fails. The paper merely shows \"we tried it, it works\" without deeper understanding.  Without such theoretical justification,  I am concerned about the experimental results, as it has been proved long ago that Top-r SVD is already optimal, unless their loss is not the standard Frobenius norm or the whitening process changes the effective metric. It is possible that the LLM weight matrix has some special features, as many previous work pointed out the mismatch of the performance and the approximation error. \n3. The design choices appear arbitrary. The residual compensation factor β is fixed at 0.05 across all layers, and compression ratios without justification. The limited sensitivity analysis tests only a narrow range and claims \"robustness,\" but this actually may indicate the lack of principled design."}, "questions": {"value": "Have you tested compressing the first k layers, middle k layers, or random k layers? What happens if you compress every other layer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h7JHW2Z8d6", "forum": "WL4qCY0nBk", "replyto": "WL4qCY0nBk", "signatures": ["ICLR.cc/2026/Conference/Submission378/Reviewer_rhMX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission378/Reviewer_rhMX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793667784, "cdate": 1761793667784, "tmdate": 1762915507302, "mdate": 1762915507302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "stH4GJwUXA", "forum": "WL4qCY0nBk", "replyto": "WL4qCY0nBk", "signatures": ["ICLR.cc/2026/Conference/Submission378/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission378/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762932420738, "cdate": 1762932420738, "tmdate": 1762932420738, "mdate": 1762932420738, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **ERC-SVD**, a post-training SVD-based compression method for large language models (LLMs).\nIt introduces two components:\n\n\n1. **Residual Compensation (REC)** – applies an additional SVD to the residual matrix left after truncation to reduce information loss.\n2. **Partial-Layer Compression (PLC)** – compresses only the last *k* layers under a fixed overall compression ratio to alleviate forward error accumulation.\n\n\nExperiments on a wide set of models (LLaMA, OPT, Mistral, Vicuna, Qwen) and tasks (language modeling, zero-shot reasoning, and vision-language benchmarks) show consistent improvements over ASVD, SVD-LLM, and AdaSVD, with stable behavior under different compression ratios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear design; both REC and PLC contribute measurable improvements.\n2. Compatible with quantization (GPTQ) and generalizes to VLMs (LLaVA)."}, "weaknesses": {"value": "1. **Limited theoretical depth**:\n\n   - Eq. (5) defines the *effective scale*: $\\alpha = \\frac{m n}{m + n}$. \n   But no derivation or intuition is given for why this specific harmonic-like mean is appropriate for rank scaling.\n\n   - The per-layer rank formula: $r = (1 - R_\\ell)\\,\\alpha$. \n   This is heuristic and not theoretically motivated; it is unclear why the compression ratio $R_\\ell$ interacts linearly with $\\alpha$.\n\n   - The residual compensation rank: $r_r = \\alpha\\,\\beta$ (with $\\beta = 0.05$), but without justification for this constant or explanation of its influence on stability and reconstruction accuracy. In particular, it is unclear **why $r_r$ should scale linearly with the matrix “effective size” $\\alpha$** rather than being derived from empirical reconstruction error or singular value decay. A theoretical or empirical rationale for this proportionality assumption would substantially strengthen the paper.\n\n\n2. If my understanding is correct, the model’s final performance should be primarily determined by the discrepancy between the **compressed and original outputs of the final layer**.\nIn this case, concentrating the compression budget on the **first few layers** would indeed cause larger propagated errors, but distributing the same compression ratio **uniformly or toward the middle layers** (e.g., layers 2–32) should not necessarily lead to an error explosion in the last layer. Therefore, it is unclear **why the authors always start compressing from the last layers**, instead of allocating layer-wise compression ratios according to the *incremental reconstruction error* or sensitivity of each layer. Moreover, the authors should provide a **clear table of per-layer compression ratios and resulting ranks** under different overall compression targets (e.g., 20%, 40%, 60%), to make the method reproducible and to verify whether the layer-wise allocation aligns with the claimed error-control principle.\n\n\n3. **Missing comparison with latest and conceptually closest baselines**\n The paper only compares with ASVD, SVD-LLM (2025), and AdaSVD, but omits several important recent works that are directly relevant:\n   - SVD-LLM v2(https://arxiv.org/abs/2503.12340)\n   - Basis Sharing(https://openreview.net/pdf?id=gp32jvUquq)\n   - Dobi-SVD(https://openreview.net/pdf?id=kws76i5XB8)\n\n\n4. **Incomplete experimental reporting and outdated baselines**\n In Table 10, the authors only report results at a single compression ratio (20%) and compare solely against SVD-LLM.\n For a fair and convincing evaluation, results at **all tested compression ratios (20%, 30%, 40%, 50%, 60%)** should be reported, together with comparisons to other strong and recent baselines, including **SVD-LLM v2**, **Basis Sharing**, and **Dobi-SVD**.\n Moreover, most experiments should focus on newer architectures like **LLaMA-3**, **Qwen-2.5**, **Qwen-3**, not older models such as LLaMA-7B or LLaMA-2-7B."}, "questions": {"value": "Please see the weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FGRPz63Qlk", "forum": "WL4qCY0nBk", "replyto": "WL4qCY0nBk", "signatures": ["ICLR.cc/2026/Conference/Submission378/Reviewer_7ZH4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission378/Reviewer_7ZH4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828945304, "cdate": 1761828945304, "tmdate": 1762915507162, "mdate": 1762915507162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes two main techniques to improve SVD-based low-rank approximation for neural network compression: (1) residual compensation for SVD truncation, which reportedly outperforms traditional SVD-based methods; and (2) partial-layer compression, which focuses the SVD truncation on layers closer to the LM head to reduce output error. The paper also conducts extensive experiments to validate these ideas, demonstrating improvements over existing approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The idea of partial-layer compression for SVD is novel and addresses a meaningful issue in SVD-based compression.\n\n2.Experimental design is thorough, with comprehensive coverage of relevant evaluation points."}, "weaknesses": {"value": "The paper demonstrates a lack of mathematical understanding regarding SVD: from a theoretical perspective, the proposed residual compensation for SVD truncation is equivalent to a single SVD truncation and does not provide additional benefit.\n\nThe choice of partial compression ratio lacks theoretical justification or empirical explanation. Recent works such as SHEARED LLAMA use a learn-then-prune strategy for structured pruning, which is not considered here. The proposed “error” metric is also not used as a basis for further method selection or ablation studies.\n\nResults for high compression ratios are not compared against smaller pretrained models with similar compute, leaving open the question of relative effectiveness.\n\nhere are typos in the paper: for example, many figures still use “resSVD” even though the method is named ERC-SVD.\n\nThe baseline for SVD-LLM is incomplete: it should include Alpaca-based parameter retraining, but the paper appears to only implement the first stage, resulting in weaker baseline performance. For instance, the LLaMA-7B replication underperforms compared to reported SVD-LLM results.\n\nOther SVD baselines are also applied as whole-network compression, which may be an unfair comparison. Combining partial-layer compression with other SVD variants could further validate the effectiveness of the proposed methods."}, "questions": {"value": "1.Did you consider whether the gain from residual compensation for SVD truncation is due to calculation precision or stochastic variation? Were any ablation experiments performed to isolate this effect?\n\n2.Can you clarify exactly which layers are compressed at each partial compression ratio? The paper lacks details about per-layer compression ratios, making it difficult to manually verify computational fairness. Is there any risk of unintentional unfairness due to how compression ratios are defined (since SVD truncation at rank r requires storing two matrices, so the compression rate is actually r(n+m)/nm)?\n\n3.Is the reported speedup consistent with practical LLM scenarios? In real-world deployments, LLMs often scale with output length, increasing the proportion of attention computation. Thus, the speedup may not be as significant as reported for seq_len=32."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1M9xfcmofs", "forum": "WL4qCY0nBk", "replyto": "WL4qCY0nBk", "signatures": ["ICLR.cc/2026/Conference/Submission378/Reviewer_3F7n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission378/Reviewer_3F7n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895945128, "cdate": 1761895945128, "tmdate": 1762915507006, "mdate": 1762915507006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ERC-SVD, a post-training SVD-based LLM compression method, to address the issues of large truncation loss and severe error propagation in existing SVD solutions. Its core innovations are: 1) Two-stage residual compensation, utilizing the residual matrix generated by SVD truncation to reduce loss; 2) Partial-layer compression, compressing only the last few layers of the model under a fixed overall compression ratio to minimize error accumulation. Validated on 12 models from 5 LLM families and 10 benchmark datasets, ERC-SVD consistently outperforms baselines like traditional SVD and ASVD. For instance, under 30% compression ratio, the PTB perplexity of LLaMA-2-7B is reduced by 75% compared to SVD-LLM. It can also be extended to VLMs and combined with quantization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Two-stage residual compensation and partial-layer compression provide a new perspective for SVD compression.\n- Rigorous theory and strict experimental control ensure reliable results.\n- Complex technologies are described in plain language, with formulas and pseudocode for high readability."}, "weaknesses": {"value": "- Only the calculation process for residual compensation is given, without more detailed theoretical derivation from the loss function to explain why this method is effective.\n- A notable limitation of the experiments presented in the paper lies in the selection of baselines for comparison—specifically, the exclusion of newer SVD-based LLM compression methods that are highly relevant to the research topic, such as Dobi-SVD."}, "questions": {"value": "- The theoretical derivation needs to be supplemented to explain why residual compensation can reduce errors. \n- There was no comparison with newer baselines, such as Dobi-SVD.\n- The largest tested model (7B parameters) falls short of demonstrating scalability to larger-scale models (e.g., 70B), which are more representative of real-world deployment scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QuycRoYLE4", "forum": "WL4qCY0nBk", "replyto": "WL4qCY0nBk", "signatures": ["ICLR.cc/2026/Conference/Submission378/Reviewer_B5yp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission378/Reviewer_B5yp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995753359, "cdate": 1761995753359, "tmdate": 1762915506896, "mdate": 1762915506896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}