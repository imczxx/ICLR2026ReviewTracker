{"id": "k4wasQnZ3w", "number": 10791, "cdate": 1758182036767, "mdate": 1759897628576, "content": {"title": "Implicit Neural Representation Generation with Hypernetworks", "abstract": "Implicit Neural Representations (INRs) are versatile tools widely used for representing images, geometry, and radiance fields. By parameterizing target signals as neural networks, INRs provide superior advantages such as flexible resolutions. Recently, researchers have applied hypernetworks to generate INRs. However, current approaches either fail to capture dependencies between layers in the target network or lack scalability, limiting their effectiveness. In this paper, we introduce a novel hypernetwork framework that both models these dependencies and scales easily. Our approach treats the generation of target network parameters as an optimization process, using the chain rule to capture layer dependencies. We develop a simple yet effective tokenization mechanism that allows us to leverage Transformers as our architecture, ensuring scalability. Additionally, we introduce a practical weights initialization method that stabilizes the training process. Extensive experiments across various datasets consistently show our approach outperforms existing works.", "tldr": "We present a novel hypernetwork approach to generate Implicit Neural Representations", "keywords": ["Hypernetworks", "Implicit Neural Representations"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38d88a65d4e30f1a490e046a1016ab98614834e4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a Transformer-based hypernetwork framework for generating Implicit Neural Representations (INRs). The method addresses two key limitations of previous approaches: the lack of inter-layer dependency modeling and limited scalability. By viewing INR parameter generation as an optimization process, the authors employ cross-attention to model both the forward and backward gradient flows according to the chain rule, effectively capturing dependencies between layers.\n\nThe overall concept is interesting and theoretically sound, but the experimental evaluation is relatively weak, and the paper lacks sufficient implementation details to assess reproducibility fully."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and well-motivated concept. The paper introduces an original idea that connects gradient-based optimization theory with Transformer attention mechanisms for INR generation. Modeling inter-layer dependencies through cross-attention and the chain rule is both elegant and theoretically justified. \n\n2. Clear methodological design and firm theoretical grounding. The proposed architecture integrates attention-based gradient estimation, row-wise tokenization, and adaptive initialization coherently. The formulation is mathematically consistent and provides a principled alternative to previous MLP-based hypernetworks."}, "weaknesses": {"value": "1. Limited experimental rigor. Although the reported results are promising, the experiments are relatively shallow. There is little statistical analysis, no mention of variance across runs, and no comparison of computational efficiency or training time with baselines. \n\n2. Missing implementation details. Important architectural and training parameters are not fully specified, including the number of Transformer layers, embedding dimensions, token sequence lengths, and optimization hyperparameters. This makes reproducibility difficult. \n\n3. Weak evaluation of scalability. The paper frequently claims scalability, but there is no empirical evidence such as runtime analysis, memory consumption, or scaling curves for large datasets or high-resolution signals. \n\n4. Limited diversity of evaluation metrics. The experiments rely mainly on PSNR and simple accuracy measures. Additional metrics such as SSIM, Chamfer Distance, or LPIPS would provide a more complete evaluation, especially for 3D and visual quality tasks. \n\n5. Overfitting to specific architectures. The adaptive initialization and SIREN-based design seem tuned to a particular INR architecture. It is unclear whether the proposed method generalizes to other types of implicit networks, such as ReLU or Fourier-feature-based models. \n\n6. Presentation and clarity issues in the experimental section. The visual presentation of the results is weak. The figures are poorly designed and do not effectively support the claims made in the text. Figure 1 adds little value, as it does not clearly illustrate the proposed method or its components. Figure 4 shows very low-quality reconstructions, suggesting that the model may be undertrained or not properly converged. Figure 3 is poorly described and difficult to interpret, with unclear labeling and low visual readability. Overall, the experimental figures fail to convincingly demonstrate the strengths of the proposed approach."}, "questions": {"value": "1. Can the authors provide statistical measures (e.g., mean ± std over multiple seeds) to assess the stability of training?\n2. How does the proposed method compare to baselines in terms of computational efficiency, convergence speed, or training time?\n3. Is the code (or sufficient pseudocode) available to enable reproducibility of the reported results?\n4. What empirical evidence supports the claimed scalability of the approach?\n5. Why are only PSNR or simple accuracy measures used?\n6. Could additional perceptual or geometric metrics (e.g., SSIM, LPIPS, Chamfer Distance) provide a more comprehensive evaluation, particularly for 3D or visual reconstruction tasks?\n7. Does the method generalize beyond SIREN-based INRs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cuYGuUek7t", "forum": "k4wasQnZ3w", "replyto": "k4wasQnZ3w", "signatures": ["ICLR.cc/2026/Conference/Submission10791/Reviewer_Q7Jz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10791/Reviewer_Q7Jz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761046611618, "cdate": 1761046611618, "tmdate": 1762922006397, "mdate": 1762922006397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to improve implicit neural representation (INR) generation from hypernetworks by improving on HyPoGen. HyPoGen generates the weights of an INR by mimicking backward passes (i.e. INR optimization) with neural network modules. This paper improves on HyPoGen in three ways: 1) using an attention-based module to mimic gradient updates, 2) a way to tokenize INR parameters for input, and 3) an initialization strategy to stabilize training when the INR whose weights are being learned is SIREN. This paper evaluates on generation INRs for 2D images (CelebA-HQ, ImageNet), climate data (ERA5), and 3D data (ShapeNet chairs, NeRF dataset)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a new hypernetwork architecture, based on HyPoGen, inspired by the idea that representing the “backward pass” as a neural network module can be done with attention. The evaluation is done on several different tasks and datasets."}, "weaknesses": {"value": "**(W1) Novelty**: This method makes incremental improvements over HyPoGen, on which it is based.\n\n**(W2) Clarity**: From the writing, it is hard to understand how a forward pass of the model works, and it’s also hard to understand how the learned weights are ultimately used. It is also not very clear what loss functions (e.g. sample-supervised or weight-supervised) are used to train the models for each of the datasets.\n\n**(W3) Empirical evaluation**: This paper is missing baselines on some tasks. For novel view synthesis, there are many works using hypernetworks to accomplish this task [1-5], using a variety of different approaches. \n\nAdditionally, it is difficult to compare approaches because there is no comparison of the number of model parameters between models. With some of the models using the proposed method being quite large (up to 480M parameters, for the novel view synthesis task), it raises the possibility that the proposed method is performing better due to being much larger than the other models. Similarly, there is no indication that each of the models is using the same size INR, which could also improve results without demonstrating the superiority of the proposed method.\n\nI also don’t understand why accuracy is used to evaluate performance on 3D INR generation with ShapeNet chairs. Why not use PSNR, SSIM, LPIPS, FID? This metric seems less than ideal because it may reward reproducing all the background pixels correctly while not correctly rewarding reproducing the shape of the chair. \n\n**(W4) Potential limitations**: Related to the above limitation, it’s not clear if the proposed model needs to be supervised on the weights for good performance (c.f. Table 3). This is a major limitation due to the need to collect weight datasets for training. \n\n[1] Chen, Yinbo, and Xiaolong Wang. \"Transformers as meta-learners for implicit neural representations.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n[2] Kim, Chiheon, et al. \"Generalizable implicit neural representations via instance pattern composers.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[3] Gu, Jeffrey, Kuan-Chieh Wang, and Serena Yeung. \"Generalizable neural fields as partially observed neural processes.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[4] Lee, Doyup, et al. \"Locality-aware generalizable implicit neural representation.\" Advances in Neural Information Processing Systems 36 (2023): 48363-48381.\n\n[5] Gu, Jeffrey, and Serena Yeung-Levy. \"Foundation models secretly understand neural network weights: Enhancing hypernetwork architectures with foundation models.\" arXiv preprint arXiv:2503.00838 (2025)."}, "questions": {"value": "**(Q1)**: What are the computational resources used by the proposed method and baseline methods?\n\n**(Q2)**: Do all methods use the same size INRs and the same conditions (where relevant)?\n\n**(Q3)**: How does a full forward pass of the network proceed?\n\n**(Q4)**: Why is accuracy used instead of PSNR (or similar metrics) to evaluate performance on ShapeNet Chairs (Table 2)?\n\n**(Q5)**: How does the performance of image-only, weight-only, and image + weight supervision compare?\n\n**(Q6)**: For Table 3 (NeRF dataset), what is the performance of hypernetworks compared to the pre-trained NeRFs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nvfXK3aqOB", "forum": "k4wasQnZ3w", "replyto": "k4wasQnZ3w", "signatures": ["ICLR.cc/2026/Conference/Submission10791/Reviewer_3wHy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10791/Reviewer_3wHy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761153958605, "cdate": 1761153958605, "tmdate": 1762922005800, "mdate": 1762922005800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel hypernetwork for generating INRs by explicitly modeling the target network's optimization process. Its Transformer-based architecture simulates both forward and backward (gradient flow) passes to capture inter-layer dependencies. The method achieves state-of-the-art performance on diverse 2D and 3D tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The main idea of structuring the hypernetwork to explicitly simulate the gradient backpropagation of the target network is highly original. It connects optimization theory directly to the architecture design, which is very elegant.\n\n2. By tackling the inter-layer dependency issue in a scalable way, this work addresses a very important and fundamental problem in hypernetwork-based INR generation. The findings could influence how future generative models for neural network weights are designed."}, "weaknesses": {"value": "1. Attributing the performance gains to the novel gradient-flow simulation is not fully convincing. The experiments lack a crucial control: a generic Transformer of comparable capacity but without the specialized architecture. It is therefore unclear if the improvements stem from the paper's core conceptual insight or simply from a higher model capacity than the baselines.\n\n\n2. The proposed architecture introduces significant computational overhead compared to simpler methods. However, the paper provides no comparison of training time, inference speed, or memory usage. This makes it difficult to judge the practical trade-off. Without this information, the claims of superiority are not complete.\n\n3. The paper shows that modeling dependencies is better, but it doesn't explore when it becomes truly necessary. For simpler tasks, a simpler model like LDMI which ignores these dependencies might be \"good enough,\" and its performance loss is very small (results in Tab.2). There might be a \"critical point\" of task complexity where explicitly modeling gradient flow becomes crucial. The paper does not provide any insight into this. Understanding this boundary is important for guiding researchers on whether they should adopt such a complex and costly architecture for their specific problems."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0PhgHm0n3S", "forum": "k4wasQnZ3w", "replyto": "k4wasQnZ3w", "signatures": ["ICLR.cc/2026/Conference/Submission10791/Reviewer_aU3q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10791/Reviewer_aU3q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770743096, "cdate": 1761770743096, "tmdate": 1762922004727, "mdate": 1762922004727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hypernetwork decoder for INR generation that leverages cross-attention layers to approximate mathematical dependencies between network parameters and activations in both forward and backward passes. This decoder is tested in combination with a variational autoencoder to generate INRs of images, 2D meteorological data, voxel grids, and NeRFs, which are shown to display better reconstruction quality than previous INR generation methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of this paper lies in its motivation and underlying intuition: previous work (HyPoGen) has shown success in modeling chain-rule-based layer dependencies within hypernetwork architectures, but has done so through MLPs. Cross-attention, on the other hand, provides an off-the-shelf primitive that naturally captures pairwise relationships such as those encoded in Jacobians computed during the backward pass. Rather than relying on MLPs as generic function approximators to learn these relationships from scratch, the authors propose leveraging cross-attention to equip the hypernetwork with a stronger inductive bias.\n\nThe experimental results confirm this intuition, as the proposed method is shown to outperform competitors in INR reconstruction quality. Furthermore, to the best of my knowledge, this is the first INR generation paper to include the NeRF dataset by [1] in its experimental evaluation, which is an interesting addition to the usual datasets featured in previous works on this topic.\n\n---\n\n[1] Zama Ramirez et al. Deep Learning on Object-centric 3D Neural Fields. TPAMI 2024."}, "weaknesses": {"value": "The main weakness of this paper is its pervasive lack of clarity, both in the method definition and in the description of the experimental settings. This lack of clarity involves meaningful portions of the paper, most notably:\n1. The training objective used in the experiments (see **Q1**).\n1. How to go from the output of the backward module $\\partial f_\\theta(\\mathbf{x})/\\partial\\theta_i$ to the predicted INR parameters $\\theta$ (see **Q2**).\n1. The specifics of the INR initialization strategy, listed as one of the paper's contributions (see **Q4**).\n1. Several other missing/unclear experimental results/details (see **Q3**, **Q5–Q9**).\n\nFurthermore, the proposed row-wise tokenization of parameter matrices should be regarded as a naive baseline rather than a contribution (as claimed by the authors), as it is a straightforward solution that does not account for neural network parameter symmetries [1, 2].\n\n---\n\n[1] Hecht-Nielsen. On the algebraic structure of feedforward network weight spaces. Advanced Neural Computers, 1990.\n\n[2] Godfrey et al. On the symmetries of deep learning models and their internal representations. NeurIPS 2022."}, "questions": {"value": "Answers to the following questions might lead to a change in my rating:\n\n- **Q1.** Sec. 3 introduces two types of loss: sample-supervised and weight-supervised.\n    - Which one of those is used in the experiments of Sec. 5? Sample-supervised for 2D INRs and ShapeNet Chairs, whereas weight-supervised for NeRFs? If so, what does \"weight & image\" supervision mean in the NeRF experiment?\n    - Is the VAE output supervised at all in any of the experiments of Sec. 5? If so, how are the VAE and hypernetwork losses combined?\n    - Why is there a summation over tasks $\\tau$ in Eq. 2 and 3? Assuming that each experiment is Sec. 5 (i.e., each dataset) is a \"task\", shouldn't the loss be computed on a single $\\tau$?\n\n    To answer the above questions, could the authors provide complete loss equations describing the supervision used in each experiment of Sec. 5?\n\n- **Q2.** Sec. 3 describes optimization-biased hypernetworks as producing $\\Delta\\theta$ as output and then computing INR parameters iteratively as $\\theta^{(t)}=\\lambda\\Delta\\theta^{(t-1)}+\\theta^{(t-1)}$. \n    - How does your method go from the output of the backward module $\\partial f_\\theta(\\mathbf{x})/\\partial\\theta_i$ to $\\Delta\\theta$?\n    - And from  $\\Delta\\theta$ to $\\theta$? In other words, how does the iterative process work in your framework exactly? \n    - Which value does $\\lambda$ have? And $T$? (Assuming $t=0\\dots T$).\n    - Could the authors expand the equations provided as answers to Q1 by taking into account that the actual output of the hypernetwork is $\\Delta\\theta$ instead of $\\theta$?\n\n- **Q3.** Which layer does the $\\tau_*$ input to the cross-attention module computing $\\partial\\textbf{h}_i/\\partial\\theta_i$ belong to?\n    - \"Structure Overview\" in Fig. 2 says $i$, \"Backward Module\" in Fig. 2 says $i-1$, Eq. 8 says $i-1$, and the text below the equation says $i$.\n\n- **Q4.** Sec. 4.2 (\"Adaptive Parameter Initialization\") leaves key details unexplained:\n    - How is $s_\\tau$ computed exactly?\n    - How is $s_\\tau$ \"dynamically adjusted\" exactly?\n    - What is the relationship between the content of Sec. 4.2 and that of App. A.2.1 (\"Token Initialization Network\")?\n\n- **Q5.** Why is the VAMoH PSNR on ImageNet missing in Tab. 1?\n    - Could the authors compute it?\n\n- **Q6.** Why does the ShapeNet Chairs experiment use the percentage of correct occupancy predictions as metric? \n   - Could the authors also provide the PSNR?\n\n- **Q7.** Why are HyperDiffusion results reported for the NeRF experiment only (Tab. 3)? \n    - Could the authors compute HyperDiffusion results for 2D INRs and ShapeNet Chairs also? \n    - Why does the HyperDiffusion PSNR have the same value (20.0) for both types of supervision in Tab. 3?\n\n- **Q8.** Sec. 5.3 (\"Ablation Studies\") lacks relevant details:\n    - What does \"Q-KV reverse\" mean exactly? Does it involve every cross-attention equation?\n    - What do the authors mean by \"single optimization layer\"?\n    - Where are the results mentioned in the following sentence?\n    > This initialization is essential for SIREN-based INRs, significantly improving CelebA-HQ reconstruction quality from 11.8 to 26.1.\n\n        On that note, could the authors extend the results in Tab. 4 to the other 2D datasets and to ShapeNet Chairs?\n    - Which section of the appendix does this sentence refer to?\n    > For more analyses, please refer to the appendix.\n\n- **Q9.** App. A.2.2 claims that Tab. 5 validates the importance of inter-layer dependency modeling. However, the enhanced LDMI variant does not always lead to an improvement compared to standard LDMI in Tab. 5. \n    - Could the authors comment on these results?\n\nMinor comments/typos:\n- What is the difference between \"Input\" and \"GT\" in Fig. 4?\n- The authors should add the year to the HyPoGen reference (Ren et al.)\n- Line 081: geometry → geometric\n- Line 131: maps\n- Line 132: $(c,\\sigma)$ → $(\\textbf{c},\\sigma)$\n- Eq. 2: $\\textbf{y}_\\tau$ is not defined\n- Lines 160–161: $z_\\tau$ → $\\mathbf{z}_\\tau$\n- Line 294: downstream activations → parameters"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tYLb14TNbQ", "forum": "k4wasQnZ3w", "replyto": "k4wasQnZ3w", "signatures": ["ICLR.cc/2026/Conference/Submission10791/Reviewer_yYeZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10791/Reviewer_yYeZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930284476, "cdate": 1761930284476, "tmdate": 1762922004337, "mdate": 1762922004337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}