{"id": "28ve0ItkGl", "number": 9750, "cdate": 1758138071695, "mdate": 1759897700669, "content": {"title": "Inference Time Alignment for Code Auditing", "abstract": "Software auditing is an increasingly critical task in the era of rapid code generation. While LLM-based auditors have demonstrated strong potential, their effectiveness remains limited by misalignment with the highly complex, domain-specific nature of bug detection. In this work, we build on recent advances in inference-time alignment and propose a novel technique that draws inspiration from human auditing practices. Our method structures auditing into a three-step workflow and aligns LLMs to each step by studying real bug reports, generating diverse examples, and abstracting concise alignment guidelines.\nOn a curated dataset of 40 real-world bugs from 21 widely used open-source projects, BugScope achieves 86.05% precision and 87.88% recall, corresponding to an F1 score of 0.87. By comparison, leading industrial tools such as Cursor BugBot and CodeRabbit achieve F1 scores of only 0.43 and 0.29, respectively. Beyond benchmarks, large-scale evaluation on projects such as the Linux kernel uncovered 141 previously unknown bugs, of which 78 have already been fixed and 7 explicitly confirmed by developers.", "tldr": "An alignment-based approach that empowers LLMs to detect complex software bugs through structured reasoning.", "keywords": ["agent", "code reasoning", "code auditing", "bug detection"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c6b90b2d27c8cbcc759e625762af89749f00c53.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces BugScope, a framework that leverages LLMs for software bug detection. The method emulates a human expert's auditing process through a three-step workflow: (1) identifying a suspicious code seed, (2) retrieving relevant code context, and (3) applying detection rules. A key aspect of BugScope is its learning phase, where it learns retrieval and detection guidelines from real bug reports to synthesize agent strategies and prompts, all without model retraining. An empirical evaluation on a curated dataset of 33 bugs from popular open-source projects shows that BugScope outperforms several industrial and research baselines (RepoAudit, Cursor BugBot, CodeRabbit, Meta Infer) in F1 score. Furthermore, the paper provides evidence of its capability to detect novel bugs in large, real-world codebases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, clearly structured, and easy to follow. The motivating examples and figures effectively illustrate the core concepts of the approach. \n2. The discovery and confirmation of a large number of previously unknown bugs in major projects like the Linux kernel is a practical achievement and demonstrates that the system is capable of producing valuable results. \n3. Multiple baselines, ablation studies, and real‑world case studies are provided."}, "weaknesses": {"value": "1. The definition of BUGSCOPE in the abstract is somewhat ambiguous.\n2. A potential limitation is the scale of the primary benchmark. While the 33 bugs used for testing are acknowledged to be complex, this small sample size makes it difficult to draw broad conclusions about the method's generalizability and statistical significance. The findings would be substantially stronger if validated on a larger and more diverse set of bugs. \n3. The evaluation primarily focuses on bug types (e.g., Out-of-Bounds, Divide-by-Zero) that are relatively well-structured and have been targets for traditional static analysis. It remains unclear how the proposed \"alignment by example\" approach would generalize to more complex, semantic bug classes that are less structurally defined, such as race conditions, logic errors, or security vulnerabilities like buffer overflows, SQL injection, and Cross-Site Scripting (XSS)."}, "questions": {"value": "1. To facilitate reproducibility and encourage future research, do the authors plan to release the source code and the curated dataset?\n2. The paper doesn't discuss the monetary or computational cost of baselines. Does BugScope have cost advantage?\n3. I noticed that some bug reports or commits listed in Table 3 date back to 2022. Given that many LLMs have training data cutoffs around 2023-2024, could the authors clarify how they ensured that the test cases were not part of the pre-training data of the LLMs used in the framework?\n4. To better assess the generalization capabilities of the learning phase, it would be insightful to see how BugScope performs on bug types that were explicitly not included in the learning stage. Have any such `zero-shot` generalization experiments been conducted? \n5. The discovery of 141 new bugs in the Linux kernel is impressive. The paper states that 78 have been fixed and 7 confirmed. Could the authors clarify the status of the remaining 56 reports (141 - 78 - 7)? If these are considered false positives, the effective precision on new discoveries would be approximately 60% ((78+7)/141). While high for bug detection, this could still lead to considerable `alert fatigue` for developers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "itrtk0Q13H", "forum": "28ve0ItkGl", "replyto": "28ve0ItkGl", "signatures": ["ICLR.cc/2026/Conference/Submission9750/Reviewer_8ctR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9750/Reviewer_8ctR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761036250203, "cdate": 1761036250203, "tmdate": 1762921244526, "mdate": 1762921244526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The study proposes a novel technique for software auditing, BUGSCOPE, which aligns LLMs with each step of the auditing process. The method, based on real bug reports and examples, achieves 86.05% precision and 87.88% recall on a dataset of 40 real-world bugs from 21 open-source projects, outperforming leading industrial tools like Cursor BugBot and CodeRabbit."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ A good work in improving the performance of current code auditing techniques.\n+ The authors introduce a structured code auditing workflow (seed → retrieval → detection).\n+ The evaluation is comprehensive."}, "weaknesses": {"value": "- The paper focuses only on three bug categories: OOB, DBZ, and MLK. It’s unclear how well the approach generalizes to other vulnerabilities.\n- A few technical details should be clarified."}, "questions": {"value": "1. Section 3.1, Data Augmentation: The validity and reliability of the generated synthetic data should be justified and evaluated, since it is the basis for the following steps.\n\n2. The found bugs's link and issue ID are not shown in the manuscript.\n\n3. Section 3: How deep (in terms of data dependency) can BUGSCOPE dive into? How many files can be covered at the same time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "esHqgdD98n", "forum": "28ve0ItkGl", "replyto": "28ve0ItkGl", "signatures": ["ICLR.cc/2026/Conference/Submission9750/Reviewer_rayu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9750/Reviewer_rayu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761393562370, "cdate": 1761393562370, "tmdate": 1762921244271, "mdate": 1762921244271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper „Inference Time Alignment for Code Auditing” presents an approach that integrates symbolic analysis of code (control and dataflow) for retrieval of relevant code snippets with an LLM-based approach to identify specific types of bugs. The tool goes beyond prior work by structuring the identification workflow to first retrieve the relevant context in a guided manner, and only then try to identify the bug. The developed tool BugScope is tested for three categories of bugs, i.e., out-of-bounds, divide-by-zero, and memory leaks. The authors test the tool against other LLM-based tools on 40 bugs and find that their approach outperforms the related work. An application of the method in the wild led to the discovery of multiple bugs that were reported and are already partially fixed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I like the way the context is built step by step in a guided manner. This is a good and original strategy. Moreover, the real-world impact demonstrated by finding and reporting actual bugs is a good way to demonstrate potential."}, "weaknesses": {"value": "While I like the general approach and the paper reports results that indicate that the approach may outperform prior work, the paper still has severe problems. Many details required for reproduction are missing and the soundness of the results based on such small sample sizes is questionable. Moreover, the presentation of the paper has multiple problems. Please find the details below.\n1) The main comparison is based on an extremely small sample of bugs: 33 bugs for testing, i.e., on average 11 per class of bug, and 4.7 per anti-pattern. Performance values on such small numbers cannot be trusted and are not sufficiently sound to demonstrate a robust and reliable performance improvement over prior work. \n2) There is no use of statistical methods to determine if any differences are significant or if they can be explained by random effects. \n3) Many details regarding how data collected are missing. While the authors list some prior works, I see nothing on existing bug data sets (e.g., for vulnerabilities like Devign or bugs in general like Defects4j, bugs.jar, etc), or how the sample of 40 bugs was selected based on these prior works.\n4) The application to real-world projects lacks information required to understand the process for selecting the targets, which is important to understand the quality of the results. There is some limited information for the Linux kernel issues, but basically nothing that explains anything regarding the data reported in Table 2. It is unclear how the projects were selected, and which kind of experimentation was conducted for each project, other than that the number of seeds per repository was limited. Moreover, it is unclear to if there are false positives and how they were handled. Note that this last point might also lead to ethical issues (see below).\n5) I would not be able to re-implement the work based on the level of detail of the reporting. Many important details are left out. As an extreme case, consider page 5, Line 255: “These steps rely on standard techniques and are our main contributions”. The only information we have is that “an agent is employed” for this. How this agent works and why an agent is chosen over algorithmic approaches for defect data collection (e.g., based on some SZZ algorithm variant), remains unclear. The other aspects of the method description have similar issues. For example, Page 6, Line 297 states that “relevant code should be collected by tracing data dependence and control dependence”. How exactly this works (e.g., based on a data flow graph of control flow graph), which steps are used, etc., is left unexplained. \n6) The authors argue that manually defined patterns (even large amounts of patterns) are still insufficient (Page 4, Line 184). However, they never back this claim with data. They never study how LLM-inferred rules compare to a large set of human-curated rules for the bug types they study. This needs to be put into proper context reducing the claims or data need to be provided that the suggested method is better than human experts defining rules. \n7) Many acronyms are not properly introduced. This includes the bug type “NPD” for the linux kernel. The acronyms for the anti-patterns are only introduced in the Appendix. While is in general okay, the main body at least needs to specify where this information can be found. \n8) A possible way to make room for more details is to avoid redundancies in the first sections. The general three-step process is explained multiple times, including twice in the introduction, once in the motivation and twice when explaining the idea. While is explanation is somewhat different from the others, often adding some piece of information, large portions are also redundant. \n9) All references in the paper are wrongly formatted (missing brackets), probably because this was converted from a different LaTeX style without checking."}, "questions": {"value": "1) Is there more data to support the validity the results and can it be shown that improvements are non-random?\n2) Is there any data that supports that this approach beats manually curated guidelines?\n3) How exactly was the application to the real-world software conducted?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Experimentation in the wild possibly requires an ethics statement. If this applies here is unclear. If the authors manually validated all bugs prior to reporting, I do not see an issue. If the authors reported all detected bugs and used the developers to determine if issues were true positives or false positives, this means they used the developers as human subjects for experimentation and used the work of OS developers to label their data. The latter would be a breach of good conduct and a practice that was multiple times vigorously opposed by the open-source community, the most extreme case being a study in which actual vulns were introduced as “test” (https://www.theverge.com/2021/4/30/22410164/linux-kernel-university-of-minnesota-banned-open-source). Which is the case is unclear based on the presentation within the paper."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uvCiweBWu2", "forum": "28ve0ItkGl", "replyto": "28ve0ItkGl", "signatures": ["ICLR.cc/2026/Conference/Submission9750/Reviewer_AwzN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9750/Reviewer_AwzN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553314364, "cdate": 1761553314364, "tmdate": 1762921243688, "mdate": 1762921243688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presented BugScope, a framework that used inference-time alignment to adapt LLMs for the complex task of code auditing. Specifically, BugScope decomposed the auditing process into a structured three-step workflow and synthesized alignment guidelines from examples. Evaluation was conducted on a set of 40 bugs (7 for learning and 33 for experiment ) from three different types. Results show BugScope can achieve good performance compared to the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ focus on a practical task\n+ good performance\n+ sound model architecture"}, "weaknesses": {"value": "1. The experiment dataset is too small to be reliable, and can be biased:\n\nThe primary evaluation is conducted on a dataset of only 40 bugs, with 7 used for training/guideline synthesis and 33 for testing. This scale is too small to provide statistically reliable conclusions about the framework's overall effectiveness. The selection of these specific 40 bugs from prior work introduces a risk of selection bias, potentially favoring cases that are well-suited to the method. The performance on a larger, more representative, and randomly sampled set of vulnerabilities remains unproven.\n\n2. Cover only three types of bugs: \n\nThe evaluation is limited to three classic bug types, i.e., Out-of-Bounds, Divide-by-Zero, and Memory Leak. While these are important and diverse categories, the framework's effectiveness on other types of vulnerabilities remains unknown. The paper should expand its experiment dataset and include more types of vulnerabilities.\n\n\n3. Inconsistent and indirect baseline comparisons:\n\nIn the comparison, the baselines were not re-evaluated on the exact same datasets and under the same experimental conditions (e.g., the same file scope for seed extraction). This makes it difficult to attribute the performance gap solely to BugScope's superiority. A direct, head-to-head comparison on a unified benchmark is required for a fair assessment.\n\n\n4. randomness in the data split\n\nWith such a small dataset and a fixed split, there is an inherent risk that the \"learning\" bugs and the \"evaluation\" bugs are overly similar, potentially leading to an overfitting scenario where the synthesized guidelines work well on this specific set of 33 bugs but fail to generalize to a truly different set of vulnerabilities."}, "questions": {"value": "How were the 7 bugs for the learning phase selected from the total pool of 40? Was this a random selection? If not, what criteria were used, and how do you justify that this split does not introduce bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XjLzghl0Vb", "forum": "28ve0ItkGl", "replyto": "28ve0ItkGl", "signatures": ["ICLR.cc/2026/Conference/Submission9750/Reviewer_uESd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9750/Reviewer_uESd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751707590, "cdate": 1761751707590, "tmdate": 1762921243206, "mdate": 1762921243206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}