{"id": "eMnRH5Yes3", "number": 8615, "cdate": 1758092639622, "mdate": 1759897773223, "content": {"title": "Interpretable Vision Tasks via Vision Logic Model Integrating Visual Reasoning and Textual Explanation", "abstract": "Despite remarkable advances in computer vision, most state-of-the-art models remain black boxes, offering limited insight into their decision-making processes. In this paper, we propose an interpretable vision logic model that enhances the transparency and trustworthiness of vision tasks, including classification, detection, and segmentation. Our framework not only produces standard outputs (e.g., class labels, bounding boxes, segmentation maps) but also generates dynamic visual reasoning videos and natural language explanations that reveal the underlying logic behind each prediction. Particularly, for each input image, our model visualizes the step-by-step reasoning process, highlighting critical features, attention regions, and intermediate decisions through a video output. In parallel, a textual explanation module provides a rationale in human-understandable language, offering additional context and interpretability. This end-to-end approach allows users to simultaneously obtain the main vision task result, a transparent visual narrative of the decision process, and an interpretable explanation, all within a unified framework. Experiments on standard vision benchmarks demonstrate that our method delivers high task accuracy while significantly improving explainability and user trust. Our vision logic model paves the way for more interpretable and accountable AI in critical applications, such as medical imaging, autonomous driving, and industrial inspection.", "tldr": "We propose an interpretable vision logic model that performs vision tasks while generating video-based visual reasoning and natural language explanations for transparent AI decisions.", "keywords": ["Interpretable Vision", "Explainable Artificial Intelligence", "Visual Reasoning"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4e56d18c69a960da262a1781de5c03295377edb5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Logic Vision Model (LVM), an architecture designed to unify prediction, visual reasoning, and natural-language explanation in computer vision. Instead of relying on post-hoc interpretability methods, LVM embeds a logic-memory module that stores reasoning prototypes and produces step-by-step inference trajectories. The model jointly outputs task predictions, visual reasoning sequences, and grounded textual explanations, enforcing monotonic evidence accumulation to ensure faithfulness. Experiments span classification, detection, segmentation, and reasoning benchmarks, showing competitive task performance while improving faithfulness metrics and human-rated explanation quality. Ablation studies suggest each architectural component (logic memory, transition operator, faithfulness loss) contributes meaningfully. The work also explores personalization and federated learning, demonstrating adaptability in privacy-sensitive or expert-feedback settings. Overall, LVM aims to provide a scalable route to intrinsically interpretable vision systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation for intrinsic interpretability and well-positioned against post-hoc explainability.\n\n2. Comprehensive experimental suite (CV tasks, explanation metrics, ablations, causal tests, etc).\n\n3. Claims competitive accuracy while improving faithfulness — if substantiated, impactful direction."}, "weaknesses": {"value": "1. The paper frames its method as logic-based reasoning, but the architecture appears closer to prototype attention + gated refinement rather than symbolic or rule-grounded logic. It is unclear how prototypes correspond to interpretable concepts, how they are initialized or evolve, and whether they truly encode logical structure as opposed to latent embeddings shaped by supervision. The term “logic” risks being aspirational rather than technically justified unless additional clarity or formalization is provided.\n\n2. While comparisons to Grad-CAM, ProtoPNet, and VLM justifications are included, the paper does not benchmark against recent intrinsic interpretability methods (e.g., mechanistic ViT explainability, slot-attention, causal representation learning, visual chain-of-thought models). Without these, the contribution risks feeling incremental. Just by adding attention-driven iterative refinement and constraints rather than establishing a new interpretability paradigm.\n\n3. Several components enforce monotonic confidence and explanation alignment, which could cause the model to learn plausible explanatory artifacts rather than faithful causal reasoning. Although the paper includes causal intervention tests, deeper probes are needed: e.g., OOD generalization, probing for spurious correlations, concept emergence analysis, or human-interpretable grounding of prototypes. Without these, the interpretability benefits remain largely behavioral rather than mechanistically demonstrated.\n\n4. Some key training and evaluation details are missing or lightly described. such as prototype semantics, human evaluation design, annotation criteria, and relative compute/training cost. This makes reproducibility difficult and leaves ambiguity around whether improvements persist across seeds, architectures, and data domains. More explicit algorithm steps, prototype visualizations, and explanation trace studies would strengthen trust in the method."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0DC3fYLTuo", "forum": "eMnRH5Yes3", "replyto": "eMnRH5Yes3", "signatures": ["ICLR.cc/2026/Conference/Submission8615/Reviewer_T5a5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8615/Reviewer_T5a5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804220747, "cdate": 1761804220747, "tmdate": 1762920453308, "mdate": 1762920453308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Logic Vision Model (LVM), a unified, interpretable vision framework that jointly performs prediction, reasoning, and explanation. LVM integrates a vision encoder (ResNet or ViT) with a logic-memory module and a transition operator that together generate sequential reasoning trajectories. These trajectories yield both visual reasoning videos and textual rationales, enabling intrinsic interpretability rather than post-hoc explanation. A monotonic confidence constraint enforces causal faithfulness between reasoning steps and final predictions, while personalization and federated extensions demonstrate adaptability to user-specific and privacy-preserving settings.\nEmpirical evaluations demonstrate competitive performance compared to leading baselines (ImageNet Top-1 = 82.6 vs. 82.3 for LLaVA; COCO mAP = 43.5 vs. 42.9) and notable gains in interpretability (Pointing Game = 71.8 vs. 63–66; BERTScore = 0.847 vs. 0.832–0.835). Ablation, personalization, and causal-masking analyses further substantiate the model’s design and claims."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "• LVM integrates visual reasoning trajectories and textual rationales within a single architecture, achieving intrinsic rather than post-hoc explainability.\n• Consistent accuracy is maintained while achieving measurable interpretability improvements (Pointing Game +8 points; BERTScore +0.015) across multiple benchmarks.\n• Prototype-masking experiments reveal correlated drops in both accuracy and rationale quality, substantiating that explanations reflect genuine causal dependencies.\n• The framework performs reliably across classification, detection, and segmentation tasks, supported by clear dataset splits and reproducible experiment design.\n• Personalization and federated extensions demonstrate promising directions toward privacy-preserving and user-aware trustworthy-AI applications.\n• The paper reports detailed implementation settings, dataset usage, and loss formulations, facilitating replication and follow-on research."}, "weaknesses": {"value": "1. Reasoning videos are insufficiently evidenced. The main text does not include qualitative “reasoning videos,” and only an appendix metric (temporal total variation of attention) is reported; qualitative examples are absent.\n2. Moderate novelty of primitives. The logic memory is a matrix of prototype-like slots with attention retrieval, and the transition operator is a gated update—both of which are closely related to known mechanisms; the contribution is primarily in their integration.\n3. Potential over-regularization from the faithfulness constraint. The monotonic confidence rule is enforced (removed only in a single ablation), but there’s no focused study of failure cases (e.g., multi-object ambiguity) or calibration effects beyond that one row.\n4. Small-scale federated evaluation. Only C=5 clients on ChestX-ray14 and KITTI are simulated, with sub–0.5–point differences compared to centralized training, insufficient to establish real-world scalability.\n5. Human-evaluation protocol under-specified. While 1–5 ratings are reported (correctness/sufficiency/clarity), annotator count, selection, instructions, and inter-rater reliability are not provided.\n6. Missing newer VLM baselines. Comparisons stop at BLIP-2 and LLaVA; recent systems with intrinsic explanations (e.g., LLaVA-Next, InstructBLIP, Kosmos-2.5, MiniGPT-v2, GPT-4V) are not included in this analysis.\n7. Gaps in causal-faithfulness framing. Evaluation emphasizes classic plausibility/overlap metrics; newer causal/faithfulness frameworks (e.g., recent ICLR/NeurIPS/ACL metrics) are not discussed or cited.\n8. Unquantified systems costs. Beyond a hyperparameter sensitivity table, there’s no latency/throughput/energy analysis versus reasoning steps or prototype count, leaving scalability trade-offs unclear."}, "questions": {"value": "1. Could you provide qualitative examples or quantitative evaluations that demonstrate the coherence and faithfulness of the generated reasoning videos, beyond the reported temporal-smoothness metric?\n2. How does the monotonic confidence constraint (Δpₜ ≥ 0) perform in multi-label or visually ambiguous scenes? Does it risk suppressing valid but non-monotonic reasoning patterns?\n3. To what extent does the logic-memory module generalize across domains, and what is the additional computational or memory overhead relative to ViT-B/16 and BLIP-2 baselines?\n4. Could you clarify the setup of the human evaluation (annotator pool, criteria, inter-rater reliability) and explain the rationale for excluding more recent vision–language model baselines such as LLaVA-Next, Kosmos-2.5, or GPT-4V from comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CwKV5OG873", "forum": "eMnRH5Yes3", "replyto": "eMnRH5Yes3", "signatures": ["ICLR.cc/2026/Conference/Submission8615/Reviewer_bZT6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8615/Reviewer_bZT6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883010318, "cdate": 1761883010318, "tmdate": 1762920452929, "mdate": 1762920452929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an interpretable reasoning framework that integrates prototypical reasoning with textual explanations. Specifically, it learns a set of latent prototypes and uses cross-attention to combine them with visual features. A multi-step reasoning is implemented to iteratively integrate the information. In addition to predicting task outputs, the model is also trained to generate textual explanations. Experimental results show that the proposed method shows better performance than generic models and also offers enhanced interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper focuses on developing models with transparent decision-making processes, which is crucial for systems for applications involving high-stake decisions (e.g., medical diagnosis).\n\n(2) The paper explores applications  in diverse settings, e.g., federated learning and learning with user feedback.\n\n(3) The proposed method generalizes across multiple tasks and shows consistent improvement."}, "weaknesses": {"value": "(1) It is not new to incorporate prototypical reasoning for interpretable classification (e.g., [ref1, ref2]), and the use of learnable latents with cross-attention is also widely explored (e.g., subsequent using [ref3]). Textural explanation generation has also been studied in earlier works (e.g., [ref4, ref5]). As a result, the technical contribution of the paper is relatively incremental. \n\n(2) One of the principal ideas of prototypical reasoning is to learn interpretable prototypes. The paper does not provide strong evidence/analysis on what is being learned within the prototypes, and whether or not they are interpretable.\n\n(3) A key component of the paper is the iterative fusion and reasoning process. Nevertheless, the paper does not provide an in-depth analysis of how this paradigm helps task accomplishment and what kind of information is being updated/aggregated over time.\n\n(4) In the abstract, the authors highlight the model for generalizing reasoning videos. However, I can not find corresponding examples or quantitative results in the paper.\n\n(5) I found some of the statements in the paper contradictory. For instance, Section 3.2 states that the vision encoder is explicitly designed for reasoning, while Section 3.3 mentions that the encoder provides generic semantic features. \n\n(6) The paper only compares with generic models and also does not include the more recent ones. As the method essentially requires supervised training on specific tasks, it is reasonable to compare with state-of-the-art models for each task (and other prototypical/interpretable methods). Furthermore, it would be better to integrate the method with these models, as modularity is one of the emphasis throughout the paper.\n\n(7) Looking at Table 6, it appears that the effect of masking/permutating the top-k prototypes is not very big (despite being larger than random masking). I would suggest experimenting with different k to study the contribution of prototypes.\n\nReferences:\n\n[ref1] This Looks Like Those: Illuminating Prototypical Concepts Using Multiple Visualizations. NeurIPS, 2023.\n\n[ref2] Interpretable Image Classification with Adaptive Prototype-based Vision Transformers. NeurIPS, 2024.\n\n[ref3] Perceiver io: A general architecture for structured inputs & outputs. ICLR, 2022.\n\n[ref4] Multimodal explanations: Justifying decisions and pointing to the evidence. CVPR, 2018.\n\n[ref5] e-SNLI: Natural Language Inference with Natural Language Explanations. NeurIPS, 2018."}, "questions": {"value": "(1) Please justify the key novelty of the proposed method over prior studies.\n\n(2) What do the prototypes learn? Are they interpretable? \n\n(3) How does the iterative process help with task accomplishment? How does the attention weight change over time?\n\n(4) Does the model actually generate videos for explanation?\n\n(5) In addition to evaluation with automatic metrics designed for machine translation (e.g., BLEU and ROUGE-L), please consider providing the actual explanation results and conduct user study to validate their usefulness.\n\n(6) Please consider applying the method on state-of-the-art models and include the corresponding comparisons."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YVNbsadbc6", "forum": "eMnRH5Yes3", "replyto": "eMnRH5Yes3", "signatures": ["ICLR.cc/2026/Conference/Submission8615/Reviewer_Nghw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8615/Reviewer_Nghw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946056130, "cdate": 1761946056130, "tmdate": 1762920452533, "mdate": 1762920452533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Logic Vision Model (LVM), a vision architecture that jointly produces (a) task outputs (classification/detection/segmentation), (b) a multi-step “reasoning trajectory” (attention/state transitions), and (c) a textual explanation that is conditioned on that trajectory. The core components are: a standard vision encoder, a logic memory of prototype-like slots retrieved via attention, and a transition operator that evolves the reasoning state across steps. Explanations are generated by a decoder that attends over the trajectory; the training loss combines task loss, alignment to annotated attention/rationales, and a monotonic confidence constraint along the trajectory to encourage faithfulness. Experiments cover common CV benchmarks and explanation datasets; ablations remove memory/transition/losses; the paper also sketches “personalization” and “federated” variants"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. **Intrinsic, step-grounded explanations**\n   - The explanation decoder is conditioned on the same multi-step reasoning trajectory\n     $\n     R=\\{r_t\\}_{t=1}^T\n     $\n     that drives prediction. During generation, tokens attend over $R$ and the aggregated\n     reasoning state $\\bar r$, so each piece of text is tied to concrete inference steps.\n     The paper shows alignment plots where explanation tokens point back to specific\n     steps in $R$, and qualitative examples where removing a contributing step\n     changes both the decision and the corresponding sentence.\n\n2. **Structured, multi-step reasoning via a logic memory**\n   - A dedicated logic memory $M$ stores prototype-like patterns that are **retrieved**\n     with attention and **integrated sequentially** by a transition operator $T$\n     yielding a trajectory rather than a one-shot match. Evidence accumulates across\n     steps through the weights $\\{\\pi_{t,j}\\}$. Empirically, ablations that drop $M$\n     or the transition stack degrade both task accuracy and explanation quality, and\n     step-by-step visualizations show progressively refined focus (e.g., from part to\n     whole, or from object to relation).\n\n3. **Faithfulness encouraged by monotonic confidence along the trajectory**\n   - The model encourages **evidence accumulation** by constraining predictive\n     confidence to be non-decreasing across steps:\n     $$\n     p(y\\mid r_t)\\;\\ge\\;p(y\\mid r_{t-1})\\quad\\text{for }t=2,\\ldots,T,\n     $$\n     implemented with a trajectory-level penalty\n     The paper reports that enabling this term improves step-wise calibration,\n     reduces explanation–prediction mismatches in counterfactual tests (masking or\n     permuting top-$k$ memory slots), and yields smoother confidence curves over $t$."}, "weaknesses": {"value": "1) **Prototype reliability is unproven**\n   - **What’s missing:** Evidence that prototypes are **stable** (across seeds/splits), **semantically coherent**, and **causally relevant**.\n   - **Implication:** “Logic memory” may capture spurious textures/backgrounds rather than human-meaningful concepts.\n   - **Minimal fixes:** (i) Seed/split stability matching with similarity/overlap metrics; (ii) coherence via top-k patch clustering + a small nameability study; (iii) causal ablations and counterfactual patch swaps reporting Δlogit/Δaccuracy.\n\n2) **No images or figures are included**\n   - **What’s missing:** Zero visualizations of attention maps, prototype exemplars, or step-wise trajectories.\n   - **Implication:** Readers cannot verify that explanations align with visual evidence.\n   - **Minimal fixes:** Add per-step heatmaps overlaid on inputs, top prototype exemplar grids, and before/after ablation panels (confidence + explanation changes), plus a compact appendix gallery (10–20 random cases)."}, "questions": {"value": "1. Could you run different seeds and the report the prototype overlapping? The prototypes should be similiar over different seed\n2. Could you show some cases visually about how your model explains? Human inspection will ensure the explaination is solid"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CO7H5pvzfm", "forum": "eMnRH5Yes3", "replyto": "eMnRH5Yes3", "signatures": ["ICLR.cc/2026/Conference/Submission8615/Reviewer_XFHG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8615/Reviewer_XFHG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953654758, "cdate": 1761953654758, "tmdate": 1762920451946, "mdate": 1762920451946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}