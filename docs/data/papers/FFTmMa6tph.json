{"id": "FFTmMa6tph", "number": 9362, "cdate": 1758120072804, "mdate": 1763739705812, "content": {"title": "Cross-subject decoding of human neural data for speech Brain Computer Interfaces", "abstract": "Brain-to-text systems have recently achieved impressive performance when trained on single-participant data,  but remain limited by uninvestigated cross-subject generalization. \nWe present the first neural-to-phoneme decoder trained jointly on the two largest intracortical speech datasets  (Willett et al. 2023; Card et al. 2024),  introducing day- and dataset-specific affine transforms to align neural activity into a shared space. \nA hierarchical GRU decoder with intermediate CTC supervision and feedback connections further mitigates  the conditional-independence assumption of standard CTC loss.  Our model matches or outperforms within-subject baselines while being trained across participants,  and adapts to unseen subjects using only a linear transform or brief fine-tuning. \nOn an independent inner-speech dataset (Kunz et al. 2025), our approach demonstrate generalization, by training only subject day specific transforms. These results highlight cross-subject pretraining as a practical path toward \nscalable and clinically deployable speech BCIs.", "tldr": "we demonstrate that cross-subject training of models for speech decoding is feasible and generalization to new subject as well.", "keywords": ["BCI", "neuroAI", "language decoding", "invasive neural recordings"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ec522d2236b80c136bfbe932b0cfdb896451342.pdf", "supplementary_material": "/attachment/ca834ea000c099b0c988f350ff2a664d3c302fcc.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents the first cross-subject neural-to-phoneme decoder for speech brain-computer interfaces, trained jointly on the two largest intracortical speech datasets (Willett et al. 2023 and Card et al. 2024). The proposed approach addresses the critical limitation of single-subject training paradigms that dominate current invasive BCI research, where each new user typically requires hours of supervised calibration data. The core methodology introduces day- and dataset-specific affine transformations to align neural activity from different participants and recording sessions into a shared latent space, based on the hypothesis that neural manifolds representing speech can be aligned through simple linear projections, similar to how different hand-drawn circles can be aligned via affine transforms. The decoder architecture employs a hierarchical GRU with three blocks: the first two blocks each contain two bidirectional GRU layers that produce intermediate phoneme predictions, which are projected back and added to hidden states to guide deeper layers, while the final block contains a single GRU layer producing final predictions. This hierarchical design with feedback connections aims to mitigate the conditional independence assumption of standard CTC loss, allowing the model to capture sequential dependencies between phonemes without the training instability of fully autoregressive approaches. The total loss combines CTC terms from all three layers with a weighting parameter λ=0.3 for auxiliary supervision.\n\nThe experimental evaluation demonstrates that cross-subject training is not only feasible but beneficial compared to single-subject baselines. On the Willett dataset, the joint model improves PER from 19.7% to 16.1% and WER from 17.4% to 14.5%, while on the Card dataset it achieves 9.1% PER and 6.67% WER, outperforming the single-subject baseline. The hierarchical CTC decoder provides consistent improvements over plain CTC across both datasets. To assess generalization beyond the training participants, the authors evaluate on the Kunz et al. (2025) inner speech dataset containing four participants (T12, T15, T16, T17), where T12 and T15 are the same individuals from Willett and Card datasets but recorded months later, and T16/T17 are entirely new subjects. Results show that training only the subject-specific linear transforms achieves 28-59% PER, while brief fine-tuning (5k steps) reduces this to 21-53% PER, demonstrating rapid adaptation with minimal calibration data. Analysis of the learned day-specific transforms through t-SNE visualization reveals that they successfully normalize session-to-session variability, and cross-day transform swapping experiments show reasonable performance on off-diagonal entries, suggesting the transforms capture generalizable mappings rather than overfitting to individual sessions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper makes a genuinely important contribution by demonstrating, for the first time, that cross-subject training for invasive neural speech decoding is not only feasible but actually beneficial compared to single-subject baselines. This is a significant departure from the prevailing paradigm in the field, where each participant is treated as an isolated case requiring hours of calibration. The core insight that neural manifolds can be aligned via simple day- and subject-specific affine transformations is both elegant and practically motivated, drawing an intuitive analogy to geometric alignment problems while being grounded in neuroscientific principles about conserved cortical organization for speech. The empirical validation is compelling: jointly training on Willett and Card datasets yields 16.1% PER on Willett (vs. 19.7% baseline) and 9.1% PER on Card (vs. 10.2% baseline), with particularly strong generalization demonstrated on the Kunz et al. inner speech dataset where lightweight adaptation (training only linear transforms or brief 5k-step fine-tuning) achieves reasonable performance on entirely new subjects T16 and T17, as well as on previously seen subjects T12 and T15 recorded months later under different task conditions. This cross-task and cross-time generalization provides strong evidence for the robustness of the learned representations. The hierarchical CTC architecture with feedback connections represents a well-motivated technical contribution that addresses a known limitation of standard CTC (conditional independence) without sacrificing training stability, achieving consistent improvements across datasets. The design is conceptually clean: intermediate phoneme predictions from shallower layers are fed back to inform deeper representations, partially recovering the conditional modeling power of autoregressive approaches while maintaining CTC's efficiency and robustness.\n\nThe paper excels in clarity and reproducibility, with comprehensive experimental documentation including dataset statistics, architectural specifications, training hyperparameters, and detailed task descriptions across all three datasets evaluated. The analysis of learned transforms is particularly insightful: t-SNE visualizations clearly demonstrate that day-specific projections reduce session clustering and expose task-relevant structure, while the transform swapping experiment (Figure 3B) elegantly quantifies the similarity between days and provides evidence that transforms capture generalizable mappings rather than arbitrary per-session adjustments. The qualitative examples in Figure 3C effectively illustrate system performance at different percentile levels (25th, 50th, 90th WER), showing that most errors are minor word substitutions rather than semantic failures, which provides valuable insight into failure modes. The paper honestly acknowledges important limitations and ethical considerations, including the restricted focus on speech-related tasks (only one motor intention dataset among six evaluations would be misleading since Kunz has four subjects but similar task), the simplified vocabulary in Kunz dataset that favors from-scratch training in some cases, and the critical privacy concerns around neural decoding technology. The discussion of intent-based activation mechanisms and secure mental passwords demonstrates responsible consideration of deployment implications, which is crucial as these technologies approach clinical viability."}, "weaknesses": {"value": "The paper's central claim about cross-subject generalization, while promising, is limited by several factors that constrain the strength of the conclusions. First, the cross-subject training is performed on only two participants (T12 from Willett, T15 from Card), which is a very small sample size for making broad claims about cross-subject generalization in neural decoding. While these are the largest available datasets, training on just two subjects makes it difficult to assess whether the observed benefits would scale to larger, more diverse participant populations with greater variability in electrode placement, cortical anatomy, and speech production strategies. The fact that both participants have similar implant configurations (256-channel Utah arrays in speech motor cortex) and similar task paradigms (attempted speech production) further limits the diversity of the training data. Second, the generalization evaluation on Kunz et al. dataset reveals mixed results that somewhat undermine the practical utility claims. For the two new subjects (T16, T17), even after fine-tuning the entire model for 5k steps, PERs remain quite high at 26.1% and 53.3% respectively, with T17 performing substantially worse than a model trained from scratch (53.3% vs 30.6% PER). This suggests that for some subjects, the pretrained model may not provide a better initialization than random weights, contradicting the premise that cross-subject pretraining should universally reduce calibration requirements. The authors attribute this to the simplified vocabulary in Kunz dataset (seven repeated words plus some sentences), but this explanation is somewhat unsatisfying, as real-world BCI deployment would need to handle both limited and extensive vocabularies. Third, the paper does not adequately explore the data efficiency claims that motivate cross-subject training. While the authors state that adaptation requires \"minimal calibration data,\" there is no systematic analysis of how performance scales with the amount of fine-tuning data. How many trials are needed to achieve 90%, 95%, or 99% of the fully fine-tuned performance? What is the actual reduction in calibration time compared to training from scratch? These questions are critical for clinical translation but remain unanswered.\n\nThe methodological choices and architectural contributions require more rigorous justification and analysis. The hierarchical CTC decoder with feedback connections is presented as addressing the conditional independence limitation of standard CTC, but the empirical improvements are relatively modest (16.1% vs 17.6% PER on Willett, 9.1% vs 9.6% on Card). While these improvements are consistent, they are not dramatic enough to definitively establish the superiority of the hierarchical approach, especially given that no ablation study is provided to isolate the contribution of the feedback mechanism versus simply having more layers or parameters. The choice of weighting parameter λ=0.3 for auxiliary losses is stated to be \"empirical\" with no exploration of sensitivity to this hyperparameter, and it is unclear whether this value would generalize to other datasets or architectures. The authors acknowledge that \"further hyperparameter exploration could boost the performance and was left as future work,\" which raises concerns about whether the reported results represent the best possible performance of the proposed method or simply one configuration. Additionally, the comparison between hierarchical CTC and other sequence modeling approaches is limited. The paper mentions that autoregressive transformers \"still lag behind CTC in this domain\" and suffer from training instability, but provides no empirical evidence or ablation to support this claim in their specific setup. Recent advances in sequence-to-sequence models, attention-based architectures, and hybrid CTC/attention systems are not explored, making it difficult to assess whether the hierarchical GRU represents a genuine architectural advance or simply a safe, conservative choice. The day- and subject-specific affine transformations, while conceptually appealing, are not rigorously analyzed in terms of their learned structure. The paper shows that transforms reduce day clustering in t-SNE space and enable reasonable cross-day performance, but does not investigate what these transforms actually learn: do they primarily perform scaling/normalization, or do they capture more complex rotations and shears? Are certain dimensions or channel groups transformed more than others, and does this correlate with anatomical or functional properties? Understanding the structure of these transformations could provide neuroscientific insights and guide future improvements, but this analysis is absent. Finally, the reliance on WFST-based phoneme-to-word decoding with n-gram language models represents a significant limitation that the authors acknowledge but do not address. This classical approach is \"computationally expensive, memory-intensive, and inherently limited to a fixed context window,\" and the authors note that \"most residual errors are attributable to phoneme-to-word reconstruction rather than neural-to-phoneme decoding.\" This suggests that the neural decoder may already be performing near its ceiling, and substantial further gains would require improving the language modeling component rather than the neural encoder. However, the paper does not explore modern alternatives such as end-to-end neural language models, transformer-based rescoring, or direct neural speech-to-text decoding, leaving a major component of the system unaddressed."}, "questions": {"value": "Please see your weaknesses and I will adjust the final score based on your answers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FkiGBNexWk", "forum": "FFTmMa6tph", "replyto": "FFTmMa6tph", "signatures": ["ICLR.cc/2026/Conference/Submission9362/Reviewer_1Tsj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9362/Reviewer_1Tsj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555883431, "cdate": 1761555883431, "tmdate": 1762920982404, "mdate": 1762920982404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a cross-subject neural-to-phoneme decoder for intracortical brain-to-text BCIs, combining the two largest speech datasets with day-/dataset-specific transforms for neural alignment and a hierarchical GRU with intermediate CTC supervision.  Evaluations are performed to show effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* (a) This paper is clearly written and easy to follow."}, "weaknesses": {"value": "**(a) Limited novelty**  \nThe proposed method builds directly on the GRU-based architecture introduced by Willett et al. The “Hierarchical GRU decoder with feedback” is essentially a standard GRU model with CTC loss applied at intermediate layers — a practice that is well-established in the machine learning community and widely used for sequence modeling tasks. Similarly, the “Day- and Subject-Specific Transformation” amounts to updating part of the trained model with new data, which is also a common and well-known technique in transfer learning. As such, the work primarily represents an engineering integration of existing approaches rather than introducing new insights from either the neuroscience or the machine learning perspective.\n\n**(b) Incremental improvement**  \nThe reported performance in Tables 1 and 2 is only marginally better than the baseline models used in prior work, and these gains appear modest relative to the complexity added by the proposed approach. Furthermore, the proposed transfer strategy performs notably worse than stronger baselines such as fine-tuning the entire model or training from scratch. This raises questions about the practical advantage of the method over simpler existing adaptation techniques."}, "questions": {"value": "* (a) For the cross-subject and day experiments, how much of the data is used for model calibration? Is all the available training data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7MCAeD8Ta4", "forum": "FFTmMa6tph", "replyto": "FFTmMa6tph", "signatures": ["ICLR.cc/2026/Conference/Submission9362/Reviewer_Muqi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9362/Reviewer_Muqi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567564216, "cdate": 1761567564216, "tmdate": 1762920982080, "mdate": 1762920982080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose jointly training on intracortical speech datasets to improve downstream generalisation to new datasets/subjects. They also attempt to improve over the standard CTC independence assumption by combining CTC losses from intermediate layers in the GRU model. The results show that the model may improve with joint dataset training, though statistical significance is not provided, and generalisation to an independent inner-speech dataset (Kunz) is highly effective with only a linear transform, outperforming even end-to-end training with the dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Table 2: Strong new-subject generalisation results (especially compared to training from scratch!).\n- Ablations show hierarchical CTC is likely effective in improving phoneme decoding"}, "weaknesses": {"value": "- Results with joint training (Table 1) look good or marginally better; but it’s hard to determine without error bars or any indication of statistical significance.\n- No comparisons to any of the methods in the 2024 Willett competition [A]. Is there a reason the authors did not choose to compare to any of these approaches? Can any of them be combined with the hierarchical CTC approach? Do any of them generalise to new datasets as well as your method?\n\n[A] Willett, F.R., Li, J., Le, T., Fan, C., Chen, M., Shlizerman, E., Chen, Y., Zheng, X., Okubo, T.S., Benster, T. and Lee, H.D., 2024. Brain-to-Text Benchmark'24: Lessons Learned. arXiv preprint arXiv:2412.17227."}, "questions": {"value": "- Table 2: Why do the authors think that fine-tuning the whole model for Kunz performs worse than training only a linear transform on the pre-trained model? Is this a result of limited trials in Kunz?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1eDVwhfm3Z", "forum": "FFTmMa6tph", "replyto": "FFTmMa6tph", "signatures": ["ICLR.cc/2026/Conference/Submission9362/Reviewer_L3Wt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9362/Reviewer_L3Wt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912246090, "cdate": 1761912246090, "tmdate": 1762920981672, "mdate": 1762920981672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers and the area chair for their careful reading of our submission and for the constructive feedback that helped us significantly improve the manuscript. We have integrated new analyses, clarified methodological decisions, and expanded the discussion to better address their concerns. The revised version now includes systematic sensitivity studies, a detailed structural analysis of learned affine transforms, and an improved treatment of data-efficiency and cross-dataset generalization (Appendix D).\n\nOur work is fundamentally motivated by a scientific question rather than an architectural one: is cross-subject generalization possible for intracortical speech BCIs, and if so, to what extent? The evidence provided by our experiments suggests that the answer is yes. Neural population dynamics supporting speech appear to share a stable representational backbone across individuals, and a simple affine layer is often sufficient to account for subject-specific variability. This observation is consistent with findings in other neural recording modalities—including fMRI and MEG—where cross-subject decoding becomes feasible when combined with lightweight subject-specific alignment layers or functional alignment approaches (Banville et al., Ferrante et al., Tang et al.). As in those domains, improvements over the best per-subject baselines are typically modest; this reflects a structural limitation of the field, where datasets contain thousands of trials per subject but only a handful of subjects. We expect this to evolve as larger, more heterogeneous datasets become available, enabling richer cross-participant modeling.\n\nWe also believe that ICLR is a particularly suitable venue for discussing the representational aspects of this work. While some papers aim to present the culmination of a research journey, many influential contributions in representation learning mark the beginning of new directions. Our work is intended as such a starting point: a first step toward understanding the geometry, invariances, and limits of shared neural manifolds underlying human speech production. We hope that with this contribution we will have the opportunity stimulate  and foster discussion in the community regarding how neural representations vary across individuals, how they can be aligned, and how future BCIs might leverage these shared structures to reduce calibration demands for new users.\n\nWe are grateful to the reviewers for their insightful suggestions, which have meaningfully strengthened the manuscript. We look forward to continuing this scientific dialogue and to further refining our understanding of cross-subject neural representations for speech BCIs."}}, "id": "LCbnEoWzJn", "forum": "FFTmMa6tph", "replyto": "FFTmMa6tph", "signatures": ["ICLR.cc/2026/Conference/Submission9362/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9362/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission9362/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763741221777, "cdate": 1763741221777, "tmdate": 1763741221777, "mdate": 1763741221777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}