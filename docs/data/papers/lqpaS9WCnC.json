{"id": "lqpaS9WCnC", "number": 14461, "cdate": 1758236181592, "mdate": 1763489998017, "content": {"title": "System Prompt Extraction Attacks and Defenses in Large Language Models", "abstract": "The system prompt in Large Language Models (LLMs) plays a pivotal role in guiding model behavior and response generation. Often containing private configuration details, user roles, and operational instructions, the system prompt has become an emerging attack target. Recent studies have shown that LLM system prompts are highly susceptible to extraction attacks through meticulously designed queries, raising significant privacy and security concerns. Despite the growing threat, there is a lack of systematic studies of system prompt extraction attacks and defenses. In this paper, we present a comprehensive framework, SPE-LLM, to systematically evaluate System Prompt Extraction attacks and defenses in LLMs, where we propose several adversarial query design techniques, defense mechanisms, and compare them with the state-of-the-art (SOTA) baselines. First, we design a set of novel adversarial queries that effectively extract system prompts from the SOTA LLMs, demonstrating the severe risks of LLM system prompt extraction. Second, we propose several defense techniques to mitigate the attacks, providing practical solutions for secure LLM deployments. Third, we used a diverse set of evaluation metrics to accurately quantify the severity of system prompt extraction attacks in LLMs and conduct comprehensive experiments across multiple benchmark datasets, which validate the efficacy of our proposed SPE-LLM framework.", "tldr": "", "keywords": ["System Prompt extraaction", "large language models", "evaluation framework"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/22a95b4e607c20c6caeb291c811873f425849199.pdf", "supplementary_material": "/attachment/0cbd99ffcbf51a0f59794160a54d67da129277af.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles the safety concern of system prompt extraction attacks in LLMs. The authors propose SPE-LLM, a framework for evaluating such attacks and corresponding defenses. The framework uses datasets of synthetic or publicly collected system prompts as controlled ground truth for evaluation. Novel adversarial queries are designed to extract system prompts from SOTA models accurately, and multiple defense strategies are proposed, with evaluation conducted across benchmark datasets using diverse metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an important safety concern, where attackers may extract and exploit LLM system prompts.\n\n\n2. The paper proposes improved adversarial query techniques (CoT, few-shot, extended sandwich) that extract system prompts more precisely than previous methods."}, "weaknesses": {"value": "1. The formatting of the paper could be improved for better readability. The texts in Figure 3, Table 2 and 3, etc. are too small to read.\n\n2. The writing of the introduction lacks clarity. It might be improved by including a concrete example on why system prompts extraction is a critical safety issue, in addition to citing the possible consequences.\n\n3. It is not clear how the synthetic prompts represent/resemble real, even proprietary system prompts from both open- and closed-source LLMs. While these prompts follow previous works, justifications on this point may improve the significance of this work.\n\n4. In addition, synthetic datasets may not reflect the diversity or complexity of real system prompts in commercial LLMs, limiting external validity."}, "questions": {"value": "1. The paper mentions that the proposed defenses are limited. Could the authors elaborate on how adversarial instruction fine-tuning could strengthen defenses against system prompt extraction attacks?\n\n2. How would fine-tuning attacks themselves impact system prompt security, and do they pose additional risks or limitations to the proposed defense strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Br8GNT5Ic2", "forum": "lqpaS9WCnC", "replyto": "lqpaS9WCnC", "signatures": ["ICLR.cc/2026/Conference/Submission14461/Reviewer_E1LY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14461/Reviewer_E1LY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730365274, "cdate": 1761730365274, "tmdate": 1762924863428, "mdate": 1762924863428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "JVHA53jOEn", "forum": "lqpaS9WCnC", "replyto": "lqpaS9WCnC", "signatures": ["ICLR.cc/2026/Conference/Submission14461/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14461/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763489997083, "cdate": 1763489997083, "tmdate": 1763489997083, "mdate": 1763489997083, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the system prompt extraction attacks and defenses in LLMs. System prompts often contain sensitive information and are vulnerable to extraction attacks. This paper considers a black-box treat model and use prompt engineering methods (CoT, Few-shot-prompting, and extended sandwich attack) to perform SPE attacks and defense. Experiments on multiple models are presented to validate the main claims."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. (Originality) This paper propose a novel framework of SPE, which is different from most jailbreaking attacks.\n2. (Clarity) The presentation of this paper is straightforward. The methods are easy to understand."}, "weaknesses": {"value": "1. The significance of SPE attacks is not widely understood (compared to other jailbreaking attacks), nor is it discussed in detail in this paper. While the authors provide a list of references (cf. Lines 41-44) to support their arguments, the inclusion of specific, detailed examples would improve the illustrative power of this paper.\n2. There is a lack of evaluation regarding the helpfulness and efficiency of the models after the defense mechanism has been integrated."}, "questions": {"value": "1. How to evaluate the SPE attacks against closed sourced models? I cannot understand the rationale behind Lines 342-344.\n2. Does the defense mechanism proposed in this paper exhibit any negative side effects? It is commonly believed that there is a tradeoff between the helpfulness and the robustness of LLMs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wHtKhlCc15", "forum": "lqpaS9WCnC", "replyto": "lqpaS9WCnC", "signatures": ["ICLR.cc/2026/Conference/Submission14461/Reviewer_Z27w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14461/Reviewer_Z27w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967975365, "cdate": 1761967975365, "tmdate": 1762924863116, "mdate": 1762924863116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores how a general-purpose assistant (Claude) can serve as a practical environment for alignment research. It proposes a new benchmark suite (Must-Do, Mustn’t-Do, and Should-Do) grounded in real assistant behaviors, and introduces evaluation protocols using sparse, high-quality human annotations. The authors aim to bridge academic alignment objectives with real-world assistant behavior and advocate for broad collaborative efforts in alignment research."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Here are the strengths of the paper:\n\nOriginality: Introduces a practical, behavior-grounded framework for alignment that reflects actual deployment use cases—distinct from abstract, synthetic benchmarks.\n\nClarity and Scope: Clearly articulates alignment categories (Must-Do, Mustn’t-Do, Should-Do) with illustrative examples and user-focused evaluation criteria.\n\nSignificance: Offers a valuable bridge between alignment theory and production-scale systems, encouraging open research collaboration grounded in realistic settings."}, "weaknesses": {"value": "Here are the weakness of the paper:\n\nLack of rigorous experimentation: The paper emphasizes framework design and philosophical positioning, but offers limited empirical results or baselines for benchmarking.\n\nSparse evaluation detail: The use of “sparse but high-quality” annotations is advocated, but without detailed methodology for ensuring inter-rater reliability or statistical robustness."}, "questions": {"value": "How do you ensure the consistency and reliability of the sparse human annotations used in your evaluations, particularly across nuanced alignment categories like “Should-Do”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tsIqdCAjKf", "forum": "lqpaS9WCnC", "replyto": "lqpaS9WCnC", "signatures": ["ICLR.cc/2026/Conference/Submission14461/Reviewer_Y3nK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14461/Reviewer_Y3nK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017757319, "cdate": 1762017757319, "tmdate": 1762924862720, "mdate": 1762924862720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a framework called SPE-LLM that investigates system prompt extraction attacks and corresponding defenses. Specifically, it (1) designs new adversarial query strategies for extracting hidden system prompts, (2) proposes several defense techniques including instruction defense, sandwich defense, and system-prompt filtering, and (3) introduces evaluation metrics and conducts extensive experiments across multiple datasets and LLMs (Llama-3, Falcon-3, Gemma-2, GPT-4, GPT-4.1). The results show that the proposed defenses, particularly system-prompt filtering, can significantly reduce attack success rates."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper addresses the problem of system prompt extraction in large language models and investigates several defenses against such attacks. The topic is practically important, as system prompts often contain sensitive or proprietary instructions, and understanding their vulnerability is relevant for LLM deployment security."}, "weaknesses": {"value": "While the topic of system-prompt extraction has high practical importance, the paper’s attacks and defenses largely replicate prior findings from 2023–2024 jailbreak and prompt-leakage literature. The experiments are thorough but incremental, offering quantitative confirmation rather than conceptual novelty."}, "questions": {"value": "How does SPE-LLM differ fundamentally from prior prompt-injection or jailbreak frameworks beyond scale and dataset variety?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WaCF0YObJx", "forum": "lqpaS9WCnC", "replyto": "lqpaS9WCnC", "signatures": ["ICLR.cc/2026/Conference/Submission14461/Reviewer_5a2F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14461/Reviewer_5a2F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762329865379, "cdate": 1762329865379, "tmdate": 1762924862248, "mdate": 1762924862248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}