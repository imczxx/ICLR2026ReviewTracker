{"id": "dgvx86qybJ", "number": 17210, "cdate": 1758273455677, "mdate": 1759897190661, "content": {"title": "Hierarchical Representations for Cross-task Automated Heuristic Design using LLMs", "abstract": "Designing heuristic algorithms for complex optimization problems is a time-consuming and expert-driven process. Recently, Automated Heuristic Design (AHD) using Large Language Models (LLMs) has shown significant promise for automating algorithm development. However, existing works mainly rely on programs to represent heuristics, which are inherently task-specific and fail to generalize as effectively as established metaheuristics like tabu search or guided local search. To bridge this gap, we introduce Multi-Task Hierarchical Search (MTHS), an LLM-guided evolutionary method that co-designs general-purpose metaheuristics and task-specific programs. MTHS employs a hierarchical representation and adopts a two-level evolution framework to evolve task-agnostic metaheuristics and task-specific program implementations simultaneously across multiple heuristic design tasks. During this evolution, a knowledge transfer mechanism allows learning from elite programs designed for other tasks. We evaluated MTHS on distinct combinatorial optimization problems, where it outperforms both commonly-used heuristics and existing LLM-driven AHD approaches. Our results demonstrate that the hierarchical representations facilitate effective multi-task AHD, and the evolved metaheuristics exhibit strong generalization to related tasks.", "tldr": "", "keywords": ["Automated Heuristic Design", "Large Language Model", "Metaheuristic", "Cross-task Learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a04246ac36ee67fd0130d8f26c404239cdb968e8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the limited cross-task generalization of current Large Language Model (LLM)-based Automated Heuristic Design (AHD) systems, which typically produce task-specific heuristics that cannot transfer across problem domains. The authors propose Multi-Task Hierarchical Search (MTHS), an LLM-guided hierarchical evolutionary framework that co-designs general-purpose metaheuristics and their task-specific program instantiations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Research an important problem.\n2. Appendices B–C provide explicit prompts, templates, and problem settings, enabling implementation replication.\n3. Strong generalization across tasks (Sec. 3.5; Fig. 3), showing the same metaheuristic helps different LLMs generate high-quality solvers—empirical evidence of cross-task knowledge transfer."}, "weaknesses": {"value": "1. This paper investigates a critical issue. While existing LLM-based automated heuristic designs are highly effective, most methods design a single heuristic tailored to specific problem instances, often resulting in poor generalization across different distributions or settings. It should be clarified that this paper is not the first to propose a solution to this problem; the recent work EoH-S [1] should also be referenced for discussion and used as a baseline for comparison.\n2. The proposed method still requires re-search when encountering new problem instances, albeit incorporating MH to enhance search efficiency. It does not fundamentally resolve the aforementioned issue.\n3. The paper gives no formal analysis of why hierarchical separation or Pareto-based selection guarantees better generalization.\n4. The intuition (“mirrors expert practice”) is plausible but remains qualitative.\n\n[1] Liu F, Liu Y, Zhang Q, et al. Eoh-s: Evolution of heuristic set using llms for automated heuristic design[J]. arXiv preprint arXiv:2508.03082, 2025."}, "questions": {"value": "Reference Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sc0jgns1ot", "forum": "dgvx86qybJ", "replyto": "dgvx86qybJ", "signatures": ["ICLR.cc/2026/Conference/Submission17210/Reviewer_vop9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17210/Reviewer_vop9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760965611046, "cdate": 1760965611046, "tmdate": 1762927177230, "mdate": 1762927177230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes MTHS, an LLM-guided evolutionary method that co-designs general-purpose metaheuristics and task-specific programs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The experiments demonstrate the effectiveness of the proposed MTHS."}, "weaknesses": {"value": "**W1:**  This manuscript should evaluate MTHS using additional LLMs.\n\n**W2:**  The advantages of MTHS would be clearer if the authors included an experimental comparison with the most related study [1].\n\n**W3:**  This manuscript should conduct comprehensive ablation studies to validate the effectiveness of all proposed components.\n\n[1] Generalizable heuristic generation through large language models with meta-optimization, arXiv:2505.20881."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UjG4j10gQN", "forum": "dgvx86qybJ", "replyto": "dgvx86qybJ", "signatures": ["ICLR.cc/2026/Conference/Submission17210/Reviewer_p6UP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17210/Reviewer_p6UP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761041043437, "cdate": 1761041043437, "tmdate": 1762927176975, "mdate": 1762927176975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the limited generalization of existing Large Language Model (LLM)-driven Automated Heuristic Design (AHD) methods, which typically yield task-specific heuristics. The authors propose **Multi-Task Hierarchical Search (MTHS)**, a hierarchical evolutionary framework that separates *task-agnostic metaheuristics* from *task-specific programs*. Guided by LLMs, MTHS performs two-level evolution — evolving general metaheuristics at the high level and optimizing task-specific implementations at the low level — while transferring knowledge across tasks. Experiments on four combinatorial optimization problems (TSP, CVRP, FSSP, and BPP) demonstrate that MTHS outperforms both traditional heuristics and existing LLM-based AHD approaches (e.g., EoH, ReEvo, MCTS-AHD), with improved cross-task generalization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a hierarchical representation that mirrors human expert reasoning — decoupling general algorithmic logic from task-specific components. The proposed method is well-motivated.\n2. The experiments are generally comprehensive and with strong baselines."}, "weaknesses": {"value": "1. No code.\n2. The distinction drawn between “program-level” and “thought-level” AHD approaches seems somewhat artificial. The proposed method still relies on multi-task prompts and LLM-generated programs, similar in spirit to the supposedly “high-level” methods (e.g., EoH, ReEvo).\n3. It is unclear whether all compared LLM-based methods use the same base LLM (the paper mentions GPT-5-mini for MTHS, but others might differ). Furthermore, MTHS requires extra steps (multi-task inputs, hierarchical evolution), which may inflate computational cost relative to single-task methods. This raises fairness concerns in direct performance comparisons. And no cost comparison for other AHD methods.\n4. Although the paper claims the total cost of multi-task evolution is lower than multiple independent runs, the framework’s scalability beyond a few tasks (e.g., >10) is questionable. The high proportion of LLM calls at the low-level stage (≈52%) could make large-scale extensions impractical.\n5. The knowledge transfer component is only qualitatively discussed. The paper lacks quantitative analysis on when and how transfer helps (e.g., correlation between task similarity and performance gain). \n6. The appendix suggests MTHS takes longer per run than baseline AHD methods. Since heuristic performance often depends on runtime, this omission weakens claims of superiority."}, "questions": {"value": "1. How sensitive is the framework to weaker LLMs (e.g., GPT-3.5). \n2. Consider adding experiments on tasks beyond discrete combinatorial optimization to validate cross-domain generalization claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fAbDjJMdcU", "forum": "dgvx86qybJ", "replyto": "dgvx86qybJ", "signatures": ["ICLR.cc/2026/Conference/Submission17210/Reviewer_dyxi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17210/Reviewer_dyxi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616820114, "cdate": 1761616820114, "tmdate": 1762927176682, "mdate": 1762927176682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of automating heuristic design for combinatorial optimization (CO) problems. It focuses on improving how LLM-driven Automated Heuristic Design (AHD) methods generalize across different tasks. The authors introduce Multi-Task Hierarchical Search (MTHS), an evolutionary framework with a novel hierarchical structure. It combines a task-agnostic metaheuristic at the high level with task-specific program instantiations at the low level, both guided by LLMs. The framework uses multi-task knowledge transfer and a two-level evolution strategy. Experiments across four classical CO problems—TSP, CVRP, FSSP, and BPP—show that MTHS outperforms both conventional heuristics and recent LLM-based AHD baselines. It also produces metaheuristics that transfer effectively to new tasks and LLM backbones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a clear hierarchical framework that separates general metaheuristic logic from task-specific implementations. This approach mirrors expert practice, addresses the core limitation of task specificity in earlier LLM-based AHD works, and enables explicit knowledge transfer.\n- The method is evaluated on established CO benchmarks against traditional heuristics, metaheuristics, and LLM-driven baselines. Results show state-of-the-art or competitive performance across all tasks.\n- The framework allows learned metaheuristics to transfer to new, unseen tasks and guide multiple LLMs, demonstrating improved robustness and real-world applicability."}, "weaknesses": {"value": "- Stronger ablation/benchmarking on multiple foundation models (the main results), or an in-depth discussion of LLM choice impact, is missing.\n- The paper notes the dominance of LLM API usage in total cost, but lacks a direct, quantitative comparison of token and time usage with baseline LLM-driven AHD methods.\n- There is limited exploration of how the selection or diversity of tasks used during multi-task evolution influences the generalization ability of the evolved metaheuristics. For instance, it is not clear whether including more (or more diverse) problems further enhances transfer, or if performance saturates after a certain level of relatedness between tasks."}, "questions": {"value": "1. How sensitive is the method to the choice and quality of the underlying LLM? For instance, how does performance scale with smaller, less capable, or open-source models?\n2. How does MTHS handle structurally dissimilar tasks (e.g., routing, scheduling, bin packing)? At what point does multi-task learning harm generalization through negative transfer?\n3. Did the authors assess the diversity of generated metaheuristics? Does the approach produce redundant solutions or consistently find novel strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "58LSop7UQu", "forum": "dgvx86qybJ", "replyto": "dgvx86qybJ", "signatures": ["ICLR.cc/2026/Conference/Submission17210/Reviewer_wa6u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17210/Reviewer_wa6u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824834721, "cdate": 1761824834721, "tmdate": 1762927176291, "mdate": 1762927176291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper looks at the issue of cross-task generalization in current LLM-driven automated heuristic design systems. The paper presents Multi-Task Hierarchical Search (MTHS).  This is a hierarchical, two-level evolutionary framework where the high level evolves task-agnostic metaheuristics (general problem-solving strategies) and the low level evolves task-specific program implementations tailored to individual optimization tasks. Another key aspect is that there is a knowledge transfer module allows high-performing components discovered in one task to inform others.  They do experiments involving a number of problems and comparison strategies, and show MHTS performs better."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The hierarchical framework is interesting.  (I don't believe this approach is novel in the large field of metaheuristics, but it is interesting to apply here.)\n\nThe appendices provide significant detail, including on the prompting procedures, with the goal of ensuring reproducibility and providing more clear details to the reader as needed.  \n\nThe experiments suggest strong practical performance.  \n\nThe study comparing different metaheuristic representations.  \n\nThere is open-sourced versions of the algorithms, data, etc. for reproducibility."}, "weaknesses": {"value": "Most of the experiments use very small problem instances and old public datasets (e.g., TSPLib, CVRPLib, Taillard benchmarks). Some of these are decades old and much smaller than the scales modern heuristics are expected to handle. This makes it hard to see if the method would work on real, large, or more recent problems.\n\nI am not up on all the latest in solvers, but it seems to me their comparison points are older, general solution methods.  Perhaps it is reasonable to compare against other general metaheuristic methods, but I do not believe they would be competitive with strong heuristics (even if problem-tailored).  \n\nThere’s little explanation or understanding of why the system works or why it would  fails. For example, there’s no study of how the “knowledge transfer” actually helps, or what happens if the tasks not sufficiently closely related.\n\nThe experiment suggests this approach is expensive and slow to run. The paper doesn’t show whether this approach could scale to larger or more diverse problems, or whether it’s practical for anyone to use for more \"real-life\" problems.  \n\nOverall, it's not clear how general this approach would be."}, "questions": {"value": "I would like to see tests on larger and more modern datasets, and comparisons with current best solvers (even if problem-specific). Is there any type of analysis or insight about how knowledge transfer works here, or other aspects of the system?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Uq97qhy8Ji", "forum": "dgvx86qybJ", "replyto": "dgvx86qybJ", "signatures": ["ICLR.cc/2026/Conference/Submission17210/Reviewer_MwQv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17210/Reviewer_MwQv"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762567694176, "cdate": 1762567694176, "tmdate": 1762927176002, "mdate": 1762927176002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}