{"id": "R0viKzJ70t", "number": 23890, "cdate": 1758349890992, "mdate": 1763504923065, "content": {"title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use", "abstract": "Computer-use agents (CUAs) hold promise for automating everyday digital tasks, but their unreliability and high variance hinder their application to long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method that scales over agents by generating multiple rollouts and selecting among them using behavior narratives that describe the agents' rollout. It enables both wide exploration and principled trajectory selection, substantially improving robustness and success rates. On OSWorld, our bBoN scaling method establishes a new state of the art (SoTA) at 69.9\\%, significantly outperforming prior methods and approaching 72\\% human-level performance, with comprehensive ablations validating key design choices. \nWe further demonstrate strong generalization results to different operating systems on WindowsAgentArena and AndroidWorld.\nCrucially, our results highlight the unreasonable effectiveness of scaling CUAs, when you do it right: effective scaling requires structured trajectory understanding and selection, and bBoN provides a practical framework to achieve this.", "tldr": "", "keywords": ["Computer Use", "GUI Agents", "Multimodal Large Language Models", "Planning", "Grounding", "Vision"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6239eaa2677cadaf2b1f7828984996db0747a2db.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces bBoN, a “wide‑scaling” paradigm for computer‑use agents that generates multiple trajectories in parallel and selects the best via narrative comparison, a concept well‑framed and clearly presented in the abstract. However, it provides little discussion of the substantial computational cost such scaling entails."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a central issue in computer‑use agents—how to scale behavioral selection effectively—which is timely and relevant as larger models are increasingly limited by interaction efficiency.\n\nThe proposed bBoN framework is clearly formulated, with intuitive motivation and a well‑explained pipeline from trajectory generation to narrative‑based comparison.\n\nThe experiments, especially the comparison in Figure 5, provide concrete evidence that bBoN outperforms both average‑rollout and simple WebJudge baselines, illustrating the benefit of cross‑trajectory evaluation."}, "weaknesses": {"value": "Although bBoN achieves better performance through multiple rollouts and narrative evaluations, this improvement comes with a potentially huge computational overhead. The paper does not quantitatively analyze cost, efficiency, or fairness under equal‑budget conditions, leaving readers uncertain about the real‑world practicality of the method.\n\nThe main contribution—running many rollouts in parallel and selecting via pairwise narrative evaluation—follows a relatively straightforward ensemble logic: performance generally improves when sampling more trajectories and filtering them with a learned judge. While well‑executed, this approach feels incremental rather than conceptually innovative, relying on existing ideas of scaling and comparative evaluation.\n\nThe method relies on aggregating multiple rollouts into a single narrative summary for comparison. As the number of rollouts or the length of each trajectory increases, both token‑level and computational costs grow rapidly. Moreover, very long context windows may dilute key information and amplify model attention errors, leading to less reliable judgments. The paper does not analyze how accuracy or reasoning quality scales with context length, which raises concerns about robustness in large‑scale or long‑horizon tasks."}, "questions": {"value": "How does the framework handle potential side effects or environment resets when multiple rollouts are executed repeatedly? Do many resets cost more?\n\nDoes the narrative summarization stage require multimodal input (e.g., multiple images per rollout)? If so, how is this handled given that some current LLM/VLM APIs only support single‑image inputs?\n\nWhat is the additional inference or token cost introduced by the narrative‑comparison stage?\n\nIn Section 3.2, the paper describes the Behavior Best‑of‑N Judge as performing a multiple‑choice (MCQ) comparative evaluation over N candidate trajectories. However, the corresponding system prompt (Appendix) only supports pair‑wise comparison (“which one better completes the user request”), without any structure for handling N > 2 options. This discrepancy suggests that the actual implementation is Best‑of‑2 rather than the claimed Best‑of‑N？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1vETmiBIQa", "forum": "R0viKzJ70t", "replyto": "R0viKzJ70t", "signatures": ["ICLR.cc/2026/Conference/Submission23890/Reviewer_77YS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23890/Reviewer_77YS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761033188770, "cdate": 1761033188770, "tmdate": 1762942842434, "mdate": 1762942842434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a test-time scaling framework for computer-use agents called Behavior Best-of-N (bBoN): it runs multiple full task trajectories in parallel and then selects the best one at the end, greatly improving reliability on long, brittle workflows. To make different candidates directly comparable, it compresses each step into a concise Behavior Narrative that records the action and resulting screen change, and uses a multi-way bBoN judge to pick the strongest trajectory rather than scoring runs in isolation. The authors also streamline the underlying agent (Agent S3) by removing heavy hierarchical planning and integrating a coding agent, so it can generate stronger candidates faster. Together, these components yield higher success rates and better cross-platform generalization (e.g., to Windows and Android) under practical step budgets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Running multiple full trajectories in parallel and selecting the best (bBoN) reduces single-run brittleness in long-horizon tasks and provides predictable gains as the number of candidates increases.\n- Behavior Narratives distill each step into factual “action → screen change” summaries, enabling a multiway judge to compare candidates directly, more reliable and scalable than independent scoring or pairwise elimination.\n- The streamlined base agent (Agent S3) produces stronger rollouts with lower overhead, and the full stack shows consistent success-rate improvements and positive cross-platform transfer (e.g., Windows/Android) under realistic step budgets."}, "weaknesses": {"value": "- Missing implementation details for parallel rollouts：The paper claims Best-of-N by running multiple full trajectories concurrently but omits a reproducible execution recipe: no specification of multi-machine vs. single-host isolation, environment cloning/reset procedures (images/snapshots/templates), seeding and cross-run isolation (caches, network, account state), or resource-parity policies (CPU/GPU/IO). This gap undermines reproducibility and comparability.\n- Best-of-N increases cost roughly with N, yet the paper does not report wall-clock time, CPU/GPU hours, energy, or cost-normalized success. There is no equal-compute baseline comparison (e.g., single/multi-sample methods under the same total budget), so observed gains may primarily reflect more sampling rather than a more efficient method.\n- Gains may hinge on heterogeneous candidate pools (different backbones, prompts, temperatures). Without isolating diversity vs. pure sampling, it’s unclear whether bBoN helps when all candidates come from the same model/config.\n- The method is demonstrated on tasks with clear success signals. It remains unclear how the judge handles open-ended goals, partial credit, or ambiguous outcomes where “best” is not binary.\n- The work does not characterize marginal gains as N grows or provide guidance to select N under a fixed budget, making it hard to tune for real-time constraints."}, "questions": {"value": "- When generating N parallel trajectories, what infrastructure do you use (multi-machine vs. single host with multiple VMs/containers), how are environments cloned/reset, and how are seeds and resource quotas set to ensure isolation and fairness across candidates? Could you release scripts/configurations to reproduce the same parallel conditions?\n- Under equal total compute budgets, how do bBoN results compare to single-run or few-sample baselines, and what wall-clock/CPU-GPU hours and energy are reported?\n- Do gains depend on heterogeneous candidate pools (different models/prompts/temperatures), or do they persist when all candidates come from the same model and config?\n- Are there early-exit policies to stop clearly bad candidates mid-run, or adaptive N strategies by task difficulty to control latency and cost at test time?\n- How does bBoN perform under UI/layout drift, app updates, ads/pop-ups, and network jitter, particularly when initial snapshots cannot perfectly normalize the environment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WwESbR8AWr", "forum": "R0viKzJ70t", "replyto": "R0viKzJ70t", "signatures": ["ICLR.cc/2026/Conference/Submission23890/Reviewer_q8SD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23890/Reviewer_q8SD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727182835, "cdate": 1761727182835, "tmdate": 1762942842165, "mdate": 1762942842165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a method for improving the performance of computer-use agents by generating multiple rollouts (wide scaling), summarizing their behavior by generating facts per action using a VLM (Behavior Narrative Generator), and selecting the best one through comparative evaluation using a VLM (Behavior Best-of-N Judge). The proposed framework achieves state-of-the-art performance on OSWorld, a standard benchmark for computer-use agents, in success rate for long-horizon tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Experimental setup: Detailed experimental evaluation, including ablation studies, baselines, and datasets.\n\n- Strong empirical performance: Achieves state-of-the-art success rate on OSWorld.\n\n- Clear presentation: The method is well-described and motivated."}, "weaknesses": {"value": "- Inefficiency: Naive wide scaling means running N times more rollouts, with no mechanism to prune unpromising rollouts early or summarize fewer behaviors.\n\n- No learning or adaptation: The method relies on the performance of pre-trained LLMs and VLMs in a zero-shot setting.\n\n- VLM dependency: The approach depends on pre-trained VLMs both for summarizing behavior and for selecting the best trajectory, which may compound errors."}, "questions": {"value": "- Could you add in Table 2 the runtime for different BoN values? Is it expected to be exactly N× that of Agent S3?\n\n- How many steps does an optimal agent typically need to solve OSWorld tasks? Why did you choose 50 and 100 steps? Could you report the statistics (e.g., mean, median, variance) for the number of steps of an optimal agent?\n\n- What is the impact of visual augmentations for pointer interactions on the performance of the Behavior Narrative Generator? Could you provide further details on the visual augmentation method?\n\n-  Have you tried generating facts for the entire behavior with a single prompt (i.e., a trajectory-level summary) instead of per-action summaries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LHGdi34b8k", "forum": "R0viKzJ70t", "replyto": "R0viKzJ70t", "signatures": ["ICLR.cc/2026/Conference/Submission23890/Reviewer_sHwe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23890/Reviewer_sHwe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920897536, "cdate": 1761920897536, "tmdate": 1762942841870, "mdate": 1762942841870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Behavior Best-of-N (bBoN) method, which scales over agents by generating multiple rollouts and selecting the best trajectory among them for execution."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper explores the scaling over agents by a simple but effective best-of-N method, which largely improves the performance in benchmarks. \n\nThe paper proposes an effective Behavior Judge method that could support the scaling, which is better than the WebJudge."}, "weaknesses": {"value": "1.  The proposed method is not practical. It requires a simulator or virtual machine to perform multiple rollouts before real execution, which limits its applicability and can incur excessive computational costs during testing, especially when the N is set to a large number.\n\n\n2. This is also based on a strong assumption that the environment's transition dynamics are not static and that the simulator could be exactly the same as the real environment. This is difficult to guarantee in the real world, as websites are not stationary and can always have pop-ups or ads [1]. Thus, although the proposed method can achieve better performance in benchmarks, it is actually overfitting to the benchmark, which cannot represent real-world performance. \n\n3. As one can access the simulator, it would be better to run the baseline the same N times and use the task success indicator to select the successful trial among the N trials. This allows for calculating the success rate for each task, which can serve as the performance upper bound of the best-of-N method.\n\n\n[1] DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QQ37IdShyH", "forum": "R0viKzJ70t", "replyto": "R0viKzJ70t", "signatures": ["ICLR.cc/2026/Conference/Submission23890/Reviewer_muQu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23890/Reviewer_muQu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985677000, "cdate": 1761985677000, "tmdate": 1762942841630, "mdate": 1762942841630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We thank all the reviewers for their feedback for our submission. In this section we respond to common concerns raised by the reviewers.\n\n# Summary of costs, time and total experiment time\n\nBelow we summarize the average cost and time of each module per task. We include the median time as API delays caused our average to be heavily right-skewed. This has been included in the paper revision Appendix.\n\n| Per task                 | Single Rollout | BN Gen   | Judging (N=10) |\n|--------------------------|--------------------|---------------|-----------------------|\n| Average cost         |      $0.72         |  $0.11        |          $0.03        |\n| Average time (sec)|        891          |  433.4       |            226         |\n| Median time (sec) |        626           |   265.3      |           53.7         |\n\nWe present the rollout collection details and time using gpt-5-2025-08-07 as our model below.\n\nWe collect our agent trajectories by running OSWorld on AWS where a host instance (e.g. c4.8xlarge) contains the OSWorld code and our script to run Agent S3. The OSWorld code spawns a user-specified number of EC2 instances, each of which runs an OSWorld task. More details for how to run OSWorld on AWS can be found in their public repository. \n\nA c4.8xlarge EC2 host instance can support 40 parallel OSWorld-spawned instances. We run 10 rollouts over the OSWorld benchmark (361 tasks) in parallel using 4 c4.8xlarge instances for a total of 15 hours and 54 minutes.\n\nWe ran the Behavior Narrative Generation and comparative judging locally with OpenAI API  gpt-5-2025-08-07 using 100 workers.\n\nThe Behavior Narrative Generator took about 1 hour and 19 minutes for all 10 rollouts on each of the 361 tasks. To further optimize for latency, we could have generated facts on-the-fly, but decided to run them after rollouts to individually monitor each module’s runs. Comparative judging took about 20 minutes for all 361 tasks and was run after generating every behavior narrative.\n\nIn total, running Agent S3 w/ bBoN (N=10) took 17 hours and 33 minutes to fully complete.\n\n# Concerns about efficiency\n\nWe agree it is important to ensure the efficiency of our approach given we scale over many rollouts; however, our focus in this paper is mainly on improving the performance of computer-use agents which are still lagging behind human-level performance. We achieve SoTA by a large margin demonstrating the effectiveness of our new learning paradigm, narrowing the gap between human and Compute-Use Agent from about 10% (Agent S3’s average performance) to 2%. We believe exploring such techniques in future work would be appropriate to improve the efficiency of our new paradigm.\n\n## Ensemble of cheap and expensive models maintains higher performance\n\nOne experiment we’ve included in the paper under Section 4.4 explores performance with mixture-of-models. We find that increasing model diversity in the ensemble boosts performance. Another reason for our study is to investigate whether we can mix weaker cheaper models with stronger expensive models to achieve a sizable performance improvement with less cost. We share results below, suggesting that a balance can be struck between cost and performance.\n\n| Ensemble                       | Performance |\n|--------------------------------|-------------|\n| GPT-5 (N=4)                    | 66.5        |\n| GPT-5 (N=2) & GPT-5 Mini (N=2) | 64.9        |\n| GPT-5 Mini (N=4)               | 57.0        |\n\n## Cheap open-source rollouts and closed-source bBoN leads to large gains\n\nAfter our submission, we also explored using an open-source model like Qwen3-VL-30B-A3B-Thinking. We were interested in investigating how we can utilize cheap open-source rollouts and a combination of open-source and closed-source models in the bBoN modules which are considerably cheaper (5x) than rolling out. We completed 10 OSWorld runs with this model using our Agent S3 framework, averaging 33.3% success rate. Below we present the results on the open-source model and gpt-5-2025-08-07 during Behavior Narrative generation and comparative judging.\n\n| Behavior Narrative Generation  | Comparative Judging       | Performance |\n|---------------------------|---------------------------|-------------|\n| Qwen3-VL-30B-A3B-Thinking | Qwen3-VL-30B-A3B-Thinking | 40.9%       |\n| GPT-5                     | Qwen3-VL-30B-A3B-Thinking | 44.7%       |\n| Qwen3-VL-30B-A3B-Thinking | GPT-5                     | 49.4%       |\n| GPT-5                     | GPT-5                     | 51.5%       |\n\nWe find that re-using Qwen3-VL-30B-A3B-Thinking for behavior narrative generation and comparative judging leads to a performance improvement of +7.6% while using GPT-5 for both results in an 18.2% improvement."}}, "id": "pbH3SOZkoX", "forum": "R0viKzJ70t", "replyto": "R0viKzJ70t", "signatures": ["ICLR.cc/2026/Conference/Submission23890/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23890/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission23890/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763498803946, "cdate": 1763498803946, "tmdate": 1763498803946, "mdate": 1763498803946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}