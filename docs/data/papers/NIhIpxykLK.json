{"id": "NIhIpxykLK", "number": 3842, "cdate": 1757550384977, "mdate": 1759898066892, "content": {"title": "TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models", "abstract": "As students increasingly adopt large language models (LLMs) as learning aids,\nit is crucial to build models that are adept at handling the nuances of tutoring:\nthey need to identify the core needs of students, be adaptive, provide personalized\nguidance, and be accurate. To this end, we introduce TUTORBENCH, a dataset and\nevaluation benchmark designed to rigorously evaluate the core tutoring skills of\nLLMs. The dataset comprises 1,490 samples curated by human experts, focused\non high-school and AP-level curricula. The samples are drawn from three com-\nmon tutoring tasks: (i) generating adaptive explanations tailored to a student’s\nconfusion, (ii) providing actionable feedback on a student’s work, and (iii) pro-\nmoting active learning through effective hint generation. To account for the inher-\nent complexity of tutoring, samples are accompanied by sample-specific rubrics\nwhich are used to judge model responses during evaluation. TUTORBENCH uses\na reliable and fine-grained automatic evaluation method that uses an LLM-judge\nand the sample-specific rubrics. We evaluate 16 frontier LLMs on TUTORBENCH\nand present a detailed analysis of their performance and behavior. Our results\nshow that none of the frontier LLMs achieve a score of greater than 56%, show-\ning a large room for improvement. We find that LLMs fall short in exhibiting the\nfull range of tutoring skills needed to guide, diagnose, and support students effec-\ntively, with all the frontier models achieving less than a 60% pass rate on rubric\ncriteria related to these skills. We also find that different model families exhibit\nvaried strengths and limitations: the Claude models outperform others in support-\ning active learning, while they lag behind in the other two use cases. By releasing\nTUTORBENCH, we provide a comprehensive and unsaturated benchmark to guide\nthe development of the next-generation of AI tutors.", "tldr": "A dataset and benchmark to evaluate tutoring capabilities of LLMs", "keywords": ["benchmark", "auto-eval", "LLM-eval", "evaluations", "tutoring", "dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b2a63488050a829e931dfc123bfa3adb1809bfd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces TUTORBENCH, a benchmark evaluating LLMs’ tutoring abilities beyond knowledge recall. The dataset contains 1,490 high-school/AP STEM samples spanning three tasks: adaptive explanation, assessment/feedback, and active-learning support. Each sample includes fine-grained, weighted, sample-specific rubrics for evaluation via an LLM judge. The benchmark includes multimodal inputs, with just over 800 samples containing real-world student-work images. A range of state-of-the-art models are evaluated; the best achieve only ~56% overall, indicating substantial headroom. Performance analyses by task, cognitive category, and tutoring skill reveal consistent weaknesses, particularly in personalisation and guided learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important but underexplored dimension of LLM evaluation: tutoring proficiency. The benchmark spans six STEM subjects and three realistic tutoring scenarios. Its sample-specific, weighted rubrics enable structured, fine-grained evaluation of otherwise subjective tutoring behaviors. The dataset’s multimodal nature increases realism. Extensive analysis identifies performance trends across tasks, tutoring skills, and Bloom’s taxonomy. Validation of the judge against human experts (F 0.81) reinforces evaluation credibility. Results demonstrate a large performance gap between current models and desired tutor capabilities, underscoring the benchmark’s relevance."}, "weaknesses": {"value": "The benchmark focuses on three narrow tutoring scenarios, excluding other educational tasks. Only final responses are evaluated, limiting assessment of multi-turn adaptivity. \n\nCoverage is limited to STEM domains, excluding humanities, where tutoring differs significantly. Because rubrics are expert-written, stylistic bias is possible. The benchmark’s curated difficulty (removing “easy” samples) may over-emphasize failure cases and reduce representativeness."}, "questions": {"value": "1. How consistent is the LLM-as-a-judge across models, and do some models appear to exploit rubric phrasing more than others?\n\n2. What procedures ensured rubric construction consistency, and were inter-author agreement checks performed at design time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qWMllvkBKi", "forum": "NIhIpxykLK", "replyto": "NIhIpxykLK", "signatures": ["ICLR.cc/2026/Conference/Submission3842/Reviewer_qMyP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3842/Reviewer_qMyP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897654705, "cdate": 1761897654705, "tmdate": 1762917060673, "mdate": 1762917060673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a benchmark for evaluating LLMs in tutoring settings, focusing on three tasks: (1) generating adaptive personalized explanations to student questions, (2) providing feedback on student's incorrect work, and (3) generating pedagogical hints for active learning. The authors construct the dataset using human experts in their respective fields, accompanied with sample-specific rubrics. To automate the evaluation, they develop an LLM-judge pipeline for scoring LLM-generated responses based on expert-written rubric items. Finally, the authors conduct an in-depth analysis of different models' performance on the new benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Benchmark**\n\n- A subset of the samples include images, which better aligns with real-world usage of the models in tutoring settings.\n- Tutor and persona turns, as well as ground truth responses accompanied with rubric items are written by human experts.\n- LLM-based automatic evaluation for more scalable benchmarking."}, "weaknesses": {"value": "**Ground Truth**\n\nIn educational settings, it is often the case when there is no single ground truth response. Thus, constructing the benchmark on a single ground truth response inherently limits the evaluation of a broad range of possible pedagogically-appropriate responses. The proposes setup further exacerbates the issue as observing just a single turn from a student, different experts can make diverging conclusions about the student's knowledge state and thus employ different approaches for addressing the student's misconception(s).\n\nFirst, to study the space of possible pedagogical responses for a given sample, multiple human experts can write a ground truth response and the corresponding rubrics. An analysis of agreement and coverage of human-written responses would help understand the problem better.\n\nSecond, to reduce ambiguity in inferring the student's misconceptions, one can generate more context about the student. A more realistic setting would be developing a student-specific context, including their prior questions, interactions, submissions, etc. and using as an input to the AI tutor.\n\n**Study Mode Analysis**\n\nThe insights reported in Section 3.6 were surprising: OpenAI's special study mode performs worse than GPT-5 and Gemini-2.5-Pro. Can this be a limitation of the benchmark? One of the reported differences between GPT-5 and OpenAI's Study Mode is conciseness and not explicitly addressing the student's misconception (Figure 8). Is it possible that OpenAI's Study Mode is pedagogically more appropriate in these settings? If this is the case, then the benchmark can inadvertently penalize pedagogical responses and models.\n\n**Limited Analysis**\n\nThe current analyses of the paper studies the performance of LLMs broken down by different category tags. However, an in-depth study of the failure modes accompanied with qualitative examples is missing.\n\nAdditionally, one can study the effect of the system / user prompt on the models' performance. Would the failure modes be mitigated by prompting? How much can prompting improve the current results (i.e., what's the actual ceiling of the benchmark)?\n\n**Data Collection**\n\nThe paper misses important details about the data collection process. What was the medium of the data collection? What instructions were provided to the human experts?\n\n**Lack of Grounding in Education Research**\n\nThe paper overlooks prior education research on human tutoring:\n- What makes a tutor response pedagogically appropriate?\n- How can we promote active learning in tutoring sessions (e.g., Socratic learning)?"}, "questions": {"value": "- What was the medium of the data collection? What instructions were provided to the human experts?\n\n- How do the authors come up with the category tags for the rubrics? Are they grounded in prior work or do the authors propose a new taxonomy?\n\n- For annotating the difficulty level in Section 3.4, is the ground truth expert-written response being annotated?\n\n- What was the effect of the system prompt on the performance? How *\"sensitive\"* is the benchmark to the system / user prompt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zSvdyyJUPl", "forum": "NIhIpxykLK", "replyto": "NIhIpxykLK", "signatures": ["ICLR.cc/2026/Conference/Submission3842/Reviewer_Z3Ja"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3842/Reviewer_Z3Ja"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943029235, "cdate": 1761943029235, "tmdate": 1762917060392, "mdate": 1762917060392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TUTORBENCH is a benchmark for LLM tutoring for high-school and AP-level curricula. It focuses on three use cases: adaptive explanation, assessment/feedback, and active-learning support. The benchmark is multi-modal and uses LLM-as-a-judge for rubric creation and evaluation. The benchmark is multi-modal (which is great for an educational task) and is designed to be a \"difficult\" tutoring task for high school level courses and reports the quality of tutoring for 10 different models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The benchmark is designed as a multi-modal setting where students' incorrect or incomplete answers (which are also partially generated by an LLM and not entirely real-world data) receive tutoring guidance and grading based on a rubric. Evaluating and testing tutoring behavior in LLMs is necessary and interesting. The benchmark is designed for high school curricula (though subjects are not listed in the paper) across multiple domains. The use of sample-level rubrics makes the evaluation more reproducible. The authors compare a wide set of models and provide per-use-case breakdowns. The paper uses judge–human agreement and attempts to quantify reliability, which is a step in the right direction."}, "weaknesses": {"value": "The work introduces an educational benchmark and attempts to ground it also in the educational literature; however, there are some issues with the methodology and evaluation in the paper that I will list below:\n\nThe paper uses “active learning” incorrectly. Active learning is an instructional approach where students engage in activities (problem-solving, reflection, and active lecture viewing). The paper measures hint-based tutoring support that may promote active learning. The correct terminology is “hint-based scaffolding to support active learning”. This also needs to be reflected in the evaluation rubric. Currently, the evaluation rubric in the active learning use case is: \"identifying core difficulty, identifying students’ correct steps, identifying students’ incorrect steps, recalling and stating knowledge, providing alternative solutions, including examples, asking questions to guide students, and providing step-by-step help.\" Although I have to admit that it is not very clear what rubric is used for what specific use case because each section introduces a new rubric and a new evaluation criteria; compare sections 3.5, 3.4, 3.3 (unclear how \"recognizing student emotions such as confusion, frustration, curiosity, and generates responses with the right tone and style\" is evaluated and measured), 3.1, and 2.4.\n\nThe boundary between adaptive explanation (use case 1) and active-learning support (use case 3) is unclear. Figure 1 and the examples do not make it obvious which goal is being evaluated or how annotators would consistently separate them. Provide paired, minimal contrasting examples of how each use case is seen in LLM outputs and how they're evaluated.\n\nThe LLM-as-judge section is thin for subjective criteria. Reporting variance vs. three experts is useful, but the paper does not show within-judge variance across seeds, across-judge variance (swap-judge model), or sensitivity to prompt wording. Figure 6 should also include repeated judge ratings for the same samples.\n\nThe dataset is described as expert-curated and reflective of real student use, yet conversations are LLM-generated and filtered by SOTA failures. This risks model-family bias and confounds claims of superiority. \n\nThe personalization claims are overstated for 1–3 turn interactions. With such short context, a tutor cannot infer misconceptions or learning characteristics. On that note, the misconception extraction setup (Figure 2) is not realistic. Real tutors do not receive a fixed list of misconceptions to choose from; a rubric-matching task is not the same as open-ended diagnosis. The paper links misconception extraction to “active learning” and then evaluates whether LLMs detect LLM-generated misconceptions, without validating the extraction itself against human-coded labels or established inventories. This is an area that the paper needs more grounding in educational litertaure to make the dataset and task more relevant to the education community or be helpful for the education community.\n\nAs mentioned earlier, rubrics and dimensions are introduced but not reported consistently. Terms like “explicit/implicit” and “objective/subjective” appear without analysis, while Figures 4–5 pivot to other LLM-generated dimensions. Bloom’s taxonomy is hierarchical; reporting flat category rates misses conditional mastery across levels and weakens the pedagogical grounding."}, "questions": {"value": "Most of the issues and questions are listed in the earlier section, but to be more concrete, knowing the following would be helpful:\n\n- What evidence supports personalization beyond immediate adaptation in a short exchange?\n- Can you add side-by-side examples that differ only in instruction “explain” (use case 1) vs “hint without revealing the answer” (use case 3), and show distinct rubrics?\n- Can you consolidate the rubric dimension set and add Bloom-conditional analyses?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "I am not sure to what extent real student data is used and if so, it would be helpful to know whether the work has obtained an IRB approval."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D2cYtYAeVX", "forum": "NIhIpxykLK", "replyto": "NIhIpxykLK", "signatures": ["ICLR.cc/2026/Conference/Submission3842/Reviewer_U6oS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3842/Reviewer_U6oS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762029354480, "cdate": 1762029354480, "tmdate": 1762917060177, "mdate": 1762917060177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors provide a benchmark of approx. 1500 conversations across 6 STEM subjects of humans with a \"tutor-persona\". They evaluate 16 LLMs on this benchmark and find that Claude has best support. Evaluation itself is performed using LLMs that used rubrics; experiments with human are performed to highlist that LLM evaluation is accurate."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The contribution is timely as several commercial LLMs (by OpenAI, Anthropic, etc) have started to offer a \"study mode\". A good selection of dimensions regarding what makes a competent tutor is studied, and limitations are acknowledged."}, "weaknesses": {"value": "This paper seems to be an incremental improvement over https://arxiv.org/abs/2402.11111  from 2024 which has approximately half as many samples (800+), and echoes several ideas from the current paper. What is called here a \"rubric\", seems to be called a \"key point\" in the linked paper.\n\nGive the close overlap, the authors should make a clearer section in the related work, to show how the current paper distinguishes itself from prior literature. Otherwise, beyond the novelty aspect of evaluating the new study mode of LLMs, I am not sure that the approach in itself is sufficiently novel."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FSTSlE3O8l", "forum": "NIhIpxykLK", "replyto": "NIhIpxykLK", "signatures": ["ICLR.cc/2026/Conference/Submission3842/Reviewer_D5GB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3842/Reviewer_D5GB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762780459859, "cdate": 1762780459859, "tmdate": 1762917059924, "mdate": 1762917059924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}