{"id": "zj9Mm4bCAo", "number": 11823, "cdate": 1758204076032, "mdate": 1762972143166, "content": {"title": "HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and Generation", "abstract": "In this work, we present HieraTok, a novel multi-scale Vision Transformer (ViT)-based tokenizer that overcomes the inherent limitation of modeling single-scale representations. This is realized through two key designs: (1) multi-scale downsampling applied to the token map generated by the tokenizer encoder, producing a sequence of multi-scale tokens, and (2) a scale-causal attention mechanism that enables the progressive flow of information from low-resolution global semantic features to high-resolution structural details. Coupling these designs, HieraTok achieves significant improvements in both image reconstruction and generation tasks. Under identical settings, the multi-scale visual tokenizer outperforms its single-scale counterpart by a 27.2\\% improvement in rFID ($1.47 \\rightarrow 1.07$). When integrated into downstream generation frameworks, it achieves a $1.38\\times$ faster convergence rate and an 18.9\\% boost in gFID ($16.4 \\rightarrow 13.3$), which may be attributed to the smoother and more uniformly distributed latent space. To the best of our knowledge, we are the first to introduce multi-scale ViT-based tokenizer in image reconstruction and image generation. We hope our findings and designs advance the ViT-based tokenizers in visual generation tasks.", "tldr": "", "keywords": ["visual tokenizer", "multi-scale design", "latent representation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d6f811bdddb14341c1b659c0c341db23a144d46f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents HieraTok, a multi-scale ViT-based image tokenizer for reconstruction and generation. HieraTok applies multi-scale down-sampling to the encoder features, and then uses a ViT decoder to reconstruct RGB images of the corresponding resolutions for training. By introducing scale-causal attention, HieraTok achieves performance improvements over single-scale tokenizers in both reconstruction and generation tasks, and conducts detailed ablation studies on design choices."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally well written and structured, making it easy to follow.\n- The proposed HieraTok offers a solution to address the challenges of multi-scale image tokenization. By downsampling the encoder features and reconstructing corresponding RGB images, HieraTok improvements both image reconstruction and generation.\n- Ablation results are detailed and show reasonable analysis of different model components."}, "weaknesses": {"value": "- **Unfair comparison**: Since the decoder requires more input tokens, the comparison with the single-scale baseline (Table 7) is not fair. The paper lacks a truly fair comparison (e.g., upsampling single-scale tokens before feeding them into the decoder, adding learnable tokens to the single-scale decoder, or increasing the decoder size to maintain consistent FLOPs), which substantially weakens the persuasiveness of the reported results.\n- **Incremental  improvement**: There are some concerns regarding the authors’ claim about the coarse-to-fine mechanism in the decoder. Since decoding is performed in a one-pass forward manner, for `Scale-independent Attention`, tokens of different scales merely share an FFN—there is no actual “progressive flow of information from coarse-grained semantic tokens to fine-grained structural tokens,” but rather just coarse-grained and fine-grained tokens. Moreover, the performance improvement of `Scale-causal Attention` over `Scale-independent Attention` is quite limited (Table 3), and the paper lacks a deeper analysis to substantiate the effectiveness of the coarse-to-fine design."}, "questions": {"value": "- How does the fine-to-coarse version of `Scale-causal Attention` perform?\n- Given the use of multi-scale features and *Scale-causal Attention*, why doesn’t the generator adopt VAR?\n- More fair comparisons and detailed analyses are needed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yyNuKIwklH", "forum": "zj9Mm4bCAo", "replyto": "zj9Mm4bCAo", "signatures": ["ICLR.cc/2026/Conference/Submission11823/Reviewer_Nrzb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11823/Reviewer_Nrzb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751963361, "cdate": 1761751963361, "tmdate": 1762922844862, "mdate": 1762922844862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "eMbqdCylw0", "forum": "zj9Mm4bCAo", "replyto": "zj9Mm4bCAo", "signatures": ["ICLR.cc/2026/Conference/Submission11823/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11823/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762972142398, "cdate": 1762972142398, "tmdate": 1762972142398, "mdate": 1762972142398, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HieraTok, a novel multi-scale Vision Transformer (ViT)-based visual tokenizer that overcomes the single-scale limitation of existing ViT tokenizers. Extensive experiments on ImageNet-256 show that HieraTok consistently improves both reconstruction and generation performance over single-scale ViT tokenizers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a real limitation of ViT tokenizers—fixed-scale representation—and demonstrates a well-motivated architectural solution with measurable benefits.\n2. The proposed multi-scale downsampling and scale-causal attention are conceptually straightforward and easily integrable into existing ViT architectures without changing token counts in downstream generative models.\n3. The paper provides thorough ablations covering downsampling strategies (convolution vs. interpolation), attention types (scale-causal, scale-independent, full), and number of scales. The latent-space uniformity analysis (via t-SNE, entropy, and Gini coefficients) convincingly supports the design claims."}, "weaknesses": {"value": "1. While effective, the idea of multi-scale representations has been extensively explored in FPN, Swin, and VAR-like architectures. The novelty mainly lies in adapting these ideas to ViT tokenizers rather than introducing a fundamentally new principle.\n2. It is unclear why scale-causal attention is needed, in other words, whether it is necessary to restrict high-resolution tokens to interact exclusively with preceding low-resolution tokens.\n3. Introducing multi-scale design will inevitably bring about more computation overhead, therefore, it would better to benchmark the GFLOPs/inference speed in Table 2."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OoQiKPJoMf", "forum": "zj9Mm4bCAo", "replyto": "zj9Mm4bCAo", "signatures": ["ICLR.cc/2026/Conference/Submission11823/Reviewer_tMEC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11823/Reviewer_tMEC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813552666, "cdate": 1761813552666, "tmdate": 1762922844370, "mdate": 1762922844370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents HieraTok, a novel multi-scale Vision Transformer tokenizer, and conducts experiments demonstrating that the hierarchical tokenizer indeed improves model performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow.\n\n - The experimental results demonstrate that a hierarchical tokenizer can effectively improve performance."}, "weaknesses": {"value": "- The comparison in Table 1 is unfair. Most methods compared under the ViT-based tokenizer setting are discrete tokenizers, whereas HieraTok is a continuous tokenizer and employs a DiT model as the generative backbone. Please compare HieraTok with other continuous tokenizers, or alternatively, provide results of a discrete version of HieraTok for fair comparison.\n\n - The improvement of the VAE through multi-scale design is rather trivial. While the multi-scale approach can indeed enhance reconstruction quality, a more complex VAE also makes generative model training significantly harder. It would be better to report the number of parameters for each model in Table 1 to clarify the trade-off between complexity and performance.\n\n - The results in Table 6 suggest that, when using generative models with the same number of parameters, the performance of HieraTok is much worse than that of even discrete tokenizers. Please verify this observation or provide an explanation for the discrepancy."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4bea1ZfEDN", "forum": "zj9Mm4bCAo", "replyto": "zj9Mm4bCAo", "signatures": ["ICLR.cc/2026/Conference/Submission11823/Reviewer_RnQM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11823/Reviewer_RnQM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834124650, "cdate": 1761834124650, "tmdate": 1762922843974, "mdate": 1762922843974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Problem.** ViT-based visual tokenizers typically learn single-scale latents, which may cap reconstruction fidelity and slow downstream generator convergence. **Method.** *HieraTok* introduces (i) **multi-scale token construction** by downsampling the encoder’s token map into a pyramid, concatenated low→high resolution, and (ii) a **scale-causal attention** in the decoder so high-res tokens attend to current+coarser scales only; plus **multi-scale RGB supervision** enforcing the approximation \\(D(D_s(Z))\\approx D_s(D(Z))\\). **Key innovations.** Concatenate multi-scale token maps (Eq. 5), scale-causal attention (Eq. 7, Fig. 3), and scale-consistent reconstruction losses (Eqs. 8–9). **Main results.** On ImageNet-256, multi-scale vs single-scale improves rFID by 27.2% (1.47→1.07) and gFID by 18.9% (16.4→13.3) under matched training; scaled-up training reports rFID 0.45 and gFID 1.82 among ViT tokenizers (Tables 1–2, Fig. 4). **Significance.** If the gains are not confounded by capacity/compute, a multi-scale ViT tokenizer could be a stronger default for both reconstruction and generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Clear architectural proposal.** Multi-scale tokens via simple downsampling + a principled scale-causal attention (Fig. 2–3; Eqs. 5–7).\n- **Consistent improvements under a fixed protocol.** Tables 2 and 6 show multi-scale wins across KL weights and DiT sizes.\n- **Useful ablations.** Downsampling (interp vs conv) and attention (full/scale-independent/scale-causal) dissect where gains come from (Table 3)."}, "weaknesses": {"value": "1) **Capacity and compute confound the core claim.**  \nYour “multi-scale” model has **more parameters and FLOPs** and decodes **more tokens** (401M vs 346M; 230.45 vs 182.36 GFLOPs; 341 vs 256 decoder tokens; Table 7). Without parameter-, token-, and FLOP-matched controls, the 27.2% rFID and 18.9% gFID gains may largely reflect capacity. Please add: (a) a single-scale model with matched params/decoder tokens, (b) a multi-scale model trimmed to match single-scale FLOPs/tokens, and (c) report wall-clock per-step time. (Table 7).\n\n2) **No end-to-end runtime/latency evidence.**  \nYou claim faster convergence (Fig. 4), but report *steps-to-target* rather than **images/s** or **latency**. Please provide throughput, memory, and p95/p99 step time for tokenizer pretraining and DiT training; otherwise “1.38× speedup” is ambiguous. (Fig. 4).\n\n3) **Novelty over multi-scale precedents is under-positioned.**  \nThe design echoes FPN/U-Net top-down refinement (Fig. 3; §2.3). The paper claims “first multi-scale ViT tokenizer,” but related ViT tokenizers (FlexTok, TiTok, GigaTok, MAETok, ViTok) are listed (Table 1) without a **controlled head-to-head at equal compute**. Please: (i) compare against at least two ViT tokenizers with matched params/FLOPs; (ii) add a single-scale **decoder-only pyramid** baseline to test if the benefit is from supervision rather than tokens. (Table 1; §2.1–2.3).\n\n4) **Scale-causal attention complexity and benefit are unclear.**  \nScale-causal expands keys/values to all coarser levels (Eq. 7), potentially **increasing attention cost** over the concatenated sequence. Please report attention FLOPs/heads and compare to full attention and scale-independent in wall-clock **and** quality. (Eq. 7; Table 3).\n\n5) **Multi-scale supervision may be doing the heavy lifting.**  \nThe Appendix derivation \\(D(D_s(Z))\\approx D_s(D(Z))\\) is assumed (App. B), but its empirical error is never measured. Provide \\(\\|D(D_s(Z))-D_s(D(Z))\\|_2\\) vs training time, and an ablation **without** multi-scale RGB supervision to quantify its standalone effect. (Eqs. 8–9; App. B).\n\n6) **Statistics and robustness are thin.**  \nCore tables lack μ±σ across seeds; improvements like gFID 16.4→13.3 and rFID 1.47→1.07 could be sensitive. Please add ≥3 seeds, CIs, and **paired tests**, and report FID estimators’ CI. (Tables 2, 6; Fig. 4).\n\n7) **Latent-space “uniformity” analysis is largely descriptive.**  \nt-SNE plots (Fig. 5) and Gini/entropy (Table 5) correlate with gFID but do not establish causality. Please follow VAVAE-style protocols with stronger geometry diagnostics and show how these metrics predict *per-class* convergence, not just aggregates. (Fig. 5; Table 5).\n\n8) **Downsampling choice is under-analyzed for generation.**  \nYou conclude **conv downsampling** helps generation while interpolation does not (Table 3). Why? Provide spectra/feature-rank analyses of \\(Z_s\\), and visualize cross-scale attention patterns to show semantics-→detail flow. (Table 3).\n\n9) **Limited scope: single dataset/resolution and one paradigm.**  \nExperiments are ImageNet-256 only; no COCO/LAION subsets, no higher resolution, and only DiT as the generator. Please add at least one **additional dataset** and **another generator family** (e.g., MAR/AR or LDM) to establish generality. (§5).\n\n10) **Reproducibility & fairness.**  \nImplementation details exist (App. E) but not full configs/seeds/checkpoints. Also, “training fairness” is argued by matching epochs, but equal **epochs** ≠ equal **compute** when sequences/params differ. Release scripts and report matched-compute comparisons. (App. E; C.2)."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hDZeWZMU0p", "forum": "zj9Mm4bCAo", "replyto": "zj9Mm4bCAo", "signatures": ["ICLR.cc/2026/Conference/Submission11823/Reviewer_qU1U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11823/Reviewer_qU1U"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762078311505, "cdate": 1762078311505, "tmdate": 1762922843658, "mdate": 1762922843658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}