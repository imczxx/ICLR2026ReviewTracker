{"id": "zq40cmz1JD", "number": 13973, "cdate": 1758226250307, "mdate": 1759897398953, "content": {"title": "When Speculation Spills Secrets: Side Channels via Speculative Decoding in LLMs", "abstract": "Deployed large language models (LLMs) often rely on speculative decoding, a technique that generates and verifies multiple candidate tokens in parallel, to improve throughput and latency. In this work, we reveal a new side-channel whereby input-dependent patterns of correct and incorrect speculations can be inferred by monitoring per-iteration token counts or packet sizes. We demonstrate that an adversary observing these patterns can fingerprint user queries with >90% accuracy across four speculative-decoding schemes, REST (∼100%), LADE (up to 92%), BiLD (up to 95%), and EAGLE (up to 77.6%) and leak confidential datastore contents used for prediction at rates exceeding 25 tokens/sec. We evaluate the side-channel attacks in both research prototypes as well as the production-grade vLLM serving framework. To defend against these, we propose and evaluate a suite of mitigations, including packet padding and iteration-wise token aggregation.", "tldr": "We develop a side channel attack leaking private user inputs by exploiting speculative decoding optimizations in LLM inference.", "keywords": ["Large Language Models", "Speculative Decoding", "Side Channel Attack", "Privacy"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/75aa7cb80aea540f4112e06aa772201ce8b59ba5.pdf", "supplementary_material": "/attachment/718d367deae1b66b0251d3c3b4097200a627f0f0.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a side-channel attack on speculative decoding, where the adversary can infer the user prompt by observing the packet size sent from a remotely hosted LLM (proxy metric for a per-iteration token counts, i.e., speculation accuracy). For some speculative decoding schemes (e.g., REST), the attack can fingerprint user queries (i.e., can classify the query content out of 50 predefined classes) with near-perfect accuracy. Their method is effective under a real-world deployment scenario using vLLM (Section 4.7). They further discuss that this side channel can be exploited to allow a data extraction attack on an algorithm like REST that relies on a datasource for speculation (Section 5)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper reveals the exploitability of speculation accuracy, which can be estimated in practice based on observing the packet size. It is an interesting and surprising observation that there exists a correlation between speculation accuracy and the input prompt.\n2. Their attack essentially works better when the speculation algorithm is more stable (lines 314-316), implying a security-utility tradeoff, and a growing concern as the speculation accuracy improves.\n3. They have multiple settings for many experiments (e.g., as described in Section 4.3, lines 240-260), ranging from the easiest setting that gives the upper bound of the attack success, to a more practical setting."}, "weaknesses": {"value": "1. **High-level concern about the contribution**: The method essentially reduces to a random‑forest–based 50‑class classifier (Section 4.4, lines 269–292) that uses “tokens per iteration” as the primary input feature. While reasonable, I believe either of the following must be satisfied/clarified for an acceptable contribution level: (i) technical novelty: include a component specific to the speculative‑decoding setting that materially improves attack success; or (ii) strong practicality: since the current setup assumes a predefined set of 50 classes, I remain unconvinced about the effectiveness in realistic settings where users ask arbitrary topics in arbitrary phrasings.\n\n2. **Detailed analysis of why the attack succeeds**: It is unclear “why” the attack works, i.e., what parts of the trace the classifier relies on when making predictions. The draft presents the method as generic and versatile, yet the experiments are limited to a medical dataset extracted from Han et al. (line 243), so it may be domain‑specific. For example, I hypothesize the correlation between speculation accuracy and user prompt could stem from something specific to disease names, and the method may degrade in other domains e.g., where no technical terms exist. Overall, I would like the authors to go one step deeper so they can claim that the method works for some principled reasons, and to discuss more on when / why it works.\n\n3. **Presentation (minor)**: The typo in the abstract (line 14, transposed “?”) is careless for a conference submission. Also, pasting dozens of raw prompts across 5+ pages (pages 12–18) without formatting/description (e.g., grouping or a table/figure) is unusual and makes the information hard to grasp."}, "questions": {"value": "1. **Concrete setup for OOD ablation** (Section 4.8): The authors conduct a training by using 50 diseases that are generated by GPT-4o as common topics users typically ask about, and then evaluate the prediction accuracy using the 50 predefined classes. Do the 50 classes for training and evaluation have one-to-one correspondence? If yes, how is it different from Experiment 3 (lines 250-260)? If not, how is the evaluation designed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CckZOANTk3", "forum": "zq40cmz1JD", "replyto": "zq40cmz1JD", "signatures": ["ICLR.cc/2026/Conference/Submission13973/Reviewer_LAHj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13973/Reviewer_LAHj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761505542060, "cdate": 1761505542060, "tmdate": 1762924473118, "mdate": 1762924473118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers information leakage in speculative decoding for LLMs. It proposes an attack based on packet sizes and demonstrates that it can identify user queries with high success rates across four speculative decoding schemes in specific medical query scenarios. It proposes several defense mechanisms, which can effectively reduces the attack success rates at high costs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is mostly well-written and easy to follow.\n2. It is an interesting observation that per-iteration token count or packet size can leak private information.\n3. The attack works across different speculative decoding schemes.\n4. The defense mechanisms can effective reduce the risk of information leakage."}, "weaknesses": {"value": "1. The experiments is limited to a very special medical chatbot scenario where there are only a small number of diseases. There is no experiment about scaling up the number of possible labels, or diseases in this case. \n2. The proposed defense mechanisms are all very costly and not very practical."}, "questions": {"value": "1. How will the attack success rate scale with the number of diseases? \n2. How will the attack cost, e.g. in terms of number of profiling examples and training cost,  scale with the number of diseases? \n3. In the out-of-distribution experiment, how different are the two distributions? In particular, how many of the \"50 common diseases\" overlap with the training examples? Since the attack success rate already drops significantly in the current experiment, should one expect the risk to be of little practical importance when the number of diseases is large?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FClHFaXryX", "forum": "zq40cmz1JD", "replyto": "zq40cmz1JD", "signatures": ["ICLR.cc/2026/Conference/Submission13973/Reviewer_KSN3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13973/Reviewer_KSN3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618154641, "cdate": 1761618154641, "tmdate": 1762924472463, "mdate": 1762924472463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reveals that speculative decoding techniques used to accelerate inference in large language models pose severe privacy risks. By analyzing packet-size patterns in encrypted network traffic, attackers can infer whether internal speculations succeed or fail, enabling them to identify users’ sensitive queries or to extract the confidential parameters that drive speculation. \n\nExperiments across multiple speculative-decoding schemes and real-world deployment settings confirm the effectiveness of this side-channel attack. Although defenses such as packet padding or token aggregation exist, they typically force a trade-off between performance and privacy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is the first to reveal a packet-size-based side-channel attack introduced by speculative decoding techniques in LLMs.   It explicitly differentiates this work from prior LLM side-channel attacks, such as token-length leakage  and timing attacks by focusing on input-dependent speculation patterns.\n\n2. The attack is validated across four speculative decoding schemes (REST, LADE, BiLD, EAGLE) and tested in both academic prototypes and the production-grade vLLM serving framework, confirming its novelty as the first systematic exploration of this specific side channel .\n\n3. The paper’s experiments are comprehensive, as evidenced by the design of its fingerprinting attack and multi-dimensional evaluation."}, "weaknesses": {"value": "1. There are still issues with writing and typesetting. For example, the caption of Figure 1; the font size in Figure 5 is excessively small; and in tables (e.g., Table 1, Table 4), the layout is overly compact.\n\n2. Although Experiment 3 (semantically similar but non-identical queries) and Section 4.8 (out-of-distribution training) evaluate the fingerprinting attack under approximate or out-of-distribution dataset setups, both configurations remain somewhat idealized.\n\n3. The paper provides no justification for choosing random forest over other advanced methods and does not explore whether using more sophisticated algorithms could reveal higher attack potential or more robust speculation patterns.\n\n4. While token aggregation is shown to reduce attack accuracy, the paper does not measure its impact on end-user perceived latency\n\n5. Common LLM optimizations like paged attention split the KV cache across multiple GPUs, which may interact with token aggregation to alter packet generation logic .     The paper does not test this interaction, so it remains unknown if token aggregation is broken or weakened by paged attention."}, "questions": {"value": "1. Intuitively, the traffic pattern of each prompt–response pair seems likely to be unique. Would it be possible to introduce additional metrics and perhaps specific thresholds to make the \"same/different\" distinction more explicit?\n2. Could concurrent traffic affect the attack accuracy? Have any experiments measured the fingerprinting attack’s performance under real Internet conditions?\n3. Could employing other classifier algorithms improve the overall effectiveness to some extent?\n4. The current dataset appears modest in scale, and the prompts are short and straightforward. While this nicely demonstrates the attack under ideal conditions, I wonder how it would behave when the scenarios grow more complex.\n5. The attack’s success rate varies across decoding strategies. Is there any theoretical insight that helps explain why these differences emerge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YWSGU3ucAp", "forum": "zq40cmz1JD", "replyto": "zq40cmz1JD", "signatures": ["ICLR.cc/2026/Conference/Submission13973/Reviewer_QPdf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13973/Reviewer_QPdf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925503083, "cdate": 1761925503083, "tmdate": 1762924472072, "mdate": 1762924472072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies side-channel attacks on speculative decoding in LLMs that can leak information about a user’s prompt. Different prompts/responses can lead to unique patterns of accepted and rejected draft tokens, which can be manifested as differences in packet sizes (if the response is streamed). Therefore, a network attacker can observe packet sizes to predict information about a user’s prompt.\n\nThe paper runs experiments in controlled, simulated settings, finding that the attack can accurately predict at low temperatures when the exact set of test prompts is known beforehand. At higher temperatures, or when the exact test prompts are not known beforehand, accuracy significantly decreases, but is still better than random guessing. The paper also proposes and evaluates defenses to mitigate the attack."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "It is important to draw attention to the potential privacy risks of LLM inference techniques, given how widely LLMs and inference optimizations are used today. The paper runs simple, proof-of-concept experiments in controlled, simulated settings that demonstrate that speculative decoding can leak information about user prompts via packet sizes. The paper also proposes and evaluates defenses against the attack, giving concrete mitigations that LLM providers can implement.\n\nCode and documentation are uploaded in the supplementary material, which enhances the reproducibility of the paper."}, "weaknesses": {"value": "* The high accuracies reported in the abstract are only achieved in limited settings: low temperature (0.3) and the exact set of 50 test prompts are known and used at training time. When the temperature increases, or when the exact test prompts are not trained on, the accuracies decrease significantly, although still above random guessing. It seems like much of what the attack is doing is memorizing the fingerprint for specific responses, as indicated by the brittleness to increasing temperature.\n\n  To avoid being misleading, the accuracies in the abstract should be updated or omitted, or the caveat of the limited setting should be explicitly stated.  \n* No experiments are run on real-world production systems like ChatGPT or Claude, which are done by the related works [Weiss et al. (2024)](https://arxiv.org/abs/2403.09751) and [Carlini et al. (2024)](https://arxiv.org/abs/2410.17175). This would provide more evidence of the efficacy of the attack in more realistic settings, where more factors are unknown (speculative decoding implementation, streaming logic, etc.)  \n* I think that the tables would be more naturally presented as graphs. The tables generally show how the accuracy changes as some parameter changes (temperature, traces per query, etc.). For example, Table 2 could plot accuracy against temperature, with one line representing each speculative decoding method. The tables have many numbers, making them hard to read and see the overall trends.  \n* The formatting of the paper could use some polishing. For example, parenthetical citations are not correctly formatted throughout the paper. The parentheses are missing, so they interrupt the sentence and make them harder to read."}, "questions": {"value": "### Questions\n\n1. For the locally run experiments, how are the packet sizes determined? How is it determined when to send each packet, and how many tokens are sent in each packet?  \n2. When the attack accuracy is high, how similar are the responses/traces at test time compared to the ones from training time? Is the model giving the same response, or is there more diversity?  \n3. For REST, how similar are responses to each other as the temperature increases? Since REST is retrieval-based, I am wondering if increasing temperature does not increase response diversity as much as in the other speculative decoding methods, leading to the high accuracy for REST.  \n4. Do you have a reference or explanation for modeling times in high server load with a log-normal distribution?  \n5. In the out-of-distribution experiment (Section 4.8), for each ground truth prompt, there may be several diseases that have similar symptoms. So, random guessing would have a higher performance than normal. What is the accuracy achieved by random guessing under this evaluation?  \n6. In the datastore leakage attack (Section 5), what is the false positive rate, i.e., classifying a sequence as in the datastore when it does not actually appear?  \n7. How does the packet size increase 230x when padding to 1024 bytes? This means that the original packet size is just 4 bytes, which is enough room for at most 4 characters (not even counting metadata).  \n   1. Also, the packet size can be made smaller, and if there are too many tokens generated in one iteration, it can just be split into several packets. There can be some delay between these packets so the attacker cannot tell that it was one iteration.  \n8. From the code in the supplementary material, it looks like a random forest is also used when evaluating the inter-arrival time side-channel attack (Carlini & Nasr, 2024). However, that paper uses Gaussian Mixture Models and convolutional neural networks for predicting prompts/topics from the timing data. For a fair comparison, GMMs should be used for the time side-channel, as it may achieve better performance.  \n9. More generally, did you try any methods other than random forests? Other methods may perform better on both the packet size and inter-arrival time data.  \n10. Do real-world production systems such as ChatGPT, Claude, etc. actually send multiple tokens per packet? Are there systems that always send exactly one token per packet?\n\n### Notes\n\n1. \\> appears as ¿ in the abstract.  \n2. Parenthetical citations are missing the parentheses throughout the paper. `\\citep` should be used.  \n3. There is an extra indentation at the start of lines 142 and 154\\.  \n4. Line 157 typo: “wand” should be “and”  \n5. It would be good to have more descriptive titles for each experiment in addition to just “Experiment 1”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YAO1FTCAHu", "forum": "zq40cmz1JD", "replyto": "zq40cmz1JD", "signatures": ["ICLR.cc/2026/Conference/Submission13973/Reviewer_mQbv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13973/Reviewer_mQbv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928710671, "cdate": 1761928710671, "tmdate": 1762924471619, "mdate": 1762924471619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}