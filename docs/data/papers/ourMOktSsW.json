{"id": "ourMOktSsW", "number": 13366, "cdate": 1758217020953, "mdate": 1759897442371, "content": {"title": "Rule-Bottleneck RL: Learning to Decide and Explain for Sequential Resource Allocation via LLM Agents", "abstract": "Deep Reinforcement Learning (RL) has demonstrated remarkable success in solving sequential resource allocation problems, but often suffers from limited explainability and adaptability---barriers to integration with human decision-makers. In contrast, LLM agents, powered by large language models (LLMs), provide human-understandable reasoning but may struggle with effective sequential decision making. To bridge this gap, we introduce Rule-Bottleneck RL (RBRL), a novel LLM agent framework for resource allocation problems that jointly optimizes language-based decision policy and explainability. At each step within RBRL, an LLM first generates candidate rules---language statements capturing decision priorities tailored to the current state. RL then optimizes rule selection to maximize environmental rewards and explainability, with the LLM acting as a judge. Finally, an LLM chooses the action (optimal allocation) based on the rule. We provide conditions for RBRL performance guarantees as well as the finite-horizon evaluation gap of the learned RBRL policy. Furthermore, we provide evaluations in real-world scenarios, particularly in public health, showing that RBRL not only improves the performance of baseline LLM agents, but also approximates the performance of Deep RL while producing more desirable human-readable explanations. We conduct a survey validating the improvement in the quality of the explanations.", "tldr": "We design a novel rule-based RL framework that provides joint explanation and decision optimization for high-stake resource allocation problems.", "keywords": ["Joint decision and explanation", "rule-bottleneck", "constrained-resource allocation", "agent"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd4f235c5890c74a1abdaddc32eee722280aa370.pdf", "supplementary_material": "/attachment/f2fda71a511fdd7af14e3ff12de7d3f21806a727.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes RBRL, which is a LLM agent framework for resource allocation problem that jointly optimizes language-based decision policy and explainability. The LLMs are used to generate a diverse set of rules according to the environment state. The author extend the conventional environmental state-action space by integrating the rules into state generated by LLMs, which make a novel framework that enables RL to operate on a more interpretable decision strucrture. Experiments show the satisfactory performance of the proposed model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed model integrates rule generation and selection into the reinforcement learning framework, forming an extended MDP. It employs an attention mechanism to handle dynamically changing rule sets and introduces a \"rule reward\" to jointly optimize decision-making performance and explanation quality. The idea is interesting and the motivation is novel.\n2. The experimental setup is comprehensive, covering three resource-constrained allocation domains such as wearable device assignment, heat alerts, and bin packing. The results are thoroughly analyzed, demonstrating the model's advantages."}, "weaknesses": {"value": "1. The appendix contains material that belongs in the main text, which might violate submission guidelines? Additionally, the paper lacks a conclusion section.\n2. The paper does not address the model's computational efficiency, despite each decision requiring multiple LLM calls for rule generation, which may be very time-consuming.\n3. The paper does not address how to ensure the LLM-generated rules are both diverse and high-quality. The theoretical assumption regarding rule coverage also appears too strong."}, "questions": {"value": "1. Although the paper emphasizes rules as a \"bottleneck\", it remains unclear how this bottleneck concretely governs the decision flow. Specifically, is there a strong constraint between the selected rule and the final action? Since the LLM can still freely generate actions after the RL policy selects a rule, this weak coupling may lead to a misalignment between the rule and the executed action, undermining the intended bottleneck effect. Could the authors provide corresponding theoretical analysis on this mechanism?\n2. The authors do not systematically evaluate the quality of LLM-generated rules—such as their coverage of optimal decisions, handling of redundancy or conflicts, and temporal consistency. Furthermore, the potential biases of using LLMs as judges remain unaddressed, warranting further human alignment.\n3. The computational efficiency of the proposed method remains unexamined. Since each decision step requires multiple LLM invocations, this is likely to lead to significant computational overhead. A time efficiency analysis should be provided.\n4. An ablation study is needed to compare the effect of including versus excluding the dynamically generated rule set in the state representation, as this inclusion may introduce noise and hinder policy convergence.\n5. Is it over-designed to use ”cross-attention + self-attention” for selecting rules? Could the author compare some simple rule selection strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dZSoqgapG2", "forum": "ourMOktSsW", "replyto": "ourMOktSsW", "signatures": ["ICLR.cc/2026/Conference/Submission13366/Reviewer_eNFN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13366/Reviewer_eNFN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760772237953, "cdate": 1760772237953, "tmdate": 1762924010393, "mdate": 1762924010393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Rule-Bottleneck Reinforcement Learning (RBRL) framework, which aims to combine the explainability provided by large language models (LLMs) with the reward-maximizing policy updates of reinforcement learning (RL) for sequential decision-making tasks.\nThe paper claims that:\n-  RBRL improves explainability, as it integrates an LLM component to generate interpretable reasoning.\n- It is more efficient than fine-tuning LLMs, which can be computationally expensive and time-consuming.\n- The approach still involves updating a policy neural network, and the cumulative reward increases steadily as training progresses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a novel idea of combining LLMs with RL to address resource-constrained allocation problems. This framework enables LLMs to provide natural language explanations for action choices.\n2. It offers a detailed and clear description of how to integrate LLM inference—through prompt selection and standardized schema outputs (rule sets)—with a policy-updating network, resulting in a holistic decision-making policy within an MDP framework.\n3. The authors conduct experiments on four datasets: two Wearable Device Assignment tasks (Uganda and MIMIC-III), as well as the HeatAlert and BinPacking tasks.\n4. To evaluate the explainability of decision processes, rules, and action suggestions, the authors also conducted a survey with 40 participants to assess the quality of the natural language explanations generated by the LLM."}, "weaknesses": {"value": "1. The reviewer is concerned with the **writing and organization** of the paper. For instance, in line 245, *R* should likely be written as \\( R^t \\) (if not, clarification is needed on what *R* represents). The paper lacks a clear and intuitive illustration of the RBRL framework where Figure 1 is difficult to interpret and poorly aligned with the description in Section 4. Moreover, Algorithm 1 appears too early, before many variables are defined, which disrupts the reading flow. The transition from Section 5 (theoretical guarantees) to Section 6 (experiments) also feels abrupt; after understanding RBRL, the paper would flow more naturally by moving directly into experiments. Due to page constraints, both the theoretical and experimental sections are underdeveloped—one could be prioritized to better highlight the work’s novelty and contributions. *Minor:* line 440 refers to “MIMIC-II” instead of “MIMIC-III.”  \n\n2. The **experimental design and results** raise significant concerns. The shaded areas in Figure 4 indicate high uncertainty. Furthermore, aside from the MIMIC dataset, the non-RL baselines (e.g., CoT or LLM-prompted rule bottlenecks) achieve strong cumulative rewards compared to pure RL methods, which perform worse even after extensive training. This raises the concern that the LLM (billions of parameters) might be the dominant factor driving performance, potentially benefiting from **data leakage** or strong priors inherent in the tasks.  \n\n3. The reviewer is also concerned about the **complexity of actions and states**. As the combination of possible actions at each step increases, the number of LLM-generated rules will also grow, leading to increased runtime and larger rule-selection network sizes.  \n\n4. While incorporating LLMs into the RL framework indeed improves **explainability**, it also introduces substantial computational overhead. Although the authors claim that RBRL is less resource-intensive than fine-tuning LLMs, there is no comparison of the computational cost between a pure RL system and RBRL. Since RBRL involves multiple LLM calls for text and embedding generation in each episode or exploration step, this overhead could be significant."}, "questions": {"value": "1. If the authors were asked to reconsider the writing structure, how might they reorganize it?  \n2. The reviewer has more background in the healthcare domain and understands that MIMIC-III is an offline dataset. How does such an offline environment contain all possible next states when diverse actions are applied?  \n3. Could the authors provide an explanation for the uncertainty region shown in Figure 4?  \n4. This may be an unusual question, but the reviewer hopes there exists a task where the LLM-only model performs poorly, and combining RL with LLM improves performance to a level comparable with pure RL. Is there such a case?  \n5. If the authors agree that the chosen tasks are relatively simple and have limited action spaces, what types of datasets or tasks would they explore next? Could the authors provide some intuition on how increasing action and state complexity might affect RBRL?  \n6. Can the authors provide an estimate of how many compute resources or LLM API calls RBRL typically requires compared to a pure RL method, and also compare the runtime needed to perform the same number of update steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZyYJtgDikH", "forum": "ourMOktSsW", "replyto": "ourMOktSsW", "signatures": ["ICLR.cc/2026/Conference/Submission13366/Reviewer_Yuu3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13366/Reviewer_Yuu3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885091803, "cdate": 1761885091803, "tmdate": 1762924010050, "mdate": 1762924010050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to make more explainable RL policies for decision-making environments. For a given state, an LLM is prompted to generated a set of possible rules to guide the choice of action. Then a policy model is trained to select the most appropriate rule, and another LLM call selects the next action given the state and the selected rule. The rule associated with a decision therefore provides an interpretable justification for the action taken.\n\nThe paper is well written and easy to follow, the problem tackled is valuable and the method is interesting. However I have some concerns, mostly regarding the interpretability claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The overall objective of explainable decision-making system is valuable and well motivated\n- Experiments show good results against naive LLM inference, finetuned LLMs and traditional RL agents.\n- Well written paper and easy to follow besides a few presentation issues (see below)"}, "weaknesses": {"value": "Major:\n\n- W1: Describing the rules as a bottleneck is misleading. The selection of the action $a^{env}_t$ done by the LLM in Eq 2 includes the full state description $p_t$. Therefore there is no guarantee that the LLM is actually using the rule to decide which action to take. Moreover several works have noted the risks associated with using CoT rationales to interpret an LLM’s output (see for example [1-2]).\n- W2: For the same reason as the point above, the explanation $l^{expl}$ has no guarantee of being faithful: beyond hallucinations issues, LLMs can produce plausible but spurious explanations [2].\n- W3: l.468, the authors state “explanations follow the State → Rule → Action pipeline, where the rule is the verifiable cause of the action”. Again, this is misleading since the action decision is taken from the LLM with both the state and rule in the prompt, so the causal dependency looks more like “State → Rule, State + Rule → Action”. The actual causal mechanism for the LLM’s action decision is more ambiguous than stated here.\n- W4: Given the rule selection is done by the trainable policy in the embedding space, the choice of the rule to base the action choice on is not interpretable. The LLM might be able to provide explanations post-hoc about why such a rule was relevant given the state, but the actual rule selection done by the policy is essentially a black-box.\n- W5: In the ablation experiments, a relevant baseline would be replacing the RBRL policy by the LLM choosing among the N generated rules, rather than just generating one rule as in `Rule-bottleneck (no RL)`.\n- W6: l.464 “[…] whether an explanation is convincing to humans (validated via our survey, Figure 6)” is overstating - Figure 6 shows that RBRL explanations are merely preferred to CoT explanations.\n- W7: The paper does not include a discussion of limitations, nor a conclusion.\n- W8: The computational load of the method is significantly larger compared to baselines. Even at test-time, the method requires at least 2 LLM queries at each step (one for rules generation, one for action selection given a rule). This is a major increase in computational overhead compared to standard RL agents like PPO, and might limits the applicability of the method in high-frequency settings. The authors are invited to discuss computational costs in the paper.\n\nMinor:\n\n- l.168: the set $\\mathcal{L}$ is not defined anywhere.\n- l.171 states “explanations l^expl are generated from actions and thoughts”, but Eq 2 show the explanation as generated the env action and the rule, while l.294 shows it as generated from environment action, rule, and thoughts. Which one is it?\n- l.427: the text refers to equations (2)-(5) but I believe the authors mean the four equations under Eq 2.\n\n[1] Lanham, Tamera, et al. \"Measuring faithfulness in chain-of-thought reasoning.\" arXiv preprint arXiv:2307.13702 (2023).\n\n[2] Turpin, Miles, et al. \"Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting.\" Advances in Neural Information Processing Systems 36 (2023): 74952-74965."}, "questions": {"value": "- l.315: what is $\\tau_t$ ? I can’t see it defined or explained anywhere.\n- How is it guaranteed, given an LLM, that for any action $a^{env}_t$ there exist a rule $a^{rule}_t$ such that $LLM(a^{rule}_t, p_t) = a^{env}_t $ ? \n\nI might have missed it but this seems like an important assumption for the theoretical guarantees in section 5: if the set of actions accessible in the RBRL MDP $\\tilde{\\mathcal{M}}$ is different from the original MDP $\\mathcal{M}$, then the value of the optimal RBRL policy $\\pi_{RBRL}$ would not be guaranteed to upper-bound the optimal policy $\\pi^*$ under $\\mathcal{M}$ in Theorem 5.3.\n- l.430: What does “RulesLLMOnly” refer to?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZNXkiWt2YK", "forum": "ourMOktSsW", "replyto": "ourMOktSsW", "signatures": ["ICLR.cc/2026/Conference/Submission13366/Reviewer_yoTG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13366/Reviewer_yoTG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997023602, "cdate": 1761997023602, "tmdate": 1762924009100, "mdate": 1762924009100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Rule-Bottleneck RL (RBRL), a framework that combines Large Language Models (LLMs) with Reinforcement Learning (RL) for sequential resource allocation. The key idea is to use a frozen LLM to generate candidate decision rules, while a trainable RL policy learns to select the best rule at each step. This approach jointly optimizes for both task performance and the generation of human-readable explanations, bridging the gap between powerful but opaque Deep RL and interpretable but less effective LLM agents."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The core idea of using a \"rule bottleneck\" to jointly optimize for decision-making and explainability is innovative.\n\n- The method demonstrates competitive or superior performance compared to several baselines (CoT, PPO) in multiple domains."}, "weaknesses": {"value": "While the proposed architecture is conceptually clear, my primary concern is that it appears over-engineered. This complexity may hinder its generalizability to other domains. Furthermore, its heavy reliance on an un-tuned LLM for core components raises significant doubts about the robustness and generalization capability of the RBRL framework. \n\nPlease see my specific questions below for details."}, "questions": {"value": "- The proposed RBRL architecture appears heavily over-engineered, relying on multiple, expensive LLM calls per timestep for rule generation, action selection, and explanation.\n\n-  The method requires multiple LLM calls per step. How slow is it? Please provide actual time costs per step or episode to show it's practical.\n\n- The rules are generated by an un-tuned LLM. How can you guarantee that the set of candidate rules contains good, near-optimal options, especially at the start of training?\n\n- The FinetunePPO baseline is not well-described. Please provide key implementation details.\n\n-  In HeatAlerts and BinPacking, SAC seems to perform worse than the initial policy. Why? Did you perform adequate hyperparameter tuning or longer training for SAC in these environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uKAQpoEuk7", "forum": "ourMOktSsW", "replyto": "ourMOktSsW", "signatures": ["ICLR.cc/2026/Conference/Submission13366/Reviewer_gFYN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13366/Reviewer_gFYN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998020123, "cdate": 1761998020123, "tmdate": 1762924008801, "mdate": 1762924008801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}