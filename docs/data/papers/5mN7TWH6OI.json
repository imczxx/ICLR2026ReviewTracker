{"id": "5mN7TWH6OI", "number": 15679, "cdate": 1758253809033, "mdate": 1759897289520, "content": {"title": "COFormer: Towards a Foundation Model for Solving Combinatorial Optimization Problems", "abstract": "Combinatorial Optimization Problems (COP) encompasses a wide range of real-world scenarios. While learning-based methods have achieved notable success on specialized COPs, the development of a unified architecture capable of solving diverse COPs with a single set of parameters remains an open challenge. In this work, we present COFormer, a novel framework that offers significant gains in both efficiency and practicality. Drawing inspiration from the success of next-token prediction in sequence modeling, we formulate the solution process of each COP as a Markov Decision Process (MDP), convert the resulting sequential trajectories into tokenized sequences, and train a transformer-based model on this data. To mitigate the long sequence lengths inherent in trajectory representations, we introduce a CO-prefix design that compactly encodes static problem features. Furthermore, to handle the heterogeneity between state and action tokens within the MDP, we adopt a three-stage learning strategy: first, a dynamic prediction model is pretrained via imitation learning; this model then serves as the foundation for policy generation and is subsequently fine-tuned using reinforcement learning. Extensive experiments across eight distinct COPs and various scales demonstrate COFormer’s remarkable versatility, emphasizing its ability to generalize to new, unseen problems with minimal fine-tuning, achieving even few-shot or zero-shot performance. Our approach provides a valuable complement to existing neural methods for COPs that focus on optimizing performance for individual problems.", "tldr": "COFormer is a unified transformer-based foundation model that solves diverse combinatorial optimization problems with one architecture and parameter set.", "keywords": ["Foundation Models", "Next-token Prediction", "combinatorial optimizations"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/268508799214f22ad2d8abae1c543a3a5231d809.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces COFormer, a Transformer-based framework designed to solve diverse combinatorial optimization problems (COPs) within a single unified architecture. Drawing inspiration from next-token prediction paradigms (as in large language models), COFormer reformulates COPs as sequential decision processes, representing their trajectories as token sequences. The framework introduces two key ideas:\n- A CO-prefix mechanism that encodes static problem information to reduce sequence length.\n- A three-stage learning scheme combining imitation learning (forward dynamics and policy generation) with reinforcement learning fine-tuning.\n\nThe authors evaluate COFormer on eight distinct COPs (e.g., TSP, VRP, Knapsack, FFSP, 3D Bin Packing), showing strong multi-problem generalization, few-shot transfer, and even some zero-shot capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**\n\n- The paper addresses a key open challenge in neural combinatorial optimization by developing a unified, cross-problem model. This aligns well with the current research trajectory aiming for foundation models for decision-making.\n- The use of a CO-prefix to separate static and dynamic features is elegant and practically motivated, reducing token length and improving efficiency.\n\n**Significance**\n\n- The empirical evaluation spans eight distinct CO problems, including both graph-based (TSP, VRP) and non-graph-based (FFSP, 3DBP) tasks. Considerably broader than most prior “generalist” NCO works.\n- The reported results show strong generalization and competitive performance against specialized baselines. \n\n**Clarity & Quality**\n- The paper communicates its motivation, contributions, and architecture clearly, with well-structured exposition and helpful figures, though technical descriptions, especially regarding tokenization, could be improved.\n- Quality (Strength): The learning process is well motivated and comprehensive"}, "weaknesses": {"value": "**Lacking Clarity**\n\nThe paper lacks a clear and rigorous description of how heterogeneous problem features are represented in the token space. The paper’s description of the tokenization process (Sec. 3.1.2, App. B.1) lacks sufficient clarity to reproduce or fully understand how heterogeneous problem states are unified.\n\nWhile it states that both continuous and discrete values are flattened and mapped to integer token IDs, it is unclear how structural or semantic distinctions (e.g., between coordinates, demands, processing times) are preserved.\n\nThe brief mention of “arbitrary key–value dictionaries” (App. B.1) implies flexible data interfaces but does not specify whether keys are tokenized, whether per-problem schemas are used, or how feature ordering is standardized.\n\nAs a result, it is difficult to assess whether the model genuinely learns a shared latent space across problem types, or whether it relies on per-problem positional regularities hidden in the data pipeline.\n\nThis opacity is problematic for reproducibility and for understanding how COFormer achieves semantic generalization.\n\n\n**Lack of clarity and analysis on semantic representation learning**\n\nThe paper does not clearly explain how COFormer achieves true semantic unification across heterogeneous COPs and would benefit from a deeper analysis of why COFormer can generalize across problems. For instance, are representations for routing and scheduling problems aligned in the same latent space? How sensitive is performance to feature ordering or scaling in the CO-prefix?\n\nIf features are distinguished mainly by positional or problem-specific ordering rather than shared meaning, the model’s generality may be superficial. \n\nNo ablations or visualization of learned embeddings are provided to demonstrate that the learned embeddings capture transferable structure.\n\n\n**Ablation analysis missing for representation choices** \n\nNo experiments isolate the effects of specific design choices (e.g., with/without µ-law scaling, or alternative encoding of continuous features). It’s unclear how sensitive performance is to these details\n\n**Baseline selection in experimental results**\n\nIn the experimental section, results of more recent NCO models (e.g. [1], [2], [3], [4], just to name a few) should be included. Also, comparisons with specialist models seem incomplete, as COFormer often uses sampling while specialists use greedy generation mode. For FFSP for example, MatNet achieves an average makespan of 25.4 on (probably the same) FFSP test instances with sampling enabled. [4] even brings this down to 24.9, giving a performance gap of 14%, which appears very large for such small instances. \n\n**Missing training times**\n\nThe paper claims to improve training efficiency, but does not report wall-clock training times.\n\n----- \n[1] Grinsztajn, N., Furelos-Blanco, D., Surana, S., Bonnet, C., & Barrett, T. (2023). Winner takes it all: Training performant rl populations for combinatorial optimization. Advances in Neural Information Processing Systems, 36, 48485-48509.\n\n[2] Liao, Z., Chen, J., Wang, D., Zhang, Z. &amp; Wang, J.. (2025). BOPO: Neural Combinatorial Optimization via Best-anchored and Objective-guided Preference Optimization. Proceedings of the 42nd International Conference on Machine Learning\n\n[3] Pirnay, J., & Grimm, D. G. (2024). Self-improvement for neural combinatorial optimization: Sample without replacement, but improvement. arXiv preprint arXiv:2403.15180.\n\n[4] Hottung, A., Mahajan, M., & Tierney, K. (2024). PolyNet: Learning diverse solution strategies for neural combinatorial optimization. arXiv preprint arXiv:2402.14048."}, "questions": {"value": "- How exactly is the feature ordering determined when flattening MDP states across problem types? Are keys (feature names) tokenized or embedded, or are they implicit via positional order? Can you provide a concrete example of a serialized sequence (prefix + trajectory) for two distinct problems (e.g., VRP and FFSP)?\n\n- How does the model distinguish between, e.g., a coordinate token and a demand token, if both occupy the same token ID range? How does the model, which problem it is currently solving?\n\n- Did you observe shared latent structure between different problems (e.g., via attention visualization or representation similarity)?\n\n- Can the model generalize to problems with different constraint types not seen during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OcCi3TgfLn", "forum": "5mN7TWH6OI", "replyto": "5mN7TWH6OI", "signatures": ["ICLR.cc/2026/Conference/Submission15679/Reviewer_wAjh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15679/Reviewer_wAjh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760967373714, "cdate": 1760967373714, "tmdate": 1762925932191, "mdate": 1762925932191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **COFormer**, a unified transformer-based framework designed to solve diverse combinatorial optimization problems (COPs) using a single model and shared parameters. Inspired by next-token prediction in large language models, the authors formulate COP solving as a sequential decision process, tokenizing both states and actions into unified trajectories. A **CO-prefix** mechanism is introduced to compactly encode static problem information, and a **three-stage learning scheme** (dynamics imitation, policy imitation, and reinforcement fine-tuning) is proposed to improve training efficiency and generalization. Evaluated on eight distinct COPs (e.g., TSP, VRP, Knapsack, FFSP, 3DBP), COFormer matches or surpasses problem-specific NCO methods and prior generalist baselines, while demonstrating strong few-shot and even zero-shot generalization to unseen problem types."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **\\[S1]** The idea of building a _foundation model_ for combinatorial optimization via tokenization is interesting and, to the best of my knowledge, novel.\n\n- **\\[S2]** The **CO-prefix** mechanism is a clever and effective way to handle static problem information efficiently.\n\n- **\\[S3]** The proposed **multi-stage training pipeline** is clearly structured and modular, improving interpretability and training stability.\n\n- **\\[S4]** The **cross-problem evaluation** is extensive; the benchmark setup itself constitutes a valuable contribution to the community.\n\n- **\\[S5]** The model demonstrates **promising generalization** abilities, including few-shot and even zero-shot adaptation.\n\n- **\\[S6]** The approach exhibits **good scalability**, effectively handling large instances such as TSP1000, supporting its claim as a potential foundation model for COPs."}, "weaknesses": {"value": "See “Questions” below."}, "questions": {"value": "- **\\[Q1]** In COFormer, both states and actions are tokenized and embedded into the same latent space, with their roles distinguished only by positional and separator tokens. While this enables a unified architecture across heterogeneous COPs, it also “blurs” the structural distinction between state and action representations that is central to the underlying MDP formulation. Could the authors elaborate on how this design choice affects learning efficiency and generalization? Specifically, have they compared this unified tokenization scheme to architectures where states and actions are modeled in separate embedding spaces (e.g., distinct encoders or dual-stream attention), which might preserve causal structure and reduce representational ambiguity?\n\n- **\\[Q2]** Many combinatorial optimization problems possess strong structural priors—for example, graphs in routing tasks or bipartite relations in scheduling—that can be compactly and efficiently represented in their native forms. In COFormer, however, these structured instances are flattened into token sequences, which may obscure their relational topology and lead to substantial increases in sequence length. Could the authors discuss how this loss of structural inductive bias impacts model scalability and sample efficiency? Have they considered incorporating lightweight structure-aware modules (e.g., graph or set encoders) that could preserve structural efficiency while maintaining the unified token-based framework?\n\n- **\\[Q3]** The three-stage training pipeline (dynamics imitation → policy imitation → RL fine-tuning) is a key design choice. Could the authors elaborate on how critical each stage is to final performance? For example, what happens if the dynamics stage is skipped, or if RL fine-tuning is applied directly after policy imitation? An ablation or sensitivity analysis would help clarify the necessity of this modular training design.\n\n- **\\[Q4]** Continuous features are discretized via μ-law transformation into 1,800 bins. Could the authors discuss the trade-off between discretization resolution and training stability? In particular, how sensitive is performance to the bin count or to the quantization noise introduced by this process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j7fBTosqRV", "forum": "5mN7TWH6OI", "replyto": "5mN7TWH6OI", "signatures": ["ICLR.cc/2026/Conference/Submission15679/Reviewer_PG5P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15679/Reviewer_PG5P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761035006351, "cdate": 1761035006351, "tmdate": 1762925931841, "mdate": 1762925931841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes COFormer, a unified framework based on the Transformer architecture for solving various combinatorial optimization problems. It introduces two core techniques:\nCO-prefix, which compactly encodes static problem information through prefix token blocks;\nand a three-stage training strategy, which first performs dynamic and policy pretraining via imitation learning, followed by fine-tuning through reinforcement learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.\tThe authors introduce the concept of next-token prediction to handle different combinatorial optimization problems and propose two methods to improve training efficiency. The effectiveness of these methods is validated across 8 combinatorial optimization problems."}, "weaknesses": {"value": "1.\tAlthough the paper claims that its idea is inspired by next-token prediction, it remains unclear how the proposed model fundamentally differs from existing Neural Combinatorial Optimization (NCO) approaches. NCO models also treat nodes as tokens and perform autoregressive next-token prediction using Transformer-like architectures. From the perspective of policy network and training algorithm, the method appears to simply adopt a different Transformer-like model and RL-like algorithm.\n2.\tBased on the above point, the main difference between COFormer and prior NCO methods appears to lie primarily in how problem inputs are processed and tokenized rather than in the core modeling framework. The paper proposes a unified tokenization strategy that converts heterogeneous problem inputs into sequences of tokens, claiming this enables a single model to handle various CO problems. However, this approach seems to be more of a technical workaround than a conceptual breakthrough. In essence, the heterogeneity issue in existing NCO methods arises because different problems have node features with varying dimensions when each node is treated as a token. A straightforward alternative is to flatten all node features into a one-dimensional sequence and regard each scalar feature as a token, which would already standardize the input dimensionality across different problems. From this perspective, the proposed “tokenization of each scalar” resembles such a flattening operation, and the paper does not clearly articulate what essential advantages COFormer introduces beyond NCO methods with this simple re-encoding trick.\n3.\tAlthough COFormer claims to handle diverse combinatorial optimization problems, it remains unclear whether the approach can effectively process problems with edge-level features, such as asymmetric TSP, where the token sequence could become extremely long and computationally expensive.\n4.\tIn the experimental results, COFormer does not show a clear advantage over GOAL [1] in performance. Moreover, several problem settings evaluated in GOAL, such as asymmetric TSP, are not considered in the experiments of this paper, making the claimed generalization ability less convincing.\n5.\tExisting encoder-decoder NCO models have already achieved the effect of encoding static information only once [2], so the core CO-prefix method is a relatively weak innovation.\n6.\tThere is a lack of ablation experiments separating the CO-prefix and each learning stage.\n7.\tIn imitation learning, the input content for the two stages differs for the same network, but the method explanation, including the definition of the state and action, is unclear.\n8.\tThe reinforcement learning training approach has already been extensively validated in the NCO field. Therefore, the conclusion in Section 4.5 — “The ability to improve performance without external supervision highlights the potential of COFormer as a general-purpose solver for a wide range of COPs.” — is not sufficiently supported.\n9.\tThe experimental results for the COFormer RL sampling method are missing.\n10.\tThe few-shot learning capability is only demonstrated on a few routing problems of the same type and does not include the 8 CO problems in the main results.\n\n[1] GOAL: A generalist combinatorial optimization agent learning. ICLR 2025.\n\n[2] MVMoE: Multi-task vehicle routing solver with mixture-of-experts. ICML 2024."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ADrUqVFbID", "forum": "5mN7TWH6OI", "replyto": "5mN7TWH6OI", "signatures": ["ICLR.cc/2026/Conference/Submission15679/Reviewer_LeQb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15679/Reviewer_LeQb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885136649, "cdate": 1761885136649, "tmdate": 1762925931379, "mdate": 1762925931379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces COFormer, a Transformer-based framework designed to tackle diverse combinatorial optimization problems (COPs) within a unified sequence modeling paradigm. The method incorporates three core innovations:\n\n(1) a CO-Prefix representation that encodes static, problem-specific features (e.g., city coordinates or item attributes) into reusable embeddings shared across tasks;\n\n(2) a Hybrid Non-Causal Transformer that applies bidirectional attention to static tokens and causal attention to dynamic state–action sequences, improving inductive bias and efficiency; and\n\n(3) a Three-Stage Training Paradigm comprising a dynamics pretraining stage (supervised learning of environment transitions), a policy generation stage (imitation learning from expert trajectories), and an RL finetuning stage (policy improvement beyond imitation).\n\nEmpirical evaluations across multiple benchmark problems demonstrate the potential of COFormer as a general-purpose neural solver capable of handling various COPs within a single architecture, highlighting its promise toward a unified foundation model for combinatorial optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Unified and Efficient Framework – The proposed CO-Prefix abstraction elegantly decouples static problem representations from dynamic sequences, enabling a single model to generalize across diverse COPs. This is a step toward truly general-purpose neural optimizers.\n\n2) Three-stage training paradigm – The proposed Dynamics Forward → Policy Generation → RL Finetuning pipeline is conceptually well-motivated. It mirrors the classical model-based RL structure — learning environment dynamics first, then imitation, and finally reinforcement fine-tuning.\n\n3) Promising empirical direction – The reported results show that COFormer can outperform prior methods on several small-scale COPs, suggesting potential for broader applicability."}, "weaknesses": {"value": "1) Lack of empirical validation for key components\nThe Dynamics Forward Stage—one of the paper’s most central contributions—is not ablated or analyzed in isolation.\nWithout direct comparisons (e.g., COFormer without dynamics vs. full COFormer), it remains unclear whether this stage truly improves optimization performance or merely stabilizes training. This omission significantly weakens the empirical credibility of the claimed contributions.\n\n2) Limited analysis of generalization in the Few-Shot Ability section\nIt remains unclear how the model’s few-shot capability extends beyond similar routing domains—such as to packing (3DBP) or scheduling (FFSP)—especially given that the proposed Dynamics Forward Stage should, in theory, facilitate more transferable representations across different COP families.\n\n3) Limited experimental coverage and baseline clarity\nTable 1 does not include RL fine-tuning results for 3DBP and FFSP, and the reason for this omission is not clearly explained. Additionally, it would be helpful to clarify why MatNet and PCT were not trained with RL to ensure fair and consistent comparison across methods."}, "questions": {"value": "1. Could the authors provide a quantitative ablation isolating the contribution of the Dynamics Forward Stage? What specific improvements (if any) does this stage offer in convergence speed or final solution quality?\n\n2. Has the model demonstrated cross-task transfer, e.g., training on routing and testing on scheduling or packing problems?\n\n3. Why were RL fine-tuning experiments for 3DBP and FFSP omitted, and why are some baselines trained differently? Were there technical limitations or theoretical reasons?\n\n4. From Table 5, COFormer-direct-greedy and COFormer-RL-greedy exhibit only marginal differences, which raises doubts about whether the current training paradigm scales effectively to larger instances. Could the authors clarify whether the proposed framework remains computationally and performance-wise effective as problem size grows, and also provide more evidence on how it performs across different tasks such as CVRP and FFSP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OSukPzw0Yq", "forum": "5mN7TWH6OI", "replyto": "5mN7TWH6OI", "signatures": ["ICLR.cc/2026/Conference/Submission15679/Reviewer_BUAW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15679/Reviewer_BUAW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978514120, "cdate": 1761978514120, "tmdate": 1762925930980, "mdate": 1762925930980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}