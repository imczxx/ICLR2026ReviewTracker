{"id": "HaQCWrbP5Z", "number": 13842, "cdate": 1758223546764, "mdate": 1763465002100, "content": {"title": "Leave one Expert Out: Robust Uncertainty Quantification via Intrinsic Cross-Validation", "abstract": "Estimating epistemic uncertainty remains an important challenge in modern Deep Learning (DL). We propose a novel architecture, called Leave one Expert Out (LEO), which is a form of a mixture-of-experts model with latent-space-distance-aware router and a null expert, representing prior belief, to which output of the model collapses if testing datapoint is too different from any of datapoints experts were trained on. This architecture allows to temporarily drop experts from the model, and we utilise this property to train the router to leverage the predictions of remaining experts to make predictions for the datapoints normally assigned to the expert currently removed from the model. We coin this mechanism \\textit{intrinsic cross-validation} and show, such a trained router excels at estimating epistemic uncertainty for both in and out of distribution inputs. We demonstrate state-of-art performance on uncertainty quantification in regression benchmarks, such as UCI problems or age prediction on UTK-Face, and CIFAR10 classification benchmark. We also show the proposed method can achieve superior performance in surrogate-based black-box optimization.", "tldr": "", "keywords": ["deep learning", "uncertainty quantification", "mixture of experts"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e01cec4eeba9d62b4be0619719330a1922d61b80.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces LEO, an architecture and training procedure for enabling models that quantify their epistemic uncertainty. It uses a mixture of shallow experts and a router that assigns examples to experts. The method uses a training procedure that simulates OOD scenarios during training to force the router to fall back on a prior distribution for OOD samples. This design ensures the model remains comparable in size to standard architectures, unlike resource-intensive methods like deep ensembles, which require multiple full models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is interesting and tackles the important problem of epistemic uncertainty quantification in (single) neural networks. The \"intrinsic cross-validation\" is compelling."}, "weaknesses": {"value": "- The writing is very procedural and lacks motivation and structure. For example, Section 2.2 does not mention that the purpose of the leave-one-expert-out (actually leave multiple experts out!) is to simulate OOD scenarios during training.\n- Why randomly corrupted samples in the CIFAR-10 experiments? Why not use e.g. SVHN as Van Amersfoort et al. 2020 does? Did you only evaluate on covariate shift, not on semantic shift?\n- Missing citations for claims in the introduction. Examples:\n-- Line 34 \"assessing the certainty of that prediction remains a notoriously difficult problem\" → it all depends on the context. A well calibrated model is very good at estimating the (aleatoric) uncertainty?\n-- Line 45: missing citation for aleatoric and epistemic uncertainty\n-- Line 67: \"The training process typically does not explicitly encourage the model to output high uncertainty in OoD cases.\" DeepEnsembles try to enforce diversity between the members (data shuffles, different parameter initialization, ...), resulting in dissagreement between the members for OoD inputs.\n-- Line 256: No uses of MAE anywhere\n-- Table 1 is not referenced in the text\n-- Line 299: Are there more recent references to mixture-of-experts literature also related to uncertainty quantification?"}, "questions": {"value": "- The router is an OOD detector: it is the sole mechanism that pushes the predicted distribution to the prior. Why is there not an experiment that measures how well p_ϕ(t | x) is able to distinguish OOD from ID? How is the performance of the router affected by the choice of prior? What if the prior does not minimize the loss? In that case, the main mechanism behind LEO explained on line 258 saying that the router collapses to the \"vague\" prior would not stand?\n- Unclear experimental details and lacking insights into and discussion of the results. What do you take the NLL of? Of the probability of Eq 1? How should OOD NLL in Table 2 be interpreted? Is the goal to make the likelihood of the OOD samples high? It is not clear how these numbers show that the model's epistemic uncertainty estimation is good.\n- The multiple small \"experts\" are remeniscent of \"ShallowEnsembles\" proposed by [1] and benchmarked in [2]. What if you just average the experts' predictions? Is that actually what the router does? How different are the predictions of the experts? The only purpose of the experts is to allow the simulated OOD scenarios during training. Would you agree?\n\n[1]: Lee et al. 2015 https://arxiv.org/abs/1511.06314\n[2]: Mucsanyi et al. NeurIPS 2024 https://openreview.net/forum?id=x8RgF2xQTj\n[3]: Durasov et al. CVPR'21 https://openaccess.thecvf.com/content/CVPR2021/html/Durasov_Masksembles_for_Uncertainty_Estimation_CVPR_2021_paper.html\n[4]: Laurent et al. ICLR'23 https://openreview.net/forum?id=XXTyv1zD9zD"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FfLxBEuvhb", "forum": "HaQCWrbP5Z", "replyto": "HaQCWrbP5Z", "signatures": ["ICLR.cc/2026/Conference/Submission13842/Reviewer_MYNx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13842/Reviewer_MYNx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948655460, "cdate": 1761948655460, "tmdate": 1762924365247, "mdate": 1762924365247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method called “Leave on Expert Out” (LEO) which takes a pretrained backbone network $f: \\mathcal{X} \\to \\mathbb{R}^d$ and builds a mixture-of-experts model on-top of it. The paper claims that LEO often produces a mixture-of-experts model that has both good in-distribution prediction performance and relatively good (when compared to other methods) out-of-distribution prediction performance and calibration.\n\nLEO works by training a set of expert prediction heads $h_1, \\ldots , h_T: \\mathbb{R}^d \\to \\mathcal{Y}$ on top of $f$, along with a router $r: \\mathbb{R}^d \\to \\Delta^{T + 1}$ that weights the predictions of the experts (with one extra coordinate for a user-defined fallback prediction). Inference on this backbone works in the following way:\n\n1. Given an input $x$, an output prediction $\\hat{y}_t = h_t(f(x))$ is generated for each expert $h_t$. These predictions are then converted to output distributions $\\delta_1, \\ldots, \\delta_T$. When doing regression, a scalar prediction $\\hat{y}_t$ is converted into a dirac delta function centered at that prediction. When doing classification, predicted logits $\\hat{y}_t$ are softmaxed to get a distribution over classes.\n\n2. The final prediction distribution is a mixture of the $T$ expert output distributions along with a single user-specified prior probability distribution $\\delta_0$. The weights of the mixture are given by $r(x)$.\n\n   When mixing together dirac delta distributions, in order to preserve differentiability, the output distribution is approximated by a Gaussian whose moments match the true mixture distribution.\n\nLEO training works in three stages:\n\n1. First, each training datapoint is passed through a randomly initialized backbone network to produce a random embedding. These embeddings are then projected onto a random one-dimensional subspace (the same subspace is used for all embeddings), and the datapoints are split up into T contiguous buckets along this subspace, with each contiguous bucket having the same fraction of datapoints assigned to it.\n\n2. T expert heads are created, one for each bucket. Expert-head $t$ is trained to perform well (when attached to the frozen backbone network $f$) at prediction on the $t$th bucket of data. \n\n3. Finally, the router $r$ is trained on two objectives simultaneously: Firstly, the mixture-of-experts model should perform well at in-distribution prediction. Secondly, the mixture-of-experts model should perform well on predicting datapoint $x$’s label, even when the expert corresponding to $x$’s bucket is dropped along with some other random set of experts. When dropping an expert, its output distribution is replaced by the user-specified prior distribution.\n\n   The paper term the mechanism behind this second loss term “intrinsic cross-validation”.\n\nThe paper tests LEO in three settings: regression, classification, and performance at black-box bayesian optimization of the Ackley function when LEO is used as a surrogate model to inform UCB what point to sample next. On regression and classification, LEO is comparable to the best performing baseline methods. On optimization of the Ackley function, LEO outperforms baseline methods when optimizing the Ackley function."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1: The LEO algorithm is quite interesting and was novel for me (though admittedly I am somewhat unfamiliar with methods that optimize for good calibration / OOD performance beyond the basics like ensembles). In particular, I liked how the paper was able to utilize a mixture of experts' approach, how they designed their router algorithm which naturally places weight on the prior for really anomalous embeddings, and their intrinsic cross-validation scheme.\n\nS2: I liked that the paper linked to a copy of its codebase at https://anonymous.4open.science/r/leave-one-expert-out-DF01/. Reading through this codebase helped me understand the method and experiments better."}, "weaknesses": {"value": "W1: The evaluations of LEO are all on very toy tasks and none of them involve large scale foundation models. Moreover, the evaluations that are run do not show LEO to be leading other methods by that large of a margin, instead LEO seems just to be among the set of best performing methods.\n\nW2: On the theoretical front, the paper does not really make an attempt to show why LEO should in theory perform better than existing baseline methods. The paper comments on some basic properties of LEO (e.g. lines 166-170), but does not compare its theoretical underpinnings to other methods.\n\nW3: The random-network-embedding-projection type-assignments seem quite arbitrary and not well motivated. I would be curious to compare this approach to some ablations like k-means clustering on trained embeddings, or just completely random assignments."}, "questions": {"value": "Question #1: Regarding weakness W3, what is the motivation behind this approach? Alternatively, could some ablation experiments be run to provide empirical evidence regarding its effect?\n\nSugggestion #1: The authors could address weakness W1 by running experiments with LEO on large scale foundation models. I would be most interested to see experiments on making foundation models more calibrated at very hard tasks that foundation models only sometimes get right. If LEO beats out baselines here, I would be convinced it is a very good method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d0lYgM8Znb", "forum": "HaQCWrbP5Z", "replyto": "HaQCWrbP5Z", "signatures": ["ICLR.cc/2026/Conference/Submission13842/Reviewer_WJAd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13842/Reviewer_WJAd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971240529, "cdate": 1761971240529, "tmdate": 1762924364726, "mdate": 1762924364726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes “Leave one Expert Out” (LEO) for uncertainty quantification in deep learning. LEO has a mixture of experts architecture, where the router uses the latent space representation of input to understand its distance from the training data to weight the expert predictions. It employs a null expert to collapse the model output to a prior if the input is different from the training data. The model is trained using a new mechanism called intrinsic cross-validation (ICV). ICV drops one or more experts during training, simulating out-of-distribution cases, which results in epistemic uncertainty quantification."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is well written and organized.\n- The proposed method is novel.\n- The proposed method is well motivated with connections to well established theory from Gaussian Processes and Bayesian methods.\n- Uncertainty quantification is an important field for safety critical applications of deep learning.\n- The empirical results are compelling, providing advancements on state-of-the-art in uncertainty quantification."}, "weaknesses": {"value": "- The empirical results are limited:\n   - Ablations on main components like ICV, null expert, different distance metrics are needed to understand the contributions of each.\n   - The router has hyperparameters and it would be good to showcase the sensitivity of results to these parameters. Similarly, sensitivity on the number of experts is missing.\n   - The initial “type” assessment mentioned in the appendix B1 seems random and might be subject to variability. It would be good to provide experiments showing how choices here affect the results.\n- The experiments are conducted at small scales and for certain tasks. Applying the method to text domains and LLMs would strengthen the paper."}, "questions": {"value": "I mention my main questions and concerns in the weakness section. Some minor points:\n- Figure 2b. Most of the methods seems to coincide and it is hard to understand the individual performance of the methods. Is LEO’s CI’s for OOD include ensemble's CI for OOD? If so, I’d love to hear author’s interpretation of the superior performance of ensemble method for both ID and OOD.\n- Table 4: \n   - I’d recommend adding the deterministic model’s performance numbers for comparison. \n   - Is it possible to provide a similar table for training requirements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Q9ui0UQPx", "forum": "HaQCWrbP5Z", "replyto": "HaQCWrbP5Z", "signatures": ["ICLR.cc/2026/Conference/Submission13842/Reviewer_r2SE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13842/Reviewer_r2SE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762107895996, "cdate": 1762107895996, "tmdate": 1762924364172, "mdate": 1762924364172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a single model approach to epistemic uncertainty estimation. The training samples are partitioned into disjoint sets, and during training each set is assigned to a specific expert (linear head); all experts share the same feature extractor. Some key steps are the inclusion of a null/prior expert, the training of a router module which learns the effect of expert exposure to unknown samples, and the overall two-phase training procedure denoted as intrinsic cross-validation (ICV). The system thus learns a supervised OoD signal from the training data alone, while remaining within a single-model framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- the ICV mechanism tweaks elegantly the mixture-of-experts paradigm in order to create OoD scenarios within the training set in a supervised manner (i.e., by increasing the probability of the null expert); this contrasts with methods like DUQ which rely on latent distance in an implicit manner\n\n- low memory footprint and good inference speed\n\n- the method is validated on three distinct UQ domains, with a very good performance on the BO experiment which matches/outperforms in high dimensions the GP"}, "weaknesses": {"value": "- the data partitioning strategy is badly justified, and seems arbitrary and plainly wrong with respect to the intended assumption : that the experts are specialists on *distinct* data partitions. The strategy passes samples across a freshly initialized, untrained data extractor and projects embeddings on a random vector. There is no semantic meaning whatsoever, this seems to me like a random vector projection of a random space projection, highly unlikely to create semantically coherent factors. The paper claims this creates a mismatch across experts' training distributions, but it is an arbitrary mismatch. The fact the method works well in these circumstances raises a critical question, namely is the method's strong performance a result of this very high variance partitioning setup, and if yes, why? Since it goes in my opinion against the distinct data partition assumption. Overall, this part is critical and is awfully under studied for the moment.\n\n- an ablation focusing on the number of experts is necessary; the considered number (five) which matches the number of ensemble models / droupout samples is reasonable for comparison purposes, but it is in no way a methodologically sound justification. The study is crucial since the two pitfalls I can foresee are significant : too few experts and the OoD signal becomes weak, too many experts and the method might exhibit underfitting and poor generalization. \n\n- while convenient, the two step training introduces a limitation; the latent space is good for the expert loss (CE) but it is never trained to produce a latent space useful for the router's task which uses a L2 metric in the aforementioned space.  Why dismiss  end-to-end training completely? The justification that it is this \"much faster\" is of convenience, and should be underlined rather as a limitation, and justified better following a comparison with end-to-end training\n\n- the claims of performance against SOTA are overstated and should be toned down (e.g. Figure2 Ensemble on ID NLL, ECE and DUQ/EDL on OoD NLL). LEO is good at both which places it conveniently on the Pareto front, but it is a trade-off, it does not blow away the competition."}, "questions": {"value": "- justify formally the link between the data partitioning strategy and the claimed emergence of distinct data partitions \n\n- a characterization of the choice of the number of experts which would sustain the emergence of the OoD signal\n\n- a justification for the choice of discarding the option of end-to-end training"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "nothing in particular"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r7HHSRSZQE", "forum": "HaQCWrbP5Z", "replyto": "HaQCWrbP5Z", "signatures": ["ICLR.cc/2026/Conference/Submission13842/Reviewer_orKS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13842/Reviewer_orKS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762377839343, "cdate": 1762377839343, "tmdate": 1762924363841, "mdate": 1762924363841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We would like to thank all the reviewers for spending time to read through our submission and provide useful feedback. We have now updated the submission and added Appendices I - N at the end of the document. These include the requested ablation, as well as a new Figure explaining the intuition behind the proposed data partitioning strategy. We would be most grateful to hear back from the reviewers before the end of discussion period."}}, "id": "EkDE7p6vKU", "forum": "HaQCWrbP5Z", "replyto": "HaQCWrbP5Z", "signatures": ["ICLR.cc/2026/Conference/Submission13842/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13842/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission13842/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763465854870, "cdate": 1763465854870, "tmdate": 1763465854870, "mdate": 1763465854870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}