{"id": "yngvAamNQi", "number": 15416, "cdate": 1758251137466, "mdate": 1759897308449, "content": {"title": "From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation", "abstract": "Achieving generalization in robotic manipulation remains a critical challenge, particularly for unseen scenarios and novel tasks. Current Vision-Language-Action (VLA) models, while building on top of general Vision-Language Models (VLMs), still fall short of achieving robust zero-shot performance due to the scarcity and heterogeneity prevalent in embodied datasets. To address these limitations, we propose FSD (From Seeing to Doing), a novel vision-language model that generates intermediate representations through spatial relationship reasoning, providing fine-grained guidance for robotic manipulation. Our approach combines a hierarchical data construction pipeline for training with a self-consistency mechanism that aligns spatial coordinates with visual signals. Through extensive experiments, we comprehensively validated FSD’s capabilities in both “seeing” and “doing”, achieving outstanding performance across 8 benchmarks for general spatial reasoning and embodied reference abilities, as well as on our proposed more challenging benchmark VABench. We also verified zero-shot capabilities in robot manipulation, demonstrating significant performance improvements over baseline methods in both SimplerEnv and real robot settings. Experimental results show that FSD achieves 40.6% success rate in SimplerEnv and 72% success rate across 8 real-world tasks, outperforming the strongest baseline by 30%.", "tldr": "We proposed FSD, which enhances the generalization capability of robotic manipulation by enhancing spatial visual reasoning.", "keywords": ["Spatial VLMs", "General Robotic Manipulation", "VLM Reasoning", "Spatial Chain-of-Thought"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e2324b159489a8c70b04660d258e7a1e272cd786.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces FSD, a vision-language model designed to bridge visual reasoning and robotic decision-making. FSD employs a Spatial Relationship-focused Chain-of-Thought (SrCoT) mechanism to generate intermediate visual representations (e.g., visual points and visual traces), enabling a structured transition from perception to action. It further integrates a hierarchical weak-to-strong data pipeline and a self-consistency alignment mechanism to enhance relation understanding and generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. Using visual prompts for zero-shot manipulation is a promising research direction.\n\nS2. The benchmark proposed by the authors can help the robotics community evaluate the generation and reasoning capabilities of VLM models."}, "weaknesses": {"value": "W1. Generating such a large number of visual prompts may affect inference efficiency and potentially obscure critical information. Moreover, I am not convinced that this model can achieve faster inference than end-to-end VLA models. The authors are encouraged to compare execution efficiency with Pi_0.\n\nW2. How does the method address the closed-loop problem, for example, when the scene dynamically changes after generating visual prompts? Would redoing all prompt reasoning in such cases be too inefficient?\n\nW3. Regarding the automated dataset construction, does it only build image-level planar relationships? Could the authors consider incorporating spatial relationships in the 3D Cartesian coordinate system?\n\nW4. The end-to-end VLA scores reported on SimplerEnv (WidowX Robot) appear to be taken from SpatialVLA (Qu et al., 2025). It is recommended that the authors reproduce existing VLA models for fair comparison, include more recent VLAs, and report fine-tuned results as well.\n\nW5. Although multiple visual and language prompts are generated, the accuracy does not reach the current SOTA. The authors should compare FSD with fine-tuned end-to-end VLAs, since such models are trained through imitation learning rather than a purely zero-shot paradigm."}, "questions": {"value": "Q1. It is recommended that the authors explore more challenging tasks, such as articulated object manipulation or contact-rich manipulation scenarios.\n\n\nI think this paper is currently borderline, but I’m willing to give it a borderline accept, provided that the authors address my concerns. If there is any misunderstanding on my review, I would appreciate it if the authors could clarify."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sC5cOfufXm", "forum": "yngvAamNQi", "replyto": "yngvAamNQi", "signatures": ["ICLR.cc/2026/Conference/Submission15416/Reviewer_Kh3L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15416/Reviewer_Kh3L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884804020, "cdate": 1761884804020, "tmdate": 1762925692975, "mdate": 1762925692975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FSD, a VLM-based framework designed to generate visual aids for robotic tasks. These aids—spatial affordance boxes/points and object-centric visual traces—are produced via a Spatial-relationship focused Chain-of-Thought (SrCoT) and a self-consistency alignment that binds coordinates to visual references. The model is trained using a hierarchical, weak-to-strong data construction pipeline, creating 300k SFT samples from multiple robot datasets to teach five distinct capability levels (grounding → spatial relations → spatial reasoning → affordances → traces). Experiments report strong results on five spatial-reasoning benchmarks, good object/free-space referencing, high accuracy on the new VABench for visual aids, and successful zero-shot manipulation in SimplerEnv and on an xArm across eight real-world tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear Problem Decomposition: The paper's primary strength is its clean decomposition of the problem: reason first, then produces compact, embodiment-agnostic visual aids (boxes, points, traces). This abstraction (§3.1) translates well to zero-shot action by simplifying the downstream tasks of 2D→3D back-projection and motion planning.\n\nMethodological Novelty: The Spatial-relationship focused Chain-of-Thought (SrCoT) is a novel and promising approach. By anchoring reasoning on spatial relationship graphs with strict object and coordinate bindings (<ref>, <box>, <point>), the method plausibly reduces hallucination and improves grounding—a common failure point for VLMs.\n\nSystematic Data Pipeline: The development of an automatic, multi-source 300k SFT dataset is a significant engineering contribution. The five-level capability curriculum, which uses external geometry tools (Metric3D v2, WildCamera) and rule-based filters, is well-conceived. Ablations effectively demonstrate that both the SrCoT and the alignment components contribute to performance.\n\nComprehensive Evaluation: The paper is thoroughly evaluated. FSD achieves state-of-the-art or near-SOTA results across a wide range of benchmarks (CVBench, CRPE, SAT, BLINK, EmbSpatial) and shows strong performance on grounding tasks (RoboRefIt, Where2Place). The large margins on the newly proposed VABench, combined with a 72% success rate on 8 real tasks, provide compelling evidence for the method's efficacy."}, "weaknesses": {"value": "Incomplete Compute and Latency Attribution: The paper makes claims about deployability but fails to provide a rigorous analysis of its computational cost. A single small latency table is insufficient. The review lacks a module-level breakdown of FLOPs, parameters, and wall-clock time for essential components (projector, LLM tokens, SrCoT decoding, visual-aid generation, 2D→3D lift, and planning). Without this, it is impossible to judge the method's cost-benefit trade-off against modular or end-to-end VLAs under matched hardware and batching conditions.\n\nUnexplored 2D Trace Limitations: The core robotic primitive—a 2D trace lifted to 3D via depth and pinhole back-projection—is sensitive to error. The paper does not quantify how errors in camera calibration (focal length, principal point), depth estimation noise, or occlusions accumulate and affect the final 3D trajectory. While 2D limitations are acknowledged, the lack of robustness studies is a significant gap.\n\nUnknown Data Quality and Bias: The 300k auto-constructed dataset is a potential source of unmeasured error. The paper does not report label noise rates, inter-annotator agreement for the 300 manual VABench items, or how errors from the data pipeline (eg, the 20% relative depth gap filter) propagate to the quality of the final SrCoT reasoning.\n\nLimited Scope of Robotic Interaction: The evaluation, while broad, is shallow in its robotic scope. Most executions appear to be open-loop, with closed-loop feedback mentioned but not rigorously evaluated. There is no analysis of drift, re-planning, or recovery from grasp failures. Furthermore, the tasks do not explore multi-step long-horizon sequences, heavy clutter, moving distractors, or significant camera pose changes, all of which are critical for real-world autonomy.\n\nWeak Justification for Design Choices: Several key hyperparameters are fixed without ablation (eg, T=8 trace points). The evaluation for visual aids on VABench relies on a mix of metrics (MAE/RMSE) and an unvalidated LLM-score proxy."}, "questions": {"value": "Cost Attribution and Scaling Analysis: Provide a detailed table with parameters, FLOPs, and p50/p90/p99 latencies per module (ViT, projector, SrCoT, aid decoding, planning). This analysis should be benchmarked on the same hardware used for comparisons (eg, vs. OpenVLA/MOKA) and show throughput vs. image resolution and token budget.\n\nGeometric Robustness Stress Test: Conduct simulation experiments that quantify the drop in task success rate against applied noise. This must include sensitivity analysis for: (i) camera intrinsic perturbations (±Δf, ±Δcx,cy), (ii) synthetic depth noise, and (iii) mis-calibration. Includes a qualitative gallery of failure cases where 2D traces misproject.\n\nData Quality Audit: Report spot-check precision/recall of the auto-generated grounding and relation labels against a manual ground truth. For VABench, report the inter-annotator agreement (eg, Cohen's Kappa or Fleiss' Kappa) to validate the 300 manual items. Release data filtering scripts for reproducibility.\n\nClosed-Loop and Granularity Ablations:\n\nImplement a simple closed-loop variant (eg, visual servoing or re-planning at the next waypoint) and compare its success and correction frequency against the open-loop baseline.\n\nJustify the choice of T=8 waypoints by ablating performance (VABench and SimplerEnv) vs. T ∈ {4, 16} and an adaptive T.\n\nHuman-in-the-Loop Evaluation: Strengthen the VABench claims by complementing the LLM-score proxy with a human preference study (eg, ≥300 pairwise prompts) on visual trace plausibility, reporting 95% confidence intervals."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9zu1fHuVv6", "forum": "yngvAamNQi", "replyto": "yngvAamNQi", "signatures": ["ICLR.cc/2026/Conference/Submission15416/Reviewer_vmwj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15416/Reviewer_vmwj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928559057, "cdate": 1761928559057, "tmdate": 1762925692301, "mdate": 1762925692301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FSD, a vision-language model that provides intermediate visual representations for robotic manipulation, such as affordance regions and visual traces. It presents detailed information on how to construct the dataset and train the model. Extensive experiments demonstrate the strong performance of FSD across multiple benchmarks, as well as its zero-shot open-loop capabilities. The model, along with its detailed data generation pipeline, dataset, and training strategy, offers valuable contributions for general robotic manipulation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a large dataset with visual aids, which is essential for robotic manipulation tasks. The data generation pipeline is detailed and well-designed. The experiments are extensive, and the model demonstrates strong performance.\n2. The paper proposes weak-to-strong training and a self-consistency alignment strategy, both of which are effective for model performance.\n3. The evaluations on real-world manipulation tasks and simulations are thorough, and the performance is strong. Both open-loop and closed-loop settings are tested."}, "weaknesses": {"value": "1. Inference time may become a bottleneck for real-world applications.\n2. The open-loop inference pipeline relies on complex and specifically designed submodules such as GraspNet and depth estimation. The overall success rate is influenced by compounded errors from both FSD and the downstream modules. \n3. This paper presents promising results on closed-loop robotic manipulation in Appendix K, where visual aids are generated by FSD at the beginning of the task. It would be beneficial to update these visual aids every few steps to make the entire pipeline more practical and effective in real-world scenarios."}, "questions": {"value": "1. In Table 3b, the DINOv2 predictor is compared. How many parameters of this model? Is it the same model size as FSD?\n3. In Table 3, the results show that self-consistency alignment helps improve performance. Do you have any failure cases without alignment and success cases with alignment, so that we can clearly see the effectiveness of the self-consistency alignment?\n4. How long does it take to generate the dataset?\n5. Table 5 shows the execution time. Is this time for the whole task execution? What is the FSD inference time? OpenVLA is the closed loop model, which call the model constantly. What is the time cost for each component in your pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aMrE2tuMjC", "forum": "yngvAamNQi", "replyto": "yngvAamNQi", "signatures": ["ICLR.cc/2026/Conference/Submission15416/Reviewer_ezj7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15416/Reviewer_ezj7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939502919, "cdate": 1761939502919, "tmdate": 1762925691547, "mdate": 1762925691547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presented FSD (From Seeing to Doing), a framework that connects visual reasoning with robotic manipulation using intermediate spatial representations. FSD tackles key challenges of data scarcity and heterogeneity through three core innovations: a Spatial Relationship-Focused Visual Chain-of-Thought for multi-step reasoning, a hierarchical weak-to-strong data pipeline, and a self-consistency mechanism that aligns spatial coordinates with visual signals. Experiments demonstrate FSD’s effectivenss across multiple spatial reasoning and visual aid benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear, by identifying limitations of existing VLA approaches, i.e., data scarcity, and heterogeneity, and a principled solution is proposed through object-centric intermediate representations of visual aids.\n\n2. Extensive experiments across 8 benchmarks for spatial reasoning, plus zero-shot manipulation in both simulation and real-world settings demonstrate thorough validation."}, "weaknesses": {"value": "1. The approach generates 2D trajectories that must be lifted to 3D using depth cameras. This is a significant limitation acknowledged by authors but I still have some questions:  \nA.\tThe depth model needs to predict the depth of 2D trace, which is tricky in my understanding, as this kind of trace could be out of distribution for the depth model. How robust / accurate is the depth estimation?\nB.\tHow does a 2D trace avoid the collilsion, as there is uncertainty when it’s unprojected to 3D? \n\n2. Ablation study of parameters: No ablation on the number of hierarchy levels. No comparison of different VLM backbones. No study on how performance changes with trajectory point density\n3. The model is built based on HVicuna-13B and CLIP-ViT-L, how about the performance of a naïve VLA model implementation, without visual aid? This can give us an insight about the improvement with visual aid.\n4. It uses GPT-4o to generate reasoning chains for training, then evaluates using GPT-4o for scoring (VABench GPT-score), which potentially leads the model to overfit to GPT-4o's biases."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v0JfmOb0pV", "forum": "yngvAamNQi", "replyto": "yngvAamNQi", "signatures": ["ICLR.cc/2026/Conference/Submission15416/Reviewer_aF4E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15416/Reviewer_aF4E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762357078790, "cdate": 1762357078790, "tmdate": 1762925690818, "mdate": 1762925690818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}