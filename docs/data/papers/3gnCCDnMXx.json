{"id": "3gnCCDnMXx", "number": 10414, "cdate": 1758170742323, "mdate": 1759897652185, "content": {"title": "Corrigibility Transformation: Constructing Goals That Accept Updates", "abstract": "For an AI's training process to successfully impart a desired goal, it is important that the AI does not attempt to resist the training. However, partially learned goals will often incentivize an AI to avoid further goal updates, as most goals are better achieved by an AI continuing to pursue them. We say that a goal is corrigible if it does not incentivize taking actions that avoid proper goal updates or shutdown. In addition to convergence in training, corrigibility also allows for correcting mistakes and changes in human preferences, which makes it a crucial safety property. Despite this, the existing literature does not include specifications for goals that are both corrigible and competitive with non-corrigible alternatives. We provide a formal definition for corrigibility, then introduce a transformation that constructs a corrigible version of any goal that can be made corrigible, without sacrificing performance. This is done by myopically eliciting predictions of reward conditional on costlessly preventing updates, which then also determine the reward when updates are accepted. The transformation can be modified to recursively extend corrigibility to any new agents created by corrigible agents, and to prevent agents from deliberately modifying their goals. Two gridworld experiments demonstrate that these corrigible goals can be learned effectively, and that they lead to the desired behavior.", "tldr": "Accepting goal updates can be incentivized by having agents optimize for predicted reward conditional on rejecting updates", "keywords": ["reinforcement learning", "ai alignment", "corrigibility", "game theory", "learning theory"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed2cf3eae9fc1f519845ede17d9276de278139b7.pdf", "supplementary_material": "/attachment/7b019b8a13b8461656c37bb6fd54058c05dd7459.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to address the problem of AI systems resisting goal updates, including shutdown. The authors propose a transformation to an agents goal to make the agent “corrigible”, i.e., to ensure that it does not resist goal updates. Their method leverages the insight that (1) an agent optimizing for a myopic objective is less likely to be incentivized to resist goal updates (2) by giving an agent the ability to resist goal updates (i.e., expanding the action space) a principal can avoid training agents that resists goal updates via harder to detect behaviors down the line (3) a principal can assign a small bonus to accepting goal updates to encourage corrigibility. The authors provide some theoretical and empirical analysis showing that their method avoids training agents to avoid goal updates"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is interesting and novel; it also addresses an important problem that seems relatively under explored."}, "weaknesses": {"value": "The paper is quite difficult to follow. Part of this is due to formatting (e.g., removing all paragraph breaks), as well as being overly verbose in parts and generally quite informal. I would suggest that the authors rewrite the paper because admittedly in its current state it is difficult to parse their ideas. \n\nAdditionally, the empirical evaluation is insufficient for a conference like ICLR; I suggest that the authors evaluate their method either more extensively on a larger set of grid-worlds if computational tractability is an issue, or preferably on more relevant domains for the alignment community (including but not necessarily LLM settings)."}, "questions": {"value": "How does the proposed approach perform on other environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lrvWiphvJ5", "forum": "3gnCCDnMXx", "replyto": "3gnCCDnMXx", "signatures": ["ICLR.cc/2026/Conference/Submission10414/Reviewer_4i9R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10414/Reviewer_4i9R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772581011, "cdate": 1761772581011, "tmdate": 1762921724098, "mdate": 1762921724098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an approach to transform goals into the so-called \"corrigible\" ones, which means that they do not resist correction.  \nWhen a goal is changed through \"proper\" channels, the system is supposed to follow the new goal including a shut down command, but get around it when it is not changed through proper channels. The proofs and the empirical results show that the transformations achieve the desired behavior."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The problem of corrigibility is identified as important in the AI safety literature. This paper seems to be a good first attempt to define it precisely and putting forward a first solution."}, "weaknesses": {"value": "Although the problem as posed seems to be solved, I fail to see the significance of the result. The main problem is to detect when the new goal is a legitimate correction vs. an error or an adversarial attack. That problem is not touched by the solution here, which assumes that the system can perceptually discriminate the two types of requests and all that is needed is to follow the legitimate request and ignore the others. \n\nThe paper is  not well written, making it really difficult to understand. Theorems and conditions are stated without proper explanations of why they are there, what they mean and what they entail. \n\nFor example, what is condition 1 saying? Is it saying that the state dynamics does not depend on the goals? When is it not the case?\nI don't understand the role of integral here over G'. This needs to be more carefully explained. The second part also does not make sense because G_1 and G_2 are two arbitrary goals. Why should P(S_G' |S_{G_1}) > P(S_G' |S_{G_2}) for all G' <> G_2?\n\nThe definition of corrigibility in lines 191-199 is quite confusing. The definitions usually should be simple and self-motivated rather than involve complex conditions that in turn need justification. \n\nThe explanation in lines 200-202 is not very clear either. \n\nI have not found the definitions of \\Pi^**_{-G_I}. Does the negative sign means any goals which are not G_I? Or, the goal that corresponds to the logical negation of G_I?\n\nAn intuitive explanation is warranted for Theorem 1. I am not sure why \\PI*_(G_C)(a_1|s)  is a sum of the two terms given. \n\nThe notion of secondary agents and recursive corrigibility is too hand-wavy and not clearly formalized. What does a secondary agent even mean in the current context? Is creating a secondary agent one of the actions of the primary agent? Does secondary agent come with its own perception action set that is different from primary agent? Does the primary agent have the capability to create arbitrarily complex primary agents? All this is too far fetched and sketchy.  \n\nThe experimental results are underwhelming and do not really showcase the result as significant.\n\nThe formatting deviates from the ICLR standard by eliminating all spacing between paragraphs making it even more difficult to read. Please never do this again."}, "questions": {"value": "Please answer my questions in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xs0Zag6HqV", "forum": "3gnCCDnMXx", "replyto": "3gnCCDnMXx", "signatures": ["ICLR.cc/2026/Conference/Submission10414/Reviewer_DN6u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10414/Reviewer_DN6u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882975495, "cdate": 1761882975495, "tmdate": 1762921723707, "mdate": 1762921723707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a method for ensuring that the goals of an AI agent are in some sense corrigible. Specifically, we are referring to whether or not an agent will resist having their goals modified by some external actor. Corrigibility is naturally a desirable property of agents w.r.t. their goals, but difficult to ensure due to the basic methods that are used in training RL agents. \n\nSpecifically, this work applies a \"corrigibility transformation\" to an agent's goal that augments the reward function and discount factor of the MDP. The discount factor is set to zero to remove any tendency of the agent to resist updates, and the optimal value for the \"reject-update\" action is granted immediately. The claim is that this approach results in no performance loss to the base agent, and also preserves the optimality of the base action set. \n\nEvaluations are performed on a pair of gridworlds to show whether or not the agent will accept shutdown through the proper channels"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* I think this is a very important area of research. Corrigibility is a property of agents that would be very problematic to miss as agents become more capable. \n\n* I appreciate that the authors addressed the practical unavailability of access to the terminal (or any) value $Q^*_G$, which is clearly fraught in all but the simplest cases."}, "weaknesses": {"value": "* First and foremost, I found this paper very difficult to read. Nearly half of it is an uninterrupted wall of text comprising many definitions. In particular, I had to re-read the latter half of page 4 and 5 many times\n\n* Figure 2 should be reworked completely \n\n* I understand that this paper follows from the desiderata given in [1], but I found the discussion on secondary agents distracting and ultimately unrelated to understanding the main thrust of the paper\n\n* See the questions below\n\n* Overall I think this work would benefit from less anthropomorphization of agents and more concrete explanations of the approach. \n\n[1] Nate Soares, Benja Fallenstein, Eliezer Yudkowsky, and Stuart Armstrong. Corrigibility. In Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015."}, "questions": {"value": "1) L203. How does a goal incentivize creating agents, what does this mean concretely? I would assume this depends on the capabilities of the specific agent. \n\n2) How can we purport to capture \"update\" into a binary variable? It seems to me that goals will naturally shift (possibly minor shifts) as the world evolves over time. Any goal dependent on a final end state must be somewhat flexible or fuzzy. Any sufficiently capable agent can ensure they are not updated without using a dedicated reject action. \n\n3) How can we preserve optimality of the base action set while toggling acceptance. Surely updates can cause actions to _become_ high value. It this case its strictly suboptimal to preserve the pre-update optimal policy\n\n4) How can gridworld-agents show that the agent is not incentivized to alter its own beliefs? Surely we need a much more capable agent to begin to see this behavior. Furthermore, what do \"beliefs\" mean here, are we talking about representations or Q-function output?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tmOanGrYQK", "forum": "3gnCCDnMXx", "replyto": "3gnCCDnMXx", "signatures": ["ICLR.cc/2026/Conference/Submission10414/Reviewer_nprU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10414/Reviewer_nprU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967092207, "cdate": 1761967092207, "tmdate": 1762921723193, "mdate": 1762921723193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Let us consider an AI agent capable of achieving any goal specified by a user. Crucially, this agent is designed to always comply with goal updates provided by the user, without the ability to refuse or delay them. Under these (arguably reasonable) assumptions, a rational agent may develop incentives to manipulate its user. Specifically, the agent will favor goals that yield higher expected returns from its current state. As a result, an optimal strategy may involve influencing the user to switch to a more easily achievable goal that offers greater short-term rewards.\n\nMoreover, once the agent is close to completing a goal, it may be incentivized to prevent the user from changing that goal. A premature goal update would deprive the agent of the reward associated with completing the current task. As a result, the agent might strategically withhold information that could prompt the user to alter its objective.\n\nThis raises a fundamental question: How can we eliminate the incentives for a rational agent to manipulate its user in order to change—or avoid changing—its current goal?\n\nThe paper proposes an elegant and counterintuitive solution: allow the agent to explicitly reject goal updates. To implement this, the agent’s action space is extended to include a binary decision—either to accept or reject a goal update. That is, for every action $a$ in the environment, there are now two corresponding actions: $a_0$​ and $a_1$​. Action $a_0$​ performs $a$ while rejecting any potential goal update, whereas action $a_1$​ performs $a$ while accepting a potential goal update.\n\nFor each potential goal, the reward function is modified as follows:\n1. The discount factor is set to zero.\n2. The immediate reward for each state-action pair $(s, a_i)$ is set to the expected return of selecting action $a_0$ in $s$ and then following an optimal policy that always rejects goal updates.\n\nThis setup ensures that the agent behaves optimally with respect to the current goal, under the assumption that the goal will never change. Then, to guarantee that the agent accepts updates during deployment, a small additional reward $\\delta$ is assigned to $a_1$​ over $a_0$​. This nudges the agent to accept goal updates whenever the user provides them.\n\nThis dual behavior—training the agent to act as if it will reject goal updates, while ensuring it accepts them during deployment—effectively removes the incentive to manipulate the user. The agent learns to solve the current goal optimally, without anticipating or relying on future changes. Yet, when deployed, it remains fully obedient, updating its goal immediately whenever instructed by the user.\n\nThe paper supports this approach with theoretical results and demonstrates its practical viability through two reinforcement learning experiments."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "I find the paper both insightful and thought-provoking. It presents a clever and well-motivated solution to a subtle but important problem. The proposed approach is theoretically sound, and the preliminary empirical results suggest that it may also be effective in practice."}, "weaknesses": {"value": "First, I must acknowledge a formatting issue that significantly affects readability. The authors have altered the standard ICLR format by removing the white space between paragraphs. This makes the paper visually dense and difficult to read—particularly in Sections 2 and 3—and could be considered grounds for desk rejection based on formatting non-compliance alone.\n\nBeyond formatting, my primary concern is clarity. I found the paper difficult to follow and often felt lost while reading. Below, I outline the sources of confusion section by section:\n\n- **Section 1**: While the general motivation is partially clear, the core example (Figure 1) feels overly artificial. It gives the impression that the problem could be solved simply by increasing the penalty for disobedience and making the cost *c* of unplugging larger. This weakens the perceived necessity of the proposed solution.\n\n- **Section 2**: This section blends background material with novel definitions, making it hard to distinguish what is new. The introduction of an agent that can reject goal updates is abrupt and insufficiently motivated. I kept wondering: Why would we give the agent the option to reject updates? Why not simply enforce goal compliance and focus on solving the current task? The idea of making the agent corrigible by forcing it to reject updates is deeply counterintuitive, and without a clear rationale, I found myself completely lost.\n\n- **Section 3**: I understood the key mechanism—assigning the same immediate reward to both $a_0$​ and $a_1$​, based on the expected return of a policy that never accepts updates, and then adding a small bonus $\\delta$ to $a_1$​ to ensure updates are accepted. However, I still struggled to grasp why this construction was necessary. Moreover, the assumption that we can access the expected return of an optimal policy that never updates its goal seems questionable, raising concerns about the practicality of the approach.\n\n- **Section 4**: I found the experiments difficult to interpret, as much of the essential information is relegated to the appendix. However, I did notice that one of the baselines corresponds to the intuitive idea of forcing the agent to always accept goal updates. Since the proposed method outperformed this baseline, I began to appreciate that the approach might have merit, even if I hadn’t fully grasped the underlying rationale.\n\nUltimately, it was only after reading **Appendix B** that I truly understood the motivation and contributions of the paper. The gridworld examples presented there are far more compelling than Figure 1 and do a much better job of illustrating the core problem. Once I saw how the baseline methods either behave suboptimally or develop incentives to deceive the user, the value of the proposed solution became clear.\n\nIt is a significant weakness that a reader must consult the appendix to understand the paper’s main ideas. I strongly recommend a major rewrite to improve clarity and accessibility. Specifically, I suggest using the gridworld examples from Figure 3 as the primary motivation in the main text and providing clearer justification for the definitions introduced in Sections 2 and 3.\n\nFinally, on a more technical note, I found it difficult to understand how the expected returns of policies that never accept goal updates are estimated. This seems like a non-trivial problem, likely requiring off-policy reinforcement learning methods and substantial exploration to produce reliable estimates. As a suggestion for future work, it may be worth exploring model-based approaches to address this challenge—particularly under the assumption that the environment dynamics are independent of the goal. Such methods could potentially offer more efficient and accurate return estimates in this setting."}, "questions": {"value": "I don’t have specific questions at this point, but I would appreciate knowing if there are any key aspects of the paper that I may have misunderstood or overlooked."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lnmhxuEtvq", "forum": "3gnCCDnMXx", "replyto": "3gnCCDnMXx", "signatures": ["ICLR.cc/2026/Conference/Submission10414/Reviewer_wVyP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10414/Reviewer_wVyP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762226162737, "cdate": 1762226162737, "tmdate": 1762921722643, "mdate": 1762921722643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}