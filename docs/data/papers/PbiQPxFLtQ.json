{"id": "PbiQPxFLtQ", "number": 15094, "cdate": 1758247693520, "mdate": 1759897329253, "content": {"title": "How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment", "abstract": "Graph Neural Networks (GNNs) have become essential tools for analyzing graph-structured data in domains such as drug discovery and financial analysis, leading to growing demands for model transparency. Recent advances in explainable GNNs have addressed this need by revealing important subgraphs that influence predictions, but these explanation mechanisms may inadvertently expose models to security risks. This paper investigates how such explanations potentially leak critical decision logic that can be exploited for model stealing. We propose {\\method}, a novel stealing framework that integrates explanation alignment for capturing decision logic with guided data augmentation for efficient training under limited queries, enabling effective replication of both the predictive behavior and underlying reasoning patterns of target models. Experiments on molecular graph datasets demonstrate that our approach shows advantages over conventional methods in model stealing. This work highlights important security considerations for the deployment of explainable GNNs in sensitive domains and suggests the need for protective measures against explanation-based attacks. Our code is available at https://anonymous.4open.science/r/EGSteal-2BF7.", "tldr": "", "keywords": ["Graph Neural Networks", "Explainable GNN", "Model Stealing Attack"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/90765b518abdef93a1a5fd61e995b852490ce860.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Since GNNs are widely used in many high-stake domains, the explanations on how GNN make predictions are becoming important. Existing GNN explanations can be obatined either by post-hoc explanations or self-explainable GNNs. While being useful to increase the transparency of decision making, it also increases the risk that the GNNs can be attacked (with the help of these explanations). This paper investigates the so-called model stealing attacks (namely replicating the prediction behaviour of the GNNs) with the help of GNN explanations. To achieve this, they assume that a graph can be devided into two groups: explanation subgraphs and style subgraphs. They further (strongly) assume that the explanation subgraph fully determinate the prediction while the other subgraphs have no impact on the GNN prediction. On this basis, they design a mimic model that not only aligns the predicted label but also the provided explanation subgraph. To train such a model, they augment the training pool by perturbing the style subgraphs without querying the target GNN model too many times. They perform many empirical studies to show the effectiveness and superiority of their method. However, I have many concerns that need more empirical studies, without which the soundness of this paper can be largely weakened."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well organized;\n2. Many statements are supported with experiments;\n3. They tackle a novel problem;"}, "weaknesses": {"value": "The authors assume that the provided explanation subgraph fully determinate the GNN prediction. However, most post-hoc explanations may not reflect the actual decision logic [1]; besides, even self-explainable GNNs may provide explanations that are different from the underlying subgraphs that actually drived the predictions [2][3]. This issue is called unfaithful explanations, which have been widely recognized. Let me make this comment more actionable: \n\n(1) how could you guarantee (or measure) that the provided explanations actually reflect the decision logic, especially for post-hoc explanations? and under a black-box setting?\n\n(2) if you could not guarantee (or measure) this, what is the impact of the faithfulness of explanations on your method? Quantitaive and/or qualitative analyses are necessary; \n\n(3) what if there are multiple explanation subgraphs that align with the decision logic, but the explainer only provides a single one? You will treat the others as style subgraphs that do not impact the prediction? What will be the impact on your methodology?\n\n(4) What if the GNN predictor is a weak predictor? In other words, if the clasisfication accuracy is low (for example 0.4), the GNN classifier may not follow the true decision logic (if they follow the true decision logic, the accuracy should be high). Will your stealing model be able to align with the GNN preidction as well as the explanation?\n\n## References\n[1] Faithful and Consistent Graph Neural Network Explanations with Rationale Alignment\n\n[2] How Faithful are Self-Explainable GNNs?\n\n[3]  GNN Explanations that do not Explain and How to find Them\n\nPS: I am willing to increase my ratings if my concerns are (partially) addressed. (I may lower my ratings if they are completely ignored.)"}, "questions": {"value": "Please see the weak points"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kKdLv7xgGe", "forum": "PbiQPxFLtQ", "replyto": "PbiQPxFLtQ", "signatures": ["ICLR.cc/2026/Conference/Submission15094/Reviewer_njmV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15094/Reviewer_njmV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554568736, "cdate": 1761554568736, "tmdate": 1762925417109, "mdate": 1762925417109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a GNN model extraction attack method specifically targeting models with explanation subgraph provided.  The paper utilizes the causal relation between explanation in graph and label outcome to construct intervened graphs as augmented data. The author further design a training loss for surrogate model with explanation output to ensure both it replicates the classification result and interpretable mechanism."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**1.** The idea of not only aligning mode classification but also aligning model explanation outcome is interesting. By requiring the surrogate model to have similar importance preference with target model, it may potentially make their inner mechanisms to become closer, achieving a better replication.\n\n**2.** The presentation of the paper is clean and sound. The notation in the paper is used clearly and formulations are also expressed tidily. The figures for illustration are easy to understand.\n\n**3.** The experimental results provided are reliable. There are many details of the algorithm design and hyper-parameters setting are discussed in the appendix, and the code is provided through the anonymous link. Attack performance under different attack capacity and architectures are also fully test, making the experimental results reliable."}, "weaknesses": {"value": "**1.** The proposed intervened data as augmented data may introduce \"incorrectness\". In the causal analysis the author assumed the explanation subgraph $G_{E}$ is the part that decides to the classification,  while the style graph has little impact. So the author arbitrarily perturb the style graph while holding explanation subgraph to be the same to generate the intervened graph, while given the original label and same explanation subgraphs. However, this may be incorrect in some cases. For example, if we hope to indicate a benzene structure in a molecular (6 edges in a ring), the generated intervened graph may linking edges between them, and make the graph indeed no longer contain clean benzene and GT label changed, but the augmented data is still labeled as \"contain\", so incorrectness happens; the intervened model may also be created with benzene in $G_{S}$ and lead the real $G_{E}$ change, while the augmented data still holds the original one. In all, the assumption on Eq.(3) lacks of rationality since sometimes GNN not classify a specific subgraph region but a structure pattern, which should vary when $G_{S}$ change.  \n\n**2.** The chosen datasets all contain a lower quantity of average graph size within the dataset and lacks ground-truth explanations. The proposed intervened graphs hold a relatively large perturbation space, and when the graph scale is large, it would require much more samples to satisfy the Eq.(3) and Eq.(4). However, the chosen datasets in the experiments are relatively small on every single graph, which omits this potential problem. Besides, the chosen datasets do not contain a naturally existent groundtruth explanation, which makes it hard to known if the explained results $G_{E}$ really have the equality to hold Eq.(3) and (4)."}, "questions": {"value": "**1.**  As stated in appendix, the proposed loss term requires a $TQ|V|^{2}$ complexity to calculate all pairs of nodes' importance, which is a relatively high complexity. Be So I wonder if there's runtime record for conducting an extraction attack.\n\n**2.** Following the complexity problem, it seems that the main burden comes from the loss term on aligning explanation ranking on the surrogate model and target model. So how about remove this term? Would this largely decrease the model's performance? \n\n**3.** Is the surrogate model's explainer required to be the same with the target model? If not, would this architecture shift introduce additional error on replication?(ablation study suggested) If yes, this setting seems to lose practicability since the target model's explainer may not be known to the attacker, e.g., architecture or inner parameters.\n\nI suggest the author to at least conduct ablation study on the **questions 2&3**, because it leads to the necessity and rationality of designing the alignment loss term, which is currently unclear on its utility compared with its high complexity. If these two questions are properly addressed I would like to **raise the score to at least positive**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AcnBEUtDf6", "forum": "PbiQPxFLtQ", "replyto": "PbiQPxFLtQ", "signatures": ["ICLR.cc/2026/Conference/Submission15094/Reviewer_1dwp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15094/Reviewer_1dwp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966147842, "cdate": 1761966147842, "tmdate": 1762925416573, "mdate": 1762925416573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that model explanations can leak decision logic and enable model stealing. The authors propose EGSteal, an attack framework that combines explanation alignment and data augmentation to replicate a target model’s predictions and reasoning under limited queries."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and well structured, making it easy to follow the authors' ideas and understand the main contributions.\n- The figures are well designed and intuitive, effectively illustrating how the proposed method works and helping readers grasp the key concepts at a glance."}, "weaknesses": {"value": "- The paper claims that GNN models are deployed online to protect intellectual property in critical applications (e.g., drug screening). However, in realistic IP-protection scenarios, online ML services are unlikely to provide model explanations, since explanations can reveal internal model logic and sensitive knowledge.\n\n- Only explanations that rely on internal model access (e.g., gradient- or attention-based, or self-interpretable models) provided by an online ML service are useful; for post-hoc explainers such as GNNExplainer or PGExplainer, attackers could obtain explanations locally. Suppose the target model relies on a post-hoc explainer such as PGExplainer. In that case, it is questionable whether the proposed method is stealing the GNN model itself or rather the explainer.  Even if explanations were accessible, different explanation methods often yield inconsistent results, so aligning a surrogate model based on Graph-CAM explanations without knowing the target’s explanation mechanism is weird. \n\n- The proposed data augmentation is motivated by the assumption that real-world APIs limit the number of allowed queries. However, no empirical evidence or real-world example is provided to support this claim. The authors should show that such query constraints exist in real-world services.\n\n- The theoretical assumption that $G_S$ is prediction-irrelevant is inconsistent with the implementation, where $G_S$ is determined by a rank-based threshold in Eq. (8). This creates two issues: (1) If $G_S$ is truly irrelevant, node scores should be 0 (or close to 0), rather than merely ranked low, since even low-ranked nodes could still have non-negligible importance; (2) The manual hyperparameter $\\alpha$ critically affects the construction of $G_E$ and $G_S$, as well as the rationality of data augmentation.\n\n- If the target model is truly a black box, the attacker should not know which explanation method the target uses (e.g., post-hoc vs. self-interpretable). However, most main experiments assume Graph-CAM explanations for the target model, which is overly idealized.\n\n- In the cross-dataset setting, the evaluation focuses on showing that the proposed method outperforms TS in performance metrics (Lines 396-398). However, for an attack method, the key objective should be whether the surrogate model truly replicates the target model’s behavior. The results on the AIDS dataset, where the AUC is below 60%, suggest that the proposed method may fail to effectively steal the target model when the in-distribution assumption does not hold. This raises concerns about the validity of the method under realistic scenarios."}, "questions": {"value": "- The paper focuses on node-level explanations, but in graph applications, structure-level (i.e., edge-level) explanations are more common. Could the proposed method be directly extended to edge-level explanations? I suspect that, due to the specific design of the ranking-based loss, the proposed method might not generalize well to edge-level explanations.\n- The paper presents an experiment showing the robustness of the proposed method to noisy explanations. Could the authors clarify: (1) The motivation for this experiment: In what practical scenarios would the explanations become noisy? (2) Why is the proposed method robust to such noise? Is there any specific mechanism or design choice that contributes to this robustness?\n- In the experiments, the query budget is set to at least 10% of the training dataset. This means the attacker can issue hundreds or even thousands of queries. Is it motivated by any real-world scenario (e.g., are there existing systems that limit users to a few hundred queries)?\n\n\n**I found several aspects of the paper unconvincing as currently presented, which leads me to recommend rejection. That said, I may have misunderstood some points, and I would appreciate further clarification from the authors during the rebuttal phase. I apologize if any of my remarks are based on a misunderstanding, and I thank the authors in advance for clarifying these points.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pnDwaWSOG3", "forum": "PbiQPxFLtQ", "replyto": "PbiQPxFLtQ", "signatures": ["ICLR.cc/2026/Conference/Submission15094/Reviewer_kv62"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15094/Reviewer_kv62"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991403675, "cdate": 1761991403675, "tmdate": 1762925416126, "mdate": 1762925416126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}