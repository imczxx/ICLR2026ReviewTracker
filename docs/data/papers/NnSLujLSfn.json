{"id": "NnSLujLSfn", "number": 15062, "cdate": 1758247332578, "mdate": 1763534009047, "content": {"title": "Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence", "abstract": "Large Language Models (LLMs) are intensively used to assist security analysts in counteracting the rapid exploitation of cyber threats, wherein LLMs offer cyber threat intelligence (CTI) to support vulnerability assessment and incident response. While recent work has shown that LLMs can support a wide range of CTI tasks such as threat analysis, vulnerability detection, and intrusion defense, significant performance gaps persist in practical deployments. In this paper, we investigate the intrinsic vulnerabilities of LLMs in CTI, focusing on challenges that arise from the nature of the threat landscape itself rather than the model architecture. Using large-scale evaluations across multiple CTI benchmarks and real-world threat reports, we introduce a novel categorization methodology that integrates stratification, autoregressive refinement, and human-in-the-loop supervision to reliably analyze failure instances. Through extensive experiments and human inspections, we reveal three fundamental vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization, that limit LLMs in effectively supporting CTI. Subsequently, we provide actionable insights for designing more robust LLM-powered CTI systems to facilitate future research.", "tldr": "This paper studies the vulnerabilities of LLMs in performing cyber threat intelligence tasks", "keywords": ["Large Language Model", "Cybersecurity", "Cyber Threat Intelligence (CTI)", "Failure Analysis", "LLM Robustness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d1cfedb304f8c4be2d7471333b42d79173b2295.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper systematically analyzes the intrinsic vulnerabilities of LLMs in cyber threat intelligence (CTI) tasks. This work evaluates on multiple CTI benchmarks and real-world threat reports, it identifies three major failure modes: spurious correlations, contradictory knowledge, and constrained generalization that undermine LLM reliability. The authors propose an autoregressive, human-in-the-loop framework to categorize and interpret failure instances, revealing how these vulnerabilities propagate across CTI stages."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Comprehensive evaluation: This work uses nine CTI benchmarks and real-world databases to cover contextualization, attribution, prediction, and mitigation phases, which shows a thorough coverage of real word cybersecurity vulnerabilities from LLMs.\n\nClear taxonomy of vulnerabilities: The tri-category scheme (spurious / contradictory / constrained) discovered in this paper are well-defined and tied to empirical evidence and case studies\n\nReproducibility: Authors of this paper also claim that work includes open-source code link, which is valuable for the follow-up work of this paper."}, "weaknesses": {"value": "Limited quantitative analysis (minor points): From the reviewer’s point of view, though this paper is rich qualitatively, it still lacks statistical analysis, for example, Tab. 2 shows a wide range of experiment results, but the analysis is limited.\n\nManual inspection needed: I really appreciate the Human-In-The-Loop efforts of this work, but still one of my main concerns: the framework relies heavily on expert human validation, which may limit scalability and efficiency."}, "questions": {"value": "How do you ensure data leakage is avoided between training corpora of specialized models (e.g., Cyber-Zero) and evaluation benchmarks?\n\nAs CTI tasks sometimes are timely, were all models evaluated on identical CTI data snapshots? Could temporal mismatches (recent model cutoff date for example, GPT 5) confound vulnerability comparisons, especially for temporal contradiction\n\nThe authors formulated and answered 6 key research questions. It would be valuable to further elaborate on how the proposed use of large language models for CTI tasks could translate into broader, real-world impacts across cybersecurity operations and defense workflows. Could you please elaborate a little bit?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OHO5ZSBExU", "forum": "NnSLujLSfn", "replyto": "NnSLujLSfn", "signatures": ["ICLR.cc/2026/Conference/Submission15062/Reviewer_dw1e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15062/Reviewer_dw1e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761078272923, "cdate": 1761078272923, "tmdate": 1762925381243, "mdate": 1762925381243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper presents an autoregressive, human-in-the-loop framework to efficiently categorize LLM failure instances with high reliability.\n- It surfaces three dominant vulnerability categories and distinguishes model-specific versus universal gaps across specialized and general-purpose LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Practical, human-in-the-loop pipeline for systematic failure triage rather than ad hoc error lists.\n- Careful taxonomy of failures and comparative analysis across model classes. The framework components and failure categories are described in a structured manner to identify where and how the failure cases occur.\n- Highlights and distinguishes between universal vs model-specific weaknesses, informing targeted mitigation and evaluation design."}, "weaknesses": {"value": "- Several findings feel expected (general models fail more), with limited mechanistic explanation of why and where failures arise.\n- The paper stops at characterization; concrete remediation strategies or transfer of insights into training/guardrails are underdeveloped.\n- Lacks an ablation using an LLM-as-judge to measure how failure detection and category distributions shift.\n- I enjoyed the paper and find it highly relevant, but it stops just short of the logical next steps, turning its insightful failure taxonomy into concrete remediation and transfer strategies."}, "questions": {"value": "1. What concrete interventions arise from your taxonomy (e.g., targeted data augmentation, policy shaping), and how can they reduce specific failures? \n2. Can you run a study replacing humans with an LLM-as-judge and report how failure categories and categories change?\n3.  Although not part of your study, which stages of specialized training/guardrails in specialized LLMs reduce which failure types present in general-purpose LLMs, and can you localize effects via staged ablations?\n4. Similarly, like the previous question, although not part of your study and an orthogonal problem, how to transfer insights learnt from the failure categories from specialized models to improve general models (or vice versa) and show measurable deltas?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bRXw4FvWaE", "forum": "NnSLujLSfn", "replyto": "NnSLujLSfn", "signatures": ["ICLR.cc/2026/Conference/Submission15062/Reviewer_GMQy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15062/Reviewer_GMQy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761520084863, "cdate": 1761520084863, "tmdate": 1762925380844, "mdate": 1762925380844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper systematically investigates the inherent limitations of LLMs when applied to CTI analysis and reasoning. Using large-scale evaluations across multiple CTI benchmarks, the authors propose a human-in-the-loop autoregressive framework to categorize model failure modes with high reliability. Their analysis identifies three key vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization, which stem not from model design but from the fragmented and adversarial nature of CTI data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s primary strength lies in its systematic and well-structured investigation of LLM vulnerabilities in CTI, an area that has received limited formal analysis despite growing interest in security applications of LLMs. Its originality stems from the framing of CTI-specific vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization, as distinct from typical architectural or alignment flaws studied in AI safety literature. Methodologically, the proposed human-in-the-loop, autoregressive failure categorization framework demonstrates a rigorous and reproducible approach to large-scale error analysis, supported by empirical evidence and diverse datasets. The paper is generally well written and organized, presenting the findings with clarity. Overall, its significance lies in surfacing key failure patterns that challenge the reliability of LLMs in real-world CTI workflows, thereby providing a valuable foundation for more trustworthy AI-assisted security systems."}, "weaknesses": {"value": "- While the paper presents a well-organized taxonomy of LLM vulnerabilities in CTI, its main weakness lies in the limited methodological and conceptual novelty. The proposed categories: “spurious correlations,” “contradictory knowledge,” and “constrained generalization”, largely restate known issues in LLM reasoning, such as data bias, hallucination, and domain shift.  \n\n- The evaluation setup, though comprehensive in benchmark coverage, remains largely descriptive, emphasizing error categorization rather than quantitatively analyzing how these vulnerabilities affect CTI performance or exploring mitigation strategies such as fine-tuning, retrieval filtering, or evidence disambiguation.  \n\n- The paper does not translate its diagnostic findings into actionable guidance or concrete methods for mitigating the identified vulnerabilities, which limits the practical impact of the study for designing more reliable CTI-focused LLM systems.  \n\n- The analysis would benefit from deeper causal and ablation studies to reveal how specific dataset characteristics (e.g., temporal drift, source inconsistency, or conflicting labeling) lead to distinct failure modes. Such analysis could also help identify limitations of the existing benchmarks and suggest concrete directions for improving their design and robustness."}, "questions": {"value": "- The failure categorization relies on human verification, but the paper does not report inter-annotator agreement or consistency checks. Could the authors clarify how annotation reliability and consistency were ensured during the labeling process?\n- It would strengthen the paper to include experiments with potential mitigation strategies, either at training or inference time, that could address some of the discussed vulnerabilities, as their absence limits the practical contribution of the work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7peoGncJeB", "forum": "NnSLujLSfn", "replyto": "NnSLujLSfn", "signatures": ["ICLR.cc/2026/Conference/Submission15062/Reviewer_eES9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15062/Reviewer_eES9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724730696, "cdate": 1761724730696, "tmdate": 1762925380319, "mdate": 1762925380319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the use of LLM for generating cyber threat intelligence, particularly when used under reasoning under intertwined, crowdsourced, and imbalanced evidence. Using failure evidence from datasets drawn from multiple established CTI benchmarks, a new categorization method is introduced with human-in-the-loop verification. Particularly, an iterative refinement process is used where reference-based similarity metric quantify class association along with human inspection when clear classification is hard. The authors conclude with some interesting research findings summarizing various causes of vulnerabilities, which has not been discussed in previous works."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important research issue in the area of cybersecurity: reliability and robustness of reasoning mechanisms by employing LLM to generate threat intelligence and detection, particularly as LLM-as-a-judge. \n- The use of multiple benchmark datasets showing extensive coverage of numerous cybersecurity threats and the entire lifecycle. This exposes key failure patterns as noted in the paper \n-  A key aspect in the paper is its analysis of vulnerability threats which cascade across the CTI pipeline, showing a clear understanding of reasoning decisions considered in the context of cybersecurity."}, "weaknesses": {"value": "- A primary concern with the paper is that its contribution focuses more on “how” rather than “why”. Specifically, a more in-depth causal analysis would be useful to understand LLM behavior, specifically using intervention or counterfactual evaluation.\n- The research findings are fantastic, but lack the analysis which describe how to mitigate the mentioned vulnerabilities, which would be more useful to the community here.\n- A rigorous evaluation with more statistical analysis, particularly quantifying errors using confidence intervals and hypothesis testing could make the paper stronger and enhance the confidence in the findings.\n- While the use of benchmark datasets is good, the primary problem with research in cybersecurity is its practical application, particularly in the evolving scenario of zero-day exploits and as the adversary evolves trying to evade detection. It would be interesting to understand the proposed approach when there is a lack of sufficient information from external sources.\n- The results are not very surprising, and confirms the hypothesis described in numerous previous works on LLM reasoning."}, "questions": {"value": "1. In section 3.1, the similarity metrics seems to be BLEU score. However it is not clear why this is an appropriate measure. Could contextual embeddings be a better alternative?\n2. In section 3.3, the input to step 2 is not very clear on how it refines the reasoning. Is the output of Step 1 given as input to the same model? Or is it repeated across all models in the model set/committee?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "30LExz4Zz0", "forum": "NnSLujLSfn", "replyto": "NnSLujLSfn", "signatures": ["ICLR.cc/2026/Conference/Submission15062/Reviewer_n4w1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15062/Reviewer_n4w1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803001232, "cdate": 1761803001232, "tmdate": 1762925379839, "mdate": 1762925379839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the intrinsic vulnerabilities of large language models (LLMs) when applied to cyber threat intelligence (CTI). Through large-scale evaluations across multiple CTI benchmarks and real-world threat reports, the authors identify three key vulnerability categories: spurious correlations, contradictory knowledge, and constrained generalization. The paper proposes a novel human-in-the-loop, autoregressive categorization framework for analyzing model failures, and provides detailed empirical findings and case studies. Overall, it offers an important diagnostic study of how LLMs behave in a complex, adversarial, and data-fragmented domain."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Systematic study of intrinsic LLM vulnerabilities in cyber threat intelligence.\n\n- Comprehensive experimental coverage across multiple CTI benchmarks and model families.\n\n- Clear taxonomy that is intuitive and actionable for future research.\n\n- Thoughtful use of human-in-the-loop evaluation to address limitations of “LLM-as-judge” setups.\n\n- Strong motivation and clear relevance to the reliability and safety of applied LLM systems."}, "weaknesses": {"value": "- The failure taxonomy, while useful, is qualitative and somewhat subjective; lacks quantitative or causal validation.\n\n- Reliance on LLMs to assist in labeling may introduce self-evaluation bias.\n\n- No statistical significance tests or ablation studies, making it hard to gauge robustness of results.\n\n- Experiments are based on public CTI datasets rather than operational enterprise environments, which may limit real-world generalizability.\n\n- Proposed mitigation strategies are not empirically tested, leaving the study more diagnostic than actionable."}, "questions": {"value": "- Could the authors clarify how human annotators ensured consistency and reliability in the failure labeling process?\n\n- Did the same LLM architectures participate both as subjects of evaluation and as “judges” in classification? If so, how was potential bias mitigated?\n\n- Would conducting ablations on specific data sources (for example, removing outdated or conflicting feeds) help validate the causal role of contradictory evidence?\n\n- Are there plans or intentions to release anonymized human-annotation data or inter-rater agreement metrics to support reproducibility?\n\n- Could the authors elaborate on how their proposed taxonomy might generalize beyond CTI to other high-stakes or adversarial domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eVSuUCfiwf", "forum": "NnSLujLSfn", "replyto": "NnSLujLSfn", "signatures": ["ICLR.cc/2026/Conference/Submission15062/Reviewer_TPis"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15062/Reviewer_TPis"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031339082, "cdate": 1762031339082, "tmdate": 1762925379272, "mdate": 1762925379272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}