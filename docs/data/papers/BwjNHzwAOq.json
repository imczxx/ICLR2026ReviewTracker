{"id": "BwjNHzwAOq", "number": 5571, "cdate": 1757920543544, "mdate": 1759897967306, "content": {"title": "Introducing Multimodal Paradigm for Learning Sleep Staging PSG via General-purpose Model", "abstract": "Sleep staging is essential for diagnosing sleep disorders and assessing neurological health. Existing automatic methods typically extract features from complex polysomnography (PSG) signals and train domain-specific models, which often lack intuitiveness and require large, specialized datasets. To overcome these limitations, we introduce a new paradigm for sleep staging that leverages large multimodal general-purpose models to emulate clinical diagnostic practices. Specifically, we convert raw one-dimensional PSG time-series into intuitive two-dimensional waveform images and then fine-tune a multimodal large model to learn from these representations. Experiments on three public datasets (ISRUC, MASS, SHHS) demonstrate that our approach enables general-purpose models, without prior exposure to sleep data, to acquire robust staging capabilities. Moreover, explanation analysis reveals our model learned to mimic the visual diagnostic workflow of human experts for sleep staging by PSG images. The proposed method consistently outperforms state-of-the-art baselines in accuracy and robustness, highlighting its efficiency and practical value for medical applications. The code for the signal-to-image pipeline and the PSG image dataset will be released.", "tldr": "", "keywords": ["Physiological Signal Processing", "Sleep Staging", "Brain Computer Interfaces", "Interpretable AI"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3eca1c9f2db9db708500556786dbb8ccf5ef119.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new sleep staging paradigm that converts one-dimensional PSG signals into two-dimensional waveform images and uses a large multimodal model (LLaVA) to automatically classify sleep stages. The experimental results are excellent, but there are still some limitations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method achieved leading performance on three public datasets (ISRUC, MASS, and SHHS). \n2. This method represents a promising approach to applying large models to sleep staging. \n3. Technically, LoRa fine-tuning significantly reduces the number of trainable parameters. \n4. Leveraging pre-trained knowledge, it achieves optimal performance with only a small amount of data. \n5. Interpretability results strongly validate the model's effectiveness."}, "weaknesses": {"value": "1. The model primarily relies on the application of existing large model architectures and lacks fundamental algorithmic innovation. \n2. Despite the use of LoRA to reduce training parameters, the overall model's computational cost for inference remains high. \n3. It is recommended that the model's computational efficiency and size be explored in practical applications."}, "questions": {"value": "See weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lLytvzXwlP", "forum": "BwjNHzwAOq", "replyto": "BwjNHzwAOq", "signatures": ["ICLR.cc/2026/Conference/Submission5571/Reviewer_Phtr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5571/Reviewer_Phtr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525202282, "cdate": 1761525202282, "tmdate": 1762918143432, "mdate": 1762918143432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new paradigm for automated sleep staging. The core idea is to break from the traditional approach of relying on raw time-series signals and domain-specific models. Instead, the authors convert multi-channel raw polysomnography (PSG) signals into intuitive waveform images. Subsequently, they leverage a general-purpose large multimodal model (LLaVA-Next-7B) and fine-tune it on these images using Low-Rank Adaptation (LoRA) to achieve efficient and accurate sleep stage classification. The method aims to emulate the real-world workflow of clinical experts who diagnose by visually interpreting waveform charts, making the model's decision-making process more intuitive and interpretable. Experimental results on three public datasets show that this approach outperforms a wide range of baselines, including state-of-the-art domain-specific foundation models, while also demonstrating good robustness and interpretability."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.  This paper offers a new insight over the current trend of building brain foundation models directly from physiological signals.  It proves that the utilization of fine-tuning the general foundation model (LLAVA, GPT) can be a better way compared with building domain specific foundation models from scratch.\n2. The method transforms an abstract signal analysis problem into a more intuitive visual understanding task that can better leverage powerful existing models (i.e., general-purpose large models), offering a fresh perspective for similar biosignal analysis tasks.\n3. In scenarios simulating real-world signal degradation or data loss, it’s well motivated to utilize the general-purpose model, as such large models are more general and robust. The performance drop being significantly smaller than that of other domain-specific models, a crucial advantage in clinical settings.\n4. The paper includes robustness tests (simulating signal loss and noise) and interpretability analysis, which are crucial for assessing the clinical value of a medical AI model.\n5. The paper uses feature attribution methods to deeply investigate the basis of the model's decisions. The results show that the model, like a human expert, can automatically focus on key features for different sleep stages (e.g., EOG signals in REM sleep) and actively ignore corrupted signals.\n6. The modality ablation study in Appendix J (Fig. 10) indicates that when the proposed model is restricted to EEG-channel images, its performance remains comparable to or slightly higher than EEG-only foundation models (LaBraM and CBraMod) on both datasets. This result provides partial evidence that the benefit of the “image + LMM” paradigm is not solely due to the inclusion of additional signal channels.\n7. The paper provides a high level of detail. The signal-to-image conversion pipeline is clearly specified. Furthermore, the paper also offers solid theoretical arguments for why fine-tuning a general-purpose model provides advantages like compositional generalization and implicit regularization."}, "weaknesses": {"value": "1. The signals are converted to 336x336 images, which is a specific parameter choice. How sensitive is the model's performance to visualization parameters such as image resolution, aspect ratio, or color mapping?\n2. While the experiments cover three datasets, real-world clinical environments can have even more diverse equipment and setups (e.g., different electrode layouts, non-standard sampling rates). Could you clarify how to tackle this heterogeneity?\n3. The model relies on a text-based prompt for inference (e.g., \"Classify the PSG. Choose from: W, N1, N2, N3, REM.\"). How sensitive is the model's performance to the phrasing of this prompt? Have the authors experimented with different prompt structures (e.g., more descriptive or open-ended, such as \"Describe this sleep epoch\")?"}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g6oaAP2kWa", "forum": "BwjNHzwAOq", "replyto": "BwjNHzwAOq", "signatures": ["ICLR.cc/2026/Conference/Submission5571/Reviewer_dw9s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5571/Reviewer_dw9s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551584295, "cdate": 1761551584295, "tmdate": 1762918143125, "mdate": 1762918143125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes converting polysomnography (PSG) time-series signals into 2D waveform images and fine-tuning LLaVA-Next-7B with LoRA for automated sleep staging. The authors claim this \"new paradigm\" outperforms both domain-specific models and recently developed brain foundation models on three public datasets (ISRUC, MASS, SHHS), while demonstrating superior robustness to signal corruption."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental scope is reasonable, covering three datasets (ISRUC: 29 subjects/25k epochs; MASS: 46 subjects/41k epochs; SHHS: 313 subjects/315k epochs) with diverse acquisition protocols. Training efficiency is notable—LoRA fine-tuning with rank=16 achieves convergence in 3 epochs (1 epoch for SHHS), drastically faster than training domain models from scratch. The ablation comparing full LLaVA versus CLIP-only vision encoder (Figure 3) demonstrates clear value from the language decoder component.\n\n2. The attribution analysis provides clinical interpretability. The heatmaps in Figure 7 show the model focusing on EOG for REM stage and EEG channels for N1/N2/N3, aligning with AASM guidelines. Implementation details are thorough—the 5-stage signal conversion pipeline in Section 2.1 with explicit equations (1-5) ensures reproducibility.\n\n3. Testing against 12 baselines spanning CNNs (ConvNext, ResNet), ViTs, EEG foundation models (LaBraM, CBraMod), and signal-based architectures (TinySleepNet, SleepWaveNet) provides comprehensive comparison. The paper addresses a practical problem: adapting general models when domain-specific pre-training is prohibitively expensive."}, "weaknesses": {"value": "1. Critical confound in main claim: Table 1 compares your multimodal method (EEG+EOG+EMG+ECG) against EEG-only foundation models. This isn't a fair test of \"general-purpose vs domain-specific.\" Appendix J's EEG-only ablation reveals the issue: on ISRUC, you get Acc=0.8049 vs LaBraM's 0.7917 (1.3% difference) and B-Acc=0.7674 vs LaBraM's 0.7503 (1.7% difference). On MASS, Acc=0.8847 vs CBraMod's 0.8806 (0.4% difference). These margins are slim and lack statistical testing. The performance advantage appears driven by additional modalities, not the image paradigm. You must compare LaBraM/CBraMod when given the same multi-channel inputs.\n\n2. Robustness methodology is flawed: Equation 10 in Section 2.3.1 shows you corrupt signal channels after t_fail, then convert to images. But image-based models are architecturally designed to handle spatial occlusions—this is what Vision Transformers' attention mechanisms do. You're testing architectural robustness, not paradigm robustness. Fair comparison requires corrupting raw signals identically, then comparing how both approaches handle the same degraded inputs. Figure 4's dramatic gaps (your method -6.1% B-Acc vs CBraMod -31.1% on ISRUC) likely reflect ViT's spatial robustness rather than any inherent advantage of signal-to-image conversion.\n\n3. Dataset scale concerns: With ISRUC n=29 and MASS n=46, claiming superiority over CBraMod (pre-trained on 27,062 hours) needs statistical rigor. You mention \"five different random seeds\" but provide no confidence intervals, p-values, or variance analysis. More critically: are subjects split or epochs? If subject IDs appear in both train/test, your numbers are inflated. This must be clarified explicitly.\n\n4. Appendix B's theoretical justification is weak:\n- Equations 1-3 (resampling, epoching, min-max scaling) are standard preprocessing, not novel contributions\n- Section B.2's matched filter argument (h_opt(t) = k·s(t_0-t)) actually undermines your method—matched filtering requires preserving phase, but converting to images discards Fourier phase information\n- Section B.4 claims images enable \"instantaneous feature analysis\" via A_ẋ(A sin(2πft)) = 2πfA, but short-time Fourier transforms or wavelets achieve this on raw signals without image rendering\n- The \"visual atoms\" ψ_i decomposition in Appendix C.1 is just generic transfer learning theory with no PSG-specific insight\n\n5. Missing ablations: You never compare against alternative signal-to-image encodings (spectrograms, continuous wavelet transforms, recurrence plots). Why is matplotlib-style line plotting optimal? The min-max scaling per-epoch (Equation 3) removes absolute amplitude information—have you tested whether this hurts or helps?\n\n6. Metric confusion: Table 1 includes LPIPS and DISTS (perceptual image quality metrics from generative modeling). What do these numbers mean for a classification task? LPIPS=0.5047 on DIV2K—distance to what reference? This is never explained."}, "questions": {"value": "1. Does your 80/20 train/test split separate subjects or just epochs? If subject #5's epochs appear in both sets, performance is artificially inflated. State this explicitly.\n2. Can you fine-tune LaBraM and CBraMod on full multi-channel PSG (not just EEG) and report results? This isolates the image representation question from the multimodal input advantage.\n3. In Equation 10, when channel c fails at t_fail for subject s, exactly which epochs are affected? All subsequent ones, or only those overlapping that timestamp? Was the 70% corruption rate chosen a priori or tuned on validation data?\n4. The 5-seed splits in Section 3.2—what's the standard deviation across seeds? Are improvements over baselines statistically significant (t-test, bootstrap CIs)?\n5. Have you tested spectrograms (STFT), wavelet transforms, or learned signal-to-image encodings? Why is matplotlib line plotting optimal?\n6. Equation 11-12 define attribution scores, but Table 1's LPIPS/DISTS values—what do they measure for classification? Distance to what reference distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qKsKyKYOMy", "forum": "BwjNHzwAOq", "replyto": "BwjNHzwAOq", "signatures": ["ICLR.cc/2026/Conference/Submission5571/Reviewer_fT8B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5571/Reviewer_fT8B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855657622, "cdate": 1761855657622, "tmdate": 1762918142906, "mdate": 1762918142906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new paradigm for sleep staging from polysomnography (PSG) signals. Rather than training domain-specific models on raw time-series signals, the authors convert raw one-dimensional PSG time-series into intuitive two-dimensional waveform images and then fine-tune a multimodal large model to learn from these representations. The signal-to-image pipeline includes: (1)  Unified Sampling Rate, (2) Signal Epoching (into 30-second segments), (3) Epoch-wise Min-Max Scaling, and (4) Image Rendering and Sizing. Experiments on three public datasets (ISRUC, MASS, SHHS) demonstrate that their approach enables general-purpose models, without prior exposure to sleep data, to acquire robust staging capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work presents a new approach by converting PSG signals into intuitive waveform images, aligning well with how sleep clinicians diagnose in practice and enhancing interpretability. Comprehensive evaluations on three public datasets (ISRUC, MASS, SHHS) demonstrate the effectiveness of the proposed model. The feature ablation analysis (Section 3.6, Figure 7) reveals that the model focuses on clinically-relevant features and appropriate EEG patterns for different sleep stages. Besides, the experimental results demonstrate practical value in robustness and efficiency."}, "weaknesses": {"value": "1. While the approach of converting signals to images and fine-tuning MLLMs is interesting, the technical novelty is relatively limited. Signal-to-image conversion is an established technique widely used across various applications, and LoRA fine-tuning represents an application-oriented improvement rather than a methodological innovation. \n2. While the paper demonstrates interpretability through the Feature Ablation method, some issues still need further clarification:\n1) What is the computational complexity of the Feature Ablation method? How can time efficiency be guaranteed in practical applications given the large parameter count?\n2) Inference latency (critical for clinical deployment), computational cost comparison versus baselines, and signal-to-image conversion overhead.\n3. The choice of 14×14 pixel patches lacks justification for this specific granularity. More fundamentally, while perturbation-based methods are intuitive, if patches are mutually dependent, this could introduce bias in feature importance estimation, thereby compromising the reliability of model interpretations.\n4. Question about comparison setup and ablation studies. \n1) Comparing the proposed image-based method with signal-based methods and raw-signal foundation models represents different problem formulations, making it difficult to distinguish the typical contributions of the proposed model.\n2) The ablation study does not include results for the base LLaVA model without fine-tuning. This omission makes it impossible to determine whether performance gains are attributable to fine-tuning or simply inherited from LLaVA's massive pre-training on natural images. It is possible that the base LLaVA model already achieves strong performance on PSG images without any domain adaptation. This leaves the core claim, that the proposed paradigm can achieve superior performance, without sufficient support.\n3) Lacks ablation studies on key design choices in the signal-to-image conversion pipeline, such as, various image resolutions beyond 336×336 (e.g., 224×224, 512×512, ect.)\n4) Others, refer to questions."}, "questions": {"value": "1. Can you provide cross-dataset evaluation results (e.g., train on ISRUC, test on MASS/SHHS without fine-tuning)? This would validate whether the model learns robust, transferable representations.\n2. What are the specific training time, inference latency, and computational requirements compared to baseline methods? This information is critical for assessing clinical deployment feasibility, particularly in resource-constrained settings.\n3. How sensitive is performance to image resolution (e.g., 224×224 vs. 336×336 vs. 512×512)?\n4. What is the zero-shot or non-fine-tuned LLaVA performance on PSG images? This is essential to isolate your fine-tuning contribution from the model's pre-trained capabilities.\n5. How sensitive is the model to different prompt formulations? You mention testing with a \"variety of prompts\" in Section 2, but no results or analysis are provided. Please include: 1) Examples of different prompts tested; 2) Performance variance across prompts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PxuAxjGs8o", "forum": "BwjNHzwAOq", "replyto": "BwjNHzwAOq", "signatures": ["ICLR.cc/2026/Conference/Submission5571/Reviewer_xDSG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5571/Reviewer_xDSG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914361234, "cdate": 1761914361234, "tmdate": 1762918142449, "mdate": 1762918142449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a sleep-staging pipeline that converts 30-second PSG epochs into a single stacked waveform image and fine-tunes a general-purpose vision-language model (LLaVA-NeXT) with LoRA to classify stages (W, N1, N2, N3, REM) via a short prompt. On three public datasets—ISRUC, MASS, and SHHS—the method reports accuracy competitive with or exceeding strong image and signal baselines, despite using a generalist backbone rather than a biosignal-specific model.  Overall, the contribution is a simple “signals→image→VLM” recipe that mimics human visual scoring while leveraging modern multimodal models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Evaluates on three public sleep datasets (ISRUC, MASS, SHHS).\n2. Compares against a diverse set of strong baselines (signal- and image-based, incl. recent EEG/PSG models).\n3. The presentation is clear and easy to follow: the paper lays out a standardized signal-to-image pipeline and a straightforward LoRA-tuned LLaVA setup with an explicit prompt, supported by an overview figure, which improves reproducibility and reader comprehension."}, "weaknesses": {"value": "1. Scope & Evidence Gap. The manuscript presents a \"signals-to-images + MLLM\" pipeline but evaluates it only on sleep staging. Please clarify whether this is a sleep-specific system or a general framework for physiological signals. Substantial prior work (e.g., ECGInstruct/PULSE, MEIT) has demonstrated that VLMs can interpret image-rendered biosignals. \na) If sleep-specific: Enumerate the sleep-tailored design choices that justify this narrow focus.\nb) If general: Include at least one cross-modality experiment (e.g., ECG image classification) to demonstrate generalizability.\n\n2. Representation & Baselines. The paper renders PSG as stacked trace images, but substantial literature shows that alternative time-series-to-image transformations (spectrograms via STFT, scalograms via CWT, Gramian Angular Fields such as GAF/GASF/GADF, and Markov Transition Fields) often yield strong results with modern image backbones (e.g., ViT) and vision-language models. To fairly assess your rendering approach:\na) Add image-classification baselines using these validated transforms (STFT/CWT/GAF/MTF) with the same backbone and training recipe.\nb) Report a renderer-swap ablation within your framework (e.g., trace vs. spectrogram vs. GAF/MTF).\n\n3. Backbone Diversity. The manuscript fine-tunes only LLaVA/LLaVA-NeXT, limiting conclusions about \"MLLMs for physiological signals\" to a single architecture family. To support a general framework claim, evaluate at least one additional open-source VLM from a different family."}, "questions": {"value": "Major questions are addressed in the weakness section. The following are additional minor points:\n1. You show a short, constrained prompt. What alternatives were tried (class names, rationales, instruction style)?\n2. Beyond random splits, can you train on one dataset and test on another (leave-one-dataset-out)? This would better quantify deployment realism and domain shift.\n3. In the intro/related work, acknowledge that biosignals→images plus modern backbones are well-studied. State clearly what your framework adds beyond those precedents."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DJp584LumL", "forum": "BwjNHzwAOq", "replyto": "BwjNHzwAOq", "signatures": ["ICLR.cc/2026/Conference/Submission5571/Reviewer_xaDv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5571/Reviewer_xaDv"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission5571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762279352074, "cdate": 1762279352074, "tmdate": 1762918142102, "mdate": 1762918142102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}