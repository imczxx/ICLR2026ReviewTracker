{"id": "ZsIQUjQtdW", "number": 19464, "cdate": 1758296425761, "mdate": 1759897037514, "content": {"title": "Hierarchy Decoding: A Training-free Parallel Decoding Strategy  for Diffusion Large Language Models", "abstract": "The utilization of large language models (LLMs) has become increasingly widespread, and has attracted considerable attention. Although the emergence of discrete diffusion large language models (dLLMs) mitigates the inference latency inherent in autoregressive LLM decoding, its computational overhead remains substantial. To address this challenge, we propose Hierarchy-dLLM, a hierarchical decoding framework inspired by the divide-and-conquer principle. Our method recursively partitions masked spans into smaller sub-decoding areas and decodes tokens according to their confidence, which substantially increases the number of tokens generated per forward pass and improves information utilization. Extensive experiments conducted on multiple benchmarks demonstrate that Hierarchy-dLLM achieves accuracy comparable to or even surpassing existing baselines. Meanwhile, it is up to 17× faster than vanilla decoding and about 1.5× faster than the Fast-dLLM. These results establish hierarchical decoding as a practical solution for efficient large language model inference.", "tldr": "", "keywords": ["Diffusion Large Language Models", "Inference Acceleration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed58acc4cf974a06fc53ee610f807ce22b41a2ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Hierarchy Decoding, a training-free divide-and-conquer decoding method for diffusion LLMs. It maintains sparse masking by iteratively committing high-confidence tokens and partitioning sequences into sub-decoding areas, reducing distributional drift. Experiments on reasoning and code tasks show 4–17× speed-ups with comparable or better accuracy, demonstrating an efficient and plug-and-play inference strategy for diffusion-based LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Proposes a training-free hierarchical decoding strategy that is conceptually simple yet original for diffusion LLMs.\n\n2.Provides clear empirical motivation showing sparse masks yield more stable distributions than consecutive ones.\n\n3.Achieves 5–17× decoding speed-ups with little or no accuracy loss across reasoning and code tasks."}, "weaknesses": {"value": "1.The claimed O(log n) acceleration lacks formal assumptions and a proof or bound, so the theory feels anecdotal. \n\n2.The KL analysis hinges on argmax→one-hot “ground truth,” which may not generalize beyond code/math settings.\n\n3.Minor citation issue Line146: the text still shows “MMaDA (?)” and should be fixed to the actual reference."}, "questions": {"value": "1.Does the sparse-mask advantage persist with non-argmax targets on open-ended tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9zzjXgJ66R", "forum": "ZsIQUjQtdW", "replyto": "ZsIQUjQtdW", "signatures": ["ICLR.cc/2026/Conference/Submission19464/Reviewer_G5aG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19464/Reviewer_G5aG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761602051349, "cdate": 1761602051349, "tmdate": 1762931378390, "mdate": 1762931378390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new decoding method for dllm. They decode a block in a divide-and-conquer manner. Specifically, they recursively partition masked spans into sub-decoding areas. Within each area, tokens are unmasked based on confidence scores. In this way, dllm can generate multiple tokens per step without harming accuracy and maintain stability by avoiding dense consecutive undecoded spans."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The method has a clear motivation. The KL divergence study comparing sparse vs. consecutive masking is convincing and well-presented, grounding the method in a measurable phenomenon.\n\nEmpirical results show that the proposed decoding method achieves significant speedup comparing to standard decoding.\n\nThe method is training-free. There is no retraining cost."}, "weaknesses": {"value": "**Limited novelty**: The proposed method is largely a modification of the confidence-based decoding strategy. \n\n**Performance drop**: This method improves decoding speed by forcing that at least one token is unmasked in each area in each denoising step. However, there is an assumption behind this motivation: decoding in a divide-and-conquer way won't hurt performance. This is obviously wrong. Experimental results show that this Hierarchical decoding suffers from a performance drop even with the remasking strategy (Table 2). \n\n**Questionable scalability**: When batch > 1, the hierarchical decoding is inefficient to implement."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mhBluh19Ok", "forum": "ZsIQUjQtdW", "replyto": "ZsIQUjQtdW", "signatures": ["ICLR.cc/2026/Conference/Submission19464/Reviewer_YBgB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19464/Reviewer_YBgB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820873171, "cdate": 1761820873171, "tmdate": 1762931377950, "mdate": 1762931377950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Hierarchy-dLLM, a novel decoding framework designed to address the challenge of parallel decoding in discrete Diffusion Large Language Models (dLLMs). Due to the so-called “curse of parallel decoding,” dLLMs often revert to greedy decoding, resulting in inference efficiency that lags behind autoregressive (AR) models. The authors first conduct a preliminary study revealing that the spatial distribution of masked tokens has a significant impact on decoding stability: consecutive masked regions induce large distribution shifts, whereas sparsely distributed masked tokens maintain lower KL divergence and improve prediction stability. Building on this observation, Hierarchy-dLLM adopts a divide-and-conquer strategy in a training-free manner. It begins by treating a large masked region as the initial decoding area and iteratively decodes tokens based on confidence scores. Newly decoded tokens serve as anchors to split the remaining masked regions into smaller, independent sub-decoding areas, on which the same process is recursively and parallelly applied until all tokens are decoded. Experiments conducted on dLLM models such as LLaDA and Dream, across mathematical and code generation benchmarks, demonstrate that Hierarchy-dLLM achieves up to 17× speedup over vanilla decoding and about 1.5× acceleration compared to Fast-dLLM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Significant efficiency improvement: The proposed training-free decoding strategy leads to a remarkably large speedup in inference efficiency.\n2. Clear and insightful motivation: The experiments clearly demonstrate that sparse masking significantly reduces KL divergence compared to continuous masking, providing a strong empirical foundation for the proposed approach.\n3. Comprehensive experiments: The paper presents fairly extensive experiments that demonstrate the generality and applicability of the proposed method."}, "weaknesses": {"value": "1. Hyperparameter sensitivity: The proposed method introduces three key threshold hyperparameters that require grid search within a certain range. This raises concerns about the method’s robustness to hyperparameter choices and whether a single configuration can perform well across all datasets. Moreover, in Figure 3, the explored hyperparameter range appears relatively narrow; a broader tuning analysis would be necessary to better illustrate the method’s stability and generality.\n2. Some results indicate that the acceleration comes at a substantial cost to performance, as seen in many entries of Tables 2, 5, and 6.\n3. Beyond the empirical observations, the authors should provide a clearer explanation of why spatial sparsity constitutes a superior strategy, either from an empirical justification or an intuitive theoretical perspective.\n4. This appears to be a rapidly evolving field, and it would be beneficial to include a more detailed discussion of concurrent works on parallel sampling in the final version, for example in the appendix."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OjRFBMlqzN", "forum": "ZsIQUjQtdW", "replyto": "ZsIQUjQtdW", "signatures": ["ICLR.cc/2026/Conference/Submission19464/Reviewer_Hwju"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19464/Reviewer_Hwju"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896770861, "cdate": 1761896770861, "tmdate": 1762931377610, "mdate": 1762931377610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Hierarchy-dLLM, a training-free parallel decoding strategy for dLLMs. The method is motivated by a preliminary study suggesting that decoding is more stable when the remaining masked tokens are sparsely distributed rather than in a contiguous block. Based on this, the proposed algorithm employs a divide-and-conquer approach: it recursively partitions contiguous masked spans into smaller, independent \"sub-decoding areas\" by decoding high-confidence tokens that act as separators. This process is governed by a set of confidence-based heuristics and thresholds to decide which tokens to decode in each step. The authors evaluate their method on several dLLMs and benchmarks, reporting significant speedups (up to 17x) over vanilla decoding and outperforming the Fast-dLLM baseline in both speed and, at times, accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel Motivation: The preliminary study (Section 2.2) highlighting the benefit of sparse masks is an interesting and insightful contribution that provides a new perspective on the challenges of parallel decoding.\n\n- Creative Algorithmic Design: The application of a divide-and-conquer strategy to dLLM decoding is a novel and clever idea.\n\n- Strong Reported Speedups: The method achieves very large reductions in the number of forward passes (high TPF) and significant improvements in wall-clock time (high TPS), demonstrating its potential for practical impact.\n\n- Training-Free: The approach does not require any model fine-tuning, making it readily applicable to any pre-trained dLLM."}, "weaknesses": {"value": "- Heuristic Complexity and Hyperparameter Burden: The elegance of the \"divide-and-conquer\" concept is undermined by the complexity of the decoding rules in Section 3.2. The method depends on a carefully orchestrated set of three thresholds, and the ablation study (Figure 3) shows that TPS, a key metric, is quite sensitive to the choice of `τ_high`. A \"training-free\" method that requires expensive, task-specific hyperparameter tuning is of limited practical value. The paper does not provide a principled way to set these thresholds, which is a major weakness.\n\n- Incomplete and Potentially Weak Baselines: The paper's primary parallel decoding baseline is Fast-dLLM. However, recent literature includes other advanced training-free samplers (e.g., those based on revokable decoding like WINO, or dynamic strategies like SlowFast). By not comparing against a broader set of state-of-the-art methods, the paper's claims of superiority are not fully substantiated. It is unclear if Hierarchy-dLLM is truly better or just better than Fast-dLLM.\n\n- Potentially Flawed Foundational Study: The entire motivation rests on the preliminary study in Section 2.2. The use of a one-hot approximation for the stepwise distribution and the resulting cross-entropy surrogate for KL divergence is a reasonable but non-trivial simplification. More importantly, this setup defines \"ground truth\" as the model's own sequential output, which is somewhat circular. While the trend shown is plausible, the study is not rigorous enough to serve as conclusive proof, making the foundation of the method less solid than presented.\n\n- Algorithmic Overhead is Not Analyzed: The method requires managing a dynamic set of sub-decoding areas, finding the max confidence within each area, re-partitioning, and potentially re-masking. This introduces non-negligible computational overhead on the CPU side that is not present in simpler samplers. While TPS implicitly includes this, the paper does not discuss or analyze this overhead, making it difficult to assess how the method would scale in a highly optimized inference engine."}, "questions": {"value": "1.  Can you provide a more principled method or at least a robust heuristic for setting the three thresholds (`τ_high`, `τ_low`, `τ_remask`)? How much performance is lost if a single \"default\" set of thresholds is used across all tasks and models, compared to the per-task tuned values used in the paper?\n\n2.  To strengthen the paper's claims, could you provide a comparison against at least one other recent, sophisticated training-free baseline from the literature on a key benchmark like GSM8K?\n\n3.  The pseudocode in Algorithm 1 appears to have a logical error. The two `if` conditions for decoding `t*` seem redundant and potentially contradictory. Could you please clarify the exact logic for when a token within a sub-area is decoded? For example, is it decoded if *either* condition is met, or is there a sequence of checks?\n\n4.  Regarding the preliminary study, have you considered alternative metrics besides the cross-entropy surrogate for KL divergence to validate the \"sparse mask\" hypothesis? For instance, does the token-level prediction accuracy (against ground truth) improve more under sparse masking than consecutive masking?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jx6q0e4gCE", "forum": "ZsIQUjQtdW", "replyto": "ZsIQUjQtdW", "signatures": ["ICLR.cc/2026/Conference/Submission19464/Reviewer_hjHW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19464/Reviewer_hjHW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762667341586, "cdate": 1762667341586, "tmdate": 1762931377307, "mdate": 1762931377307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}