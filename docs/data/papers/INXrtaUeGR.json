{"id": "INXrtaUeGR", "number": 21836, "cdate": 1758322439986, "mdate": 1759896900750, "content": {"title": "Refactoring Codebases Through Library Design", "abstract": "Maintainable and general software allows developers to build robust applications efficiently, yet achieving these qualities often requires refactoring specialized solutions into reusable components. This challenge becomes particularly relevant as code agents become used to solve isolated one-off programming problems. We investigate code agents' capacity to refactor code in ways that support growth and reusability. We first investigate what makes a good refactoring, finding via asymptotics analysis and a human study that Minimum Description Length best aligns with developer preferences for code refactoring quality. We then present both a benchmark and a method for refactoring: MiniCode, a benchmark where multiple files must be refactored into a shared library, and Librarian, a sample-and-rerank method for generating reusable libraries. We compare Librarian to state-of-the-art library generation methods, and study it on real-world code bases.", "tldr": "We introduce a benchmark and method for refactoring programs into libraries. Using a user study and analysis, we find Minimum Description Length (MDL) captures good refactorings. Our method outperforms prior work and refactors real-world repos.", "keywords": ["Code", "agents", "refactoring", "compression", "library learning"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/05197b2b2875cbf93e347b028c66cf21d3faf26b.pdf", "supplementary_material": "/attachment/699b5aa4da405a82c79938cefdae82599110c0a4.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces MINICODE, an open-ended benchmark for refactoring multiple source files into a reusable library, and LIBRARIAN, a sample-and-rerank approach to synthesize such libraries. A comparative study finds that Minimum Description Length aligns best with developer preferences for refactoring quality; the method is further validated on real-world repositories, showing promising practical implications."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The investigated problem is clearly defined, novel and important to the community. MINICODE emphasizes open-ended library design, objective verifiability via unit tests, and large-context understanding across multiple files, addressing gaps in prior repo-level benchmarks.\n+ LIBRARIAN combines sample-and-rerank with semantic clustering and a progressive, cross-cluster library accumulation strategy, which is practical for long-context constraints.\n+ The paper compares Tokens/MDL vs. CC/MI, shows MDL better promotes reusable abstractions, and corroborates this with a human study.\n+ Evaluations show strong empirical results. LIBRARIAN shows above 90% pass rate on CodeContests, which is a successful practical application of library learning to real software projects."}, "weaknesses": {"value": "- In practice, a single file often mixes heterogeneous concerns (utility helpers, adapters, domain logic, classes), so the reviewer is not sure that file-level clusters could be too coarse.\n- Figure 6 is a case study refactoring the code from the HuggingFace Transformers codebase. However, it is a little complex to understand. Authors are suggested to walk through the end-to-end pipeline that produced this figure."}, "questions": {"value": "No question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9lWRRvg30g", "forum": "INXrtaUeGR", "replyto": "INXrtaUeGR", "signatures": ["ICLR.cc/2026/Conference/Submission21836/Reviewer_p9FN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21836/Reviewer_p9FN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622683675, "cdate": 1761622683675, "tmdate": 1762957491301, "mdate": 1762957491301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies code refactoring in software engineering, focusing on maintainability and reusability. The authors analyze different metrics for evaluating refactoring quality and find that Minimum Description Length (MDL) is the best metric. To evaluate and advance research on code refactoring, the paper introduces the MINICODE benchmark and the LIBRARIAN method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- MINICODE offers a practical and effective benchmark for evaluating code refactoring\n- The proposed metric MDL is reasonable and interesting."}, "weaknesses": {"value": "- The proposed sample-and-rerank approach is relatively simple, and the methodological insights it provides are limited.\n- The main risk of using MDL is that it can be heavily influenced by a single model. The paper only briefly discusses cross-model agreement for MDL in Section 6; a more detailed analysis would make the claim more convincing.\n- Even if the refactored code passes all unit tests, there is still a risk of semantic inequivalence with the original code. The paper lacks an analysis of this risk."}, "questions": {"value": "- Are there any cases where refactorings produced by LIBRARIAN in real projects have been successfully merged into the community codebase?\n- Typos: line 218: “set of set of”; line 273: “more more.”\n- Missing citation for the MI metric.\n- What does “change from non-refactored” mean in Figures 2 and 3, and how is it calculated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y31JNrXCKr", "forum": "INXrtaUeGR", "replyto": "INXrtaUeGR", "signatures": ["ICLR.cc/2026/Conference/Submission21836/Reviewer_aB2G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21836/Reviewer_aB2G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895109121, "cdate": 1761895109121, "tmdate": 1762941948944, "mdate": 1762941948944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of refactoring existing code to generate library code that is then used for rewriting the original code snippets. The paper assembles a benchmark called MiniCode and proposes a technique called Librarian. The paper compares different ranking methods and finds that minimum description length (MDL) to be a suitable metric to rank different refactoring suggestions. The evaluation on MiniCode shows effectiveness of Librarian."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Code refactoring is an important software engineering activity. This paper demonstrates progress on this problem using a pipeline of clustering of code by natural language summary, cluster-specific library extraction and then rewriting the complete code corpus.\n- It assembles a benchmark taking code contest solutions, previous refactoring benchmarks and small sets of related files from transformers and diffusers libraries. The resulting refactorings are ranked using MDL and evaluated for correctness using tests. It reports better refactoring accuracy than closely related work Regal on a subset of the benchmark.\n- The comparative analysis between MDL, tokens and software engineering metrics is interesting and justifies the use of MDL in ranking."}, "weaknesses": {"value": "- While the problem of refactoring is important, the proposed method is evaluated in limited setting. It does not present results at large scale where refactorings are most important and useful. Though the paper states that the proposed method is evaluated on \"real-world code bases\", the scope is restricted to a total of 3 tasks with 10 files each from 2 repositories.\n- The paper's novelty over past work, Regal, is limited as both of them apply clustering based refactoring. \n- The study of different metrics showing MDL > tokens > MI and that test-time scaling can help are the interesting parts in the paper. As an aside, I could find a citation for the MI metric; please add."}, "questions": {"value": "- In coding contest setting, the same problem can be solved using different algorithmic techniques. How well does the clustering work in such a setting? What is the quality of clustering if you were give entire code repositories (e.g., transformers) rather than hand-selected 10 files?\n- As noted in the experiments, Librarian can create functions that are used only once. Curious if they are used within the refactored code itself (e.g., private methods) or outside?\n- How does the performance vary if you use GPT models instead of o4-mini on CodeContents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pc8xvBGJP8", "forum": "INXrtaUeGR", "replyto": "INXrtaUeGR", "signatures": ["ICLR.cc/2026/Conference/Submission21836/Reviewer_vBJz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21836/Reviewer_vBJz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907711557, "cdate": 1761907711557, "tmdate": 1762941948617, "mdate": 1762941948617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LIBRARIAN, a method for refactoring multiple code files into reusable libraries, and MINICODE, a benchmark for evaluating library design. The authors investigate what makes good refactoring metric through human study and asymptotic analysis, finding that MDL aligns best with developer preferences. They demonstrate their approach on competition programming and real-world repositories like HuggingFace Transformers and Diffusers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well written and achieves impressive real-world validation by refactoring HuggingFace production code with 67% MDL reduction while maintaining correctness.\n- This work provides systematic comparison of multiple metrics through asymptotic analysis and human studies, finding MDL superior to traditional software engineering metrics."}, "weaknesses": {"value": "- This evaluation covers only 10 Transformers files and 2 Diffusers tasks, which seems insufficient to support claims about general applicability to real software projects.\n- This human study with only 12 participants lacks statistical power to distinguish between MDL and tokens metrics, yet the authors make strong claims about MDL superiority."}, "questions": {"value": "- Could you provide concrete examples showing these improve code quality beyond just MDL score?\n- How does LIBRARIAN's performance degrade with larger cluster sizes S? The paper fixes S=3 for CodeContests but S=5 for Transformers without justification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NmJBzGFZUV", "forum": "INXrtaUeGR", "replyto": "INXrtaUeGR", "signatures": ["ICLR.cc/2026/Conference/Submission21836/Reviewer_NZ2R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21836/Reviewer_NZ2R"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968242184, "cdate": 1761968242184, "tmdate": 1762941948375, "mdate": 1762941948375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}