{"id": "XHzrBDzKaX", "number": 88, "cdate": 1756728639151, "mdate": 1763603361558, "content": {"title": "Castle-in-the-Air: Evaluating MLLM Visual Abilities on Human Cognitive Benchmarks", "abstract": "Despite significant progress on popular multimodal benchmarks, state-of-the-art Multimodal Large Language Models (MLLMs) continue to struggle with basic visual reasoning tasks that are trivially solved by humans, such as recognizing abstract patterns or identifying spatial relationships.\nSuch deficiencies undermine their efficacy and robustness, rendering high-level downstream applications (e.g., embodied AI) infeasible.\nTo systematically investigate this gap, we introduce VisFactor, a benchmark that digitizes 20 vision-centric subtests from FRCT, a well-established cognitive psychology assessment, including four domains of human visual cognition: (1) Visualization and Spatial Processing, (2) Perceptual and Closure, (3) Memory, and (4) Reasoning.\nFurthermore, we leverage parametric generation to automatically construct unlimited test cases with controllable difficulty for applicable subtests.\nUsing VisFactor, we evaluate 23 frontier MLLMs, including both proprietary (GPT, Gemini, etc.) and open-source models (LLaMA-3.2, Qwen2.5-VL, etc.).\nThe best-performing model achieves a score of only 30.17%, with consistent failures on tasks such as mental rotation, spatial relation inference, and figure–ground discrimination—regardless of model size or prompting strategy.\nThese findings suggest that performance improvements on existing general benchmarks might be castles in the air instead of mastery of human-like visual cognition, challenging the assumption that large-scale pretraining naturally induces gestalt-like perceptual capabilities.\nThe dataset and evaluation toolkit will be made publicly available upon publication.", "tldr": "", "keywords": ["Multimodal Large Language Model", "Vision Language Model", "Cognition", "Evaluation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2e89530b7cc08b3d0ac8c466c79d61e33d3aa0a.pdf", "supplementary_material": "/attachment/70605ccf308eee0a1323bf598602ed76ea43a554.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces VISFACTOR, a benchmark that digitizes 20 vision-centric subtests from FRCT, a well-established cognitive psychology assessment. It covers four domains of human visual cognition: Visualization and Spatial Processing, Perceptual and Closure, Memory, and Reasoning. Additionally, it uses parametric generation to automatically construct unlimited, difficulty-controllable test cases for applicable subtests. Furthermore, evaluations of 20 frontier MLLMs based on VISFACTOR show that the best-performing model only achieves a score of 25.19%."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper presents the benchmark that grounds MLLM assessment directly in human cognitive factors, thereby infusing psychometric rigor into multimodal evaluation.\n2. The paper digitizes all FRCT visual items, designs targeted item variants to avoid random guessing biases, and introduces controllable-difficulty item synthesis specifically for the most challenging subtests—addressing the limitation of finite original test items and enabling scalable, gradient evaluation.\n3. Additionally, the paper reduces the average random guessing accuracy via targeted format optimizations (e.g., decomposed multiple choice, grouped consistency, and other rule-based strategies), effectively minimizing score inflation from luck and enhancing the reliability of evaluation results."}, "weaknesses": {"value": "1. While a parametric generator and \"controllable difficulty\" design are introduced, the work lacks explicit verification that these difficulty gradients align with human cognitive standards—undermining the benchmark’s validity for mapping model performance to human-like visual reasoning.\n2. Though formats like \"decomposed multiple choice\" reduce random guessing, they raise demands on models’ language logic. Subtle wording differences may cause errors from language misunderstanding (not poor visual reasoning), distorting assessments of true visual capabilities.\n3. The dataset  lacks detailed specs for each type’s sample size and clear evaluation criteria—this ambiguity hurts result reproducibility and makes assessing sample statistical sufficiency difficult."}, "questions": {"value": "1. The paper states that VISFACTOR is derived from FRCT, a well-established cognitive psychology assessment. Does it provide data or analysis to confirm that the performance of human participants on VISFACTOR is consistent with their performance on the original FRCT? \n2. The paper uses parametric generation to create unlimited, difficulty-controllable test cases. What methods or experiments were conducted to verify that these automatically generated cases can accurately distinguish differences in visual cognitive capabilities (e.g., between different MLLMs or between MLLMs and humans)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Shgocul2rD", "forum": "XHzrBDzKaX", "replyto": "XHzrBDzKaX", "signatures": ["ICLR.cc/2026/Conference/Submission88/Reviewer_giwN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission88/Reviewer_giwN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission88/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761565656777, "cdate": 1761565656777, "tmdate": 1762915448761, "mdate": 1762915448761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines the limitations of current evaluation methods for large language models (LLMs). Using the “castle in the air” metaphor, it argues that many benchmark results overstate genuine reasoning or understanding, presenting an illusion of competence. The authors introduce a mid-sized dataset (~50K examples) designed to better capture multi-dimensional reasoning and factual grounding. The dataset is constructed through a hybrid process—model generation, human verification, and automatic augmentation.\n\nThe paper presents experiments across several reasoning and factual tasks, analyzing model performance and alignment gaps. The results indicate that existing metrics often fail to reflect deeper reasoning quality, and that even top-performing models can exhibit superficial correctness."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1/ The work addresses a central issue in current LLM research—evaluation reliability and interpretability. Its focus on “illusory competence” is well-motivated and aligns with active discussions in the field.\n\n2/ The dataset is thoughtfully constructed using human-in-the-loop curation and adversarial augmentation. This improves diversity and realism compared to purely synthetic benchmarks.\n\n3/ Multiple LLMs are tested across reasoning, factual, and generative dimensions. The analysis includes both quantitative and qualitative components, highlighting systematic model weaknesses.\n\n4/ The paper is well-organized, with logical flow from motivation to conclusion. The “castle in the air” framing adds conceptual coherence and readability."}, "weaknesses": {"value": "1/ The discussion of reasoning lacks connection to existing cognitive or formal reasoning theories. This reduces the conceptual depth of the argument.\n\n2/ Dataset statistics and evaluation setup could be more transparent—particularly regarding sample sizes per task, statistical significance, and model parameterization.\n\n3/ Providing case studies that how state-of-the-art VLMs try to solve these tasks might bring more insight into the field."}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "r20T7wTrsA", "forum": "XHzrBDzKaX", "replyto": "XHzrBDzKaX", "signatures": ["ICLR.cc/2026/Conference/Submission88/Reviewer_m6KY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission88/Reviewer_m6KY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission88/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659035703, "cdate": 1761659035703, "tmdate": 1762915448632, "mdate": 1762915448632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VisFactor, a benchmark that includes 20 vision-centric subtests derived from the Factor-Referenced Cognitive Test (FRCT) battery to evaluate multimodal LLMs (MLLMs) on several human visual cognition factors: visualization and spatial processing, perceptual and closure, memory, and reasoning. The authors (i) digitize FRCT items and standardize text prompts, (ii) redesign response formats to reduce chance accuracy to about 2.9%, and (iii) implement parametric generators for a subset of subtests to control difficulty. They test 20 proprietary and open models and report low absolute scores, where the best model reaches ~25% overall with consistent failures on mental rotation, spatial relations, and figure-ground discrimination. They also analyze where models perform better (memorization with semantic content) and worse (abstract patterns), arguing that current systems rely on concept-label recognition rather than low-level perception."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors systematically reformulate FRCT items with clear prompts and JSON-formatted responses, making the tasks machine-readable and easy-to-use.\n2. The \"easy–normal–hard\" variants add quantitative control over visual challenge and allow scaling to stronger models.\n3. The inclusion of 20 subtests provides broad coverage of low-level and high-level visual cognition skills.\n4. The finding that models perform well only when semantic cues exist but fail on abstract or spatial transformations reinforces the known conceptual–perceptual gap in MLLMs."}, "weaknesses": {"value": "1. Incremental over existing works. The main idea that MLLMs struggle with spatial and perceptual reasoning is consistent with plenty of prior studies [1–4]. Authors claimed it is the \"first benchmark that grounds MLLM assessment directly to human cognitive factors\". The paper does not provide deeper diagnostic insight or a new analytical perspective in my opinion.\n2. Limited psychometric validation. Since FRCT assumes factor independence, validating whether these factors transfer meaningfully to MLLMs (e.g., internal consistency or item-response correlation) is essential but missing.\n3. No human baseline under identical protocol. Without re-collected human scores on the digitized tests, the paper’s claim of “human-level gap” is qualitative.\n4. Possible data contamination and licensing concerns. FRCT items are not guaranteed to be public domain. The paper should clarify licensing or use only synthetic items. \n5. Overemphasis on reformatting. The benchmark is technically solid but largely an infrastructure contribution. The analysis section should provide more interpretation of why models fail: whether failures stem from vision backbone resolution limits or misalignment between textual and spatial representations.\n\n[1] Fu et al., “BLINK: Multimodal Large Language Models Can See but Not Perceive,” ECCV 2024. \\\n[2] Cao et al., “What is the Visual Cognition Gap between Humans and Multimodal LLMs?,” COLM 2025. \\\n[3] Li et al., “Core Knowledge Deficits in Multimodal Language Models,” ICML 2025. \\\n[4] Zhang et al., “RAVEN: A Dataset for Relational and Analogical Visual Reasoning,” CVPR 2019."}, "questions": {"value": "1. How did you validate that the digitized FRCT tasks still isolate the intended cognitive factors?\n2. Can you report internal consistency (e.g., Cronbach’s alpha) or inter-item correlation to confirm psychometric reliability?\n3. Are decoding settings (temperature, reasoning depth, chain-of-thought tokens) consistent across models?\n4. Did you check overlap between FRCT images and web-exposed examples that could leak into pretraining corpora?\n5. Have you confirmed legal licences to redistribute FRCT content, or will you release only generated data?\n6. Can you show a small-scale human baseline to quantify the human–model gap?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "Need potential ethic review to check whether the authors compliance with the license of Factor-Referenced Cognitive Test (FRCT)."}}, "id": "9E3XLMqNwS", "forum": "XHzrBDzKaX", "replyto": "XHzrBDzKaX", "signatures": ["ICLR.cc/2026/Conference/Submission88/Reviewer_gXiQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission88/Reviewer_gXiQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission88/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893225359, "cdate": 1761893225359, "tmdate": 1762915448509, "mdate": 1762915448509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VISFACTOR, a benchmark that digitizes 20 vision-centric subtests from the Factor-Referenced Cognitive Test (FRCT) battery , a well-established human cognitive assessment. VISFACTOR spans four key domains: (1) Visualization and Spatial Processing, (2) Perceptual and Closure, (3) Memory, and (4) Reasoning.\n\nThe authors evaluate 20 frontier MLLMs, including both closed-source and open-source models . The best-performing model achieves a score of only 25.19%. Failures are consistent across tasks like mental rotation, spatial relation inference, and figure-ground discrimination, regardless of model scale or prompting strategy (CoT). Failure analysis suggests models succeed by relying on interpretable, concept-level representations rather than low-level visual patterns."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper adapts a new benchmark from decades of cognitive psychology research (the FRCT), which is a highly novel and valuable approach.\n\n- The experiment (Figure 3, Table 3) comparing model performance on the MA1 (memory) task using semantically rich images versus abstract figures provides a brilliant and convincing demonstration that models are \"cheating\" by mapping images to high-level concepts rather than performing low-level visual comparison and memorization."}, "weaknesses": {"value": "- The paper's framing of MLLM failure in \"human-like visual cognition\" is weakened by the lack of a contemporary human baseline. The authors compare MLLM performance on a new digital protocol to historical norms from a paper-and-pencil task, which is an invalid comparison due to protocol changes (e.g., digitization, no time pressure). While acknowledged in Appendix C, this is a significant limitation. To substantiate claims about a \"human-like\" gap, the authors must establish a human \"ceiling\" by collecting baseline data using the identical VISFACTOR protocol given to the models.\n\n- The paper fails to sufficiently differentiate VISFACTOR from the large body of existing work on abstract visual reasoning. The authors should provide a comprehensive comparison to benchmarks like ConceptARC, MaRs-VQA, and various Bongard Problem datasets. This comparison should articulate the unique cognitive abilities or reasoning types that VISFACTOR isolates which are not already covered by prior work, thus justifying its specific contribution.\n\n- A concern is the benchmark's potential longevity. The authors do not include results from the most recent, publicly available SOTA models (e.g., GPT-5, Gemini 2.5 Pro). It is possible these models already perform at or near the human ceiling, which would render the benchmark \"solved\" and limit its utility for measuring future progress. The authors should test the strongest available models to demonstrate that VISFACTOR remains a challenging task and that its findings of MLLM \"failure\" are still relevant."}, "questions": {"value": "See weaknesses for more details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AriMkko6J0", "forum": "XHzrBDzKaX", "replyto": "XHzrBDzKaX", "signatures": ["ICLR.cc/2026/Conference/Submission88/Reviewer_7StU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission88/Reviewer_7StU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission88/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225699898, "cdate": 1762225699898, "tmdate": 1762915448321, "mdate": 1762915448321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and encouraging assessments. **We greatly appreciate the recognition of:**\n1. Grounded in cognitive science & psychology. (7StU, m6KY, giwN)\n2. Standardized, reliable evaluation. (gXiQ, m6KY, giwN)\n3. Controlled-difficulty variants. (gXiQ, giwN)\n4. Wide range of MLLMs evaluated. (gXiQ, m6KY)\n5. Analysis on memory pattern. (7StU, gXiQ)\n\nWe truly appreciate all reviewers and meta reviewer’s time and effort. **We have carefully read and addressed all your concerns, including:**\n1. Human baseline with identical testing protocol. (7StU, gXiQ, giwN)\n2. New models (GPT-5.1, Qwen-3-VL). (7StU, gXiQ)\n3. Cronbach’s alpha / Pearson correlation analysis. (gXiQ, giwN)\n4. Temperature ablation study & CoT length analysis. (gXiQ)\n5. More detailed explanation. (gXiQ, m6KY, giwN)\n\n**All major modifications are highlighted in blue in the paper.** We thank you again for your time and constructive insights."}}, "id": "uR3fWEt06s", "forum": "XHzrBDzKaX", "replyto": "XHzrBDzKaX", "signatures": ["ICLR.cc/2026/Conference/Submission88/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission88/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission88/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763603320802, "cdate": 1763603320802, "tmdate": 1763603320802, "mdate": 1763603320802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}