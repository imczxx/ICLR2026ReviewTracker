{"id": "rSetFgvtfH", "number": 21794, "cdate": 1758321856174, "mdate": 1759896902681, "content": {"title": "Enhancing LLM-Based Software Vulnerability Identification through Synthetic Reasoning", "abstract": "While large language models (LLMs) show promise in software security, they struggle to comprehend the underlying logic of vulnerabilities and are often confounded by the high semantic similarity between flawed and patched code. To overcome these limitations, we introduce HeroCode, a novel model designed to transform general-purpose LLMs into specialized vulnerability identification experts. HeroCode's advancement stems from its unique training on a synthetically generated dataset that explicitly details the reasoning behind both vulnerability exploits and their remediations. This reasoning-rich data is leveraged by our core Hierarchical Epistemic Robust Optimization (HERO) architecture, a framework that integrates distributional robustness across multiple abstraction levels to compel a deeper understanding of fundamental security patterns over superficial semantics. Empirical evaluations demonstrate that HeroCode substantially outperforms existing methods, setting new state-of-the-art performance records on the PrimeVul and SVEN benchmarks. When integrated with Qwen2.5-Coder-7B-Instruct, HeroCode achieves 60.66\\% accuracy on PrimeVul—surpassing even GPT-4's 52.24\\%—proving its exceptional capability in distinguishing between vulnerable and patched code implementations.", "tldr": "", "keywords": ["code security", "transfer learning", "reinforcement learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82c8a733950e7111f4641afb1db3cc3af72876ba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an approach to improve the performance of language models in vulnerability detection. The approach is based on fine-tuning the model on a reasoning dataset filtered through different stages."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "None"}, "weaknesses": {"value": "- The paper is extremely unclear with respect to the details of the proposed approach, which makes it almost impossible to judge the novelty and the quality of the approach.\n    \n- The values of the performance metrics shown in figure 3 do not correspond to the commentary of the results in section 4.1. This suggests that the claims made in the paper are questionable."}, "questions": {"value": "- The two theorems presented seem to be generic and not specific to vulnerability detection. Are these two novel contributions by the authors, or are they taken from related work?\n    \n- What exactly was the source of the initial 30,000 vulnerability patch pairs that were used to create the dataset?\n    \n- What is the intuition behind retaining/discarding samples from the dataset during the filtering process? Can you explain clearly how this was done?\n    \n- How was the “reasoning“ in the dataset generated? was it from a large language model?\n    \n- In section 3.3., from where did you get the information about the reasoning steps/depth (i.e, 3.2 reasoning steps on average) and pattern diversity (i.e., 89 unique vulnerability patterns) in existing datasets and approaches? Any citations?\n    \n- How do you explain that the performance metric values in section 4.1 do not match those in figure 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZlHAr0BEiu", "forum": "rSetFgvtfH", "replyto": "rSetFgvtfH", "signatures": ["ICLR.cc/2026/Conference/Submission21794/Reviewer_SWUP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21794/Reviewer_SWUP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761426513454, "cdate": 1761426513454, "tmdate": 1762941932722, "mdate": 1762941932722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HereCode, a novel model specialised in software vulnerability detection. The authors try to mitigate the key limitation in the current vulnerability detection approach: the reliance on a semantic similarity match causes the models to struggle in distinguishing vulnerable and the minimally patched version. To address this, they propose a novel dataset generation method and a new optimisation framework. For dataset generation, the authors used an Epistemic Uncertainty-Guided Vulnerability Reasoning Generation (EUG-VRG) pipeline and three-stage filtering to curate 22,400 high-quality vulnerability patch pairs. They introduce a new framework, Hierarchical Epistemic Robust Optimization (HERO), to shift models' focus from semantic matching to hierarchical pattern recognition of vulnerability characteristics. Empirical results on HeroCode outperform state-of-the-art(SOTA) vulnerability detection methods on PrimeVul and Sven Benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors propose a novel approach to solving software vulnerability detection with both data generation and optimization paradigm.\n- The paper shows a comprehensive comparative analysis of nine SOTA vulnerability detection approaches on the PrimeVul and SVEN datasets, and their approaches outperform the SOTA tools."}, "weaknesses": {"value": "- This paper is hard to understand. Their key idea is “robust pattern recognition across hierarchical abstractions”. But it is not clear how it is done.\n- Why is filtering important (Section 3.3.1)?\n- During the filtering process, the authors have not provided any exact formula or algorithm for calculating κ(v). How are control flow depth, data dependency chains and semantic abstractions combined? Are the weights of all three the same?\n- How the initial 30000 vulnerability patch pairs were collected? The claim “Starting from an initial pool of 30,000 vulnerability-patch pairs systematically collected from CVE databases and high-quality open-source repositories” seems to be vague. How do the authors determine “high-quality” open source repositories?"}, "questions": {"value": "- What does it mean by “robust pattern recognition across hierarchical abstractions”? (Contribution 2 in the introduction)\n- Reasoning chain synthesis is one of the major contributions of this project. What are the labels as reasoning results? How do you generate labels? LLMs have poor code reasoning capabilities, e.g., for vulnerability detection, how do you validate/trust LLM generated results?\n- Why is filtering important (Section 3.3.1)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Loq5xSAAVp", "forum": "rSetFgvtfH", "replyto": "rSetFgvtfH", "signatures": ["ICLR.cc/2026/Conference/Submission21794/Reviewer_RBRy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21794/Reviewer_RBRy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871241924, "cdate": 1761871241924, "tmdate": 1762941932480, "mdate": 1762941932480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HeroCode, a method to improve LLM-based software vulnerability detection by combining (1) synthetic vulnerability reasoning traces, and (2) a new training objective called HERO (Hierarchical Epistemic Robust Optimization), claimed to encourage robust reasoning across abstraction levels. The authors synthesize ~30k vulnerability–patch pairs with textual “reasoning chains,” filter them using uncertainty measures, and fine-tune existing code LLMs using a robust optimization objective with importance-weighted gradients. Experiments on PrimeVul and SVEN show improvements over several baselines, including large proprietary models, suggesting that smaller open-source models can benefit significantly from this strategy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The problem — distinguishing vulnerable from patched code — is important and timely, and LLMs often struggle with subtle security-critical differences.\n- The idea of augmenting code with structured reasoning about exploitation and remediation is reasonable and motivated by recent work.\n- The empirical results are strong on established datasets, with clear performance gains over baselines.\n- The paper presents ablation studies suggesting contributions from the proposed objective and scheduling mechanism.\n- The focus on vulnerability reasoning rather than pattern matching is conceptually appealing."}, "weaknesses": {"value": "1. **Lack of grounding in security / no motivating code example**\nDespite being a paper about vulnerability detection, the paper does not present any concrete vulnerable vs. patched code example. This is a critical omission, because the core argument is that the method learns to distinguish subtle security logic differences. Without even a simple motivating snippet, the reader cannot evaluate whether the claimed reasoning aligns with real exploit mechanics or patch logic. As a software security paper, it needs at least one real code example and corresponding reasoning trace to show what is actually being learned.\n\n2. **Unclear connection between mathematical formulation and vulnerability detection**\nA large portion of the paper is devoted to formal DRO-style optimization, nested ambiguity sets, and dual forms. However, the connection between these formulations and concrete vulnerability-specific phenomena is never convincingly articulated. The paper reads as if sophisticated optimization machinery was introduced first, and the task was attached to it afterward. Key concepts (e.g., epistemic uncertainty, hierarchical robustness) are described abstractly rather than grounded in security context or failure modes in real vulnerability detection models.\n\n3. **Ultimately still supervised fine-tuning**\n  Despite the novel terminology, the method is functionally supervised fine-tuning of an LLM on synthetic data with a modified loss. The paper does not compare against strong but simpler baselines such as:\n    - plain full-parameter SFT on the same data\n    - LoRA / PEFT fine-tuning on the same data\n    - simpler uncertainty-filtered CoT reasoning datasets\n\n  It is therefore unclear whether the gains arise primarily from the data curation or the proposed optimization objective.\n\n4. **Limited transparency on training practicality**\nThe paper does not report training compute, memory footprint, or parameter update strategy in detail (e.g., full LLM fine-tuning vs partial). Without compute reporting or efficiency discussion, it is hard to judge practical adoption.\n\n5. **Synthetic reasoning quality never demonstrated**\nThe paper claims high-quality reasoning sequences but never shows even a single example of the generated reasoning text, nor evaluates its correctness. This undermines the central narrative that the model is “learning to think like a security analyst.”"}, "questions": {"value": "1. Please include one real vulnerable–patched code pair with your generated reasoning and demonstrate how HERO training changes the prediction.\n2. How much of the performance gain remains if you use plain SFT on your filtered synthetic data?\n3. Why not compare against LoRA / PEFT baselines? Would your robust objective still offer an advantage in a parameter-efficient setting?\n4. What is the training compute budget (tokens, GPU hours, batch size, sequence length)?\n5. How do you ensure that the synthetic explanations are correct and security-meaningful, vs plausible but incorrect?\n6. Do the dual-robustness formulations correspond to any specific vulnerability reasoning abstractions (e.g., control-flow validation layers, memory safety hierarchy), or is the mapping conceptual rather than grounded in exploit logic?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qsEqw7b7Fc", "forum": "rSetFgvtfH", "replyto": "rSetFgvtfH", "signatures": ["ICLR.cc/2026/Conference/Submission21794/Reviewer_pbrr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21794/Reviewer_pbrr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971926109, "cdate": 1761971926109, "tmdate": 1762941932155, "mdate": 1762941932155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HeroCode, a model that enhances LLMs’ ability to detect software vulnerabilities by training them to understand the reasoning behind security flaws and fixes. It employs a Hierarchical Epistemic Robust Optimization (HERO) architecture to promote deep comprehension of security patterns rather than surface-level code similarities. Experiments show that when integrated with Qwen2.5 Coder-7B-Instruct, HeroCode achieves 61% accuracy on PrimeVul, surpassing GPT-4’s 52%."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 4}, "strengths": {"value": "1. Addresses an important problem of improving LLMs' ability to comprehend security vulnerabilities. The key idea is an epistemic uncertainty-guided approach called HERO that synthesizes detailed reasoning chains explaining both how vulnerabilities can be exploited and why specific patches prevent such exploitation. \n\n2. Develops a reasoning-rich dataset comprising 22,400 carefully crafted instances filtered from 30,000 initial candidates of vulnerability-patch pairs systematically collected from CVE databases and open-source repositories. The dataset will be made public.\n\n3. Demonstrates significant performance boost with different pretrained open-source models like Qwen-2.5-7B, Llama-3.1-8B, and StarCoder-2-7B.  With Qwen-2.5-7B, the method attains 60.66% accuracy on PrimeVul, an 8.42 percentage point improvement over the previous best result of 52.24% achieved by GPT-4. The performance gains are even more pronounced on SVEN."}, "weaknesses": {"value": "1. My biggest concern with the paper is the presentation. Section 3 starts out with a lot of formal equations without any explanation or intuition.  Algorithm 1 nor Figures 1 and 2 nor various theorems are cited in the text, nor explained. As such, it is extremely difficult to understand and evaluate the merits of the proposed approach.\n\n2. The method seems to have a unique strength in that parts of it are used both for synthesizing reasoning chains in the training data and subsequently to train a base model. But it is not clear to me what those parts are. A figure that illustrates the end-to-end pipeline and is explained in the text would be helpful.\n\n3. I am unsure about how the resulting model is applied at test time. Does it itself synthesize the reasoning chain and then make a prediction?  Or does it directly make a prediction?\n\n4. Did you apply the model to out-of-distribution data? For instance, training the model on PrimeVul and applying it to SVEN, or perturbing the test set in some other way (e.g. injecting dead code, changing variable names, etc. without changing the program semantics)?"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "L3V4RvuCNT", "forum": "rSetFgvtfH", "replyto": "rSetFgvtfH", "signatures": ["ICLR.cc/2026/Conference/Submission21794/Reviewer_1t6Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21794/Reviewer_1t6Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762173528025, "cdate": 1762173528025, "tmdate": 1762941931829, "mdate": 1762941931829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}