{"id": "wOaXd83Yio", "number": 15525, "cdate": 1758252307066, "mdate": 1759897301270, "content": {"title": "Personalized Visual Representation Alignment for Generative Multimodal Recommendation", "abstract": "With the development of Vision-Language Models (VLMs) for multimodal understanding, recommender systems have increasingly leveraged them to process heterogeneous sources of user-interacted items for recommendation. By fine-tuning VLMs on user interaction data, prior works have adapted these models to capture user preferences, enabling personalized multimodal recommendation. Despite these advances, however, we identify two key limitations: 1) the visual features directly extracted by vision encoders (e.g., CLIP) are insufficient for capturing personalized user preferences, as such encoders are generally trained for generic visual perception rather than capturing user-specific preferences; and 2) existing VLM-based methods often underutilize visual features of user-interacted items in later LLM layers, relying instead on textual descriptions for recommendation—an unexpected bias that diminishes the contribution of visual features. To address these two limitations, we propose PerVRA, a VLM-based recommendation model consisting of a Personalized Visual Representation Learning (PVRL) module and a Personalized Multimodal Alignment (PMA) module. Specifically, we employ dual contrastive learning, where each module is equipped with its own contrastive objective: The PVRL module learns personalized visual features from user interaction history, while the PMA module enhances the contribution of visual features to the VLMs by explicitly aligning them with text features. Extensive experiments on real-world Amazon and H\\&M Fashion datasets demonstrate that PerVRA consistently outperforms strong VLM-based methods over diverse personalized tasks. Moreover, our ablation studies show that addressing these two limitations is critical for building effective VLM-based recommender systems.", "tldr": "We propose personalized multimodal representation learning for personalized recommendation.", "keywords": ["Sequential Recommendation", "Multimodal RecommeXx"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7586118e5e8fc6acca24d533081364b615e0931b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the PerVRA framework for VLM-based recommendation systems, employing dual contrastive learning to learn personalized visual representations and align them with textual features. The paper identifies key limitations of existing methods, proposes reasonable solutions, and validates effectiveness across multiple datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. Figure 1 provides intuitive visual evidence that text-only model outperforms multimodal variant.\n\nS2. The paper clearly identifies two key issues in VLM-based recommendation systems, the motivation is  convincing.\n\nS3. The PVRL and PMA modules have clear division of labor, optimizing visual space and multimodal alignment respectively."}, "weaknesses": {"value": "W1. The contrastive learning in recommender systems is not novel, and the core dual contrastive learning idea lacks originality.\n\nW2.  No analysis of λ1 and λ2 impact, Table 4 only tests complete removal.\n\nW3. Design of Equation 9 lacks theoretical basis, and it may lead to negative loss."}, "questions": {"value": "Q1. Suggest clarifying why this form is used instead of standard InfoNCE in Eq.4.\n\nQ2. Why not contrast lt with lt+ in Eq.7? What are the advantages of this design?\n\nQ3. The  \"PerVRA\" and \"PerVLA\" are used mixed in multiple places, e.g., Equation 8.\n\nQ4. On H&M dataset in Table3, PerVRA's text-only evaluation (HR@5=0.079) is even slightly higher than multimodal evaluation (HR@5=0.078). This contradicts core motivation that \"visual information is important\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RibKGNzBnj", "forum": "wOaXd83Yio", "replyto": "wOaXd83Yio", "signatures": ["ICLR.cc/2026/Conference/Submission15525/Reviewer_zYZi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15525/Reviewer_zYZi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834163718, "cdate": 1761834163718, "tmdate": 1762925807927, "mdate": 1762925807927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PerVRA, a personalized visual representation alignment framework addressing two key issues in VLM-based recommendation: (1) frozen vision encoders lack user-specific preference modeling; (2) VLMs underutilize visual signals and over-rely on text. PerVRA consists of PVRL and PMA modules with dual contrastive learning, treating user history as both positives and hard negatives. The training objective combines task loss and contrastive losses without adding inference cost. Experiments on multiple datasets show strong gains over SOTA baselines and robustness to missing modalities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes a novel dual-role use of user history (as both positive and negative) for contrastive learning at both visual (PVRL) and textual (PMA) levels, moving beyond standard image-text matching or InfoNCE.  \n- Outperforms UniMP in sequential recommendation across Amazon/H&M/Netflix/Book, and shows improvements across search, selection, preference prediction, and explanation tasks; robust under missing modality settings.  \n- Objective functions and pipeline are clearly illustrated, with t-SNE plots supporting the method’s effect on latent structure.  \n- Demonstrates practical value for making vision \"count\" in unified VLM frameworks for personalized recommendation, without adding inference cost."}, "weaknesses": {"value": "1. Results are mostly single-run without variance/confidence intervals or t-tests, making it hard to judge reproducibility, especially for low-score datasets. Recommend reporting mean ± std over 3–5 seeds.  \n2. Hyperparameters like λ₁, λ₂, τ, and MLP size are fixed but not analyzed. The effect of sampling ratio and strategy (e.g., history as hard negatives) is also unclear.  \n3. Equation (9) introduces a repetition penalty term, but its intuition and difference from standard diversity constraints are insufficiently explained.  \n4. The mechanism for selecting historical items as negatives is not fully detailed (e.g., sampling ratio, subsampling, temporal decay).  \n5. Although inference cost is equal, training conditions may differ (e.g., batch size, resolution, cleaning), possibly leading to unfair baseline comparisons.  \n7. The method is validated on CLIP ViT-L + 3B LLM only. It’s unclear how it generalizes across backbones, model sizes, or alignment layers."}, "questions": {"value": "1. What is the sampling strategy and N for using history items as positives/negatives in UCL/RCL? Is there any N-sweep or heatmap to show trade-offs?  \n2. How does the repetition penalty differ from typical diversity or de-duplication rules in generation? Any ablation before/after applying it?  \n3. In missing modality experiments, does randomly dropping half of the images during training reflect real-world distributions? Are text-only test sets matched in candidate composition?  \n4. PMA aligns visual to the final LLM layer. Has the method tried aligning at cross-attention or intermediate layers (multi-layer distillation)?  \n5. With 4×A6000 and 10 epochs, how does training cost compare to UniMP? If training cost is equalized, are the gains still significant?  \n6. Amazon Beauty shows strong gains, but what about visually weak or long-tail domains (e.g., books/comments)? Any cross-domain fine-tuning or zero-shot generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QzpY44ldZg", "forum": "wOaXd83Yio", "replyto": "wOaXd83Yio", "signatures": ["ICLR.cc/2026/Conference/Submission15525/Reviewer_SWiu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15525/Reviewer_SWiu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762186383368, "cdate": 1762186383368, "tmdate": 1762925806968, "mdate": 1762925806968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper points out that there are two key problems in current VLM recommendation tasks: 1) the features extracted by visual encoders are universal perception-oriented and lack personalization;2) visual features are underestimated in the LLM layer, and recommendation systems rely too much on textual information. To this end, PerVRA has introduced two modules: PVRL (Personalized Visual Representation Learning) and PMA (Personalized Multimodal Alignment), which respectively enhance the personalized expression of visual features through double contrast learning goals and establish more effective alignment between vision and text."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The motivation of VLM's bias in recommendations is interesting.\n+ The evaluation is extensive, and the experimental results look promising."}, "weaknesses": {"value": "+ Existing VLM-based methods often underestimate visual features of user-interacting items in later LLM layers, which is only an empirical inference and lacks rigorous proof.\n+ The reason behind the design choice of the method is not clearly explained.\n+ There are some presentation errors in the paper, such as line 329.\n+ PerVRA and PerVLA alternately appear, which makes it confusing.\n+ Lack of comparison with other methods that focus on personalized visual modeling limits the method's innovative evaluation of visual personalization"}, "questions": {"value": "+ Why Text-only is better than Text+image is contrary to the conclusion in UniMP.\n+ Why can dual contrastive learning objectives balance the contributions of visual and textual features and avoid bias?\n+ Multiple contrastive losses and modules are introduced during the training phase, and the actual training costs are not detailed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GjkVVgxROI", "forum": "wOaXd83Yio", "replyto": "wOaXd83Yio", "signatures": ["ICLR.cc/2026/Conference/Submission15525/Reviewer_189T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15525/Reviewer_189T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762224172589, "cdate": 1762224172589, "tmdate": 1762925806565, "mdate": 1762925806565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address two limitations in existing VLM-based recommendation tasks: (1) visual features are insufficient for capturing personalized user preferences, and (2) visual features are underutilized by current VLM recommendation models. To tackle these issues, the authors propose PerVRA, which consists of a Personalized Visual Representation Learning (PVRL) module and a Personalized Multimodal Alignment (PMA) module, both built upon contrastive learning. The experiments are conducted using the UniMP model and the Amazon review dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The task is clearly defined, and the limitations are explicitly described with empirical evidence.\n2. The paper is easy to read and understand.\n3. PerVRA shows significant improvements over baseline approaches."}, "weaknesses": {"value": "1. The main concern is the generalization of this method. Since PerVRA has been specifically designed for UniMP, it is unclear whether it can be applied to or remain effective for other VLM-based recommendation models. For example, if the vision encoder and text encoder are already highly aligned in a VLM, would PerVRA still provide improvements?\n2. Figure 1(a) appears to be inconsistent with Table  3. In Figure 1(a), the text-only setting outperforms the multimodal setting, while in Table 3, the text-only setting performs worse than multimodal.\n3. There are no hyperparameter sensitivity experiments. As such, it is unclear how changes to $\\lambda_1$ and $\\lambda_2$ would affect the results.\n4. Several typos exist. For instance, in the OpenReview keywords, “Multimodal RecommeXx” should be corrected. In Section 4.1, Line 329, “Book-Crossing () datasets” appears incomplete."}, "questions": {"value": "If a VLM has a strong text encoder, and the visual encoder that is highly aligned with the text encoder, the problem described by the authors, such as “if a user prefers kitchen-related items, objects like knives and frying pans should be embedded closer together rather than treated as distinct classes”, may not occur, since knives and frying pans would already be close in the semantic space. In such a scenario, would the problem that PerVRA aims to address still exist?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uHviC0JEjw", "forum": "wOaXd83Yio", "replyto": "wOaXd83Yio", "signatures": ["ICLR.cc/2026/Conference/Submission15525/Reviewer_xxnQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15525/Reviewer_xxnQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762227849767, "cdate": 1762227849767, "tmdate": 1762925806119, "mdate": 1762925806119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}