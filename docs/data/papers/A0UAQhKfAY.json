{"id": "A0UAQhKfAY", "number": 18444, "cdate": 1758287850398, "mdate": 1759897103121, "content": {"title": "Meta-LoRA: Meta-Learning LoRA Components for Domain-Aware ID Personalization", "abstract": "Personalizing text-to-image models to create subject-specific content from limited images is a critical challenge in generative AI. Current methods force a difficult choice between slow, high-fidelity fine-tuning and fast, tuning-free approaches that can struggle with identity details and often replicate the reference pose. We introduce Meta-Low-Rank Adaptation (Meta-LoRA), a novel framework that enhances LoRA-based personalization by meta-learning a domain-specific prior for human identity. Our key insight is to learn a shared, low-dimensional manifold of general identity features from multiple subjects, which provides a powerful foundation for rapidly adapting a small, identity-specific component to a new person from a single image. To enable a rigorous evaluation that addresses pose-copying biases, we introduce Meta-PHD, a diverse benchmark dataset, and R-FaceSim, a robust new similarity metric. On this benchmark, Meta-LoRA achieves a 1.67x faster convergence than standard LoRA while reaching superior identity fidelity. Our findings show that Meta-LoRA not only outperforms its direct baseline but also achieves a more effective balance between identity preservation and prompt adherence than state-of-the-art tuning-free methods. More broadly, our work demonstrates that meta-learning provides a practical and efficient pathway for adapting large generative models, bridging the gap between existing fine-tuning and conditioning-based paradigms. The code, model weights, and dataset will be released publicly upon acceptance.", "tldr": "We introduce Meta-Low-Rank Adaptation (Meta-LoRA), a novel framework that leverages meta-learning to encode domain-specific priors into LoRA-based identity personalization.", "keywords": ["meta-learning", "lora", "generative models", "id personalization", "flux"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31570cf31c2c4f8b571a9250b451c7c61bd51f69.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces Meta-LoRA, a fine-tuning-based approach designed to enhance the fidelity of personalized text-to-image models. By employing a two-stage training paradigm, Meta-LoRA separates shared domain priors from compact, subject-specific components. It accelerates adaptation by 1.67x compared to the standard LoRA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-presented and easy to follow. The methodology is described with sufficient clarity, and the figures effectively illustrate the proposed architecture.\n- The decomposition-based LoRA design, separating shared and identity-specific parameter, is a interesting architectural choice that offers a degree of modularity."}, "weaknesses": {"value": "- The proposed method requires both large-scale pre-training and per-identity fine-tuning, inheriting the drawbacks of both paradigms: high computational cost, extensive data requirements, and complex deployment logistics.\n- The experimental setup is flawed, as the baselines used for comparison are predominantly zero-shot or non-fine-tuned methods. This comparison is inherently unfair, as the proposed method leverages identity-specific training data while the baselines do not. A more rigorous evaluation should include strong fine-tuning-based competitors.\n- The paper lacks discussion of overall efficiency, both during training and, critically, at inference time."}, "questions": {"value": "- In what real-world scenarios does the authors envision this pre-training + ID-specific tuning pipeline being preferable to existing single-stage personalization methods?\n- How does the proposed method compare to baselines in terms of computational cost during inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UuLtLmKDh6", "forum": "A0UAQhKfAY", "replyto": "A0UAQhKfAY", "signatures": ["ICLR.cc/2026/Conference/Submission18444/Reviewer_av9Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18444/Reviewer_av9Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761486481410, "cdate": 1761486481410, "tmdate": 1762928141946, "mdate": 1762928141946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, they propose Meta-LoRA, a new framework for the personalization training of text-to-image models. Unlike existing LoRA method, they separate the LoRA parameters into a shared up-LoRA and an identity-specific down-LoRA, designed for training on domain-specific datasets, such as face. This approach achieves faster convergence and a better quality-identity trade off with less training data compared to standard LoRA training. Furthermore, they propose a controlled metric, R-FaceSim, considering that the existing FaceSim metric fails to capture fine-grained identity differences and suffers from identity leakage between training and inference. Overall, the proposed method demonstrates superior trade-offs between quality and identity preservation compared to various existing LoRA training methods and pre-trained models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**S1**. The paper is well-structured and clearly presented, with a solid experimental evaluation. The paper is well-organized and easy to follow.\n\n**S2**. The proposed method demonstrates strong quantitative and qualitative performance, and achieves faster test-time adaptation speed compared to existing approaches.\n\n**S3**. The proposed metric and dataset are well-motivated and sound, and they have the potential to make a valuable contribution to the research community."}, "weaknesses": {"value": "**W1**. The proposed method still requires a curated face dataset and a large number of test-time adaptation iterations, which limits its practical applicability.\n\n**W2**. (This is my major concern) This shared-LoRA concept, in a broader sense, differs primarily in terms of architecture but is conceptually similar to encoder-based methods (InstantID, IP-adapter...). Therefore, a direct comparison with encoder-based approaches incorporating test-time adaptation (e.g., LoRA) is essential.\n\n**W3**. In recent research, human personalization tasks have been extended to handle multiple identities in a single generated image. It is important to include experiments on multi-human scenarios or LoRA merging techniques to evaluate the method’s scalability and generalization.\n\nIf my concerns are addressed, I would be happy to reconsider the score."}, "questions": {"value": "**Q1**. Similar to W2, are there any results showing the training speed or metric performance when attaching LoRA to an encoder-based method? Such a setting would likely enable a fairer comparison between the approaches.\n\n**Q2**. Have the authors evaluated the method on non-human domains? While the method itself does not appear to be explicitly face-specific, it would be valuable to examine whether it generalizes to other domains or can be applied to multiple domains simultaneously."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WZugHCW1rH", "forum": "A0UAQhKfAY", "replyto": "A0UAQhKfAY", "signatures": ["ICLR.cc/2026/Conference/Submission18444/Reviewer_zbHs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18444/Reviewer_zbHs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761671414382, "cdate": 1761671414382, "tmdate": 1762928141437, "mdate": 1762928141437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the trade-off issue between \"slow but high-fidelity fine-tuning\" and \"fast but low-quality identity detail preservation with a high tendency to replicate reference poses\" in the personalization of text-to-image models, this paper proposes the Meta-LoRA framework. This framework meta-learns domain-specific priors for human identity: first, it learns a shared, low-dimensional manifold of general identity features from multiple subjects, and then rapidly adapts identity-specific LoRA Mid (LoM) and LoRA Up (LoU) components from a single image. Experiments have demonstrated the effectiveness of this method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work establishes a rigorous and reliable evaluation system to solve inconsistencies in existing assessments.\n2. This work designs an efficient two-stage training paradigm with strong parameter efficiency.\n3. This proposed metdho demonstrate data efficiency, comparing to state-of-the-art methods, Meta-LoRA’s meta-training dataset is only 0.035%–18.75% the size of baselines.\n4. This paper is well-written and easy to follow."}, "weaknesses": {"value": "1. Meta-LoRA still requires a dedicated training step for each identity, unlike tuning-free feed-forward conditioning methods that avoid test-time fine-tuning entirely.\n2. Lack of novelty.\n3. The meta-training dataset suffers from significant gender imbalance, with 1,050 female subjects compared to only 400 male subjects, such an imbalance may introduce implicit biases in the learned domain priors and limits the method’s representativeness across diverse groups.\n4. Direct comparative evaluation with leading methods (e.g., InstantID, PhotoMaker) is compromised by inconsistent base diffusion models. Meta-LoRA is built on FLUX.1-dev, while InstantID and PhotoMaker use SD-XL or SD-XL Lightning."}, "questions": {"value": "1. Meta-LoRA still requires a dedicated training step for each identity, unlike tuning-free feed-forward conditioning methods that avoid test-time fine-tuning entirely.\n2. Lack of novelty.\n3. The meta-training dataset suffers from significant gender imbalance, with 1,050 female subjects compared to only 400 male subjects, such an imbalance may introduce implicit biases in the learned domain priors and limits the method’s representativeness across diverse groups.\n4. Direct comparative evaluation with leading methods (e.g., InstantID, PhotoMaker) is compromised by inconsistent base diffusion models. Meta-LoRA is built on FLUX.1-dev, while InstantID and PhotoMaker use SD-XL or SD-XL Lightning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bXxWN6vRX2", "forum": "A0UAQhKfAY", "replyto": "A0UAQhKfAY", "signatures": ["ICLR.cc/2026/Conference/Submission18444/Reviewer_tXE3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18444/Reviewer_tXE3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901528641, "cdate": 1761901528641, "tmdate": 1762928141007, "mdate": 1762928141007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Meta-LoRA for personalized text-to-image diffusion models.  Unlike standard LoRA-based methods (e.g., DreamBooth) that learns a LoRA for each subject, Meta-LoRA consists of LoRA Meta-Down for identity-independent domain priors and LoRA Up Blocks for identity-specific features. Meta-LoRA first meta-trains shared “Meta-Down” components capturing domain priors (e.g., general facial features), and then personalizes only small “LoRA Mid” and “LoRA Up” modules from a single reference image. This yields faster convergence (1.67× speedup), improved identity fidelity, and efficient adaptation. \nAdditionally, this paper introduces a new benchmark for personalized image generation, called Meta-LoRA Personalization of Humans Dataset (Meta-PHD). Moreover, this paper points that current Face similarity metric based on face embedding lacks the evaluation of fine-grained details and contains bias towards generating images with the same pose and gaze. To this end, this paper proposes Robust Face similarity (R-FaceSim), that calculates the similarity between generated images with other images of the same person, rather than the input reference image. Experimental results suggests the proposed method outperforms baseline models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed two-stage meta learning method is novel and interesting, as it decouple general facial features and identity-specific feature. Additionally, it has better efficiency with faster convergence speed. \n\n2. The new personalized image generation benchmark is helpful for the community. \n\n3. The concern of current face similarity calculation is reasonable, and the proposed solution (R-FaceSim) seems a practical solution."}, "weaknesses": {"value": "1. Given that a new benchmark is proposed, more evaluation on other methods are preferred. Ideally, this paper could presents a leaderboard for all types of methods. The current submission only evaluates a few methods. \n\n2. This paper only evaluates Meta-LoRA on Flux.1-dev model. The generalization ability of the proposed Meta-LoRA is in doubt. It would be better if this paper could demonstrate that Meta-LoRA is still effective over the standard LoRA on other foundation models, such as SDXL."}, "questions": {"value": "This paper proposes a novel method and a new benchmark for personalized image generation. To further improve this paper: \n\nFor the method side, this paper needs demonstrate its effectiveness on other foundation models. \n\nFor the benchmark side, this paper needs to present more results of other methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XC1FFyCql4", "forum": "A0UAQhKfAY", "replyto": "A0UAQhKfAY", "signatures": ["ICLR.cc/2026/Conference/Submission18444/Reviewer_pYkt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18444/Reviewer_pYkt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762126214499, "cdate": 1762126214499, "tmdate": 1762928139028, "mdate": 1762928139028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}