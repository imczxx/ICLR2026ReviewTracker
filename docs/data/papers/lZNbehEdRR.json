{"id": "lZNbehEdRR", "number": 5072, "cdate": 1757842368573, "mdate": 1759897996491, "content": {"title": "Replacement Learning: Training Neural Networks with Fewer Parameters", "abstract": "Traditional End-to-End deep learning models typically enhance feature representation capabilities by increasing network depth and complexity. While such an approach improves performance, it inevitably leads to issues such as parameter redundancy and inefficient resource utilization, which become increasingly pronounced as the network deepens. Existing methods have attempted to alleviate these problems by skipping or removing redundant layers. However, they often rely on complex manual designs, which may result in performance degradation, increased computational costs, and reduced memory efficiency.\nTo address these challenges, we propose a novel training paradigm termed Replacement Learning. This method selectively removes certain layers from the network and substitutes them with additional computing layers in an efficient and automated manner, thereby compensating for the potential performance loss caused by layer removal. Specifically, a computing layer is inserted between the neighboring layers of the removed layer, and it utilizes parameters from the adjacent layers to construct a transformed parameter representation through a simple and efficient learnable block. This transformed representation is then used to perform additional computation on the output of the preceding layer, yielding the final output passed to the subsequent layer. Furthermore, to accommodate architectural variations such as feature map sizes and channel dimensions in different network types, we design a tailored, lightweight learnable block accordingly. Replacement Learning leverages the contextual flow of information between adjacent layers to eliminate unnecessary computation, significantly reducing computational complexity, saving GPU memory usage, and accelerating training. More importantly, it achieves a balanced integration of historical context and newly introduced features, thereby enhancing the overall model performance. We validate the effectiveness of Replacement Learning on five benchmarks—CIFAR-10, STL-10, SVHN, ImageNet, and COCO—across classification and detection tasks using both CNNs and ViTs architectures. Results demonstrate that our method not only significantly reduces the number of network parameters, shortens training time, and lowers memory consumption, but also surpasses traditional End-to-End trained models in performance.", "tldr": "", "keywords": ["Efficient Training Method"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b8399195c7b4153ef755be5a098678e2a0c0455d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper describes a process of replacing a neural network layer with lightweight process for reducing the computational complexity and memory requirements called Replacement Learning. By applying this process every k layers on a neural network architecture, a lighter one is obtained. This process is demonstrated for two types of layers, convolutional and transformer. Experimental results on four public datasets show this benefit compared to the full network architecture trained end-to-end by BP. \n\n\nThe same effect has been achieved with existing methodologies, parameter pruning being the most dominant one. If the argument is that the proposed method does not need extensive training of the entire architecture, then pruning adapters would also satisfy this requirement. The paper does not provide any comparisons with the actual competing ideas and methods. Moreover, the proposed replacement process does not lead to an exact replacement of the computations/representation done by the original network. In practice this is similar to neural architecture search ideas where two neural layers are compared and one is chosen (however, here there is one choice which is always taken and the layer to be replaced is pre-specified irrespectively of its representation power)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The proposed process leads to similar performance compared to the original neural network architecture.\n - A reduction in computations and memory is obtained."}, "weaknesses": {"value": "- Similar (or even better) effects on computation and memory reductions have been achieved with other approaches (parameter pruning, pruning adapters).\n - No comparisons with the actual competing approaches/methods is provided.\n - Claims in the paper seem to be unfounded (they are not shown to hold neither theoretically, nor experimentally). E.g. \"The design notably enhances the network’s capacity to capture local features in shallow layers and global representations in deeper layers, thereby promoting a more effective integration of low-level and high-level features.\"\n - In the experiments, no pre-trained models are used. Instead, the authors train all networks from scratch. This seems odd, as the main benefit from using such an approach would be to use high-performing pre-trained models and highly reduce their computations and memory while retaining their performance. \n - The method selects equally-spaced layers in the network architecture without trying to preserve highly-capable layers and remove redundant ones."}, "questions": {"value": "- How does the method compare with the actual competition?\n - How to choose layers to be removed?\n - What is the effect of using high-performing pre-trained models in the reported gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "V7whDsJeFz", "forum": "lZNbehEdRR", "replyto": "lZNbehEdRR", "signatures": ["ICLR.cc/2026/Conference/Submission5072/Reviewer_JPWu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5072/Reviewer_JPWu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634900897, "cdate": 1761634900897, "tmdate": 1762917855991, "mdate": 1762917855991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new method called Replacement Learning (RepL), aimed at improving the efficiency and performance of neural networks by reducing computational overhead. The method involves selectively removing layers from a deep neural network and replacing them with lightweight computing layers. These computing layers integrate information from adjacent layers to preserve performance while reducing the number of parameters, memory usage, and training time. The authors demonstrate the effectiveness of RepL on various benchmarks, including CIFAR-10, STL-10, SVHN, ImageNet, and COCO, using both CNNs and Vision Transformers (ViTs). Experimental results show that RepL significantly reduces GPU memory consumption and training time while surpassing traditional End-to-End training methods in terms of performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The concept of replacing traditional layers with learnable computing layers is novel, offering an alternative approach to optimizing neural network performance.\n\nQuality: The experimental setup is robust, and the results on several datasets are strong. RepL improves resource efficiency while maintaining or enhancing model accuracy, which is a significant achievement in deep learning.\n\nClarity: The paper is generally well-written, with clear explanations of the method and experimental results. The use of figures to illustrate results helps clarify the impact of RepL.\n\nSignificance: If the method can be generalized to other domains like NLP, it could have broad implications for resource-efficient deep learning models."}, "weaknesses": {"value": "1 Performance Degradation When Removing Too Many Layers\nThe method’s reliance on removing layers to save resources does raise a concern: if you remove too many layers, there’s a risk of significant performance loss. While RepL does a good job of maintaining performance by replacing layers with computing blocks, the trade-off between saving memory and keeping performance high isn’t always clear. It would be helpful to see more detailed experiments exploring just how much layer removal is optimal before the network starts to suffer in terms of accuracy.\n2 Complexity of the Approach\nThe method of fusing parameters from neighboring layers sounds great in theory, but it does introduce complexity. The learnable blocks that are inserted between layers to fuse information add a layer of overhead that could potentially complicate the model. For instance, choosing the right interval k for layer removal and figuring out how to balance the parameters is not trivial. More guidance on how to tune these parameters or handle different architectures could make RepL more user-friendly and widely applicable.\n3 Limited Comparison with Other Techniques\nThe paper compares RepL to traditional End-to-End training and a few related methods like Skip-Attention, but it doesn’t dive deeply into other popular methods that also aim to reduce computational cost, such as Stochastic Depth or Checkpointing. A broader comparison with these methods would give a better sense of where RepL stands in the landscape of existing solutions. Without this, it’s hard to say if RepL offers a real advantage in all scenarios or if it’s just a slightly better approach for the specific benchmarks tested."}, "questions": {"value": "1 How might the method perform with more complex models, such as those used in NLP or multimodal learning?\n\n2 Could there be a risk of information loss if too many layers are removed, especially for certain tasks that require deep feature representations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ezlNMkUjyt", "forum": "lZNbehEdRR", "replyto": "lZNbehEdRR", "signatures": ["ICLR.cc/2026/Conference/Submission5072/Reviewer_yhWM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5072/Reviewer_yhWM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817317863, "cdate": 1761817317863, "tmdate": 1762917855417, "mdate": 1762917855417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Replacement Learning (RepL), a training paradigm that aims to reduce the parameter and computational cost of deep neural networks while maintaining or improving performance. Instead of training all layers end-to-end, RepL removes every k-th layer and inserts a lightweight learnable computing layer that synthesizes parameters from the preceding and succeeding layers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of layer-replacement is intuitive and can be applied to various architectures.\n2. Experiments across CNNs and ViTs demonstrate consistent memory and time savings without hurting accuracy.\n3. The paper is clearly written."}, "weaknesses": {"value": "1. The paper does not compare against recent state-of-the-art compression methods in pruning, knowledge distillation, or neural architecture search. Modern approaches such as [1,2,3] have demonstrated substantial accuracy–efficiency trade-offs on both CNNs and Transformers. In contrast, RepL retrains networks from scratch, overlooking the practical scenario where compressing pre-trained models is more feasible and widely adopted.\n\n2. RepL primarily enhances training efficiency, not deployment efficiency. At inference, the replacement layers (e.g., $1 \\times 1$ convolutions or lightweight fusers) remain part of the model and still incur computational cost. Thus, the approach may not yield meaningful speed-ups or compression benefits at inference compared to pruning or distillation methods.\n\n3. The paper repeatedly emphasizes improved accuracy, yet the reported gains are small and often within normal training variance or achievable through modest hyperparameter tuning; there is no statistical test of significance. \n\n[1] Fang, Gongfan, et al. \"Depgraph: Towards any structural pruning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.\n\n[2] Dong, Peijie, Lujun Li, and Zimian Wei. \"Diswot: Student architecture search for distillation without training.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[3] Li, Guihong, et al. \"Zero-shot neural architecture search: Challenges, solutions, and opportunities.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 46.12 (2024): 7618-7635."}, "questions": {"value": "Please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rGMEyA8Xm7", "forum": "lZNbehEdRR", "replyto": "lZNbehEdRR", "signatures": ["ICLR.cc/2026/Conference/Submission5072/Reviewer_k8jc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5072/Reviewer_k8jc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852842835, "cdate": 1761852842835, "tmdate": 1762917854992, "mdate": 1762917854992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Replacement Learning (RepL): remove every k-th block and insert a lightweight learnable “computing layer” that synthesizes a replacement operator from adjacent layers’ weights. For CNNs, the synthesized operator is a fused 1×1 conv built by channel-mode mixers; for ViTs, the layer comprises two d×d linear maps parameterized by scalars (α, β) that linearly combine the previous/next block’s attention and MLP weights. A global forward rule formalizes the execution when a site is replaced, and an operator ledger details what is removed/added. Reported benefits are reduced parameters/FLOPs/activation memory with similar or better accuracy across several vision benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear mechanism & implementable details for CNNs/ViTs (shapes, synthesis, forward/backward).\n- Runs at inference with real savings (not only a training trick).\n- Simple instantiations: channel-mode synthesis for CNNs; two-parameter (α,β) span for ViTs.\n- Strong Experimental Results."}, "weaknesses": {"value": "- Fixed periodic removal; no adaptivity.\nRemoval set is hard-coded to every k-th site: $F = { i | i mod k = 0, i < n }$ (Eq. 2). Default k = 4.\n- ViT expressivity limited to a 2-D neighbor span. By construction, the replacement lies in span ${A_{i−1}, A_{i+1}}$ × span{$M_{i−1}, M_{i+1}$}; specifically, $A^b_i = α_i A_{i−1} + β_i A_{i+1}, M^c_i = α_i M_{i−1} + β_i M_{i+1}$. (Eqs. 9–10; 2-D span statement in Appendix).\n- Approximation/gradient bias assumed bounded; error accumulation acknowledged. Theory uses a bounded-bias assumption with gradient deviation bound $‖∇_θℓ_b − ∇_θℓ‖ ≤ L·H_max·ε$, and notes that multi-replacement discrepancy grows at most linearly with the number r of replacements (non-expansive case)."}, "questions": {"value": "- Adaptive selection: Can you evaluate an importance-aware or learned removal policy versus the fixed periodic rule?\n- Span diagnostics (ViT): How close are removed blocks to span ${A_{i−1}, A_{i+1}}$ and span ${M_{i−1}, M_{i+1}}$ (principal angles or reconstruction error) across depth?\n- Bias in practice: Please report empirical proxies for $ε (e.g., ‖F_b − F‖ and ‖∇ℓ_b − ∇ℓ‖)$  as the number of replacements increases, to validate $‖∇_θℓ_b − ∇_θℓ‖ ≤ L·H_max·ε.$"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ggqLd6OWXo", "forum": "lZNbehEdRR", "replyto": "lZNbehEdRR", "signatures": ["ICLR.cc/2026/Conference/Submission5072/Reviewer_UEBC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5072/Reviewer_UEBC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929914756, "cdate": 1761929914756, "tmdate": 1762917854612, "mdate": 1762917854612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}