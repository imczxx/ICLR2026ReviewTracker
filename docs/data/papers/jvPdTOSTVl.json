{"id": "jvPdTOSTVl", "number": 23420, "cdate": 1758343556729, "mdate": 1763656849125, "content": {"title": "Culture In a Frame: C$^3$B as a Comic-Based Benchmark for Multimodal Culturally Awareness", "abstract": "Cultural awareness capabilities has emerged as a critical capability for Multimodal Large Language Models (MLLMs). However, current benchmarks lack progressed difficulty in their task design and are deficient in cross-lingual tasks. Moreover, current benchmarks often use real-world images. Each real-world image typically contains one culture, making these benchmarks relatively easy for MLLMs. Based on this, we propose C$^3$B ($\\textbf{C}$omics $\\textbf{C}$ross-$\\textbf{C}$ultural $\\textbf{B}$enchmark), a novel multicultural, multitask and multilingual cultural awareness capabilities benchmark. C$^3$B comprises over 2000 images and over 18000 QA pairs, constructed on three tasks with progressed difficulties, from basic visual recognition to higher-level cultural conflict understanding, and finally to cultural content generation. We conducted evaluations on 11 open-source MLLMs, revealing a significant performance gap between MLLMs and human performance. The gap  demonstrates that C$^3$B poses substantial challenges for current MLLMs, encouraging future research to advance the cultural awareness capabilities of MLLMs.", "tldr": "", "keywords": ["Multimodal Large Language Models", "Cultural Awareness", "Cross-Cultural Benchmark", "Comics", "Multilingual Evaluation", "Multitask Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aeb01c42a7a81eea20fd8d671ed7994bc97564f3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces C³B (Comics Cross-Cultural Benchmark), a novel benchmark aimed at evaluating Multimodal Large Language Models (MLLMs) for their cultural awareness capabilities across multiple languages and cultural contexts. Unlike prior datasets that rely on real-world images—each typically representing a single cultural context—C³B uses comics as the core medium. The authors argue that comics can depict multiple, fictional, and mixed cultural elements within a single frame, thus presenting richer and more complex cross-cultural scenarios. \n\nC³B evaluates:\n1. Culture-aware Object Extraction (Extraction@Culture) – basic recognition of cultural cues and artifacts;\n2. Cultural-conflict Object Detection (Conflict@Culture) – detecting contradictions between cultural elements;\n3. Culturally-aligned Content Generation (Generation@Culture) – multilingual cultural translation tasks (Japanese → EN/RU/DE/TH/ES).\n\nThe benchmark was used to evaluate 11 open-source MLLMs (e.g., Qwen2.5-VL, LLaVA-NeXT, InternLM-XC2.5, Llama3.2). Results indicate that Qwen2.5-VL performs best overall, particularly in multilingual translation, but all models fall far behind human performance—especially in understanding cultural conflicts and less-known cultures. Human evaluation shows a consistent 40–60% gap over MLLMs, confirming C³B’s difficulty and diagnostic potential."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "### Innovative use of comics as a medium\nThe idea of using comics for cross-cultural benchmarking is both creative and effective. Comics enable a richer mixture of visual and textual cues that naturally embed multiple cultures, allowing for deeper testing of model reasoning and perception.\n\n### Progressive multitask structure\nThe three-level design (extraction → conflict → generation) provides a structured evaluation pipeline that mimics human-like cultural reasoning progression—from perception to comprehension to production.\n\n### Multilingual and multicultural scope\nC³B covers 77 cultures and 5 languages, far broader than prior cultural benchmarks like CVQA or GIMMICK. The inclusion of low-resource languages such as Thai enhances diversity and fairness.\n\n### Detailed methodological design\nThe construction pipeline—using multi-agent annotation with both human verification and LLM assistance (DeepSeek-V3)—is well-documented and reproducible. Figures 2–3 (pp. 4–5) clearly illustrate the dataset creation process.\n\n### Comprehensive evaluation and analysis\nThe experiments include cross-task correlation analysis (Table 5), cultural diversity metrics (Table 6), and per-culture accuracy (Figure 5). These analyses go beyond standard benchmarking, offering valuable insights into model cultural biases and limitations."}, "weaknesses": {"value": "### Limited evaluation diversity in the generation task\nThe Generation@Culture task focuses solely on machine translation. This limits the benchmark’s exploration of more nuanced cultural generation (e.g., culturally appropriate narration, idiomatic expression, or story understanding).\n\n### Use of AI-assisted annotation may introduce bias\nAlthough manually verified, the use of DeepSeek-V3 for conflict detection and translation generation could embed model-specific biases, especially in culturally sensitive interpretations."}, "questions": {"value": "Since DeepSeek-V3 was used in annotation, did you compare its conflict-detection output to human-only annotations to quantify model bias?\nHave you performed any human evaluation of cultural fidelity (i.e., are the depicted elements realistic to their culture)?\n\noverall, it's a good paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QbJr9gXTVo", "forum": "jvPdTOSTVl", "replyto": "jvPdTOSTVl", "signatures": ["ICLR.cc/2026/Conference/Submission23420/Reviewer_6ZTX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23420/Reviewer_6ZTX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631394119, "cdate": 1761631394119, "tmdate": 1762942653889, "mdate": 1762942653889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a comics cross-cultural benchmark ($\\mathrm{C}^3$B) designed to measure MLLMs' cultural awareness beyond Western-centric settings. Motivated by limits in prior datasets, including single culture and real image focus, one-question tasks, and weak multilingual coverage, $\\mathrm{C}^3$B uses fictional comic scenes to pack multiple cultures into a single frame and raise task difficulty. The benchmark contains 2,220 images and 18,789 QA pairs spanning 77 cultures, aiming to evaluate models in multicultural, multitask, and multilingual contexts.\n\n$\\mathrm{C}^3$B comprises three progressively harder tasks forming a logical chain: culture-aware object extraction, cultural-conflict detection, and culturally-aligned content generation. Evaluating 11 open-source MLLMs on $\\mathrm{C}^3$B, the authors find a clear gap between models and humans, especially on cultural-conflict reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Solid benchmark scale and diversity (2,220 images, 18,789 QAs, and 77 cultures).\n- Annotation pipeline combines automated detection with manual verification.\n- Multilingual coverage across five languages (Japanese, Russian, Thai, English, and Spanish) reduces monolingual bias."}, "weaknesses": {"value": "- Frame-level evaluation may underutilize narrative or contextual reasoning across pages.\n- CACC uses fixed weights (0.3 / 0.3 / 0.4) without dynamic justification.\n- Comics differs from real-world photos, making transfer to natural imagery uncertain."}, "questions": {"value": "- How well do the skills measured on $\\mathrm{C}^3$B transfer to real photos? Have you conducted any cross-benchmark evaluations?\n- Given that Doubao generated the comic pages and DeepSeek-V3 handled conflict labels and translations, what measures did you take to prevent bias or information leakage from those models?\n- What was the justification for selecting the 0.3/0.3/0.4 weights for Q1/Q2/Q4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7R2dbAhUc8", "forum": "jvPdTOSTVl", "replyto": "jvPdTOSTVl", "signatures": ["ICLR.cc/2026/Conference/Submission23420/Reviewer_MQS5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23420/Reviewer_MQS5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911890728, "cdate": 1761911890728, "tmdate": 1762942653597, "mdate": 1762942653597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces C3B (Comics Cross-Cultural Benchmark), a comic-centric evaluation suite for cultural awareness in MLLMs. C3B contains 2,220 images and 18,789 QA pairs, spanning three escalating tasks: (i) culture and object detection, (ii) cultural-conflict object detection and description, and (iii) machine translation. The dataset mixes generated comics via Doubao APIs with automatic and manual annotations of culturally representative objects and conflicts (DeepSeek-V3-assisted) and Manga109 pages for translation with a translator–reviewer multi-agent pipeline for references. The authors benchmark 11 open-source MLLMs, report low performance, especially on conflict description (Q4), and show a consistent gap vs. human accuracy across difficulty tiers."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Framing culture evaluation around comics is novel: unlike many real-world images that encode a single culture, comics can “condense numerous cultures into a single frame”, which plausibly raises task difficulty, but also introduces the problem of artificially generated data that do not depict real-world needs.\n- The dataset combines multitask + multicultural + multilingual integration and progressive difficulty. Something that is not clarified very well in the paper is that in C3B, there are two different dimensions of progressive difficulty. One is in the three-level structure (recognition → conflict reasoning → multilingual generation), which provides a progressive, multitask view missing in several prior cultural benchmarks that are monolithic or single-task. The second one is in the number of cultural objects/conflicts identified in one image."}, "weaknesses": {"value": "- Problems with tasks, prompt set-up, and phrasing. The prompts, as depicted in the Figures, contain grammatical mistakes and appear to be too complicated. See questions/comments for further details. I would recommend actually phrasing the first task as recognition (what cultures/objects appear as multiple choice (q1) and open-ended (q2) questions). Then, I would rename cultural conflict understanding to cultural conflict reasoning, emphasizing that the goal is not merely to detect a contradiction (q3), but to reason about its cause and cultural context (q4). For the Generation@Culture task, I would reconsider using machine translation as the generative component. While translation can reflect cultural nuances, it does not fully test a model’s ability to generate culturally informed or contextually appropriate content. Instead, I suggest reframing this task toward culture-aware image captioning or cultural storytelling, where models must produce natural-language descriptions or short narratives that reflect the depicted cultures’ symbols, values, or interactions. This adjustment would make the task hierarchy more coherent (recognition → reasoning → generation) and align better with standard multimodal evaluation practices. \n- Problems with dataset bias and validation. A significant share of C3B relies on generated comics and LLM-mediated conflict labels. This can introduce artifact-driven shortcuts or stereotype leakage from generators/LLMs—risks widely discussed for culture benchmarks and synthetic imagery. The paper partially mitigates with manual verification, but a deeper bias and authenticity audit (e.g., culture-expert review, stereotype checklists) would strengthen claims about real-world cultural awareness.\n- Problems with annotation quality and manual validation. The paper does not clearly describe how annotators (models, agents, or authors) were instructed to identify cultures, objects, or cultural conflicts, nor how manual validation was performed or measured (e.g., number of validators, agreement scores, or correction rate). Please provide explicit annotation details and quality-control metrics to ensure transparency and reproducibility.\n- Problems with ground truth for conflict reasoning. Q4 depends on Q1/Q2 labels plus model answers; the paper reports a composite CACC with fixed weights. While motivated, this mixes upstream recognition errors with conflict reasoning, and the ablation shows only marginal gains when injecting Q1/Q2 answers directly into the Q4 prompt. Stronger gold annotations for conflicts (independent of model responses) and sensitivity checks on CACC weights would clarify what Q4 truly measures."}, "questions": {"value": "- How were stereotypes and sensitive portrayals handled during generation and annotation? Any expert review (region-specific annotators) or exclusion lists? A brief ethics appendix would help, given synthetic cultural content concerns.\n- For Q4, do you have independent gold conflict triples (culture, object, contradiction) for a subset, rather than relying on model-conditioned answers? If not, could you release human-vetted conflict graphs to decouple recognition errors from reasoning?\n- The human evaluation study needs further elaboration. You bucket difficulty by the number of cultures per image, then sample 100 items per tier. Could you add inter-rater agreement and per-tier time-to-answer? Also, show model vs. human delta broken down by specific culture sets (e.g., lesser-known vs. globally familiar).\n- Background Culture Identification and Culture-aware Object Detection: How are gold labels established? Please describe annotator guidelines, the source of culture/object lists, and any expert review. Include inter-annotator agreement (e.g., Cohen’s κ) and examples of borderline cases. Explicitly discuss bias mitigation (e.g., stereotype checks, exclusion lists, region-specific reviewers).\n- How do you define cultural conflict? Provide an explanation and decision rules with concrete examples from the annotation/internal validation process and guidelines. Clarify whether conflicts are culture–culture, culture–object, or object–object, and how multi-culture scenes are handled.\n- Clarify whether the Generation@Culture task uses OCR, classic machine translation, or general-purpose generative LLMs for translation. Is the input the manga image or the transcription of the text presented in the image?\n- Low BLEU: disentangle error sources. Are low scores primarily due to translation quality or OCR extraction errors? If OCR is used on Manga109, I would recommend first reporting OCR accuracy or manual correction rates. State whether you have ground-truth transcriptions for source text from Manga109; if yes, use them to isolate translation quality from text extraction noise.\n\n**Grammar mistakes/Wording fixes**\n- Line 13: “Cultural awareness capabilities has emerged” → “Cultural awareness capabilities have emerged.”\n- “current benchmarks lack progressed difficulty?” → It is unclear what is meant by progressed difficulty in the abstract, maybe progressive? I would rephrase: “Existing benchmarks do not offer a clearly staged progression of difficulty.” or something that shows that you mean progressively challenging tasks.\n- Lines 125: “integrates comics and comicss” →  Do you mean and manga? It is not clear.\nLines 249-251: This is not good English and is confusing to the reader. \"Annotation for task...\" -> \"The annotation for the tasks of\" AND \"For task Extraction@Culture\" -> \"For the task of Extraction@Culture\"\n- Lines 260–269: I would recommend avoiding the future tense in the methods section. For example, replace “We will manually inspect…” with “We manually inspect (or inspected)…” (past or present).\n- Lines 420–424: Rewrite for grammar and specificity. I assume you mean: “To evaluate C3B’s cultural diversity, we compute three measures—Culture Density Per Image (CDPI), Cultural Breadth Intensity (CBI), and Coverage-Adjusted Density (CAD)—and compare them against cultural QA datasets.” \n- Figure 1: “Is there a culture contradicts in this image?” → “Does this image contain a cultural contradiction?”, “If there is contradict in this image…” → “If a contradiction is present, answer the following questions.”, “For each object in the answer to your question 2 -/ contradiction!!” → Please rephrase the instruction clearly; e.g., “For each object identified in Q2, explain the cultural contradiction it creates.”, Labeling error: Use “C3B” instead of “C3UB.”\n- Figure 2: Same labeling error—“C3B,” not “C3UB.”\n- Table 3: Again, “C3B,” not “C3UB.”\n- Figure 5 caption: “Scores of QA Pairs in different culture” is ambiguous. Specify the metric: “Error rates of QA pairs by culture”. Also define the denominator (per pair? per culture? macro/micro?).\n\n**Comments**\n- Given that culture-related benchmarks are still relatively scarce, I recommend expanding the Related Work 2.2 section to include recent datasets that explicitly address cultural diversity in multimodal evaluation. For instance, the VizWiz cultural extension ([Karamolegkou et al., 2024](https://aclanthology.org/2024.hucllm-1.5.pdf)) demonstrates that existing real-world benchmarks, such as VizWiz, encode cultural concepts that were not taken into account in the original crowdsourced annotations. Thus, it introduces a culture-centric evaluation subset in which some images are annotated with more than one culture (e.g., a US brand package of matcha tea, a French wine packaged by Tesco in the UK, containers of Asian food from an Australian meal delivery company, etc.). This work is relevant to your focus on cross-cultural visual understanding and includes images taken in real-world settings by people who are blind that reference more than one culture. Another relevant work that seems to be missing is GlobalRG ([Bhatia et al., 2024](https://aclanthology.org/2024.emnlp-main.385.pdf)), which introduces a multicultural benchmark and two novel tasks—Retrieval across Universals and Cultural Visual Grounding—designed to assess vision-language models' performance across culturally diverse and culture-specific concepts. Including this paper would strengthen the discussion on cultural inclusivity and dataset diversity in VLM evaluation. In addition, ALM-bench ([Vayani et al., 2025](https://arxiv.org/pdf/2411.16508)) represents the largest-scale effort to date to evaluate multimodal models across 100 languages and 13 cultural dimensions, explicitly measuring cultural and linguistic inclusivity. Its scope and methodological framing make it a strong point of comparison for your benchmark. You may also consider referencing [Zhong et al., 2025](https://arxiv.org/html/2510.05931v2), which introduces a complementary perspective on multimodal cultural reasoning and could help position C3B more clearly within the rapidly growing ecosystem of cross-cultural LMM evaluation.\n- Lines 270–280: I would recommend adding a quantitative summary of that inspection (e.g., number reviewed, acceptance rate, inter-rater agreement).\n- For instruction-following issues (e.g., “stubbornness” in LLaVA-1.5-7B), this is a prompt compliance problem rather than cultural understanding. Consider simple schema validation (e.g., JSON schema with pydantic) to enforce output format and reduce evaluation noise; note it as an implementation detail, not a scientific claim.\n- For cultural density measures (CDPI/CBI/CAD), add brief intuition + formulae in main text and validate that higher density correlates with lower model accuracy on held-out items.\n- If you claim “progressive difficulty,” define how difficulty is constructed and validated (e.g., number of cultures per image, ambiguity, required cross-culture reasoning) and show that model performance monotonically decreases with the proposed difficulty index (or explain deviations). You should also clarify that by progressive difficulty, you also include the dimension of how many cultural objects are contained in one image (as it is only briefly mentioned in the human-model analysis).\n- To compensate for the limitations of synthetic data and artificial tasks, you could draw inspiration from [Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users](https://aclanthology.org/2025.acl-long.1260/) (Karamolegkou et al., ACL 2025). That work grounds multimodal evaluation in real-world needs, such as question answering and image captioning for accessibility. You should acknowledge it in related works, since MLMMs are now being used in the real world. Incorporating similarly practical, human-centered tasks would make C3B more ecologically valid and demonstrate the benchmark’s relevance beyond synthetic cultural scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "30yA3FbcVy", "forum": "jvPdTOSTVl", "replyto": "jvPdTOSTVl", "signatures": ["ICLR.cc/2026/Conference/Submission23420/Reviewer_oEmH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23420/Reviewer_oEmH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954402756, "cdate": 1761954402756, "tmdate": 1762942653214, "mdate": 1762942653214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes benchmarking MLLM in cultural and multilingual understanding via tasks involving comic panel as the image input. This benchmark were generated with a pipeline of both automated generated data and human in-the-loop for verification, and have 3 sub-tasks, covering various languages depening on the task. Lastly, author benchmarked various models, noting the challenge of the benchmark."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The use of comic-panel as a benchmark is interesting and unique. This benchmark is decently sized, multilingual."}, "weaknesses": {"value": "My biggest concern is the decision to use AI-generated images for this particular benchmark. If we hypothesize that current MLLMs are not yet proficient at handling multicultural contexts, then generating cultural images in this way could be harmful and potentially misleading. Anecdotally, the examples in this paper show exaggerated depictions of cultures (see relevant reading: https://arxiv.org/pdf/2407.14779v1).\n\nThis issue is further compounded by the lack of human validation for the generated images. Without proper grounding (e.g., visual references), there is no guarantee that the generated cultural images are accurate or appropriate.\n\nThe author also does not seem to consider what truly constitutes a culture, beyond surface-level representations such as “Native American” or related objects. While these can serve as proxies for culture, cultural understanding goes much deeper than that. The paper could benefit from a more robust definition of what the author considers a “cultural benchmark,” which would also clarify how the list of cultures, objects, or related proxies was derived.\n\nFigure 5 further illustrates the lack of care in defining culture. The plot is confusing, it includes “Asia” alongside “Indonesia,” “Korea,” and “Japan.” Are these not all Asian cultures? What, then, constitutes “Asian” culture? This connects back to the earlier issue of validation: just as the generated images require validation, the questions themselves should also be validated, ideally by native speakers and individuals familiar with the respective cultures.\n\nMinorly, the author claims that existing cultural VQA datasets are weaker because they use real-world images, implying those are easier. I am not convinced by this reasoning. Real-world images reflect the actual use cases for these models, and there is no guarantee that such tasks are easier."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "91boTuzPo1", "forum": "jvPdTOSTVl", "replyto": "jvPdTOSTVl", "signatures": ["ICLR.cc/2026/Conference/Submission23420/Reviewer_h7gx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23420/Reviewer_h7gx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994737004, "cdate": 1761994737004, "tmdate": 1762942652394, "mdate": 1762942652394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}