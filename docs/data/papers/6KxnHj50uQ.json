{"id": "6KxnHj50uQ", "number": 15331, "cdate": 1758250343413, "mdate": 1763545269227, "content": {"title": "NeuroSketch: An Effective Framework for Neural Decoding via Systematic Architectural Optimization", "abstract": "Neural decoding, a critical component of Brain-Computer Interface (BCI), has recently attracted increasing research interest. Previous research has focused on leveraging signal processing and deep learning methods to enhance neural decoding performance. However, the in-depth exploration of model architectures remains underexplored, despite its proven effectiveness in other tasks such as energy forecasting and image classification. In this study, we propose NeuroSketch, an effective framework for neural decoding via systematic architecture optimization. Starting with the basic architecture study, we find that CNN-2D outperforms other architectures in neural decoding tasks and explore its effectiveness from temporal and spatial perspectives. Building on this, we optimize the architecture from macro- to micro-level, achieving improvements in performance at each step. The exploration process and model validations take over 5,000 experiments spanning three distinct modalities (visual, auditory, and speech), three types of brain signals (EEG, SEEG, and ECoG), and eight diverse decoding tasks. Experimental results indicate that NeuroSketch achieves state-of-the-art (SOTA) performance across all evaluated datasets, positioning it as a powerful tool for neural decoding.", "tldr": "We propose an effective framework, NeuroSketch, for neural decoding via systematic architectural optimization.", "keywords": ["Neural Decoding", "Architectural Optimization", "Brain Signals"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e297475581c20de9447aced2b9986deb3ba2d82e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces NeuroSketch, a framework for neural decoding tasks achieved via systematic architectural optimization. The authors investigate various neural network backbones (CNNs, GRUs, Transformers, hybrids) in detail, perform macro- and micro-level optimizations (e.g., latent space transformations, convolutional choices), and gradually distill these principles into the NeuroSketch design, which is an enhanced CNN-2D-based model. NeuroSketch is validated on eight neural decoding tasks spanning three modalities (speech, visual, auditory), three signal types (EEG, SEEG, ECoG), and is shown to outperform a suite of state-of-the-art baselines across multiple datasets and tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This study presents a comprehensive evaluation, validating the proposed model across eight datasets and three distinct recording modalities: EEG, sEEG, and ECoG.\n\n2. After extensive tuning of model hyperparameters, the model surpasses other models.\n\n3. The authors provide open-source reproducibility commitments and dataset transparency."}, "weaknesses": {"value": "1. The implementation details of baselines are missing. The performance of Conformer on Du-IN in Table 4 significantly underperforms the reported value in the Du-IN paper. We need specific details of the baseline implementation to ensure the reliability of the conclusions of the overall work and fair comparison.\n\n2. The CNN-2D introduces translation invariance, which seems to be uncommon in brain signal modeling. It would be useful to include more analysis about the features obtained by the models (e.g., spatial locality), helping us understand the effectiveness of CNN-2D-based BCIs.\n\n3. The framework appears largely a product of empirical exploration, with each step optimized via benchmark wins (cf. Table 2 and Table 3). While this is valuable, the paper does not elevate its findings to generalizable new principles for neural decoding. For example, the manner in which CNN-2D exploits spatial locality is discussed but not quantified, and the underlying reason for Transformer's poor performance isn’t deeply analyzed or theoretically unpacked. The reader is left with “what works” but not always “why” -- limiting the scientific insight provided."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AxEtzGfBI8", "forum": "6KxnHj50uQ", "replyto": "6KxnHj50uQ", "signatures": ["ICLR.cc/2026/Conference/Submission15331/Reviewer_q1SR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15331/Reviewer_q1SR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760777110917, "cdate": 1760777110917, "tmdate": 1762925626549, "mdate": 1762925626549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework for systematic architecture optimisation. Starting with exploring various model types and then with the best design going to a macro-micro-analysis, the paper introduces a model with competitive performance compared to other deep and foundation model baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-structured and it follows a nice step-by-step analysis on how specific details were implemented in the final architecture. It’s the first time I see an analysis like this one. Most papers just introduce a new architecture without any design justifications."}, "weaknesses": {"value": "The paper fails to provide more comparisons with better models (deep learning and foundation models). In addition, although thorough the analysis does not provide interpretable insights behind the choices.\n\nWriting:\nPaper is well-written and good structured.\n\nOverall:\nThe paper shows some merits but it would be vital to have my questions answered."}, "questions": {"value": "1. I wonder if the initially tested architectures - like CNN or transformer - get deeper or if more / less data is used during training for each model (for example, transformer based models need more data), would that affect the initial observations ?\n2. How about other more compact advanced deep baselines like EEGInception and Brainwave scattering net in table 4?\n3. How about other foundation models like LaBraM, NeuroGPT, EEGPT etc. in table 4 ? It seems the foundation models section is not very well represented. \n4. How about the number of parameters of these models as well  ? Any relationship between the number of parameters and performance ?\n5. Is there any actual meaning behind the design choices ? For example, EEGNet provides interpretable insights. In other words, is it just black box or is there an actual neurological meaning why these design choices do work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hMaAqU95Lu", "forum": "6KxnHj50uQ", "replyto": "6KxnHj50uQ", "signatures": ["ICLR.cc/2026/Conference/Submission15331/Reviewer_s4VP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15331/Reviewer_s4VP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619084068, "cdate": 1761619084068, "tmdate": 1762925626236, "mdate": 1762925626236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NeuroSketch, a neural decoding framework that enhances the performance of decoding multiple types of brain signals (EEG, SEEG, ECoG) through systematic architectural optimization—from basic architecture selection to macro- and micro-level structural improvements. The authors conducted over 5,000 experiments across eight tasks (covering visual, auditory, and speech modalities), demonstrating that NeuroSketch achieves state-of-the-art (SOTA) performance on multiple benchmarks. The core contribution of this framework lies in its ability to model the spatiotemporal characteristics of brain signals, with step-by-step optimizations showing consistent performance gains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Systematic Architectural Exploration with a Clear Optimization Path:\nThe paper begins with a comparison of basic architectures (CNN-1D/2D, GRU, Transformer, etc.) and progressively delves into macro (latent space transformation) and micro (convolution operation optimization) levels of design. This forms a complete and logically rigorous optimization roadmap, offering high interpretability and methodological value.\n\n2. Large-Scale, Multi-Modal Experimental Validation:\nExtensive validation was performed across three modalities (visual, auditory, speech), three brain signal types (EEG, SEEG, ECoG), and eight tasks. The experimental scale is substantial (over 5,000 experiments), making the results highly credible and demonstrating strong generalization capability.\n\n3. Tailored Modeling of Brain Signal Characteristics:\nThe work explicitly addresses the transient temporal dynamics and spatial locality inherent in neural decoding tasks. Designs such as CNN-2D, group convolution, and early downsampling (Pagoda approach) effectively capture these characteristics, reflecting a deep understanding of the nature of neural signals.\n\n4. Effective Balance Between Computational Efficiency and Performance:\nThe optimization process considers not only performance improvement but also computational cost (e.g., GFLOPs comparison). Proposed strategies like the Step approach, Pagoda approach, and Group Convolution significantly reduce computational burden while maintaining or even improving performance."}, "weaknesses": {"value": "1. Lack of Discussion on Neurophysiological Interpretability:\nAlthough the model performs excellently, the paper does not deeply analyze whether the neural representations learned by NeuroSketch are interpretable from a neuroscience perspective (e.g., correspondence to brain region activation or cognitive processes). This is an important dimension in BCI research.\n\n2. Insufficient Comparison with Some Existing Neural Decoding-Specific Models:\nWhile comparisons are made against several general time-series models and some brain-specific models, the comparison with certain recent architectures specifically designed for EEG/SEEG [1,2] is not comprehensive enough. This might fail to fully demonstrate its advantages over the best methods in the field.\n\n3. Insufficient Exploration of Multi-Modal Fusion and Cross-Modal Generalization:\nAlthough tested on multiple modalities, the paper does not explore the model's generalization ability across modalities (e.g., transferring a model trained on visual tasks to auditory tasks), nor does it attempt multi-modal fusion decoding, which holds significant value for future BCI systems.\n\n4. Inadequate Analysis of Individual Differences and Cross-Subject Generalization:\nAlthough experiments were conducted on data from different subjects, the systematic analysis of the model's ability to handle individual differences and its cross-subject decoding capability is relatively limited. Results for cross-subject unified training or adaptation strategies are not provided.\n\n5. Ablation Studies are Not Comprehensive Enough:\nAlthough the step-by-step optimization process is presented, a systematic ablation study on the independent contribution of each component in the final NeuroSketch model (e.g., GeM pooling, residual connections) is lacking. This makes it difficult to judge the specific impact of each part on the final performance.\n\n**References:**\n\n[1] Singh, A., Thomas, T., Li, J., Hickok, G., Pitkow, X., & Tandon, N. (2025). Transfer learning via distributed brain recordings enables reliable speech decoding. *Nature Communications, 16*(1), 8749.\n\n[2] Chen, X., Wang, R., Khalilian-Gourtani, A., Yu, L., Dugan, P., Friedman, D., ... & Flinker, A. (2024). A neural speech decoding framework leveraging deep learning and speech synthesis. *Nature Machine Intelligence, 6*(4), 467-480."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns are apparent. The public datasets are used appropriately."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GTQbD81OKX", "forum": "6KxnHj50uQ", "replyto": "6KxnHj50uQ", "signatures": ["ICLR.cc/2026/Conference/Submission15331/Reviewer_jvHa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15331/Reviewer_jvHa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821523615, "cdate": 1761821523615, "tmdate": 1762925625853, "mdate": 1762925625853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To All Reviewers: Additional Analyses of Interpretability 2"}, "comment": {"value": "(2) Representation Analysis of CNN-2D and Transformer‑Based Architectures\n\nTo investigate this, we examined the model representations from a rank‑based perspective, following the framework proposed by Yu et al.[6] The rank of a feature representation reflects its expressive capacity: a higher rank indicates richer and more diverse features, while a lower rank implies excessive compression and loss of independent information. In a well‑organized hierarchy, the rank is expected to remain high or increase with depth, as deeper layers capture more abstract yet independent features. In our evaluation, since the CNN-2D-based model and the Transformer-based model achieved the overall best and worst performance, respectively, we conducted an analysis on NeuroSketch-Large (CNN-2D-based) and MedFormer (Transformer-based) to examine how the rank of the feature representations produced by each layer evolves across network depth.\n\nFor any layer of a given model, let the output representation of a sample be denoted as $\\mathbf{O}\\in \\mathbb{R}^{s\\times d}$,  where $s$ represents the sequence length and $d$ the feature dimension. We performed singular value decomposition (SVD) on $\\mathbf{O}$ as $\\mathbf{O}=\\mathbf{U}\\Sigma \\mathbf{V}^{T}$,  where $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal matrices, and $\\Sigma$ is a diagonal matrix whose diagonal entries $\\sigma _1\\ge \\sigma _2\\ge \\cdots \\ge \\sigma _{\\min(s,d)}$ are the singular values. Although the algebraic rank, i.e., the number of nonzero singular values, serves as a strict measure of rank, real‑world data are often noisy. Therefore, we adopt a more practical numerical rank. For a tolerance $\\varepsilon>0$, the $\\varepsilon$-rank of $\\mathbf{O}$ is defined as the number of singular values that are significant relative to the largest one. Specifically, only singular values $\\sigma$ that satisfy $\\sigma / \\sigma_0 > \\varepsilon$ are counted as significant. Since the maximum rank (defined as $\\min(s, d)$) varies across layers depending on $s$ and $d$, we further compute the $\\varepsilon$-rank ratio, defined as the $\\varepsilon$-rank relative to the maximum rank. This metric allows for a more intuitive comparison of how the effective rank evolves across layers. \n\nFor MedFormer, we extracted the $\\varepsilon$-rank ratios of the embedding layer and each of its six encoder layers under three different patch lengths (5, 10, and 20) to examine how patch length affects representational rank. For NeuroSketch‑Large, we computed the $\\varepsilon$-rank ratios of the outputs of its three stem layers and all layers across the four feature‑extraction stages, transforming the height–width dimensions of each feature map into a single sequence length for a consistent rank analysis. From the two models' evolution of the $\\varepsilon$-rank ratios, we can observe that (1) for multichannel neural signals, the input embeddings (the initial embeddings mapped from raw input signals) are not as low‑rank as those typically observed in some general time‑series foundation models [1, 2].  Specifically, for both MedFormer and NeuroSketch‑Large, when $\\varepsilon \\le 10^{-2.5}$, the $\\varepsilon$-rank ratio of the input embeddings remain above 0.6, indicating that the input embeddings do not collapse into a low‑rank subspace; (2) for both NeuroSketch and MedFormer, we observe distinct trends in how the $\\varepsilon$-rank ratio evolves across layers. As the network depth increases, MedFormer tends to progressively reduce the rank of its feature representations, indicating excessive compression of the input information into a lower‑dimensional subspace. In contrast, NeuroSketch exhibits the opposite trend: the rank of its representations gradually increases, approaching full rank in deeper layers. These contrasting trends imply that the two architectures differ fundamentally in how they organize their representation spaces. In neural decoding tasks, compared with the transformer architecture, the CNN-2D architecture is able to compress more robust and information-rich representations.\n\nCitations:\n\n[1] Yu A, Maddix D C, Han B, et al. Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility[J]. arXiv preprint arXiv:2510.03358, 2025.\n\n[2] Liang Z, Zhu J, Sun W. Why attention fails: The degeneration of transformers into mlps in time series forecasting[J]. arXiv preprint arXiv:2509.20942, 2025."}}, "id": "Nl8No27ogV", "forum": "6KxnHj50uQ", "replyto": "6KxnHj50uQ", "signatures": ["ICLR.cc/2026/Conference/Submission15331/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15331/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission15331/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763545523544, "cdate": 1763545523544, "tmdate": 1763545523544, "mdate": 1763545523544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To All Reviewers: Additional Analyses of Interpretability 1"}, "comment": {"value": "We sincerely thank all three reviewers for their thoughtful and valuable feedback. We notice that all reviewers raised concerns regarding the interpretability of our work. In response, we have conducted two additional experiments to enhance the interpretability of our model and results. We have included these analyses and results in Appendix F.1 and F.2 of the revised manuscript, and we hope that they help clarify and address your concern. The detailed descriptions of these two experiments are provided below.\n\n(1) Neurophysiological Interpretability\n\nTo further investigate the spatiotemporal features captured by NeuroSketch, we conducted a case study on the Du‑IN dataset[1]. In this analysis, we trained NeuroSketch‑Large on all 115 channels from subject 02 and 90 channels from subject 11, and applied the Score‑CAM [2] method to visualize the model’s decision‑making process across both spatial and temporal dimensions.\n\nScore‑CAM generates class‑activation maps by computing the gradient‑free importance of each feature map and projecting it back to the input space, thereby highlighting which spatiotemporal regions most strongly influence the predicted class. For each sample, we obtained a saliency map and subsequently averaged these maps across all samples from subject 02 and subject 11, respectively, to produce subject‑wise saliency maps. These aggregated maps(Figure 4 in Appendix F.1) were then used to identify regions of interest (ROIs) within both the spatial and temporal domains.\n\nFor the temporal analysis, the model exhibits a pronounced activation hotspot concentrated around the mid‑trial period, approximately between 800–1600 ms for subject 11 and 1200–1800 ms for subject 02, followed by a gradual decrease in saliency toward both the early and late segments. This pattern indicates that NeuroSketch focuses on short‑range dependencies rather than allocating attention uniformly over time, which aligns with the transient temporal dynamics of neural signals. \n\nFor the spatial analysis, we averaged each subject‑wise saliency map across the temporal dimension to obtain a saliency score for every channel. Notably, for subject 11, the channels previously reported as significant in the original study [1] were 52, 55, 56, 57, 65, 74, 75, 76, 77, 78. Except for channel 65, all nine of these channels appeared within our top 20 most salient channels identified by NeuroSketch. Similarly, for subject 02, the significant channels reported in [1] were 72, 73, 74, 75, 76, 77, 100, 109, 110, 111, among which channels 72–77 were also ranked within our top 20. Beyond these specific electrodes, the remaining highly salient channels detected by our model were found to be spatially clustered around these previously reported regions, indicating that NeuroSketch captures spatial activity patterns that are consistent with experimentally validated cortical regions. \n\nMore importantly, the top‑contributing electrodes identified by NeuroSketch are located within or near the ventral sensorimotor cortex (vSMC) and the bilateral superior temporal gyrus (STG), two cortical regions well established as the core network for speech motor control[3-5]. The model’s focus on these regions highlights its ability to capture biologically interpretable and functionally grounded spatial activation patterns, further validating the neurophysiological relevance of our saliency findings.\n\nCitations:\n\n[1] Zheng, H., Wang, H., Jiang, W., Chen, Z., He, L., Lin, P., ... & Liu, Y. (2024). Du-IN: Discrete units-guided mask modeling for decoding speech from Intracranial Neural signals. *Advances in Neural Information Processing Systems*, *37*, 79996-80033.\n\n[2] Wang, H., Wang, Z., Du, M., Yang, F., Zhang, Z., Ding, S., ... & Hu, X. (2020). Score-CAM: Score-weighted visual explanations for convolutional neural networks. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops* (pp. 24-25).\n\n[3] Bouchard K E, Mesgarani N, Johnson K, et al. Functional organization of human sensorimotor cortex for speech articulation[J]. Nature, 2013, 495(7441): 327-332.\n\n[4] Hickok G, Poeppel D. The cortical organization of speech processing[J]. Nature reviews neuroscience, 2007, 8(5): 393-402.\n\n[5] Chartier J, Anumanchipalli G K, Johnson K, et al. Encoding of articulatory kinematic trajectories in human speech sensorimotor cortex[J]. Neuron, 2018, 98(5): 1042-1054. e4."}}, "id": "g5qxs4sRUX", "forum": "6KxnHj50uQ", "replyto": "6KxnHj50uQ", "signatures": ["ICLR.cc/2026/Conference/Submission15331/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15331/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission15331/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763545637618, "cdate": 1763545637618, "tmdate": 1763545637618, "mdate": 1763545637618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}