{"id": "FFIn2TH7aU", "number": 25479, "cdate": 1758368496820, "mdate": 1759896719508, "content": {"title": "Supra-Tuning: Combining Outlier and Low-Rank Adaptation for Sparse and Efficient LLM Fine-Tuning", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities but remain expensive to fine-tune due to their size. Recent parameter-efficient tuning methods, such as Low-Rank Adaptation (LoRA), reduce the number of trainable parameters while maintaining performance. In this work, we introduce Super, a novel sparse adaptation technique that selects and trains only a small set of influential weights—so-called super weights—identified via outlier metrics such as WANDA. We show that fine-tuning these outlier weights yields strong performance with minimal parameter updates. Building on this idea, we propose Supra, a hybrid method that combines Super with LoRA, merging sparse and low-rank adaptations into a unified tuning strategy. Our experiments on several LLMs and downstream tasks demonstrate that both Super and Supra outperform existing sparse or low-rank methods alone in perplexity and task performance, while reducing computational and memory overhead. Supra-Tuning offers a simple yet powerful framework for efficient and scalable adaptation of LLMs.", "tldr": "We propose Super, a sparse fine-tuning method that updates only key outlier weights, and Supra, a hybrid that combines Super with LoRA.", "keywords": ["PEFT", "Fine Tuning", "LLM", "Training", "Deep Learning", "AI", "Language Models", "Llama", "Wanda", "Outliers"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef2d5d42522f6fb793cf9016ec50951c01523469.pdf", "supplementary_material": "/attachment/e78122283e5a148788fcb1be05f92ee3aaae6bcd.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents Super, which selects and trains only a small set of influential weights, and proposes Supra, which combines with LoRA.\nThey demonstrate strong performance in downstream tasks while reducing computational and memory overhead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Super uses the WANDA saliency computed in a single forward pass, which is easy to reproduce and deploy at scale."}, "weaknesses": {"value": "- limited novelty: Both outlier metrics (e.g., WANDA) and LoRA are known; the main idea is to combine them.\n- Narrow evaluation scope: Experiments are restricted to the math dataset."}, "questions": {"value": "- r = 8 and r = 4 are described as “moderate” and “low” budgets, but could the authors provide actual metrics (e.g., training time, GPU memory, or FLOPs) to quantify the real difference in efficiency?\n- How sensitive is Super/Supra to the WANDA ranking?\n- Could you include computational cost study (e.g., wall-clock time) for Super, LoRA, and Supra to substantiate claims of reduced overhead?\n- Since evaluation is math-centric, how do results transfer to other tasks?\n- Lack of study on equation 7."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TOyesVXzMf", "forum": "FFIn2TH7aU", "replyto": "FFIn2TH7aU", "signatures": ["ICLR.cc/2026/Conference/Submission25479/Reviewer_vVKC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25479/Reviewer_vVKC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761441931501, "cdate": 1761441931501, "tmdate": 1762943448417, "mdate": 1762943448417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Supra, a parameter-efficient tuning method with two parts: **Super** selects a small set of high-impact “super weights” using a WANDA-style outlier score and trains only those, while **Supra** unifies Super with LoRA by allocating a fixed per-layer parameter budget between sparse and low-rank updates through a simple ratio α. Experiments show that Super excels under tight budgets and Supra dominates when more parameters are available, outperforming existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is conceptually simple and enables fast division of trainable parameters into sparse and low-rank parts.\n2. The paper provides detailed ablation studies on hyperparameters and reports variance statistics, enhancing the credibility of the results."}, "weaknesses": {"value": "1. The parameter selection strategy follows the same principle as WANDA, offering limited novelty.\n2. The comparative analysis is somewhat narrow, as the paper only benchmarks against basic ablation variants such as LoRA, SIFT, and RoSA. Comparisons with more recent methods (e.g., NeFT, SpIEL, DoRA, HiRA) would strengthen the claims.\n3. The evaluation datasets are relatively limited, and performance reported on GSM8K is less than similar works.\n4. Model configurations are not clear. The details on LLaMA-3 (1B/3B/8B) settings are missing.\n5. Several tables have formatting issues (lines extending beyond page margins).\n6. Appendix C appears incomplete."}, "questions": {"value": "Beyond the issues mentioned above, a key question concerns the sample selection for the WANDA metric. How were these samples chosen, and does the approach generalize well to other tasks and domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "F3nHtX9ZdQ", "forum": "FFIn2TH7aU", "replyto": "FFIn2TH7aU", "signatures": ["ICLR.cc/2026/Conference/Submission25479/Reviewer_pxQb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25479/Reviewer_pxQb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727045805, "cdate": 1761727045805, "tmdate": 1762943448209, "mdate": 1762943448209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes two efficient LLM fine-tuning methods: Super and Supra. Super uses the WANDA metric to filter outlier weights for sparse fine-tuning, while Supra combines Super with LoRA to achieve a fusion of sparsity and low-rank adaptation. Experiments show that both methods require fewer parameters and outperform existing methods on mathematical reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Two efficient algorithms, Super and Supra, are proposed, eliminating the dependency on LoRA's specific parameter structure updates.\n\n- Figure 1 clearly explains the algorithms proposed by the authors."}, "weaknesses": {"value": "- Appendix C, “ADDITIONAL EXPERIMENTS,” contains only a title and no content.\n\n- Appendix D's tables are missing numerous experimental results without any explanation.\n\n- Table 2's “GSM8K” column does not highlight the best results.\n\n- Only the main experiment is shown; further analytical experiments are missing."}, "questions": {"value": "- Compared to the baseline method, how efficient are Super and Supra? Aren't they faster?\n\n- When training with Super and Supra, does the effective rank of each matrix in the model change during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D9heiuapp8", "forum": "FFIn2TH7aU", "replyto": "FFIn2TH7aU", "signatures": ["ICLR.cc/2026/Conference/Submission25479/Reviewer_mwmk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25479/Reviewer_mwmk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967792779, "cdate": 1761967792779, "tmdate": 1762943447932, "mdate": 1762943447932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Super, a sparse fine-tuning method selecting outlier weights via the WANDA metric, and Supra, a hybrid combining Super with LoRA for large language model adaptation. Experiments on the Math10K dataset and six math reasoning benchmarks show modest gains over existing PEFT baselines such as LoRA and SIFT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation of combining sparse and low-rank adaptation is reasonable and aligns with emerging trends in efficient LLM fine-tuning."}, "weaknesses": {"value": "- Writing is poor. The text is verbose, repetitive, and lacks clarity in technical exposition. Many notations, equations, and definitions are redundant or inconsistently introduced. For example, Sec 1.1 said that all notations will be present in Section A, but Sec 3.1 introduces notations. Algorithm 1 only has two lines, which is strange.\n\nThe writing showed this work is away from completeness with proper proof-read."}, "questions": {"value": "See the comment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UtLwwq0dpz", "forum": "FFIn2TH7aU", "replyto": "FFIn2TH7aU", "signatures": ["ICLR.cc/2026/Conference/Submission25479/Reviewer_HyVw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25479/Reviewer_HyVw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153041832, "cdate": 1762153041832, "tmdate": 1762943447697, "mdate": 1762943447697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}