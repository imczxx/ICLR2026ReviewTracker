{"id": "QFJkvv3zMi", "number": 15997, "cdate": 1758258301856, "mdate": 1759897268313, "content": {"title": "EgoTwin: Dreaming Body and View in First Person", "abstract": "While exocentric video synthesis has achieved great progress, egocentric video generation remains largely underexplored, which requires modeling first-person view content along with camera motion patterns induced by the wearer's body movements. To bridge this gap, we introduce a novel task of joint egocentric video and human motion generation, characterized by two key challenges: 1) Viewpoint Alignment: the camera trajectory in the generated video must accurately align with the head trajectory derived from human motion; 2) Causal Interplay: the synthesized human motion must causally align with the observed visual dynamics across adjacent video frames. To address these challenges, we propose EgoTwin, a joint video-motion generation framework built on the diffusion transformer architecture. Specifically, EgoTwin introduces a head-centric motion representation that anchors the human motion to the head joint and incorporates a cybernetics-inspired interaction mechanism that explicitly captures the causal interplay between video and motion within attention operations. For comprehensive evaluation, we curate a large-scale real-world dataset of synchronized text-video-motion triplets and design novel metrics to assess video-motion consistency. Extensive experiments demonstrate the effectiveness of the EgoTwin framework. Qualitative results are available on our project page: https://egotwin.pages.dev/.", "tldr": "We propose EgoTwin, a diffusion-based framework that jointly generates egocentric video and human motion in a viewpoint consistent and causally coherent manner.", "keywords": ["Egocentric Vision"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76db70df7ec5fdc07a1544c3538a69726835e094.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces EgoTwin, a diffusion framework for the novel task of jointly generating egocentric video and the wearer's full-body motion. The method tackles the key challenges of Viewpoint Alignment (matching camera to head) and Causal Interplay (the feedback loop between seeing and acting) . The architecture is a triple-branch (text, video, motion) Diffusion Transformer featuring two main contributions: a head-centric motion representation for direct alignment and a cybernetics-inspired attention mask to model causality. Using the Nymeria dataset and new consistency metrics, the paper demonstrates strong performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a novel and clearly defined problem of *joint egocentric video and human motion generation*, with strong motivation and potential applications in embodied AI and AR.\n- The head-centric motion representation is a well-motivated and technically sound reformulation that effectively exposes egocentric cues for video-motion alignment.\n- The model design—a triple-branch diffusion transformer with asynchronous diffusion and causal attention—is elegant and verified through systematic ablations.\n- Comprehensive evaluation, including novel *video-motion consistency metrics*, supports the technical claims. The qualitative results are convincing."}, "weaknesses": {"value": "- The paper relies exclusively on the Nymeria dataset. While uniquely suited, this makes it difficult to assess model robustness. A brief justification for not using other large-scale egocentric datasets (e.g., EgoExo4D) would be helpful. \n- All data are segmented into 5-second clips. It's unclear if this is a dataset constraint (narration length) or a computational limit of the diffusion model. A discussion on this limitation and the path toward modeling longer action sequences would be beneficial.\n- The degree to which the text prompts are \"free-form\" is not analyzed. A description of the text diversity in the Nymeria dataset would be helpful to understand the model's linguistic generalization\n- The main paper's baseline comparison (Table 1) is limited to one method. The more informative comparison against strong unimodal baselines (video-only and motion-only) is in the appendix (Table 3). Maybe the authors could move Table 3 to the main paper to better highlight the benefits of joint modeling."}, "questions": {"value": "This is a very strong and interesting paper. My main questions for clarification are the weaknesses listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZZDMDYZ6ib", "forum": "QFJkvv3zMi", "replyto": "QFJkvv3zMi", "signatures": ["ICLR.cc/2026/Conference/Submission15997/Reviewer_ZiMJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15997/Reviewer_ZiMJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791019481, "cdate": 1761791019481, "tmdate": 1762926204588, "mdate": 1762926204588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed EgoTwin, a newly defined task that joins the generations of egocentric (first-person) video and human motion, addressing two core challenges: Viewpoint Alignment (ensuring camera trajectories in generated videos match head trajectories from human motion) and Causal Interplay (modeling influences between visual results and human actions). Formally, the authors proposed a multi-modal generation model to tackle this task, including a motion branch and a video generation branch. For the motion generation, a head-centric motion representation is proposed, while the Joint Text-Video-Motion Attention is used to unify both modalities. Experiments show the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. To the best of my knowledge, EgoTwin is the first work to explicitly model joint egocentric video and human motion generation.\n2. Several useful and reasonable techniques are proposed in this paper to handle this joint generation. The head-centric motion representation directly solves the limitation of root-centric representations. The attention mechanism is adjusted to improve the multi-modal learning and address causal interplay."}, "weaknesses": {"value": "1. The main concern is the unclear motivation. The paper does not sufficiently justify why a complex triple-branch diffusion generation (with specialized motion generation/video interaction modules) is required, especially given recent advances in scaling video foundation models (e.g., Genie3) that can generate high-fidelity, interactive egocentric videos via implicit scene and motion modeling. EgoTwin heavily depends on explicit motion representation and cross-modal modules, and well-labeled data (Nymeria), which may not be necessary if foundation models can implicitly address these issues with less annotation and better generalization. While Nymeria is large (170K samples in 5s), the diversity and generalization are still questionable.\n\n2. The illustration of the Interaction Mechanism is chaotic and confusing. What is the meaning of \"human motion is typically captured at a higher temporal resolution than egocentric video\", and why set $N_m = 2N_v$? The presentation of $O$ and $A$ are not well defined. What is the relation among action $A$, observation $O$, pose $P$, and view sequence $I$? Figure 3 is also very confusing without a proper illustration."}, "questions": {"value": "The authors should clarify the motivation and generalizability of the proposed approach, and providing more details about the Interaction Mechanism would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KSs4ZW2cwP", "forum": "QFJkvv3zMi", "replyto": "QFJkvv3zMi", "signatures": ["ICLR.cc/2026/Conference/Submission15997/Reviewer_Dv46"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15997/Reviewer_Dv46"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845112310, "cdate": 1761845112310, "tmdate": 1762926204137, "mdate": 1762926204137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EgoTwin, a generative approach that synthesizes ego-centric video and human poses jointly based on text instructions. It presents a novel motion representation for aligning viewpoints from a head-centric perspective. To model the relationships among text, motion, and video, the paper introduces an advanced interaction mechanism. Experiments on video quality, motion quality, and video-motion consistency underline the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is well-written and easy to follow, with the main idea clearly articulated. The implementation details are sufficient for reproduction.\n2. The design of head-centric motion tokenization is novel.\n3. The interaction mechanism is reasonable; it employs local temporal attention, which not only improves accuracy but also reduces computational load."}, "weaknesses": {"value": "1. While head-centric motion tokenization may enhance head-centric evaluations, the quality of the whole body is not assessed. Can a full-body evaluation be included?\n2. A related paper is not cited: https://egoallo.github.io."}, "questions": {"value": "1. What does \"c\" represent in Figure 3?\n2. I don't fully understand why two different time steps for video and motion generation are necessary. It appears that a single time step would suffice for joint motion and video generation. What considerations influenced this design choice?\n3. Regarding unimodal sampling in Equation 3, I understand that \"T\" is set to zero on the left-hand side due to elimination. However, why is the variable \"T\" on the right-hand side? Should it be sampled over a grid of \"t\" and \"T\"?\n4. Similarly, in Equation 4, why does the left-hand side use both \"t\" and \"t,\" while the right-hand side uses \"t\" and \"T\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KS8F3vHORJ", "forum": "QFJkvv3zMi", "replyto": "QFJkvv3zMi", "signatures": ["ICLR.cc/2026/Conference/Submission15997/Reviewer_9aiA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15997/Reviewer_9aiA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895665606, "cdate": 1761895665606, "tmdate": 1762926203696, "mdate": 1762926203696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EgoTwin, a diffusion-based generative framework for jointly producing egocentric videos and human motion that are both viewpoint consistent (ensuring alignment between camera and head movement) and causally coherent (visual observations and human actions influence each other over time). To train and evaluate the model, the authors leverage the Nymeria dataset of synchronized text–video–motion triplets and introduce video–motion consistency metrics to assess alignment between generated video and motion."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed approach, built on a triple-branch diffusion transformer, effectively models text, video, and motion within a unified framework. The cybernetics-inspired interaction mechanism is interesting, which captures the bidirectional dependencies between visual and motion streams.\n\n- The paper is well-written, clearly structured, and easy to follow, with strong motivation and coherent presentation of technical details.\n\n- The evaluation is extensive and thorough, covering diverse quantitative and qualitative metrics that  demonstrate the model’s performance."}, "weaknesses": {"value": "- The approach relies exclusively on the Nymeria dataset for training and evaluation. While Nymeria is a large and well-curated dataset, the generalization of EgoTwin to other egocentric or synthetic environments remains untested, like EgoExo4D.\n\n- The base model design choice for the text–video component relies on CogVideoX. While this is a strong foundation, it raises the question of whether using a more recent or higher-capacity base model could further improve video quality or multimodal alignment."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PyU1kMqr4l", "forum": "QFJkvv3zMi", "replyto": "QFJkvv3zMi", "signatures": ["ICLR.cc/2026/Conference/Submission15997/Reviewer_LqoE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15997/Reviewer_LqoE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138542861, "cdate": 1762138542861, "tmdate": 1762926203161, "mdate": 1762926203161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}