{"id": "SRl0xy0UOj", "number": 13292, "cdate": 1758216076630, "mdate": 1759897448356, "content": {"title": "Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping", "abstract": "Multimodal large language models (MLLMs) often miss small details and spatial relations in cluttered scenes, leading to errors in fine-grained perceptual grounding. We introduce AttWarp, a lightweight method that allocates more resolution to query-relevant content while compressing less informative areas, all while preserving global context. At test time, the approach uses an MLLM's cross-modal attention to perform rectilinear warping of the input image, reallocating spatial resolution toward regions the model deems important, without changing model weights or architecture. This attention-guided warping preserves all original image information but redistributes it non-uniformly, so small objects and subtle relationships become easier for the same model to read while the global layout remains intact. Across five benchmarks (TextVQA, GQA, DocVQA, POPE, MMMU) and four MLLMs (LLaVA, Qwen-VL, InternVL, and InstructBLIP), AttWarp consistently improves accuracy, strengthens compositional reasoning, and reduces hallucinations, outperforming four competitive baselines that manipulate raw images at test time. Together, these results show that attention-guided warping prioritizes information relevant to the query while preserving context, and that the same MLLMs perform better when given such warped inputs.", "tldr": "We warp images using the model’s own attention so it “looks closer” at important parts, boosting accuracy without changing the model.", "keywords": ["Multimodal LLMs", "Vision-Language Models", "Fine Grained Visual Grounding", "Image Warping"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba982f6f2925956ea7d7ef1401d3ba435fb832e7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AttWarp, a lightweight, plug-and-play image warping technique designed to improve the fine-grained perceptual grounding of MLLMs. The method begins by extracting cross-modal attention maps from the MLLM's internal layers based on a given image and query. These attention maps are then used to guide a rectilinear warping of the input image, which reallocates spatial resolution to magnify query-relevant regions while compressing less informative areas. The authors demonstrate consistent performance gains across five diverse benchmarks and four different MLLM architectures, showcasing the method's effectiveness and generalizability."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. As a model-agnostic enhancement, AttWarp is a practical plug-and-play solution that can be readily applied to improve existing models without retraining.\n\n2. The paper introduces a complete framework, including an iterative version (AttWarp-Chain) for hard cases and a distilled version (AttWarp-Distill) for applications.\n\n3. The work provides compelling ablations and analyses that validate its design choices. A key finding is that the rectilinear nature of the warp preserves the underlying feature distribution of the images, thus avoiding the out-of-distribution issues that can plague other image manipulation techniques.\n\n4. The method shows consistent performance gains across a wide range of benchmarks and MLLM architectures."}, "weaknesses": {"value": "1. The claim of being \"plug-and-play\" is slightly weakened by the need to identify the optimal attention layer for each new MLLM architecture (e.g., layer 20 for LLaVA, layer 16 for Qwen-VL). This requires an empirical, model-by-model search, which adds a setup cost for new models.\n\n2. In Error Analysis, the authors state that AttWarp is prone to errors in cases such as  size, hallucination, and misaligned attention. Have the authors attempted any framework modifications to specifically address these failure cases? For example, have they considered introducing a classifier to determine when to apply AttWarp?"}, "questions": {"value": "1. Is there a pattern in the indices of optimal attention layers across different MLLMs? Can we approximately determine which layer is most suitable for AttWarp?\n\n2. Are there methods to mitigate the limitations of AttWarp in cases such as size and hallucination?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fTn4Xh2Xqb", "forum": "SRl0xy0UOj", "replyto": "SRl0xy0UOj", "signatures": ["ICLR.cc/2026/Conference/Submission13292/Reviewer_J5yH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13292/Reviewer_J5yH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761380168766, "cdate": 1761380168766, "tmdate": 1762923961124, "mdate": 1762923961124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **AttWarp**, a lightweight method for improving multimodal large language models' (MLLMs') fine-grained perceptual grounding in cluttered scenes. AttWarp leverages an MLLM's cross-modal attention to perform rectilinear warping on input images during testing, reallocating resolution to query-relevant areas while preserving global context and image information.  Without modifying model weights or architecture, this attention-guided warping enhances the readability of small objects and subtle relationships. Experiments show consistent accuracy improvements across five benchmarks (TextVQA, GQA, DocVQA, POPE, MMMU) and four MLLMs (LLaVA, Qwen-VL, InternVL, and InstructBLIP), outperforming competitive baselines. These results highlight AttWarp's ability to optimize spatial resolution for query-relevant content while preserving global structure, boosting MLLM performance with warped inputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clarity and Ease of Use**: The paper is well-written, easy to understand, and straightforward to follow. The proposed method, AttWarp, is plug-and-play, delivering significant performance improvements without requiring additional training.  \n\n2. **Intuitive and Novel Approach**: Using attention feedback to enhance the resolution of focus areas is an intuitive yet innovative idea. The method avoids retraining models while achieving substantial gains, making it particularly practical and impactful.  \n\n3. **Comprehensive Validation**: The exploration of AttWarp through two variations—AttWarp-Chain and AttWarp-Distill—effectively demonstrates the feasibility and upper bounds of the method's generalization capabilities. Its success across multiple multimodal language models and benchmarks further validates the robustness and versatility of the approach, making it an interesting and promising contribution."}, "weaknesses": {"value": "While AttWarp has demonstrated significant improvements across a range of text-centric multimodal tasks (e.g., TextVQA, GQA, DocVQA, POPE, MMMU), it lacks evaluation on visual-centric benchmarks that focus more on fine-grained visual perception. Tasks such as **MMVP**, **BLINK**, **RealWorldQA**, and **MIA**, which emphasize nuanced visual grounding and object-level understanding, are particularly relevant for showcasing the strengths of AttWarp's attention-guided resolution reallocation.  \n\nIncluding these visual-centric evaluations could illustrate its potential for enhancing visual perception capabilities further, as the method is well-suited to improve the detection of subtle details and spatial relationships that are critical for such tasks. Without this, the generalizability and impact of AttWarp on visually demanding applications remain underexplored. Evaluating the method on these benchmarks would provide a more comprehensive picture of its capabilities and further highlight its benefits."}, "questions": {"value": "Do you think applying reinforcement learning (RL) to AttWarp could further enhance its capabilities, using MLLMs to validate the effectiveness of perturbed images in solving queries and providing reward feedback?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CRrsq4w6zu", "forum": "SRl0xy0UOj", "replyto": "SRl0xy0UOj", "signatures": ["ICLR.cc/2026/Conference/Submission13292/Reviewer_Tmyc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13292/Reviewer_Tmyc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749539169, "cdate": 1761749539169, "tmdate": 1762923960763, "mdate": 1762923960763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AttWarp, a plug-and-play, attention-guided image warping method that improves fine-grained perception in multimodal large language models (MLLMs) without modifying their architecture or parameters. The method reallocates spatial resolution toward query-relevant regions using the model’s own cross-modal attention, yielding consistent gains across multiple benchmarks and MLLM backbones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of leveraging model attention to reshape the input space rather than internal representations is conceptually elegant and complementary to existing attention-tuning methods.\n\n- The method is tested on five diverse benchmarks and four architectures, showing consistent improvements with detailed ablations.\n\n- The approach is simple, lightweight, and does not require retraining.\n\n- The paper includes ablations on attention quality, warping stability, and distributional integrity, which strengthen its credibility."}, "weaknesses": {"value": "- The contribution of this paper feels more like a clever engineering refinement than a fundamentally new paradigm.\n\n- The method assumes reliable attention maps; performance may degrade under noisy or misaligned attention, but this limitation is only briefly mentioned.\n\n- The paper’s justification for why rectilinear warping improves reasoning remains empirical and lacks a more formal analysis of perceptual geometry or attention dynamics."}, "questions": {"value": "please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HMImKGoTqw", "forum": "SRl0xy0UOj", "replyto": "SRl0xy0UOj", "signatures": ["ICLR.cc/2026/Conference/Submission13292/Reviewer_BFqe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13292/Reviewer_BFqe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828522276, "cdate": 1761828522276, "tmdate": 1762923960304, "mdate": 1762923960304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Constructive Distortion, a training strategy for large vision-language models designed to enhance robustness and fine-grained understanding through targeted visual perturbations. Instead of random noise or masking, the approach applies semantically constructive distortions—guided transformations (e.g., spatial deformation, contrast warping) that preserve semantics while challenging the model’s visual encoder. The method aims to improve generalization to distorted or out-of-distribution visual inputs without sacrificing in-distribution performance. Experiments on benchmarks such as MM-Vet, MME, and LLaVA-Bench demonstrate consistent improvements, particularly under degraded or perturbed visual conditions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel concept: Shifts from destructive to constructive data augmentation, promoting robustness while maintaining semantic fidelity.\n\n- Strong empirical results across diverse LVLMs and benchmarks, with detailed ablations on different distortion types and intensities.\n\n- Practical contribution: The approach is plug-and-play and compatible with existing LVLM training pipelines.\n\n- Clear motivation and presentation: The paper is well-written and the concept of “constructive” perturbation is intuitively appealing."}, "weaknesses": {"value": "Lack of comparison with visual token compression methods. The paper does not contextualize its approach relative to recent efficient LVLM frameworks that also modify the visual representation process. Works such as [1] PVC: Progressive Visual Token Compression (Yang et al., 2024), [2] Efficient Large Multi-Modal Models via Visual Context Compression (Chen et al., NeurIPS 2024), and [3] An Image Is Worth 1/2 Tokens After Layer 2 (Chen et al., ECCV 2024) explore representation simplification and robustness trade-offs at the token level. Including comparisons or discussion would clarify whether constructive distortions yield complementary or competing benefits.\n\n[1] Yang C, Dong X, Zhu X, et al. PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision- \n\n[2] Chen J, Ye L, He J, et al. Efficient large multi-modal models via visual context compression[J]. Advances in Neural Information Processing Systems, 2024, 37: 73986-74007. \n\n[3] Chen L, Zhao H, Liu T, et al. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024: 19-35."}, "questions": {"value": "Could you please provide more comprehensive evaluation (e.g., VQAv2) and (attention-based) token compression baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2ZquhK6xiH", "forum": "SRl0xy0UOj", "replyto": "SRl0xy0UOj", "signatures": ["ICLR.cc/2026/Conference/Submission13292/Reviewer_6v6J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13292/Reviewer_6v6J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762335136912, "cdate": 1762335136912, "tmdate": 1762923959975, "mdate": 1762923959975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}