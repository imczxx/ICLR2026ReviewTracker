{"id": "2EQPpEZtEK", "number": 16779, "cdate": 1758268583067, "mdate": 1759897219832, "content": {"title": "DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation", "abstract": "Recent attempts to interleave autoregressive (AR) sketchers with diffusion-based refiners over continuous speech representations have shown promise, but they remain brittle under distribution shift and offer limited levers for controllability. We introduce DiSTAR, a zero-shot text-to-speech framework that operates entirely in a discrete residual vector quantization (RVQ) code space and tightly couples an AR language model with a masked diffusion model, without forced alignment or a duration predictor. Concretely, DiSTAR drafts block-level RVQ tokens with an AR language model and then performs parallel masked-diffusion infilling conditioned on the draft to complete the next block, yielding long-form synthesis with blockwise parallelism while mitigating classic AR exposure bias. The discrete code space affords explicit control at inference: DiSTAR produces high-quality audio under both greedy and sample-based decoding using classifier-free guidance, supports trade-offs between robustness and diversity, and enables variable bit-rate and controllable computation via RVQ layer pruning at test time. Extensive experiments and ablations demonstrate that DiSTAR surpasses state-of-the-art zero-shot TTS systems in robustness, naturalness, and speaker/style consistency, while maintaining rich output diversity. Audio samples are provided on \\url{https://anonymous.4open.science/w/DiSTAR_demo}.", "tldr": "", "keywords": ["text-to-speech", "residual vector quantization", "masked diffusion model", "autoregressive language model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff35f0a1f8ab31a3526b894371a9e1461ebd302f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper extends DiTAR by replacing continuous code with RVQ codes and use a LLaDA style masked diffusion transformer to predict the next code patch."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. There is some limited novelty in combining LLaDA style diffusion transformer with DiTAR approach."}, "weaknesses": {"value": "1. The paper is a bit difficult to read with some grammar issues. If possible, I suggest the authors to seek help from native English speakers to make the paper more reader friendly.\n2. The authors claim the model to be SOTA in robustness, speaker similarity and naturalness but the results in Table 1 seems to indicate otherwise? The Speaker SIM and UTMOS scores are lower than competitors.\n3. The evaluation section seems a bit sketchy overall. Why are the models compared in Table 2 different from Table 1? Subjective evaluations are missing key details (e.g. number of evaluators, number of samples per evaluator). The ablation study only covers decoding strategies but not other design choices."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ojQvbXl8X8", "forum": "2EQPpEZtEK", "replyto": "2EQPpEZtEK", "signatures": ["ICLR.cc/2026/Conference/Submission16779/Reviewer_rLfL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16779/Reviewer_rLfL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761167077445, "cdate": 1761167077445, "tmdate": 1762926822183, "mdate": 1762926822183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, authors proposes DiSTAR, a zero-shot TTS framework that works entirely in a discrete residual vector quantizatio code space, coupling an autoregressive language model (sketcher) with a masked diffusion Transformer (refiner). The approach avoids forced alignments and duration predictors, instead using blockwise parallelism where the AR model drafts RVQ token sketches for each patch and the diffusion model performs parallel masked infilling to complete the block. Using DiSTAR discrete latent space can be directly used for controllability, supports a variety of decoding strategies, and allows inference-time bitrate and compute control by pruning RVQ layers. The system is evaluated on standard zero-shot TTS benchmarks. DiSTAR  demonstrates improvements over recent baselines in robustness, naturalness, and speaker consistency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(a) The paper presents a integration of an autoregressive LM and a diffusion model operating on RVQ discrete tokens. This combination addresses weaknesses of purely-AR or purely-diffusion approaches. Furthermore, the idea of iterative discrete demasking (inspired by LLaDA) is technically interesting and new in the TTS domain.\n\n(b) The empirical results back up the claims of improved robustness, high naturalness, and better speaker consistency (SMOS) across unseen voices.\n\n(c) Eliminating the need for forced aligners, duration models, or external text-speech alignment is a another practical strength of the proposed work."}, "weaknesses": {"value": "(a) The proposed appraoch is the combination of different techniques, each individual component draws on previously known ideas, so the perceived novelty is Incremental. DiSTAR’s core innovation is applying masked diffusion in the discrete RVQ domain, which is new, but conceptually it parallels prior AR+refinement pipelines and the LLaDA diffusion LM approach in NLP.\n\n(b) The method is complex and lack in clarity. Furthermore the system involves multiple components and a non-trivial training procedure, which are not fully transparent in the description .For example i cannot understand clearly how the AR hidden sketch is defined and used.  Is it generating one coarse codebook stream, a fused embedding per frame, or something else?\n\n(c) The results claim comparable inference speed to a baseline (DiTAR), but since it still relies on an iterative diffusion process for each patch, which may be a bottleneck.\n\n(d) Couple of relevant baselines are absent. In particular, there is no direct comparison to a pure AR discrete token model of comparable size on the same data. Without an explicit AR-only baseline, it’s hard to isolate how much the diffusion refiner helps beyond a standard AR approach\n\n(e) I think an ablation where the diffusion module is removed (i.e. the AR alone generates all codebooks) would be insightful. Does the diffusion mainly help with fine detail, or also with stability (WER)?\n\n(f) The authors mention that DiSTAR is less sensitive to high-frequency artifacts in the reference prompt than others, attributing better speaker cloning to this. However, it’s unclear why ? Is the diffusion refiner helps ignore prompt noise.? There is a need to evaluate the robustness under prompt domain shift."}, "questions": {"value": "(a) How are the AR drafter and diffusion refiner trained jointly or sequentially? It is implied in the paper that a shared token space allowing end-to-end optimization , but can you clarify if you train the AR LM and the diffusion Transformer simultaneously or in stages ?\n\n(b) Did you test scenarios beyond the lengths in the benchmark (e.g., generating several minutes of speech concatenating multiple paragraphs)? Does the model maintain speaker identity and prosody consistently in truly long sequences?\n\n(c) Could you provide more details on pruning? For example, if you drop the top $k$ codebooks at inference (using only the first $L-k$ RVQ layers), how does it impact MOS or WER?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5KZcUseHun", "forum": "2EQPpEZtEK", "replyto": "2EQPpEZtEK", "signatures": ["ICLR.cc/2026/Conference/Submission16779/Reviewer_tPVS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16779/Reviewer_tPVS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922817050, "cdate": 1761922817050, "tmdate": 1762926821816, "mdate": 1762926821816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DiSTAR is a method involving the recently popular paradigm of combining the benefits of autoregressive decoder-only LMs with diffusion models. In this specific instance, the entire architecture relies only on discrete tokens from an RVQ codec-based audio tokenizer, which is unlike previous work (DiTAR) where continuous latents are used. Consequently, the diffusion process is now a masked diffusion model. \n\nThe DiSTAR architecture involves aggregated patch-wise tokens fed to the AR model which “sketches” the next patch. The MDM refines the aggregate token into the RVQ tokens conditioned on previous token predictions. The method involves training tricks like dropping out RVQ layers that help the model remain robust across a wide range of bitrates.\n\nComparing with other state-of-the-art TTS models shows that the DiSTAR achieves comparable quality across various metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of the paper is the fact that it shows how to apply the AR + Diffusion paradigm to TTS using multi-level RVQ discrete audio tokens which helps remove the need for separate duration predictors and stop predictor; simply predicting [eos] tokens in the AR step is enough. This approach of patch-wise AR prediction mitigates some of the error-accumulation issues since the finer RVQ tokens are being generated in the masked diffusion sampling stage. The results also look good, with both subjective and objective metrics showing comparable results against strong baselines."}, "weaknesses": {"value": "Weaknesses and questions:\n- The authors use some embedding initialization trick but do not cite any existing work or ablate the design to prove it is effective.\n- Similarly the utility of stochastic layer truncation is not cited/ablated. I believe the DAC (descript audio codec) paper does use this technique in the training of DAC but the authors are using it in the training of the LM and MDM on top of the RVQ codec. Will this still be needed if the authors used DAC or the RVQ decoder is already trained with quantizer dropout?\n- The authors mention in the abstract that AR+Diffusion models on continuous latents are brittle under distribution shifts but do not really run any experiments that compare DiTAR vs DiSTAR under such a setting.\n- The claims in the abstract regarding surpassing state-of-the-art for speaker similarity seem a little exaggerated based on the results shown in the paper. SMOS is very close to E2TTS with a wide spread, and the objective SIM metric is also lower than some other baselines."}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oBwROuronm", "forum": "2EQPpEZtEK", "replyto": "2EQPpEZtEK", "signatures": ["ICLR.cc/2026/Conference/Submission16779/Reviewer_mwhZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16779/Reviewer_mwhZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979374173, "cdate": 1761979374173, "tmdate": 1762926821274, "mdate": 1762926821274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}