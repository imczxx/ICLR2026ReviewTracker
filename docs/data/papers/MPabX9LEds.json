{"id": "MPabX9LEds", "number": 2497, "cdate": 1757126086858, "mdate": 1759898144504, "content": {"title": "Learning Massively Multitask World Models for Continuous Control", "abstract": "General-purpose control demands agents that act across many tasks and embodiments, yet research on reinforcement learning (RL) for continuous control remains dominated by single-task or offline regimens, reinforcing a view that online RL does not scale. Inspired by the foundation model recipe (large-scale pretraining followed by light RL) we ask whether a single agent can be trained on hundreds of tasks with online interaction. To accelerate research in this direction, we introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, each with language instructions, demonstrations, and optionally image observations. We then present Newt, a language-conditioned multitask world model that is first pretrained on demonstrations to acquire task-aware representations and action priors, and then jointly optimized with online interaction across all tasks. Experiments show that Newt yields better multitask performance and data-efficiency than a set of strong baselines, exhibits strong open-loop control, and enables rapid adaptation to unseen tasks. We release our environments, demonstrations, code for training and evaluation, as well as 200+ checkpoints. Website: https://newt-world-models.github.io", "tldr": "We introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, and train a language-conditioned multi-task world model on all 200 tasks via online interaction.", "keywords": ["reinforcement learning", "world models", "continuous control"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cead0514942eabe10ef5f3ba507e0a336b2330c6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents an architecture to perform online RL on a bigger scale, training one policy for many different tasks. The learning algorithm requires as a first step a behavioural cloning approach to warm-start the network and make the exploration problem simpler. It then performs online RL, outputting actions after an MPC planning step trough a world model. It hence combines model-based and model-free RL approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents good results on continous  control benchmarks which is still an interesting problem. Especially training one policy over this big variety of tasks is interesting to see. The algorithm is presented clearly and is easy to follow, and the model-based MPC aspect of it is interesting. I really appreciate the effort of the authors of making the code and the checkpoints accessible, this makes it possible to reproduce the results and build on-top of them."}, "weaknesses": {"value": "My major concern is the applicability of this to real-world continuous control problems. While in simulation the results look good, it requires over 100M steps to train this policy which would be unfeasable on a real-world application. I also think the paper would benefit from ablating the usefulness of the different components - specifically interesting would be to understand how useful is the mpc planning is, how much learning of the world model helps performance as well as  how much does the initial bc training helps."}, "questions": {"value": "how useful is the mpc planning? \nhow much does learning of the world model helps performance?\nhow much does the initial bc training help?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZyO9oP7m6d", "forum": "MPabX9LEds", "replyto": "MPabX9LEds", "signatures": ["ICLR.cc/2026/Conference/Submission2497/Reviewer_uUxq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2497/Reviewer_uUxq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761228045105, "cdate": 1761228045105, "tmdate": 1762916256020, "mdate": 1762916256020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the scalability challenge of online RL for continuous control by introducing MMBench, a large-scale benchmark spanning 200 diverse tasks across 10 domains (e.g., DMControl, Meta-World, Atari, and a newly proposed MiniArcade). Each task includes language instructions, demonstrations, and multi-modal observations. The authors also propose Newt, a language-conditioned multitask world model built upon TD-MPC2. Newt first pretrains on demonstrations to obtain task-aware representations and action priors, then performs online optimization across tasks with architectural refinements and action supervision. Experiments show that Newt outperforms several baselines (BC, PPO, FastTD3) in multitask performance and exhibits generalization to unseen tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper has several notable merits:\n- MMBench provides a unified framework across 10 heterogeneous domains with consistent data handling and language-conditioned tasks.\n- The paper introduces reasonable design choices such as discrete regression for reward/value prediction and per-task discount factors, with comprehensive ablations supporting their impact.\n- The paper is well-organized, figures effectively illustrate key results, and open-sourced resources (200+ checkpoints, 4000+ demos) significantly enhance reproducibility.\n\nOverall, the paper represents a solid step toward scalable, general-purpose control systems."}, "weaknesses": {"value": "### 1. Novelty concerns in core contributions\n\nWhile MMBench contains 200 tasks, most of them are directly inherited from existing benchmarks (e.g., DMControl, Meta-World). This limits its novelty compared to benchmarks such as ManiSkill3, which introduces fundamentally new task paradigms. Similarly, Newt—although incorporating CLIP/DINOv2 encoders and demonstration conditioning—builds incrementally upon TD-MPC2, without a clear paradigm shift.\n\n### 2. Missing analysis of task scalability\n\nThe paper emphasizes scaling to hundreds of tasks but lacks quantitative analysis on scaling behavior. For example, performance is not evaluated as the task count increases (e.g., 50 → 100 → 200 tasks), leaving unclear whether multitask training indeed benefits from more tasks. Moreover, since MMBench spans disjoint domains (e.g., Atari vs. Meta-World), the paper should analyze cross-domain transfer—whether training on visually distinct domains interferes with or enhances learning in others—and include ablations comparing full multitask vs. domain-specific training.\n\n### 3. Incomplete baselines in MBRL\n\nExperimental comparisons primarily focus on model-free RL and behavioral cloning baselines, while omitting competitive model-based RL counterparts. In particular, DreamerV3, which is explicitly designed for multitask continuous control, is absent, as is a multitask-adapted TD-MPC2 baseline (used for demo collection). Without these, it is difficult to attribute performance gains to the proposed architectural innovations rather than inherited advantages from TD-MPC2.\n\n### 4. Insufficient evaluation under state-limited conditions\n\nMost evaluations assume access to full low-dimensional states, which is unrealistic in real-world control settings. The paper lacks experiments under state-limited or purely visual conditions (e.g., partial observations or agent-only states), which would better demonstrate robustness and practical applicability."}, "questions": {"value": "1. The paper employs masking to handle inconsistent action/state dimensions. Could masking lead to sparse gradient updates or optimization inefficiencies for high-dimensional action spaces (e.g., humanoid control with 50+ joints)? Have you explored task-specific action embeddings or shared latent action representations as scalable alternatives?\n\n2. Table 3 reports Newt’s training time, but it is unclear whether this includes the cost of training 200 single-task TD-MPC2 agents used for demonstration generation. Please clarify whether the total computation time includes both stages (demo collection + multitask training) to ensure fair comparison and full cost transparency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "55C1gPr24b", "forum": "MPabX9LEds", "replyto": "MPabX9LEds", "signatures": ["ICLR.cc/2026/Conference/Submission2497/Reviewer_73Ch"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2497/Reviewer_73Ch"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704205049, "cdate": 1761704205049, "tmdate": 1762916255683, "mdate": 1762916255683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MMBench, a multitask RL benchmark containing 200 different control tasks across 10 domains. TDMPC2 agents on each single task are trained to collect expert demonstrations, while both the model checkpoints and demonstrations are open-source. In addition, this paper substitute language descriptions for task indices for better distinguish among tasks, which makes Newt,  a multi-task world model based on TD-MPC2. Experiments show comparable results with a strong baseline FastTD3."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a benchmark which integrates domains that are popularly studied in the RL community, releasing single-task checkpoints and dataset which is significant not only for multi-task RL but also offline, O2O and continuous RL research.\n2. The empirical results show advantages over baselines on ManiSkill and DMControl. \n3. Model info such as training time, model architecture is detailed presented.\n4. The figures are well drawn and easy to understand."}, "weaknesses": {"value": "1. There is no preliminary description so the problem setting confuses me at the beginning. In the multi-task RL setting a task label $n$ should be added to the original $(s, a, s', r)$ but in Line.275 there is only $(s, a, r)$. \n\n2. Newt only shows performance boost over baselines in DMC and Maniskill out of all 10 domains. In Meta-World, MuJoCo, Box2D, Robodesk and Atari it's just on par with FastTD3, while in OGBench and MiniArcade it's on par with behavior cloning. \n\n3. Selected baselines are not strong enough. FastTD3 is a strong baseline and in its original paper is compared with strong model-based baselines such as TDMPC2 and Dreamerv3, but it mainly reports results on humanoidbench, mujoco playground and Issaclab, neither of them are included in MMBench. Moreover, as long as Newt is built upon TDMPC2, it surprises me that TDMPC2 is not listed as a baseline in this paper."}, "questions": {"value": "1. To make it clear, is there only one agent being trained to interact with all 200 environments and collect online trajectories?\n\n2. What does the \"Language Instructions: None\" refer to in Line 399-405? Is there a task index provided for each trajectory?\n\n3. Is there any results that support the claim in Line 147 that one-hot encoding limits the potential for transferring to unseen tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W8pvGCVoVm", "forum": "MPabX9LEds", "replyto": "MPabX9LEds", "signatures": ["ICLR.cc/2026/Conference/Submission2497/Reviewer_nJ1K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2497/Reviewer_nJ1K"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831714071, "cdate": 1761831714071, "tmdate": 1762916255407, "mdate": 1762916255407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}