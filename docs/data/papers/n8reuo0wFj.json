{"id": "n8reuo0wFj", "number": 20984, "cdate": 1758312370155, "mdate": 1763708666473, "content": {"title": "Interpretable Graph Embeddings: Feature-Level Decomposition for Trustworthy Graph Neural Networks", "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance on tasks such as user-item interaction prediction in recommender systems, molecular property classification, and credit risk scoring and fraud detection in financial risk modeling. However, their opaque embedding mechanisms raise critical concerns about transparency and trustworthiness. Existing explainability approaches largely focus on identifying the nodes, edges, or subgraphs that influence the model's prediction but fail to disentangle how individual node features shape learned embeddings. In this work, we propose a novel decomposition framework that systematically attributes each embedding to original node and/or edge features. We qualitatively demonstrate the framework on Graph Convolutional Networks (GCN) and Heterogeneous GraphSAGE (HinSAGE) using Cora and MovieLens, and quantitatively benchmark against widely adopted baselines across multiple datasets. Results indicate that our approach improves interpretability by revealing how node features contribute to individual graph embeddings and clarifying the role of neighborhood aggregation in shaping predictions. This work connects structural explainability and feature-level attribution, providing a principled foundation for trustworthy and actionable GNN explanations.", "tldr": "", "keywords": ["Graph Neural Networks", "Embedding Decomposition", "Explainable AI"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4ee5cacd02de8bcbb6fab00432bbd05432afde7.pdf", "supplementary_material": "/attachment/23aaae3770f3ce030c92bdacb74ab976f840e2a1.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a method to decompose GNN embeddings. The core idea is that for each node (or edge) embedding dimension, trace exactly which raw features from which nodes contributed to it, and by how much. The authors argue that this matters in practice because graph embeddings are often exported as engineered features into downstream models. Then the paper walks through this idea for a 2-layer GCN on Cora and for a 1-layer HinSAGE / GraphSAGE-style model on MovieLens. After reading the main text and appendix, I lean toward **rejection rating since the empirical evidence is mostly case studies with limited quantitative rigor, some important details are under-specified or internally inconsistent, the writing and presentation quality are poor, and the required ICLR submission compliance items (ethics statement, reproducibility statement) are also missing.**"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Clear practical motivation. The paper targets a nice point: people deploy GNNs to generate embeddings, then feed those embeddings into non-GNN models, and regulators ask what those embedding dimensions mean.\n2. Concrete layer-wise accounting. For simple message passing with ReLU, the paper gives an explicit linear expansion that attributes each embedding coordinate of node $v$ to specific source features along $w \\to u \\to v$ paths, and then repeats the exercise on a heterogeneous user–movie graph to separate self-node vs. neighbor-type contributions. I think this is technically transparent and easy to follow for shallow GCN layers."}, "weaknesses": {"value": "1. **Insufficient quantitative evidence and unclear evaluation protocol.**\n   - The Cora study reports that replacing bag-of-words with learned GCN embeddings boosts downstream XGBoost performance (accuracy 0.57→0.76, weighted F1 0.56→0.75), and the MovieLens study reports that adding HinSAGE-derived user/movie embeddings to a regressor improves MSE, MAE, and $R^2$.     However, the paper does not fully spell out how training vs. testing separation is enforced when those downstream models consume embeddings. For MovieLens in particular, it is not explained whether held-out user–movie rating edges are completely excluded from the message-passing graph during training or whether information can leak in through sampled neighborhoods.   This matters for how strong the reported gains really are.\n   - Also, despite repeatedly calling the method “exact,” there is no sanity check that the summed per-feature attributions numerically reconstruct the original embedding vector with near-zero error. Finally, the “noisy Cora” benchmark is only described qualitatively. I do not see concrete numbers, baseline hyperparameters, so it is hard to verify the claims of best noise suppression. \n\n2. **Over-claiming determinism and inconsistency.**\n   - The paper’s pitch is that attribution is deterministic, closed-form, and faithful. For a 2-layer GCN with fixed ReLU masks, that is largely true.   But later sections acknowledge that for smoother activations (GELU, Swish, Mish), LayerNorm / BatchNorm, attention, or pooling, the method needs approximations, local linearization, or heuristic redistribution of contribution.   Likewise, HinSAGE relies on stochastic neighborhood sampling, and the text itself says explanations are “conditional on the sampled computation graph,” suggesting that multiple samples must be averaged to get stable attributions. That weakens the claim of strict determinism. \n   - In addition, in the MovieLens case study the narrative sometimes refers to “dimension 25,” “dimension 11,” and four disjoint 8-D blocks (user self, user’s neighbor movies, movie self, movie’s neighbor users), but Appendix A.3 then says the concatenated user/movie embeddings are passed through an extra dense layer before regression.     It is unclear which vector (pre-MLP concat vs post-MLP hidden) is actually being interpreted when making age/genre claims. This ambiguity undercuts the “faithful, auditable story” the paper promises.\n\n3. **Reproducibility and reporting gaps.**\n   - The paper does not provide ICLR required reproducibility statement and any other related stuff like code, anonymized repo, or even pseudocode for the attribution backprop and the PCA aggregation step that collapses 16 embedding dims into 5 “principal explanation axes.”   It also does not clearly define which PCA-based score ($s^{\\text{PCA}}$ or $s^{\\text{PCA-var}}$ in the appendix) is used to generate each figure. These missing details reduce the soundness of the experiment results and make it hard to verify or reuse the method.\n\n4. **Writing and presentation issues.**\n   The writing quality is poor. And there are repeated sentences, spacing/typo issues (e.g., “interprete” in line 255). The paper pasted an image as a table in Fig. 3 (Actually, this is greatly discouraged from the ICLR template). The future work section in 4.2 seems not give any insightful analysis and just lists GAT and GT (especially on GT, the whole paragraph is just a common intro and conveys nothing related to this paper)\n\n5. **Missing required ICLR statements.**\n   The submission lacks an Ethics Statement and a Reproducibility Statement, and it does not include anonymous code or supplementary artifacts."}, "questions": {"value": "1. Downstream usage protocol. When you train the external XGBoost (for Cora classification and for MovieLens rating regression), how exactly do you split train vs. test so that no test embeddings or edges leak into training, even indirectly through message passing or neighbor sampling? Please spell out that protocol and confirm that the reported accuracy / F1 / MSE / MAE / $R^2$ numbers are on held-out data only.\n\n2. Faithfulness sanity check. Can you report the $\\ell_2$ reconstruction error between a learned embedding vector $h_v$ and the sum of all propagated per-feature contributions plus bias terms? This would give direct numerical evidence for the claimed “exact” decomposition.\n\n3. Determinism vs. sampling. For HinSAGE, you mention that explanations are conditional on the sampled computation graph and could be averaged over multiple samples. How much variance do you actually observe across samples in practice, and is that variance acceptable for audit scenarios? \n\n4. PCA aggregation. In Figures where you say “dimension 11 drives younger users to like sci-fi,” which representation are we talking about: the raw concatenated [user emb ∥ movie emb] vector, or the 16-D dense layer after concatenation?\n\n5. Baselines in the noisy-feature stress test. For GNNExplainer, Integrated Gradients, GraphLIME, GOAt, etc., what hyperparameters and runtime settings did you use, and can you share the numeric counts of noisy features recovered in Top-10 plus mean ± std runtime? Without numbers, it is hard to judge the “we are both cleaner and faster” claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "This paper lacks the required Ethics Statement in the submission. Personally, I think no ethics review is needed, yet the MovieLens case explicitly analyzes demographic attributes such as age and gender as explanatory drivers for recommendation-like predictions, which may raise related concerns, so giving the required Ethics Statement in the paper is necessary."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zg4DDtM9zj", "forum": "n8reuo0wFj", "replyto": "n8reuo0wFj", "signatures": ["ICLR.cc/2026/Conference/Submission20984/Reviewer_7hVM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20984/Reviewer_7hVM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681361049, "cdate": 1761681361049, "tmdate": 1762939778170, "mdate": 1762939778170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for interpreting graph neural networks (GNNs) by focusing on the decomposition of embeddings at the feature level. The authors propose an technique for GNN layers, transforming each layer into a contribution pathway, which allows fine-grained attribution across heterogeneous feature streams. The framework is demonstrated on two GNN architectures: Graph Convolutional Networks (GCN) and Heterogeneous GraphSAGE (HinSAGE), with experiments conducted on Cora and MovieLens datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is well-written and clearly organized."}, "weaknesses": {"value": "1.\tWhat is the difference and relationship between the method proposed by the authors and the GNN-LRP [1] method? Is it a variant of the GNN-LRP method or a new approach for contribution allocation?\n2.\tThe authors should compare their method with more path-based explanation methods for GNNs, such as GNN-LRP [1], FlowX [2], and AxiomPath-Convex [3], as the method proposed by authors also calculates contribution values ​​based on paths.\n3.\tThe authors should test their method on more datasets, as the current number of datasets is too limited.\n\n[1] Schnake T, Eberle O, Lederer J, et al. Higher-order explanations of graph neural networks via relevant walks[J]. IEEE transactions on pattern analysis and machine intelligence, 2021, 44(11): 7581-7596.\n\n[2] Gui S, Yuan H, Wang J, et al. Flowx: Towards explainable graph neural networks via message flows[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023, 46(7): 4567-4578.\n\n[3] Liu Y, Zhang X, Xie S. A differential geometric view and explainability of gnn on evolving graphs[J]. arXiv preprint arXiv:2403.06425, 2024."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ips3hEyrxH", "forum": "n8reuo0wFj", "replyto": "n8reuo0wFj", "signatures": ["ICLR.cc/2026/Conference/Submission20984/Reviewer_mPHL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20984/Reviewer_mPHL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817826199, "cdate": 1761817826199, "tmdate": 1762939726514, "mdate": 1762939726514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a decomposition framework for explaining GNN embeddings by attributing each embedding dimension to original node and edge features. The core idea is to treat GNN layers as linear operators (after fixing activation patterns) and propagate contribution matrices backward through the network in parallel with forward pass.\n\nThe paper addresses a relevant problem with a mathematically sound approach, but has significant weaknesses in evaluation methodology, technical novelty, and experimental validation. The lack of quantitative metrics and baseline comparisons is particularly concerning for a paper claiming to provide \"trustworthy\" explanations. Major revisions addressing these issues would be necessary before the paper meets standards for publication at a top-tier venue."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "S1: The paper addresses a relevant gap in GNN explainability literature. When GNN embeddings are used as engineered features in downstream models, understanding what information these embeddings capture becomes critical\n\nS2: The framework is applied to two different GNN architectures and two different tasks, showing that the core idea can be adapted to different settings.\n\nS3: The method is computationally efficient. It reduces to simple matrix multiplications and does not require optimization, sampling, or iterative procedures."}, "weaknesses": {"value": "W1: The paper's most critical limitation is absence of quantitative evaluation of explanation quality. All main results (Figures 1, 2) are purely qualitative visualizations without numerical metrics. There is no formal definition of what constitutes a \"good\" or \"faithful\" explanation, no metrics to measure correctness of attributions, and no way to objectively compare explanation quality across methods. The only quantitative experiment (Section 3.3) measures noise feature suppression, which is insufficient to validate the method comprehensively. For a paper claiming to provide \"trustworthy\" explanations, this is a significant gap.\n\nW2: The paper does not compare against straightforward adaptations of gradient-based explanation methods (e.g., Integrated Gradients, GradCAM) for explaining embeddings. Computing $\\frac{\\partial h^{(L)}_v[d]}{\\partial x_w[p]}$ for each embedding dimension $d$ with respect to input features would provide conceptually similar attributions. Without this comparison, it is unclear what advantages the proposed method offers beyond standard gradient-based approaches. The comparison in Section 3.3 uses methods designed for explaining predictions, not embeddings, which makes comparison unfair.\n\nW3: The core technical contribution (Section 2.1) is essentially standard application of chain rule for backpropagating through linear transformations with ReLU activations. The propagation equation is straightforward linear algebra. The extensions to GCN and HinSAGE (Sections 2.2-2.3) are direct applications without significant innovation.\n\nW4: \tSeveral highly relevant works on embedding interpretability are missing:\n- Piaggesi et al. \"Dine: Dimensional interpretability of node embeddings\" (TKDE 2024)\n- Shafi et al. \"Generating human understandable explanations for node embeddings\" (2024)\n- Dalmia et al. \"Towards interpretation of node embeddings\" (WWW 2018)\n- Piaggesi et al. \"Disentangled and Self-Explainable Node Representation Learning\" (TMLR 2025)\nThese works contain relevant metrics for quantitative evaluation that could strengthen this paper."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DWX5s21hWK", "forum": "n8reuo0wFj", "replyto": "n8reuo0wFj", "signatures": ["ICLR.cc/2026/Conference/Submission20984/Reviewer_9dA9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20984/Reviewer_9dA9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950566292, "cdate": 1761950566292, "tmdate": 1762939640529, "mdate": 1762939640529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a framework aimed at explaining the embeddings of GNNs at the feature level, which is a clear task relevant for the community. The central premise is that existing GNN explainability methods focus on identifying important graph structures (nodes/edges) for a given prediction, but fail to decompose the learned embedding itself into contributions from the original input features. To address this, the paper proposes a method to \"invert\" GNN layers. The core idea is to treat a GNN layer as a linear operator once the outcomes of its non-linear activation functions are fixed for a specific input. By propagating \"contribution matrices\" backward through this linearized computational graph, the method claims to produce an exact decomposition of the final embedding vector into a sum of contributions from every input feature of every node in its computational subgraph. The approach is demonstrated on GCN and HinSAGE architectures using the Cora and MovieLens datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper identifies an interesting and relevant problem within the GNN explainability literature. The distinction between explaining a final prediction and explaining the intermediate embedding representation is a valid one, and the goal of achieving feature-level attribution is desirable. The authors additionally frame the vast literature about explainability for GNN, comparing some popular methods with each other (Table 1), and showing potential gaps and future research directions."}, "weaknesses": {"value": "Despite the interesting problem statement, the paper's methodology, claims, and experimental validation suffer from fundamental flaws, lack of clarity, and are of insufficient quality for publication at a top-tier conference as ICLR. \nHere are some points that could be addressed by the authors to improve their work:\n\n- The central and most critical weakness lies in the claim of providing an \"exact decomposition\" of the GNN's embedding. The method does not explain the true GNN model; instead it explains a linearized surrogate of the model that is valid only for a single input instance. The core of the method is to replace the non-linear activation function with a diagonal gating matrix $D$ (not formally defined). While the decomposition is \"exact\" for this new linear model, this is not the GNN that made the prediction. The non-linearity is a crucial component of the GNN's expressive power (or any Neural Network); hence, by neglecting it, the authors are analyzing a totally different function. This all raises severe questions about the method's faithfulness to the original model's learned representations and its overall justification. \nAdditionally, in Sec 4.1 the authors admit that this method achieves \"exactness only for specific activation and normalization classes.\", i.e., piece-wise linear activations, relegating the contribution of this paper to a narrow set of architectures. Therefore, the proposed scope of a general framework for feature-level explanation of GNN is rather a misleading characterization, as well as an overstatement.\n- The experimental evaluation is limited to two specific datasets, Cora and Movielens. These are small and outdated benchmark datasets that do not test the efficacy of an explanation method in complex scenarios, or with larger graphs, or with higher-dimensional features and more subtle feature interactions. There are many other benchmarks, also for XAI, that should be considered for an impactful publication, which are employed in the related works cited by the authors too. Also, the authors compare their embedding explainer against methods designed to explain predictions (like GNNExplainer), which feels like a mismatched comparison.\n- The evaluation is largely qualitative and anecdotal. The t-SNE plots (Fig 1) are visually appealing but notoriously unreliable for drawing firm conclusions (it does not really show \"clear separation\" across categories). The case studies on MovieLens (Figures 2, 6, 7, 8) present \"just-so\" stories about specific embeddings and features. While illustrative, they lack a rigorous and quantitative validation, and it is unclear if these cherry-picked examples are representative or meaningful (What about the other embeddings?).\n-  The method outputs a contribution matrix $C$ where $C[p, k]$ represents the contribution of input feature $p$ to the $k$-th dimension of the final embedding (for some given node). It is entirely unclear how a user is meant to interpret this. What does it mean for the \"scaled_age\" feature to contribute -0.21 to the 25th embedding dimension for \"animation\" movies? The paper attempts to solve this by using PCA to create aggregate scores, but this introduces yet another layer of approximation and obfuscation, as the principal components themselves often lack clear semantic meaning. The final explanations are not inherently interpretable and require significant post-hoc processing that further distances them from the original model.\n- The code is not provided, nor as supplementary material nor in an anonymized repo, hence it is impossible to reproduce the results and make this work less trustworthy.\n\nMinor comments that did not affect the score:\n- Although the part on the related work is clear and well-written, many arXiv references should be avoided as they are not peer-reviewed scientific contributions. Please refer to published versions when available, or avoid referencing arXiv unless strictly necessary.\n- Sec. 2.1 on the definition of a GNN is not correct. GNNs entail a broad class of methods, and what you describe is not a \"generic\" GNN, but rather an instance of the message-passing relational graph convolutional operator (e.g., Schlichtkrull 2017). This should be properly defined.\n- The quality of figures and their labels do not meet publication standards (e.g., Fig. 2 has fonts too small).\n- Section 4.2 about Future works is inconclusive. What other advancements are possible beyond implementing this work in other convolutions?\n- The statement \"our proposed attribution-based decomposition not only provides interpretability for graph-based embeddings but also uncovers actionable insights into how heterogeneous relational signals shape predictive performance, thereby bridging the gap between representation learning and model explainability.\" is too exaggerated. You reported one simple example, prone to confirmation bias, that does not \"bridge the gap between representation learning and model explainability\", nor theoretically nor experimentally."}, "questions": {"value": "- How can you claim the method is \"exact\" and faithful when it analyzes a linearized and input-dependent (PCA) surrogate model? The function being explained is fundamentally different from the GNN used for prediction. Could you please clarify this discrepancy?\n- How is the \" $D_v^{(l)}$ Diagonal mask from activation of node $v$ at layer $ℓ$\" defined? It is a relevant component of your framework, but it is not obvious and never defined.\n- The experimental validation relies heavily on qualitative case studies and a single synthetic downstream task. Why were established quantitative explainability metrics (e.g., Fidelity, Sparsity, Robustness) not used to provide a more rigorous comparison against baseline methods? What is the reason behind using only 140 for training in Cora? Did you cross-validate your results across other data splits and multiple runs?\n- You are using XGBoost for downstream tasks on GNN embeddings instead of the original high-dimensional data embeddings. You retrieve the feature contributions via a PCA decomposition. Could you please try to apply PCA on Cora and then XGBoost without the GNN part? This ablation study is missing, but it would be essential to pinpoint the effects of the GNN (and its embeddings).\n- How is it possible that your method is an order of magnitude faster than a random explainer that does not perform any computation at all?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iFBgcmMwRR", "forum": "n8reuo0wFj", "replyto": "n8reuo0wFj", "signatures": ["ICLR.cc/2026/Conference/Submission20984/Reviewer_KsJu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20984/Reviewer_KsJu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762434819427, "cdate": 1762434819427, "tmdate": 1762939567659, "mdate": 1762939567659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}