{"id": "FStqfhYUSI", "number": 8757, "cdate": 1758097292637, "mdate": 1759897765685, "content": {"title": "Atoms to Events: Categorical Evidence Composition for Video Anomaly Detection", "abstract": "Video anomaly detection (VAD) seeks to identify events that deviate from learned normality. Current Vision–Language Models (VLMs) face significant challenges: anomalies are rare, labels are weak, and visual appearance varies drastically. Mainstream VLMs directly map visual features to events, they overfit to intermediate incidental cues which are present during training and generalize poorly. To address this issue, we propose a categorical view of anomaly understanding that replaces this \"visual features to event\" mapping with a \"visual features to learnable atoms, then to event\" framework that models direct, indirect, and counter evidence cues. Firstly, an Unsupervised Anomalous Period Detector (UAPD) is proposed to identify abnormal periods. Next, a Category-based Atom Miner (CAM) is proposed to map visual features to learned atoms in video segments, and learn the roles of atoms. In inference, CAM provides role-aware indications to VLM which maps meaningful atoms and visual features to event predictions. This framework harnesses meaningful evidence and preserves the generalization capacity of VLMs. Extensive experiments and ablations show consistent gains over strong vision‑only and fine‑tuned VLM baselines.", "tldr": "", "keywords": ["Video Anomaly Detection", "Vision–Language Models (VLMs)", "categorical view", "Unsupervised Anomalous Period Detector (UAPD)", "Category-based Atom Miner (CAM)"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e18b63506172237108d8c0a728c1244ecb0c121c.pdf", "supplementary_material": "/attachment/f8b077ec8063dcf6515dd1cc8ab1a11b8d46178e.pdf"}, "replies": [{"content": {"summary": {"value": "The authors propose an Unsupervised Anomalous Period Detector (UAPD) and a Category-based Atom Miner (CAM) to identify and learn the roles of direct, indirect, and counter evidence atoms. Guided by these atoms, a Vision–Language Model (VLM) performs interpretable and generalizable anomaly detection."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "a. The overall detection performance is compelling and achieves SOTA results.\n\nb. The framework combines unsupervised detection and language-guided reasoning, which is a promising direction for explainable video anomaly detection."}, "weaknesses": {"value": "a. The presentation quality of the paper is poor. Many newly introduced concepts are not clearly defined or intuitively explained, making the methodology difficult to follow.\n\nb. Figures are not clearly presented or discussed in the main text. Figure 1 is not referenced in the introduction, and its depiction of roles and relationships is unclear. Figure 2 also lacks clarity regarding the architecture and data flow — for example, it is not obvious what the three outputs of CAM represent, how CGSGM operates internally, or how the online and offline stages are connected."}, "questions": {"value": "a. Figure 1 is presented early but not referenced or explained in the Introduction. How does it support the motivation of the framework?\n\nb. The concept of an “Atom” should be clarified. What are its visual or semantic boundaries, and how are atoms identified in practice?\n\nc. Please provide detailed and concrete examples of “Direct,” “Indirect,” and “Counter” atoms in real video contexts. How are these categories determined or learned?\n\nd. How are anomaly scores computed? Are they produced only by UAPD, or further refined by CAM and VLM?\n\ne. What is the final output of CAM — are these atom presence scores, event likelihoods, or another representation?\n\nf. Please clarify the relationship between offline training and online inference. Is UAPD involved during inference, or only used for pre-segmentation during training?\n\ng. In Figure 2, the input-output relationships among UAPD, CAM, and VLM should be explicitly shown. It is currently unclear how information flows across these modules"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b6npUuWuKN", "forum": "FStqfhYUSI", "replyto": "FStqfhYUSI", "signatures": ["ICLR.cc/2026/Conference/Submission8757/Reviewer_cKc1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8757/Reviewer_cKc1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760602145641, "cdate": 1760602145641, "tmdate": 1762920541880, "mdate": 1762920541880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an “atoms-to-events” framework that decomposes video anomalies into interpretable evidence atoms before event prediction. It improves both accuracy and interpretability over existing vision–language and vision-only baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: The “atoms-to-events” decomposition introduces a novel conceptual perspective for anomaly reasoning, providing a more interpretable and modular structure compared to conventional direct feature-to-event mappings. However, most components of the framework are adapted from existing architectures and methods, making the originality of the work lie primarily in its integration and system design rather than in fundamentally new algorithmic innovations.\n\n- Quality: The framework achieves SOTA on both UCF-Crime and XD-Violence. The ablation studies in both main paper and appendix are thorough, and clearly show how atoms, segmentation, and the CAM–VLM integration each affect the results. Training details are extensive and reproducibility claims are strong.\n\n- Clarity: The “atoms-as-evidence” perspective is well motivated and clearly justified through diagrams and textual expositions.\n\n- Significance: This paper addresses an important challenge in video anomaly detection by proposing an interpretable “atoms to events” reasoning framework. The idea is promising and could influence future work on structured reasoning in vision–language models."}, "weaknesses": {"value": "- The core novelty lies in framing, not architecture or algorithms.\n- The “categorical” foundation is only loosely connected to actual category theory, making the theoretical justification seem superficial.\n- Although qualitative results are shown, there is no quantitative validation of interpretability.\n- Runtime and scalability analyses are absent."}, "questions": {"value": "- Can authors provides with some quantitative validation of interpretability?\n- How does the model handle overlapping or ambiguous events that may involve multiple atom types simultaneously? \n- What is the computational cost of the full pipeline, and can it run efficiently on long or real-time video streams?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nU11RjdnZx", "forum": "FStqfhYUSI", "replyto": "FStqfhYUSI", "signatures": ["ICLR.cc/2026/Conference/Submission8757/Reviewer_XACT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8757/Reviewer_XACT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557202216, "cdate": 1761557202216, "tmdate": 1762920541558, "mdate": 1762920541558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "While this paper addresses a critical challenge in video anomaly detection (VAD)—mitigating VLM overfitting to spurious cues via a \"visual features → atoms → events\" framework—this paper suffers from insufficient methodological validation, limited experimental scope, and unaddressed practical limitations that undermine its scientific rigor and contribution to the field. Specifically,\n1. Inadequate Validation of Core Methodological Components\n2. The core experiments focus exclusively on UCF-Crime and XD-Violence, two well-studied but homogeneous datasets (primarily surveillance footage).\n3. The paper’s qualitative results and ablation studies are insufficient to confirm the framework’s interpretability and core design choices.\n\nIn summary, this paper’s core idea of \"atoms as intermediate evidence\" is intriguing, but it is undermined by incomplete methodological validation, narrow experiments, and unaddressed practical limitations. The gaps in component ablation, SOTA benchmarking, and generalization testing mean the work does not meet ICLR’s standards for scientific rigor and impact. A reject decision is warranted."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses a critical challenge in video anomaly detection (VAD)—mitigating VLM overfitting to spurious cues via a \"visual features → atoms → events\" framework"}, "weaknesses": {"value": "1. The proposed framework’s three pillars (UAPD, CAM, Atom-Guided VLM) lack rigorous validation of their individual and interactive efficacy, leaving critical questions about their necessity and design choices unresolved.\n2. The experimental design is narrow, failing to demonstrate the framework’s robustness, generalizability, or superiority to state-of-the-art (SOTA) methods in diverse scenarios.\n3. The core experiments focus exclusively on UCF-Crime and XD-Violence, two well-studied but homogeneous datasets (primarily surveillance footage). Appendix experiments on NWPU and Ubnormal are underdeveloped\n4, Lack of Meaningful Qualitative Evaluation. Figures 3 and 5 show only 4 qualitative cases (shooting, accident, etc.) with no statistical validation. It does not report quantitative metrics for qualitative performance (e.g., intersection-over-union (IoU) between predicted and ground-truth anomalous intervals across the test set).\n5. The paper’s atoms are tied to pre-defined anomaly types (shooting, explosion, etc.). It provides no experiments on unseen anomaly types (e.g., \"crowd stampede,\" \"fire outbreak\")—a critical gap for generalizable VAD. Can the framework automatically mine new atoms for unseen anomalies, or does it require retraining?"}, "questions": {"value": "See the above comments"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Kcohaoyj6J", "forum": "FStqfhYUSI", "replyto": "FStqfhYUSI", "signatures": ["ICLR.cc/2026/Conference/Submission8757/Reviewer_n466"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8757/Reviewer_n466"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704504027, "cdate": 1761704504027, "tmdate": 1762920541234, "mdate": 1762920541234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces to replace “visual features to event” mapping with “visual features to learnable atoms then to events”, a new framework for video anomaly detection.\n\nThe proposed method learns the atoms in video segments and the roles of atoms by incorporating the VLM for event predictions.\n\nThe evaluations are performed on two benchmark datasets and show improved performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "+ The paper is technical sound.\n\n+ In introduction section, this work is somehow well motivated with clearly listed contributions at the end.\n\n+ The evaluations show improved performance on two datasets."}, "weaknesses": {"value": "- Related work section listed many existing works; however, these works are not being discussed or analysed in this section. The challenges are outlined, but why existing methods are unable to address them, and how the proposed method addresses these are unclear. Also it would be better to outline how the proposed method differs from existing ones, and provide some insights. The current related work section needs significant revisions to reflect on these.\n\n- Although the idea of using atoms for event predictions is interesting, (i) it would be better to have a notation detailing the maths symbols and operations used in the paper, eg, what are scalars, vectors and matrices etc. (ii) The overview of this work is not clearly and properly presented. This section is organised in an unnecessary complex way, and each part is quite standalone and does not very well connected to each other, this makes the assessments on novelty, core innovations harder.\n\n- The authors introduced some new concepts; however, these concepts are not being clearly explained before using them. Also in the appendix, there is no enough information presented, making the understanding of the paper challenging. Some of the maths symbols are not being introduced before using them.\n\n- Datasets used in evaluations tend to be small-scale and old fashioned. The authors should explore some new challenging datasets to validate the effectiveness of the proposed method, such as [A]. It would be more interesting to see how the proposed model reflect on both scenario-level and anomaly-type-level evaluations. \n\n[A] L Zhu, L Wang, A Raj, T Gedeon, and C Chen. Advancing Video Anomaly Detection: A Concise Review and a New Dataset. Advances in Neural Information Processing Systems (NeurIPS). 2024.\n\n- The evaluations tend to be limited. (i) Lack of enough hyperparameters evaluations and exploration on different model variants to show the effectiveness and robustness. (ii) Most experimental results are presented in the form of tables, it is suggested to use more visualisations such as plots, figures, attention visualisations to show the effectiveness of the proposed method compared to existing methods in the literature. \n\n-Fig 3 presented some visualisations, but still, very limited. On the other hand, it is suggested to also plot eg how existing state of the art methods perform on the qualitative results (plots, proposed method vs ground truth vs existing SOTAs). The discussions and analysis also tend to be a bit limited and it is suggested to improve."}, "questions": {"value": "Refer to weaknesses, and also some questions below.\n\n- Pay more attention to punctuations, eg, quotation marks should appear in pairs in the whole paper (eg, Line 16-17).\n\n- Algorithm 1 is also unclear, as the steps are not clearly presented (one step per sentence).\n\n- Limitations and future works could be provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lmdEakLK3f", "forum": "FStqfhYUSI", "replyto": "FStqfhYUSI", "signatures": ["ICLR.cc/2026/Conference/Submission8757/Reviewer_Rw8p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8757/Reviewer_Rw8p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799039691, "cdate": 1761799039691, "tmdate": 1762920540866, "mdate": 1762920540866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}