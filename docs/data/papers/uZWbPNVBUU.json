{"id": "uZWbPNVBUU", "number": 20033, "cdate": 1758301732161, "mdate": 1763576318994, "content": {"title": "Socially Inspired Coalition Formation and Client Selection in Federated Learning", "abstract": "Federated Learning (FL) enables privacy-preserving collaborative model training, but its effectiveness is often limited by client data heterogeneity. \nWe introduce a client-selection algorithm that (i) dynamically forms non-overlapping coalitions of clients based on asymptotic agreement and (ii) selects one representative from each coalition to minimize the variance of model updates. Our approach is inspired by social-network modeling, leveraging homophily-based proximity matrices for spectral clustering and techniques for identifying the most the most informative individuals to estimate a group’s aggregate opinion. We provide theoretical convergence guarantees for the algorithm under mild, standard FL assumptions.\nFinally, we validate our approach by benchmarking it against three strong heterogeneity-aware baselines; the results show higher accuracy and faster convergence, indicating that the framework is both theoretically grounded and effective in practice.", "tldr": "FedCVR-Bolt, a federated learning algorithm that reduces update variance by clustering clients based on asymptotic agreement and selecting one representative per cluster.", "keywords": ["Federated Learning", "Variance Reduction", "Social Systems"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/92b05fdd2e5a3a6b5ec291b438e696aa261e7ded.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The author propose FedCVR-Bolt , a client sampling algorithm, which improves performance in heterogeneous FL settings compared to existing sampling strategies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Innovatively incorporates social dynamics models, treating clients as nodes in a social network and using homophily to construct influence matrices, which is inspiring.\n2. heoretical foundation with mathematical derivations for variance reduction and convergence guarantees, enhancing credibility."}, "weaknesses": {"value": "1. Experimental models are too simplistic; only MLP and a simple CNN are used, lacking validation on more complex architectures like ResNet or RegNet.\n2. The number of coalitions P is fixed a priori, which may limit adaptability in dynamically changing data environments; sensitivity analysis on P is missing.\n3. Insufficient computational cost analysis; no actual runtime or resource consumption metrics are provided.\n4. Limited heterogeneity simulation; only Dirichlet distribution with α=0.1 and fixed client count are used, lacking diversity in settings and ablation studies.\n5. Baseline comparisons are outdated; no comparison with more recent state-of-the-art methods"}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vTfxGpj8l7", "forum": "uZWbPNVBUU", "replyto": "uZWbPNVBUU", "signatures": ["ICLR.cc/2026/Conference/Submission20033/Reviewer_A8ac"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20033/Reviewer_A8ac"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820694274, "cdate": 1761820694274, "tmdate": 1762932931956, "mdate": 1762932931956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Dear AC, PC, and Reviewers,\n\nFirst of all, we would like to thank you for having handled our submissions and for the feedback provided and for the observations regarding the methodological aspects of the paper. We recognize that carrying out the additional experiments suggested would require more time than is currently available. For this reason, we have decided to respectfully withdraw our submission."}}, "id": "0QIhqLIQ3i", "forum": "uZWbPNVBUU", "replyto": "uZWbPNVBUU", "signatures": ["ICLR.cc/2026/Conference/Submission20033/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20033/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763576158824, "cdate": 1763576158824, "tmdate": 1763576158824, "mdate": 1763576158824, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FedCVR-Bolt, a novel client selection algorithm for Federated Learning (FL)  designed to mitigate challenges from statistical heterogeneity. The method draws inspiration from social dynamics and involves two stages: (1) Coalition Formation: It uses a homophily-based similarity matrix and spectral clustering  to dynamically group clients into \"coalitions.\" (2) Client Selection: From each coalition, it probabilistically selects one representative client by maximizing variance reduction combined with Boltzmann exploration."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The introduction of concepts from social dynamics, such as \"coalition formation\" and \"homophily\", to the FL client selection problem provides an innovative and intuitive framework for handling client clustering.\n2.The algorithm is not just a heuristic. The authors provide detailed theoretical derivations for the variance reduction metric (Propositions 1 & 2) and a convergence analysis (Proposition A3), which strengthens the method's reliability."}, "weaknesses": {"value": "The experimental results are insufficient, and the latest methods need to be compared.\n\nThe current classification experiments are limited to an extreme heterogeneity setting (α=0.1) , and the regression experiments are confined to a simple cluster setup (J=2) . To more comprehensively evaluate the algorithm's robustness across different degrees of heterogeneity, the authors can supplementing the experiments with moderate (e.g., α=0.5) or mild (e.g., α=1.0) non-IID settings."}, "questions": {"value": "The method's core relies on estimating, storing, and computing with D separateK ×K covariance matrices (C^d). This introduces a significant O(D×K^2 ) storage overhead on the server. More critically, when the number of clients K is large, and especially when the model dimension D is large (e.g., millions of parameters in modern neural networks), the O(D×K^2 )  computational cost required to calculate all  v_k values in each round becomes prohibitive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6gzNt7LxPt", "forum": "uZWbPNVBUU", "replyto": "uZWbPNVBUU", "signatures": ["ICLR.cc/2026/Conference/Submission20033/Reviewer_ccPy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20033/Reviewer_ccPy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958420823, "cdate": 1761958420823, "tmdate": 1762932931542, "mdate": 1762932931542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studied the problem of federated learning under data heterogeneity. It introduced a client-selection algorithm, which dynamically formed non-overlapping coalitions of clients and selected one representative from each coalition to minimize the variance of model updates. The efficacy of the proposed client-selection algorithm was verified theoretically and empirically."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality:** It introduced a novel client sampling algorithm FedCVR-Bolt for heterogeneous federated learning. The Boltzmann exploration policy was adopted to improve the client sampling strategy. Finally, the convergence of the FedCVR-Bolt algorithm was analyzed.\n\n**Quality:** The sampling strategy based on the variance reduction was theoretically justified. Experimental results on the synthetic regression and real-world image datasets demonstrated the effectiveness of the proposed FedCVR-Bolt algorithm.\n\n\n**Clarity:** The proposed sampling and updating policies were well-motivated. The proposed FedCVR-Bolt was easy to follow.\n\n\n**Significance:** It provided a novel client sampling method to advance federated learning under data heterogeneity."}, "weaknesses": {"value": "(1) The impact of the Boltzmann exploration policy on the variance reduction is unclear. Subsection 3.1 justified the rationale of the sampling strategy in Eq. (5) based on the variance reduction. The Boltzmann exploration policy transforms the deterministic selection based on the estimated variance into a Boltzmann-like probability measure. However, it is unclear whether this probability measure in Eq. (8) theoretically supports the variance reduction like Corollary 1. Besides, lines 259-260 state that it can \"mitigate the risk of becoming trapped by early biased or noisy estimates of the covariance matrix\". This can be further justified. Appendix C.2 mentions that a uniformly random sampling strategy during the initial 30 rounds is used. Then why is the Boltzmann exploration policy required to mitigate the risk of becoming trapped by early biased estimates of the covariance matrix?\n\n(2) The coalition formation and updating policy can be further clarified.\n-  Subsection 2.1 presents the partial-participation strategy with the randomly selected subset of clients, which is the commonly used policy in FL with a large number of clients. However, this is fundamentally different from the proposed updating policy in Section 3. It carefully selects one representative per cluster. Instead, the coalition formation requires accessing the model parameters from all clients during each training round. This is more similar to cross-silo FL.\n- Lines 249-250 show that selecting one representative per cluster mitigates overfitting to any single sub-distribution. It is unclear why this selection strategy can mitigate overfitting to any single sub-distribution.\n- Step 18 of Algorithm 1 shows the estimation of the global aggregated parameters. But it is unclear how it will be used.\n\n(3) The experiments can be significantly improved to justify the advantages of the proposed algorithm.\n- The convergence of FedCVR-Bolt can be verified in the experiments. Compared to baselines, will FedCVR-Bolt have an improved convergence rate?\n- It would be convincing to visualize what clients are selected during training. Appendix C.2 mentions that a uniformly random sampling strategy during the initial 30 rounds is used. This policy can also be validated.\n- The running efficiency of FedCVR-Bolt can also be validated. Section 4 shows the computational cost of $O(K^2D)$. This can be very computationally expensive when the number of clients $K$ is very large."}, "questions": {"value": "(1) Why are some results of baseline FL approaches very weak compared to FedAvg, e.g., FedProx on CIFAR-10 and FeSEM on CIFAR-10/CIFAR-100?\n\n(2) Compared to baselines, the running efficiency of FedCVR-Bolt can be further validated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UtJQ7ZYfRe", "forum": "uZWbPNVBUU", "replyto": "uZWbPNVBUU", "signatures": ["ICLR.cc/2026/Conference/Submission20033/Reviewer_qJjp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20033/Reviewer_qJjp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959529106, "cdate": 1761959529106, "tmdate": 1762932931092, "mdate": 1762932931092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper developed a client selection method for federated learning. It is derived from a variance reduction technique (Raineri et al., 2025), and it employs an Boltzmann exploration (Eq.(8)) to balance exploration and exploitation. Experiments were conducted on synthetic regression and standard classification datasets (MNIST & CIFAR) to show its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing and presentation are very clear and easy to follow.\n\n2. The algorithm is theoretically motivated."}, "weaknesses": {"value": "1. There are noticeable drawbacks in the theory.\n\n- The paper neglects the changing nature of model parameter distributions over time. Specifically, it is an over-simplification to say that $\\theta_k(t)$ is a realization of the random variable mentioned in L159. What is the underlying distribution? Clearly, the parameters are closer to a local minimum at later stages of learning, so the underlying distribution is changing over time. This has noticeable consequences later. For example, using (11) assumes that the model parameter distributions are stationary.\n- The selection is based on a greedy strategy, not accounting for the correlations among parameters. L364 focuses on the sum of reduced variances, which is less helpful when the parameters are correlated.\n\n2. Experiments are limited\n\n- Baselines are limited. The algorithm focuses on client selection, but only two of the baselines are selection methods. It would make sense to compare with other selection methods, such as Balakrishnan et al., (2022), Tian et al., (2022) and Chen et al. (2024).\n- The regression problem is too simple with only two clusters and at most two-dimensional parameters. What if we have $J=10$ clusters, but the algorithm sets $P=5$ or $P=20$?\n- There are no ablation studies on the choices of the hyper-parameters for the classification experiments, such as $P$, $\\gamma$ in (4) and $\\beta$ in (8).\n\nRef\n\n- Tian, C., Shi, Z., Li, L. and Xu, C.Z., 2024, July. Ranking-based client imitation selection for efficient federated learning. In *Forty-first International Conference on Machine Learning*.\n- Chen, H. and Vikalo, H., 2024. Heterogeneity-guided client sampling: Towards fast and efficient non-iid federated learning. *Advances in Neural Information Processing Systems*, *37*, pp.65525-65561.\n\nMinor comments\n\n- For some reason, the Related Work section has no section number.\n- L341: Raineri et al. (2025) has no Lemma 2."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6UDeIm4L1l", "forum": "uZWbPNVBUU", "replyto": "uZWbPNVBUU", "signatures": ["ICLR.cc/2026/Conference/Submission20033/Reviewer_FuJ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20033/Reviewer_FuJ6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974373028, "cdate": 1761974373028, "tmdate": 1762932930405, "mdate": 1762932930405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}