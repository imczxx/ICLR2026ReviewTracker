{"id": "SHbUxEKMBD", "number": 13641, "cdate": 1758220298410, "mdate": 1759897422910, "content": {"title": "NEUTAG: Graph Transformer for Attributed Graphs", "abstract": "Graph Transformers (\\textsc{GT}) have demonstrated their superiority in graph classification tasks, but their performance in node classification settings remains below par. They are designed for either homophilic or heterophilic graphs and show poor scalability to million-sized graphs. In this paper, we address these limitations for node classification tasks by designing a model that utilizes a special feature encoding that transforms the input graph separating nodes and features, which enables the flow of information not only from the local neighborhood of a node but also from distant nodes, via their connections through shared feature nodes. We theoretically demonstrate that this design allows each node to exchange information with all nodes in the graph, effectively mimicking all-node-pair message passing while avoiding $\\mathcal{O}(N^2)$ computation. We further analyze the universal approximation ability of the proposed transformer. Finally, we demonstrate the effectiveness of the proposed method on diverse sets of large-scale graphs, including the homophilic \\& the heterophilic varieties.", "tldr": "We propose NUTAG, a scalable sparse Graph Transformer using feature-based virtual nodes, enabling efficient attention with theoretical guarantees and effective learning on homophilic, heterophilic, and large-scale graphs.", "keywords": ["Graph Transformer", "Sparse Attention", "Graph Representation Learning", "Efficient Attention Mechanisms", "Efficient Transformers"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6a99861b07808632c473001058aa8686a0304fa1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper develops a new graph Transformer called NEUTAG which is based on the constructed node-feature graph. Moreover, the authors also define positive and negative neighborhood according to the relevance between nodes and features. Then, NEUTAG also leverages the Transformer backbone to learn node representations. The experimental results showcase the effectiveness of NEUTAG for node classification."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This paper is well-organized and easy to follow.\n\n2.The authors provide the theoretical analysis of the proposed method.\n\n3.The authors select various baselines for performance comparison."}, "weaknesses": {"value": "1.The research gap is overclaimed.\n\n2.Mainstream datasets are missing.\n\n3.Some important experiments are missing."}, "questions": {"value": "1.The authors claim three limitations in existing graph Transformers which lack objectivity. The first limitation only appears in the hybrid-based graph Transformer which needs to combine Transformer with GNN-like modules. The rest two limitations have been widely studied in recent GTs. So, I do not think the authors clearly present the research gaps between NEUTAG and previous methods.\n\n2.The selection of the dataset is inappropriate. For instance, the Chameleon dataset has been shown to contain a significant number of duplicate nodes. Furthermore, the authors did not employ established mainstream datasets for evaluation, large-scale graphs in NAGphormer or different types of graphs Polynormer.\n\n3.The authors emphasize the scalability of GTs but they do not conduct efficiency study in this paper. I think it is required to compare the training cost of NEUTAG and other representative baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gUPz8jLpK9", "forum": "SHbUxEKMBD", "replyto": "SHbUxEKMBD", "signatures": ["ICLR.cc/2026/Conference/Submission13641/Reviewer_bqMz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13641/Reviewer_bqMz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761209464659, "cdate": 1761209464659, "tmdate": 1762924219780, "mdate": 1762924219780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a graph transformer that leverages visual nodes for message propagation. The authors provide theoretical analysis to demonstrate the efficiency of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Some theoretical analisys.\n\nThe method is conceptually straightforward."}, "weaknesses": {"value": "The authors claim that existing GTs cannot perform well on both homophilous and heterophilous graphs. However, many works have demonstrated strong performance on both types of graphs. For sxample, [1][2]...\n\nFor the non-scalable, this issue has been extensively addressed by many recent techniques, such as linear transformers[3] and their graph-specific variants, which significantly reduce the computational and memory complexity while maintaining competitive performance.\n\n\nThe paper is not well organized, which makes it difficult to follow. For example, Figure 1 is not referenced in the main text, and the concept of “Feature node” is not clearly defined. This raises a question: how should the connections between feature nodes and graph nodes be defined when the node features are not based on a bag-of-words representation?\n\nThe experimental results show that the proposed method does not achieve significant improvement. The datasets with the largest reported gains, Arxiv-year and Snap-patents, in fact, do not yield satisfactory results. For comparison, some directed graph methods such as Dir-GNN [4] achieve much higher performance 64.08 and 73.95 on these two datasets, which is far surpassing the proposed method’s 53.96 and 63.0. Moreover, Dir-GNN is simpler and more lightweight.\n\n[1] GOAT: A Global Transformer on Large‑scale Graphs\n[2] Rethinking Graph Transformer Architecture Design for Node Classification\n[3] Sgformer: Simplifying and empowering transformers for large-graph representations\n[4] Edge directionality improves learning on heterophilic graphs"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kh69jaVeSc", "forum": "SHbUxEKMBD", "replyto": "SHbUxEKMBD", "signatures": ["ICLR.cc/2026/Conference/Submission13641/Reviewer_yYo4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13641/Reviewer_yYo4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536013893, "cdate": 1761536013893, "tmdate": 1762924219200, "mdate": 1762924219200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes NEUTAG, a scalable transformer that operates on a rewired graph composed of original nodes and additional \"feature\" nodes. NEUTAG uses both sparse and full attention, capturing homophily and heterophily. Theoretical analyses show that NEUTAG increases graph connectivity and can approximate dense attention. NEUTAG is benchmarked against popular GTs and MPNNs on homophilous and heterophilous benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The research question on designing more powerful and scalable graph transformers is interesting\n2. The approximation results are interesting given the more efficient attention can approximate the dense version\n3. The transformer baselines in the experiments are extensive."}, "weaknesses": {"value": "1. For the limitations of existing graph transformers I'm not sure I appreciate or understand these given my following thoughts:  \n* redundant dependency on GNNs. the paper claims that since transformers are universal, GNNs and full attention are redundant together. while transformers are universal, I think it could still be useful to have a component that captures local connectivity as this would require lots of data for the transformer to learn. in other words, the local message-passing by GNNs is a good inductive bias that otherwise a transformer would need to learn from a lot of data. moreover, I think NEUTAG is using a form of message-passing in its local attention along only neighbors, so it doesn't completely do-away with the GNN component. \n* given my argument above that local attention is a form of message-passing, NEUTAG could also inherit the homophily biases similar to other transformers that use a GNN component. \n* given that scalable GTs exist, what are the limitations with GTs like GOAT or polynormer?\n\n2. To me, the core contribution seems to be the proposal of the \"metamorphosis\" graph, which augments the original graph with new \"feature\" nodes that connect to original nodes. Rather than a new transformer, this is closer to graph rewiring approaches, which are not discussed as related works or compared against empirically. The other novel component in addition to the metamorphosis graph is the feature2feature full attention which is where the main computational savings come from in comparison to other full attention GTs. From the Appendix ablations though, it seems removing this component only slightly drops performance, whereas removal of local neighbors causes the biggest drop. This seems to indicate that the local neighborhood attention, which is similar to message-passing, is the most important to the architecture. So to sum up, the main gains are coming from the metamorphosis graph, similar to rewiring approaches, and local attention, a form of message-passing, while the feature2feature attention improves performance not as significantly. I would thus like to see comparisons to other rewiring techniques perhaps also with your version of local attention.\n\n3. NEUTAG seems only to apply to graphs with nodes of only binary features (given the present/not present figure). I appreciate that this limitation is mentioned in the Appendix, and I agree that this is a weakness for NEUTAG. This seems limited since in general a graph can have categorical or continuous features, and I'm not sure what the prevalence of graphs with only binary features is. In its current form there doesn't seem to be a discussion on the generalization of NEUTAG to graphs with features that are not just binary vectors, so the approach seems quite limited. \n\n4. Empirically, Polynormer comes quite close across all benchmarks except for snap, while NEUTAG wins 1/3 on larger benchmarks in comparison to other GTs (table 1)."}, "questions": {"value": "My questions largely follow my weaknesses: \n\n1. how is the local attention different than message-passing? and if not, why does NEUTAG circumvent the homophily-biases?\n2. what is the relation of NEUTAG's metamorphosis step to existing graph rewiring techniques? it seems similar to an approach that adds edges between nodes of similar features. would you expect local attention + graph rewiring to perform similarly?\n3. how would you generalize NEUTAG to non-binary features?\n4. The connectivity argument makes sense, but if you are adding connections, does this increase the rate of oversmoothing? or does it affect oversquashing since now the degree of each node increases significantly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w4Gnsprn3V", "forum": "SHbUxEKMBD", "replyto": "SHbUxEKMBD", "signatures": ["ICLR.cc/2026/Conference/Submission13641/Reviewer_bm5V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13641/Reviewer_bm5V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712124362, "cdate": 1761712124362, "tmdate": 1762924218502, "mdate": 1762924218502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a novel GT architecture that employs a special feature encoding to separate nodes and features, enabling information flow not only from local neighborhoods but also between distant nodes via shared feature connections. Experiments are conducted on a variety of graph datasets to validate the approach."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The method is straightforward, and the authors provide supporting theoretical analysis."}, "weaknesses": {"value": "1. The novelty of introducing new virtual nodes to connect nodes in the graph appears to be limited.\n\n2. The experimental results do not clearly demonstrate the effectiveness of the proposed Graph Transformer method compared to existing GT approaches.\n\n3. Some recent Graph Transformer methods [1–5] that also utilize virtual nodes are not discussed.\n\n4. The paper requires careful proofreading and polishing.\n \n[1] Wenhao Zhu, et al. Hierarchical transformer for scalable graph learning, 2023. \n\n[2] Wenhao Zhu, et al. Anchorgt: Efficient and flexible attention architecture for scalable graph transformers, 2024. \n\n[3] Weirui Kuang, et al. Coarformer: Transformer for large graph via graph coarsening, 2022. \n\n[4] Chuang Liu, et al. \"Gapformer: Graph Transformer with Graph Pooling for Node Classification.\" IJCAI. 2023.\n\n[5] Xueqi Ma, et al. \"HOGT: High-Order Graph Transformers.\""}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KX1WOKnX8y", "forum": "SHbUxEKMBD", "replyto": "SHbUxEKMBD", "signatures": ["ICLR.cc/2026/Conference/Submission13641/Reviewer_eUPb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13641/Reviewer_eUPb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971896707, "cdate": 1761971896707, "tmdate": 1762924218096, "mdate": 1762924218096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}