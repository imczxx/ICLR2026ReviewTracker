{"id": "sruA4AZmZI", "number": 1876, "cdate": 1756956225381, "mdate": 1759898181256, "content": {"title": "Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward", "abstract": "Recent advances in large language models (LLMs) have utilized reinforcement learning with verifiable rewards (RLVR) to improve reasoning capabilities.\nHowever, scaling these methods typically requires massive data and extensive rollout computations, leading to high training costs and low data efficiency.\nTo mitigate this issue, we propose DEPO, a Data-Efficient Policy Optimization approach that combines optimized strategies for both offline and online data selection.\nIn the offline phase, we curate a high-quality subset of training data based on multiple objectives, including diversity, influence, and difficulty.\nDuring online RLVR training, we propose a sample-level explorability metric to dynamically filter out samples with low exploration potential, thereby reducing substantial rollout computational costs.\nAdditionally, we employ a replay mechanism for under-explored samples to ensure sufficient training, which enhances the final convergence performance.\nExperiments on five reasoning benchmarks show that DEPO consistently outperforms existing methods in both offline and online data selection scenarios.\nNotably, using only 20% of the training data, our approach achieves a 1.85 $\\times$ speed-up on AIME24 and a 1.66 $\\times$ speed-up on AIME25 compared to GRPO trained on the full dataset.", "tldr": "We propose a Data-Efficient Policy Optimization pipeline for Reinforcement Learning with Verifiable Reward that integrates optimized strategies for both offline and online data selection, namely DEPO.", "keywords": ["Data Efficiency", "Reinforcement Learning with Verifiable Reward"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/23cb4ce97b05b56dc0978911931af2e936d1d3e4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DEPO, a two-stage data selection pipeline, to address the high training costs and low data efficiency often encountered in Reinforcement Learning with Value Response (RLVR). The process begins with offline data curation, where the data is meticulously chosen based on three distinct criteria: diversity, influence, and difficulty. Following this, the second stage, online rollout pruning, selects appropriate rollout samples by utilizing a metric focused on explorability. The empirical results demonstrate that DEPO achieves sample efficiency and faster learning performance compared to conventional methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper proposes a comprehensive learning paradigm in RLVR that systematically integrates both an offline data curation phase and an online data collection phase.\n* The metrics used for data selection during both the offline and online processes are highly refined and logically compelling.\n* The efficiency and efficacy of the proposed method are validated through extensive experimental results conducted across a diverse range of datasets."}, "weaknesses": {"value": "* Regarding the significant reduction in overall training time, it would be beneficial for the authors to clarify whether the initial cost of the offline data curation is included in this calculation. Since the graph construction and DPP application likely involve a notable overhead, quantifying this pre-computation cost would provide a more complete picture of the method's overall efficiency.\n* The offline curation relies on difficulty-aware sampling from a fixed distribution determined before training. While this is shown to be effective, this dataset could become less challenging for the policy as it improves. The paper's claims on sample efficiency would be even more compelling with a brief discussion or analysis of why this static approach remains effective throughout the entire training process."}, "questions": {"value": "* It would be very helpful if the authors could provide a quantitative measure of the specific pre-computation cost required for the offline curation stage.\n* Given that the offline subset $D^{sub}$ relies on a static difficulty measurement, it would be interesting to know if the authors considered or experimented with a dynamic offline curation strategy? For instance, periodically re-sampling $D^{sub}$ to reflect the evolving capabilities of the policy might be an interesting alternative to explore.\n* Could the authors provide a bit more detail on how the specific filtering threshold $\\lambda=1.5$ was determined? Since this value multiplies a dynamic term that changes during training, a justification for why fixing the value of $\\lambda$ is an appropriate and robust choice would be appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ci8WucoCMW", "forum": "sruA4AZmZI", "replyto": "sruA4AZmZI", "signatures": ["ICLR.cc/2026/Conference/Submission1876/Reviewer_1LYZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1876/Reviewer_1LYZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551981568, "cdate": 1761551981568, "tmdate": 1762915924226, "mdate": 1762915924226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the high data and computational costs associated with Reinforcement Learning with Verifiable Rewards (RLVR) for enhancing the reasoning capabilities of Large Language Models (LLMs). The authors propose DEPO (Data-Efficient Policy Optimization), a pipeline that combines offline data curation and online rollout pruning. In the offline stage, a PageRank-weighted Determinantal Point Process is used to select a diverse and influential data subset, which is then refined to favor samples of moderate difficulty aligned with the model's current capabilities. In the online stage, an \"explorability\" metric is used to dynamically skip rollouts for low-potential samples, thereby improving computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1, The proposed method, DEPO, achieves performance nearly equivalent to GRPO training on the full dataset while using only 20% of the data and significantly reducing training time. Specifically, on the DeepSeek-R1-Distill-Qwen-7B model, the full DEPO pipeline achieved a comparable average accuracy (61.1% vs. 61.7%) using only 57% of the training time and 40% of the rollouts compared to training on the full dataset (Table 1). This is a substantial practical contribution that could lower the barrier to entry for RLVR training.\n\n2, The paper presents one of the first approaches to systematically integrate both offline data selection and online rollout pruning. This dual optimization strategy, which considers diversity, influence, and difficulty offline (Section 3.2) while performing dynamic sample filtering online (Section 3.3), is a distinguishing feature compared to prior work that has typically focused on only one of these aspects.\n\n3, The effectiveness of the proposed method is validated across three different LLMs and five reasoning benchmarks (Table 1). The paper provides extensive empirical evidence to support its claims, including sensitivity analyses on varying dataset sizes (Figure 4), sampling ratios (Figure 5a), and rollout ratios (Figure 5b), as well as a clear ablation study demonstrating the contribution of each component (Table 2)."}, "weaknesses": {"value": "1, The core components of the proposed method are largely a combination of well-established ideas. The use of Determinantal Point Processes (DPP) and PageRank for offline selection (Section 3.2.1), and difficulty-based sampling aligned with model capability (i.e., curriculum learning) (Section 3.2.2) are not new concepts. The entropy-based \"explorability\" metric for online pruning (Equation 4) appears to be an intuitive extension of existing RL techniques that leverage entropy to encourage exploration. While the authors claim the \"integration\" of offline and online strategies as a key contribution, the lack of originality in the individual components limits the overall novelty of the work.\n\n2, The most significant weakness is the lack of statistical validation for the experimental results. Most results, including the main ones in Table 1, present figures from a single run without reporting means and standard deviations (or confidence intervals) over multiple seeds. For instance, the performance difference between DEPO-Offline and the Full dataset on the Deepseek-R1-Distill-Qwen-7B model is a mere 0.3 percentage points (61.4 vs. 61.7). It is impossible to determine if this difference is statistically significant or simply due to random variance. This severely undermines the credibility of the paper's central claim of \"maintaining comparable performance while improving efficiency.\" \n\n3,The \"explorability\" metric (Equation 4) is intuitive but designed in a somewhat heuristic manner. The paper lacks a theoretical justification or deeper analysis for the specific combination of normalized reward (Ât), entropy, and a hard threshold (λ)."}, "questions": {"value": "1, To assess the reliability of the core results, could you please repeat the key experiments in Table 1 (e.g., for the DeepSeek-R1-Distill-Qwen-7B model) using at least three different random seeds and report the mean and standard deviation? This is crucial to verify whether the performance difference between your method (DEPO) and full-dataset training is statistically insignificant.\n\n2, The offline \"Difficulty-aware Normal Distribution Sampling\" strategy (Section 3.2.2) statically prioritizes moderately difficult samples. This overlooks a comparison with a simpler, dynamic curriculum learning strategy (e.g., moving from easy to hard samples as training progresses). Could you provide a comparative experiment against such a classical curriculum strategy to demonstrate the superiority of your proposed sampling method?\n\n3, The \"explorability\" metric (Equation 4), central to your online pruning, combines normalized reward (Ât) and entropy (E(o_i^t)). To isolate the contribution of these two factors, could you add to your ablation study (Table 2) a variant that performs pruning based solely on entropy, removing the reward term (Ât)? This would help determine whether the combination with the reward signal is essential.\n\n4, The offline data curation is a two-step process: DPP pruning to 50%, followed by difficulty sampling to 20%. What is the advantage of this two-step approach over a single-step method that could, for instance, incorporate a difficulty score directly into the DPP kernel? Could you provide an experimental comparison or a stronger justification for this design choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "svY67lrh5j", "forum": "sruA4AZmZI", "replyto": "sruA4AZmZI", "signatures": ["ICLR.cc/2026/Conference/Submission1876/Reviewer_Gqob"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1876/Reviewer_Gqob"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908978646, "cdate": 1761908978646, "tmdate": 1762915924087, "mdate": 1762915924087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses improving sample efficiency in Reinforcement Learning with Verifiable Reward (RLVR). It introduces DEPO, a method that integrates offline and online strategies to reduce the data required during finetuning. DEPO is evaluated on three pre-trained models across three mathematics benchmarks, compared with baselines --- both offline and online variants of RLVR, and demonstrates strong performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental results are comprehensive. Many ablation studies of different components, different hyperparameters are done to clearly show the contributions and robustness of the proposed components."}, "weaknesses": {"value": "1. The writing could be improved, particularly in the methodology section. Additional clarification questions are provided below.\n2. The work introduces many components—three in the offline phase and one in the online phase—but the related work for each component is not sufficiently discussed. For example, while the introduction touches on high-level prior work for both phases, detailed, technical comparisons are missing. This makes it somewhat difficult for reviewers to assess the novelty of the proposed algorithm."}, "questions": {"value": "1. Related works do not sufficiently cover the literature. For instance, for RLVR, the reviewer believes that after DeepSeek-R1, there are many representative works that have used it in different places. But there are only about 5 papers are cited.\n2. What is $\\mathcal{B}$ in equation (5)?\n3. Could you give a more detailed explanation for the exploration metric in (4)? The equation is too complicated without an intuitive explanation to validate each term.\n4. In equation (6), what does top-α_e% means, which term should be the top of which set? The same for \"bottom-ρ%\"\n5. In the final objective function (5), the reviewer didn't see the role of the final offline dataset D^{sub} if the reviewer didn't miss something. So what are the roles of the offline and online phase?\n6. Could the author explain more about this claim/intuition around line 177: \"To promote diversity, we use Determinantal Point Process (DPP) (Kulesza & Taskar, 2012a) to identify a subset that maximizes the determinant of the corresponding similarity submatrix: maxY ⊆P det(Y ), where Y is the submatrix of the full similarity matrix P, and det(·) represents its determinant value. This refers to selecting samples that form a larger volume in the feature space, with a larger volume indicating greater diversity.\" Why a larger determinant of the similarity matrix P may lead to diversity? Since intuitively, if some entries in P_ij is small, that means the corresponding sample i and j are different, giving more diversity. So should we just find those regions with small elements in P? Why the determinant is related to the diversity of samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eT0IfUdNwl", "forum": "sruA4AZmZI", "replyto": "sruA4AZmZI", "signatures": ["ICLR.cc/2026/Conference/Submission1876/Reviewer_L77o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1876/Reviewer_L77o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969686909, "cdate": 1761969686909, "tmdate": 1762915923961, "mdate": 1762915923961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}