{"id": "RFQhCkUcko", "number": 3334, "cdate": 1757404324194, "mdate": 1759898095171, "content": {"title": "Robust Adversarial Policy Optimization Under Dynamics Uncertainty", "abstract": "Reinforcement learning (RL) policies often fail under dynamics that differ from training, a gap not fully addressed by domain randomization or existing adversarial RL methods. Distributionally robust RL provides a formal remedy but still relies on surrogate adversaries to approximate intractable primal problems, leaving blind spots that potentially cause instability and over-conservatism.\nWe propose a dual formulation that directly exposes the robustness–performance trade-off. At the trajectory level, a temperature parameter from the dual is approximated with an adversarial network, yielding efficient and stable worst-case rollouts within a divergence bound. At the model level, we employ Boltzmann reweighting over dynamics ensembles, focusing on more adverse environments to the current policy rather than uniform sampling. Two components act independently and complement each other: trajectory-level steering ensures robust rollouts, while model-level sampling provides policy-sensitive coverage of adverse dynamics.\nThe resulting framework, robust adversarial policy optimization (RAPO) outperforms robust RL baselines, improving resilience to uncertainty and generalization to out-of-distribution dynamics while maintaining dual tractability.", "tldr": "We propose RAPO, a dual-based robust RL framework that combines trajectory-level adversarial rollouts and model-level policy-sensitive sampling to close the theory–practice gap and improve generalization under dynamics uncertainty.", "keywords": ["Reinforcement Learning", "Robust Reinforcement Learning", "Adversarial Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ba3cef7f733670a373112252e698aee61a3b2d6.pdf", "supplementary_material": "/attachment/05a76f6747a8e9325f6ced6d8c9bfa7883bd35b0.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes RAPO (Robust Adversarial Policy Optimization), a dual-based framework for robust reinforcement learning under dynamics uncertainty. Starting from a KL-constrained robust MDP, the authors derive a dual formulation introducing a temperature variable that balances performance and robustness. RAPO uses two complementary mechanisms: an adversarial network (AdvNet) to amortize the dual variable estimation at the trajectory level, and Boltzmann reweighting across an ensemble of dynamics models for model-level robustness. Experiments on Walker2d and a quadrotor payload task show that RAPO improves out-of-distribution robustness compared to prior works while maintaining in-distribution performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a well-motivated unification of distributional robustness (dual formulation) and adversarial training through trajectory-level and model-level mechanisms, both controlled by KL budgets.\n- The dual-level design (AdvNet + Boltzmann reweighting) is intuitive and provides a fine-grained control between local (trajectory) and global (model) robustness.\n- The paper is clearly structured, with theory, algorithms, and experiments aligned. The authors articulate trade-offs (e.g., robustness vs. performance degradation) clearly and transparently."}, "weaknesses": {"value": "- The experiments primarily assess robustness to environmental parameter shifts (e.g., mass, friction, inertia) rather than active adversarial perturbations. Since RAPO explicitly claims adversarial robustness, it would be valuable to include tests under learned or adaptive adversarial agents, as used in RARL [1], QARL [2], ROSE [3], to evaluate resilience against deliberate attacks.\n- The latest robust RL baseline compared is RARL (2017), while several more recent algorithms [2, 3, 4] provide stronger and more diverse perspectives on robustness. Without these comparisons, it is difficult to judge whether RAPO represents a genuine advance over current state-of-the-art.\n- Figures 1 and 2 could benefit from larger fonts for readability. \n- Conceptually, RAPO can be seen as combining two existing ideas: distributional robustness via dual tilting and weighted domain randomization within a PPO loop. While this integration is elegant, it may be viewed as incremental unless stronger empirical gains or theoretical guarantees are provided over existing robust RL frameworks.\n- The dual formulation is motivated by avoiding the limitations of sample-based adversarial training, where finite sampling may fail to cover the entire ambiguity set, leaving blind spots or inducing over-conservatism. However, recent methods such as ROSE [3] tackle the same issue using Stein variational policy gradients to approximate the worst-case distribution. A comparison or discussion of how RAPO’s dual approach differs from or improves upon these variational methods would strengthen the related work and clarify the novelty claim.\n\n\n\n#### [1] Lerrel Pinto et al., Robust Adversarial Reinforcement Learning, ICML 2017\n#### [2] Aryaman Reddi et al., Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula, ICLR 2024\n#### [3] Juncheng Dong et al., Variational Adversarial Training Towards Policies with Improved Robustness, AISTATS 2024\n#### [4] Takumi Tanabe et al., Max-Min Off-Policy Actor-Critic Method Focusing on Worst-Case Robustness to Model Misspecification, NeurIPS 2022"}, "questions": {"value": "- The dual decomposition and Boltzmann reweighting are both well-established concepts. Could the authors highlight what specific theoretical or algorithmic innovation is unique to RAPO beyond integrating these elements within PPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0JjLrJYWKv", "forum": "RFQhCkUcko", "replyto": "RFQhCkUcko", "signatures": ["ICLR.cc/2026/Conference/Submission3334/Reviewer_7nsZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3334/Reviewer_7nsZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760841731756, "cdate": 1760841731756, "tmdate": 1762916677655, "mdate": 1762916677655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduced robust adversarial policy optimization (RAPO) which solve the dual of the robust RL problem that collapse an infinite-dimensional search space down to a scalar within the KL budget, which can help provide sufficient coverage over challenging scenarios and corner cases (i.e., out-of-distribution dynamics), followed by using boltzmann sampling to steer more visitation toward low-return/challenging regions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The approach is designed following very clear line of thoughts, e.g., why solving the dual problem and the need for AdvNet and Boltzman sampling.\n* Sufficient theoritical analyses were provided to support the design choices and insights toward various properties of the approach, e.g., convergence of the ensemble estimation, the existence of stationary/saddle points and robust value drop bounds.\n* Experiments clearly ablates the AdvNet and Boltzman sampling components respectively, which empirically justifies the design of the approach."}, "weaknesses": {"value": "* From the reviewer's point of view, this work is also closely related to the distributionally robust RL work [1-3 below for example] in general, which shared similar high-level objectives of addressing out-of-distribution scenarios. It could be worthy to discuss the connections and distinctions to that line of work.\n  * It would be interesting to compare against some of those works in the experiment as well, if applicable.\n* The reviewer appreciated that the authors compared to robust RL baselines including RARL and EPOpt. It would be interesting to see how the approaches that directly solve the primal would contrast. For example, RNAC and Gleave et al. as cited in the paper.\n* The robust RL environments the authors used in the experiments are typical, while the reviewer is also curious how this work could also facilitate exploration efficiency in larger state-action spaces (with little-to-mild robustness needed in terms of friction/inertia changes). For example, how RAPO would perfom in higher degree-of-freedom humanoid or hand manipulation (Adroit) environments, or more precise tasks like object picking with robot manipulators.\n\n[1] Ramesh, Shyam Sundhar, et al. \"Distributionally robust model-based reinforcement learning with large state spaces.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2024.\n\n[2] Shi, Laixi, et al. \"The curious price of distributional robustness in reinforcement learning with a generative model.\" Advances in Neural Information Processing Systems 36 (2023): 79903-79917.\n\n[3] Liu, Zijian, et al. \"Distributionally Robust -Learning.\" International Conference on Machine Learning. PMLR, 2022."}, "questions": {"value": "One clarification question -- in AdvNet, $m$ samples needed to be obtained for the given $(s,a)$ (page 4 line 201, and Alg. 2 line 1). The reviewer was wondering how this could be done practically in a more realistic setup, e.g., with an actual robotic manipulator does it imply that the approach would need the manipulator to be reset to the same state again and again to obtain $m$ samples of the next states?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zUSTfY74I9", "forum": "RFQhCkUcko", "replyto": "RFQhCkUcko", "signatures": ["ICLR.cc/2026/Conference/Submission3334/Reviewer_Gvbk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3334/Reviewer_Gvbk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883584372, "cdate": 1761883584372, "tmdate": 1762916677317, "mdate": 1762916677317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the robustness of reinforcement learning (RL) policies to shifts in dynamics distributions post-training. The proposed method employs a dual formulation of value maximization under distributional uncertainty that reduces distributional sampling to a parametric optimization and a Boltzmann reweighting scheme to prioritize difficult training examples. Experiments support the method's generalization to out-of-distribution dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The dual formulation approach is interesting and gives insight into how efficient distributional policies can be trained.\n- The paper is organized well."}, "weaknesses": {"value": "### Motivation\n- The motivation of the dual formulation is unclear. The stated reason, that a finite set of training distributions leaves blind spots or is otherwise not comprehensive, is somewhat contradicted by the later inclusion of the ensemble method. If exponential tilting is sufficient to ensure robustness, why is explicit sampling used both in the AdvNet and reweighting components?\n- The use of the word \"adversarial\" is a little confusing, since Section 4.2 states that aleatoric uncertainty is out of scope. Generally, in adversarial RL,  perturbations are assumed to be chosen at test time such that the resulting mistakes minimize the target's reward; this changes the problem from epistemic to aleatoric in that the resulting perturbation is definitively not a natural occurrence. \n\n### Distinction from prior work\n- The stated weakness of existing Monte Carlo methods is that they are unable to fully capture the uncertainty set via sampling. The intuition behind random sampling is that, given an infinite number of samples, the full distribution will be captured, so the problem is not in theory but in practice. With that in mind, the practical aspects of the method (i.e. sample complexity, performance in low-occurrence distributions) should be examined empirically.\n- The paper states that $\\eta^*$ functions as a robustness-performance knob that is absent in primal formulations, which seems untrue. Many methods in adversarial RL use some variation on the general form $(1-\\lambda)V(\\cdot) + \\lambda V^{adv}(\\cdot)$, where $\\lambda$ is a robustness temperature. See [1] as an example.\n\n### Experiments\n- The paper shows comparisons to two outdated robust baselines, dating from 2016 and 2017. There are many peer-reviewed methods published more recently that would serve as a stronger comparison.\n\n[1] Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane S. Boning, Cho-Jui Hsieh: Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations. NeurIPS 2020"}, "questions": {"value": "- Is there a concrete example of a domain where the $\\eta^*$ dual solution captures critical dynamics that discrete sampling would not? Intuitively, it seems that most or all realistic domains are \"smooth\", i.e. dynamics do not vary wildly across small distances. This seems supported by the ablation in Figure 2; one would expect failures of the naive solution to have a more \"jagged\" shape were this not the case. \n- What is the distinction of the dual function from prior work? By reading the paper, one can understand that it provides robustness guarantees beyond the finite sample set. Is this proven, and are there other qualities that can be stated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Qpicd6QLYq", "forum": "RFQhCkUcko", "replyto": "RFQhCkUcko", "signatures": ["ICLR.cc/2026/Conference/Submission3334/Reviewer_NxQa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3334/Reviewer_NxQa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903025594, "cdate": 1761903025594, "tmdate": 1762916676360, "mdate": 1762916676360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new algorithm RAPO that steers the trajectories towards low return ones, resembling the trajectories drawn from adversarial kernel. This  makes the policy robust."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea is good and motivation is intuitive. However my main concern is that the work is not compared with [1] which seems very similar in setting and motivation."}, "weaknesses": {"value": "The current approach seems very similar to [1] where they also do pessimistic sampling, can authors compare and contrast with [1]. \n\n\n\n[1]@inproceedings{\ngadot2024bring,\ntitle={Bring Your Own (Non-Robust) Algorithm to Solve Robust {MDP}s by Estimating The Worst Kernel},\nauthor={Uri Gadot and Kaixin Wang and Navdeep Kumar and Kfir Yehuda Levy and Shie Mannor},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=UqoG0YRfQx}\n}"}, "questions": {"value": "Q1) The paper considers sa-rectangular uncertainty sets which are very conservative. Can authors comment if this approach can be extended to s-rectangularn [2] or non-rectangular uncertainty sets [3,4]. \n\n\n\n\n\n[2]@inproceedings{\nkumar2024efficient,\ntitle={Efficient Value Iteration for s-rectangular Robust Markov Decision Processes},\nauthor={Navdeep Kumar and Kaixin Wang and Kfir Yehuda Levy and Shie Mannor},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=J4LTDgwAZq}\n}\n\n[3] @inproceedings{\nkumar2025nonrectangular,\ntitle={Non-rectangular Robust {MDP}s with Normed  Uncertainty Sets},\nauthor={Navdeep Kumar and Adarsh Gupta and Maxence Mohamed ELFATIHI and Giorgia Ramponi and Kfir Yehuda Levy and Shie Mannor},\nbooktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},\nyear={2025},\nurl={https://openreview.net/forum?id=Xx0cJGXU7n}\n}\n\n[4]@misc{li2025policygradientalgorithmsrobust,\n      title={Policy Gradient Algorithms for Robust MDPs with Non-Rectangular Uncertainty Sets}, \n      author={Mengmeng Li and Daniel Kuhn and Tobias Sutter},\n      year={2025},\n      eprint={2305.19004},\n      archivePrefix={arXiv},\n      primaryClass={math.OC},\n      url={https://arxiv.org/abs/2305.19004}, \n}"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hcr639pRlT", "forum": "RFQhCkUcko", "replyto": "RFQhCkUcko", "signatures": ["ICLR.cc/2026/Conference/Submission3334/Reviewer_qPaT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3334/Reviewer_qPaT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982864528, "cdate": 1761982864528, "tmdate": 1762916675937, "mdate": 1762916675937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}