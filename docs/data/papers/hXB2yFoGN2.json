{"id": "hXB2yFoGN2", "number": 16910, "cdate": 1758270171468, "mdate": 1759897210709, "content": {"title": "Tabular Data: Is Deep Learning All You Need?", "abstract": "Tabular data represent one of the most prevalent data formats in applied machine learning, largely because they accommodate a broad spectrum of real-world problems.  \nExisting literature has studied many of the shortcomings of neural architectures on tabular data and has repeatedly confirmed the scalability and robustness of gradient-boosted decision trees across varied datasets. However, recent deep learning models have not been subjected to a comprehensive evaluation under conditions that allow for a fair comparison with existing classical approaches. This situation motivates an investigation into whether recent deep-learning paradigms outperform classical ML methods on tabular data. \nOur survey fills this gap by benchmarking seventeen state-of-the-art methods, spanning neural networks, classical ML and AutoML techniques. Our empirical results over 68 diverse datasets from a well-established benchmark indicate a paradigm shift, where Deep Learning methods outperform classical approaches.", "tldr": "", "keywords": ["tabula data", "tabular survey", "tabular foundation models", "neural networks", "tree-based methods", "hyperparameter optimization"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cbf7b14dba12c197a09df9b2e1ed29812ed49f3f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This is a rigorous, large-scale empirical study asking whether modern deep learning now consistently outperforms classical methods on tabular classification. The authors benchmark 17 methods (foundation models, dataset-specific NNs, GBDTs, and AutoML) across 68 OpenMLCC18 datasets with nested 10-fold CV, model-based HPO (Optuna/TPE; up to 100 trials or 23h), and a refit-after-HPO protocol. Headline results: deep learning—especially in-context foundation models (TabICL, TabPFNv2) and the MLP ensemble TabM—dominates GBDTs overall; refitting after HPO improves ranks and can reshuffle winners; and DL wins across data-regimes, including “small data,” contrary to long-standing folklore. Code is released for reproducibility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The nested CV + model-based HPO + refitting design is stronger than prior surveys relying on random search and no refit. The authors explicitly motivate these choices and quantify their impact.  \n\nInclusion of both in-context tabular FMs and strong MLP ensembles (TabM), alongside AutoML, makes conclusions relevant to practice in 2025.  \n\nRank distributions, one-vs-one win-rates, CD diagrams, and regime plots tell a consistent story (TabICL/TabPFNv2 ≳ TabM ≳ CatBoost/XGBoost).  \n\nActionable insights: (i) Refitting after HPO improves results and may change leaderboards; (ii) HPO helps several methods materially (e.g., XGBoost, XTab), with detailed hyperparameter importance analyses.  \n\nContrary to the common belief that trees rule small tables, TabICL/TabPFNv2 (and TabM) are highly competitive, often surpassing CatBoost/LightGBM."}, "weaknesses": {"value": "Many plots focus on ranks/win-rates. Please complement with absolute ROC-AUC deltas (mean/median ± CIs) and per-dataset paired tests to quantify practical margins.  \n\nYou note ~8M evaluations; please add wall-clock/energy summaries and cost-normalized leaderboards (e.g., AUC per hour) to contextualize results for practitioners.\n\nSome models use bespoke preprocessing; batch size is heuristic due to memory. A short ablation on preprocessing/batch sensitivity for a few methods would strengthen fairness claims."}, "questions": {"value": "Add tables with median ΔAUC vs top GBDT per dataset family (small/medium/large), with 95% CIs.  \nProvide cost-performance plots (AUC vs GPU-hours) and a practitioner-oriented “best under X hours” guide.  \nInclude a compact regression panel or, at minimum, temper the title/claims to “classification.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SNVUPHtcEj", "forum": "hXB2yFoGN2", "replyto": "hXB2yFoGN2", "signatures": ["ICLR.cc/2026/Conference/Submission16910/Reviewer_oVYN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16910/Reviewer_oVYN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760727415635, "cdate": 1760727415635, "tmdate": 1762926937581, "mdate": 1762926937581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a survey and benchmarking study of ML and deep learning methods on a diverse collection of tabular datasets. It also offers useful insights into the role of hyperparameter optimization and the cost-performance trade-offs between different model families. However, the main weakness is its novelty: the recent TabArena benchmark [1] already provides an extensive, reproducible, and **continuously maintained** benchmarking effort. This raises questions about how the present work advances the state of knowledge beyond TabArena.\n\n\n\nReferences:\n[1] Erickson, Nick, Lennart Purucker, Andrej Tschalzev, David Holzmüller, Prateek Mutalik Desai, David Salinas, and Frank Hutter. \"Tabarena: A living benchmark for machine learning on tabular data.\" arXiv preprint arXiv:2506.16791 (2025)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The manuscript addresses a central challenge in tabular learning e.g. fairly benchmarking ML and DL methods on tabular datasets\n- It provides valuable insights for practitioners, particularly regarding training strategies and hyperparameter sensitivity.\n- The paper is clearly written and easy to follow."}, "weaknesses": {"value": "The primary weakness is limited novelty. The TabArena work [1] already provides: (1) a large-scale, reproducible benchmarking ecosystem, (2) a live leaderboard that can be continuously updated, (3) a carefully curated dataset collection, and (4) strong baselines with advanced evaluation protocols. Moreover, TabArena reports similar empirical conclusions—for example, that DL methods can outperform classical methods in certain regimes. As a result, it is unclear what unique contribution this paper makes beyond prior work."}, "questions": {"value": "I have one main question: What are advantages of the presented work over the TabArena? \n\nAlso, If the intention is to study a simpler or more controlled benchmarking setting than TabArena, this should be explicitly justified, along with the scientific insight that such a restriction reveals.\n\nI'm ready to update my rating depending on the authors response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MTV7fT2aYn", "forum": "hXB2yFoGN2", "replyto": "hXB2yFoGN2", "signatures": ["ICLR.cc/2026/Conference/Submission16910/Reviewer_qEWD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16910/Reviewer_qEWD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761241210025, "cdate": 1761241210025, "tmdate": 1762926937150, "mdate": 1762926937150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper performs an extensive and thourough comparison of the recent tabural ML approaches. The authors demonstrate that for classification tasks DNN-based methods outperform GBDTs on the benchmark of classification problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The fact that refitting (after performing hyperparameter optimization) is more beneficial for GBDTs than for DNNs is interesting and new to me, at least I have never met it in the literature."}, "weaknesses": {"value": "(1) The paper investigates only classification problems and it is not clear from the title/abstract/contributions bullet list. The final claim in the abstract can be misleading, for instance, \"deep learning methods outperform classical approaches\" can be false for regression, see the recent TabArena leaderboard for regression.\n\n(2) The chosen wording can also be misleading. For instance, the claim \"nonfinetuned foundation models outperform fine-tuned ones\" can be unclear, since rigorously speaking, meta-learned foundational models can be finetuned (e.g. see \"On finetuning tabular foundation models\", Rubachev et al.) and then the authors' claim is false.\n\n(3) The authors do not include the recent non-parametric models (TabR, ModernNCA) in their evaluation, despite these models being strong players in the field. For instance, TabICL paper demonstrated that TabR and ModernNCA can outperform meta-learned models. I believe that in such kind of papers all the recent models should be used.\n\n(4) In my opinion, most of the news from this submission are already known by the people in the field. For instance, TabM paper already reported the advantage of DNNs over GBDT, as well as TabICL and TabArena papers."}, "questions": {"value": "I do not have any specific questions for the rebuttal, if the authors will address by concerns from the weaknesses section, I will appreciate that."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DY6rra9ibI", "forum": "hXB2yFoGN2", "replyto": "hXB2yFoGN2", "signatures": ["ICLR.cc/2026/Conference/Submission16910/Reviewer_81kb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16910/Reviewer_81kb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750582732, "cdate": 1761750582732, "tmdate": 1762926936526, "mdate": 1762926936526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper re-examines the question of deep learning vs decision trees on tabular data. It uses the OpenML-CC18 benchmark suite and proposes it's own experimental protocol (with main difference from prior work being refitting). The paper compares AutoML system (in autogluon), in-context learning (ICL) based tabular foundation models (in TabPFN(v1, v2) and TabICL), parametirc transformer-based foundation models that require finetuning (XTab, TPBERTa, CARTE), GBDT and neural network baselines (in TabM, RealMLP, FT-Transformer, SAINT, ResNet and MLP). The paper core message seems to be the highlight of paradigm shift, where present-day tabular DL models outperform GBDTs. Mostly through TabM and ICL-based foundation models. The paper also makes multiple finer observations like ICL-based foundation models being superior to finetuning-based parametric ones or looking at the dataset size win-rate profiles and model hyperparameter sensitivity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The first (broader) strength and argument for the paper is in it's ovearall message and survey-ish nature. The field of Tabular Deep Learning did progress in recent years and the paper manages to convey this message. I think it may be important for the broader DL community to know about the subfield advances.\n\nAnother strong aspect (much less important in my view) is in more subtle findings that are novel:\n- RealMLP performance seems to be much less good compared to it's resulst on TabArena - this is interesting and warrants at least some investigation? Does the setup difference matter this much?\n- XTab and TPBERTa lacking behind ICL-based foundation models - this result is important to set the record straight (but may also warrant some explanation of why this is the case, compared to the success in the original papers)\n- An observation that small datasets is where the ICL-based foundation models have more wins\n- Demonstration that refitting provides some improvements"}, "weaknesses": {"value": "The core weakness of this work is in lacking the field context. By lacking context I mean that the paper for it's main goal (that seems to be conveying the message of progress in DL for tabular data), misses a bit on where the field is actually at.\n\nFirst, I think that recent focus on dataset quality in benchmarks brought up in recent work ([Erickson et al.](https://arxiv.org/abs/2506.16791), [Rubachev et al.](https://arxiv.org/abs/2406.19380), [Tschalzev et al.](https://arxiv.org/abs/2503.09159) should be discussed and taken into account in a new \"benchmark\"/\"revisiting the state of X\" paper.\n\nSecond, It is very hard to discount the existence of TabArena (([Erickson et al.](https://arxiv.org/abs/2506.16791)), which is not discussed in the current paper. In my view TabArena is a step in the right direction for the field in general. There is a certain blueprint to revisiting DL for tabular data publications (e.g. [Tabzilla](https://arxiv.org/abs/2305.02997), [TALENT](https://arxiv.org/abs/2407.00956), [MultiTab](https://arxiv.org/abs/2505.14312), and I'm probably missing some more, but you see the point) which is roughly take some dataset suite, take some recent models, compare. I believe that it is wastefull to reinvent the evaluation over and over again each year. From my point of view the field is slowly growing past that blueprint: more downstream task relevant benchmarks being introduced (like TabReD [Rubachev et al.](https://arxiv.org/abs/2406.19380) - covering Industry ML use-cases or [Barkov et al.](https://arxiv.org/abs/2508.09888) covering digital soil mapping applications, and I think we should have much more of that kind of work). With all the more specific benchmarks, TabArena stands for the general MMLU/GLUE-like proxy for researcher fast iteration.\n\nFurthermore, the present paper study misses some important baselines in ModernNCA, TabR and the more recent LimiX tabular foundation model. In contrast, TabArena has the first two (covering a paradigm important in modern tabular deep learning), and the LimiX (which is recent and may be hard to ask to put in an ICLR submission) is on the way there https://github.com/autogluon/tabarena/pull/208\n\nTo circle back and summarize my argument: I think that the paper may be important to share with the broader DL community at ICLR, but as I outlined above, it feels a bit disconnected from many recent developments in the field of tabular deep learning. \n\nThere are other strenghts that are related to more specific findings, but these lack deeper analysis and provide limited insight or little to no-explanation."}, "questions": {"value": "I am very skeptical that it is possible to address the core weakness outlined above, but I believe that I can be swayed by deeper analysis, specifically in two of the following areas:\n\n- Why does RealMLP performance differ in your protocol and in tabarena?\n- Why do non-ICL foundation models fail?\n- A  more  in-depth study of dataset-size and ICL models dominance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eEdFKPUYxN", "forum": "hXB2yFoGN2", "replyto": "hXB2yFoGN2", "signatures": ["ICLR.cc/2026/Conference/Submission16910/Reviewer_15cs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16910/Reviewer_15cs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766483953, "cdate": 1761766483953, "tmdate": 1762926936076, "mdate": 1762926936076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}