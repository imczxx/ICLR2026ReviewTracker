{"id": "G7OqPC2OeR", "number": 4708, "cdate": 1757751414215, "mdate": 1759898018578, "content": {"title": "How to Spin an Object: First, Get the Shape Right", "abstract": "We present unPIC, a method for generating novel 3D-consistent views of an object from a single image. Given one input view, unPIC produces a full spin of the object around its vertical axis, a process that is typically a precursor for reconstructing the object in 3D.\nOur key idea is to predict the object's underlying 3D geometry from the input image _before_ predicting the textured appearance of the novel views. To this end, unPIC consists of two modules: a multiview geometry _prior_, followed by a multiview appearance _decoder_, both implemented as diffusion models but trained separately. During inference, the geometry serves as a blueprint to coordinate the generation of the final novel views, thus enforcing consistency across the object's 360-degree spin. We introduce a novel pointmap-based representation to capture the geometry, with one key advantage: it allows us to obtain a 3D point cloud directly as part of the view-synthesis process, rather than a post-hoc step. \nOur modular, geometry-driven framework demonstrates superior performance, outperforming leading methods like InstantMesh, EscherNet, CAT3D, and Direct3D on novel-view quality, geometric accuracy, and multiview-consistency metrics. Furthermore, unPIC shows strong generalization to challenging, real-world captures from datasets like Google Scanned Objects and the Digital Twin Catalog.", "tldr": "Direct image-to-3D via view synthesis using a novel pointmap representation", "keywords": ["image-to-3D", "multiview diffusion", "pointmaps"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/57dcdfa1c6f1a496c880bad76e8c923a5a965ee3.pdf", "supplementary_material": "/attachment/3e553100548513305cabbf62a48e828100f5e397.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method to generate novel-view point maps and RGB images from single-view inputs. The key idea is to adopt multiview diffusion to generate both. Then, the point cloud could be directly extracted from the generated point maps and RGB images. Finally, the performance is evaluated on the novel view synthesis and 3D reconstruction tasks, outperforming baselines like Free3D, One-2-3-45 and OpenLRM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The whole pipeline is reasonable and authors successfully train the model and demonstrate the performance."}, "weaknesses": {"value": "1. The idea of generating point maps has already been explored by SweetDreamer (ICLR'23) two years ago. Some recent works, like World-consistent Video Diffusion with Explicit 3D Modeling (CVPR'25), also use this idea. The paper does not discuss the difference. Some very similar papers about multiview diffusion papers, like MVDream, SyncDreamer, and Wonder3D, are not included in the discussion either. The method proposed by the paper is already well-studied in these existing works.\n2. Another main problem is that the paper seems to miss a whole set of papers about latent vecset diffusion, like CLAY, Hunyuan, TripoSG, and so on, which could produce much better results than the proposed method.\n\nIn summary, the idea is already well-explored by existing works, and the authors are encouraged to read these papers and include more discussion on the differences from the existing works."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ApkkrhMEpR", "forum": "G7OqPC2OeR", "replyto": "G7OqPC2OeR", "signatures": ["ICLR.cc/2026/Conference/Submission4708/Reviewer_Um9x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4708/Reviewer_Um9x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482920051, "cdate": 1761482920051, "tmdate": 1762917527505, "mdate": 1762917527505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method to generate novel-view point maps and RGB images from single-view inputs. The key idea is to adopt multiview diffusion to generate both. Then, the point cloud could be directly extracted from the generated point maps and RGB images. Finally, the performance is evaluated on the novel view synthesis and 3D reconstruction tasks, outperforming baselines like Free3D, One-2-3-45 and OpenLRM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The whole pipeline is reasonable and authors successfully train the model and demonstrate the performance."}, "weaknesses": {"value": "1. The idea of generating point maps has already been explored by SweetDreamer (ICLR'23) two years ago. Some recent works, like World-consistent Video Diffusion with Explicit 3D Modeling (CVPR'25), also use this idea. The paper does not discuss the difference. Some very similar papers about multiview diffusion papers, like MVDream, SyncDreamer, and Wonder3D, are not included in the discussion either. The method proposed by the paper is already well-studied in these existing works.\n2. Another main problem is that the paper seems to miss a whole set of papers about latent vecset diffusion, like CLAY, Hunyuan, TripoSG, and so on, which could produce much better results than the proposed method.\n\nIn summary, the idea is already well-explored by existing works, and the authors are encouraged to read these papers and include more discussion on the differences from the existing works."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ApkkrhMEpR", "forum": "G7OqPC2OeR", "replyto": "G7OqPC2OeR", "signatures": ["ICLR.cc/2026/Conference/Submission4708/Reviewer_Um9x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4708/Reviewer_Um9x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482920051, "cdate": 1761482920051, "tmdate": 1763697996655, "mdate": 1763697996655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces unPIC, a method for generating a fully 3D-consistent spin of an object from a single input image by explicitly separating the prediction of underlying 3D geometry from textured appearance.\n\nThis hierarchical generation is implemented using two independently trained diffusion models: a multiview geometry prior, followed by a multiview appearance decoder. A key contribution enabling this architecture is a novel geometric representation called CROCS (Camera-Relative Object Coordinates), which provides dense pointmaps encoding per-pixel 3D coordinates anchored to the source camera.\n\nThe predicted geometry serves as a blueprint to coordinate the final views, enforcing consistency and enabling the direct generation of a 3D point cloud without a separate post-hoc reconstruction step.\n\nThis geometry-driven framework significantly outperforms leading methods on novel-view quality, geometric accuracy, and multiview consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces CROCS (Camera-Relative Object Coordinates), a novel dense pointmap representation that is critical to the method's success. Empirical evidence strongly supports its effectiveness.\n  - By conditioning the appearance decoder on CROCS, the framework allows for direct 3D generation. The output multiview CROCS images provide the vertices, and the RGB images provide the vertex colors, which assemble directly into a colored point cloud, bypassing the need for a separate post-hoc reconstruction step common in other pipelines.\n- unPIC demonstrates superior quality, consistently outperforming strong contemporary baselines. The authors provide extensive experiments that substantiate the method’s performance advantages, and the model exhibits robust generalization to challenging real-world captures.\n- The paper is easy to follow, with clear writing."}, "weaknesses": {"value": "- The authors made a deliberate design choice not to canonicalize for changes in camera elevation. While this choice aligns with typical human mental rotation habits, it forces the model to implicitly infer the camera elevation from the source image. This implicit reliance on the appearance module to deduce a crucial geometric parameter is a source of fragility, which is confirmed by the observed failure case where the model misinterprets the source image (e.g., as an overhead view) and performs incorrect planar rotation.\n\n- Both the geometry prior and the appearance decoder are implemented as multi-view diffusion models (MVD) and trained separately. This hierarchical two-stage MVD architecture, totaling 1.1 Billion parameters, is inherently computationally expensive."}, "questions": {"value": "- Why did the CROCS VAE have a significantly lower KL divergence before fine-tuning compared to the RGB VAE (20823 vs 28005)? Does this suggest that the latent space of the CROCS representation is inherently smoother or closer to a standard Gaussian, contributing directly to CROCS's superior predictability?\n- Could the authors provide a detailed breakdown of the total 1m13s inference time? Specifically, what proportion is spent in the Geometry Prior module versus the Appearance Decoder module? Such a decomposition would be valuable for guiding subsequent runtime optimizations.\n- unPIC provides \"Direct 3D\" output by combining CROCS vertices and RGB colors into a colored point cloud. Unlike geometry-supervised pipelines that return explicit surfaces, the point cloud is not a ready-to-use explicit surface, potentially necessitating further reconstruction steps for applications requiring watertight meshes. Given that the output is a high-accuracy colored point cloud, have the authors explored post-processing to convert this point cloud into an explicit mesh? If so, what are the resulting mesh quality and the practical utility for downstream applications?\n\n- A key advantage claimed for unPIC is its hierarchical approach designed to maximize diversity by sampling multiple geometries and appearances; however, the main evaluation focuses on the accuracy of a single best output. How is diversity quantified? Have the authors conducted a quantitative assessment of generative diversity—for example, for a given input image, how much variation is observed among the N geometry latents $\\hat{Z}_{g}$ produced by the Prior (e.g., via Chamfer distance or LPIPS), and among different appearances $\\hat{Z}_a$ generated by the Decoder conditioned on the same $\\hat{Z}_g$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LYfhZqf8X7", "forum": "G7OqPC2OeR", "replyto": "G7OqPC2OeR", "signatures": ["ICLR.cc/2026/Conference/Submission4708/Reviewer_BpYY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4708/Reviewer_BpYY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985619622, "cdate": 1761985619622, "tmdate": 1762917527142, "mdate": 1762917527142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a framework for generating 3D-consistent novel views of an object from a single image. Unlike previous methods that jointly infer geometry and appearance, unPIC disentangles them in two stages: geometry prior and appearance decoding. In the first stage, the model predicts the object geometry using CROCS (Camera-Relative Object Coordinates), a dense, camera-aligned geometric representation. In the second stage, it decodes this geometry into multiview textured images, ensuring geometric consistency across different views. Moreover, the use of CROCS also enables direct reconstruction of 3D point clouds from the generated views."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a simple yet sound idea that leads to clear performance improvements.\n- The use of CROCS allows consistent 3D encoding without explicit segmentation or class priors, outperforming existing alternatives like NOCS.\n- CROCS also allows direct extraction of 3D point clouds from generated views, simplifying 3D reconstruction pipelines by removing postprocessing.\n- The experiments are extensive\n  - Tab3 shows NVS results on 4 different datasets (Objaverse-XL, GSO, ABO, and DTC) and comparing with 6 baselines.\n  - Tab5 and Fig5 demonstrate superior performance on 3D reconstruction.\n  - The author also ablates the importance of geometry prior in Tab4."}, "weaknesses": {"value": "- There are insufficient qualitative results showing the reconstructed 3D objects – either as point clouds or meshes. Only a single example is provided in Fig5.\n- The quantitative results in Tab3, particularly the PSNR values, appear unusually high compared to the baselines. However, the qualitative examples (Fig4 and Supp.) do not seem to reflect such a large margin of improvement. This discrepancy raises concerns about how the metrics were computed and whether all methods were evaluated under the same viewing conditions."}, "questions": {"value": "- Since some test images include camera elevation, is elevation provided or conditioned for the baseline methods?\n- In Fig1, the authors refer to “input image(s),” suggesting that the method may accept multiple input views. However, the paper only presents results using a single input image. It would be helpful to clarify whether the proposed framework can be extended to multi-view inputs, and if so, how the model’s performance scales with additional views."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nVGbeH2Jnw", "forum": "G7OqPC2OeR", "replyto": "G7OqPC2OeR", "signatures": ["ICLR.cc/2026/Conference/Submission4708/Reviewer_Y436"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4708/Reviewer_Y436"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988116236, "cdate": 1761988116236, "tmdate": 1762917526843, "mdate": 1762917526843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper is clearly written, with many details deferred to a well-organized appendix.\n- It fine-tunes a dedicated VAE for CROCS and reports a substantial VAE score improvement after fine-tuning.\n- It presents extensive quantitative experiments and ablations, with results consistently favoring CROCS on the novel-view synthesis tasks.\n- Code and checkpoints are open-sourced."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written, with many details deferred to a well-organized appendix.\n- It fine-tunes a dedicated VAE for CROCS and reports a substantial VAE score improvement after fine-tuning.\n- It presents extensive quantitative experiments and ablations, with results consistently favoring CROCS on the novel-view synthesis tasks.\n- Code and checkpoints are open-sourced."}, "weaknesses": {"value": "- Figure 5 is the only qualitative 3D reconstruction example; stronger qualitative evidence is needed. CROCS point maps may appear noisy on edges and thin structures, making denoising and detail preservation non-trivial; the resulting point cloud may be coarse, and vertex color aggregation across predicted views can be inconsistent, with non-trivial texture post-processing.\n- Novel view synthesis is restricted to eight canonical views and cannot sample arbitrary viewpoints.\n- Baselines are a little outdated; recent open-source SOTA (e.g., TRELLIS) is missing.\n- The novelty of CROCS is limited: CROCS is adapted based on SpaRP’s NOCS variant (Xu et al., 2024a). Sec. 3.3 and Figure 3 explain the differences between the original NOCS and CROCS. However, both SpaRP’s NOCS variant and CROCS are oriented by the source camera’s azimuth and are axis-aligned; at the source view (Figure 3), CROCS is the same as SpaRP’s NOCS."}, "questions": {"value": "- Please provide more qualitative examples and comparisons for 3D reconstruction results (point clouds) to demonstrate usefulness beyond canonical novel views.\n- In the novel view synthesis experiments (Table 3), how are target views selected for each method? These baselines define different canonical target views than unPIC. Please clarify to ensure a fair comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "obVtqg2EIP", "forum": "G7OqPC2OeR", "replyto": "G7OqPC2OeR", "signatures": ["ICLR.cc/2026/Conference/Submission4708/Reviewer_uYkA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4708/Reviewer_uYkA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162076051, "cdate": 1762162076051, "tmdate": 1762917526612, "mdate": 1762917526612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}