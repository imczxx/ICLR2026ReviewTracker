{"id": "PmYX0XiQQ0", "number": 8496, "cdate": 1758087363443, "mdate": 1759897780583, "content": {"title": "VLAC: A Generalist Action-Critic Model via Pair-wise Progress Understanding", "abstract": "Recent advances in Vision-Language-Action (VLA) models have significantly improved robotic perception and manipulation capabilities. However, robots deployed in real-world settings still struggle to adapt in dynamic, open-ended environments due to a lack of reliable task progress feedback and improvement mechanisms. To address these challenges, we propose a generalist Vision Language Action-Critic model, VLAC, which can integrate both human and robot data, and unify action generation and task progress understanding within a single autoregressive architecture. Specifically, we propose a scalable and generalizable pair-wise progress understanding approach to predict the task progress delta between any two images in one visual trajectory, and generate the action based on the first image. The model is trained on large-scale, multi-source human data without action annotations and robot data with action information, while also incorporating general vision-language data yielding world knowledge understanding. Furthermore, we deploy reinforcement learning where VLAC can autonomously evaluate task progress to feedback intrinsic rewards. We evaluated our model's progress understanding across eight datasets and show that it not only generalizes to new tasks and environments but also discriminates success from failure trajectories, e.g., on RoboFAC dataset, it reaches VOC-F1 0.89 for successful versus 0.44 for failed trajectories, providing dependable dense reward signals. Then, we evaluated action generation and real-world reinforcement learning performance on diverse real-world robotic manipulation tasks. Experimental results indicate strong disturbance robustness in VLAC’s action generation, while integrating pairwise progress prediction allows real-world RL to improve success from roughly 30\\% to 90\\% within 200 episodes.", "tldr": "Vision-Language-Action-Critic Model", "keywords": ["vision language model", "robotics", "multi-modal", "reward model", "real world rl"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/81b83f976dc5ed03b45debf25e1b85d7c3382614.pdf", "supplementary_material": "/attachment/b746246dda485450bf2f4da1f722f07b2f1721c8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes VLAC, a unified Vision–Language–Action–Critic architecture that aims to predict both robot actions and progress toward task completion. The method uses a pairwise progress-understanding mechanism: given two frames from a video, the model predicts their relative task progress and uses this signal as an intrinsic reward for reinforcement learning. The model is trained on a large mixture of web-scale video, human demonstration, and robot datasets. Experiments claim that the learned critic can generalize across several benchmarks and that using its reward signal helps improve on-robot reinforcement learning efficiency and success rate."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•\tThe topic—learning dense progress and reward signals from visual data for RL—is relevant to embodied intelligence and of potential long-term interest.\n\n•\tThe model attempts to unify vision, language, and action learning under a shared architecture, which aligns with the trend of generalist agent design."}, "weaknesses": {"value": "•\tThe pairwise progress prediction and intrinsic reward learning are straightforward extensions of established ideas in temporal contrastive learning and visual reward modeling. There is little conceptual or methodological innovation beyond scaling existing components.\n\n•\tKey mechanisms—such as how progress labels are computed, how critic and actor losses interact, and how the model architecture fuses modalities—are described imprecisely. The paper lacks equations, algorithmic clarity, and ablation to demonstrate each part’s contribution.\n\n•\tExperimental results are not rigorously supported. No proper statistical tests or confidence intervals are reported, and baselines are weak or missing (e.g., comparison to existing learned reward or progress models). Claims of “generalization across unseen datasets” are not verified with strong quantitative metrics or external benchmarks.\n\n•\tThe training pipeline depends on massive compute (200 A100 GPUs), and many details are hidden in appendices. There is no released code, no hyperparameter sweep, and insufficient description to allow independent reproduction.\n\n•\tThe paper repeatedly claims strong generalization and practical real-world success, but given the weak evidence and missing baselines (e.g. ReinboT), these claims are overstated relative to the presented data. \n\n•\tThe writing is unclear, especially in the methods section. The internal logical relationships between the various modules are difficult to understand. The experimental setup and result presentation are crude, making it difficult to convincingly demonstrate the effectiveness of the methods.\n\nRef: Zhang, H., Zhuang, Z., Zhao, H., Ding, P., Lu, H., & Wang, D. (2025). ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning. arXiv preprint arXiv:2505.07395."}, "questions": {"value": "1.\tHow is the pairwise progress label precisely defined? Is it a normalized temporal difference or learned via contrastive supervision? Please provide mathematical formulation.\n\n2.\tHow is the critic integrated into RL training—does it produce scalar rewards or dense time-series signals? How are they scaled relative to environment rewards?\n\n3.\tWhat are the quantitative baselines for the critic compared to existing visual progress estimators or learned reward models?\n\n4.\tWhat measures were taken to ensure that the critic does not produce misleading rewards in visually noisy or multi-goal environments?\n\n5.\tCan the reported real-robot improvements (≈30%→90%) be replicated with fewer compute resources or smaller models, or are they specific to large-scale pretraining?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uqcaUVN8U2", "forum": "PmYX0XiQQ0", "replyto": "PmYX0XiQQ0", "signatures": ["ICLR.cc/2026/Conference/Submission8496/Reviewer_fPty"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8496/Reviewer_fPty"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761307229789, "cdate": 1761307229789, "tmdate": 1762920368656, "mdate": 1762920368656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VLAC, a VLA model which also predicts the number of frames between subsequent images. This is used as a goal-conditioned critic function. The authors"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* Though not new, the idea of using a goal representation (or second image) for progress estimation is good.\n* The authors provide fairly extensive detail on all of the different training data they use for the VLAC model."}, "weaknesses": {"value": "To me, it seems like the authors have a) proposed a goal-conditioned version of GVL and b) trained a model for GVL instead of using an off-the-shelf model (which was the purpose of GVL -- no training required). However, these choices do not appear extremely well justified at present.\n* The point of GVL is that it was training free -- you could just prompt an existing model to get value estimates. In that regard it doesn't make sense to have GVL be the main baseline here. The authors should be comparing to other approaches or other ways of training a value function (particularly other ways of training a goal conditioned value function)\n* The value-function based evaluation is fairly weak. Only VOC is used (which I think is a rather weak metric as maximizing VOC is only meaningful if the data is actually from a true expert, which it rarely is). Why not consider policy learning on filtered data etc.?\n* The authors call their method \"pair-wise progress estimation\" when it really seems like a goal conditioned value function $V(s,g)$ instead of just $V(s)$. This makes it seem like \"pair-wise proress estimation\" is more of a new idea than I think it really is. The authors could do a better job contextualizing their method. There have been several works that have tried to train large scale goal conditioned value functions. For example, ViVa: Video-Trained Value Functions for Guiding Online RL from Diverse Data.\n* the connection between the policy learning experiments and the story is a bit unclear. Is VLAC a critic or is it a VLA? Is the point that by being a critic and an actor at the same time you get better performance? This is a bit clear. One could evaluation the critic performance by filtering data according to the value function. One would show that doing both is better than doing just one by doing careful ablations -- but that isn't clear. What are the policy learning results trying to show in the context of the story?\n* there are no baselines for RL experiments.\n* I think training a model is useful and cool! but it is not necessarily a scientific contribution on its own -- what constitutes a contribution are the lessons learned, discoveries made, resources provided, new methods etc. While the paper does a good job detailing what the VLAC model is and how it performs, what the scientific contribution is remains a bit unclear to me. \n\nNits:\n* The authors frequently complain about \"single-point estimates\" but this term isn't well defined. I would usually expect it to indicate a single-sample estimate, but I think the authors are implying that they use an image pair instead of one image. However think that these are different modeling assumptions, since neither approach is actually using multiple independent samples."}, "questions": {"value": "* How is VOC computed on the dataset given the model is \"goal-conditioned\"? Do you compute the VOC by using subsequent frames and summing up the differences? Do you compute it by comparing each frame with the final frame? this is important because it can determine what VLAC is good at -- maybe its really good at seeing if there is progress in neighboring frames but worse at evaluation between distant frames.\n* If the authors are going to train a model from scratch, why did they not compare to other approaches that train from scratch? E.g. learning a goal conditioned value function with TD, latent intentions, VIP, etc. These seem like more apt baselines than GVL, which was intended as a training-free method.\n* Is there contamination between the VLAC pretraining data and the evaluation tasks? Are they more indistribution than the pi0 pretraining mixture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X8jOUxpcj3", "forum": "PmYX0XiQQ0", "replyto": "PmYX0XiQQ0", "signatures": ["ICLR.cc/2026/Conference/Submission8496/Reviewer_bypM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8496/Reviewer_bypM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865283617, "cdate": 1761865283617, "tmdate": 1762920368362, "mdate": 1762920368362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a generalist vision-language-action model that unifies action generation and task progress understanding within a single autoregressive architecture. Specifically, the authors introduce a pairwise progress understanding approach that predicts the task progress delta between any two images in a visual trajectory, while generating the corresponding action based on the first image. The model is trained using a combination of large-scale, multi-source human data (without explicit action annotations) and robot data (with action information), while also incorporating general vision-language data to enhance its world knowledge. The proposed framework’s progress understanding capabilities are evaluated across eight diverse datasets, demonstrating strong generalization to new tasks and environments. Furthermore, the authors validate the model’s effectiveness through real-world manipulation experiments."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors investigate an important and timely direction, i.e., progress understanding for real-world manipulation tasks. They propose a pairwise progress understanding approach that predicts the task progress delta between any two images within a visual trajectory and generates the corresponding action based on the first image. The proposed method demonstrates strong performance across eight public datasets, including six unseen during training, highlighting the model’s impressive generalization capability."}, "weaknesses": {"value": "Overall, the proposed framework addresses a timely and important direction. However, I have several concerns regarding the overall presentation of the manuscript:\n\n1. **Motivation:** In the first paragraph, the authors argue that existing VLA models fail to generalize across scenes with robustness due to the lack of efficient feedback. I find this claim overstated. There are multiple factors that hinder the generalization ability of VLA models. The authors are encouraged to tone down this statement and clarify that the direction explored in this manuscript represents one of several possible approaches to improving generalization.\n2. **Pairwise progress comparison is not a new idea:** The concept of progress understanding through pairwise comparison has been explored in prior works (e.g., [1–3]). However, the manuscript does not discuss how the proposed approach differs from these existing studies. Moreover, the paper lacks a discussion of the specific challenges encountered when incorporating this idea into a VLA framework, which makes the contribution appear less insightful and more like a system implementation paper.\n\n[1] Hung et al., VICtoR: Learning Hierarchical Vision-Instruction Correlation Rewards for Long-horizon Manipulation, ICLR 2024\n\n[2] Xue et al.,, Progress-Aware Video Frame Captioning, CVPR 2025\n\n[3] Kung et al., What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning, ICCV 2025.\n\n3. **Is the proposed pairwise progress comparison the main contributor to progress understanding?** From Figure 3, it appears that this component contributes to performance improvements on datasets such as RT-1, Bridge, DROID, and Dobb-E. However, the improvement is not consistent across datasets. In particular, RoboFAC shows a significant performance drop when using pairwise comparison, suggesting that the module may be ineffective in estimating progress in certain scenarios. Although performance improves when additional information is incorporated, this inconsistency raises concerns about the true contribution of the pairwise comparison module.\n4. **What is the impact of pairwise progression on action generation?** A key follow-up question is whether pairwise progression meaningfully contributes to real-world robotic manipulation. The results presented in Table 1 do not clearly convey this point, making it difficult to assess the practical importance of the proposed mechanism for action generation.\n5. **Limitations in ablation studies for real-world manipulation:** The current evaluation and demonstrations are insufficient to validate the effectiveness of the proposed framework in real-world manipulation tasks. Specifically, the paper lacks ablation studies comparing model performance with and without the pairwise progress understanding module. Given that the overall pipeline involves multiple components, including diverse datasets and design choices, it is challenging to isolate and attribute performance gains specifically to the proposed progress understanding approach.\n6. **Lack of empirical results for progress estimation in real-world robotic manipulation:** The current demonstrations are entirely static (e.g., Figures 4 and 6), making it difficult to justify the framework’s effectiveness in real-world manipulation scenarios.\n\n**Additional Notes:**\n\n1. The overall writing quality of the manuscript requires significant improvement. For example:\n    - **L44–46:** This segment is not a complete sentence and should be revised for grammatical correctness.\n    - **Equations (1)–(3):** The notations are not rigorous. In particular, the same function is presented with different argument formats, which is conceptually inconsistent and potentially confusing."}, "questions": {"value": "1. Motivation: Do the authors provide sufficient justification for their claim that the lack of efficient feedback is the primary reason existing VLA models fail to generalize across scenes? Could this claim be overstated given that multiple factors affect generalization, and should the authors clarify that their proposed direction is only one of several possible approaches?\n\n2. Novelty of the pairwise progress comparison: How does the proposed pairwise progress comparison approach differ from prior works such as Hung et al. (ICLR 2024), Xue et al. (CVPR 2025), and Kung et al. (ICCV 2025)? What unique challenges or insights arise when incorporating this idea into a VLA framework?\n\n3. Contribution of the pairwise progress comparison module: Is the proposed pairwise progress comparison truly the main contributor to progress understanding? Given that performance improvements are inconsistent across datasets (e.g., performance drops on RoboFAC), how do the authors explain these variations and assess the module’s overall effectiveness?\n\n4. Effect on action generation: To what extent does pairwise progression improve action generation performance, particularly in real-world robotic manipulation? The results in Table 1 do not make this relationship clear—can the authors provide more direct evidence or analysis?\n\n5. Ablation studies: Why are there no ablation studies comparing model performance with and without the pairwise progress understanding module? Considering the pipeline includes multiple components and datasets, how can one isolate the true contribution of this module to overall performance?\n\n6. Empirical validation in real-world settings: How effective is the proposed framework for progress estimation in real-world robotic manipulation? Since the demonstrations presented (e.g., Figures 4 and 6) are static, can the authors provide dynamic or real-robot experiments to support their claims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JEI5XXQTUY", "forum": "PmYX0XiQQ0", "replyto": "PmYX0XiQQ0", "signatures": ["ICLR.cc/2026/Conference/Submission8496/Reviewer_chZw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8496/Reviewer_chZw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076776509, "cdate": 1762076776509, "tmdate": 1762920367983, "mdate": 1762920367983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}