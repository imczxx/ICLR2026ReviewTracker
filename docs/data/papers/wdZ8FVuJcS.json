{"id": "wdZ8FVuJcS", "number": 22435, "cdate": 1758331030552, "mdate": 1763653661712, "content": {"title": "Be Affective, Not Just Cognitive - Towards Imparting Pertinent Empathy in Dialogue Agents", "abstract": "Empathetic Response Generation (ERG) has gained significant attention in diverse areas but still faces challenges that hinder its effectiveness. These challenges include $1$) the lack of affective empathy in existing works, where they exhibit cognitive empathy (feel $\\textit{for}$ user); $2$) generate generic responses, where agents address an emotion with monotonous replies; $3$) have limited user relatability. To tackle these issues, we propose incorporating affective empathy in models through additional pre-training. We introduce a benchmark dataset and its collection mechanism, that helps curate an $8.5$GB dataset, enabling the agent to truly feel $\\textit{with}$ user. Using this pre-trained model, our framework EMPATH enhances ERG by reducing generic responses. This is achieved by a novel loss function that involves both conversation history and golden response. EMPATH also enhances user relatability by accounting for multiple emotions and their underlying causes via explainability. Through extensive experimentation, we demonstrate the effectiveness of our dataset on our proposed framework and other existing approaches. Additionally, we depict EMPATH's superior performance in ERG on benchmark datasets across various metrics.", "tldr": "", "keywords": ["Dialogue and Interactive Systems", "Empathetic Response Generation", "Affective Empathy", "Pre-Training", "Synthetic Data"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a9b57dc4bfd390728e03efb33676321b5675191.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "EMPATH effectively enhances affective empathy and response specificity through its synthetic dataset and emotion-aware loss design, achieving significant performance gains across several benchmarks. The explainability-based emotion cause extraction improves interpretability and conciseness compared to traditional clause-based methods. However, the paper lacks verification of the synthetic dataset’s generalization to real-world human dialogues, and the hyperparameter optimization process for the proposed loss remains insufficiently detailed. Furthermore, the statistical significance of reported improvements is not analyzed, and reproducibility information is incomplete."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The study introduces a large, well-curated synthetic dataset that substantially expands resources for empathetic dialogue modeling and effectively supports the learning of affective empathy.\n\n2.The explainability-based emotion cause extraction improves interpretability and user relatability by achieving higher conciseness and accuracy than existing approaches.\n\n3.The proposed emotion-aware contextual loss successfully reduces generic responses by balancing emotional alignment, contextual relevance, and empathy validation."}, "weaknesses": {"value": "1.The real-world validity of the synthetic dataset remains untested. No experiments compare the model’s performance on synthetic versus human-written empathetic data, which raises concerns about its generalization to real user conversations.\n\n2.The paper does not include statistical significance analysis of the results. All metrics are presented as single values without variance or confidence intervals, leaving uncertainty about whether the reported improvements over baselines are meaningful.\n\n3.The process for tuning hyperparameters (λ₁/λ₂/λ₃) in the emotion-aware contextual loss is insufficiently described. The paper does not explain how the loss weights are optimized and lacks ablation studies to assess the contribution of each component.\n\n4.The framework’s behavior in non-empathetic contexts is not examined. There is no evaluation of whether EMPATH can avoid expressing empathy when it is inappropriate, which may lead to misalignment with user intent.\n\n5.The study does not consider multilingual or cross-cultural dimensions of empathy. All experiments are conducted in English, with no analysis of how cultural differences in emotional expression may influence model performance."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gOe2LofAGf", "forum": "wdZ8FVuJcS", "replyto": "wdZ8FVuJcS", "signatures": ["ICLR.cc/2026/Conference/Submission22435/Reviewer_NJTA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22435/Reviewer_NJTA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761500727982, "cdate": 1761500727982, "tmdate": 1762942217549, "mdate": 1762942217549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a full pipeline for endowing dialogue systems with affective (not only cognitive) empathy. The authors (i) construct a large “empathic statements” corpus by sampling ChatGPT across ~500 rounds (upper bound ≈125k sentences; after filtering ~111k unique) and then applying two-stage paraphrasing with PEGASUS-Large to reach ~81 M sentence, with Perspective-API filtering and 1% human-in-the-loop active learning; (ii) continue pre-training a T5-base model on this corpus to obtain E-T5; and (iii) introduce EMPATH, a decoding/learning framework that combines multi-emotion detection, explanation-based emotion-cause extraction via Integrated Gradients, and a triple loss (emotion-distribution KL, contextual cosine similarity, and an empathy-classifier MSE) within an iterative generation loop.\n\nEvaluation: Empathetic response generation (ERG) is reported on two datasets (EmpatheticDialog, CHASE). Emotion-cause extraction (ECE) is evaluated on RECCON. EMPATH outperforms diverse baselines on automatic metrics (e.g., perplexity, BLEU, Distinct-n, SES) and human preference ratings. Ablations indicate all modules contribute."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Resource contribution. The paper releases (and documents filtering for) an 80M-scale corpus explicitly targeting affective empathy, which is rare and potentially valuable to the community.\n\n2. Methodological novelty. Coupling multi-emotion awareness with explanation-based cause extraction and a unified triple loss is conceptually clean and addresses the “template empathy” failure mode seen in prior work.\n\n3. Empirical coverage for tasks. ERG and ECE are both covered, with ablations that clarify the effect of affective pre-training (E-T5), multi-emotion gating, explanation-based causes, and the loss design."}, "weaknesses": {"value": "1. Baseline fairness. E-T5 enjoys a substantial extra pre-training advantage (~8.5 GB) whereas several baselines are only fine-tuned. There is no comparison to size-matched strong baselines (e.g., FLAN-T5-base) or to frontier LLMs run at high temperature for diversity.\n\n2. Diversity claim under-evidenced. The stated motivation that PEGASUS paraphrasing yields more diverse outputs than ChatGPT is not empirically verified via Distinct-n, self-BLEU, or MAUVE comparisons.\n\n3. Human evaluation scale. Only six graduate annotators are used, and inter-rater reliability is not reported, limiting the strength of human-study conclusions."}, "questions": {"value": "1. Please clarify the fairness of the baseline comparisons.\n\n2. Please specify the exact model version and hyperparameters used for ChatGPT, Claude, and Gemini (precise model versions,  decoding parameters such as temperature/top-p/max tokens, context length, prompts, and seeds)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GwbdKFSvep", "forum": "wdZ8FVuJcS", "replyto": "wdZ8FVuJcS", "signatures": ["ICLR.cc/2026/Conference/Submission22435/Reviewer_E3kz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22435/Reviewer_E3kz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632541768, "cdate": 1761632541768, "tmdate": 1762942217335, "mdate": 1762942217335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel framework, EMPATH, to enhance empathetic response generation (ERG) in dialogue systems. It addresses limitations in existing models that exhibit only cognitive empathy and produce generic, unrelatable responses. The authors construct an 8.5GB affective empathy dataset using ChatGPT and Pegasus-based paraphrasing to pre-train a T5-based model. EMPATH combines affective and cognitive empathy, uses explainability for multi-emotion cause extraction, and introduces an emotion-aware contextual loss. Extensive experiments show superior empathetic, contextually relevant, and diverse responses across benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper correctly identifies an important and underexplored limitation in current ERG systems—the absence of affective empathy in responses.\n- The authors perform extensive evaluation using both quantitative metrics (BLEU, ROUGE, Distinct-n, etc.) and human assessments, providing evidence of performance gains."}, "weaknesses": {"value": "- Writing:\n  - The paper is difficult to follow due to the mixed presentation of methodology and implementation details.\n  - Many design choices in dataset construction and model formulation are not well motivated. Readers are often left uncertain why a particular step was necessary or how it contributes to affective empathy.\n  - Some sentences are very confusing, e.g., “we additionally pre-train the T5 model due to the open-source unavailability of LLMs and its strong performance.” Overall writing quality significantly hinders comprehension.\n- Evaluation: Although the paper poses the lack of affective empathy as a key research question, the evaluation does not directly measure it. The presented metrics (BLEU, Distinct-n, etc.) and corpus-level analysis do not show whether responses are indeed more affective rather than merely cognitive. Human evaluation focusing on empathy type or emotional depth would strengthen the claims.\n- The core claim—that affective empathy can be explicitly modelled is not contrasted against simpler or more intuitive alternatives. For instance, prompting LLMs with explicit instructions to generate affective (and avoid purely cognitive) responses could serve as an important baseline. Without such comparisons, it is unclear whether EMPATH’s complexity is necessary."}, "questions": {"value": "See the above comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qFeaecgqpp", "forum": "wdZ8FVuJcS", "replyto": "wdZ8FVuJcS", "signatures": ["ICLR.cc/2026/Conference/Submission22435/Reviewer_vqtz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22435/Reviewer_vqtz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761785863769, "cdate": 1761785863769, "tmdate": 1762942217088, "mdate": 1762942217088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}