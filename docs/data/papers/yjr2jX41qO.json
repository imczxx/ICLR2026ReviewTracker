{"id": "yjr2jX41qO", "number": 2974, "cdate": 1757309660939, "mdate": 1759898116295, "content": {"title": "Channel-Aware Mixed-Precision Quantization for Efficient Long-Context Inference", "abstract": "The key-value (KV) cache plays a vital role in accelerating autoregressive inference for large language models (LLMs). However, its linear memory growth with sequence length poses significant memory bottlenecks, especially in long-context scenarios.\nQuantization offers a promising solution for memory efficiency. While existing methods typically apply channel-wise quantization to the key cache and token-wise quantization to the value cache, they suffer from severe performance degradation under low-bit configurations.\nOur analysis reveals that this degradation stems from uniform bit allocation across channels, as different KV channels contribute variously to model performance.\nFollowing this finding, we propose ChanMix, a mixed-precision quantization framework that supports channel-wise quantization on 2-bit setting with FP8 precision with a custom Triton kernel implementation. To improve low-bit quantization performance, we introduce a channel-aware bit reallocation strategy, which allocates bits across channel sensitivity.\nThrough extensive evaluation, ChanMix demonstrates superior performance across the NIAH, RULER, and InfiniteBench benchmarks for the Llama, Mistral, and Qwen model families, achieving improvements of at least 5\\% on RULER compared to all baseline methods. Additionally, ChanMix enables a 2.3× increase in batch size and supports a 1.5× longer context length during inference.", "tldr": "", "keywords": ["Large Language Models", "Long Context", "Efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/23e0b8f41c574431b0359f08b13a95ef6c31b1a3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies KV cache compression, which is important for LLM serving in real-world applications, especially in long-context scenarios. The authors claim that uniform bit allocation is the source of error for methods applying channel-wise quantization of keys and token-wise quantization of values. The authors propose ChanMix, a mixed-precision outlier-aware framework specifically designed for KV cache quantization. They evaluate their method on several benchmarks and develop a custom Triton kernel implementation to support their claims."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper is well written. The tables and figures are well presented.\n2.\tThe numerical results (e.g. Table 1) are promising: ChanMix outperform several existing methods in most cases.\n3.\tThe efficiency analysis is clear, and a kernel implementation is useful for the community."}, "weaknesses": {"value": "1. Lines 17–19 and 53 overstate the findings. While the data is compelling, it does not support the claim that all quantization error stems from uniform bit-width allocation. I recommend softening the language to allow for nuance. Mixed-precision quantization is not new, and sensitivity variation across channels is expected. That said, this appears to be the first application of mixed-precision to KV, which is appreciated. The paper is well-motivated, but the novelty and breadth of the claims should be tempered.\n2. While ‘outlier’ is a widely used term in quantization literature, retrieval channel is a more recent, niche idea emerging from interpretability studies of attention heads, not a canonical term in the context of quantization. The corresponding part in section 4.1 should be expanded to provide more definition and background as retrieval channel is important in the motivation and implementation of the method proposed in this paper."}, "questions": {"value": "1. How are outliers and subnormal channels defined? The first usage is Section 4.1 without proper definition.\n2. Can you please provide more explanation for the statement ‘Channel reordering ensures efficient 8-bit aligned storage of the quantized cache’? It is also not clear to me how this reordering is done.\n3. The largest models the paper has experiments with are Llama-3-8B and Qwen-2.5-14B. It would be good to test on larger models, such as Qwen3-32B, to see if the method scales well. Since sensitivity-aware bit allocation is a key contribution, I think the paper would benefit from adding QAQ into the comparison. Have the authors done such experiments?\n\nI am open to increasing my rating if the weaknesses and questions are resolved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Gnt7BNeEt", "forum": "yjr2jX41qO", "replyto": "yjr2jX41qO", "signatures": ["ICLR.cc/2026/Conference/Submission2974/Reviewer_HDr3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2974/Reviewer_HDr3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687676711, "cdate": 1761687676711, "tmdate": 1762916473610, "mdate": 1762916473610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ChanMix, a channel-aware mixed-precision quantization framework for compressing the key-value cache to enable efficient long-context inference. It identifies asymmetric sensitivities across KV channels, categorizing them as outlier, subnormal, and retrieval-sensitive, and allocates bits adaptively (1-4 bits) to minimize quantization error while maintaining low memory usage. The framework supports 2-bit to FP8 precision, includes channel reordering for 8-bit-aligned packing, and uses custom Triton kernels for implementation (but the code is not provided). Evaluations on Llama, Mistral, and Qwen models across NIAH, RULER, and InfiniteBench benchmarks show at least 5% improvement on RULER over baselines, with 2.3× larger batch sizes and 1.5× longer contexts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's originality stems from its novel three-way channel sensitivity categorization (outlier, subnormal, retrieval) and adaptive bit allocation (4 bits for retrieval, 3 for outliers, 1 for subnormals; Figure 3, lines 290-294).\n- This assessment is based on the methodology (Section 4, lines 162-323) and claims in the abstract/introduction, emphasizing how it removes limitations from prior quantization works (e.g., uniform bits in KIVI, line 119) while the setting is very limited."}, "weaknesses": {"value": "- Code is not provided, resulting in a lack of reproducibility.\n- The experimental setup does not clearly demonstrate the effectiveness of the proposed method. In `L-323`, a group size of 32 and a residual length of 128 are mentioned. The group size is relatively small, while the residual length is large. This configuration could be used to show the method’s effectiveness under less restrictive settings.\n- The retrieval-head detection hyperparameters (n, t) are fixed “by experience.” However, stability across different values and architectures is not explored"}, "questions": {"value": "- Please address the items mentioned under Weaknesses. For example:\na. Lack of reproducibility\nb. Use of limited settings for group size and residual length\n\n- Your evaluation focuses mainly on long-context input-short-context output datasets. What is the performance of your method on long-context output tasks?\n- There is ambiguity in the “≥5% improvement” claim. The abstract states “improvements of at least 5% on RULER,” but the tables show absolute percentage-point gains compared to the baselines (e.g., +5-8 points). Please clarify whether “%” refers to percentage points or relative percent."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "itiOIQfAri", "forum": "yjr2jX41qO", "replyto": "yjr2jX41qO", "signatures": ["ICLR.cc/2026/Conference/Submission2974/Reviewer_oLua"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2974/Reviewer_oLua"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867421515, "cdate": 1761867421515, "tmdate": 1762916473331, "mdate": 1762916473331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the KV cache memory bottleneck for long-context inference. It identifies that the performance degradation of uniform low-bit quantization stems from asymmetric channel sensitivity. The authors propose ChanMix, a mixed-precision quantization framework that allocates bits based on a three-way channel classification: retrieval-sensitive channels (e.g., 4-bit), outlier channels (e.g., 3-bit), and robust/subnormal channels (e.g., 1-bit). The paper introduces an efficient one-shot method to identify these critical retrieval channels. \n\nChanMix is implemented with custom Triton kernels for efficient, 8-bit-aligned storage and dequantization. Experiments on Llama, Mistral, and Qwen show state-of-the-art results on RULER, NIAH, and InfiniteBench, significantly outperforming prior quantization methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper's core strength is the novel insight that \"retrieval-sensitive\" channels are a distinct category from magnitude-based \"outlier\" channels, and both require higher precision . This three-way sensitivity (retrieval, outlier, subnormal) is well-motivated by analysis and validated by strong ablation studies . \n- The proposed one-shot method for identifying retrieval heads is simple and efficient . The SOTA results (particularly the >5% gain on RULER) are significant. \n- I like the custom Triton kernel implementation part, which fuses channel reordering with (de)quantization, makes the method highly practical and efficient"}, "weaknesses": {"value": "- The primary weakness is the heuristic nature of the bit allocation policy (1, 2, 3, 4 bits). The paper does not provide an analysis of how this policy was derived or its optimality. \n- The generalizability of the one-time, offline channel profile is not thoroughly tested. It is unclear if a profile from Wikitext and a synthetic prompt  holds for all downstream tasks. \n- The paper asymmetrically analyzes the K cache (channel-wise) but not the V cache (token-wise), lacking justification for this design choice."}, "questions": {"value": "- How was the specific bit allocation (1, 2, 3, 4 bits) determined, and how sensitive is the model to changes in this policy?\n- How robust is the offline channel profile? Does a profile generated on Wikitext transfer to specialized domains (e.g., code, math), or is reprofiling required for optimal performance?\n- Why is the V cache not analyzed for channel-sensitivity and quantized channel-wise, similar to the K cache? Is the V cache less sensitive, or is this a design choice for simplicity?\n\nI will raise my score if authors give a good rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1prylks72S", "forum": "yjr2jX41qO", "replyto": "yjr2jX41qO", "signatures": ["ICLR.cc/2026/Conference/Submission2974/Reviewer_6Vs6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2974/Reviewer_6Vs6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895287297, "cdate": 1761895287297, "tmdate": 1762916472623, "mdate": 1762916472623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}