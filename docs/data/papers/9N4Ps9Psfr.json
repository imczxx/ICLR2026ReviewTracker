{"id": "9N4Ps9Psfr", "number": 19204, "cdate": 1758294404541, "mdate": 1759897052227, "content": {"title": "Programming with Pixels: Can Computer-Use Agents do Software Engineering?", "abstract": "Computer-use agents (CUAs) hold the promise of performing a wide variety of general tasks, but current evaluations have primarily focused on simple scenarios. \nIt therefore remains unclear whether such generalist agents can automate more sophisticated and specialized work such as software engineering (SWE). \nTo investigate this, we introduce Programming with Pixels (PwP), the first comprehensive computer-use environment for software engineering, where agents visually control an IDE to perform diverse software engineering tasks. \nTo enable holistic evaluation, we also introduce PwP-Bench, a benchmark of 15 existing and new software-engineering tasks spanning multiple modalities, programming languages, and skillsets. \nWe perform an extensive evaluation of state-of-the-art open-weight and closed-weight CUAs and find that when interacting purely visually, they perform significantly worse than specialized coding agents. \nHowever, when the same CUAs are given direct access to just two APIs—file editing and bash operations—performance jumps, often reaching the levels of specialized agents despite having a task-agnostic design. \nFurthermore, when given access to additional IDE tools via text APIs, all models show further gains. \nOur analysis shows that current CUAs fall short mainly due to limited visual grounding and the inability to take full advantage of the rich environment, leaving clear room for future improvements.\nPwP establishes software engineering as a natural domain for benchmarking whether generalist computer-use agents can reach specialist-level performance on sophisticated tasks.", "tldr": "We introduce ProgrammingwithPixels (PwP), an environment for evaluating computer use agents (CUAs) for software engineering. Our evaluations on the 15-task PwP-Bench reveals current CUA limitations and potential directions for their improvement.", "keywords": ["computer-use agents", "evaluation", "benchmark", "code-generation", "multimodal"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f41bb14f72b9e6233a0fe87fb4c895c48de1123b.pdf", "supplementary_material": "/attachment/88c6230111c94a576da7aace228d232350b1ea30.zip"}, "replies": [{"content": {"summary": {"value": "The paper benchmarks multimodal LMs on solving software engineering tasks using an IDE.\nThis PwP benchmark is built from many existing benchmarks, plus 20 tasks that have been added.\nOverall, the paper presents results based on a random sample of 300 task instances (though the full benchmark contains more than 5000).\nIt clearly demonstrates that when only given computer use controls, all models significantly underperform on almost all areas of tasks when compared to models that are given simple APIs.  The paper also makes some observations about how performance could be improved on the benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Significance: Computer use is a very relevant skill and the constructed benchmark is very challenging (with a current best score of 23% for computer-use agents) while staying close to real-world use cases. Especially if LMs were to be evaluated on the full set of > 5000 task instances, this would show a very fine-grained picture of LM skills. Because so many different benchmarks are combined, the data obtained is rich and could be analyzed even further. Overall, I expect this benchmark to become relevant as a computer-use agent benchmark especially as easier benchmarks get saturated. \n\nOriginality: The benchmark is mostly constructed from existing benchmarks, but using them to evaluate computer use is new.\n\nQuality: The paper is solidly executed and evaluating on tasks from so many different existing benchmarks, as well as running computer use agents to scale is not easy. The quantitative and qualitative observations on computer use agent behavior are interesting.\n\nClarity: Well written and easy to understand."}, "weaknesses": {"value": "I mostly had some questions about the composition of the benchmark as well as the score, I've put them all in the questions section.\n\nMinor presentation details:\n\n* p.6 official name seems to be mini-swe-agent (with dashes) (also appears completely without dashes in tab1). It might also be good to call your version mini-swe-agent multimodal or similar or clarify in tab 1 (when I first looked at tab 1, I thought this was the off-the-shelves mini-swe-agent, i.e., without any multimodal support built in)\n* Tab 1: There are two \"best numbers\" in bold in column 3 \n* Tab 1 caption should point out that these are the results on the lite split of the dataset. It would also be helpful to put `n=` for the different evaluated categories"}, "questions": {"value": "* It would be interesting to have a baseline completely without multimodal tools, for example by running an unmodified mini-swe-agent. This would be an obvious comparison for code generation & editing, but some multimodal problems can also be solved without access to screenshotting etc. (and if not this would show us the extent to which the benchmarks _require_ multimodality to be solved). For example many instances SWE-bench multimodal can definitely be solved without having the agent use screenshots for verification etc. (though including the image in the initial task instance might be required for some instances)\n* Since the pwp benchmark mixes so many very different benchmarks, understanding how the categories are made up and how the lite split is built might be main-text-worthy. Maybe just mentioning on p. 4+5 what the percentage of each of the benchmarks is in the corresponding category? Or even just some `n=` number of instances in tab 1 to understand how many instances are evaluated. \n* If I understand the appendix correctly, and the paper samples 20 instances for the lite split from each task source, then this also leads to very different numbers of tasks for the different categories (e.g., General SWE tasks only has 20 instances in total), so these instances might have a disproportionate sway over the overall avg if you take the unweighted average. It will also significantly increase the statistical uncertainty on the scores. \n* Could you show the prompts that you use for the agent in the appdx? One through-line of the paper is the comparison of task-specific software engineering agents (like mini-swe-agent) and general purpose agents. However, mini-swe-agent seems to be pretty general, too, since it doesn't have any specific tools for software engineering (other than that the prompt mentions a few strategies). So unless your prompt is much more general, I'm not sure if this generalized vs specialized narrative really is supported.\n* How can mini-swe-agent (even with screenshot support) score in some tasks of the \"General SWE task\" category? The appdendix for example gives \"installing of vscode extensions\" as an example. Can this be done from the bash?\n* Since there are only 20 tasks that are added and because they have a strong influence on the overall score that is reported, it might be worth putting a summary of each of them in the appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VAh9wgEn7i", "forum": "9N4Ps9Psfr", "replyto": "9N4Ps9Psfr", "signatures": ["ICLR.cc/2026/Conference/Submission19204/Reviewer_KWZS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19204/Reviewer_KWZS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762104459044, "cdate": 1762104459044, "tmdate": 1762931200337, "mdate": 1762931200337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new environment and benchmark to test whether general-purpose computer-use agents (CUAs) can perform complex software engineering (SWE) tasks. The authors develop Programming with Pixels (PwP), a VSCode-based environment where agents interact visually via clicks, typing, and screen observation to complete tasks such as code editing, debugging, and UI design. They also propose PwP-Bench, a 15-task benchmark covering multiple languages and modalities, to systematically assess agent performance. Through extensive evaluation, the study finds that purely visual CUAs perform far below specialized coding agents, but when granted limited text-based APIs (file editing and bash), their performance nearly doubles, approaching specialized agent levels. The analysis highlights poor visual grounding and limited tool usage as key weaknesses, while emphasizing the rapid progress and potential of CUAs for future generalist software engineering applications."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper introduces Programming with Pixels (PwP), the first comprehensive IDE-based environment that allows computer-use agents to perform real software engineering tasks through visual interaction, offering a realistic and general testbed. They proposes PwP-Bench, a well-designed and diverse benchmark spanning 15 tasks across multiple programming languages, modalities, and software engineering domains—covering code, UI, data science, and IDE configuration.\n\n- It provides clear empirical insights showing how adding minimal text-based APIs dramatically boosts agent performance, helping isolate the impact of visual grounding and tool-use limitations.\n\n- It establishes software engineering as a meaningful and economically significant domain for evaluating generalist computer-use agents, setting the stage for future research toward specialist-level generalist systems."}, "weaknesses": {"value": "- There is limited discussion on the cost, efficiency, or scalability of running PwP which are important factors in order to serve as a community standard benchmark.\n\n- Although PwP mimics a realistic IDE, the experiments do not explore longer, multi-step development workflows or collaborative settings common in real-world software engineering.\n\n- The evaluation metrics are primarily accuracy-based and may not fully capture partial progress, reasoning quality, or the richness of multi-step interaction trajectories."}, "questions": {"value": "Since file and bash APIs drastically boost performance, do the authors believe that future progress should prioritize improving visual grounding or integrating more structured textual interfaces? and how might that affect the generalist nature of computer-use agents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eadHMXttFq", "forum": "9N4Ps9Psfr", "replyto": "9N4Ps9Psfr", "signatures": ["ICLR.cc/2026/Conference/Submission19204/Reviewer_Syoz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19204/Reviewer_Syoz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123308163, "cdate": 1762123308163, "tmdate": 1762931199646, "mdate": 1762931199646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PwP, an environment for evaluating the software engineering (SWE) capabilities of computer-use agents (CUAs). It also presents PwP-Bench, a benchmark covering 15 task types, including newly proposed datasets for VSCode-use and general SWE tasks. Experimental results show that current CUAs perform poorly on SWE-related tasks. Only when a bash tool is added do they achieve performance comparable to a purely text-based mini-swe-agent that relies solely on bash operations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Evaluating CUAs on more complex tasks to comprehensively assess their capabilities is an important and valuable direction.\n\n- The paper aggregates a wide range of existing datasets and adds two new SWE-related task sets, resulting in a large and diverse benchmark."}, "weaknesses": {"value": "- The motivation is somewhat confusing. General-purpose CUAs are primarily designed for everyday computer-use tasks, such as web navigation or basic UI operations. While CUAs **can** perform SWE tasks, that doesn’t necessarily mean they **should** be expected to do well on SWE. This feels similar to asking *whether an image generation model can produce good videos*—theoretically possible, but not aligned with its intended design or primary evaluation purpose.\n\n- I also find it difficult to identify the essential difference between PwP-Bench and SWE-Bench (-MM). If the goal is to test SWE performance, then a simple bash environment with read, write, and execute (-rwx) capabilities is sufficient (which is also the design strategy of Mini-SWE-Agent). Notably, code is inherently *textual*. Even web front-end layouts are generated from *textual style configs*. If one wants to expose the model to rendered visual outputs, webpage-based inputs (as used in SWE-Bench-MM) already achieve this. Could the authors clarify what is the core different between using webpages as vision input a  using IDE screenshots? Furthermore, everything visible in an IDE can already be accessed in text form, without introducing noise or visual ambiguity. Thus, if the goal is to evaluate SWE ability, feeding noisy visual input to the agent seems counterproductive. PwP-Bench assumes that the agent must read from IDE images,  which introduces an artificial constraint that inherently weakens any agent’s performance on SWE tasks.\n\n- Moreover, the authors seem aware of the limitation of solely using visual input for SWE: They add a bash tool to CUAs, and indeed, Claude-Sonnet-4.0 with bash achieves the best performance—nearly identical to Claude-Sonnet-4.0 Mini-SWE-Agent. Again, it is a very, very small agents with only **100 lines** of code and **only one simple bash tool**. This strongly suggests that SWE tasks are simply not well-suited for general-purpose CUAs. Even a minimal agent—an LLM with just a bash tool—can match or outperform them.\nThis further reinforces the view that CUAs are not suited for solving SWE tasks, thereby undermining the claimed contribution of this work.\n\nI agree that CUAs should be tested on more complex tasks, but SWE may not be the right choice. Since CUAs’ key advantage lies in their visual interaction capabilities, it might make more sense to evaluate them on tasks that genuinely depend on visual understanding, rather than on code-centric problems that are mostly text-based."}, "questions": {"value": "- What is the fundamental difference between using IDE images as input in PwP-Bench and using webpages as input in SWE-Bench-MM?\n- Is a CUA with bash tools still a general-purpose CUA? Does it become a SWE-specialized CUA or even a vision-enabled SWE agent?What exactly are we evaluating here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vpwvgqAsVV", "forum": "9N4Ps9Psfr", "replyto": "9N4Ps9Psfr", "signatures": ["ICLR.cc/2026/Conference/Submission19204/Reviewer_5ZwW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19204/Reviewer_5ZwW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762702587805, "cdate": 1762702587805, "tmdate": 1762931199136, "mdate": 1762931199136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Following are the key components of the research article, although there are issues with some of the sections that will be listed later.\n\nProblem Statement:\n- The problem statement is clearly articulated in the Introduction (Page 1-2):\n- Primary question: \"can general-purpose computer-use agents match specialized agents in complex domains like software engineering?\"\n- Motivation: Current evaluations of computer-use agents focus on simple tasks, leaving unclear whether they can handle sophisticated tasks like software engineering. \n\nState of the Art:\n- Related work is covered in Section 2 (Page 2-3):\n- Comprehensive coverage of multimodal and computer-use agents\n- Software engineering agents literature\n- Comparison with existing environments and benchmarks\n\nMethod Explanation\n- The methods are explained in Section 3 (Pages 3-5):\n- Programming with Pixels (PwP) environment as a POMDP\n- PwP-Bench benchmark with 15 tasks across multiple categories\n- Evaluation methodology clearly described\n\nResults Presentation\n- Results are presented in Section 5 (Pages 5-11):\n- Table 1 shows comprehensive performance evaluation\n- Clear comparison between different agent types\n- Detailed analysis of limitations and capabilities\n\nClaims vs Results\n- The abstract makes specific claims that are supported by results:\n- Claim: \"pure visual interaction performs significantly worse\" -> Supported by 22.9% vs 48.8% performance\n- Claim: \"file editing and bash operations dramatically improve performance\" -> Supported by 50.7% accuracy with APIs\n- Claim: \"clear room for future improvements\" -> Supported by visual grounding analysis and improvement potential demonstrated\n\n\nThe authors systematically address whether computer-use agents can handle software engineering by creating a comprehensive IDE environment that captures real software engineering complexity. Then they evaluate across diverse tasks spanning 15 different scenarios, which provides fair comparison with specialized agents through the same environment. Authors state that no prior comprehensive environment existed for this domain. While individual components exist (computer-use agents, SWE benchmarks), their combination is novel. Table-3 mentions which areas were covered by other works as compared to this work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The PwP environment addresses a critical gap in evaluating computer-use agents for software engineering tasks. By providing a realistic IDE interface through VSCode, the environment enables comprehensive testing of agent capabilities while maintaining the expressiveness needed for diverse software engineering activities. The extensible design, with features like checkpointing, multimodal support, and easy benchmark addition, positions it as a lasting contribution to the research community.\n\nThe study maintains fidelity to established benchmarks by using programmatic verifiers and metric calculations that match the original dataset specifications. This approach ensures meaningful comparisons with prior work and establishes a reliable baseline for future research, while the 15-task PwP-Bench provides comprehensive coverage of software engineering activities.\n\nThe combination of detailed performance tables with concrete examples of agent behavior (such as visual grounding errors, successful tool usage, and failure modes) provides a balanced perspective. This multi-faceted analysis helps explain why certain performance patterns emerge, making the results actionable for both understanding current limitations and guiding future improvements in computer-use agent design."}, "weaknesses": {"value": "The following are key issues identified in the work\n\n- The reliance on PwP-Bench-Lite (300 instances) due to computational constraints creates potential sampling bias, potentially missing important edge cases that would emerge in full-scale evaluation. The 20-step limitation, while computationally necessary, may artificially constrain performance on genuinely complex software engineering tasks, though the supplementary 250-step experiments provide partial validation.\n\n- The heavy focus on Claude-based agents (3.5, 3.7, 4.0) introduces anthropocentric bias, as these models may share architectural similarities that don't represent the broader landscape of computer-use agents. Additionally, evaluating only VSCode limits generalizability across the diverse IDE ecosystem used by developers worldwide.\n\n- The inconsistent file editing capabilities across models and the limited investigation into API-driven improvements suggest insufficient mechanistic understanding of why certain interventions succeed. The study documents performance differences but lacks deeper analysis of the underlying factors driving these disparities, limiting its prescriptive value for future agent development."}, "questions": {"value": "The following recommendations address critical gaps in statistical rigor and model diversity that would significantly strengthen the study's scientific validity:\n\n- The current study's minimal statistical testing (only one p-value reported) represents a fundamental weakness for a comprehensive benchmarking effort. How the p-value is derived is also missing. Implementing proper statistical methodology with confidence intervals, effect sizes, and multiple comparison corrections would transform the findings from descriptive observations to statistically significant conclusions. Power analysis would ensure the 300-instance sample size is adequate for detecting meaningful differences, while systematic statistical testing across all 15 datasets would provide robust evidence for claimed performance gaps.\n\n- The absence of mixture-of-experts models represents a significant oversight, as these architectures are increasingly important in the LLM landscape and may exhibit different performance characteristics on computer-use tasks. Expanding beyond Claude-centric evaluation to include these diverse model architectures would provide crucial insights into whether performance differences stem from fundamental model design choices rather than implementation details. This broader coverage would make the benchmark more representative of the current AI landscape and more valuable for future research direction.\n\n\nThe following are a few minor issues identified in the article\n- Page 5: \"evluate\" should be \"evaluate\"\n- Page 11: \"intutively\" should be \"intuitively\" \n- Page 13: \"potetntially\" should be \"potentially\"\n- Page 21 (and some other places): \"os\" should be \"OS\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WyiUBLRRFM", "forum": "9N4Ps9Psfr", "replyto": "9N4Ps9Psfr", "signatures": ["ICLR.cc/2026/Conference/Submission19204/Reviewer_pzk4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19204/Reviewer_pzk4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762760406824, "cdate": 1762760406824, "tmdate": 1762931198730, "mdate": 1762931198730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an environment to approach software engineering using a visual interface. The visual interface consists of screenshots, sometimes enhanced by the set-of-mark technique, which provides a list of elements in addition to the screenshot. They provide an evaluation benchmark consisting of 15 tasks with an OS and a VSCode-like IDE. The VSCode IDE is changed so that it supports the set-of-mark technique and gives the DOM tree.\n\nThey then conducted a comprehensive evaluation of different computer-use agents and found that they perform poorly on this benchmark using only visual inputs and actions like 'type' and 'click'. Then they added two functionalities that boost the performance of agents. They gave them text-based terminal access and text-based file editing tools. These two additional text-based abilities boosted the performance of agents. They also investigated and saw that on some tasks the agents mostly use the text-based interface, while on others they only use the visual interface.\n\nThe paper also investigates why pure visual agents fail, identifying two main issues. The first and primary weakness is poor visual grounding. The second issue is the agent's inability to leverage sophisticated IDE features, such as debuggers or profilers, even when these tools could simplify the task."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The provided benchmark is very valuable, and its container-based approach makes evaluations consistent.\n2. The provided VSCode IDE that returns screenshots with set-of-marks and DOM is valuable and can be used in future research.\n3. The evaluation of various computer-use agents is comprehensive.\n4. The ablation studies are also comprehensive, and the results on how much the text-based interface vs. the visual interface is used are very interesting."}, "weaknesses": {"value": "1. To measure the importance of the visual interface, it would be interesting to measure the performance when agents only have access to text-based tools (file edit, terminal, and task-specific tools). It would show the computer-use agents' ability in text-only settings.\n2. Some tasks aren’t relevant to agentic software development. For example, changing the theme of the IDE is not that important, while similar IDE setting changes, like turning on auto-suggestion, could be helpful. \n3. Some deeper insights into which IDE features agents are using would be very beneficial to add to the paper. For example, can the paper provide a number on whether the agent uses debugging tools, or if it ever did? The framework would benefit if these insights are generated after running tests, which would help those developing agents using this benchmark."}, "questions": {"value": "1. The testing environment needs more clarification on its safeguards, as it has been seen that LLMs might simply remove tests if they have access to those files. Do they have access to them through bash, file editing, or the visual interface, in theory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dTpMKFU4xZ", "forum": "9N4Ps9Psfr", "replyto": "9N4Ps9Psfr", "signatures": ["ICLR.cc/2026/Conference/Submission19204/Reviewer_xKNb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19204/Reviewer_xKNb"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762803779217, "cdate": 1762803779217, "tmdate": 1762931198320, "mdate": 1762931198320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Programming with Pixels (PwP), the first IDE-based environment and benchmark for evaluating computer-use agents (CUAs) on software-engineering tasks. PwP enables agents to control a VSCode IDE visually through keyboard and mouse, while PwP-Bench unifies 15 multi-language, multimodal SWE tasks. The authors systematically compare pure visual CUAs, CUAs with minimal text APIs, and specialized SWE agents. Results show that visual CUAs perform poorly (22.9%) but reach near-specialist performance (50.7%) when given file-edit and bash APIs. PwP thus establishes software engineering as a fair, comprehensive testbed for generalist computer-use agents."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Addresses an important intersection between two major agent paradigms — IDE-based (CUA) and API-based (SWE) — by providing a unified testbed for fair comparison.\n\nNovel environment design: the first fully interactive VSCode-based setup enabling realistic software-engineering evaluation.\n\nComprehensive benchmark: integrates 15 diverse SWE tasks across modalities and languages.\n\nInsightful analysis: identifies visual grounding and tool-usage limitations as key failure modes, offering concrete directions for improvement."}, "weaknesses": {"value": "Only one specialized SWE baseline (Mini-SWEAgent) is included; broader baselines would strengthen comparisons.\n\nConceptual clarity: definitions of computer-use agent vs SWE agent and the rationale for “fair evaluation” (why expressiveness and access matter) could be better explained.\n\nPwP-Bench-Lite covers many task types but has very few instances (20 per task), and the paper reports no repeated trials or variance, raising robustness concerns.\n\nTechnically, the benchmark adapts existing datasets rather than introducing new ones."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SOmIVrYRM8", "forum": "9N4Ps9Psfr", "replyto": "9N4Ps9Psfr", "signatures": ["ICLR.cc/2026/Conference/Submission19204/Reviewer_f9o7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19204/Reviewer_f9o7"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission19204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762814412844, "cdate": 1762814412844, "tmdate": 1762931197910, "mdate": 1762931197910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}