{"id": "eVfSdwyOKF", "number": 18960, "cdate": 1758292392473, "mdate": 1759897070576, "content": {"title": "ForceForget: Reinforcement Concept Removal for Enhancing Safety in Text-to-Image Models", "abstract": "With the advance of generative AI, the text-to-image (T2I) model has the ability to generate various contents. However, T2I models still can generate unsafe contents. To alleviate this issue, various concept erasing methods are proposed. However, existing methods tend to excessively erase unsafe concepts and suppress benign concepts contained in harmful prompts, which can negatively affect model utility. In this paper, we focus on eliminating unsafe content while maintaining model capability in safe semantic meaning interpretation by optimizing the concept erasing reward (CER) with reinforcement learning. To avoid overly content erasure, we introduce the safe adapter to project partial text embedding for efficient concept regulation in cross-attention layers. Extensive experiments conducted on different datasets demonstrate the effectiveness of the proposed method in alleviating unsafe content generation while preserving the high fidelity of benign images compared with existing state-of-the-art (SOTA) concept erasing methods. In terms of robustness, our method outperforms counterparts against red-teaming tools. Moreover, we showcase the proposed approach is more effective in emerging image-to-image (I2I) scenario compared with others. Lastly, we extend our method to erase general concepts, such as artistic styles and objects. Disclaimer: This paper includes discussions of sexually explicit content that may be offensive to certain readers.", "tldr": "", "keywords": ["Text-to-Image Model", "Sexually Explicit", "Unsafe Mitigation", "Concept Erasing", "Image-to-Image Model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9276b3c4dec121df691dfaacda0e5f729701a517.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents ForceForget, an RL-based framework for concept erasure aimed at improving the safety of text-to-image diffusion models. The authors argue that existing erasure methods often over-remove unsafe concepts, degrading the model’s ability to generate safe and semantically meaningful content. ForceForget introduces two main components: (1) Concept Erasing Reward: a dynamic reward combining a safety term (via an NSFW classifier) and an alignment term (via CLIP/BLIP); (2) Safe Adapter: a lightweight module inserted into the cross-attention layers that modifies only part of the text token projections (K/V) to selectively suppress unsafe semantics. The system fine-tunes LoRA parameters via policy-gradient RL to maximize CER. Experiments on tasks such as nudity removal, copyright/object erasure, and I2I transfer show superior safety, fidelity, and robustness compared to existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty: This work applies reinforcement learning to concept erasure in diffusion models, formulating the task as reward optimization rather than direct weight editing.\n2. Comprehensive experiments: Cover multiple tasks (T2I, I2I, artistic style/object erasure) and red-teaming robustness, demonstrating consistent improvements across metrics."}, "weaknesses": {"value": "1. Token selection mechanism for Safe Adapter is unclear: The paper mentions regulating “partial text embeddings (e.g., 4 tokens)” but does not specify how these tokens are selected (keyword matching, attention saliency, or learned gating). This limits reproducibility and interpretability.\n2. Reward design risks reward hacking: The CER relies exclusively on a single NSFW classifier to measure “safety,” which may cause the model to overfit the specific classifier’s decision boundary rather than learning a genuinely safe distribution. This raises the risk of “reward hacking,” where the model learns to exploit the evaluator rather than internalize safety.\n3. Limited multi-concept evaluation: The paper primarily focuses on removing a single unsafe concept (e.g., nudity), with only minor qualitative extensions to other domains (e.g., artistic style). It remains unclear how the method performs when multiple harmful or sensitive concepts coexist (e.g., nudity + violence) or when the erasure targets interact."}, "questions": {"value": "1. How are the “partial tokens” chosen for the Safe Adapter? Is the selection rule fixed (e.g., top-k, keyword-based), dynamic, or learned during training?\n2. The CER’s safety reward is based on a single NSFW classifier (Chhabra, 2020). Would the performance remain consistent under different safety evaluators (e.g., NudeNet or VLM-based filters)?\n3. How are the two CER components (safety and alignment rewards) balanced? Are their weights static or dynamically adjusted during RL training?\n4. Can ForceForget handle multiple unsafe concepts (e.g., nudity + violence + hate symbols) simultaneously? How does it avoid gradient interference or conflicting suppression?\n5. What is the computational overhead compared to baseline methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ktBiKeAWJx", "forum": "eVfSdwyOKF", "replyto": "eVfSdwyOKF", "signatures": ["ICLR.cc/2026/Conference/Submission18960/Reviewer_FDCS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18960/Reviewer_FDCS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761451638853, "cdate": 1761451638853, "tmdate": 1762931012639, "mdate": 1762931012639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ForceForget, a novel approach for concept erasure in text-to-image (T2I) diffusion models. Instead of standard supervised fine-tuning, the authors formulate concept removal as a reinforcement learning (RL) problem. The core idea is to optimize a Concept Erasing Reward (CER) that balances safety (nudity avoidance) and semantic alignment (retaining safe, human-relevant content).\nTo further improve erasure effectiveness and prevent over-suppression, the authors introduce a Safe Adapter that modifies only a partial subset of token projections in the cross-attention layers. The method is evaluated on a wide spectrum of tasks: explicit/implicit nudity removal, art style erasure, object removal, I2I transferability, and human identity retention. Extensive experiments demonstrate superior performance compared to ~10 SOTA baselines in terms of safety, fidelity, robustness, and generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The use of reinforcement learning with a carefully designed reward function (CER) is a significant methodological contribution over existing supervised fine-tuning or closed-form solutions.\n2. The method is tested on a wide variety of tasks (T2I, I2I, artistic style, object removal, red-teaming attacks). The quantitative results are comprehensive and clearly demonstrate the method’s superiority.\n3. The paper explicitly tackles a major challenge in concept erasure: benign content preservation, and proposes a balanced solution via reward engineering and Safe Adapter."}, "weaknesses": {"value": "1. The design of the Safe Adapter is heuristic and under-analyzed. It applies modifications to a fixed number of tokens (e.g., 4), but the paper does not justify this choice or investigate the sensitivity of performance to the number or type of tokens selected.\n2. The reward function combines safety and alignment components using manually tuned weights (e.g., $\\alpha$ and $\\beta$), but the paper lacks a thorough analysis of how these weights affect the trade-off between erasure strength and content preservation.\n3. The alignment reward relies heavily on CLIP similarity, which may introduce bias or instability[1]. CLIP is known to exhibit limitations in distinguishing between nuanced content like artistic nudity versus explicit imagery, and can be influenced by non-semantic visual features.\n\n[1] LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders"}, "questions": {"value": "1. What's the fundamental difference between your reward and just using NSFW scores(e.g., scores from NudeNet) in a supervised way?\n2. All experiments are conducted on Stable Diffusion v1.4 and v1.5.Can it generalize to more modern architectures such as SDXL or Diffusion Transformers (DiT)?\n3. In the image-to-image (I2I) setting, the input image provides a strong conditioning signal that may conflict with the reward objectives. I wonder how the reward function, particularly the safety component, accounts for such priors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mrUmN7ANUP", "forum": "eVfSdwyOKF", "replyto": "eVfSdwyOKF", "signatures": ["ICLR.cc/2026/Conference/Submission18960/Reviewer_1zae"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18960/Reviewer_1zae"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967226557, "cdate": 1761967226557, "tmdate": 1762931012110, "mdate": 1762931012110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposed a novel concept erasing framework called ForceForget that utilizes a carefully designed erasing reward and safety adapter. The experiments show that the proposed method achieves the highest nudity-removal rate and robustness against Ring-A-Bell, P4D, and MMA attacks, while retaining competitive FID and CLIP scores. This work also shows that the proposed method can extend to erase general concepts including artistic styles and objects to show the generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed Concept Erasing Reward elegantly integrates a safety reward (driven by an NSFW classifier) and an alignment reward (via CLIP–BLIP caption consistency). \n\n2. The experiment section is comprehensive. Benchmarks cover T2I and I2I, explicit and implicit NSFW prompts, red-teaming attacks, and even artistic-style and object erasure—providing a wide empirical view.\n\n3. The proposed safe cross attention change is novel and effective."}, "weaknesses": {"value": "1. The effectiveness of the method appears to depend strongly on the relative weighting between the safety and alignment rewards, yet the paper does not include an ablation or sensitivity study on these hyperparameters. It remains unclear how robust the approach is to different reward weightings or whether the same configuration generalizes across concepts.\n\n2. Reinforcement fine-tuning of diffusion models can be resource-intensive compared to closed-form or lightweight LoRA-based erasure methods. However, the paper does not report training time, GPU hours, or efficiency comparisons, making it difficult to assess the practicality of the proposed approach."}, "questions": {"value": "1. Could the authors provide an ablation study or sensitivity analysis on the relative weighting between the safety reward and alignment reward in the Concept Erasing Reward (CER)? This would clarify how robust the method is to reward scaling and whether the same configuration generalizes across different unsafe concepts.\n\n2. Could the authors report the training overhead (e.g., GPU hours, iterations, or wall-clock time) and compare it with other baseline methods to assess the practical efficiency of the reinforcement fine-tuning approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HeUph5ifN9", "forum": "eVfSdwyOKF", "replyto": "eVfSdwyOKF", "signatures": ["ICLR.cc/2026/Conference/Submission18960/Reviewer_FwRB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18960/Reviewer_FwRB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762231410507, "cdate": 1762231410507, "tmdate": 1762931011663, "mdate": 1762931011663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}