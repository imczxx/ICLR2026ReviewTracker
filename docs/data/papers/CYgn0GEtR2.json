{"id": "CYgn0GEtR2", "number": 19845, "cdate": 1758299912126, "mdate": 1763421460882, "content": {"title": "Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver", "abstract": "Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regimes. We also investigate its connections with other architectural primitives, such as the Fourier neural operator layer and convolutional layers. We experimentally validate the data efficiency of Neural-HSS on the three-dimensional Poisson equation over a grid of two million points, demonstrating its superior ability to learn from data generated by elliptic PDEs in the low-data regime while outperforming baseline methods. Finally, we demonstrate its capability to learn from data arising from a broad class of PDEs in diverse domains, including electromagnetism, fluid dynamics, and biology.", "tldr": "", "keywords": ["Hierarchical", "low-rank", "HSS"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa29262841c8413acef804160f48036fd0f2d8c4.pdf", "supplementary_material": "/attachment/685e50037804dc983f916b544e5e66f7789f23c8.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Neural-HSS, an efficient neural operator based on the Hierarchical Semi-Separable structure for learning solution operators of PDEs. Leveraging the low-rank property of the Green’s function in elliptic PDEs, Neural-HSS embeds the HSS structure into the network to jointly model local and long-range interactions. This design achieves superior performance in terms of parameter efficiency, data efficiency, and computational cost. Theoretically, the authors establish the approximation and exact recovery guarantees of Neural-HSS; experimentally, the model demonstrates outstanding data efficiency and scalability across multidimensional tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is strong in both conceptual innovation and methodological rigor. It introduces a well-motivated and theoretically grounded neural operator architecture that effectively integrates structural priors from numerical analysis into deep learning. The work stands out for its clear theoretical foundations, demonstrating provable guarantees, and for its strong empirical performance, showing consistent efficiency and scalability across diverse PDE tasks."}, "weaknesses": {"value": "The paper lacks detailed ablation or interpretability studies to elucidate how specific HSS components contribute to overall performance, leaving uncertainty about which architectural factors are most critical. Moreover, the experimental comparison focuses primarily on FNO, ResNet, and DeepONet, without including more recent neural operator baselines, which somewhat limits the empirical breadth and generality of the conclusions."}, "questions": {"value": "1. The comparison focuses mainly on FNO(2020), ResNet(2016), and DeepONet(2019). Have the authors considered including more recent neural operator architectures to strengthen the empirical validity of the conclusions?\n2. The paper would benefit from a more detailed ablation study. Could the authors analyze how different components of the HSS structure affect model performance and efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pKZNqaWnYt", "forum": "CYgn0GEtR2", "replyto": "CYgn0GEtR2", "signatures": ["ICLR.cc/2026/Conference/Submission19845/Reviewer_xoXJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19845/Reviewer_xoXJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760891908567, "cdate": 1760891908567, "tmdate": 1762932017588, "mdate": 1762932017588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript proposed the Neural-HSS framework, which is a neural operator framework integrated with the Hierarchically Semi-Separable (HSS) matrix factorization. The method employs a neural encoder-decoder that captures multilevel block interactions, followed by low-rank coupling networks that replace analytical low-rank factorization routines, and a recursive HSS-based forward solver that ensures efficient inversion and matrix-vector multiplication. The network supports both supervised training and self-supervised training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Theoretical novelty**: The idea of learning HSS-style hierarchical structures within NNs is insightful. \n- **Interpretable architecture**: Each network component has a clear algebraic analog."}, "weaknesses": {"value": "- **Restricted system matrix formulations**: All tests use symmetric positive-definite or weakly oscillatory kernels. Whether the proposed methods work on highly indefinite or ill-conditioned systems (e.g., high-frequency Helmholtz problems) is unclear.\n- **Insufficient baselines**: Comparisons with legacy solvers, e.g., legacy GMERS (with or without learned preconditioners), NVIDIA AmgX, etc., are missing. Without a significant performance boost over legacy routines, it remains questionable why we'd take a neural method. \n- **Overclaimed complexity**: The claim of “O(N log N)” complexity is empirically observed but not theoretically proven. \n- **No inference efficiency comparison**: Only the \"forward + backward\" time is evaluated. However, in practice, the training time is amortized, and the inference efficiency is the true speed metric. However, this metric is missing."}, "questions": {"value": "- Please refer to the \"Weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aM7RgIk8yM", "forum": "CYgn0GEtR2", "replyto": "CYgn0GEtR2", "signatures": ["ICLR.cc/2026/Conference/Submission19845/Reviewer_NhmF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19845/Reviewer_NhmF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760932392572, "cdate": 1760932392572, "tmdate": 1762932016973, "mdate": 1762932016973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript proposed the Neural-HSS framework, which is a neural operator framework integrated with the Hierarchically Semi-Separable (HSS) matrix factorization. The method employs a neural encoder-decoder that captures multilevel block interactions, followed by low-rank coupling networks that replace analytical low-rank factorization routines, and a recursive HSS-based forward solver that ensures efficient inversion and matrix-vector multiplication. The network supports both supervised training and self-supervised training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Theoretical novelty**: The idea of learning HSS-style hierarchical structures within NNs is insightful. \n- **Interpretable architecture**: Each network component has a clear algebraic analog."}, "weaknesses": {"value": "- **Restricted system matrix formulations**: All tests use symmetric positive-definite or weakly oscillatory kernels. Whether the proposed methods work on highly indefinite or ill-conditioned systems (e.g., high-frequency Helmholtz problems) is unclear. **Update**: I appreciate the authors' pointers to Tables 1-2. Regarding elliptic problems, where the lack of system matrix definiteness of matrices calls for extra concern, the quantitative results on Helmholtz problems address my concerns. \n- **Insufficient baselines**: Comparisons with legacy solvers, e.g., legacy GMERS (with or without learned preconditioners), NVIDIA AmgX, etc., are missing. Without a significant performance boost over legacy routines, it remains questionable why we'd take a neural method. **Update**: This issue is resolved by the additional quantitative results. \n- **Overclaimed complexity**: The claim of “O(N log N)” complexity is empirically observed but not theoretically proven. **Update**: The \"n log n\" here is a typo. The authors have provided pointers to this issue, and this concern is resolved. \n- **No inference efficiency comparison**: Only the \"forward + backward\" time is evaluated. However, in practice, the training time is amortized, and the inference efficiency is the true speed metric. However, this metric is missing. **Update**: Pointers to the appendix are provided; thanks for the authors."}, "questions": {"value": "- Please refer to the \"Weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aM7RgIk8yM", "forum": "CYgn0GEtR2", "replyto": "CYgn0GEtR2", "signatures": ["ICLR.cc/2026/Conference/Submission19845/Reviewer_NhmF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19845/Reviewer_NhmF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760932392572, "cdate": 1760932392572, "tmdate": 1763420868737, "mdate": 1763420868737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Inspired by the semi low rank property of the Green's function of elliptic PDE, the paper presents a deep learning framework to imitate the semi low rank Green's function for solving elliptic PDE. Experiments on various types of equations, such as Poisson, Gray–Scott, Navier-Stokes, etc, valid the effectiveness of the approach."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Theorems 2.2 and 2.3 provide the solid theoretical foundation, proving that convolutional kernel can be approximated by HSS and exact recovery and data-efficiency.\n\nThe experiments are comprehensive testing several types of PDEs and compare with the SOTA methods."}, "weaknesses": {"value": "It seems that the method only support Dirichlet boundary condition. Can it support non-zero Dirichlet boundary conditions?\n\nThe method only supports elliptic PDEs. The Green's functions of other PDEs are not semi low rank, so it is hard to be extended to other PDEs."}, "questions": {"value": "Is it possible to support Neumann boundary conditions?\n\nAre there failed cases, where the solver doesn't converge?\n\nIt it supports non-rectangular domain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ShZl4NcPiN", "forum": "CYgn0GEtR2", "replyto": "CYgn0GEtR2", "signatures": ["ICLR.cc/2026/Conference/Submission19845/Reviewer_g8X8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19845/Reviewer_g8X8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575254137, "cdate": 1761575254137, "tmdate": 1762932016504, "mdate": 1762932016504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SchulzNN, a three-layer linear network that exactly mimics one step of the classical Schulz iteration for matrix inversion. With a fixed middle A-layer, the forward map is\n  $$\n  \\hat{x}=2W_1 b - W_3 A W_1 b,\n  $$\n  which, under $W_1=W_3=A_0$, equals the single-step update $(2I-A_0A)\\,A_0 b$. Training minimizes an unsupervised residual such as\n  $$\n  \\mathcal{L}=\\frac{1}{m}\\sum_{i=1}^{m}\\frac{\\|A\\hat{x}_i-b_i\\|}{\\|b_i\\|},\n  $$\n  and stacking $k$ blocks (SchulzNN$_k$) emulates $k$ Schulz steps. When $A$ admits an IDBF factorization, mat–vecs are $O(N\\log N)$ and per-epoch training scales as $O(N^2\\log N)$. Experiments on strictly diagonally dominant, permutation, discrete Helmholtz, and perturbed-identity matrices show that $k=3$ attains Helmholtz accuracy around $10^{-4}$ (e.g., $\\epsilon_{\\text{inv}}\\approx1.16\\times10^{-4}$, $\\epsilon_{\\text{sub}}\\approx7.9\\times10^{-5}$), and that fine-tuning adapts to moderate perturbations at $\\sim10^{-3}$ while failing beyond that regime."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The single block is algebraically identical to one Schulz step, i.e., $(2I-A_0A)A_0b$, grounding the design in a classical fixed-point map rather than heuristics.\nThe residual objective avoids needing $A^{-1}$; depth clearly helps on hard spectra: for a discrete Helmholtz matrix, $k=3$ reaches $\\epsilon_{\\text{inv}}\\approx1.16\\times10^{-4}$ while $k\\le2$ underperforms. The perturbation protocol—$A+\\varepsilon I$ and $A+R$ with $r_{ij}\\sim U(-\\varepsilon,\\varepsilon)$—demonstrates adaptation down to $\\epsilon_{\\text{inv}}\\sim10^{-3}$ within stated ranges.\n The paper cleanly specifies $\\hat{x}$, the loss $\\mathcal{L}$, and the recursion SchulzNN$_k$, and makes the role of the fixed A-layer explicit; metrics $\\epsilon_{\\text{inv}}$ and $\\epsilon_{\\text{sub}}=\\|AA_d^{-1}-I\\|/\\|I\\|$ are well-motivated.\n In fixed-$A$, many-$b$ regimes, the learned operator is a plausible approximate inverse / preconditioner surrogate; with IDBF, the $O(N^2\\log N)$ per-epoch cost is attractive relative to naive dense inversion."}, "weaknesses": {"value": "The method is trained per matrix $A$, and the paper does not examine whether a model generalizes across a family of matrices.  \n There are no guarantees for the trained deep composition, and the threshold between successful and failed fine-tuning is only reported empirically.  \n  When used as a preconditioner, application-level comparisons in terms of wall-clock time and iteration counts against strong baselines are not provided.  \n  Efficiency claims rely on IDBF-type hierarchical low-rank structure; behavior on dense, non-hierarchical $A$ remains unspecified."}, "questions": {"value": "1. Cross-$A$ generalization: would results hold on a parametric family $A(\\theta)$ (e.g., elliptic PDEs with varying coefficients) with train/validation/test splits across $\\theta$? Please report $\\epsilon_{\\text{sub}}=\\|AA_d^{-1}-I\\|/\\|I\\|$ and right-hand-side OOD stress.  \n2. Trained-model guarantees: can you provide a spectral bound tying depth $k$ and an effective initializer $A_0^\\star$ such that $\\rho(I-AA_0^\\star)<1$, or characterize adaptation success using $\\|\\Delta A\\|$, $\\|A^{-1}\\Delta A\\|$, or eigenvalue drift?  \n3.  Preconditioning evidence: with $A_d^{-1}=(2I-W_3A)W_1$ as a left/right preconditioner in CG/MINRES/GMRES on Poisson/Helmholtz (Dirichlet/Neumann/Robin), how do iteration counts and wall-clock compare to AMG/geometric MG for $N=2^{10}\\text{–}2^{16}$?  \n4. Non-IDBF regime: on dense synthetic $A$ with controlled condition numbers, how does total time (training+inference) compare to IC(0)+CG (SPD) or ILU+GMRES (indefinite) to reach $\\epsilon_{\\text{sub}}\\le10^{-3}$?  \n 5. Depth effect: why does $k=3$ succeed on Helmholtz while $k\\le2$ fails—can this be linked to entering the quadratic-convergence basin of Schulz for an implicit $A_0^\\star$ encoded by $(W_1,W_3)$?\n\n-"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Potentially harmful insights, methodologies and applications", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)", "Yes, Research integrity issues (e.g., plagiarism, dual submission)", "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)", "Yes, Other reasons (please specify below)"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DsVasyvb3F", "forum": "CYgn0GEtR2", "replyto": "CYgn0GEtR2", "signatures": ["ICLR.cc/2026/Conference/Submission19845/Reviewer_xDzu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19845/Reviewer_xDzu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981569401, "cdate": 1761981569401, "tmdate": 1762932016061, "mdate": 1762932016061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}