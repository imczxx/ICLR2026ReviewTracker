{"id": "LEWQk3j73j", "number": 17529, "cdate": 1758277223753, "mdate": 1759897168863, "content": {"title": "BESTI: BAYESIAN CLASS-WISE TRUST-WEIGHTED ENSEMBLE WITH STRUCTURED SAMPLING FOR IMBALANCED MULTI-CLASS CLASSIFICATION", "abstract": "Class imbalance is caused by both practical and structural reasons, such as variance in occurrence frequency, biases in collection environments, differences in labeling costs, and imbalances in conceptual definitions. Such an imbalance introduces diverse problems, including under-representation of minority classes, distortion of metrics, and deterioration of model fairness and generalization capabilities. To challenge this issue, we propose a Bayesian Class-wise Trust-Weighted Ensemble with Structured Sampling for Imbalanced Multi-Class Classification, named BESTI. BESTI starts by constructing multiple sub-training sets from the original dataset that represent varying degrees of data imbalance. By initializing clipping thresholds in a structured manner, classes with larger sample counts than thresholds are downsampled, and others are retained. This creates a series of training sets, each reflecting a different class distribution. After, independent models are trained from each of the training sets, generating multiple specialized models for a certain degree of imbalance. We aggregate these models taking their trustworthiness into account. Based on Bayes’ theorem, this trustworthiness is equivalent to the class-wise precision of the model. Utilizing this precision as a weight, BESTI ensembles multiple models to make the final decision. Our test results show that BESTI successfully improves the overall performance of the model, including the minority classes. In addition to that, BESTI shows competitiveness compared to state-of-the-art methods, often outperforming them significantly in certain domains.", "tldr": "", "keywords": ["Data Imbalance", "Class Imbalance", "Ensemble", "Classification"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/166815ec89b4f2fc8de2159f54fc0a98e2c06f96.pdf", "supplementary_material": "/attachment/bc2c983de720a5f4e168811a4d9b7baeef3f2d10.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes BESTI — a Bayesian Class-wise Trust-Weighted Ensemble framework with structured sampling for imbalanced multi-class classification. The method generates multiple sub-training sets with different imbalance levels via threshold-based clipping, trains a separate model for each subset, and aggregates predictions based on class-wise precision interpreted as a Bayesian trust score. Experiments on CIFAR100-LT, ImageNet-LT, and Places-LT claim the improvement of the proposed framework BESTI over a basic CE baseline and competitive performance against existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) This work introduces a structured sampling scheme that generates multiple sub-training datasets with varying imbalance levels, offering a novel perspective compared to conventional single-set resampling approaches.\n\n(2) It provides a Bayesian interpretation of class-wise precision as trust, giving a theoretically grounded weighting mechanism in ensemble learning.\n\n(3) The methodology is clearly stated, with intuitive motivation and mathematical formalization."}, "weaknesses": {"value": "(1) The idea of generating multiple resampled subsets and ensemble models has been widely explored. The key novelty (Bayesian weighting using precision) is conceptually simple and may be seen as a straightforward reinterpretation rather than a fundamentally new algorithmic contribution.\n\n(2) Precision is affected by class priors and sample imbalance, which can cause unstable weights for very rare classes. The paper does not analyze risk amplification when precision is unreliable or noisy.\n\n(3) Only image classification benchmarks are evaluated, despite claims that BESTI benefits high-risk domains (e.g., healthcare, finance).\n\n(4) Reported tables appear to be single-run results, without standard deviation, confidence intervals, or statistical tests. This makes the reliability of model improvements unclear, especially because sampling randomness can heavily influence performance in imbalanced settings.\n\n(5) Only accuracy-based metrics are reported. For imbalanced classification, macro-F1, balanced accuracy, and AUC are essential, yet largely missing. Include class-balanced metrics to more appropriately measure impact on few-shot classes.\n\n(6)  Experimental evidence is insufficient to fully support the claims. For example, in Table 3, the proposed method does not demonstrate a clear advantage compared to the baseline methods."}, "questions": {"value": "The following are my detailed questions and concerns:\n\n(1) It is unclear why structured thresholds produce better class representation diversity than a single optimized sampling strategy. No ablation is shown to justify the design.\n\n(2) The claim that models trained with different imbalance levels specialize in specific classes requires empirical proof (e.g., per-class performance visualization).\n\n(3) The authors claim parallelizability, but do not show actual parallel performance gains or resource requirements.\n\n(4) High precision does not guarantee good minority recall. Why is precision chosen over recall in imbalanced settings? The motivation of using precision is not clear.\n\n(5) Improvements are reported as single values without statistical testing (e.g., variance / confidence intervals). Are results consistent across multiple random seeds or different random dataset split?\n\n(6) Since sampling is random, how is stability ensured? Are multiple runs averaged or is there high variance in created subsets and trained models? For example in table 1, why not report the mean and std results?\n\n(7) Dataset subsets progressively discard majority class data. How does this affect learning of decision boundaries? Does performance degrade with too many stages?\n\n(8) In table 3, the performance of the proposed BESTI does not demonstrate an advantage over other baselines, especially in Many, Med, Few group on ImageNet-LT and Places-LT. How to claim that BESTI performs better than others? And again, why not run multiple times and report the average performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FXpTkQ5JzE", "forum": "LEWQk3j73j", "replyto": "LEWQk3j73j", "signatures": ["ICLR.cc/2026/Conference/Submission17529/Reviewer_qFFk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17529/Reviewer_qFFk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761290651096, "cdate": 1761290651096, "tmdate": 1762927407696, "mdate": 1762927407696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework called BESTI for handling imbalanced multi-class classification based on three components: i) generating multiple training subsets with structured sampling to create diverse models, ii) training independent models on these subsets, and iii) aggregating their predictions using a Bayesian trust-weighting scheme where each model's class-wise precision determines its influence. Experiments on long-tailed image benchmarks (CIFAR-LT, ImageNet-LT) show that BESTI effectively improves overall accuracy, particularly for minority classes, and achieves competitive performance with state-of-the-art methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "-  Experiments conducted on 3 challenging datasets show the method consistently demonstrates improved performance, particularly for the crucial medium- and few-shot classes. Although this is something that has been studied and demonstrated for years in ensemble learning for imbalanced problems.\n- The decoupling of \"Structured Sampling\" (generation) and \"Trust-Weighted Ensemble\" (prediction) is a great thing as it allows users to use each module independent of the other, and is also aligned with the ensemble literature which divides a whole ensemble into generation, selection and aggregation\n- source code is available as supplementary material which is good for reproducibility"}, "weaknesses": {"value": "**Major weaknesses:**\n- Lines 314–316 state that “BESTI … can flexibly integrate with more advanced weighting schemes, such as LDAM, DRW [Cao et al., 2019b], and class-based loss [Cui et al., 2019].” This flexibility contradicts the paper’s central claim of a Bayesian class-wise trust-weighted combination rule. If external weighting schemes replace or modify the proposed trust weights, the “Bayesian Trust” principle is no longer preserved.\n\t\n- The claim that BESTI introduces a “Bayesian Class-wise Trust-Weighted Ensemble” and its novelty is quite overstated. Computing per-class precision and using it as a weighting factor is conceptually equivalent to earlier local accuracy estimation and trainable combiner approaches [1][2]. Those works already treat class-specific performance as a competence estimate for combination weights. The paper should demonstrate a theoretical advancement beyond these models or tone down the “Bayesian” framing, which currently lacks derivation or probabilistic justification.\n\t\n- The proposed Structured Sampling (SS) strategy resembles existing resampling-based ensembles such as Random Balance [3][4] and related diversity-enhancing methods [5]. The paper should clarify how SS fundamentally differs from these techniques beyond using a deterministic threshold schedule and include empirical or theoretical justification for calling it new. Also, why not consider oversampling the minority classes together as an approach to handle this problem? \n\t\n- The manuscript includes strong claims about the method’s advantages: “incorporates as much diversity in imbalance level as possible” and produces “interpretable and decisive predictions” , without supporting analysis. The first statement is inaccurate: the Structured Sampling procedure only reduces majority classes progressively, never generating subsets where minority classes become dominant. Prior ensemble works on imbalance (e.g., Random Balance [3][4][5]) explicitly vary class priors in both directions and empirically show that such bidirectional sampling increases robustness. Thus, BESTI does not maximize diversity in class distributions, but rather explores a restricted region of the sampling space. The second statement on “interpretable and decisive predictions” is generic to any argmax-based soft ensemble [6]. These claims should be rephrased or substantiated with empirical or theoretical justification. \n\t\n- Section 2 begins with One-vs-All (OVA) classification, which disrupts the flow of the imbalance discussion. OVA can induce imbalance, but BESTI’s setting naturally long-tailed datasets is independent of that decomposition. The section should open with long-tailed [8] and ensemble literature instead, mentioning OVA only briefly as background.\n\t\n- Diversity is measured only via KL divergence between training-set class distributions, not via prediction-level behavior. In ensemble learning, what matters is complementarity of predictions [7]; training differences are only a proxy. The paper should quantify prediction-level diversity (e.g., pairwise disagreement, Q-statistic, co-error rate, double fault) to verify that experts truly specialize rather than replicate one another.\n\t\n- The text claims the approach is “parallelizable and scalable,” yet Algorithm 1 and the staged sampling description imply sequential dependency (stage 1, 2, …) implying later models depend on progressively clipped subsets. The authors should clarify how models are generated and whether they can indeed be trained in parallel as well as reflect that in the writing.\n\t\n- In computing per-class precision, each model M_iis evaluated on D_train, which partially overlaps with its training subset S_i. This compromises the claim of using “largely unseen data.” The paper should quantify the overlap and discuss its potential impact on the trust estimation.\n\tDifferent backbones are used for different datasets without justification. For xcomplete comparison, each backbone should be tested across all datasets or clear justification for not using them (e.g., computational limits, incompatibility) should be provided. Similarly, it is not specified which weighting variants, optimizers etc were tested on which backbones, leaving the methodology ambiguous.\n\t\n-Although the limitations section mentions efficiency, the paper provides no quantitative analysis of computational cost as the ensemble grows . Since inference cost scales linearly with ensemble size, a cost–performance curve or per-model accuracy gain would strengthen the empirical evaluation.\n\n- The ablation studies do not fully isolate the contributions of BESTI’s three core components: Structured Sampling, independent model training, and Trust-Weighted Aggregation. For instance, the paper reports variants such as “w/o BSM” or “Naive Ensemble,” but never evaluates the effect of removing Structured Sampling itself or replacing it with a known baseline (e.g., Random Balance). As a result, it remains unclear how much of the improvement comes from each module. Since the paper emphasizes BESTI’s modularity and “efficient parallelizability” , a comprehensive ablation should be done.\n\n**Minor presentation issues:**\nboldfacing best scores on tables (or using colors etc)  would help the reader finding trends in the results. Also, the authors do not mention the code is available as supplementary material in the open-review website. Some readers may miss that.\n\n**References**\n\n[1] Woods, K., Kegelmeyer, W. P., & Bowyer, K. (1997). Combination of multiple classifiers using local accuracy estimates. IEEE TPAMI 19(4), 405–410.\n[2] Kuncheva, L. I. (2014). Combining Pattern Classifiers: Methods and Algorithms. Wiley.\n[3] Díez-Pastor, J. F., et al. (2015). Random balance: ensembles of variable priors classifiers for imbalanced data. Knowledge-Based Systems 85, 96–111.\n[4] Rodríguez, J. J., et al. (2020). Random Balance ensembles for multiclass imbalance learning. Knowledge-Based Systems 193, 105434.\n[5] Díez-Pastor, J. F., et al. (2015). Diversity techniques improve the performance of the best imbalance learning ensembles. Information Sciences 325, 98–117.\n[6] Kittler, J., et al. (2002). On combining classifiers. IEEE TPAMI 20(3), 226–239.\n[7] Duin, R. P. W. (2002). The combining classifier: to train or not to train? Proc. ICPR Vol. 2, 765–770.\n[8] Zhang, Chongsheng, et al. \"A systematic review on long-tailed learning.\" IEEE Transactions on Neural Networks and Learning Systems (2025)."}, "questions": {"value": "Here are major points i would like a clarification in a rebuttal based on the weaknesses above:\n\n- Your paper's central contribution is the \"Bayesian Class-wise Trust-Weighted Ensemble,\" where the trust weight is defined as the class-wise precision. However, you also claim BESTI can \"flexibly integrate\" with external weighting schemes like LDAM or DRW. If these schemes modify the combination scheme, doesn't this alter the fundamental definition of your Bayesian trust weight, potentially breaking the theoretical justification? Could you clarify what \"integration\" means here and how the core Bayesian trust principle is preserved when using these external methods? Would it be that other components (Structured sampling) that can be coupled with other models?\n- Could you provide a more complete ablation study that isolates the contribution of each core component? Specifically, what is the performance of BESTI without SS (e.g., using a baseline like Random Balance) but with your trust weighting, to show SS's unique benefit?\n-  the method is stated \"parallelizable,\" yet the algorithm and sampling seem sequential. Please clarify whether the training procedure can be done entirely in parallel. \n- What is the cost-performance trade-off (e.g., a plot of accuracy vs. ensemble size/inference time) ?\n-  The weighting mechanism uses class-wise precision. How is this fundamentally different from, or an advancement over, established \"local accuracy\" or \"classifier competence\" concepts from prior ensemble literature (see references above).\n- How does sampling diversity (subset) correlate with ensemble diversity in model prediciton (see references above about diversity measures)?\n- Lastly"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Oqqa2v35sl", "forum": "LEWQk3j73j", "replyto": "LEWQk3j73j", "signatures": ["ICLR.cc/2026/Conference/Submission17529/Reviewer_DQbA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17529/Reviewer_DQbA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533028602, "cdate": 1761533028602, "tmdate": 1762927407196, "mdate": 1762927407196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "BESTI, an ensemble framework for long-tailed classification that (1) creates multiple sub-training sets via threshold-based structured sampling (progressively downsampling head classes), and (2) aggregates independently trained models with a Bayesian class-wise trust weighting, where “trust” is each model’s per-class precision estimated from a confusion matrix. Experiments on CIFAR-100-LT, ImageNet-LT, Places-LT claim consistent gains over a very small pool of outdated baselines (CE, Balanced Softmax, MiSLAS, LADE, RIDE, SADE). With older baselines, modest methodological novelty, limited metrics, and potential trust-weight leakage, the paper does not meet ICLR’s high standards on novelty and soundness in long-tail learning. I recommend reject."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper is clearly written and modular: structured clipping → independent training → precision-weighted ensembling; the method is easy to follow, understand, and reproduce.\n+ Ablations across clipping schedules (exponential, uniform, quantile, k-means) and ensemble size k help demystify design choices; some runs include seeds/± values.\n+ Results are reported on three standard LT benchmarks with many/med/few breakdowns and macro (“All”) accuracy."}, "weaknesses": {"value": "+ One of my biggest gripes with this paper is the outdated baseline suite. The strongest comparison is to SADE (2022); others are RIDE (2020), MiSLAS (2021), LADE (2021), Balanced Softmax (2020)—all pre-2023. No recent long-tail advances (post-2022), no modern calibration-aware or distributionally-robust LT losses, and no contemporary ViT-era decoupling/tuning lines are included. With only older baselines, the competitiveness claim is weak for ICLR 2026. \n+ BESTI is essentially cluster/threshold-guided undersampling + ensembling with precision-weighted voting; the “Bayesian” part reduces to the textbook identity that posterior(correct|pred=c) = precision (proved from the confusion matrix). This is a sensible heuristic, but not a conceptual leap beyond classic undersampling ensembles and class-wise weighting.\n+ The paper computes each model’s confusion matrix on the training set (even if trained on a clipped subset) to derive precision weights. That is not a standard, leakage-aware protocol; it couples weights to training distribution quirks rather than a held-out set and can overstate “trust” for heads. \n+ I find the evaluation scope too narrow for a claim of broad LT robustness. Metrics center on accuracy by shot bins; there is no calibration, tail-recall@k, AUCPR, or cost/threshold-aware analysis—important in long-tail. The paper also lacks open-set / class-presence shift probes and label-noise stress tests that are common in recent LT work.\n+ BESTI trains k models; aside from a few ± numbers, there’s no wall-clock or energy comparison vs. strong single-model LT methods or lighter ensembles. The KL-based rule to stop at k is heuristic and only shown as a trend plot.\n+ I  my opinion the current ablation study don’t isolate what truly matters. Gains could stem largely from downsampled re-training + ensembling. There’s no comparison to (i) simple uniform ensembling of k independently seeded models on the full data, (ii) head class temperature/threshold calibration, or (iii) precision weighting computed on a held-out split. The small drop from replacing trust with uniform averaging suggests the weighting has limited marginal effect."}, "questions": {"value": "+ Why are post-2022 long-tail methods absent? Please add competitive 2023–2025 LT baselines and recent calibration/DRO objectives; otherwise the SOTA claim is not credible for ICLR.\n+ Can you re-estimate class-wise precision on a held-out validation (or via cross-fitting) instead of the training set to avoid biasing the “trust” weights?\n+ How does BESTI compare to a naïve k-model ensemble trained on the full data with different seeds (no clipping), and to simple per-class threshold calibration? Please provide matched-compute studies.\n+ Please report calibration (ECE/ACE), AUCPR, tail-recall@k, and cost-sensitive metrics, and add label-noise and class-presence shift stress tests to reflect modern LT evaluation. \n+ Your KL-based stopping rule is heuristic; can you give a compute/accuracy Pareto (k vs. hours/energy) and justify k in practical compute budgets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "J2bbHlFH0g", "forum": "LEWQk3j73j", "replyto": "LEWQk3j73j", "signatures": ["ICLR.cc/2026/Conference/Submission17529/Reviewer_dnnP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17529/Reviewer_dnnP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775037347, "cdate": 1761775037347, "tmdate": 1762927406846, "mdate": 1762927406846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-class classification algorithm to handle the issue of class imbalanced that combined structured sampling and Bayesian interpretation. The experimental results outperform some existing methods. However, the paper lacks rigor in its details and a detailed explanation of the issue solved by each module, limiting its overall originality and impact."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper handles the issue of class imbalance in multi-class classification, which is practically meaningful.\n2. The proposed method was empirically validated, showing superior performance compared with several SOTA."}, "weaknesses": {"value": "1. The connection between theoretical derivation and practical implementation is weak; some notations and their roles are ambiguous.\n2. The description of the method part is not clear enough, some symbols lack definition, which makes it difficult to read, and the details are not rigorous enough.\n3. The level of originality is limited, as the method largely combines existing ideas into a single framework without strong theoretical innovation.\n4. The analysis, compare and discuss of recent related work are little."}, "questions": {"value": "1. Please state key quantitative results (e.g., performance metrics) to underscore the effectiveness of the proposed method at the end of the abstract.\n2. Enrich the related work by discussing more recent advancements in multi-label learning.\n3. The latest method that the authors compare is in 2022, which is a bit old. The more recent algorithms should be compared.\n4. C_{max} and r are two key parameters. How do they affect the performance of the algorithm?\n5. Computational experience should be better explained. What's the computational burden of the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u4rYBwYRm7", "forum": "LEWQk3j73j", "replyto": "LEWQk3j73j", "signatures": ["ICLR.cc/2026/Conference/Submission17529/Reviewer_CSPN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17529/Reviewer_CSPN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995338397, "cdate": 1761995338397, "tmdate": 1762927405538, "mdate": 1762927405538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}