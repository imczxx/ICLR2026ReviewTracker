{"id": "OZTdj0qFmB", "number": 23647, "cdate": 1758346751968, "mdate": 1759896803317, "content": {"title": "SimReg: Achieving Higher Convergence and Generalization in the LLM Pretraining via Embedding Similarity Regularization", "abstract": "Pretraining large language models (LLMs) with next-token prediction has led to remarkable advances, yet the context-dependent nature of token embeddings in such models results in high intra-class variance and inter-class similarity, thus hindering the efficiency of representation learning. While similarity-based regularization has demonstrated benefit in supervised fine-tuning and classification tasks, its application and efficacy in large-scale LLM pretraining remains underexplored. In this work, we propose the SimReg, an embedding similarity regularization loss that explicitly encourages token representations with the same ground-truth label within each sequence to be more similar, while enforcing separation from different-label tokens via a contrastive loss. Our comprehensive theoretical analysis elucidates how SimReg improves both classification margins and generalization in the pretraining stage. Extensive experiments across dense and Mixture-of-Experts (MoE) architectures demonstrate that SimReg consistently accelerates training convergence by over 30\\% and improves average zero-shot downstream performance by over 1\\% across standard benchmarks. Further ablation and analysis provide practical recommendations for hyperparameter selection and loss application, offering constructive insights for efficient pretraining of LLMs.", "tldr": "We introduce embedding similarity supervision to assist the cross-entropy loss and accelerate large-scale pre-training of LLMs.", "keywords": ["Embedding similarity", "cross entropy", "pretraining"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f114faf341807d89582d9ac008b2a54b50cc424.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SimReg, a contrastive regularization loss that addresses a key limitation of cross-entropy training in LLMs: embeddings predicting the same token can vary substantially due to different contexts, and cross-entropy stops enforcing embedding separability once basic classification is achieved. SimReg explicitly pulls together embeddings with the same prediction target while pushing apart those with different targets, which theoretically enlarges classification margins and can accelerate optimization. The method includes practical design choices for stability and scalability, with hyperparameters that scale predictably with model size. Experiments show that SimReg achieves faster convergence and higher model quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses sample efficiency in LLM pre-training pipelines, a key factor for developing more useful models and reducing operating costs. \n- The idea of using cosine-similarity of readily available embeddings as an additional component of the loss enriches the optimization process and can yield overall better performance"}, "weaknesses": {"value": "- My main concern is that the paper does not look into other loss-regularizing/-reweighting techniques that augment the optimization process [1, 2, 3, 4]. In particular, I would be interested better understanding how the cost dynamics compare.\n- Chunking seems to be a large part of making Simreg practically applicable. How should the chunk size have to be configured in context of other hyperparameters like the learning rate? \n- I am generally missing the stanard deviation in all plots and tables. Since the performance improvements of Simreg on the various benchmarks in Table 1 appear to be quite small, getting a full picture requires the standard deviation.\n\n**Minor remarks:**\n- Line 79: The word \"only\" seems to be misplaced\n- Line 161: Sentence starting with \"The model typically employ...\" Here is an s missing. \n- Line 262: typo \"kernal\"\n\n**Sources**\n[1] Text and Code Embeddings by Contrastive Pre-Training,  Neelakantan et al., 2022\n[2] Rho-1: Not All Tokens Are What You Need, Lin et al., 2024\n[3] DoGE: Domain Reweighting with Generalization Estimation, Fan et al., 2023\n[4] Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining, Sow et al., 2025"}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EgnqAEo1hV", "forum": "OZTdj0qFmB", "replyto": "OZTdj0qFmB", "signatures": ["ICLR.cc/2026/Conference/Submission23647/Reviewer_ueGi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23647/Reviewer_ueGi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753308138, "cdate": 1761753308138, "tmdate": 1762942746503, "mdate": 1762942746503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose SimReg, a consistency regularisation term designed to enhance the representational capacity of the model. The objective of SimReg is to effectively pull embeddings of the same class together while push them apart from different classes. The authors also provide a theoretical analysis attempting to establish a formal link between the SimReg and the enhancement of classification margins."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Significance: The work tackles a core challenge in LLM development: improving pretraining efficiency and model generalisation.\n2. Theoretical Foundation: The authors provide a valuable theoretical analysis (Lemmas 4.1 and 4.2) that formally connects the SimReg to the enlargement of classification margins, providing a theoretical rationale for SimReg.  \n3. Empirical Results: The experimental validation demonstrates improvements in both convergence speed (reportedly over 30%) and downstream task performance (over 1% on average in zero shot benchmarks)."}, "weaknesses": {"value": "1. Gap in Theoretical Analysis: The abstract claims \"Our comprehensive theoretical analysis elucidates how SIMREG improves both classification margins and generalization in the pretraining stage\". However, the provided Lemmas and the discussion in Section 4 only establishes a link between the SimReg objective and the margin. The crucial link between an enlarged margin and improved generalisation ability (i.e., test error/performance on downstream tasks) is supported only by empirical results rather than a formal proof, which does not constitute a comprehensive theoretical analysis.  \n2. Missing Experimental Comparisons: The paper lacks experimental comparisons with existing contrastive or consistency-based methods that could be applied to large language model pretraining.\n3. Insufficient Discussion of Results: The paper does not sufficiently analyse or discuss certain anomalous experimental outcomes. For instance, in Table 1 (LLaMA 7B), the chunked approximation (SIMREG CHUNK) marginally outperforms the full SIMREG (52.77 vs 52.67), which is counter intuitive given that it is an approximation. Furthermore, on specific tasks (like Arc E for LLaMA 7B), SIMREG performs worse than the baseline. A deeper investigation into these phenomena would strengthen the paper's conclusions.\n4. Lack of Quantitative Overhead Analysis: While a chunking method is proposed to reduce complexity, this claim is not substantiated with a quantitative analysis of the actual impact on training time, memory, or computational resources, particularly in long sequence scenarios.\n5. Minor Typos: The paper contains several typos. For example:  \n(1) In the definition of the function g (around line 288), ${y_{i}}$ should probably be ${y_k}$, $f_P(k)$ should probably be $f_P(\\mathbf e_k)$;     \n(2) In the discussion of the margin (around line 322), the text states the margin can \"reduce\". This should likely be \"increase\" or \"improve\". The index $i$ in $m_i'$ should probably be $k$."}, "questions": {"value": "1. Could the authors elaborate on the theoretical link between the margin illustrated in the paper and generalisation ability (i.e., test error/performance on downstream tasks) to fully support the theoretical contribution claimed in the abstract?\n2. Could the authors include additional experimental results and analyses comparing SimReg with other contrastive or consistency-based methods? This would strengthen the support for the method’s empirical performance and help demonstrate that the proposed contrastive learning strategy is particularly well-suited for LLM pre-training.\n3. Do the authors have an explanation or hypothesis for the anomalous results observed (e.g., SIMREG CHUNK outperforming SIMREG, or SIMREG underperforming the baseline on specific tasks)? \n4. Could the authors provide a quantitative analysis of the computational overhead (e.g., percentage increase in training time or memory usage) introduced by SimReg?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1xPCPBgjZE", "forum": "OZTdj0qFmB", "replyto": "OZTdj0qFmB", "signatures": ["ICLR.cc/2026/Conference/Submission23647/Reviewer_Vb22"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23647/Reviewer_Vb22"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867383616, "cdate": 1761867383616, "tmdate": 1762942746260, "mdate": 1762942746260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SIMREG, a similarity regularization loss that improves convergence and generalization in large-scale language model pretraining. By complementing cross-entropy, SIMREG enforces higher intra-class embedding similarity and inter-class separability through a contrastive cosine-based objective. It integrates efficiently into both dense and Mixture-of-Experts architectures and supports chunk-wise parallelization for long sequences. Theoretical analysis links SIMREG to enlarged classification margins and faster optimization, while experiments on LLaMA and Mixtral models show 30–37% faster convergence and about 1% average improvement in zero-shot downstream benchmarks. Overall, SIMREG provides a simple yet effective framework for enhancing embedding consistency during LLM pretraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- SIMREG successfully complements CE by imposing mandatory constraints on feature similarity, resulting in significantly enhanced separability. \n- SIMREG shows a wide range of applicability regarding its weighting hyperparameter (λ). The paper also provides critical practical insights, recommending that SIMREG is most effective when applied solely at the output of the final layer of the network. \n- The inclusion of a detailed theoretical analysis explaining how the SIMREG loss translates into increased classification margins and accelerated loss reduction provides strong justification for the observed empirical performance boosts."}, "weaknesses": {"value": "- While the paper mentions that applying SIMREG only at the last layer results in almost no overhead, the actual runtime or GPU cost increase relative to the baseline training step is not explicitly quantified.\n- The derivation of the hyperparameter scaling law for λ relies on treating embeddings x,y as independent and identically distributed isotropic random variables. This assumption may overly simplify the highly non-isotropic and context-dependent nature of token embeddings in real LLMs, potentially limiting the generalizability of the scaling guidance. \n- The paper notes that LLM pretraining data exhibits a pronounced long-tail effect where frequent tokens dominate the CE loss. While SIMREG aims to stabilize representations, it is not clearly discussed how the required positive sets are handled for extremely rare tokens."}, "questions": {"value": "- Can the authors provide an analysis or visualization demonstrating how SIMREG specifically influences the clustering and separability of tokens belonging to the heavy long-tail compared to high-frequency tokens?\n- Could the authors provide a quantitative comparison of the per-step training time for the CE-only baseline versus the proposed CE+SIMREG configuration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tWhauFmz8E", "forum": "OZTdj0qFmB", "replyto": "OZTdj0qFmB", "signatures": ["ICLR.cc/2026/Conference/Submission23647/Reviewer_JGnN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23647/Reviewer_JGnN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963266673, "cdate": 1761963266673, "tmdate": 1762942746026, "mdate": 1762942746026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces consistency regularization loss to improve the convergence and generalization of LLM pretraining. The proposed method, SimReg, leverages embedding similarity regularization to encourage similar token representations for same label and to separate different labeled tokens. Moreover, the authors provide theoretical analysis to explain how the SimReg loss enhances the convergence speed of training process. The experiments on LLaMA and Mixtral demonstrate the proposed method convergence faster and achieve better generalization performance than CE loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- SimReg complements CE by imposing constraints on feature similarity, resulting in enhanced separability. \n- SimReg shows a wide range of applicability regarding its weighting hyperparameter (λ). The authors show that SimReg is effective when applied at the output of the final layer. \n- The authors provide theoretical analysis to explain how the SimReg increase classification margins and accelerated the optimization. Moreover, the experiments matches well with the theoretical insight."}, "weaknesses": {"value": "- While the paper mentions that applying SIMREG only at the last layer results in almost no overhead, the runtime or GPU cost increase relative to the baseline training step is not explicitly quantified.\n- Line 734. The derivation of the hyperparameter scaling law for λ relies on treating embeddings x,y as independent and identically distributed isotropic random variables. This assumption may simplify the non-isotropic and context-dependent tokens in practice. \n- The paper notes that LLM pretraining data exhibits long-tail effect where frequent tokens dominate the CE loss. SimReg aims to improve the stability of representations. To me, it is not clear how the positive sets are handled for extremely rare tokens."}, "questions": {"value": "- I recommend the authors provide an analysis demonstrating how SimReg influences the clustering and separability of heavy-tail tokens compared to high-frequency tokens?\n- I recommend the authors provide a quantitative comparison of the per-step training time for the CE-only baseline versus the proposed SimReg method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tWhauFmz8E", "forum": "OZTdj0qFmB", "replyto": "OZTdj0qFmB", "signatures": ["ICLR.cc/2026/Conference/Submission23647/Reviewer_JGnN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23647/Reviewer_JGnN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963266673, "cdate": 1761963266673, "tmdate": 1763715162499, "mdate": 1763715162499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}