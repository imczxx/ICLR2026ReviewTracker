{"id": "s5FME7A0TY", "number": 10060, "cdate": 1758159244962, "mdate": 1759897677337, "content": {"title": "The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs", "abstract": "Co-optimizing data and model configurations for LLMs presents a classic chicken-and-egg dilemma: the best training data configuration (e.g., training data composition) depends on the chosen model configuration (e.g., model architecture, fine-tuning configuration), but the best model configuration also depends on the chosen training data. However, jointly optimizing both data and model configurations is intractable, with existing methods focusing only on data or model selection in isolation without considering their complex interdependence. We introduce JoBS, an efficient method that jointly optimizes LLM training data and model configurations by framing the problem as a black-box optimization problem. Central to our method is a novel performance scaling law predictor, which learns a diverse family of performance scaling laws for different configurations and cheaply predicts how promising a particular training configuration is. This enables us to quickly build an approximate LLM performance landscape and efficiently find optimal training configurations with Bayesian Optimization (BO). JoBS not only outperforms existing baselines across diverse tasks in the fine-tuning setting, but also runs up to 12.4× faster. We hope our work draws more attention to the chicken-and-egg dilemma inherent in co-optimizing training configurations for LLMs. Our anonymized code is available at https://github.com/a35453779/JoBS.", "tldr": "We introduce JoBS, an efficient algorithm to co-optimize data and model configurations for LLMs - an endeavor once thought to be intractable.", "keywords": ["Co-optimization", "Joint Optimization of LLM data and model configurations"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6e02816ab1e6ddd44e52baf7448ce5556796bb8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the classic “chicken-and-egg” problem in LLM training — the strong interdependence between data configuration and model configuration — and proposes JoBS, a method that jointly optimizes both by modeling the performance landscape with Gaussian processes and accelerating search using a performance scaling predictor. Experiments show that JoBS consistently outperforms independent optimization approaches: for example, it achieves 80.4% accuracy on gsm8k (vs. ~74.7% for the best baseline) and 75.8% on TruthfulQA (vs. ~71.7%), delivering a stable 6–7% interaction improvement across tasks and models (around 7B). Thanks to its predictor, JoBS can forecast the final training performance based on early-stage results, significantly reducing the time cost. It is worth noting that the experiments are conducted under the LoRA setting and do not explore other tunable training parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The predictor proposed in the paper is simple yet effective. Predicting the final training outcome based on early-stage performance is an excellent idea, and the experiments demonstrate that this approach alleviates the problem of reduced time efficiency caused by an excessively large search space.\n\n- The interdependence between data configuration and model configuration is indeed an important aspect of LLM training. The use of Bayesian optimization gives this approach the potential to be extended to different model architectures and various task domains.\n\n- The paper conducts extensive experiments based on the LoRA configuration, and the results demonstrate the reliable performance improvements of JoBS over the baselines, further validated across multiple cross-domain datasets. Additional supplementary experiments and analyses also provide deeper insights."}, "weaknesses": {"value": "- Although the predictor can help forecast results when trained properly, it remains a risky choice: an unusual training curve (e.g., one rarely seen in the training data) might cause a promising training configuration to be overlooked. Since LLM training is influenced by many hyperparameters, predictor failures may occur frequently. Moreover, the need to train a new predictor for each task limits the method’s applicability and stability in multi-task joint SFT scenarios.\n\n- The experiments in the paper seem to be limited to the LoRA SFT setting. While LoRA has indeed become the primary choice for downstream fine-tuning under limited compute, considering that hyperparameter space search itself already requires substantial time and resources, it may be necessary to demonstrate JoBS’s performance on full-parameter fine-tuning (which could also introduce new challenges for components such as the predictor). This would make JoBS more practically valuable for real-world applications.\n\n- JoBS also lacks experiments on larger models beyond 7B, such as 32B or 72B. Moreover, considering the current development of LLMs, JoBS performs data configuration optimization based on proportion search within a fixed data pool rather than dynamically constructing or filtering data. This raises concerns about its robustness in “open-domain” or highly variable data distribution scenarios — conditions that are common in real-world applications where training datasets are continuously expanding. As a result, introducing JoBS in such settings could potentially lead to additional training costs."}, "questions": {"value": "- Can the paper provide evidence of the predictor’s performance in multi-task joint SFT scenarios?\n\n- Can the paper provide results of JoBS in more realistic settings, such as full-parameter fine-tuning, larger model scales, or more open-ended tasks?\n\n- Can the authors provide more analysis or evidence on how robust this predictor is to noisy or unstable early training dynamics, which are common in large-scale LLM training (e.g., due to optimizer warm-up, curriculum learning, or data distribution shifts)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kHOVOm7hYy", "forum": "s5FME7A0TY", "replyto": "s5FME7A0TY", "signatures": ["ICLR.cc/2026/Conference/Submission10060/Reviewer_W1bf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10060/Reviewer_W1bf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761035225241, "cdate": 1761035225241, "tmdate": 1762921456654, "mdate": 1762921456654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the \"chicken-and-egg dilemma\" in LLM optimization: the ideal training data configuration (e.g., data mixture) depends on the model configuration (e.g., PEFT hyperparameters), and vice versa. Optimizing these factors independently leads to suboptimal results. The authors propose JOBS (Joint Bayesian Optimization with Scaling Laws), a method that frames this as a black-box optimization problem. JOBS uses Bayesian Optimization (BO) to efficiently search the joint configuration space. To make this process computationally feasible, it introduces a neural network-based performance predictor that extrapolates final model performance from short, inexpensive training trials. The authors demonstrate empirically that this joint optimization approach significantly outperforms methods that optimize data and model configurations independently or in an alternating fashion, while also being much faster."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper designs its method using BO techniques flexibly and appropriately. The core idea of using a performance predictor to amortize the cost of BO evaluations is well-motivated and practical. The writing is easy to follow, and the overall narrative is clear. The empirical results look good and strong, showing consistent improvements over a wide range of baselines, including independent and alternating optimization schemes, across multiple tasks and models. The \"interaction improvement\" claim is well-supported by the main results tables."}, "weaknesses": {"value": "The problem formulation relies on a fixed training time budget. However, training time is highly sensitive to the implementation (e.g., specific frameworks for PEFT or inference). It is questionable whether using time as the primary budget is a robust choice, as opposed to a more implementation-agnostic budget like total tokens or training steps or FLOPs or other potential choices.\n\nThe motivation in Section 3, particularly Figure 2, is a key pillar of the paper. However, I am wondering if the performance variance shown in Figure 2 is due to training instability. Many papers suggest that PEFT can result in high variance in training performance. The paper does not seem to address this; for instance, it's not clear if each point in the landscape is an average of multiple runs. If the variance from training instability has a greater magnitude than the \"valleys\" created by the model/data strategy itself, then Figure 2 cannot faithfully support the claim that the landscape is smooth but complex.\n\nFor Section 2, I don't think this makes for a comprehensive related work review. The paper introduces BO and scaling laws as core components, but the related works section does not sufficiently discuss prior work using BO and scaling laws specifically for training configuration optimization and performance prediction.\n\nI am curious why a neural network was chosen as the scaling law predictor. A scaling law is usually a symbolic expression, which makes it explainable and allows it to generalize to unseen extrapolated settings (as the law is not overly complex). A neural network, while a flexible function approximator, can fit the training data well but may not generalize well, is a black-box predictor, and may exhibit non-smooth behavior. The paper could benefit from a clearer justification for this choice over more traditional scaling law formulations.\n\n(minor) Using $\\mathcal L$ as the performance metric notation is weird. This notation almost universally denotes a loss function, so using it for accuracy or another performance metric is confusing."}, "questions": {"value": "1. The paper studies \"data mixture\" as a form of data strategy, parameterizing the data configuration as a probability simplex over N datasets. However, in many real applications, there are no clear \"datasets,\" and data selection is performed instance-wise. How do you see this method handling such a scenario? For example, can this method be scaled to \"instance-level\" data selection (e.g., where we have many data points and each is labeled as \"selected\" or \"excluded\")? This would dramatically increase the dimensionality of the data configuration space $\\mathcal{X}$.\n2. The entire method relies on the Gaussian Process (GP) surrogate being a good model of the true performance landscape $\\mathcal{L}$. How good can the GP depict the real metric of $\\mathcal{L}$? Although it is computationally expensive to validate this exhaustively, can we get a sense of the surrogate model's fidelity from some smaller-scale experiments (e.g., by comparing the GP's predictions to a densely-sampled ground truth in a low-dimensional version of the problem)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QCccvWnU96", "forum": "s5FME7A0TY", "replyto": "s5FME7A0TY", "signatures": ["ICLR.cc/2026/Conference/Submission10060/Reviewer_AUVd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10060/Reviewer_AUVd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662989784, "cdate": 1761662989784, "tmdate": 1762921456395, "mdate": 1762921456395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Joint Bayesian Optimization with Scaling Laws, a framework to co-optimize both training data configurations and model configurations for large language model fine-tuning. The key idea is to treat the fine-tuned model performance as a black-box function over data–model configuration space and use Bayesian Optimization to efficiently search for optimal configurations. To further reduce computational cost, the authors propose a neural performance scaling law predictor that extrapolates the final model performance from short fine-tuning runs. Theoretical analysis provides a cumulative regret bound even under noisy performance predictions. Empirical results on multiple LLMs and tasks demonstrate that JoBS achieves a consistent improvement over independent data or model optimization methods while being faster."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper focuses on an important question by explicitly formulating the chicken-and-egg dilemma between training data and model configuration in LLM fine-tuning as a joint black-box optimization problem. It proposes JoBS, which combines Bayesian Optimization with a neural performance scaling-law predictor to efficiently explore the configuration space. It models LLM performance as a Gaussian process, uses a predictor to extrapolate final results from short training runs, and provides a theoretical convergence guarantee under prediction noise. Experiments across several LLMs and benchmarks demonstrate consistent performance improvements and faster optimization compared to independent data- or model-selection baselines."}, "weaknesses": {"value": "1. The paper references the term $\\gamma_T$ in the main theorem, but does not explicitly define it within the text. \n\n2. All experiments are conducted on relatively small LLMs (up to 8B parameters). It remains unclear whether the proposed method scales to larger backbone models, where optimization dynamics may differ.\n\n3. The compared baselines optimize data and model configurations separately. It would strengthen the empirical evidence to include or discuss any existing methods (if any) that attempt to jointly optimize both components. If JoBS is the first to do so, it should be clearly emphasized."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6FbIbLIxvL", "forum": "s5FME7A0TY", "replyto": "s5FME7A0TY", "signatures": ["ICLR.cc/2026/Conference/Submission10060/Reviewer_vLaL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10060/Reviewer_vLaL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953448824, "cdate": 1761953448824, "tmdate": 1762921456035, "mdate": 1762921456035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the interdependence between data mixture configuration and model training configuration (LoRA parameters) in PEFT fine-tuning. The authors propose JoBS, combining Bayesian Optimization with a neural network predictor that estimates full fine-tuning performance from short training runs. Experiments on 7 tasks with 3 LLM families show 6-7% improvement over independent optimization and 12.4× speedup. Theorem 4.1 provides convergence guarantees under noisy predictions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Problem formulation**: Explicitly formulating the interdependence between data mixture ratios and LoRA training configurations as a joint optimization problem is novel. Figure 2b demonstrates that optimal data mixtures vary across LoRA configurations, which is non-intuitive and well-illustrated.\n\n**Technical approach**: Combining BO with a scaling law predictor is sound. Theorem 4.1 shows prediction noise is handled as observation noise. Deep kernels for heteroskedastic modeling and continuous parameterization for mixed discrete-continuous spaces are appropriate. Experimental design with 5-trial averaging and comprehensive baselines is rigorous.\n\n**Empirical validation**: Consistent improvements across tasks and models. Ablations in Figure 4 provide useful insights into component contributions."}, "weaknesses": {"value": "**Severely limited scope**: Only PEFT (specifically LoRA) is evaluated, not full fine-tuning or other PEFT methods (prefix tuning, adapters). The abstract and title should make the scope clear. The claim that \"JoBS can also be adapted for LLM pretraining\" is unsupported speculation with limited evidence.\n\n**Questionable significance**: If this problem is important, why does no prior work address it? The baseline comparisons require running data mixture optimization then model training configuration optimization separately—no appropriate baseline jointly optimizing both exists, making it unclear if the 6-7% gain comes from joint optimization or simply more compute. The improvement is modest for 9+ GPU hours of optimization.\n\n**Missing practical guidance**: Critical hyperparameters (30 initial samples, Bsmall=50s, 64-width 3-layer MLP) appear arbitrary without sensitivity analysis. No guidance on when joint optimization justifies the computational cost versus using standard LoRA defaults. Scalability to larger models or longer training is unaddressed.\n\n**Lack of failure analysis**: When does the GP smoothness assumption fail? Theorem 4.1 assumes bounded RKHS norm and sub-Gaussian noise but never validates these empirically. Table 3 shows alternating optimization sometimes degrades performance—dismissed as \"saddle points\" without investigation."}, "questions": {"value": "How sensitive are results to the 30 initial samples, predictor architecture, and Bsmall? Can you provide guidance for practitioners on setting these for new scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fjbOpI3Tqw", "forum": "s5FME7A0TY", "replyto": "s5FME7A0TY", "signatures": ["ICLR.cc/2026/Conference/Submission10060/Reviewer_F3SD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10060/Reviewer_F3SD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966452880, "cdate": 1761966452880, "tmdate": 1762921455761, "mdate": 1762921455761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}