{"id": "NYUxN6plEh", "number": 12573, "cdate": 1758208704183, "mdate": 1759897501096, "content": {"title": "Learn to Merge: Meta-Learning for Adaptive Multi-Task Model Merging", "abstract": "Model merging in the pretrain-finetune paradigm has proven effective by combining multiple finetuned models into one with multi-task capabilities. Recent merging methods aim to boost merged models’ performance through strategies such as mitigating conflicts, adding trainable modules, and incorporating task-specific components. In most methods, the parameter merging procedure is based on Task Arithmetic, a widely used technique that creates task vectors from each finetuned model and linearly combines them with coefficients into consolidated model parameters. Except for studies specifically focusing on the merging coefficients, many other methods treat them as hand-tuned hyperparameters. However, the merging coefficients, which govern the entire merging process, including the subsequent module training, are empirically crucial for achieving optimal performance and tradeoff across tasks. Thus, this paper proposed an innovative model merging framework called MetaMerging, which constructs the merged model with a unified model and lightweight task-specific adapters. Specifically, the adapters are efficiently trained without labels via feature alignment with fine-tuned models, while the unified model is obtained by merging task vectors with coefficients adaptively optimized through meta-learning, which enhances the generalization and enables more effective adapter training. Extensive experiments on CV and NLP fields show strong performance of MetaMerging on various downstream tasks and demonstrate the effectiveness of meta-learning in our method compared to other parameter merging methods. Our code is available at https://anonymous.4open.science/r/MetaMerging-53A1", "tldr": "", "keywords": ["Model Merging", "Meta Learning", "Parameter Efficient Fine-Tuning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e5c6661abaa52c8a5c826a757a5792f3f19a325.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MetaMerging, a meta-learning-based framework for adaptive model merging in multi-task learning. Traditional model merging methods (e.g., Task Arithmetic, Ties-Merging, Surgery, AdaMerging) often rely on fixed or manually tuned merging coefficients when combining task vectors from fine-tuned models. The proposed method innovatively uses meta-learning to automatically optimize these merging coefficients, improving the generalization of the unified model and facilitating the training of task-specific adapters. Experiments on vision (ViT-B/32, ViT-L/14) and language (GPT-2) models show that MetaMerging achieves higher average accuracy across multiple tasks than prior merging methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The key contribution—using meta-learning to optimize merging coefficients—is conceptually elegant and fills a gap in existing merging methods.\n\nThis paper is well-written.\n\nFigure 3 is easy to follow."}, "weaknesses": {"value": "Although comparisons are made to AdaMerging, Surgery, and Pareto Merging, newer or hybrid model merging approaches (e.g., MoE-based fusion or gradient-space merging methods) are not included. The omission limits the claim of state-of-the-art performance.\n\nMeta learning requires backward transfer that needs the calculation of gradient, even second-order gradient, which is infeasible for large-scale language models, such as Qwen3 32B.\n\nThe collection of meta-train and meta-test sets is a challenge for powerful models, such as LLMs.\n\nThe improvements over Surgery are limited, while the training and memory requirements seem much higher than Surgery.\n\nTable 4 should include the training time of existing model merging methods."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "d2rzy3Fnk9", "forum": "NYUxN6plEh", "replyto": "NYUxN6plEh", "signatures": ["ICLR.cc/2026/Conference/Submission12573/Reviewer_9SxD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12573/Reviewer_9SxD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761142663286, "cdate": 1761142663286, "tmdate": 1762923426242, "mdate": 1762923426242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of merging multiple fine-tuned models into a unified multi-task model, aiming to enhance efficiency and performance across diverse tasks. The authors propose a meta-learning-based framework, MetaMerging, which adaptively learns optimal merging coefficients for task vectors, moving beyond fixed or heuristic merging strategies. This method allows the unified model to better preserve and leverage task-specific knowledge, facilitating improved adapter training and multi-task generalization. Experimental results on both vision and language benchmarks demonstrate that MetaMerging consistently outperforms conventional model merging techniques in accuracy and training efficiency. The work offers a principled approach for adaptive model consolidation, contributing to the advancement of scalable and robust multi-task learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality:**\n\nThe paper proposes a meta-learning approach for merging multiple fine-tuned models, offering a new way of adaptively learning merging coefficients rather than relying on fixed or manual strategies. This creative combination of meta-learning and multi-task model consolidation addresses a practical gap in existing literature.\n\n**Quality:**\n\nMethodologically, the framework is well-founded and experimentally validated across diverse benchmarks in both vision and language domains. The empirical analysis demonstrates consistent improvements over standard baseline methods, with clear, quantitative results supporting the core claims.\n\n**Clarity:**\n\nThe presentation is logical, with well-structured explanations, informative figures, and thorough comparative tables. While dense in the technical sections, the overall narrative is coherent and the methodology is transparent for readers familiar with deep learning.\n\n**Significance:**\n\nBy providing a principled solution for scalable model merging, the work has practical significance for resource-efficient deployment of multi-task systems. Its adaptive strategy enables better generalization and performance, contributing meaningfully to the progress of the field and addressing real-world challenges in model consolidation and transfer learning."}, "weaknesses": {"value": "- **Limited Novelty Relative to Existing Adaptive Merging:** Related ideas of adaptive weighting exist in ensemble learning and prior neural model merging. The paper would benefit from clearer differentiation and explicit discussion of how its approach surpasses past adaptive strategies (e.g., with more theoretical justification or unique robustness properties).\n- **Generalization Beyond Benchmarks:** The experiments, though thorough on selected datasets and architectures, are limited to standard benchmarks. To strengthen the validity of claims, additional studies on more diverse domains, truly large-scale settings, or real-world multi-task scenarios (with heterogeneous architectures or data) would be valuable.\n- **Accessibility and Implementation Details:** The methodology sections (3.1,3.2,3.3) remain technically dense, making it difficult for broader audiences to understand the key concept. I suggest author to provide a brief introduction to basically describe the motivation and purpose and methodology of this section. Being a high-level one for the user to understand the general idea. Then you can describe your detailed procedure or massive computation."}, "questions": {"value": "1. Can the authors provide evidence or commentary on how their meta-merging approach would handle truly large-scale, real-world scenarios with many diverse tasks, different model architectures, or highly imbalanced datasets? What specific challenges or modifications might arise when scaling beyond the standard benchmarks presented?\n2. Could the authors clarify how their meta-learning method differs fundamentally from previous adaptive weighting schemes in model merging and ensembling literature? Are there theoretical advantages, unique robustness properties, or empirical tests that distinguish MetaMerging as more than an incremental improvement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1Xmt5Nl1Xa", "forum": "NYUxN6plEh", "replyto": "NYUxN6plEh", "signatures": ["ICLR.cc/2026/Conference/Submission12573/Reviewer_5icd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12573/Reviewer_5icd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881370478, "cdate": 1761881370478, "tmdate": 1762923425908, "mdate": 1762923425908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a model merging technique with meta-learning of merging coefficients for training task-specific adapters. The study followed closely on how model merging problems and experiments have been defined and studied. The writing and organization of the paper are good overall."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The work investigates the method & impact of task-specific adapters for a multi-task model, and shows that the proposed method could slightly perform better than the compared works.\n2) The method is clearly described for meta-learning and adapters.\n3) The experiment is followed model merging literature closely."}, "weaknesses": {"value": "1) Figures 2 and 3 can be improved; it is difficult to understand without reading the entire paper.\n2) Missing SOTA comparison with the data-less model merging method, WUDI merging.\n3) The performance improvement against SOTA is marginal for VIT, despite requiring data, gradient, and additional adapters.\n4) In the NLP experiment, the proposed method is only compared with weak baselines (all before the end of 2023), and a relatively small GPT2 was used (recent model merging at least uses llama2/3)."}, "questions": {"value": "1) Table 4 should include the running times for all methods compared.\n2) It would be good to evaluate out-of-domain generalisation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kFIC030egq", "forum": "NYUxN6plEh", "replyto": "NYUxN6plEh", "signatures": ["ICLR.cc/2026/Conference/Submission12573/Reviewer_uyTM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12573/Reviewer_uyTM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916782106, "cdate": 1761916782106, "tmdate": 1762923425546, "mdate": 1762923425546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MetaMerging, a meta-learning framework for model merging under the pretrain-finetune paradigm. Instead of treating merging coefficients as fixed or manually tuned hyperparameters, the method meta-learns the coefficients so that the unified merged model yields better downstream task-specific adapter training. Experiments on vision and NLP tasks show improved unified and adapter-augmented performance over prior merging methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1, Motivated formulation. \\\nClearly identifies the overlooked importance of merging coefficients in task-vector–based model merging and motivates learning them dynamically.\n\n2, Method soundness. \\\nUses a meta-learning framework inspired by MAML to optimize coefficients for better downstream adaptation, which is conceptually meaningful and aligned with the merging problem structure."}, "weaknesses": {"value": "1, Adapter-dependent benefit: \\\nImprovements are strongest when adapters are added post-merge, raising questions on the intrinsic generalization of the unified model alone vs. the joint effect with adapters.\n\n2, Limited analysis on when meta-merging helps: \\\nThere is insufficient theoretical or empirical characterization of: \n\n2.1 when adaptive coefficients matter most, \n\n2.1 how task similarity affects meta-learning benefit, \n\n2.3 failure cases (e.g., conflicting tasks, negative transfer).\n\n3, Limited novelty vs. MAML:\n\nThe contribution mainly adapts MAML to the setting of merging coefficients. The algorithmic innovation is relatively incremental; most complexity lies in applying known meta-learning ideas to merging.\n\n4, Limited evaluation:\n\nThere should be some experiments on merging LLMs.\n\n5. Limited baselines:\n\nThere should be comprasion with lossless methods like: EMR-Merging, Talls-Mask and Free-Merging.\n\n6, The method still rely on data samples, which is not data-free and may face more challenges for LLMs."}, "questions": {"value": "1，Adapter structure comparability & design choices:\n\nThe performance gains appear closely tied to the adapter modules. Could the authors clarify: What specific adapter architecture is used (e.g., LoRA, bottleneck adapters, MoE heads)? Are adapter capacities kept strictly equal across baselines to ensure a fair comparison?\nHave you tested whether the method still holds with different adapter forms (e.g., shared vs. per-task adapters, low-rank variants, different insertion layers)? Since adapters play a key role in the pipeline, a more detailed justification and ablation of adapter design would strengthen the claim that improvements primarily come from better merging rather than from architectural choices.\n\n2，Compared with simple coefficient search:\n\nWhat is the actual meta-training overhead compared to simple coefficient search (e.g., CMA-ES or Bayesian tuning)? Is the method scalable to very large models or many tasks?\n\n3, How many data samples you used for feature loss computing, if the loss can be replaced by L1 or others ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MQLOKhrDq9", "forum": "NYUxN6plEh", "replyto": "NYUxN6plEh", "signatures": ["ICLR.cc/2026/Conference/Submission12573/Reviewer_oWra"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12573/Reviewer_oWra"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925797619, "cdate": 1761925797619, "tmdate": 1762923424560, "mdate": 1762923424560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}