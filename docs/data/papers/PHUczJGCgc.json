{"id": "PHUczJGCgc", "number": 12998, "cdate": 1758212588366, "mdate": 1759897471655, "content": {"title": "Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels", "abstract": "Inferring the physical properties of 3D scenes from visual information is a critical yet challenging task for creating interactive and realistic virtual worlds. While humans intuitively grasp material characteristics such as elasticity or stiffness, existing methods often rely on slow, per-scene optimization, limiting their generalizability and application. To address this problem, we introduce PIXIE, a novel method that trains a generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supervised losses. Once trained, our feed-forward network can perform fast inference of plausible material fields, which coupled with a learned static scene representation like Gaussian Splatting enables realistic physics simulation under external forces. To facilitate this research, we also collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and physic material annotations. Extensive evaluations demonstrate that PIXIE is about 1.46-4.39x better and orders of magnitude faster than test-time optimization methods. By leveraging pretrained visual features like CLIP, our method can also zero-shot generalize to real-world scenes despite only ever been trained on synthetic data. https://pixie-2026-12998.github.io/", "tldr": "", "keywords": ["Computer vision", "4D reconstruction", "Physics learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/62f3fecd94564295b3416ae6fd490091be4453bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes PIXIE, a generalizable feed-forward neural network that predicts physical properties from 3D visual features. The authors further introduce a semi-automatic data annotation pipeline and release PIXIEVERSE, a large-scale dataset with physical material annotations. Experimental results show that PIXIE achieves a 1.46–4.39× improvement in realism scores and faster inference than prior test-time optimization approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n2. Compared with prior test-time optimization methods, PIXIE is a generalizable feed-forward architecture that achieves higher realism scores while being substantially faster at inference.\n3. The authors introduce a semi-automatic data-annotation pipeline and release PIXIEVERSE, an open-source dataset of 3D objects with physical material annotations.\n4. Despite being trained on synthetic data, PIXIE generalizes effectively to real-world data by leveraging pretrained visual features (e.g., CLIP)."}, "weaknesses": {"value": "1. Instead of NeRF + CLIP + voxelization followed by transfer to 3DGS, why not learn 3DGS directly with distilled CLIP features, which could be more direct and efficient?\n2. Compared to previous methods such as DreamPhysics and OmniPhysGS, PIXIE relies on an auxiliary NeRF with distilled CLIP features, the acquisition of which can be time-consuming. It would be preferable to report the additional runtime overheads, including NeRF training, feature-field voxelization, and transfer to 3DGS.\n3. The authors employ a feature projector to map CLIP features onto a low-dimensional manifold, whereas in NeRF, the original high-dimensional features are retained, which may lead to inefficiencies in runtime and memory usage. Previous works have addressed this issue by either utilizing an autoencoder or PCA to compress CLIP features into a lower-dimensional space [1,2], or by introducing a learnable linear layer that maps low-dimensional 2D renderings back to high-dimensional feature maps [3]. As these approaches learn low-dimensional representations directly in 3D space, they could potentially offer greater efficiency than the design adopted by the authors.\n4. Since the PIXIEVERSE dataset contains only 10 semantic classes, it is unclear how well the model generalizes to unseen categories in real-world scenarios. This limitation may result in implausible physical parameter estimations or unstable visual outputs when encountering out-of-distribution objects.\n\n[1] Qin, Minghan, et al. \"Langsplat: 3d language gaussian splatting.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n[2] Yang, Jiawei, et al. \"Emernerf: Emergent spatial-temporal scene decomposition via self-supervision.\" arXiv preprint arXiv:2311.02077 (2023).\n\n[3] Zhao, Yanpeng, et al. \"Dynamic Scene Understanding Through Object-Centric Voxelization and Neural Rendering.\" IEEE Transactions on Pattern Analysis and Machine Intelligence (2025)."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AY0YK5tF71", "forum": "PHUczJGCgc", "replyto": "PHUczJGCgc", "signatures": ["ICLR.cc/2026/Conference/Submission12998/Reviewer_nTtB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12998/Reviewer_nTtB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760953761594, "cdate": 1760953761594, "tmdate": 1762923745651, "mdate": 1762923745651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a supervised framework that predicts physical properties of 3D scenes directly from visual inputs. The method, PIXIE, trains a feed-forward 3D U-Net on CLIP-distilled volumetric features to infer both discrete material types and continuous physical parameters from multi-view RGB images. These predictions can be coupled with Gaussian Splatting and simulated using the Material Point Method (MPM) to produce realistic physics-based animations. To support training, the authors introduce PIXIEVERSE, a dataset of 3D assets labeled with physical material annotations across 10 semantic categories. Experiments show that PIXIE achieves higher realism scores and is three orders of magnitude faster than test-time optimization baselines, while also generalizing zero-shot to real-world scenes despite being trained solely on synthetic data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces PIXIEVERSE, an open-source dataset of 1,624 3D assets annotated with physical material parameters, enabling future research.\n- The paper proposes the first supervised learning method that directly predicts both discrete material classes and continuous physical parameters (Young’s modulus, Poisson’s ratio, density) from 3D visual features, which enables faster inference than prior test-time optimization methods.\n- Extensive experiments on synthetic data and real-world data show the superior performance of the proposed method."}, "weaknesses": {"value": "- The PIXIEVERSE dataset relies heavily on semi-automatic annotations generated by vision-language models. Such labels may contain systematic biases or noise, and their accuracy is not quantitatively validated.\n- While Figure 4 reports a 2-second inference time, this does not account for the required NeRF/feature-field reconstruction step, which can be computationally expensive.\n- Although the paper claims zero-shot generalization on Spring-Gaus data, it omits comparisons against Spring-Gaus, instead asserting that \"no other baseline can generalize under this setting\".\n- Missing related work: It would be better if the author could compare with [1], which aims to estimate physical properties implicitly from videos.\n\n[1] Zhu X, Deng H, Yuan H, et al. Latent Intuitive Physics: Learning to Transfer Hidden Physics from A 3D Video. ICLR 2024."}, "questions": {"value": "1. In real-world environments with cluttered backgrounds and potentially moving objects, how does the proposed method identify and isolate the dynamic regions relevant for physical simulation? \n2. The paper states that each Gaussian in the Gaussian Splatting model is treated as an MPM particle (Sec. 3.1), but this mapping might be uneven, as splats are not uniformly distributed and often concentrate near visible surfaces. Could this cause inconsistencies in material distribution or simulation stability? Have any corrective strategies been applied to mitigate these issues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aNoWYQ6lpy", "forum": "PHUczJGCgc", "replyto": "PHUczJGCgc", "signatures": ["ICLR.cc/2026/Conference/Submission12998/Reviewer_wPyp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12998/Reviewer_wPyp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641813291, "cdate": 1761641813291, "tmdate": 1762923744881, "mdate": 1762923744881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a feed-forward model to directly predict material parameters for 3D objects, from an aligned and distilled CLIP field. In order to train the model, the paper also proposes a dataset with 3D objects and material annotation pairs, aligned by both VLM models and humans. The proposed model shows great efficiency and good results on synthetic dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Learning physics is meaningful and important for visual understanding and embodied AIs.\n\n2. The proposed method does not require test-time optimization, making it efficient in parameter estimation. \n\n3. The paper is well-organized and very easy to follow."}, "weaknesses": {"value": "Despite the strengths above, this paper has the following weaknesses:\n\n1. The motivation is problematic. The material parameters are purely predicted based on static semantics. Although the paper claims this as an advantage, this is a significant weakness in my opinion. \n\nFirstly, even for the same material, parameters can vary widely. For example, rubber can be either hard or soft. \n\nSecondly, material properties and mass distribution are interdependent, as demonstrated in [1]. This means that identical materials with different mass distributions can exhibit different \"equivalent\" parameters. For instance, the iron in a box can be made thicker, resulting in a higher effective Young’s modulus.\n\nFundamentally, these properties cannot be inferred from static semantic information alone. Therefore, the proposed method faces a theoretical limitation that cannot be overcome simply by scaling up or modifying the model under the current assumptions.\n\n2. Following weakness 1, the method lacks flexibility to adjust its predictions when the semantic-material relationship falls outside the training distribution. If the initial prediction is incorrect, additional observations will not correct it. For example, if the training set only includes hard rubber, the model will consistently predict rubber as hard, even when it appears soft in real-world videos.\n\n3. Baselines as Vid2Sim should be compared to demonstrate the basic assumption that only static images are needed. \n\n[1] Takuhiro Kaneko, Structure from Collision, CVPR 2025"}, "questions": {"value": "1. How to do define material type $\\ell$ in the proposed model? \n\n2. Since different materials need different parameters, how to decide which set of parameters to learn for different kinds of objects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WBjwO7HMpi", "forum": "PHUczJGCgc", "replyto": "PHUczJGCgc", "signatures": ["ICLR.cc/2026/Conference/Submission12998/Reviewer_74dK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12998/Reviewer_74dK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836998481, "cdate": 1761836998481, "tmdate": 1762923744437, "mdate": 1762923744437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel approach to predict physical parameters of a dynamic object to be used in MPM-based simulation. This is done by curating a large-scale paired synthetic dataset of dense annotations of physical parameters of dynamic objects, and training a feed-forward amortized inference model to predict the physical parameters, only considering visual input.\n\nIn short, we can consider this paper to be doing the following: it uses a VLM-based semi-automated pipeline to extract physical parameters, and uses a neural network to perform amortized inference. So basically, the model is trying to memorize how the semi-automated pipeline works by learning on some synthetic data. Therefore, I think the core contribution of this paper should be the data curation framework."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written, with a very clear flow of the story. The figures, as well as the supplementary website, are aesthetically pleasing and convey the results pretty well.\n- This paper is one of the first works that attempt to perform physical parameter prediction of 3D objects in a feed-forward manner, and this indeed leads to a reduced time for test-time optimization.\n- The qualitative results and renderings look very nice."}, "weaknesses": {"value": "- The collected dataset only contains 10 categories of objects. This is partially because of the highly engineered design of their data annotation framework - it seems to be very complex and ad-hoc, and I do not believe that it can be really scaled up to model a more diverse range of object categories, not to mention that there are a lot of objects that simply cannot be categorized into some categories.\n- There is no evaluation on the reliability of the data curation process. How reliable is the proposed framework? How hard is it to migrate to include more categories? An actionable plan would be collecting some real-world objects where you can measure the physical parameters, and compare the result produced by the data curation pipeline and the ground-truth physical parameter.\n- The evaluation protocol, as described in L323, is vague. What does \"manually verified by humans\" mean? How large is the test dataset?\n- Using VLM to judge a video's quality is not reliable. A user study is needed to show whether the generated video is really better than the baselines.\n- How can the evaluation be based on reconstruction quality, given that the physical parameters are inherently ambiguous, given only a single image? \n- Also, it's not fair to me to only compare on the proposed dataset, as the model is explicitly trained on this data, yet other methods do not have this information. \n- It would be very helpful if there could be quantitative comparisons with baselines on the real-world demos."}, "questions": {"value": "- How does the proposed model deal with the ambiguity of the physical parameter prediction? It now only takes multi-view images as input, and it is clearly not enough to determine the dynamic physical parameters. Will the neural network tend to produce averaged parameter sets? This leads to a larger concern about the scalability of the proposed pipeline - for a more complex dataset, there will be more ambiguities. How will this framework handle that?\n- Is it possible to combine video model-based physical parameter estimation with the proposed pipeline?  For example, is it possible to use PhysDreamer to annotate parameters for the dataset?\n- L274: unfinished sentence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QIOe892gdH", "forum": "PHUczJGCgc", "replyto": "PHUczJGCgc", "signatures": ["ICLR.cc/2026/Conference/Submission12998/Reviewer_dw3d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12998/Reviewer_dw3d"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979133721, "cdate": 1761979133721, "tmdate": 1762923744078, "mdate": 1762923744078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}