{"id": "PSIe9mmF7a", "number": 22423, "cdate": 1758330847140, "mdate": 1759896867226, "content": {"title": "Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test", "abstract": "As API access becomes a primary interface to large language models (LLMs), users often interact with black-box systems that offer little transparency into the deployed model. To reduce costs or maliciously alter model behaviors, API providers may discreetly serve quantized or fine-tuned variants, which can degrade performance and compromise safety. Detecting such substitutions is difficult, as users lack access to model weights and, in most cases, even output logits. To tackle this problem, we propose a rank-based uniformity test (RUT) that can verify the behavioral equality of a black-box LLM to a locally deployed authentic model. Our method is accurate, query-efficient, and avoids detectable query patterns, making it robust to adversarial providers that reroute or mix responses upon the detection of testing attempts. We evaluate the approach across diverse query domains and threat scenarios, including quantization, harmful fine-tuning, jailbreak prompts, full model substitution, showing that it consistently achieves superior detection power over prior methods under constrained query budgets.", "tldr": "", "keywords": ["LLM", "API auditing", "untargeted fingerprinting"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eeeb7f0a4415c0a1444ece46f65c5ab45f1e106d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a Rank-based Uniformity Test (RUT) for verifying whether a black-box LLM behaves identically to a locally deployed authentic model. The extensive experiments demonstrate that the proposed method is robust to quantization, harmful fine-tuning, jailbreak prompts, and even full model substitution."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This is an interesting and solid idea, and the theoretical formulation is elegant and well grounded. The method is flexible and can be applied across different scenarios (even though its performance sometimes appears less strong, which is understandable given the introduction of probabilistic substitution attacks)."}, "weaknesses": {"value": "- When I first saw the title, I actually thought of another line of work also related to auditing LLM APIs. In that problem setting, the auditor’s goal is to identify which base model or API underlies a released service, even when the service uses a fine-tuned version. The motivation there is to protect intellectual property and ensure robustness of auditing across model variants. Your task, however, seems slightly different. You aim to verify whether a claimed model has been altered, which emphasizes sensitivity rather than robustness. The distinction between these two auditing objectives initially caused some confusion. I think the two methods are not interchangeable, so it would be very helpful if you could more clearly describe their similarities and differences in the related work section. Although some fingerprinting-based studies are mentioned, the difference between the two problem settings is not clearly explained. It would also help if Section 3 could further clarify the specific auditing scenario considered in this paper.\n\n- My second concern relates to the applicability of the proposed method. The paper shows that RUT can easily detect discrepancies when jailbreak prompts are introduced, but this raises another question. It is quite common for service providers to add system instructions to improve model performance or to embed watermarks in outputs to protect intellectual property and prevent model misuse or content leakage. These are entirely legitimate and even responsible practices, which do not involve replacing the claimed model. Would such cases also be flagged as not the claimed model? If so, that seems problematic, as the underlying motivations are reasonable. Since many real-world services adopt these practices, such high sensitivity could make the method less practical. It may end up flagging a large number of legitimate services as inconsistent, thereby limiting its real-world utility."}, "questions": {"value": "- What are their similarities and differences between these two tasks?\n- What if the service provider adds system instructions or watermarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3XSJewM4ue", "forum": "PSIe9mmF7a", "replyto": "PSIe9mmF7a", "signatures": ["ICLR.cc/2026/Conference/Submission22423/Reviewer_1Ptp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22423/Reviewer_1Ptp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570681019, "cdate": 1761570681019, "tmdate": 1762942212977, "mdate": 1762942212977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper devises an improved statistical test procedure for detecting if textual model responses served to a user vary distributional in a significant way from a reference model, such that one should assume the model and its system has been replaced or modified (including by variations to a system prompt)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "+ The idea is fairly nifty, and appears to be a meaningful (if minor) improvement on other pre-existing techniques listed. \n+ Discrimination works with relatively few samples and limited assumptions (though there are some weaknesses in those)\n+ The evaluation is mostly fair and appropriate\n+ There is some potentially for this to be practically useful in comparing model APIs across providers under certain assumptions"}, "weaknesses": {"value": "- Could improve the evaluation of the false-alarm scenario, where a model is compared to itself (currently only showing self-pairs)\n- Worth showing the variance in evaluation with multiple draws of the 100 samples\n- Some minor issues with metrics and choices in the eval"}, "questions": {"value": "I'm largely fairly happy with this paper, within the bounds it sets itself.\n\nThere could be a little more sophistication in the evaluation, but this is far enough outside my expertise that I struggle to provide concrete suggestions myself—so take the below with a grain of salt.\n\nMy sense is that there's a minor conflict in choosing the scoring function on wildchat and then evaluating it against wildchat without a hold-out set. This is somewhat remediated by the test on other datasets. However, this only impacts the robustness of the choice of f and not the overall technique.\n\nThe other evaluation critique I have is with the choice of AUC as the key metric used for the final evaluations. A stealthy \"attacker\" might choose to route a relatively small number of prompts, and evaluating on AUC down-weights this regime. Could consider: power at small q values. Partial AUC integrated over small q. The smallest q where distinguishing is effective. \n\nFinally, worth being explicit about the threat model this paper is describing: you have to have access to the reference model, which narrows the scenarios under which this is useful. Likewise, with system prompt substitutions, this presumes there really is a true reference.\n\nDespite these critiques, I'm positive about the paper and thought it was appropriately scoped and evaluated. Room for improvement, but overall I think this result is one that should be in the literature. Good luck!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2MpAtNEp4W", "forum": "PSIe9mmF7a", "replyto": "PSIe9mmF7a", "signatures": ["ICLR.cc/2026/Conference/Submission22423/Reviewer_wtoi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22423/Reviewer_wtoi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682449859, "cdate": 1761682449859, "tmdate": 1762942212673, "mdate": 1762942212673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Rank-based Uniformity Test (RUT), a statistical method for auditing whether a deployed LLM API matches a claimed reference model. The key idea is to evaluate the rank percentile of an API’s response under the reference model’s response distribution and test for uniformity via the Cramér–von Mises statistic. The authors claim RUT is query-efficient, robust to adversarial rerouting, and effective under various threat models (quantization, jailbreak prompts, fine-tuning, and full model replacement). Extensive experiments are reported across open-weight models (Llama, Gemma, Mistral) and simulated API providers, showing RUT’s superior detection power over baselines such as Maximum Mean Discrepancy (MMD) and Kolmogorov–Smirnov (KS) tests."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The authors proposed a conceptually simple yet general test based on rank uniformity, adapting classical nonparametric tests (CvM, rank statistics) to LLM auditing.\n\n2. Experiments are extensive, covering several realistic scenarios (quantization, jailbreak, SFT).\n\n3. Implementation details are clear, with ablations on score functions and comparisons to reasonable baselines (MMD, KS).\n\n4. Figures and tables (especially Table 2, Figure 4) effectively summarize empirical trends."}, "weaknesses": {"value": "1. The proposed RUT is essentially an application of probability integral transform + CvM uniformity testing on log-rank scores — standard tools in nonparametric statistics. The methodological leap from MMD or KS is small; the main difference lies in choice of feature (log-rank) rather than test design. The paper frames RUT as “novel,” but it’s largely a recombination of well-known components. There is little theoretical innovation beyond empirical tuning.\n\n2. While the paper claims higher “statistical power,” the advantage appears numerically small and inconsistent across settings (see Table 2a, where 8-bit quantization detection remains near random). The results lack statistical significance testing (e.g., confidence intervals for AUC). Moreover, RUT’s improvements might stem from using richer local sampling (100× reference draws per prompt) rather than an inherently stronger test.\n\n3. The approach requires many reference samples per prompt (m=100), which is computationally expensive and unrealistic for auditing large APIs. The “query efficiency” claim is therefore misleading—it’s query-efficient only w.r.t. API calls, not total inference cost.\nThe assumption that the same decoding parameters and tokenizer are available is very strong; real APIs often obscure these details. The paper briefly mentions this but does not evaluate robustness under parameter mismatch or tokenization drift.\nThe adversarial rerouting model is simplistic (Equation 3) and not validated against actual API behaviors.\n\n4. No analytical characterization is provided for Type-I/Type-II error bounds or sample complexity under common substitution settings. The “uniformity under H₀” claim is intuitive but lacks formal proof of robustness when Fπref is empirically estimated. Without finite-sample guarantees, the method’s reliability remains unclear.\n\n5. Recent works such as Cai et al. 2025 (Are You Getting What You Pay For?) and Gu et al. 2025 (Auditing Prompt Caching) are only cited but not empirically compared. Many state-of-the-art auditing or watermarking methods (e.g., fingerprint-based ones like Pasquini et al. 2024) are ignored in experiments, weakening the completeness of the evaluation.\n\n6. RUT provides only a binary “reject or not” decision with no explanation of why a model differs. In practical auditing, it is crucial to pinpoint whether deviation stems from quantization, fine-tuning, or system prompt injection. The method offers no such diagnostic insight, limiting its operational usefulness.\n\n7. The paper repeatedly claims “robustness to adversarial rerouting” but provides no adversarial evaluation where the provider actively detects audit traffic. All experiments are offline simulations assuming passive substitution. The claim is therefore unsubstantiated.\n\n8. Several figures (especially AUROC plots in Appendix A.2) provide redundant visualizations without interpretation. The statistical terminology (e.g., “rank percentile under empirical CDF”) is used loosely. The method’s dependence on randomization (U~Uniform[0,1]) introduces variance but isn’t analyzed."}, "questions": {"value": "1. How sensitive is RUT to tokenization mismatches or decoding parameter drift between π_ref and π_tgt? Could small discrepancies trigger false positives?\n\n2. What is the theoretical sample complexity (number of prompts or reference draws) required to detect a given substitution level with 95% confidence?\n\n3. How does RUT perform when API responses are post-processed (e.g., truncation, moderation filtering)?\n\n4. Can the method scale to GPT-4/Claude-level APIs where reference sampling (m=100) is impractical?\n\n5. Can you clarify whether the rank-based test is truly undetectable by adversarial rerouting? Have you simulated a provider that dynamically switches models upon detecting repeated prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "plgfbPXt7l", "forum": "PSIe9mmF7a", "replyto": "PSIe9mmF7a", "signatures": ["ICLR.cc/2026/Conference/Submission22423/Reviewer_6Aub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22423/Reviewer_6Aub"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833169814, "cdate": 1761833169814, "tmdate": 1762942212322, "mdate": 1762942212322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a testing methodology to audit black-box LLM APIs. It aims to verify if a given API corresponds to a specific model or configuration, through model version, prompt cache behavior, and decoding parameters. Since commercial LLM APIs are opaque, the authors propose a statistical testing framework that uses carefully designed query-response pairs to detect discrepancies between a target (claimed) model and the API being audited. Experiment results show that the audit can detect subtle deviations from the claimed model with high sensitivity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation and practicality: the paper addresses an important and underexplored issue, the lack of transparency in commercial LLM APIs, and it works in a practical black-box setting.\n2. It formalizes the auditing task as a statistical hypothesis testing problem; also, the proposed Average AUROC algorithm is intuitive and effective for summarizing model separability without requiring access to logits or gradients."}, "weaknesses": {"value": "1. The auditing power heavily depends on how prompts are sampled. It remains unclear how different prompt types, for example, factual benign prompts vs. adversarial prompts, affect the sensitivity of the proposed method. \n2. The framework requires repeated querying of both the reference and target APIs, which is expensive and may be infeasible for large-scale or continuous audits.\n3. Changes in the decoding parameters, e.g., temperature or top-p, could artificially inflate AUROC values even when the underlying models are identical. This potential confound remains underexplored."}, "questions": {"value": "1. Why was AUROC chosen over other divergence-based measures like Jensen–Shannon or Wasserstein distances? \n2. Would prompts tailored to model weaknesses, for example, factual reasoning, yield more sensitive audits than random samples?\n3. How does the method distinguish between genuine model differences and sampling variability caused by small changes in temperature or top-p?\n4. How small a fine-tuning or low-rank adaptation can the audit reliably detect? Is there an empirical detection threshold?\n5. What’s the minimal number of queries needed to achieve a statistically significant audit result? Can the method be optimized to reduce query costs?\n6. If the target API changes gradually via some silent model updates, would the audit detect incremental drift or only sharp transitions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qsrYz5dafV", "forum": "PSIe9mmF7a", "replyto": "PSIe9mmF7a", "signatures": ["ICLR.cc/2026/Conference/Submission22423/Reviewer_v4bF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22423/Reviewer_v4bF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980252334, "cdate": 1761980252334, "tmdate": 1762942212097, "mdate": 1762942212097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}