{"id": "VCjlm003WL", "number": 16215, "cdate": 1758261806910, "mdate": 1763742320349, "content": {"title": "Characterizing Pattern Matching and Its Limits on Compositional Task Structures", "abstract": "Despite impressive capabilities, LLMs' successes often rely on pattern-matching behaviors, yet these are also linked to OOD generalization failures in compositional tasks.\nHowever, behavioral studies commonly employ task setups that allow multiple generalization sources (e.g., algebraic invariances, structural repetition), obscuring a precise and testable account of how well LLMs perform generalization through pattern matching and their limitations.\nTo address this ambiguity, we first formalize pattern matching as functional equivalence, i.e., identifying pairs of subsequences of inputs that consistently lead to identical results when the rest of the input is held constant.\nThen, we systematically study how decoder-only Transformer and Mamba behave in controlled tasks with compositional structures that isolate this mechanism.\nOur formalism yields predictive and quantitative insights:\n(1) Instance-wise success of pattern matching is well predicted by the number of contexts witnessing the relevant functional equivalence.\n(2) We prove the sample complexity upper bound of learning a two-hop structure: data sufficient for perfect in-domain generalization scales polynomially with token-set size. Empirical results align with the theoretical prediction, with an exponent stable under 20× parameter scaling and across architectures.\n(3) Path ambiguity is a structural barrier: when a variable influences the output via multiple paths, models fail to form unified intermediate state representations, impairing accuracy and interpretability.\n(4) Chain-of-Thought reduces data requirements yet does not resolve path ambiguity.\nHence, we provide a predictive, falsifiable boundary for pattern matching and a foundational diagnostic for disentangling mixed generalization mechanisms.", "tldr": "", "keywords": ["pattern matching", "compositional generalization", "functional equivalence", "coverage", "path ambiguity", "mechanistic interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f14c27becf0f3378fbd968136220adbeabf8876.pdf", "supplementary_material": "/attachment/35404b9ffdeb953829cad88fc3224481f4020685.zip"}, "replies": [{"content": {"summary": {"value": "This work presents a formalization of pattern matching by quantifying evidence for functional equivalence. The authors systematically study how strength of functional equivalence in the data leads to different kinds of success and failures in compositional generalization tasks. The formalization is accompanied by empirical evidence with both Transformers and mamba architectures on data scaling, learned internal representation of functional equivalence, and the role of CoT on a set of synthetic compositional generalization tasks. The results yield important insights on the limitations of compositional generalization supported by pattern-matching behavior, including generalizing to in-frequent data and multi-hop problems."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- This work makes a significant contribution in providing a formalization to the boundary of pattern matching and compositional generalization.\n- The paper is dense but I find it a good read. I appreciate that the authors studied a range of important issues beyond the base setting within the task suite, including testing two architectures, data/model scaling, interpretability, tasks with different compositional structures, and CoT.\n- The results make several interesting implications for the capabilities of larger models."}, "weaknesses": {"value": "- I think it would be good if the implications for specific LLM failures are discussed in more detail, as well as more discussions of whether certain aspects of the formalization would/would not apply given how natural language data may differ in properties studied in the synthetic settings. Though I understand this could be in part due to the space limit."}, "questions": {"value": "- For clarification, how is k computed if there is functional equivalence between more than two subsequences?\n- I'm curious what might happen if the models are trained on a mixture of different compositional tasks, or a mixture of compositional/non-compositional tasks? IMO this potentially captures the nuanced structures in natural languages, in which certain aspects are strongly compositional and others less so. Without explicit task cues, how would this change the learned strategy? Would functional equivalence be learned earlier or later? Would models still develop some form of context/task-dependent functional equivalence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NzWMw6BHHv", "forum": "VCjlm003WL", "replyto": "VCjlm003WL", "signatures": ["ICLR.cc/2026/Conference/Submission16215/Reviewer_ZpTF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16215/Reviewer_ZpTF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761351659734, "cdate": 1761351659734, "tmdate": 1762926374668, "mdate": 1762926374668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global comment (1/2)"}, "comment": {"value": "We genuinely appreciate all reviewers for their thoughtful and helpful feedback, which has significantly improved the paper. In this global comment, we would like (i) to recall the strengths of our work raised by reviewers, (ii) to summarize our updates and clarifications in our revised manuscripts as per the reviewers’ comments, and (iii) to highlight our major updates in our theoretical analysis established upon our formal framework.\n\n## Strengths\n\nWe are encouraged that the reviewers recognized several core strengths of the work:\n\n- **Problem and framework:** tackling compositional generalization via a “simple and naturalistic” (cspU) and “significant” (ZpTF) formalization of pattern matching (5m5w, ntJX).\n- **Experimental design:** using a “thoughtful” (ntJX) and “clean” (cspU) synthetic domain that isolates pattern-matching behaviors.\n- **Analysis:** providing “systematic” (ZpTF) analyses that “strongly support the narrative” (ntJX) and yield “important insights” (ZpTF).\n\n## Common Questions & Clarifications\n\nThe reviewers raised valid points regarding the framing of pattern matching and the work's connection to real-world implications. We summarize our updates and clarifications below:\n\n- **Stance on 'Pattern Matching' (ntJX, cspU):** We clarified that our view of pattern matching is **neutral** and its desirability depends on the definition of generalization (desirable for in-domain/interpolation, and insufficient for OOD/extrapolation). We have revised the abstract and the introduction to remove any inadvertently negative framing.\n- **Motivation for Synthetic/Controlled Setup (5m5w, ntJX):** We discussed why a controlled setup is methodologically important. It allows us:\n    1. to isolate the specific mechanism of functional equivalence from other confounds (like algebraic properties),\n    2. to derive precise quantitative boundaries (such as our scaling laws) that help explain LLM failures on real-world tasks, and\n    3. to provide a clean setting that can inform ongoing debates about which neural mechanisms may be required for systematic compositionality [1].\n- **Practical Implications (ZpTF, 5m5w, ntJX):** We elaborated on the practical implications, including:\n    1. quantitatively helping to explain the data-hungry nature of compositional tasks,\n    2. providing a theoretical basis for targeted data augmentation [2], and\n    3. offering a mechanistic hypothesis for well-known failure cases of LLMs, including the reversal curse [3] and negation failures [4]. We will expand this discussion in the final version, provided with additional space.\n\n## **Major Upgrade in Theoretical Analysis: Rigorous Proof of Scaling Law (Theorem E.6)**\n\nWe have significantly strengthened the theoretical foundation of our work by replacing the heuristic derivation in Section 6 with a **rigorous formal proof** (now **Theorem 6.1**). For a large enough vocabulary size $n$, the theorem provides a **sample complexity upper bound** $\\tilde{O} \\left(n^{2.5-\\frac{0.5}{k}}\\right)$ (ignoring polylogarithmic factors in $n$) to achieve perfect in-domain generalization with high probability, which is formally stated and proved in **Appendix E** in the updated revision.\n\nThe proof proceeds in three key steps to rigorously handle the dependencies in the data sampling process:\n\n1. **Reduction to Graph Connectivity:** We first prove (in Lemma E.9) a sufficient condition for perfect coverage of all ID data: it suffices to prove the connectivity of all evidence graphs (Definition E.5, whose vertices are certain input subsequences sharing the intermediate states, and edges represent functional $k$-equivalence between two subsequences) with high probability.\n2. **Poissonization:** To disentangle the dependencies inherent in fixed-size dataset sampling (characterized with a multinomial distribution), we apply Poissonization techniques. This allows us to model the edge formation in evidence graphs as independent events under a Poisson process, yielding upper bounds on the failure probability (Lemma E.11).\n3. **Random Graph Theory:** We identify the evidence graphs as instances of random $k$-intersection graphs under Poissonization. We then leverage the threshold results from the random graph theory literature to derive the sufficient sample size $N$.\n\nIn Appendix E, we actually provide more general sample complexity upper bound results under mild constraints about the cardinalities of the token sets (e.g., $\\mathcal{X}_1, \\mathcal{X}_2, \\mathcal{X}_3$) in Theorems E.6 (for $k\\ge 2$) and E.7 (for $k=1$). Combining these two and further assuming the balanced token set cardinalities at a rate of $\\Theta(n)$, we yield a simpler form of sample complexity as in Theorem 6.1 (or, Corollary E.8 for a more general form).\nWe have reflected this in the revision, with the updated sentences annotated in green.\n\nWe sincerely thank the reviewers again, and we hope our response addresses all points raised by the reviewers."}}, "id": "3txYDz4Zno", "forum": "VCjlm003WL", "replyto": "VCjlm003WL", "signatures": ["ICLR.cc/2026/Conference/Submission16215/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16215/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16215/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763742642331, "cdate": 1763742642331, "tmdate": 1763743804491, "mdate": 1763743804491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper characterizes “pattern matching” in sequence-to-sequence tasks as the identification of particular equivalences between pairs of certain input variables in sequences (subsequences of the input sequence), when all else is held equal. They investigate the ability of models to learn this kind of information by creating a synthetic domain where no other information exists, specifically by framing the problem as the composition of binary functions of the input, each of which is a random lookup table. In general, they find that transformers appear to be using this kind of pattern recognition when trained on this kind of data; but only when these equivalences appear several times. In general, in situations where such an equivalence is found, evidence can be found in intermediate layers for clustering. They develop a scaling law for the data needed to perform this kind of pattern matching. Additionally, they discover that having a non-tree-structured computation graph (where the same input variable can affect the output via multiple paths), causes pattern recognition to become much harder."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The domain setup seems to eliminate other potential sources of information cleanly. The definition of functional equivalence and specifically k-equivalence are simple and naturalistic definitions.\n\nThe large sweep over a variety of dataset sizes is also helpful for determining the role of data access."}, "weaknesses": {"value": "The abstract and first paragraph of the introduction do not make it clear enough that “pattern matching\" is undesirable. The first sentence could be read as “pattern matching\" performed by LLMs as being too surface level. This reading recontextualizes later uses of the term to be neutral rather than negative, confusing such a reader. It should be made more clear that “pattern matching\" specifically is being used to exclusively refer to undesirably syntactic/surface level heuristics.\n\n\"Functional equivalence, i.e., substituting input fragments observed to result in identical outputs in shared contexts” (used in the abstract and introduction lede) should be rephrased as, by itself, this is too vague to communicate what it means to someone who has not read the paper.. Perhaps something like “functional equivalence, i.e., identifying pairs of subsequences of inputs that consistently lead to identical results when the rest of the input is held constant.”\n\n“Tightly ordered by” is used several times, but as far as I can tell is not a standard term. I think a term like “well predicted by” would be more standard and thus easier to read."}, "questions": {"value": "Pattern matching is generally defined in terms of substitution rules (i.e., $a * (b + c) \\to a * b + a * c$. Is the notion of pattern matching you define equivalent to unbounded substitution rules (with an unlimited number of variables on each side of the $\\to$)? Is it a generalization? Is it a subset? A discussion of this would help ground your definition in the context of existing notions of pattern matching.\n\nWhat is the purpose of holding out 30% of inputs and having a test condition where you add a new one in? In my understanding, this task is impossible to perform above chance on, as the primitives are defined as random tables."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZD5WRQYcc1", "forum": "VCjlm003WL", "replyto": "VCjlm003WL", "signatures": ["ICLR.cc/2026/Conference/Submission16215/Reviewer_cspU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16215/Reviewer_cspU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854626756, "cdate": 1761854626756, "tmdate": 1762926374127, "mdate": 1762926374127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is about when LLMs can learn many-to-one (or, equivalently, non-injective) maps. A many-to-one map is a function such that two different inputs yield the same output. All maps are deterministic. In their experiments, an LLM is trained on many examples of such functions (sometimes composed with each other) and is asked to produce outputs on unseen inputs. For example, if we see $f_1(a) = f_1(b) = c$ and $f_2(f_1(b), d) = e$, we can infer that $f_2(f_1(a), d) = e$ as well, without having seen this particular example before. \n\nThey use many-to-one maps as a formalization of what pattern matching means in LLMs. They then create quantifiable tests for when pattern matching is or is not happening. They also explore how data requirements scale with the complexity of the task, and how the amount of computation necessary to learn pattern matching scales with the size of the input vocabulary. Each section of the paper is summarized as follows:\n\n- Their first experiments show how the number of times an invariance is observed affects the test accuracy on the many-to-one maps. Despite the fact that just two observations of an invariance is sufficient to implement an algorithm that solves the task perfectly, the LLM only begins to generalize well until after more examples of the invariance are seen (3+, depending on the amount of training).\n- They next develop a metric for measuring how well internal representations capture shared states. In particular, one might hope that if two function inputs lead to the same output, the internal representations would be similar after a few layers. Similarly, if two function inputs lead to different outputs, they internal representations should be different at all points. They capture this by measuring cosine similarity between inputs that induce the same output and cosine between inputs that induce different outputs and then taking the difference between these two quantities. They then show that the models trained on their many-to-one task exhibit high values on the metric.\n- They next consider how much data should be needed to learn the task, for a given input vocabulary. They show that, depending on how many examples are needed to confirm an invariance, the number of examples needed scales as polynomial in the size of the domain, with exponent between 2 and 2.5. By scaling the input size, they verify empirically that these scaling laws hold.\n- When the function composition structure is more complex (the computation graph is not a tree, and is instead some general directed graph), they show the LLMs do not perform well. This is because it is harder to generalize when the same token is used for multiple different arguments to a function.\n- They also train the models to output intermediate computations and find that it leads to more data-efficient generalization. However, when learning with general graphs rather than trees there is still no generalization."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- At a high level, thinking about generalization in terms of many-to-one functions seems like it clearly captures a kind of task-level generalization. Completing the task correctly requires non-trivial logical reasoning. The task has the nice properties that (1) it is possible to get 100% accuracy when correctly applying logical reasoning / a graph algorithm and (2) the LLM never sees the exact problem instance it is evaluated on.\n- The empirical results in the paper strongly support the narrative presented. The setups of the experiments are thoughtful, and the analysis is suitable for the questions addressed in the paper. I would say the first results (that more examples of an invariance are helpful) is not very surprising, but I learned a lot from the second through fourth analyses (scaling laws, non-tree tasks, and CoT)."}, "weaknesses": {"value": "- I found the exposition introducing the problem to be a bit confusing. It wasn’t clear to me whether pattern matching is a desirable or undesirable property of transformers (is it capturing overfitting or generalizing?) The abstract suggests that surface-level pattern matching is bad, but perhaps that deeper pattern matching (which survives multiple logical steps) is a good thing.\n- I am also confused about why it is interesting to understand pattern matching in LLMs. I’m not sure how the toy problem is supposed to generalize to other tasks, or shed insight into broad phenomena around LLM reasoning. The introduction did not really have examples of pattern matching in real-world LLM uses, so this added to the confusion. There were many citations supposedly about pattern matching, but not any explanation of how we are supposed to imagine pattern matching works beyond the toy problems in this paper itself. I believe there could be a good motivation for studying this problem, but I don’t get it from this paper, and I’d like the authors to explain.\n- I think there could be more discussion of what makes the non-tree case hard. Is there not a way to reduce the non-tree task to the tree task? Or is generalization impossible because of the same token is used for multiple arguments.\n- Another minor critique I have is about terminology: “functional equivalence” to me suggests that two functions are the same. I think “functional invariance” is a clearer term?"}, "questions": {"value": "What are the class of real-world problems or tasks that being good at pattern matching helps solve? Is it supposed to capture logical reasoning? Why this particular logical reasoning task versus another (like syllogisms or SAT problems)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cuZ1aKSoCE", "forum": "VCjlm003WL", "replyto": "VCjlm003WL", "signatures": ["ICLR.cc/2026/Conference/Submission16215/Reviewer_ntJX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16215/Reviewer_ntJX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942504085, "cdate": 1761942504085, "tmdate": 1762926373189, "mdate": 1762926373189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper formulates functional equivalence as the condition where two sets of tokens produce the same output in the same context. The experimental results demonstrate that the model's performance and internal representation of these equivalences get stronger as more of these functional equivalences occur in the training data. The paper also analyzes the scaling law (required training data size depends on the vocabulary size), the effect of ambiguous composition, and the same phenomenon in CoT training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper studies an important problem of the compositional generalization of language models.\n\n2. The experiments include various settings of practical relevance."}, "weaknesses": {"value": "1. The results are limited to small synthetic task structures.\n\n2. The settings require a deterministic function and strict functional equivalence, which may be too restrictive in a real-world NLP dataset."}, "questions": {"value": "1. If the task structure is more complicated, would observing robust functional equivalence still be enough for compositional generalization? Wouldn't task complexity challenge the model's understanding of the task, resulting in training failure?\n\n2. Is there any equivalent empirical observation where compositional generalization requires robust observation of functional equivalence in a real-world NLP dataset?\n\n3. Would the formulation in the paper generalize to stochastic functions and approximate functional equivalence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z3Q66OWhLd", "forum": "VCjlm003WL", "replyto": "VCjlm003WL", "signatures": ["ICLR.cc/2026/Conference/Submission16215/Reviewer_5m5w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16215/Reviewer_5m5w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968213833, "cdate": 1761968213833, "tmdate": 1762926372338, "mdate": 1762926372338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}