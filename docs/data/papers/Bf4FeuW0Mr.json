{"id": "Bf4FeuW0Mr", "number": 3128, "cdate": 1757339026818, "mdate": 1759898107642, "content": {"title": "DemoGrasp: Universal Dexterous Grasping from a Single Demonstration", "abstract": "Universal grasping with multi-fingered dexterous hands is a fundamental challenge in robotic manipulation. While recent approaches successfully learn closed-loop grasping policies using reinforcement learning (RL), the inherent difficulty of high-dimensional, long-horizon exploration necessitates complex reward and curriculum design, often resulting in suboptimal solutions across diverse objects. We propose DemoGrasp, a simple yet effective method for learning universal dexterous grasping. We start from a single successful demonstration trajectory of grasping a specific object and adapt to novel objects and poses by editing the robot actions in this trajectory: changing the wrist pose determines where to grasp, and changing the hand joint angles determines how to grasp. We formulate this trajectory editing as a single-step Markov Decision Process (MDP) and use RL to optimize a universal policy across hundreds of objects in parallel in simulation, with a simple reward consisting of a binary success term and a robot–table collision penalty. In simulation, DemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow Hand, outperforming previous state-of-the-art methods. It also shows strong transferability, achieving an average success rate of 84.6% across diverse dexterous hand embodiments on six unseen object datasets, while being trained on only 175 objects. Through vision-based imitation learning, our policy successfully grasps 110 unseen real-world objects, including small, thin items. It generalizes to spatial, background, and lighting changes, supports both RGB and depth inputs, and extends to language-guided grasping in cluttered scenes.", "tldr": "", "keywords": ["dexterous grasping", "reinforcement learning", "sim-to-real"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/65e49d834229b903bde3ffdba99aa12be1c5322e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a method for training dexterous grasping policies from a single demonstration in simulation. The key idea is to frame the problem as demonstration editing, where a reinforcement learning (RL) policy learns to generate residual corrections that enable grasping of novel objects. This formulation helps address the exploration challenge and leads to more efficient learning. Extensive real world evaluations are carried out to validate the method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and easy to follow.\n\nThe proposed method is conceptually sound and addresses an important problem in dexterous manipulation."}, "weaknesses": {"value": "1. It is unclear what range of novel objects this approach can effectively handle given only a single demonstration on one object. For example, if the demonstration involves grasping a bottle, it may be difficult to transfer that experience to grasping a flat plate. It would be valuable to include experiments illustrating which types of object transfers are successful, and to discuss how such transferability might be predicted a priori.\n\n2. The computational efficiency of the proposed method appears to remain high compared to sampling-based or template-based approaches. Including quantitative comparisons of computational cost and a discussion of when this approach would be preferable to these alternatives would strengthen the paper.\n\n3. The high-level idea is not that surprising, given recent papers like DexMachina."}, "questions": {"value": "See Weakness 1. I would like to see more discussion on this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gm3e17Bcf4", "forum": "Bf4FeuW0Mr", "replyto": "Bf4FeuW0Mr", "signatures": ["ICLR.cc/2026/Conference/Submission3128/Reviewer_kYw5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3128/Reviewer_kYw5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956706904, "cdate": 1761956706904, "tmdate": 1762916563143, "mdate": 1762916563143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method to learn dextrous grasping from just a single demonstration. The key differentiator with existing approaches is to formulate the problem as a 1 step markov decision process where the policy predicts a residual trajectory acting as a demonstration editor. The entire trajectory is treated as a single action executed using a lower level motion planner. This makes exploration and reward design much more simpler than imitating the entire trajectory. Once a state-based policy is trained in simulation, a vision-action dataset is collected to then learn a vision-based policy using behavior cloning.\nThe results show generalization to novel objects as well as zero shot transfer to real world robots."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well structured and easy to follow. It provides sufficient background, comparison to existing literature and clearly establishes the motivation for the paper. \n\nThe paper provides sufficient details about training, provides comprehensive quantitative comparison against relevant baselines across different object datasets and robot embodiments. The results and the supplementary videos show effective grasp success rates. \nThe paper also presents in depth ablation studies that sheds light on the necessity for RL instead of sampling based methods, the action space used in RL, camera configurations as well as demonstration quality. The proposed method performs better than baselines in all scenarios. \n\nAn additional strength of the paper is real-world results which is the ultimate test for any robot learning method."}, "weaknesses": {"value": "While the results are promising, I think the biggest weakness is the lack of generalizability of the proposed method to more dynamic environments where a reactive policy might be required to avoid collisions or adjust a grasp mid motion.\n\nAdding details about how the language conditioned policy is learned can be extremely beneficial for a reader. Similarly, more details on the lower level motion planner that executes the trajectory would be really helpful."}, "questions": {"value": "1) The problem formulation is not too clear - In section 3.1, the authors mention that the goal of the policy is to maximize the discounted sum of returns (like standard RL), and the equation suggests that the horizon “T” is the length of the demonstration trajectory. But this is at odds with the single step MDP that the authors claim in the beginning. Even in table 12, the episode length is set to 1. How exactly is the problem formulated and what is the discount factor?\n\n2) What is the motion planner used? How robust is the policy to the motion planner?\n\n3) The vision based policy is closed loop per grasp attempt, correct? What is the frequency of that loop? \n\n4) It would also be great to know how tactile feedback can be incorporated into this framework in future work section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G9b6poJxuQ", "forum": "Bf4FeuW0Mr", "replyto": "Bf4FeuW0Mr", "signatures": ["ICLR.cc/2026/Conference/Submission3128/Reviewer_mozF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3128/Reviewer_mozF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968495568, "cdate": 1761968495568, "tmdate": 1762916562977, "mdate": 1762916562977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows framework for dexterous grasping policies using RL with single demonstration augmented. The policy could grasp a huge amount of objects and its appreciate the author conduct steps for sim-to-real, instead of simply reply the trajectory in simulation. The trajectory dataset and the sim-to-real for these trajectories could be a good contribution."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Appreciate the sparse reward and simple collision penalty. Easy to read and follow the idea. The presentation is good. The experiments are comprehensive."}, "weaknesses": {"value": "Minor weakness: \n\n1. the abstract contain many contents but not organize well. It get reader confuse that the author using RL in simulation but directly switch to imitation learning in the real-world without any context.\n\n2. line 090, please specify what is the \"prior methods\" mentioned, eg cite the corresponding paper.\n\n3. line 105-106, please specify what is \"previous state-of-the-art-methods\" here.\n\n4. Front size in figure 1 is too small to recognize.\n\n\nMajor weakness:\n\n1. when perform sim-to-real transfer, the author select the successfully RL rollout, then apply an additional training stage and apply domain randomization in imitation stage. why not directly train a vision based policy and directly transfer to the real. \n\n2. For two stage training, why select stage based RL and do a selection and then do BC? Have you ever research train a state based RL first and then perform a teacher-student pipeline to distill a vision policy, which can also be able to transfer to real.\n\n3. For the sim to real, the author mention the domain randomization for the RGB and depth. Have you apply any method on robot control side. eg. simulation hand is usually more compliant than real hand, especially for the inspire hand, which may like to broken if the collision is too large. Also for the arm, have you notice any control gap between sim and real.\n\n4. Would be more interested the pipeline and application to extend to beyond grasping task. \n\n5. How much randomization for the object initial pose? For a simple demo augumentation, does the larger randomization area decrease the max success rate of RL? \n\n6. While appreciate conduct the BC for sim-to-real, more experiments to justify the BC training would make experiments more comprehensive. Eg. better generalization, or better design choice, or help on implementation."}, "questions": {"value": "See weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rc4FL3o7Gs", "forum": "Bf4FeuW0Mr", "replyto": "Bf4FeuW0Mr", "signatures": ["ICLR.cc/2026/Conference/Submission3128/Reviewer_QqKj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3128/Reviewer_QqKj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970832629, "cdate": 1761970832629, "tmdate": 1762916562418, "mdate": 1762916562418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DemoGrasp, a framework that first augments demonstrations with pose and object randomization and a reinforcement learning (RL) residual policy, and then distills the results into an imitation learning policy. Extensive experiments are conducted in both simulation and the real world. The ablation studies on RL-sampling comparison, camera configuration, and RL action spaces are well designed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The experiments are extensive, and the real-world demonstrations looks robust.  \nThe analysis of RL-sampling comparison, camera configuration, and action spaces is insightful.  \nRL residual learning and RGB sim-to-real transfer are challenging, and the paper addresses them successfully.  \nThe presentation of the paper is clear and well structured."}, "weaknesses": {"value": "- One weakness is that more details about the experimental settings should be provided, like RGB randomization details in the paper.\n- It is suggested to replace some edited videos on the website with uncut, continuous grasping videos."}, "questions": {"value": "- It would be helpful to discuss more recent related works in the paper.  \n- The quality of figures and videos can be improved, preferably with higher resolution.\n- It would be good to include an open-source commitment in the paper to increase its reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CEdt41UXPr", "forum": "Bf4FeuW0Mr", "replyto": "Bf4FeuW0Mr", "signatures": ["ICLR.cc/2026/Conference/Submission3128/Reviewer_uPY2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3128/Reviewer_uPY2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037563309, "cdate": 1762037563309, "tmdate": 1762916561466, "mdate": 1762916561466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}