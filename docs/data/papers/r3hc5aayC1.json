{"id": "r3hc5aayC1", "number": 15295, "cdate": 1758249944614, "mdate": 1763689490594, "content": {"title": "Class-Adaptive Rectification with Experts for Robust Long-Tailed Noisy Label Learning", "abstract": "Real-world datasets frequently exhibit long-tailed class distributions alongside noisy labels, posing compounded challenges for robust learning.\nWhile recent methods have made progress, they often neglect the uneven impact of label noise across classes, resulting in insufficient correction for tail classes.\nThis imbalance further introduces erroneous over-regularization on other classes, ultimately undermining long-tailed learning.\nTo address these challenges, we propose Class-Adaptive Rectification with Experts (CARE), a parameter-efficient framework built upon visionâ€“language models, which performs class-aware label correction by jointly leveraging three complementary sources of supervision: noisy observed labels, text embeddings, and image features. \nCARE further employs a class-adaptive Top-$K$ expert consensus mechanism, which assigns smaller $K$ to tail classes in order to extract reliable candidate labels and recalibrate class frequencies.\nThis refinement yields faithful class-frequency estimation, thereby enabling more reliable long-tailed calibration.\nWe evaluate CARE on CIFAR-100-LTN, mini-ImageNet-LTN, and real-world datasets, including Food101N and WebVision-50. \nAcross all benchmarks, CARE consistently surpasses recent state-of-the-art methods, achieving up to 3.0\\% accuracy improvements in certain settings.\nThe source code is temporarily available at https://anonymous.4open.science/r/CARE-9F10.", "tldr": "Long-Tailed Noisy Label Learning", "keywords": ["long-tail learning", "noisy label learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc0ed0e06a6f46ae3b298131018cd3acc70ab3f1.pdf", "supplementary_material": "/attachment/856d200f223c7f489a09e2ddd9c3ae4ce26f2e54.zip"}, "replies": [{"content": {"summary": {"value": "The paper propose a method for label correction by merging information from text-image alignment, pseudo label and noise label. Also it use class adaptive strategy to apply different level of noise correction for class of different frequency.  The experiments are conducted on both synthetic data and real data. Compared with CLIP based methods, it can bring further improvement. However, there lack ablations to support the key claims, and there missing some details for the whole pipeline. The suggest is to reject."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method merges priors in text/image alignment(TE), pseudo labels (IE) and noise labels (BE) to generate a better label closer to ground truth distribution, yielding to better results. Also it emphasizes different impact of label noise across classes."}, "weaknesses": {"value": "The motivation sounds good but there lack experiments supporting the claim. \nWhy the tail classes requires more corrections. There is no analysis showing something like marginal benefit when applying label corrections for head classes.\nHow the proposed method apply more corrections on the tail classes? There is a theorem (Proposition 3) but the proof for it is empirical and there is no comparison for different settings for an actual proof. \n\nSee questions part below for other concerns"}, "questions": {"value": "1. How is the NR calulated in Table1. From figure 1 it seems RLD 1,2,3 have much lower noise level than reported in table 1. \n2. What is NR for data at different frequency in RLD 1,2,3 in Table1? \n3. For the proof of Theorem 3, the proof seems empirical but not a strict proof. Like \"Since tail class t has\nlow sample frequency, ... Thus, Pt(K) grows faster than Tt(K). \"  How low frequency class t should be to make it happen? At least Pt(K), Tt(K) should be written in a function of class frequency. Otherwise it is hard to tell whether the claim is correct\n4. The relationship between Kc and class frequency n_c is the key for class aware noise controlling. How the choice of the relationship affect the results? Like using a global K for all the class, what the actual result would be?  Will the noise level of tail class increase or decrease?\n5. The name of Table 7 and Table 8 should exchange?\n6. In Table 7, the first three rows have exact the same NR and accuracy, which is strange.\n7. Does the improvement come from the class-aware noise assignment or the expert mixing strategy? From table 1 and figure 1, the RLD 3 have more noise level at the \"Many\" class than RLD 1,2 but it still can achieve better accuracy at head classes. \n8. In Figure 2 case1, there is a class with 0.4 probability in the IE part,  which is removed in the final accumulation. I don't find the corresponding strategy in the paper. Is that a mistake in the figure?\n9. The pipeline remains unclear to me. The AdaptFormer and w is for calculation of IE, and used in label correction. After that, which part of the model will be trained using the corrected label to get the final results? How is the AdaptFormer and w initialized? Do they need cold start training using the noise label?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Diy2yNtULn", "forum": "r3hc5aayC1", "replyto": "r3hc5aayC1", "signatures": ["ICLR.cc/2026/Conference/Submission15295/Reviewer_oswe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15295/Reviewer_oswe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911617989, "cdate": 1761911617989, "tmdate": 1762925590639, "mdate": 1762925590639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work is about learning from datasets that have long tailed class distributions and label noise. The method proposed is to leverage three complementary experts (text, image, and observed labels) with a class adaptive top K consensus mechanism to correct noisy labels. The central argument is that tail classes require more conservative consensus (smaller K) to avoid confirmation bias, while head classes can tolerate more greater consensus (larger K)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The identified problem is timely and interesting. The class agnostic label correction can actually harm performance by insufficiently correcting tail class labels.\n+ The theoretical analysis is interesting and showing how consensus based refinement amplifies reliability. It provides good intuition for the design choices.\n+ While missing some standard datasets in this task like CIFAR-10, the experiments are thorough enough, they spans on CIFAR-100-LTN, and mini-ImageNet-LTN, which are synthetic datasets, and real world datasets like Food101N, WebVision-50 under various noise types and imbalance factors. Results consistently show improvements, particularly notable gains for severe imbalance scenarios.\n+ It is well presented."}, "weaknesses": {"value": "- While the combination is novel and backed by the theoretical analysis, the individual components, like using CLIP for label correction, expert consensus, and top k voting have been explored previously . The main contribution is the class adaptive mechanism, which while effective, represents an incremental advance.\n- Compared to the state of the art evaluations, real world noise evaluation is limited to webvision50 and food101n. Using Clothing1M would strengthen the claims.\n- The method fundamentally relies on CLIP, limiting applicability to domains where such pre-trained models aren't available or perform poorly.\n- The proof of Theorem 1 assumes conditional independence between pTE and pIE, but this assumption may not hold since both derive from CLIP's shared vision language space.\n\nMinor:\n- Eq 4 appears complex and could benefit from clearer notation."}, "questions": {"value": "- how to handle edge cases e.g. when all experts disagree completely or when Kc becomes 0 for extremely rare classes?\n- can you discuss failure cases or provide error analysis to understand when the proposed method might underperform?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jwlRbE0r5R", "forum": "r3hc5aayC1", "replyto": "r3hc5aayC1", "signatures": ["ICLR.cc/2026/Conference/Submission15295/Reviewer_oKDJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15295/Reviewer_oKDJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941186241, "cdate": 1761941186241, "tmdate": 1762925590203, "mdate": 1762925590203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents \"Class-Adaptive Rectification with Experts (CARE),\" a framework for robust learning in the context of long-tailed noisy label (LTNL) problems. It addresses the challenges of label noise and class imbalance, particularly in tail classes, by leveraging a class-aware Top-K expert consensus mechanism. The proposed method integrates noisy labels, text embeddings, and image features to improve label correction and class frequency estimation. Experimental results on multiple benchmarks, including CIFAR-100-LTN and real-world datasets, show that CARE consistently outperforms existing state-of-the-art methods, achieving significant accuracy improvements. The authors provide a comprehensive analysis of their approach and its implications for future research."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Innovative Approach: The CARE framework effectively combines label rectification with class-adaptive techniques, addressing both noisy labels and long-tailed distributions.\n\nComprehensive Evaluation: The framework is tested on various benchmarks, demonstrating consistent performance improvements over state-of-the-art methods.\n\nStrong Theoretical Basis: The paper provides solid theoretical justifications for its design choices, enhancing its credibility and understanding."}, "weaknesses": {"value": "1. The CARE framework may require significant computational resources due to the integration of multiple expert models, which could limit its practicality in resource-constrained environments.\n\n2. The evaluation primarily focuses on specific datasets, raising concerns about the framework's adaptability to other domains or types of noise not covered in the experiments.\n\n3. The emphasis on correcting tail class labels could lead to overfitting, especially if the tail classes are too small or underrepresented in the training data.\n\n4.  The reliance on accuracy as the sole performance metric may overlook other critical aspects of model performance, such as precision, recall, or F1-score, particularly in imbalanced datasets."}, "questions": {"value": "How does CARE perform with larger datasets?\n\nCan it handle other noise types or distributions?\n\nWhat can be done to prevent overfitting on tail classes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZH7c6UgMVp", "forum": "r3hc5aayC1", "replyto": "r3hc5aayC1", "signatures": ["ICLR.cc/2026/Conference/Submission15295/Reviewer_JGXp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15295/Reviewer_JGXp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051648315, "cdate": 1762051648315, "tmdate": 1762925589776, "mdate": 1762925589776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}