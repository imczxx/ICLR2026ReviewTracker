{"id": "tAJRXq7BZp", "number": 20809, "cdate": 1758310442092, "mdate": 1763622196841, "content": {"title": "Transformer-based Unsupervised Graph Domain Adaptation", "abstract": "Unsupervised Graph Domain Adaptation (UGDA) addresses the challenge of domain shift in transferring knowledge from a labeled source graph to an unlabeled target graph. The existing UGDA methods are based solely on graph neural networks (GNNs) with limited receptive fields. This characterization of UGDA methods constrains their ability to capture long-range dependencies between domains and effectively adapt to sparse graph domains. To overcome this limitation, we introduce a novel transformer-based UGDA (TUGDA) framework that sequentially integrates transformers and asymmetric GCNs to capture both global and local structural dependencies between source and target graphs. Our framework leverages a transformer backbone, enriched with centrality and spatial positional encodings to enhance structural information. We further propose a new cross-attention mechanism that explicitly aligns source and target representations, along with theoretical analysis for reducing domain divergence through Wasserstein distance minimization. Extensive experiments on six cross-domain tasks in three real-world homophilic citation graphs show significant improvements over SOTA UGDA baselines. Our results validate TUGDA's ability to learn transferable, domain-invariant representations. Critically, to address practical scenarios where privacy or security constraints restrict access to real source domains, we demonstrate that TUGDA maintains strong performance using synthetic source graphs generated by a foundational model, outperforming leading baselines by up to 15% across six cross-domain tasks.", "tldr": "", "keywords": ["Unsupervised Graph Domain Adaptation", "Graph Transformer", "Cross-Attention", "Transfer Learning", "Graph Neural Network"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3cddcfbcb49f6cce3ec494b9027b73680d263d15.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TUGDA, a novel framework for Unsupervised Graph Domain Adaptation (UGDA) that addresses the limited receptive fields of existing GNN-based methods. It proposes a sequential architecture that first uses a transformer backbone, enhanced with centrality and spatial positional encodings, to capture global long-range dependencies, and then refines these representations with an asymmetric GCN to incorporate local structural information. A key contribution is a new cross-attention mechanism that directly aligns source and target node representations, which is theoretically shown to minimize domain divergence. Experiments demonstrate that TUGDA achieves good results on standard citation graph benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. This is the first work, to my knowledge, to apply a transformer-based architecture to the UGDA problem. The motivation to use transformers to overcome the locality bias of GNNs is clear and well-argued. The sequential combination of a global-aware transformer and a local-aware GCN is an elegant way to capture both scales of graph information.\n\n2. The inclusion of theoretical analysis in Section 5 adds technical depth. While the DA bound itself is standard, the authors do a good job of connecting their specific architectural choices (cross-attention and GCN smoothing) back to the terms of the bound (Wasserstein distance and the Lipschitz constant), providing a formal justification for why their design should work."}, "weaknesses": {"value": "1. In Table 1 (real-to-real adaptation), the performance gains over the next-best methods (TDSS, DGSDA) are often quite small (e.g., 80.86% vs 81.84% on A→C; 80.72% vs 80.27% on D→C, where TUGDA is second-best on Ma-F1). This suggests that for these standard datasets, the additional complexity of the transformer offers only a minor benefit. The paper's narrative could be strengthened by acknowledging this and placing more emphasis on the synthetic-to-real results, where the architecture truly shines."}, "questions": {"value": "1. Do the authors have an intuition for why TUGDA is so much better in the synthetic-to-real setting? Does the GraphMaker model (Li et al., 2023) fail to reproduce fine-grained local neighborhood structures, causing GNN-based methods like A2GNN to fail, while TUGDA's global attention is more robust to this? A deeper discussion of this phenomenon would be highly valuable.\n\n2. The paper repeatedly mentions using \"asymmetric GCN propagation\" inspired by A2GNN (Sections 3, 3.4). However, the methodology section is vague on the exact implementation. Equation 2 describes a standard GCN layer with shared weights $W_{GCN}$. Figure 4c/d explores different propagation layer counts for source and target, but it's unclear if this is a tuned hyperparameter or an integral part of the asymmetric design (as in A2GNN). Could the authors please clarify exactly how the GCN propagation is asymmetric? Is it simply that the number of layers ($K_s, K_t$) is different, and if so, how is this handled in the shared-weight architecture?\n\n3. The evaluation is performed exclusively on three small, sparse, homophilic citation networks (ACM, Citation, DBLP). The paper's motivation is to capture long-range dependencies in sparse graphs, but all these graphs fit that description. The claims of generality would be much stronger if TUGDA were tested on more diverse graph types, such as:\n\n- Graphs with significant heterophily.\n- Graphs with larger diameters, where long-range dependencies are demonstrably more critical."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ThLpA33aQj", "forum": "tAJRXq7BZp", "replyto": "tAJRXq7BZp", "signatures": ["ICLR.cc/2026/Conference/Submission20809/Reviewer_iuWq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20809/Reviewer_iuWq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616223412, "cdate": 1761616223412, "tmdate": 1762935784482, "mdate": 1762935784482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of unsupervised cross-domain knowledge transfer on graphs. The authors propose a framework combining GCN, cross-attention mechanisms, and output alignment to capture both global and local structural dependencies. The paper also explores model utility from a privacy-preserving perspective, which is a valuable entry point."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•\tClear Model Architecture: The overall method's architecture is clear, combining cross-attention, output alignment, and GCN adjustments to address cross-domain knowledge transfer.\n\n•\tSufficient Theoretical Analysis: The authors provide some experiments to validate the model's effectiveness and attempt to offer a theoretical justification for the method's rationale based on the domain adaptation bound.\n\n•\tPractical Significance: The paper introduces a privacy-preserving perspective to the problem of graph domain adaptation, which is a novel and practical contribution."}, "weaknesses": {"value": "My concerns are primarily focused on three areas: methodological novelty, experimental rigor, and problem positioning.\n1. Regarding Methodological Novelty:  The proposed framework largely appears to be a direct combination of existing, mature techniques (GCN, Transformer/Cross-Attention, Distribution Alignment). The authors need to more clearly articulate what unique technical insights or contributions are provided beyond this combination. Why is this specific assembly superior to other possible combinations?\n2. Regarding Experimental Validation and Analysis: There is a significant figure-text misalignment in the experimental section. The text explicitly states, “Figure 4(a) demonstrates the impact of removing individual components from our transformer framework,” which points to an Ablation Study. However, Figure 4(a) itself does not appear to contain any analysis regarding the \"removal of components.\"\nThe analysis of the \"synthetic source domain\" is confusing. The experimental results show that using synthetic data has a huge impact on performance, but this part is severely under-explained. The authors must clarify: (a) How was the \"synthetic source domain\" generated? What are the exact differences in graph structure, node features, and distribution compared to the \"original source domain\"? (b) Why does this difference lead to such a significant performance change? Does this reveal a particular vulnerability or dependency of the model?\nConfusing Presentation of Parameter Analysis: The presentation of the parameter analysis is perplexing. It seems to show the independent performance of model components (e.g., GCN and Transformer) under various hyperparameter settings, rather than the performance of the Full Model. This presentation makes it difficult for reviewers to analyze (a) the optimal parameter choice for the full model and (b) the final model's sensitivity to these parameters. Besides,  Why only use A2GNN as baseline for comparison for generative graph? It clearly needs thorough discussion and more advanced baselines.\n3. Regarding Problem Positioning: The authors need to discuss more deeply the positioning and limitations of this research in the current era of Foundation Models. A major trend in the field is the pursuit of \"train-once, generalize-everywhere\" Domain Generalization (DG) foundation models. In contrast, the Domain Adaptation (DA) paradigm used in this paper is \"target-specific,\" meaning that when a new target domain appears, it must (at least in the inference phase) access the domain's data and (usually) be retrained or fine-tuned.\n\nMinor points:\n\n•\tI think there is a format problem in line 350-351 after Table 2. This paragraph begins with \"provides\" ?\n\n•\tThe sub-figures in Fig. 2 are not aligned.\n\n•\tThere is unnecessary symbol present in line 795."}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N6Q48TUkuD", "forum": "tAJRXq7BZp", "replyto": "tAJRXq7BZp", "signatures": ["ICLR.cc/2026/Conference/Submission20809/Reviewer_E2f4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20809/Reviewer_E2f4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732177564, "cdate": 1761732177564, "tmdate": 1762935757645, "mdate": 1762935757645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TUGDA, a Transformer-based Unsupervised Graph Domain Adaptation framework designed to overcome the locality bias of GNN-based UGDA models. The authors argue that traditional GNNs fail to capture long-range dependencies critical for domain transfer on sparse graphs. To address this, TUGDA sequentially integrates a Transformer backbone with an asymmetric GCN, aiming to model both global and local structural dependencies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Clarity and structure:** The paper is generally well organized and clearly written, with a consistent presentation of architecture, loss formulation, and theoretical background. The inclusion of an algorithmic summary and sensitivity analyses in the appendix enhances readability and reproducibility.\n* **Quality of experiments:** The authors benchmark TUGDA against a wide set of prior GDA models and include ablations on positional encoding and cross-attention, which is valuable for understanding component contributions.\n* **Significance:** Addressing sparse-graph domain adaptation through global structural modeling is an important direction. The exploration of *synthetic-source transfer* under privacy constraints is a timely and practically interesting scenario."}, "weaknesses": {"value": "1. **Limited conceptual contribution.**\n   The central claim—that TUGDA “effectively captures both global and local structural dependencies” is not convincingly distinguished from existing baselines such as **UDAGCN** (Wu et al., 2020) and **A2GNN** (Liu et al., 2024). Both already combine multi-hop message passing or asymmetric propagation with feature-level regularization, which implicitly captures similar dependencies. Simply inserting a Transformer encoder into a UGDA pipeline, without a clear new inductive bias or learning principle, contributes to appear **incremental** rather than conceptual. The paper would benefit from a deeper justification of *why* the Transformer-GCN composition leads to qualitatively different representations beyond receptive-field size.\n\n2. **Theoretical analysis lacks distinct insight.**\n   The theoretical part relies on a **Wasserstein-distance-based** risk bound, similar in spirit to the analysis in **SepReg** (You et al., 2023) and **HGDA** (Fang et al., 2025). However, the claimed connection between the *cross-attention matrix* and the Wasserstein coupling term is described only heuristically (Eq. 6), with no rigorous mapping between transport plan optimization and attention weight learning. The paper should clarify how the attention-induced alignment differs mathematically from the spectral or transport formulations in [1] and [2].\n\n3. **Missing or incomplete baseline coverage.**\n   Several representative and recent UGDA methods are not compared, including **COCO** [3], **SAGA** [4], and **GMM-GDA** [5]. These baselines specifically address structural alignment, distribution modeling, and statistical regularization, which are conceptually close to TUGDA. Omitting them weakens the empirical claims of state-of-the-art performance.\n\n4. **Evaluation metric choice.**\n   The paper primarily reports *macro-F1* and *micro-F1* scores, which are reasonable, but the omission of *accuracy* (ACC) as a standard classification measure limits interpretability and cross-comparison. The authors should include ACC on all datasets to make results directly comparable to prior UGDA literature.\n\nSome typo issue: \nRepeated citations in the paper (lines 491-499)\nThe text mentions Figure 4(a), but subsequent figure titles are \"Figure 2: Component contributions...\". The numbering seems inconsistent; please standardize it.\n\n[1] You Y., Chen T., Wang Z., et al. Graph Domain Adaptation via Theory-Grounded Spectral Regularization. ICLR 2023.\n[2] Fang R., Li B., Zeng Q., et al. On the Benefits of Attribute-Driven Graph Domain Adaptation. ICLR 2025.\n[3] Yin N., Shen L., Wang M., et al. COCO: A Coupled Contrastive Framework for Unsupervised Domain-Adaptive Graph Classification. ICML 2023.\n[4] Fang R., Li B., Zeng Q., et al. On the Benefits of Attribute-Driven Graph Domain Adaptation. ICLR 2025.\n[5] Wang M., Ren W., Zhang Y., et al. Gaussian Mixture Model for Graph Domain Adaptation. IJCAI 2025."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cCR6lOGEko", "forum": "tAJRXq7BZp", "replyto": "tAJRXq7BZp", "signatures": ["ICLR.cc/2026/Conference/Submission20809/Reviewer_udZi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20809/Reviewer_udZi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786803014, "cdate": 1761786803014, "tmdate": 1762935694688, "mdate": 1762935694688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies unsupervised graph domain adaptation (UGDA) and proposes TUGDA, a pipeline that first applies a graph transformer encoder to capture long-range, global dependencies, then uses a cross-attention module to align source and target node representations before graph propagation via a GNN. The authors derive domain-adaptation style generalization bounds that motivate the use of cross-attention and backbone transformers for alignment, and validate TUGDA on several citation-network benchmarks, reporting improvements over a set of recent baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Introducing the graph transformer is a meaningful and timely direction in the context of graph domain adaptation. Graph transformers provide a flexible mechanism for learning pairwise affinities that go beyond local neighborhood aggregation, and applying them to UGDA is a relatively recent and promising approach. \n\n2. The paper includes a domain-adaptation generalization bound that ties errors to representation alignment terms. This theoretical work — particularly the way the bound highlights the capacity for cross-attention to reduce domain discrepancy in representation space — gives additional, nontrivial support to the architectural choice of combining transformers with cross-attention for UGDA."}, "weaknesses": {"value": "1. Weak motivation for Transformer choice\n\nThe core idea—combining a graph transformer (SGFormer) with GNN propagation and using cross-attention to align source and target—is not demonstrated to be conceptually novel relative to prior global-local hybrids in graph learning. The authors do not convincingly justify why a Transformer is a better or necessary mechanism for capturing global alignment than simpler or well-established alternatives (e.g., representation subspace learning, nonparametric k-NN graph construction). The paper should either theoretically or empirically demonstrate that these alternatives fail on the same tasks, or sharpen what the transformer uniquely enables for UGDA.\n\n2. Time/space complexity and scalability concerns\n\nThe manuscript lacks a clear complexity analysis for training and inference and provides runtime / GPU memory measurements. Claims about linear complexity are misleading without clarifying how cross-domain attention scales when source and target node counts differ. No experiments are presented on truly large graphs to validate scalability; all reported datasets are relatively small (ACMv9, Citationv1, DBLPv7). This weakens claims of practical applicability and leaves unclear whether the method will scale to realistic UGDA scenarios.\n\n3. Experimental evaluation is narrow and insufficient\n\nThe evaluation uses only three citation datasets (A, C, D). These are all in the same domain family (citation networks) and modest in size, which limits the claim of generalization across domains. Key UGDA benchmarks from other domains (e.g., Airport/Blog/other cross-domain datasets, or larger real-world graphs) are missing.\n\n4. Organization and placement of technical content\n\nThe theoretical analyses are placed after experiments, which disrupts the paper’s logical flow. The authors should present the model and accompanying theoretical justification earlier (within the Method/Preliminaries) so readers can evaluate the theory before seeing experimental claims."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F4xzCh1fjk", "forum": "tAJRXq7BZp", "replyto": "tAJRXq7BZp", "signatures": ["ICLR.cc/2026/Conference/Submission20809/Reviewer_9K6M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20809/Reviewer_9K6M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971310017, "cdate": 1761971310017, "tmdate": 1762935653006, "mdate": 1762935653006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}