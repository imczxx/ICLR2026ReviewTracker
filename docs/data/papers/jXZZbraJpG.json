{"id": "jXZZbraJpG", "number": 2646, "cdate": 1757180165217, "mdate": 1759898135694, "content": {"title": "Playing with Transformer at 30+ FPS via Next-Frame Diffusion", "abstract": "Autoregressive video models offer distinct advantages over bidirectional diffusion models in creating interactive video content and supporting streaming applications with arbitrary duration.  Nonetheless, achieving real-time video generation remains a significant challenge for such models, primarily due to the high computational cost associated with diffusion sampling and the hardware inefficiencies inherent to autoregressive generation. In this work, we present Next-Frame Diffusion (NFD), an autoregressive diffusion transformer that incorporates block-wise causal attention, enabling iterative sampling and efficient inference via parallel token generation within each frame. To address the aforementioned challenges, we introduce two innovations: (1) We for the first time extend consistency distillation to the video domain and adapt it specifically for video models, enabling efficient inference with few sampling steps; (2) To fully leverage parallel computation, motivated by the observation that adjacent frames often share the identical action input, we propose speculative sampling. In this approach, the model generates next few frames using current action input, and discard speculatively generated frames if the input action differs. Experiments on a large-scale action-conditioned video generation benchmark demonstrate that NFD beats autoregressive baselines in terms of both visual quality and sampling efficiency. We, for the first time, achieves autoregressive video generation at over $30$ Frames Per Second (FPS) on an A100 GPU using a 310M model.", "tldr": "", "keywords": ["video generation", "autoregressive models", "diffusion models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/64ace1ebc85ad0dc8effb8bb8b8ea2382e7502cc.pdf", "supplementary_material": "/attachment/4762a35d41be1dab326082e784ffa23e49899334.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a pixel-level, next-frame video prediction diffusion model that is conditioned on actions and optimized for efficient inference. The authors employ an efficient sampling strategy and a few-step model distillation technique to accelerate the model. Furthermore, a multi-frame prediction method is introduced to enhance generation speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Targeted Engineering for Acceleration:** The paper presents specific engineering designs aimed at accelerating video diffusion models, which is a significant contribution to the field.\n    \n2. **State-of-the-Art Performance:** The proposed model achieves state-of-the-art visual quality and sampling efficiency when compared to baseline methods, as demonstrated by the quantitative results."}, "weaknesses": {"value": "1. **Lack of Qualitative Comparison:** Despite achieving strong numerical results, the paper lacks a qualitative comparison with baseline methods, such as a user study. Visual examples are crucial for a comprehensive evaluation of generative models.\n    \n2. **Hybrid Contribution:** The paper's contributions appear to be a mix of engineering and algorithmic improvements. However, the engineering enhancements are not explored to their full potential, and the algorithmic advancements are largely adaptations of existing work."}, "questions": {"value": "1. **Source of Improvement:** Could you provide a more detailed analysis of the source of the model's improvements?\n    \n    - In Table 1, the performance of NFD-310M is comparable to Oasis-500M, despite having fewer parameters. A deeper analysis of the architectural design choices that lead to this efficiency would be beneficial.\n        \n2. **Necessity of a User Study:** Given the subjective nature of video quality assessment, a user study would significantly strengthen the paper's claims, especially for evaluating the model's performance on action sequences created by humans.\n    \n3. **Effectiveness of \"Speculative Decoding\":** The efficacy of the proposed \"speculative decoding\" method seems highly dependent on the frequency of action changes within the dataset.\n    \n    - What is the action change frequency in the test set?\n        \n    - How does the \"speculative decoding\" method perform when the action change frequency is high or erratic?\n        \n    - **Clarification on Terminology:** The term \"speculative decoding\" as used here appears to differ from its common usage in the Large Language Model (LLM) community, where a smaller model predicts and a larger model verifies. Have you considered a similar approach, for instance, using a one-step prediction and then verifying it based on mathematical conditions like temporal consistency or proximity to the subsequent step's result?\n        \n4. **Dynamic Frame Repetition in “speculative decoding“:** The NFD+ series models achieve real-time performance (24-42 FPS). Have you considered implementing a dynamic `N` (frame repetition number) that could adapt in real-time to user action inputs? This could allow for more interactive and responsive generation by modifying the repetition times of the current action context on the fly.\n    \n5. **Performance Discrepancy:** Could you explain the significant difference in inference speed between CausVid\\* and NFD+? Given that both models have a similar number of parameters and, presumably, the same number of sampling steps (4 for the distilled models), the reason for this performance gap is not immediately clear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CLdGU2CioY", "forum": "jXZZbraJpG", "replyto": "jXZZbraJpG", "signatures": ["ICLR.cc/2026/Conference/Submission2646/Reviewer_mZoC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2646/Reviewer_mZoC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667291351, "cdate": 1761667291351, "tmdate": 1762916317864, "mdate": 1762916317864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a **next-frame diffusion** framework aimed at achieving real-time video generation. The work highlights two main contributions:\n\n1. **Consistency distillation** is introduced into video diffusion models, reducing the number of diffusion sampling steps to only a few.  \n2. **Speculative sampling** is employed to predict multiple frames simultaneously, significantly accelerating inference.\n\nTogether, these methods enable real-time generation at up to **30 FPS**."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is **well-structured** and **clearly written**, making the main ideas easy to follow.  \n- The **methodology and motivation** are clearly articulated.  \n- The **experimental comparisons** are straightforward and provide solid empirical evidence for the proposed improvements."}, "weaknesses": {"value": "- The **novelty** of the contributions is limited. Both *consistency distillation* and *speculative sampling* are existing ideas that have been explored in prior works.  \n- The paper primarily adapts these known techniques to the *next-frame diffusion* setting, resulting in a contribution that feels **incremental rather than groundbreaking**.  \n- The paper would benefit from a deeper analysis or theoretical insight to strengthen its originality."}, "questions": {"value": "1. **FVD Evaluation**  \n   It is unclear at which autoregressive (AR) step the reported FVD results are measured.  \n   Since AR methods are prone to error accumulation, the authors should compare FVD across multiple AR step lengths to better understand how performance degrades over time.  \n   This is especially important in acceleration scenarios, where such effects can be amplified.\n\n2. **Speed vs. AR Step Length**  \n   As the AR step length increases, the **cache size** grows correspondingly, which should impact generation speed.  \n   For long video sequences, per-frame latency likely increases significantly.  \n   Instead of reporting a single speed value, the paper should provide a **curve of generation speed versus AR step length** for a more complete performance picture.\n\n3. **Speculative Sampling Quality Control**  \n   The paper mentions that speculative samples are rejected only when the predicted and actual actions differ, but it does not discuss how **frame quality** is considered.  \n   If the multi-frame predictions are of poor quality, are they still accepted?  \n   The current experiments focus on speed improvements but **omit comparisons of generation quality** under speculative sampling, which is essential for evaluating the trade-off between quality and speed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aHTSdvEAYL", "forum": "jXZZbraJpG", "replyto": "jXZZbraJpG", "signatures": ["ICLR.cc/2026/Conference/Submission2646/Reviewer_5NKJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2646/Reviewer_5NKJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834260478, "cdate": 1761834260478, "tmdate": 1762916317675, "mdate": 1762916317675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Next-Frame Diffusion (NFD), an autoregressive diffusion transformer for action-conditioned video generation. The key architectural idea is block-wise causal attention, which allows bidirectional attention within a frame while enforcing causality across frames so that all tokens of the next frame can be generated in parallel. To reach real-time speeds, the authors (i) extend continuous-time consistency distillation (sCM) from images to the video domain to cut diffusion steps to four, and (ii) introduce speculative sampling that pre-generates a few future frames under the assumption the current action persists, discarding any that disagree when the action actually changes. On Minecraft/VPT data, NFD attains >30 FPS on an A100 with a 310M model (NFD+ variant) at 384×224 resolution, and reports FVD/PSNR/LPIPS/SSIM competitive with or better than discrete autoregressive baselines like MineWorld, with a speed–quality trade-off between the undistilled NFD and the accelerated NFD+. The paper includes ablations for speculative sampling (up to 1.26× faster), sCM schedules, KV caching (up to 1.33× faster), and some architecture comparisons. Limitations acknowledge short temporal context (32 frames), Minecraft-only data, and fixed, relatively low resolution."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clean combination of block-causal attention with diffusion-based frame-wise prediction, plus an explicit extension of consistency distillation to video. The speculative-sampling idea for action-conditioned video is straightforward but practical.\n2. Results table transparently shows the speed–quality trade-off: e.g., NFD (310M) FVD 212 at 6.15 FPS vs NFD+ (310M) FVD 227 at 31.14 FPS.\n3. The paper is easy to follow, with an architectural figure and explicit training/sampling equations, plus a pseudocode sketch for speculative sampling. Limitations are candid."}, "weaknesses": {"value": "1. Novelty relative to prior “frame-wise” diffusion variants appears incremental. The work builds on a now-active line of next-frame diffusion with parallel per-frame token generation and noise-perturbed contexts (e.g., Diffusion Forcing, CausVid/self-forcing-style distillations). While the paper positions its sCM extension and speculative sampling as first, readers may see these as natural adaptations rather than conceptual leaps. A head-to-head with Diffusion Forcing and CausVid under identical data/tokenizers would better clarify novelty/margins beyond the included CausVid student result. \n2. All results are Minecraft (VPT). The paper itself notes domain limitation. The qualitative scenes and the chosen 384×224 resolution are unlikely to stress long-range temporal coherence, complex motion, or photorealistic detail; hence the scalability of the approach to harder domains/resolutions remains unproven.\n3. Speed claims hinge on specific settings and show a quality drop after distillation. The flagship 30+ FPS is reported for NFD+ (4 steps) with speculative sampling and KV caching at low resolution and batch size 1; undistilled NFD is ~6 FPS for 310M. Quality regresses (FVD 212->227) with NFD+, suggesting the distilled model pays a non-trivial perceptual cost for speed. More end-to-end latency breakdowns (tokenizer, I/O), memory bandwidth profiling, and multi-resolution scaling curves would strengthen the case."}, "questions": {"value": "1. What are the measured FPS and quality when scaling to, say 480p or 720p, with 81/121 context frames? Any stability issues (e.g., error accumulation) and how do sCM and context-noise injections behave under those regimes? \n2. Speculative sampling under non-stationary actions. How often are speculative frames discarded on validation logs? Please report (i) discard rate vs N, (ii) net wall-clock speedup vs. GPU utilization, and (iii) quality when actions change rapidly (camera whip-pans, frequent tool switches). A small user-study or scripted stress-test would help.\n3. Ablations on block-causal attention vs. alternatives. The appendix compares “2D+1D” vs block-causal. Could you also show full 3D attention and windowed causal variants at matched compute to establish where the FPS gains primarily come from (attention pattern vs. implementation/kv-cache)?\n4. Since NFD+ trades quality for speed, can you provide (a) user preference studies, (b) VBench breakdowns beyond the three metrics shown, and (c) samples at equal FPS budgets (e.g., NFD with fewer NFEs) to clarify whether sCM is the dominant factor? \n5. Generalization beyond Minecraft. Any preliminary results on real-world or non-voxel games (e.g., driving/world-model datasets, Sekai) to test motion complexity and texture diversity? Even short proofs-of-concept would help address external validity concerns raised in your Limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iZBEGGBOXc", "forum": "jXZZbraJpG", "replyto": "jXZZbraJpG", "signatures": ["ICLR.cc/2026/Conference/Submission2646/Reviewer_5oQx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2646/Reviewer_5oQx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864396655, "cdate": 1761864396655, "tmdate": 1762916317541, "mdate": 1762916317541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Next-Frame Diffusion (NFD), an autoregressive diffusion model designed for real-time video generation. NFD combines the strengths of diffusion models (high fidelity) and autoregressive models (temporal causality) to achieve over 30 FPS on an A100 GPU. Key innovations include block-wise causal attention, consistency distillation, and speculative sampling, which significantly enhance sampling efficiency and visual quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-writing: The paper is well-written, clear, and concise, effectively communicating its ideas without unnecessary complexity.\n- Compelling scalability analysis: The model's performance improves with increased size, indicating potential for further gains with larger models and datasets."}, "weaknesses": {"value": "- **Concerns on incremental novelty.** The proposed method appears to integrate existing techniques such as consistency distillation and speculative decoding, which may limit its conceptual novelty and suggest an incremental contribution.\n- **Limited data validation and strong assumptions in speculative decoding.**  The assumption that actions remain identical during speculative decoding may be overly restrictive; I expect evaluation on more complex datasets to better validate the robustness and acceleration efficiency of this approach."}, "questions": {"value": "- See weaknesses.\n- In short, the work presents Next-Frame Diffusion (NFD), an autoregressive diffusion model designed for real-time video generation. \n- My main concerns are the novelty of this work, as well as the limited data validation and the strong assumptions in speculative decoding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TzaWxkYAxE", "forum": "jXZZbraJpG", "replyto": "jXZZbraJpG", "signatures": ["ICLR.cc/2026/Conference/Submission2646/Reviewer_eMsJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2646/Reviewer_eMsJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970709528, "cdate": 1761970709528, "tmdate": 1762916317126, "mdate": 1762916317126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}