{"id": "0A3qzLmRHd", "number": 14192, "cdate": 1758230106795, "mdate": 1759897384880, "content": {"title": "SECVULEVAL: Benchmarking LLMs for Real-World C/C++ Vulnerability Detection", "abstract": "Large Language Models (LLMs) have shown promise in various software engineering tasks, but evaluating their effectiveness in vulnerability detection remains challenging due to the lack of high-quality benchmark datasets. Most existing datasets are limited to function-level labels, ignoring finer-grained vulnerability patterns and crucial contextual information. They also often suffer from poor data quality, such as mislabeling, inconsistent annotations, and duplicates, which can lead to inflated performance and weak generalization. Moreover, by including only the vulnerable functions, these datasets miss broader program context, like data/control dependencies and interprocedural interactions, that are essential for accurately detecting and understanding real-world security flaws. Without this context, detection models are evaluated under unrealistic assumptions, limiting their practical impact. To address these limitations, this paper introduces SECVULEVAL, a comprehensive benchmark designed to support fine-grained evaluation of LLMs and other detection methods with rich contextual information. SECVULEVAL focuses on real-world C/C++ vulnerabilities at the statement level. This granularity enables more precise evaluation of a model’s ability to localize and understand vulnerabilities, beyond simple binary classification at the function level. By incorporating rich contextual information, SECVULEVAL sets a new standard for benchmarking vulnerability detection in realistic software development scenarios. This benchmark includes 25,440 function samples covering 5,867 unique CVEs in C/C++ projects from 1999 to 2024. We evaluated the SOTA LLMs with a multi-agent-based approach. The evaluation on our dataset shows that the models are still far from accurately predicting vulnerable statements in a given function. The best-performing Claude-3.7-Sonnet model achieves a 23.83% F1-score for detecting vulnerable statements with correct reasoning, with GPT-4.1 closely behind. We also evaluate the effect of using contextual information for the vulnerability detection task. Finally, we analyze the LLM outputs and provide insights into their behavior in vulnerability detection for C/C++.", "tldr": "", "keywords": ["Benchmark", "Security Vunerability", "Large Language Model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9568b0a1094b7803769e4cfc3d795afafd2e0f92.pdf", "supplementary_material": "/attachment/89003334187309cbc3769306208e64115cd2fa03.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces SECVULEVAL, a new vulnerability detection dataset focusing on LLM-based solutions and C/C++ projects. The authors collected the vulnerability data from the national vulnerability database (NVD), and extracted line-level vulnerability labels. Furthermore, they used an LLM to extract useful contextual information which is added to the dataset. The paper finally benchmarks five state-of-the-art LLMs on two tasks: vulnerability detection and context identification, and shows that those models struggle to achieve acceptable performance, with F1-scores below 25%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors provide fair amount of evidence to highlight the novelty of the dataset (de-duplication, accurate contextual information, statement-level labels).\n    \n- Experiment 2 (context identification) is interesting and partly highlights the reason why SoTA LLMs are failing in the vulnerability detection task."}, "weaknesses": {"value": "- While the novelty of the dataset is shown, it is not clear why the proposed dataset is valuable for the end goal of vulnerability detection. The paper would have benefited from an experimental comparison to existing similar datasets. For example, how does pretraining or fine-tuning a model on your dataset (compared to other datasets) improve their performance?\n    \n- The paper only focuses on LLM-based vulnerability detection solutions, and lacks comparison with static analyzers and deep learning-based methods. Similarly the paper focuses on C/C++ vulnerabilities without enough justification for the exclusion of other languages.\n    \n- Minor issue: The authors claim that, to the best of their knowledge, they are the first to apply a multi-agent pipeline for vulnerability detection. However, this has recently been considered by many works ([A], [B], [C], [D], [E] to name a few).\n    \n\n[A] Widyasari, Ratnadira, et al. \"Let the Trial Begin: A Mock-Court Approach to Vulnerability Detection using LLM-Based Agents.\" *arXiv preprint arXiv:2505.10961* (2025).\n\n[B] Z. Wei et al., \"Advanced Smart Contract Vulnerability Detection via LLM-Powered Multi-Agent Systems,\" in IEEE Transactions on Software Engineering, vol. 51, no. 10, pp. 2830-2846, Oct. 2025, doi: 10.1109/TSE.2025.3597319.\n\n[C] Wang, Ziliang, et al. \"VulAgent: Hypothesis-Validation based Multi-Agent Vulnerability Detection.\" *arXiv preprint arXiv:2509.11523* (2025).\n\n[D] Seo, Minjae, et al. \"AutoPatch: Multi-Agent Framework for Patching Real-World CVE Vulnerabilities.\" *arXiv preprint arXiv:2505.04195* (2025).\n\n[E] Yildiz, Alperen, et al. \"Benchmarking LLMs and LLM-based Agents in Practical Vulnerability Detection for Code Repositories.\" *arXiv preprint arXiv:2503.03586* (2025)."}, "questions": {"value": "- Why did the authors not qualitatively compare their dataset to other existing datasets, in terms of its effect on developing effective vulnerability detection solutions (e.g., training or fine-tuning a model on the dataset, check above point in weaknesses)? How else would you prove the value of the proposed dataset?\n    \n- What is the reason for the narrow focus on C/C++ vulnerabilities, and on LLM-based vulnerability detectors? For example, is the dataset incompatible with deep learning-based detectors?\n    \n- The authors mention that GPT-4.1 was 83.16% accurate in extracting relevant contextual information to include in the dataset. However, it is not clear whether experiment 2 was benchmarked against the 83.16% accurate samples, or the ground truth 1k subset of the dataset.\n    \n- Related to the above question, does this 16.84% inaccuracy have any effect on the downstream task of vulnerability detection?\n    \n- In table 1, the proposed SECVULEVAL is presented as the only dataset with statement-level labels. Do not the BigVul, SVEN, CVEFixes datasets have statement-level labels as well?\n    \n- Can you explain the novelty of the contextual information collection in your dataset in comparison to other related work e.g., [A], [B], and [C]?\n    \n- How do you justify the uncommon (lenient) definitions of success of context identification in section 3.4 (lines 299-300), and the success of vulnerability detection in section 5.1 (lines 410-415) ? Would a stricter definition strongly influence the results, especially for section 3.4 where this could affect the accuracy of the dataset itself?\n    \n- For the function-level detection in Table 3, most of the F1 scores are below 50%. Does this means that those models perform worse than random guessing?\n    \n- The authors mention in section 5.1 (line 405) that it is not possible to automatically evaluate the explanations given by the LLMs. Do you consider this as a limitation of your proposed dataset?\n    \n\n[A] Yang, Yixin, et al. \"Context-enhanced vulnerability detection based on large language model.\" *arXiv preprint arXiv:2504.16877* (2025).\n\n[B] Li, Yikun, et al. \"CleanVul: Automatic Function-Level Vulnerability Detection in Code Commits Using LLM Heuristics.\" *arXiv preprint arXiv:2411.17274* (2024).\n\n[C] Wang, Xinchen, et al. \"Reposvul: A repository-level high-quality vulnerability dataset.\" *Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings*. 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vIjXB140a5", "forum": "0A3qzLmRHd", "replyto": "0A3qzLmRHd", "signatures": ["ICLR.cc/2026/Conference/Submission14192/Reviewer_5HW9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14192/Reviewer_5HW9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761149128718, "cdate": 1761149128718, "tmdate": 1762924646869, "mdate": 1762924646869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SECVULEVAL, a C/C++ vulnerability benchmark designed for function and statement-level detection with rich contextual information. It supplies metadata (CVE/CWE, commit IDs/messages, pre- and post-patch code), and five classes of “context” (arguments, external functions, type definitions, globals, environment).  Duplicate functions are removed via MD5 hashing over normalized functions. The authors also propose a multi-agent LLM pipeline (planning → context extraction → detection → validation), and evaluate 5 models. The best model (Claude-3.7-Sonnet) achieves F1 scores of 53.89% and 23.83% at the function level and statement level vulnerability detection, respectively, with GPT-4.1 being close behind. Ablations without the agent pipeline show very low F1 (<4%) for function-level vulnerability detection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clear and well-written.\n- The paper moves beyond function-level labels to statement-level.\n- The paper proposes a novel idea of collecting vulnerability context, such as variable state returned from an external function, function arguments, execution environment, etc, which are essential for detecting and understanding vulnerabilities.\n- The paper proposes a multi-agent pipeline to detect vulnerabilities."}, "weaknesses": {"value": "- No comparison against classical/static analyzers or existing deep learning based  SOTA approaches on the same tasks/splits, which would ground LLM performance against established tools.\n- Does the multi-agent system have advantage for function level vulnerability detection? Some comparison with base LLMs can be interesting.\n- GPT-4.1 is used to create the “required context” annotations, then later models are evaluated using these contexts. Even with a 1k sample audit (~83% accuracy), this introduces model-generated ground truth and potential bias toward models similar to the annotator. Inter-annotator agreement among humans is not reported.\n- The paper does not have enough explanation of how statements are labeled. It appears that the statements are collected based on patch diffs; however, the added/deleted lines do not always correspond one-to-one with the causal vulnerable statement. What if an added line is just a semantic transformation of a deleted line?\n- This paper contains low ML novelty and contributions. It will be interesting to see ablations of each agent."}, "questions": {"value": "- Are the statement label data always collected from the patch? Are all the added/deleted lines from the selected functions labeled as vulnerable? How do you identify that some are truly causal vulnerable statements?  How do you identify that the added line is not semantically the same as the deleted lines? \n- Why is there no comparison against the existing SOTA approaches?  Does your multi-agent system have advantage of function level vulnerability detection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "az0XoYwq4v", "forum": "0A3qzLmRHd", "replyto": "0A3qzLmRHd", "signatures": ["ICLR.cc/2026/Conference/Submission14192/Reviewer_mwoF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14192/Reviewer_mwoF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872072299, "cdate": 1761872072299, "tmdate": 1762924646268, "mdate": 1762924646268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SECVULEVAL, a C/C++ vulnerability detection benchmark with 5,867 CVEs covering 25,440 functions with statement-level annotations and contextual information. The authors evaluate five LLMs using a multi-agent pipeline, finding that even the best model (Claude-3.7-Sonnet) achieves only 23.83% F1-score for statement-level detection with correct reasoning. The dataset addresses limitations in existing benchmarks by providing finer granularity, rigorous deduplication, and five categories of contextual information."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a genuine problem in vulnerability detection benchmarking. The statement-level granularity is a meaningful improvement over function-level labels, and the inclusion of contextual information (validated at 83.16% accuracy) adds practical value. The rigorous deduplication process and filtering pipeline demonstrate careful data curation. The dataset's scale (707 projects, 145 CWE types) and temporal span provide good diversity."}, "weaknesses": {"value": "The core contribution is essentially a dataset with better labels, which feels incremental rather than transformative. The 83.16% context extraction accuracy means ~17% of the dataset contains noisy annotations, yet no analysis quantifies how this affects downstream evaluation reliability. The multi-agent pipeline, while interesting, conflates dataset contribution with methodological innovation—it's unclear whether improvements come from the data or the approach. The paper lacks critical analysis: why do models fail? Are certain vulnerability patterns fundamentally harder? The ablation study is relegated to appendix and shows models achieve <4% F1 without agents, but this deserves main text discussion. The statement-level vs line-level distinction, while technically correct, feels overstated—many statements span single lines anyway. Finally, the paper doesn't demonstrate that anyone can actually use this dataset to train better models; it only shows LLMs fail at detection."}, "questions": {"value": "see in the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2jeQWM45Hi", "forum": "0A3qzLmRHd", "replyto": "0A3qzLmRHd", "signatures": ["ICLR.cc/2026/Conference/Submission14192/Reviewer_iCnF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14192/Reviewer_iCnF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143555130, "cdate": 1762143555130, "tmdate": 1762924645902, "mdate": 1762924645902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}