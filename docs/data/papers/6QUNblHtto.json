{"id": "6QUNblHtto", "number": 19540, "cdate": 1758297072415, "mdate": 1759897033802, "content": {"title": "CEA: Context Engineering Agent for Enhanced Reliability and Sustainability in Deep Research Systems", "abstract": "The increasing capacity of frontier models to process long contexts has fueled enthousiasm for deep research agents. However, longer contexts alone do not guarantee better responses. In fact, context overloading can lead to unexpected agent failures. \nTo tackle this challenge, we introduce an autonomous context control framework built around a Context Engineering Agent (CEA). The CEA maintains structured context by efficiently managing historical interactions, tracking ongoing progress, and identifying critical clues, hence achieving an optimal trade-off between token efficiency and memory integrity.\nIn conjunction with this framework, we introduce CERL, an end-to-end multi-turn reinforcement learning method designed for CEA. We enhance training by filtering out trajectories with non CEA-attributable errors before gradient updates, thereby enhancing the stability of training.\nOur \\textbf{CEA} approach has demonstrated substantial efficacy in enhancing performance on complex information-seeking tasks, as evidenced by increased interaction sustainability and notable performance improvements across various benchmarks.\nDespite its sophisticated context-processing mechanisms, CEA is a plug-and-play solution that seamlessly integrates into existing systems, enhancing agents' context management with minimal code modifications. This combination of internal sophistication and external simplicity makes CEA both powerful and practical for real-world deployment.", "tldr": "We propose an autonomous context control framework to address the context rot issue in large language model agents for better response quality.", "keywords": ["Deep Research", "LLM", "Agent", "Context Engineering", "AgentRL", "long context"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c6dcd5f3b250378e5b8283ef1ee5b16ead6615d1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work aims to tackle the problem of excessive context lengths required by agentic frameworks by introducing Context Engineering Agent (CEA). CEA is an isolated module and instantiated as an LLM that compresses a task query, a dynamic plan, an episodic history, and semantic facts into an actionable context for a reasoning module represented by another LLM. Furthermore, CEA can be trained via RL to maximize task completion including a certain filtering that isolates contributions of CEA. The method is evaluated on several benchmarks and the results show consistent improvements over not using CEA and other agentic frameworks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Well-structured experiment design\n- CEA provides consistent performance improvements \n- CEA is agnostic to the reasoner and can be incorporated into any agentic pipeline"}, "weaknesses": {"value": "**Novelty**\n\nThe main contribution of this work revolves around compressing and abstracting context into smaller digestible chunks that improve performance and enables longer rollouts. One concern of mine is that prior works like [1,2] already explored similar ideas and it is not clear to me whether the contributions of this work are novel. This is the main reason for my current rating leaning towards rejection, however I am not particularly familiar with the literature on agentic frameworks, so if the authors can clarify the novelty of their contributions I am willing to increase my score.\n\n[1] Reflexion: Language Agents with Verbal Reinforcement Learning, Shinn et al., NeurIPS 2023\n\n[2] Generative Agents: Interactive Simulacra of Human Behavior, Park et al., UIST 2023\n\n**Baselines**\n\nThe comparisons to baselines appears pretty sparse, namely there are only comparisons to ReAct and Search-o1. Are these agentic frameworks state-of-the-art on those benchmarks? Are there any other baselines that could be compared to, like [1,2]? A more thorough comparison would help determine the significance of results. In addition I recommend to also report error bars for the collected results.\n\n[1] Reflexion: Language Agents with Verbal Reinforcement Learning, Shinn et al., NeurIPS 2023\n\n[2] Generative Agents: Interactive Simulacra of Human Behavior, Park et al., UIST 2023\n\n**Presentation**\n\nWhile the paper is generally easy to follow it lacks important details, especially when it comes to the methodology. For example, to me it is unclear what the input to the CEA module is, Figure 3 shows only the query as input, while equation (1) and Section 3.1.2 list more information. LLM-as-a-judge is mentioned for assigning rewards without elaborating on how it is implemented. Reward distribution is mentioned without any specifics. Figure 3 also shows a reference model which is not explained at all in the methodology. It is unclear how a rollout looks like and how it is scored. Moreover, Equation (2) contains several symbols that are undefined, such as $\\theta$, $S$, $\\mathcal{T}_\\text{all}$. To clarify those points, I recommend to add an algorithm that clearly defines all introduced symbols and formally describes how the rollout and the update of CEA works in detail. \n\nAnother point I was struggling with is to understand the parallel execution of CEA. How is it possible to execute CEA in parallel during inference when performing a rollout? Or is the parallel execution strictly about the training phase? If so, this should be clarified in the text, ideally in the methodology section.\n\n**Minor remarks:**\n\n- Table 1 is confusing as the second column in the first block is different from the same column in the lower block without explicit mention about it."}, "questions": {"value": "Line 239: How is the reward distributed across the entire trajectory?\n- Is CERL just a fine-tuned Qwen3-8B model? It is not really explicitly defined anywhere, or maybe I missed it.\n- An important missing information is some characteristics about the used benchmarks, i.e. how many turns do they feature, what context length would be required if you provide everything in context, etc. This is valuable to determine how much CEA compresses the context.\n- What is the concept of compelled response generation?\n- It would be good to provide a definition of the term sustainability, it was not always entirely clear what it means to me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lm4inPJAqD", "forum": "6QUNblHtto", "replyto": "6QUNblHtto", "signatures": ["ICLR.cc/2026/Conference/Submission19540/Reviewer_ijDE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19540/Reviewer_ijDE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683142315, "cdate": 1761683142315, "tmdate": 1762931427319, "mdate": 1762931427319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors have developed a CEA (Context Engineering Agent), which is a modular architecture which claims to help to manage context in long-horizon LLM-based general research agents . It aims to fix context rot through a two-component design that splits the processing of reasoning from context management. CEA is built from a four-part context (Task Query, Dynamic Plan, Historical Memory, Semantic Facts) and is trained via a novel algorithm which is labeled Context Engineering RL, which filters out trajectories to focus on context-attributable errors. Experiments on three web-browsing benchmarks (two of which are the same, but one in Chinese and one in English) show 30–100% improvements over ReAct (2023) and Search-o1 (2025) baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The problem that is being tackled (context rot) is a well-known one, which is clearly a bottleneck in many long context-window systems, so the aims of the work are clear and important.\n2) The architectural design itself is clean and makes intuitive sense. Having the Reasoner/CEA split is sensible, and the four-part context representation is well-structured and also aligns with the cognitive needs of multi-step reasoning.\n3) The empirical results are clearly very strong and show very large improvement over the baselines.\n4) There are some pseudo-ablations implemented (invocation frequency comparison, serial versus parallel execution, compelled response versus standard reasoning)"}, "weaknesses": {"value": "There are some very important weaknesses to the work, which I believe are, at the moment critical, but could potentially all be addressed.\n1) The main methodological innovation in CERL seems to be that of filtering trajectories to find CEA-attributable errors. However, the details of this are entirely missing. The paper refers to sections of the supplementary material (appendix 11 and 12) for algorithmic details, but these don't seem to exist and contain neither pseudocode nor ablation results which verify the mechanism. The explanation is completely absent from the main text where the contribution is claimed. The main missing details are:\na) The algorithm or indeed any heuristic for error attribution.\nb) The labelling procedure (human vs. automated) and error taxonomy\nc) The filtering statistics (acceptance rate, error-type distribution)\nd) Any validation method for attribution accuracy\ne) Any true ablations: no-filter, random-filter, threshold sweep\n2) There is only a very narrow task scope. The paper claims to solve a problem in a lot more generality than is actually shown. All of the benchmarks involve web-information retrieval. There are no benchmarks that include pure reasoning without external search (like mathematical reasoning, code generation, or long-context Q&A). This is fine, but then the claims have to be appropriately tempered. The empirical results only indicate that the approach works as a specialized web-retrieval agent\n3) The baselines are very limited. One baseline is from 2023 and only one is from 2025, but in the related material, many other approaches are discussed (like HiRA, BrowseMaster, WebRL, and DeepResearcher). It's claimed that \"These approaches have significantly advanced agent capabilities but have not specifically focused on optimizing context management as a distinct learning objective\" but they still seem like important comparators. It's not clear why there is such a narrow comparison when it would seem that there are other systems out there that may be different but still comparable in terms of empirical results.\n4) The authors have included prompt templates and an overview of the architecture but they haven't included some really important information, like the CERL hyper parameters, the compute and data budgets, the parallel execution protocol or more in-depth wall-clock profiling (there is, but it's very limited). Where there are prompt templates, there's no clarity in the details.\n5) As indicated already, there are major issues with the supplementary material:\na) There are no numbered or labeled sections despite references to them (appendix 11 and 12 are mentioned in the main text, but don't exist).\nb) There is placeholder text such as “add caption” left in tables.\nc) There is also misplaced content, including a block labeled turn 31 appearing mid-section and Example 2 which is presented out of sequence.\n\nSome more minor typos and issues:\nTypo: “enthousiasm” → “enthusiasm”\nFigure 2: What does \"CEA pad zoom details.\" mean?\nEq. 2: sampling process undefined anywhere"}, "questions": {"value": "The questions are all related to the weaknesses and so there is a lot which is unclear from the text which needs to be addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u8lARmEKZQ", "forum": "6QUNblHtto", "replyto": "6QUNblHtto", "signatures": ["ICLR.cc/2026/Conference/Submission19540/Reviewer_zA4Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19540/Reviewer_zA4Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895728787, "cdate": 1761895728787, "tmdate": 1762931426863, "mdate": 1762931426863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work attempts to address an alleged inference failure called “context rot,” arguing that as the context window grows, large language models’ accuracy degrades and on binary-success tasks could fail entirely. To counter this, the authors propose a Context Engineering Agent (CEA) that leverages trajectories of intermediate reasoning and responses to track task progress and surface the indispensable words within each step; they also explore boosting reasoning with GRPO and CEA as CERL. Empirically, CEA outperforms ReAct and Search-o1 across reported settings. Results for the CERL variant are more mixed: on XBench-DS it ties GPT-OSS-120B and surpasses Doubao-Think-1.6, while larger models remain stronger on other benchmarks. The reported improvement depends on applying CEA at every turn of the reasoning process. The authors then investigated compelled response generation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Demonstrates consistent performance gains across numerous experiments, even when limited to small language models. This is useful for realistic settings with tight memory and compute budgets.\n* Presents a model-agnostic approach: CEA can be layered onto existing LLMs by adjusting reasoning chains and context handling, making it straightforward to adopt without retraining full models.\n* Provides clear prompt templates in the appendix, which improves transparency and reproducibility for future comparisons and deployments even if the code is absent."}, "weaknesses": {"value": "* The abstract’s claim that “longer contexts alone do not guarantee better responses” is uncited and conflicts with parts of the scaling literature (see https://aclanthology.org/2024.naacl-long.260/, https://arxiv.org/abs/2403.05530, https://aclanthology.org/2024.acl-long.776/, https://arxiv.org/abs/2402.13753 ); it needs empirical support or authoritative references. \n* Marketing-style metrics (“optimal trade-off between token efficiency and memory integrity”) are undefined; there’s no formal metric, measurement procedure, or units reported.\n* The “context rot” problem relies on a single technical report (Hong et al., 2025) from a startup; the authors are not findable on Google Scholar, and there’s no peer-reviewed corroboration. Further, the propagation of errors affecting performance is well known in lots of sequential modelling tasks for machine learning. I don’t see how creating a theory de nouveau “context rot” offers anything substantial when long-term credit assignment (see https://ieeexplore.ieee.org/document/4066245, https://dl.acm.org/doi/10.5555/2969239.2969370,) /vanishing gradients (see https://ieeexplore.ieee.org/document/279181, https://ieeexplore.ieee.org/abstract/document/6795963) /redundant information (see https://arxiv.org/abs/physics/0004057)  has existed in machine learning for a while. The authors should engage with more foundational work. \n* Typo: “yan yan” on line 064.\n* Long-horizon claims (e.g., “compounding errors accumulate rapidly…”) are uncited and should be backed by prior work or new experiments.\n* I want to acknowledge related work is mostly from 2025 preprints. This may be common with how fast LLM development happens but engaging with past related work is essential and offers better context on the novelty of the presented work; established literature on long-horizon reasoning, memory, retrieval/summarization, and hierarchical planning is under-cited.\n* Reward design is underexplored as a 0/1 output; stronger alternatives (eg. KL-regularized rewards, learned judges,) are not compared.\n* GRPO is re-derived without a clear difference from the standard method; the only change appears to be using CEA trajectories.\n* Performance claims are overstated: CEA performed better than search o-1 and ReAct but CERL does not perform well over normal CEA for the majority of models and ties/loses on some benchmarks; qualify wins, ties, and losses precisely. Why use CERL at all?\n* Benchmark scope is limited by search-API quotas. Remaining benchmarks need to be included.\n* “Affect” should be “effect” in “greatly affect the inference performance.”\n* Baseline/model choices are questionable (use of an un-optimized Qwen3-8B rather than an optimized Qwen).\n* The section “HOW OFTEN SHOULD WE USE CEA TO MANAGE CONTEXT?” lacks motivation given the finding that it “always improves”; it reads like an appendix-level ablation.\n* The statement that prior “deep research” compelled response generation for higher scores lacks citations.\n* The references are ill formatted in the middle of the appendix. This is ironic in that the authors acknowledged LLM assistance for writing a paper about improvement LLM’s retrieval and response abilities"}, "questions": {"value": "This work generally does not feel finished and the claims seem too extended from what was presented. Particularly, the references in the middle of the appendix and not evaluating on all benchmarks  (since they are different from each other) for the last half of the paper allude to unfinished work.\n\n\n\n* Why not replicate the experiments from the ‘Context Rot’ technical report to improve the validity of this allegedly unique phenomena? \n\n* The paper mentioned CEA and CERL decouples context engineering from the primary reasoning tasks. Why not test performances on Graphwalks (https://huggingface.co/datasets/openai/graphwalks), which was tested in the ‘Context Rot’, and the matheval dataset (https://huggingface.co/datasets/RyanYr/MathEval) to see how reasoning ability is maintained or not?\n\n* GRPO is a popular baseline but why not use Dr. GRPO (https://arxiv.org/abs/2503.20783) or RLOO with a normalized baseline instead due to the resource constraints? \n\n* None of the accuracy metrics in the tables include an interval of uncertainty like standard error. Why not decrease the temperature to .80 or more and use the mean accuracy to gather a standard error or deviation? If omitting the uncertainty intervals is standard in this research area, it would be novel to include them.\n\n* Does CERL possibly scale with larger models like QWEN 32B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uBnv8aCoi6", "forum": "6QUNblHtto", "replyto": "6QUNblHtto", "signatures": ["ICLR.cc/2026/Conference/Submission19540/Reviewer_9Jgo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19540/Reviewer_9Jgo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943857974, "cdate": 1761943857974, "tmdate": 1762931426360, "mdate": 1762931426360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies deep research agents, focusing on the challenge of long-context reasoning. Longer contexts do not necessarily lead to better responses.\n\nTo address this issue, the paper introduces an autonomous context control framework built around a Context Engineering Agent (CEA). The proposed method maintains a structured context by efficiently managing historical interactions, tracking ongoing progress, and identifying critical clues.\n\nIn addition, the paper presents CERL, an end-to-end multi-turn reinforcement learning method designed specifically for training the CEA. The overall framework leverages hierarchical agent architectures and reinforcement learning for LLMs to achieve effective context management."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The problem of ensuring the reliability and robustness of deep research agents is interesting and important.\n\nCEA provides an effective approach to optimising context management.\n\nThe paper’s use of hierarchical agent architectures and reinforcement learning for LLMs in managing context is also noteworthy."}, "weaknesses": {"value": "What is the relationship between the CEA module and the LLM-powered memory operations?\n\nIt would be helpful to provide more details about Figure 2. Specifically, where are the four components located in the figure, and what does the Plan Checklist represent?\nDoes the History Summary include only the content generated by the LLM, or are other sources involved?\nHow is the Clues Memo constructed?\n\nCEA appears to follow a hierarchical agent architecture. In Figure 3, is the CEA rollout performed by a single agent? It would be better to include an example of a trajectory for clarity.\nWhat is the reference model used in the experiments?\n\nThe training details of the CEA model are missing and should be described more thoroughly.\n\nIn Table 1, the proposed method does not outperform Qwen3-32B, which requires further explanation."}, "questions": {"value": "Could the paper provide more details to enable the reproduction of the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uuiwTHFcXK", "forum": "6QUNblHtto", "replyto": "6QUNblHtto", "signatures": ["ICLR.cc/2026/Conference/Submission19540/Reviewer_RNRB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19540/Reviewer_RNRB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19540/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763251401939, "cdate": 1763251401939, "tmdate": 1763251401939, "mdate": 1763251401939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}