{"id": "k3Lnh6jp0w", "number": 8898, "cdate": 1758101718532, "mdate": 1759897755643, "content": {"title": "CONVERGENCE OF OPTIMIZERS IMPLIES EIGENVALUES FILTERING AT EQUILIBRIUM", "abstract": "Ample empirical evidence in deep neural network training suggests that a variety of optimizers tend to find nearly global optima. In this article, we adopt the reversed perspective that convergence to an arbitrary point is assumed rather than proven, focusing on the consequences of this assumption. From this viewpoint, in line with recent advances on the edge-of-stability phenomenon, we argue that different optimizers effectively act as eigenvalue filters determined by their hyperparameters. Specifically, the standard gradient descent method inherently avoids the sharpest minima, whereas Sharpness-Aware Minimization (SAM) algorithms go even further by actively favoring wider basins. Inspired by these insights, we propose two novel algorithms that exhibit enhanced eigenvalue filtering, effectively promoting wider minima.Our theoretical analysis leverages a generalized Hadamard–Perron theorem and applies to general semialgebraic $C^2$ functions, without requiring additional non-degeneracy conditions or global Lipschitz bound assumptions. We support our conclusions with numerical experiments on feed-forward neural networks.", "tldr": "Convergence of optimizers implies eigenvalues filtering at equilibrium: \"if it converged eigenvalues were small enough\"", "keywords": ["Optimization", "Convergence", "Stability", "Large-Steps", "Curvature", "SAM", "Edge-of-stability"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5bbc5229d4d25ea260d384d445b65cbfe783a4ec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this work the authors develop a general framework for analyzing the properties of the loss landscape after convergence. Their methodology uses tools from analysis/dynamical systems theory to derive the geometric properties of any converged points. Their framework recapitulates known results for GD and some momentum variants, as well as some SAM variants.\n\nThe authors propose some SAM variants with different properties at convergence according to their methods, and provide some experimental evidence that these variants have stronger regularization properties."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper provides a very clean general setup for analyzing the loss landscape properties at convergence. In addition all the main theoretical results are cleanly presented and generally reference the previous works well."}, "weaknesses": {"value": "The main weakness of the work is that it doesn't give any actionable lessons to the community working on sharpness regularization. The method called Hessian SAM in the paper is known in the literature (e.g. as Penalty SAM here [1]). The 2-step SAM has no benefits for a large computational cost. The general idea that SAM modifies the edge of stability (and therefore converged eigenvalues) is already known. While having a more precise theoretical characterization is nice, many of these results come quite naturally from Taylor expansion of losses around stationary points.\n\nThe experiments use minibatching, which can strongly affect the curvature dynamics, particularly as it can become unclear if networks actually reach convergence in this regime. In addition, some of the experimental settings use ReLU, which has to be treated very carefully with what the authors call Hessian SAM (Penalty SAM in the reference) [1].\n\n\n\n[1] https://proceedings.neurips.cc/paper_files/paper/2024/hash/ee3ce0121939f42098cdefd3ea025bf1-Abstract-Conference.html"}, "questions": {"value": "How does proposition 3.7 compare to the edge of stability bound in [1], e.g. Equation 16? there appears to be a potential discrepancy of a factor of $\\lambda$ in the term linear in $\\rho$.\n\nHessian USAM has been studied elsewhere, particularly in [2] where it is called \"Penalty SAM\". How do the results of [2] relate to the results in the paper?\n\n[1] https://proceedings.mlr.press/v202/agarwala23a\n\n[2] https://proceedings.neurips.cc/paper_files/paper/2024/hash/ee3ce0121939f42098cdefd3ea025bf1-Abstract-Conference.html"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "E1fcTLu4Mm", "forum": "k3Lnh6jp0w", "replyto": "k3Lnh6jp0w", "signatures": ["ICLR.cc/2026/Conference/Submission8898/Reviewer_ocNd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8898/Reviewer_ocNd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761588639770, "cdate": 1761588639770, "tmdate": 1762920653626, "mdate": 1762920653626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified theoretical perspective to analyze the convergence of various optimization algorithms. Building on this perspective, the authors introduce two new optimizers, USAM2 and HUSAM, and provide convergence results. Experimental validation is conducted on MNIST and CIFAR10 using standard architectures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of viewing the convergence of different optimizers through a unified lens is interesting and conceptually valuable.  \n\n2. The theoretical development for the proposed USAM2 and Hessian-USAM algorithms is well-presented."}, "weaknesses": {"value": "1. Aside from the authors’ own methods (USAM2 and HUSAM), the theoretical results for other optimizers seem largely covered by prior work, so the novelty is limited.  \n\n2. Experimental evaluation is weak. MNIST is too small to provide convincing evidence, and the performance gains on MNIST of USAM2 and HUSAM, especially in accuracy, are very marginal.  \n\n3. The WideResNet-16-8 on CIFAR10 setting is outdated. In this setting, USAM2 even underperforms USAM. Although HSAM shows a more noticeable improvement, it requires second-order Hessian information. It is questionable whether such a method is practical or valuable for large-scale neural networks.  \n\n4. The empirical section lacks experiments on modern benchmarks and larger-scale models where the claimed contributions would matter."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GwWQwkpdjq", "forum": "k3Lnh6jp0w", "replyto": "k3Lnh6jp0w", "signatures": ["ICLR.cc/2026/Conference/Submission8898/Reviewer_1fUe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8898/Reviewer_1fUe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761680012007, "cdate": 1761680012007, "tmdate": 1762920653218, "mdate": 1762920653218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides an interesting new perspective on the properties that the convergence point should satisfy under the assumption that the algorithm converges. The authors present a general statement based on the Stable Manifold Theorem, showing that the corresponding spectral radius is at most 1. They further apply their theory to several different algorithms, including gd, usam, and momentum variants, and demonstrate that different algorithms essentially select minima with different degrees of flatness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper establishes an interesting and novel theoretical framework that studies a “dual” version of stability analysis, in which the algorithm is assumed to converge and the focus is on the properties of the initialization and the solution. The theory is built upon a generalized Hadamard–Perron stable manifold theorem. In contrast to stability analyses that require local diffeomorphism assumptions, this work relies on much milder conditions, such as the requirement of only semi-algebraic functions, and remains valid for large step sizes. The authors apply their theory to various practical algorithms, demonstrating different eigenvalue filtering effects and providing experimental results that validate their theoretical claims. Overall, the arguments presented in this paper are sound and well supported."}, "weaknesses": {"value": "Although I enjoyed reading Section 2 of this paper, there are several shortcomings. The authors repeatedly claim that their theory is built upon weaker assumptions; however, their framework requires an additional assumption of algorithmic convergence, which is itself non-trivial and implicitly imposes constraints. For example, while Theorem 2.1 is stated to hold for large (even unbounded) step sizes, excessively large step sizes would clearly lead to divergence.\n\nMore importantly, to the best of my knowledge, the analyses for different algorithms in this paper do not appear to yield new results. For instance, Proposition 3.1 does not differ in essence from the well-known stability condition, and Proposition 3.4 does not go beyond the findings of Zhou et al. (2025), who analyzed the stability of USAM in detail. Although the theoretical perspective is interesting, it seems to offer limited new insights to the machine learning community."}, "questions": {"value": "The class of semi-algebraic functions does not include common deep learning activation functions such as sigmoid and tanh. Although the authors claim that their results can be extended to all smooth losses, it remains unclear why this was not done, which may give the impression that the work is somewhat incomplete.\n\nThe authors mention the connection to EOS several times, but the discussion is too brief. Would it be possible to conduct a deeper analysis to provide new insights, or include some related experimental evidence?\n\nSince the results presented in this paper are for the full-batch setting, why are stochastic variants still used in the experiments?\n\nMinor: The definition on Semi-algebraic sets and functions should be moved to the beginning of Section 2 to aid understanding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8uELTXXx2K", "forum": "k3Lnh6jp0w", "replyto": "k3Lnh6jp0w", "signatures": ["ICLR.cc/2026/Conference/Submission8898/Reviewer_UZZ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8898/Reviewer_UZZ3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876048124, "cdate": 1761876048124, "tmdate": 1762920652601, "mdate": 1762920652601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the following optimization question: assuming an algorithm converges, what properties does its limiting point satisfy? The authors show that at any convergence point, the Jacobian of the update map has a spectral radius $\\le 1$, which imposes eigenvalue filters on the Hessian that depend on the hyperparameters. They then apply this general result to several examples, including Gradient Descent, Heavy Ball, Nesterov’s method, and Unnormalized Sharpness-Aware Minimization (USAM). For USAM, the filter can admit negative eigenvalues, suggesting possible convergence to saddle points. Motivated by this observation, the authors propose Two-step USAM and Hessian USAM, whose filters exclude negative curvature, thus avoiding strict saddles. Small-scale experiments (MLPs on MNIST/Fashion-MNIST and WRN on CIFAR-10) show smaller top Hessian eigenvalues at convergence and similar performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tUnderstanding the implicit bias or structure of the minima that optimization algorithms converge to is an interesting research problem.\n-\tThe paper’s main contribution is to characterize the geometry of the minima to which the algorithm can converge (assuming convergence). The assumption is quite general and covers many relevant settings.\n-\tOverall, the paper is well written and easy to follow."}, "weaknesses": {"value": "-\tThe proposed algorithms, Two-step USAM and Hessian USAM, appear to require more computation or backpropagation per iteration than SAM. Since SAM updates are already relatively expensive, the proposed methods may be slower for large-scale applications.\n-\tThe experiments conducted on small-scale datasets are not sufficient to support the empirical claims, especially it is not clear to see the difference between the performance. Larger-scale experiments would better validate the results."}, "questions": {"value": "-\tIs there any analysis or discussion of the additional complexity of the proposed algorithms compared to SAM?\n-\tCan the results be extended to the stochastic setting or to adaptive methods such as Adam?\n-\tIf additional structure of the problem is known (e.g., certain smoothness assumptions on the objective function), could the results be strengthened to hold for all parameters instead of almost surely?\n-\tIs there a robustness version of the results? For $x$ near the limiting point, is the Jacobian’s spectral radius bounded by $1+\\delta$ for some small $\\delta$?\n-\tIt would be helpful to include more details about the experimental setup in the appendix. For example, how is the performance of different algorithms compared fairly. Are they run for the same number of epochs or the same number of backpropagations (since the proposed algorithm seems to require more backpropagations per iteration)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XcueNV9Ht3", "forum": "k3Lnh6jp0w", "replyto": "k3Lnh6jp0w", "signatures": ["ICLR.cc/2026/Conference/Submission8898/Reviewer_crYV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8898/Reviewer_crYV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986795628, "cdate": 1761986795628, "tmdate": 1762920652129, "mdate": 1762920652129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}