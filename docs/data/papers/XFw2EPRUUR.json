{"id": "XFw2EPRUUR", "number": 11894, "cdate": 1758204508998, "mdate": 1759897548318, "content": {"title": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks", "abstract": "Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes.\nMixture‑of‑Experts (MoE) models, now standard in state‑of‑the‑art systems, introduce a new sparsity dimension that current dense‑model frontiers overlook.\nWe investigate how MoE sparsity influences two distinct capability regimes: memorization skills and reasoning skills.\nBy training MoE families that vary total parameters, active parameters, and top-$k$ routing under fixed compute budgets, we disentangle pre-training loss from downstream accuracy. \nOur results reveal two principles. First, Active FLOPs: models with identical training loss but greater active compute achieve higher reasoning accuracy. Second, Total tokens per parameter (TPP): memorization tasks improve with more parameters, while reasoning tasks benefit from optimal TPP, indicating that reasoning is data-hungry. \nNeither reinforcement learning post-training (GRPO) nor increased test-time compute alters these trends. \nWe therefore argue that optimal MoE sparsity must be determined jointly by active FLOPs and TPP, revising the classical picture of compute-optimal scaling. \nAll code, data sources, and logs are released to facilitate reproducibility and future work.", "tldr": "Memorization skills consistently benefit from higher sparsity, while reasoning skills require balancing active FLOPs with total tokens per parameter; the optimal point shifts with the compute budget.", "keywords": ["Mixture of Experts", "memorization", "reasoning", "scaling laws", "large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa622e7ca311375ab42c0ab5dd8d18ef74080287.pdf", "supplementary_material": "/attachment/df31aa73f1aeffa1cb765205b461238323b67315.zip"}, "replies": [{"content": {"summary": {"value": "This paper explores how the sparsity of MoE models affects their performance on downstream tasks. While prior scaling law works have mostly focused on pretraining loss or efficiency, this work reveals that downstream memorization and reasoning capabilities respond differently to sparsity. The authors conduct very extensive empirical analysis on a range of benchmarks. Through various analyses, they observe interesting points (see Strengths section) that could be further explored and taken into consideration when building or deploying MoEs."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents several insightful findings based on systematic analysis of MoE sparsity across a variety of experiments.\n\n- Active FLOPs has a significant effect on the downstream task performance, not just determined by pretraining loss alone.\n- The study uncovers an important trade-off btw memorization and reasoning with TPP, where memorization skills are parameter-hungry and reasoning skills are data-hungry.\n- Post-training or test-time scaling do not change the memorization-reasoning gap, so the optimal sparsity must be determined pretty much during the pretraining stage."}, "weaknesses": {"value": "While the observations are novel and well-supported empirically, I would consider it more significant to find any **intuitive or theoretical rationales** behind them. For instance, why does reasoning capability require denser MoEs, while memorization thrives on sparsity? Why doesn’t simply scaling the parameter count work?"}, "questions": {"value": "In Line 321-323:  \n\n> *At lower FLOPs, increasing sparsity still reduces loss and improves accuracy; however, once the FLOPs budget grows, denser models begin to perform better, achieving both lower loss and higher accuracy.*\n\nHowever, I could not find where this trend is clearly demonstrated. Figure 5 appears to fix the FLOPs budget, so it doesn’t reveal how model performance varies as FLOPs increases."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sIoWFqOrzq", "forum": "XFw2EPRUUR", "replyto": "XFw2EPRUUR", "signatures": ["ICLR.cc/2026/Conference/Submission11894/Reviewer_fnio"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11894/Reviewer_fnio"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761228405585, "cdate": 1761228405585, "tmdate": 1762922908360, "mdate": 1762922908360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper empirically studies the effect of sparsity in sparse mixture of experts (MoEs) on downstream tasks. Specifically the paper trains a series of models based on Mixtral architecture and studies downstream performance on tasks that tend to rely on memorization vs tasks that test reasoning capabilities of language models (LMs). The paper studies the relationship between training loss and task loss and provides insights on the effect of active parameter count and tokens per parameter (TPP) on downstream tasks described above. The main findings include active FLOPs may lead to higher reasoning accuracy and that higher tokens per parameter may be preferable for reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper studies the effect of sparsity in MoEs on downstream tasks. This area has not been examined in detail so the study in the paper is timely and will likely be of interest to many researchers & practitioners.\n\n- The empirical setup including models, data and downstream tasks are  described clearly in the paper. This gives me confidence that the experiments are reproducible.\n\n- The models considered in the paper are not necessarily compute-optimal. This detail may provide additional insight into how to optimally train and use MoEs at scale.\n\n- The paper proposes tokens per parameter (TPP) as a metric to track in addition to active parameters. This metric provides additional insight into the role of data in training MoEs that work well on downstream reasoning-type tasks. This may be interesting to many readers (and is definitely interesting to this reader)"}, "weaknesses": {"value": "- The paper considers a single architecture inspired by Mixtral family of MoEs in the work. It's understandable why this choice was made (experiment volume) but I do wonder if other architecture choices can change the conclusions made here. If possible, please discuss why Mixtral was chosen as opposed to other choices. \n\n- The fact that memorization depends on total parameter count is known from prior literature. Furthermore, active number of parameters (inference FLOPs) have also observed to improve certain downstream tasks' performance (Abnar et al., 2205). So the claim that these are new contributions is weak.\n\n- While sparsity in mentioned in the paper, the plots (Figure 1) for instance do not show this value but instead show top-K. This makes it hard for the reader to infer the effect of sparsity on empirical observations."}, "questions": {"value": "- Is there a way to show how downstream performs with sparsity where sparsity is defined in the paper as 1 - (active / total experts)? Sparsity is mentioned in the paper but is not shown explicitly in scaling plots. Please include sparsity value, if possible, with the plots. Only Figure 5 appears to include sparsity (via density term which is its complement).\n\n- The range of accuracy/error rate for GSM8K task appears to be on the lower side? Are these values good enough for readers to draw valid conclusions? A discussion on what is reasonable would be very useful to help the reader."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GhcntL4HWM", "forum": "XFw2EPRUUR", "replyto": "XFw2EPRUUR", "signatures": ["ICLR.cc/2026/Conference/Submission11894/Reviewer_tKtK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11894/Reviewer_tKtK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955420609, "cdate": 1761955420609, "tmdate": 1762922907910, "mdate": 1762922907910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to investigate how MoE sparsity influences two distinct capability regimes: memorization skills and reasoning skills. The work shows how Active FLOP is more important for reasoning, while memorization improves with number of total parameters. Another interesting finding provided in this work is that changing the k in top-k routing has a negligible effect if the number of active parameters is kept constant."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. It is an important observation that for MoE models, downstream accuracy can deviate from the predictions of conventional scaling laws, and these deviations may vary across different tasks.\n\n2. Exhaustive experimentation is done in reasoning and coding tasks to demonstrate the U shape of tasks performance with the increase of total parameters at a FLOP controlled setting\n\n3. Exhaustive experiments are done to show that post training couldn't improve this."}, "weaknesses": {"value": "1. The number of tokens used seems small to if we are targeting End task performance, specially for MOE models\n2. It would be good to get some ablation for various router choices, though than can be a future work\n3. In Page 9, figure 8, it would be good do the study at k>1 (ideally 8) and E >8\n4. More details about the post training setup is helpful. How many tokens in the post training set?\n5. No details have been provided whether Continuous training is done or learning rate is annealed before evaluating end task"}, "questions": {"value": "1. The number of tokens used seems small to if we are targeting End task performance, specially for MOE models\n2. It would be good to get some ablation for various router choices, though than can be a future work\n3. In Page 9, figure 8, it would be good do the study at k>1 (ideally 8) and E >8\n4. More details about the post training setup is helpful. How many tokens in the post training set?\n5. No details have been provided whether Continuous training is done or learning rate is annealed before evaluating end task"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nSXezajIgt", "forum": "XFw2EPRUUR", "replyto": "XFw2EPRUUR", "signatures": ["ICLR.cc/2026/Conference/Submission11894/Reviewer_pkQZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11894/Reviewer_pkQZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961397795, "cdate": 1761961397795, "tmdate": 1762922907478, "mdate": 1762922907478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the optimal sparsity of Mixture-of-Experts models under memorization and reasoning skills by training MoE models with varying total parameters, sparsity, and top-k routing under the fixed budget. Through extensive experiments, the paper concludes that 1. the downstream reasoning quality is decided by both the active FLOPs and pretraining loss and 2. there exist different optimal tokens-per-parameter ratios for memorization and reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to understand.\n- The experiments are comprehensive while supporting the major claims of the paper.\n- One of the main findings is surprising, as it shows that higher sparsity only improves performance under memorization instead of reasoning tasks under the iso-FLOP settings."}, "weaknesses": {"value": "- The paper might need to address more about its intuition and originality from previous works such as [1] and [2], since similar observations regarding the optimal sparsity in MoE models have been made.\n- Theoretical insights are encouraged to explain the experimental findings.\n- The U-shape trend plot for reasoning tasks in Figure 2 is very interesting, and I suggest the authors to verify such finding under more reasoning tasks.\n\n[1] Samira, Abnar, et al. \"Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models.\" arXiv:2501.12370 (2025).\n\n[2] Zhao, Jinze, et al. \"Sparse Mixture-of-Experts for Compositional Generalization: Empirical Evidence and Theoretical Foundations of Optimal Sparsity.\" arXiv:2410.13964 (2025)"}, "questions": {"value": "Questions are addressed in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nxeoKou1vw", "forum": "XFw2EPRUUR", "replyto": "XFw2EPRUUR", "signatures": ["ICLR.cc/2026/Conference/Submission11894/Reviewer_p7WF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11894/Reviewer_p7WF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762090428092, "cdate": 1762090428092, "tmdate": 1762922906752, "mdate": 1762922906752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}