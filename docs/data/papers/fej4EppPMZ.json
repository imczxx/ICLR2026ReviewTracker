{"id": "fej4EppPMZ", "number": 1425, "cdate": 1756881477437, "mdate": 1762930912621, "content": {"title": "Zero-to-Interaction: Generating Dynamic Videos from Synthetic State Transitions", "abstract": "While recent video generative models can synthesize high-fidelity videos, they struggle to portray plausible physical interactions and the resulting state transitions, a critical bottleneck for applications in robotics and VR/AR. To address this, we introduce a framework to generate a scalable synthetic dataset of controllable interactions. Our pipeline leverages a structured taxonomy and state-of-the-art image editing models to create explicit 'start' and 'end' state images, which serve as visual anchors for the interaction. To generate a seamless video utilizing these anchors, we propose State-Guided Sampling (SGS), a novel sampling technique that mitigates artifacts common in naive conditional generation. Furthermore, we develop and validate a new automated evaluation system that aligns with human judgments to ensure data quality. Experiments show that fine-tuning a base model on our dataset significantly enhances its ability to generate plausible interactions. The dataset, code, and evaluation tools will be released.", "tldr": "We propose a framework featuring State-Guided Sampling that utilizes 'start' and 'end' state images to generate a synthetic dataset of physical interactions, which enhances a base model's ability to create plausible state transitions via fine-tuning.", "keywords": ["Video Generative Model", "Dataset", "Benchmark"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f51c60e42be2b56f33a53611a93c58d84a58f72d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a pipeline for generating synthetic interaction videos, which leverages a structured taxonomy to create prompts, generates \"start\" and \"end\" state images, and uses State-Guided Sampling to produce seamless videos. It also develops a benchmark to evaluate such videos, assessing four key criteria via a hybrid framework integrating VLM scores and auxiliary features to align with human judgments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "From some results, it can be seen that the quality of interactions in the generated videos has indeed improved."}, "weaknesses": {"value": "- Collecting better data to tune the model for improved performance seems obvious; this aspect lacks academic innovation and is more of an engineering effort.\n- The multi-stage method for generating video data with high-quality interactions is very straightforward. It is a common approach used in both industry and academia to construct large-scale image/video editing datasets, making it hard to perceive impactful insights.\n- The authors discuss the respective advantages and disadvantages of the I2V model and FLF model, then attempt to achieve optimal results using SGS. From the results provided by the authors, I do not actually perceive obvious differences between the methods. Another question is whether the poor performance of the FLF model is simply due to inadequate training of Wan-FLF; a better FLF model might eliminate the need for methods like SGS, which undermines the contribution and innovation of this approach. (to my knowledge, the FLF model is not a universally accepted term in academia, and the authors have not provided references for it)"}, "questions": {"value": "Can the quality of interactions in videos generated by the model be improved by repeatedly trying different text prompts and image prompts? Can this method be included in the comparative experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k1dEgEzWEL", "forum": "fej4EppPMZ", "replyto": "fej4EppPMZ", "signatures": ["ICLR.cc/2026/Conference/Submission1425/Reviewer_c29K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1425/Reviewer_c29K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1425/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726421039, "cdate": 1761726421039, "tmdate": 1762915766403, "mdate": 1762915766403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "W0DsB8ZNUE", "forum": "fej4EppPMZ", "replyto": "fej4EppPMZ", "signatures": ["ICLR.cc/2026/Conference/Submission1425/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1425/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762930521012, "cdate": 1762930521012, "tmdate": 1762930521012, "mdate": 1762930521012, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for adapting the pre-trained video generative model to follow the dynamic rules and synthesize physically plausible transitions. The proposed framework consists of prompt editing, image synthesis, and state-guided sampling. Its performance is evaluated on several tasks across diverse dimensions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-motivated, and the proposed method is sound. This paper proposes an entire framework to enhance the dynamic following ability of pre-trained video generative models. \n- This paper also includes a detailed evaluation pipeline to test whether the generated videos are temporally and dynamically consistent."}, "weaknesses": {"value": "- The proposed method is not novel, and there are limited insights in this paper. The start-and-end-frame condition is not new. And the proposed state-and-guided sampling is not theoretically sound. I suspect whether the weighted velocity preserves a valid marginal data distribution $p(x_0)$. Any theoretical analysis is welcome to be added to the paper.\n- Moreover, the proposed method requires additional computation for generating a video. For example, the prompt and image generation that needs a large model (e.g., GPT4o) and a double denoising forward process.\n- The empirical performance is not obviously advantageous compared with baselines. For example, the results in Table 1 and Table 4. Additionally, no standard deviation is provided.\n- The demos provided in the paper and webpage are not that impressive. Considering the more computation required by the method, I expect to see more distinct advantages.\n- Overall, while I appreciate the engineering efforts made in this work, I tend to a 'reject' rating considering the limited novelty and insights."}, "questions": {"value": "- Does the proposed method work for other pre-trained video models? Can this method be adapted to an autoregressive transformer-based architecture?\n- Are there any failure cases? I believe the video generation quality still heavily depends on the performance bound of the pre-trained models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LS0BlrPnK1", "forum": "fej4EppPMZ", "replyto": "fej4EppPMZ", "signatures": ["ICLR.cc/2026/Conference/Submission1425/Reviewer_FK5a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1425/Reviewer_FK5a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1425/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994837752, "cdate": 1761994837752, "tmdate": 1762915766273, "mdate": 1762915766273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Zero-to-Interaction, a modular framework for generating physically consistent object-interaction videos from synthetic data. It leverages large vision-language models to construct structured prompts and generate paired “start” and “end” state images, which serve as anchors for interaction synthesis. A new **State-Guided Sampling (SGS)** method then blends I2V and FLF (start-and-end-frame conditioned) velocity fields to produce smooth transitions. An automated **VLM-based evaluation framework** further assesses interaction quality. Experiments show clear gains over baselines such as HunyuanVideo, PhyI2V, and Wan 2.1 in both quantitative and human evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Originality\n1. The framework of this article is innovative, using VLM as a proxy for preprocessing the input, the idea of starting and ending states is original, and the proposed State-Guided Sampling algorithm is also original.\n\n### Technical Quality\n1. The taxonomy-based prompt system is well-structured and scalable, yielding diverse and creative interaction scenarios beyond existing datasets.\n2. The hybrid VLM + auxiliary metric system (plausibility probe, artifact detector, etc.) is a notable contribution for automated video assessment, aligning well with human judgment.\n\n### Clarity\n1. The implementation is well-documented and modular, with commitment to releasing data, code, and evaluation tools, enhancing reproducibility and community impact.\n\n### Significance\n1. The paper tackles a relatively underexplored problem: enabling video generation models to represent physically consistent object interactions and state transitionsKey components (e.g., GPT-4o, Gemini-2.5) and commercial image editors reduce reproducibility and limit transparency, even though the pipeline itself is modular., a capability that holds significant relevance for fields such as robotics, world modeling, and visual simulation."}, "weaknesses": {"value": "1. Key components (e.g., GPT-4o, Gemini-2.5) and commercial image editors reduce reproducibility and limit transparency, while also increasing the overall computational and financial cost of using the proposed framework.\n2. Since the evaluation system partially reuses VLM-based components similar to those in training or data generation, it may not fully avoid self-consistency bias. Independent benchmarks could strengthen credibility.\n3. While SGS improves temporal smoothness, it introduces two separate models, which increase the overall system complexity and computational cost; however, the paper lacks corresponding analysis or justification for this design choice."}, "questions": {"value": "1. What is the practical scalability of the proposed pipeline — specifically, how long and how costly is it to generate one high-quality interaction video when the entire pipeline is used (including VLM-based preprocessing for prompt construction and for synthesizing the start/end state images)?\n2. The paper claims that the proposed method benefits applications in robotics and VR/AR. How is this claim substantiated? Specifically, how does the method perform in robotics-related data generation, and is there any evidence that it actually helps improve the training of embodied AI?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JM6I5T3xFY", "forum": "fej4EppPMZ", "replyto": "fej4EppPMZ", "signatures": ["ICLR.cc/2026/Conference/Submission1425/Reviewer_91ac"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1425/Reviewer_91ac"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1425/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997138437, "cdate": 1761997138437, "tmdate": 1762915766126, "mdate": 1762915766126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a way to prompt the existing image2video generation model to produce more finegrained and realistic dynamic motion. This is achieved by first creating plausible start and end frames from the text, then taking those frames to generate the final video using a proposed guided sampling. The paper also introduces a quality metric assessment system using finetuned VLM. It also introduce a synthetic dataset of around 5k videos with comprehensive quality control for finegrained motion in the videos. Overall, the proposed method seems to produce better dynamic videos than prior baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A simple method to squeeze out more video generation with fine-grained motion capabilities from existing open-weight models\n2. A controlled synthetic dataset with fine-grained motion"}, "weaknesses": {"value": "1. It is not clear that the method is scalable to produce even more dynamic video needed for training even better foundation video model. The generation ability is limited by the ability of the pretrained model. \n2. It used the LLM to generate interaction and dynamic state changes. It is also limited by the generation of the LLM.\n3. It is unclear whether the proposed the temporal artifact detection is helpful given that current video generation model, like Veo3, Sora2, Wan, or HunyuanVideo, are often already very realistic. It is also trained on UCF-101 which is low-res and limited in motion (vs. something newer like Kinetics or HMDB51)"}, "questions": {"value": "n/a. The authors should address the weekness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YRzuyOitoD", "forum": "fej4EppPMZ", "replyto": "fej4EppPMZ", "signatures": ["ICLR.cc/2026/Conference/Submission1425/Reviewer_VDEv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1425/Reviewer_VDEv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1425/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016121160, "cdate": 1762016121160, "tmdate": 1762915766012, "mdate": 1762915766012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}