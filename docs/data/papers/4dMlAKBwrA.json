{"id": "4dMlAKBwrA", "number": 9735, "cdate": 1758137107969, "mdate": 1759897701611, "content": {"title": "MULTIMODALITY AS SUPERVISION: SELF-SUPERVISED SPECIALIZATION TO THE TEST ENVIRONMENT VIA MULTIMODALITY", "abstract": "The common approach for developing a vision model is generalism, which involves training on a large diverse dataset to cover the varied deployment environments and leads to a model that is expected to solve the problem everywhere. However, many practical applications need to operate in a specific test space, e.g., a robot deployed in a single house, and do not necessarily need to generalize to novel environments. In this work, we explore whether we can use rich multimodal data only from the test environment to pre-train a representation in a self-supervised way, without access to any external data.\nWe find that this approach can match and, in most cases, outperform generalists pre-trained on large-scale Internet datasets, including popular off-the-shelf models, CLIP and DINOv2. We study the effectiveness of this approach by evaluating the models on various datasets and downstream tasks, such as semantic segmentation, captioning, and object detection, as well as a set of ablations and analyses to extract insights. This approach raises intriguing points on substituting data with (multi)modality, enabling an alternative scenario where the need for external Internet-scale datasets for pre-training models is reduced. It also shows that merely benefiting from test-space data was insufficient for achieving competitive results, and multimodality was essential for that purpose.", "tldr": "Studying 'multimodality as self-supervision', to learn a representation that achieves SOTA in the test environment without using external/internet-based data", "keywords": ["specialization", "multimodal", "transfer learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d31027bdf270b828d39a1365c1220ae7edb120b.pdf", "supplementary_material": "/attachment/5699e495a02ad74b0adb78a08ca1b1d9fa30ad41.zip"}, "replies": [{"content": {"summary": {"value": "This paper explores how rich multimodal data collected directly from the test environment can be used to pre-train visual representations in a self-supervised manner, without relying on any external data. To assess its effectiveness, the authors evaluate the proposed approach across multiple datasets and downstream tasks, showing that it can even outperform generalist models pre-trained on large-scale Internet datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes training a specialized model using rich multimodal data collected from the test space without relying on external data sources, which is highly practical and closely reflects real-world scenarios. Moreover, they provide an insightful analysis of the trade-off between specialization and generalization, as well as the effectiveness of specialization, demonstrating its practical value.\n\n2. The viewpoint presented in lines 172–174 and fig. 4 is quite insightful. Nowadays, most research focuses on training large-scale generalist foundation models with massive datasets, but this paper highlights the often-overlooked importance of multimodality.\n\n3. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. The authors pre-train their model on the entire dataset but evaluate it on only a small portion of it. For instance, in the Replica dataset, the model is pre-trained on 84,889 samples but evaluated on only 5,000 images. Could the authors clarify the rationale behind this evaluation setting and discuss whether it might affect the reported results?\n\n2. In Table 2, the superscript “²” attached to Task-Specific Methods is referenced on page 8, but its explanatory note appears on page 7."}, "questions": {"value": "Could the authors provide more explanation on why TST-MM performs less effectively on the captioning task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0CjrlBLtPd", "forum": "4dMlAKBwrA", "replyto": "4dMlAKBwrA", "signatures": ["ICLR.cc/2026/Conference/Submission9735/Reviewer_r7Ks"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9735/Reviewer_r7Ks"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768241477, "cdate": 1761768241477, "tmdate": 1762921231698, "mdate": 1762921231698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles an important problem, how to use data from a given deployment scenario effectively in the pre-train and post-train phase to maximally improve performance in that specific scenario we care about -- this would be the central challenge in deploying robotics applications in the real world. \n\nThe proposed method is straightforward, they use a self-supervised multi-modality objective to pre-train the model on the test space only. Additionally, they use features from web-scale models as additional self-supervisory alignment targets for improving performance, showing that direct access to web-scale data while training is unnecessary and methods like Attention Transfer are enough. The authors further show more analysis of the specialization-generalization tradeoffs, paving the way for personalization of models for each deployment site through their simple training framework."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 4}, "strengths": {"value": "In general, a well-executed paper with thoughtful experiments targeting a real problem.\n\n1. The idea of employing test-space data to personalize the model to a given deployment scenario is feasible and very useful in many real world situations.\n2. TST-Adaptation and TST-Sensory methods make sense to me, glad to know self-supervised cross modality helps as a self-supervised objective.\n2. Very useful ablations targeting the real deployment problems that would plague perception and robotics in the coming years. Namely, \"How many spaces is one test space worth?\" and the \"specialization-generalization tradeoffs\" are valuable signals and will become increasingly critical for robotics deployment in the coming years."}, "weaknesses": {"value": "1. Parts of the paper over claims w.r.t the TST-MM method. \n\n(a) TST-MM is trained on additional pseudo modalities obtained by executing generalist models on the test space data. If we employ pseudo labelling from generalist models like CLIP, ImageBind and SAM, this is akin to distilling features (i.e. some spiritual variant of Attention Transfer [a]) of the generalist models, thus utilizing the pre-trained nature of the generalist model trained on internet data. This is framed as \"psuedo-modality\" instead of calling the spade what a spade is -- i.e. leveraging web-scale model's generalist pre-trained feature space.\n\n(b) The central claim that \"access to\" generalist data is unnecessary, while true, is riddled with caveats. The specific writing felt misleading as it seemed to imply that web-scale data/models are not needed and the caveat should be clarified at the outset in the abstract, teaser figure and introduction. As the paper itself shows, It is definitely necessary to have access to the generalist models (and indirectly their data and learned manifolds) to obtain the pseudo modalities for training. \n\n(c) The phrasing of the abstract and introduction seem to paint a picture that web scale datasets (and models) are not needed and generalist pre-training is not needed -- this is not true as it's indirectly used by this method. Please clarify what am I missing here, maybe the key claim should be rephrased to say \"one does not need to 'directly' access and train on web-scale data while deploying models to a specialized test space, feature alignment from such generalist models is enough\".\n\n2. TST-Sensory: How does it compare with TST-MM (i.e. with the web scale targets as modality sources)?\n\n3. When the generalist models (DINOv2, CLIP etc) are trained on the small external dataset, is the generalist backbone frozen? Many vision works have shown that -- surprisingly -- finetuning the DINOv2 backbone is counter-productive.\n\n4. No discussion of privacy concerns when using test space data. Is it possible to do this form of (pre/post-)training on-premise/on-device efficiently. For example, How do we ensure that data from deployment is minimally accessed by a robotics developer due to heightened privacy concerns of accessing such test space data. In the considered scenario, house hold robots would collect data of end-users engaging in day to day activities, I'm not sure how all the stakeholders would react to their private data being accessed over the internet, and what ethical, technical and sociological frameworks are necessary to address issues arising from the concerns of such stakeholders. While I don't expect this specific paper to solve this problem completely, a discussion or feasibility study keeping these concerns in mind would significantly strengthen this paper given it's applicability to real world deployment.\n\n[a] On the Surprising Effectiveness of Attention Transfer for Vision Transformers, NeurIPS 2024"}, "questions": {"value": "Please clarify the points made in weaknesses. I like this paper overall and would be open in increasing my rating further, however, the writing in a few places feels misleading to me and paints a picture that would border on over-claiming in my view (maybe the terminology needs to be refined in this case) -- please list down steps that would make the claims in the abstract and introduction much more tighter and easier to digest."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2ThdXqlMWe", "forum": "4dMlAKBwrA", "replyto": "4dMlAKBwrA", "signatures": ["ICLR.cc/2026/Conference/Submission9735/Reviewer_ygUd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9735/Reviewer_ygUd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882029510, "cdate": 1761882029510, "tmdate": 1762921231375, "mdate": 1762921231375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Test-Space Training (TST), a self-supervised framework that learns visual representations directly from multimodal data within the deployment environment, without using any external datasets. By leveraging cross-modal masked modeling among locally available sensors (e.g., RGB, depth), TST treats multimodality as supervision to pre-train models specialized for the target test space. Experiments across different datasets and various downstream tasks show that TST consistently matches or surpasses large-scale Internet-pretrained models (e.g., CLIP, DINOv2, 4M-21)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper explores an interesting and meaningful problem. It is clearly written and well-structured, making the methodology and insights easy to follow."}, "weaknesses": {"value": "1. The paper raises an interesting problem but lacks methodological innovation — the proposed TST framework mainly integrates existing components, with less than one page describing the method in detail.\n2. Inference relies on multimodal inputs, yet potential modality bias or inconsistency is not addressed; the paper should clarify how multimodal information is effectively fused for downstream tasks.\n3. In Figure 3, TST-MM still performs noticeably worse than large-scale Internet-based pre-training, and the paper should analyze the reasons behind this gap.\n4. There are several formatting issues: for instance, the description of Table 1 should appear on the same page, and figure order needs adjustment (e.g., Figures 6–8 appear out of sequence).\n5. The meaning of different colors in Figure 8 is not explained and should be clarified."}, "questions": {"value": "1. The proposed TST framework mainly combines existing components with limited methodological detail. Could the authors clarify what the core technical novelty of TST.\n2. During inference, different modalities may introduce cross-modal bias or inconsistency. How does the proposed method effectively handle or align such discrepancies.\n3. In Figure 3, TST-MM still performs worse than large-scale Internet-pretrained models. Could the authors provide an analysis or discussion explaining the potential causes of this performance gap.\n4. In Figure 8, the meaning of different colors is not clearly explained. Could the authors specify what each color represents."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AaF5axs4DR", "forum": "4dMlAKBwrA", "replyto": "4dMlAKBwrA", "signatures": ["ICLR.cc/2026/Conference/Submission9735/Reviewer_NjHj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9735/Reviewer_NjHj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922270557, "cdate": 1761922270557, "tmdate": 1762921231093, "mdate": 1762921231093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}