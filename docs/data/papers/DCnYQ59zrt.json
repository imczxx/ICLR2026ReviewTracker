{"id": "DCnYQ59zrt", "number": 19233, "cdate": 1758294632946, "mdate": 1759897050952, "content": {"title": "MIND: Market Interpretation DSL for Unified Market Design and Simulation", "abstract": "Market mechanisms such as auctions and matchings coordinate supply and demand at scale, yet their implementations remain locked in rigid procedural code that hinders iteration and auditing. We introduce the Market Interpretation DSL (MIND), a typed language and toolchain for declarative market specification to achieve unified market design and simulation. MIND comprises (i) a core grammar with a phased Intermediate Representation (IR) and economic safety checks, (ii) a natural language assistant that translates descriptions into DSL with automated diagnostics and safe rewrites, and (iii) rule-based simulation and convex optimization backends. Using synthetic specifications generated across 87 domains with held-out validation, our fine-tuned Llama-3-8B assistant achieves 96.33% semantic correctness, measured as IR equivalence to gold programs, surpassing few-shot GPT-4o at 91.41%. Across second-price auctions, multi-stage auctions, and matching markets, MIND reduces specification complexity by approximately 79% in lines of code compared to Python implementations. In a preregistered within-subjects study with 17 participants, mechanism modifications were completed 4 to 10 times faster using MIND. Code, dataset, and models will be released upon acceptance.", "tldr": "", "keywords": ["Market Design", "Domain-Specific Language", "DSL", "AI Copilot", "Auctions", "Matching Markets", "Mechanism Design", "Code Generation", "Symbolic AI", "Computational Economics"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9713561a4c0cc276bdfa0350434b1803644a7812.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The primary contribution of the paper is creating a domain-specific language (DSL) with which one can express simple auction and matching problems. On top of this DSL, they build a pipeline for converting natural language descriptions of markets into DSL code (which in turn can be converted into IR code, and then fed into a solver). Section 4.1 describes (on a cursory level) results of a workflow study that highlights the advantages of their DSL over general-purpose languages (Python/AnyLogic), however the study details are not provided. Sections 4.2 and 5 describe the design and evaluation of their \"AI Copilot\" system to convert natural language (NL) descriptions of markets into DSL code. They find that Llama 3 fine-tuned on a curated subset of NL->DSL examples outperforms baselines, including few-shot GPT-4o."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1. The appendix clearly lays out most of (but not all, see W3) details of the DSL. The worked example in Appendix G was particularly helpful. \n\nS2. DSLs for mechanism design seems like a fruitful approach, and while this paper is not the first to take this approach (as the authors describe in Section 2), they are an early work in this seemingly promising area."}, "weaknesses": {"value": "W1 (major). The DSL workflow evaluation described in Section 4.1 is missing details: information about experimental design is completely omitted, and results are only described on a cursory level. The only place details can be found is in the abstract (\"In a pre-registered within-subjects study with 17 participants, mechanism modifications were completed 4 to 10 times faster using MIND.\", Ln 23-25), they are not present anywhere in the main body or appendix. \n\nW2. For Section 5, is the test set sampled from the filtered dataset, or the unfiltered dataset? If it is sampled from the filtered dataset, then this result is not really surprising, because the dataset under \"Full Pipeline\" would be drawn from the same distribution as the test set, and the other datasets would not be. Relatedly, how much is filtered out in each step of the dataset generation (section 3.4) is not described. For example, if the final step (description-DSL) filters out a nontrivial proportion of examples, it could be that the final dataset is subject to selection bias, in which only easy (NL description, DSL) pairs are kept. \n\nW3. Some aspects of the DSL, like which auction types, matching types, allocation rules, and payment rules are allowed, are not clearly described. For example, are first_price and second_price the only two allowed auction types, or are there others?"}, "questions": {"value": "Q1. I understand that the purpose of a DSL is to sacrifice expressiveness for the sake of provable correctness. Still, it seems like the range of mechanisms this DSL can express is quite limited. For example, is it tractable to express a combinatorial auction or assignment problem in this framework (without additive valuations)? Can the authors describe more clearly what this DSL can and cannot express? \n\nQ2. The authors mention in a few places \"governance artifacts\" as being an advantage of their approach, however more detail is not provided. Can the authors explain what they mean by this? If they just mean by this that the DSL code is easy to audit, then would this not also be an advantage shared by prior work? (Cf. the authors write \"These DSLs improve mechanism specification but are scoped to individual subdomains and do not provide [...] governance artifacts\", Ln 115-117)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IdRZBFiFIf", "forum": "DCnYQ59zrt", "replyto": "DCnYQ59zrt", "signatures": ["ICLR.cc/2026/Conference/Submission19233/Reviewer_6kfJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19233/Reviewer_6kfJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856951224, "cdate": 1761856951224, "tmdate": 1762931215670, "mdate": 1762931215670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Imagine that one wanted to use natural language to quickly and correctly build a market mechanism, such as an auction or matching market.\n\nThis manuscript provides that: a fine-tuned LLM converts natural language descriptions to a domain-specific language, MIND; checks performed on MIND then ensure correctness, and adherence to the original specification.  Ablation studies allow assessment of the importance of each step in the pipeline.\n\nThe DSL allows far more efficient representations (than e.g. Python), is more readable, and easier to alter (e.g. from a first-price to a second-price auction)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I like this paper.  I think it's sensible, well executed and described."}, "weaknesses": {"value": "I would have liked to have seen a motivating example earlier in the paper.  Figure 1 is presented on p.2, but not mentioned until p.3.  This left me trying to understand, concretely, what was being proposed.\n\nAt the bottom of p.8, 'end-to-end correctness' and 'IR equivalence' seem to be used somewhat interchangeably.  This should be clearer.\n\nRelatedly, at the top of p.8, I didn't understand the use of cosine similarity for the training data.  Can that sentence be unpacked at bit?"}, "questions": {"value": "1. Changing a first-price to a second-price auction _should_ be fairly easy - but may also be too easy a challenge: auctions deployed in e.g. e-commerce environments (e.g. Google AdWords) will be highly customised.  How easily can new, custom mechanisms be specified?\n\n1. Relatedly, how easily can new validators be added?\n\n1. Concretely, how easy would it be to specify e.g. the Leyton-Brown, Milgrom and Segal FCC incentive auction?  All the FCC commissioners agreed that this was a singularly complex auction: verifying something on its order seems very useful.\n\n1. Can the authors say more about the routing to the backends, Pandas and CVXPY?  This _feels_ somewhat non-robust, and possibly hard to extend.\n\n1. when and how do the authors see MIND in use?  I ask as I suspect that there are a relatively small number of market mechanisms in use, and that these may be embedded in existing toolchains.  Thus, I wonder how easy it would be to introduce new, highly-specialised tooling like MIND.\n\n1. how important is it to have a natural language frontend rather than, say, a series of pull-down options?  Mechanism design is far more constrained than general language.\n\n1. Is it worth saying anything about existing work such as Passmore & Ignatovich's Imandra tooling for encoding market logic, Caminati et al's 2015 work on verifying auction properties, combinations of LLMs and theorem provers (e.g. DeepSeek's Prover, which uses Lean), or AWS' Automated Reasoning Group's 'Bedrock Guardrails'?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tK5YvqZuPD", "forum": "DCnYQ59zrt", "replyto": "DCnYQ59zrt", "signatures": ["ICLR.cc/2026/Conference/Submission19233/Reviewer_N77G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19233/Reviewer_N77G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905092381, "cdate": 1761905092381, "tmdate": 1762931215009, "mdate": 1762931215009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MIND, a typed domain-specific language and toolchain for market design, paired with a natural-language copilot trained on synthetic data. It claims that MIND makes specifications simpler than writing them in Python, achieves high correctness on synthetic benchmarks across many domains, and leads to faster modifications in a preregistered user study."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The architectural separation and the different components is clean and auditable. The combination of LLM tools and a new language specialized in the market design domain is an interesting direction."}, "weaknesses": {"value": "In general, the current contribution reads as a well-engineered wrapper that does well on simple scenarios but does not yet demonstrate clear advantages or robustness guarantees on the messy, complex mechanisms practitioners or researchers in the community care about.\n\n- The primary workflow study uses three very tractable and arguably simple settings (second price, a two-stage reserve auction, and a basic compatibility matching). The main results show comparisons based on Python lines of code (e.g., ~40–60 for second price; ~100–120 for the two-stage variant) are used to argue a 79% reduction in 'complexity', but these tasks are easy to implement directly and lines of code is a weak, setup-dependent proxy for difficulty. Second price in particular is not a hard environment: given individual valuations, you allocate to the highest valuation and charge the second highest price. It is not clear why this is considered complex in Python to begin with. Performance results beyond the lines-of-code angle also feel thin - in Table 2, the highlighted results are 100% parse success, 96% 'IR equivalence'.  This is compared to 98%, 91% for gpt-4o. That is a small improvement for a fine-tuned system, and with widely adopted reasoning models like GPT-5 Thinking and Gemini 2.5 Pro that perform far beyond GPT-4o on coding tasks, the practical advantage of this method is unclear.\n\n- There is a real risk of overfitting to synthetic data and limited external validity. Nearly all results come from synthetic prompts and descriptions filtered by LLMs; this may overfit to templated distributions and prompt artifacts. Evidence on human-authored specifications, messy real-world tie-breaking, and irregular data feeds is minimal. The human audit is small, and details of the seventeen-participant user study are only in the abstract, with no information about venues, tasks, sampling, or statistics. In realistic use, people (practitioners or researchers) will want highly configurable and complex auctions: custom tie-breaking, clock speed and observability in clock auctions, missing or messy data, arbitrary allocation and payment rules with constraints, and other edge conditions. These are off-distribution for the training procedure, and it is not clear how the system would generalize, if at all.\n\n- The most interesting promise for a new language would be the ability to represent genuinely complicated auction formats and to perform meaningful checks that advise, correct, or interpret those specifications. The paper does not convincingly deliver on either. On representation, coverage appears narrow relative to the complexity seen in practice (see above point for detail). On checking, the focus is on basic items like unique names, nonnegative reserves, and simple reference integrity, plus mapping known aliases for auction types and rules. Those are easy to implement in ordinary languages and the alias mapping underlines limited versatility. In real workflows, if the data format from a live auction differs from what this system expects, it often seems faster to write a small script directly than to adapt everything into this syntax and toolchain. There are no strong robustness guarantees to justify that translation cost. Given that strong general-purpose models like GPT-5 Thinking can already produce end-to-end Python or optimization code from natural language with high quality, the concrete gain from moving to this synthetic data focused constrained syntax is not established."}, "questions": {"value": "- Is there a head-to-head comparison against state of the art reasoning models that generate Python or optimization code directly from natural language (for example, GPT-5 Thinking or Gemini 2.5 Pro)? In addition, can the authors clarify steps they take to prevent overfitting and testing on training data? For example, is the test set structurally different from the training set, and do the fine-tuned models show generalization to settings they were not trained on?\n\n- What exactly does “intermediate representation equivalence” mean in your evaluation? When you say you “compare the generated intermediate representation to ground truth using graph isomorphism on the abstract syntax tree,” what differences are allowed or ignored? Do you treat variable renaming, order of commutative operations, defaulted parameters, or refactorings as equivalent? How do you handle cases where two specifications compile to the same behavior but the trees differ?\n\n- Beyond second price and the multi-stage reserve auction, what concrete auction formats does the language support today without workarounds? Can a user specify arbitrary allocation and payment rules with constraints, or are they limited to a named menu of rules?\n\n- Real data rarely matches a clean schema. How does the system ingest and normalize arbitrary auction logs and bidder data with missing fields, extra columns, or format drift? Or is this left to ad-hoc preprocessing outside the framework?\n\n- The user study is mentioned briefly and lacks details. Could you provide high level details on venue, recruitment details, tasks, and statistical analysis? Also, do you have results on human-authored specifications sourced from practitioners, not synthetic prompts?\n\n- It might help to highlight where this is genuinely the better tool. Can the authors point to specific realistic settings or published academic research papers in the field where the framework would clearly and significantly save time, make analysis easier, or reduce errors compared to writing a script or using a general model? Does the framework enable researchers to do more than they can today?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qOguaXjRjH", "forum": "DCnYQ59zrt", "replyto": "DCnYQ59zrt", "signatures": ["ICLR.cc/2026/Conference/Submission19233/Reviewer_yi5a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19233/Reviewer_yi5a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937329080, "cdate": 1761937329080, "tmdate": 1762931214588, "mdate": 1762931214588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Market Interpretation DSL that allows LLM-based simulation to adhere to market-related constraints."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This is a good paper. The authors seem to have given enough thought to building scaffolding for the particular task of market simulation. For that, the choice of building a DSL makes sense. Cool (reaffirmation, really) to see that finetuning helps compared to few-shot ICL."}, "weaknesses": {"value": "The first confusion I have is about the IR:\nThe paper says, \"To solve this problem, we introduce an Intermediate Representation (IR) (Lattner et al., 2021) as a critical abstraction layer.\" So truly, the IR is introduced as an additional scaffold to help in parsing and compilation. However, in the results, the proposed method (MIND) only outperforms a standard LLM in IR (for completion and parsing, it remains negligibly different). This raises a question about the technical novelty and effectiveness of the work. It is true that additional scaffolding helps, but how this extends the knowledge of the field with regard to AI agents is unclear.\n\n\"AI Copilot\" -- I am not sure what that means. Is it the DSL? the framework? the LLM itself? and agent? For all academic purposes, it might be useful to precisely define a term like such in a paper. Not a rejection point, but strongly recommended for review. \n\n\"In practice, one execution engine cannot serve all market designs well.\" -- why? Does this apply to a different design, or does one need to build an application-specific execution engine? And the DSL is agnostic to such choices?\n\nTable 1: Does the Python code generated by an LLM? If so, why do we care about the flexibility if the pipeline is automated? I guess without this, the results from Table 2 (which seem to be the main results) don't add a lot of value."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KwezZNeMki", "forum": "DCnYQ59zrt", "replyto": "DCnYQ59zrt", "signatures": ["ICLR.cc/2026/Conference/Submission19233/Reviewer_ia69"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19233/Reviewer_ia69"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762747398233, "cdate": 1762747398233, "tmdate": 1762931214292, "mdate": 1762931214292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}