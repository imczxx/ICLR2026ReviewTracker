{"id": "DCnYQ59zrt", "number": 19233, "cdate": 1758294632946, "mdate": 1759897050952, "content": {"title": "MIND: Market Interpretation DSL for Unified Market Design and Simulation", "abstract": "Market mechanisms such as auctions and matchings coordinate supply and demand at scale, yet their implementations remain locked in rigid procedural code that hinders iteration and auditing. We introduce the Market Interpretation DSL (MIND), a typed language and toolchain for declarative market specification to achieve unified market design and simulation. MIND comprises (i) a core grammar with a phased Intermediate Representation (IR) and economic safety checks, (ii) a natural language assistant that translates descriptions into DSL with automated diagnostics and safe rewrites, and (iii) rule-based simulation and convex optimization backends. Using synthetic specifications generated across 87 domains with held-out validation, our fine-tuned Llama-3-8B assistant achieves 96.33% semantic correctness, measured as IR equivalence to gold programs, surpassing few-shot GPT-4o at 91.41%. Across second-price auctions, multi-stage auctions, and matching markets, MIND reduces specification complexity by approximately 79% in lines of code compared to Python implementations. In a preregistered within-subjects study with 17 participants, mechanism modifications were completed 4 to 10 times faster using MIND. Code, dataset, and models will be released upon acceptance.", "tldr": "", "keywords": ["Market Design", "Domain-Specific Language", "DSL", "AI Copilot", "Auctions", "Matching Markets", "Mechanism Design", "Code Generation", "Symbolic AI", "Computational Economics"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9713561a4c0cc276bdfa0350434b1803644a7812.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The primary contribution of the paper is creating a domain-specific language (DSL) with which one can express simple auction and matching problems. On top of this DSL, they build a pipeline for converting natural language descriptions of markets into DSL code (which in turn can be converted into IR code, and then fed into a solver). Section 4.1 describes (on a cursory level) results of a workflow study that highlights the advantages of their DSL over general-purpose languages (Python/AnyLogic), however the study details are not provided. Sections 4.2 and 5 describe the design and evaluation of their \"AI Copilot\" system to convert natural language (NL) descriptions of markets into DSL code. They find that Llama 3 fine-tuned on a curated subset of NL->DSL examples outperforms baselines, including few-shot GPT-4o."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1. The appendix clearly lays out most of (but not all, see W3) details of the DSL. The worked example in Appendix G was particularly helpful. \n\nS2. DSLs for mechanism design seems like a fruitful approach, and while this paper is not the first to take this approach (as the authors describe in Section 2), they are an early work in this seemingly promising area."}, "weaknesses": {"value": "W1 (major). The DSL workflow evaluation described in Section 4.1 is missing details: information about experimental design is completely omitted, and results are only described on a cursory level. The only place details can be found is in the abstract (\"In a pre-registered within-subjects study with 17 participants, mechanism modifications were completed 4 to 10 times faster using MIND.\", Ln 23-25), they are not present anywhere in the main body or appendix. \n\nW2. For Section 5, is the test set sampled from the filtered dataset, or the unfiltered dataset? If it is sampled from the filtered dataset, then this result is not really surprising, because the dataset under \"Full Pipeline\" would be drawn from the same distribution as the test set, and the other datasets would not be. Relatedly, how much is filtered out in each step of the dataset generation (section 3.4) is not described. For example, if the final step (description-DSL) filters out a nontrivial proportion of examples, it could be that the final dataset is subject to selection bias, in which only easy (NL description, DSL) pairs are kept. \n\nW3. Some aspects of the DSL, like which auction types, matching types, allocation rules, and payment rules are allowed, are not clearly described. For example, are first_price and second_price the only two allowed auction types, or are there others?"}, "questions": {"value": "Q1. I understand that the purpose of a DSL is to sacrifice expressiveness for the sake of provable correctness. Still, it seems like the range of mechanisms this DSL can express is quite limited. For example, is it tractable to express a combinatorial auction or assignment problem in this framework (without additive valuations)? Can the authors describe more clearly what this DSL can and cannot express? \n\nQ2. The authors mention in a few places \"governance artifacts\" as being an advantage of their approach, however more detail is not provided. Can the authors explain what they mean by this? If they just mean by this that the DSL code is easy to audit, then would this not also be an advantage shared by prior work? (Cf. the authors write \"These DSLs improve mechanism specification but are scoped to individual subdomains and do not provide [...] governance artifacts\", Ln 115-117)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IdRZBFiFIf", "forum": "DCnYQ59zrt", "replyto": "DCnYQ59zrt", "signatures": ["ICLR.cc/2026/Conference/Submission19233/Reviewer_6kfJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19233/Reviewer_6kfJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856951224, "cdate": 1761856951224, "tmdate": 1762931215670, "mdate": 1762931215670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Author Overall Response (1/3)"}, "comment": {"value": "## Author Response\nWe thank all reviewers for their detailed comments. We are encouraged that several reviewers view DSLs for mechanism design as a “fruitful approach” and find our system “sensible, well executed and described.” We address the main concerns below, then highlight reviewer-specific points.\n\n---\n\n### 1. What is actually new?\n\nOur contribution is not just a prompt wrapper around Python, but a full stack for *verifiable* market specifications:\n\n- **A typed market IR:** a backend-independent AST over auctions, stages, matchings, constraints, and objectives (e.g., `AuctionNode`, `ConstraintNode`). The parser only produces IR; backends only consume IR.  \n- **A three-stage validator pipeline on IR** (`CoreMarket`, `StageAndMatching`, `AdvancedOptimization`) that enforces syntactic, semantic, and basic economic consistency, with safe autofixes and structured reports.  \n- **A dual-backend execution framework** (Pandas/NetworkX simulation and CVXPY optimization) that must agree on allocations/payments for any IR, enabling cross-backend consistency checks.  \n- **A curated NL→DSL dataset (~11k examples)** built via a 4-stage program pipeline + description-consistency checking + human audit, and a fine-tuned 8B copilot that reaches **96.33% IR equivalence vs. 91.41%** for few-shot GPT-4o.\n\nThis architecture follows the general *“intent → copilot → DSL → compiler → backend”* pattern advocated for DSL copilots: it turns natural-language market descriptions into transparent, testable, deterministic, traceable, policy-preserving specifications rather than opaque one-off scripts.\n\n---\n\n### 2. IR, metrics, and “AI Copilot”\n\n#### IR as the semantic and governance layer\n\nThe IR is the single source of truth for mechanism semantics, not just an intermediate parsing artifact. Validators, backend routing, and our evaluation metric (“IR equivalence”) are all defined over IR, not raw DSL text. IR nodes, validator report IDs, and compile artifacts are logged with each execution, so any result can be replayed with the exact spec and validator configuration.\n\nWe will clarify:\n\n- **IR equivalence:** graph isomorphism between generated and reference IR ASTs, modulo variable renaming, commutative ordering, and alias normalization.  \n- **End-to-end correctness:** passes grammar, all three validators, compilation to both backends, IR equivalence, and execution checks on 300 scenarios.\n\nTable 3 shows that as we move from parse-only filtering to the full IR-aware pipeline, IR equivalence jumps from **66.33%** to **96.33%** with the same 8B base model – a 30-point gain driven by IR-level validation and description–DSL alignment, not by a larger LLM.\n\n#### “AI Copilot” definition\n\nWe will define “AI Copilot” once and use it consistently to mean the **NL→DSL assistant module** (Llama-3-8B + LoRA with its prompts), plus the optional Completeness Agent that fills missing fields before translation. It is **not** the DSL itself and not the whole framework.\n\n---\n\n### 3. Data pipeline, overfitting, and generalization\n\nWe will make the dataset pipeline and split explicit:\n\n- We start from **900+ use cases over 87 domains** and generate candidate (description, DSL) pairs with GPT-4o-mini, given the grammar and few-shot exemplars.  \n- Programs are filtered through a **4-stage pipeline**: grammar parse → three IR validators → compilation to both backends → execution without runtime errors.  \n- We then rewrite descriptions to be complete given the DSL and run a second LLM as a description–DSL verifier, keeping only pairs labeled “YES”.  \n- From the resulting pool, we form an ~**11k-example training set** and a **300-example test set**, ensuring cosine similarity < 0.85 between any train/test description (TF–IDF) and distinct IR hashes (no structural duplicates).  \n\nWe agree synthetic data limits external validity; in the revision we will (i) add a small table showing how many candidates are dropped at each pipeline step, and (ii) explicitly list richer human-authored specs and messy production logs as future evaluation targets."}}, "id": "3ftXOuZet1", "forum": "DCnYQ59zrt", "replyto": "DCnYQ59zrt", "signatures": ["ICLR.cc/2026/Conference/Submission19233/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19233/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19233/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763744908254, "cdate": 1763744908254, "tmdate": 1763745303873, "mdate": 1763745303873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Imagine that one wanted to use natural language to quickly and correctly build a market mechanism, such as an auction or matching market.\n\nThis manuscript provides that: a fine-tuned LLM converts natural language descriptions to a domain-specific language, MIND; checks performed on MIND then ensure correctness, and adherence to the original specification.  Ablation studies allow assessment of the importance of each step in the pipeline.\n\nThe DSL allows far more efficient representations (than e.g. Python), is more readable, and easier to alter (e.g. from a first-price to a second-price auction)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I like this paper.  I think it's sensible, well executed and described."}, "weaknesses": {"value": "I would have liked to have seen a motivating example earlier in the paper.  Figure 1 is presented on p.2, but not mentioned until p.3.  This left me trying to understand, concretely, what was being proposed.\n\nAt the bottom of p.8, 'end-to-end correctness' and 'IR equivalence' seem to be used somewhat interchangeably.  This should be clearer.\n\nRelatedly, at the top of p.8, I didn't understand the use of cosine similarity for the training data.  Can that sentence be unpacked at bit?"}, "questions": {"value": "1. Changing a first-price to a second-price auction _should_ be fairly easy - but may also be too easy a challenge: auctions deployed in e.g. e-commerce environments (e.g. Google AdWords) will be highly customised.  How easily can new, custom mechanisms be specified?\n\n1. Relatedly, how easily can new validators be added?\n\n1. Concretely, how easy would it be to specify e.g. the Leyton-Brown, Milgrom and Segal FCC incentive auction?  All the FCC commissioners agreed that this was a singularly complex auction: verifying something on its order seems very useful.\n\n1. Can the authors say more about the routing to the backends, Pandas and CVXPY?  This _feels_ somewhat non-robust, and possibly hard to extend.\n\n1. when and how do the authors see MIND in use?  I ask as I suspect that there are a relatively small number of market mechanisms in use, and that these may be embedded in existing toolchains.  Thus, I wonder how easy it would be to introduce new, highly-specialised tooling like MIND.\n\n1. how important is it to have a natural language frontend rather than, say, a series of pull-down options?  Mechanism design is far more constrained than general language.\n\n1. Is it worth saying anything about existing work such as Passmore & Ignatovich's Imandra tooling for encoding market logic, Caminati et al's 2015 work on verifying auction properties, combinations of LLMs and theorem provers (e.g. DeepSeek's Prover, which uses Lean), or AWS' Automated Reasoning Group's 'Bedrock Guardrails'?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tK5YvqZuPD", "forum": "DCnYQ59zrt", "replyto": "DCnYQ59zrt", "signatures": ["ICLR.cc/2026/Conference/Submission19233/Reviewer_N77G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19233/Reviewer_N77G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905092381, "cdate": 1761905092381, "tmdate": 1762931215009, "mdate": 1762931215009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Author Overall Response (2/3)"}, "comment": {"value": "### 4. Scope, messy data, and “simple tasks”\n\n#### What MIND supports now\n\nFrom the current grammar and IR (Appendix A–D), MIND can encode:\n\n- Single- and multi-stage sealed-bid auctions with standard allocation/payment rules and reserve policies.  \n- Compatibility-constrained matching markets (e.g., bipartite/stable matching) with explicit compatibility graphs.  \n- Mechanisms with convex objectives and linear constraints that can be dispatched to the CVXPY backend (e.g., capacity/budget constraints with revenue objectives).\n\nWe do **not** claim to support fully general combinatorial auctions, arbitrary non-additive valuations, or FCC-style spectrum auctions in this version; these are explicitly listed as future work in the paper. We will add a short table summarizing “supported now” vs. “out of scope”.\n\n#### Benchmarks and complexity\n\nWe agree that second price, a two-stage reserve auction, and a simple compatibility matching are mathematically simple. We chose them deliberately as canonical templates for three families: single-stage auctions, multi-stage reserve/re-auction processes, and compatibility-constrained matchings. They already exercise stages, cross-block coordination, and graph-based matching, while remaining easy to interpret for readers.\n\nTable 1 shows that even for these “simple” mechanisms, MIND reduces implementation from **40–120 Python LoC to 10–25 DSL lines** and makes policy modifications (e.g., changing pricing rules or stages) trivial. We will make this framing explicit and add brief examples of more complex mechanisms we have encoded (e.g., multi-unit and constrained auctions from recent EC/SIGecom work) without claiming full coverage of all designs.\n\n#### Messy data and schema drift\n\nAt present, MIND assumes that auction and matching data have been normalized into tabular form (participants, goods, bids, compatibility edges) before ingestion. The DSL and IR operate *above* this layer; ETL and schema-drift handling are done with small preprocessing scripts or existing data tools. We will state this clearly and mark schema-aware extensions (e.g., detecting incompatible logs or automatically generating adapters) as orthogonal future work. This avoids silently changing market semantics when raw data formats evolve.\n\n#### NL input vs. dropdowns / GUIs\n\nWe view natural-language input as complementary to structured interfaces. Designers often start from prose policies and informal specs; our Completeness Agent and Copilot turn those into structured DSL programs. The same IR can then be driven by a form-based or dropdown UI for routine changes. We will add a sentence clarifying that MIND’s front end is pluggable: NL, forms, or both, all targeting the same DSL/IR.\n\n---\n\n### 5. User study and practical impact\n\nThe current draft mentions the 17-participant user study only in the abstract; we will move a concise description into Section 4.1 and an appendix. In brief:\n\n- **Design:** preregistered within-subjects study with 17 participants, comparing mechanism modifications in MIND vs. Python/AnyLogic on the three benchmark mechanisms.  \n- **Tasks:** change a pricing rule, add a reserve stage, and alter compatibility constraints, starting from a working implementation in each environment.  \n- **Metrics:** completion time and correctness.\n\nThis directly addresses whether MIND helps with iterating on mechanisms (the common real-world task) rather than only initial implementation. We will also tie each benchmark more explicitly to real applications (e.g., reserve policies in ad auctions or government sales; compatibility matchings on platforms) and emphasize that the typed specification + validator reports + cross-backend logs form governance artifacts useful for audit and policy iteration.\n\n---\n\n### 6. Relation to stronger general-purpose models\n\nOur experiments use GPT-4o as a strong proprietary baseline, showing that a smaller 8B model with our IR-aware data curation achieves higher semantic correctness (**96.33% vs. 91.41% IR equivalence**) and 100% validation/compilation success. We fully agree that newer reasoning models (e.g., GPT-5-class or Gemini-class models) are powerful; our design is front-end agnostic: any NL→DSL model can plug into the same IR, validators, backends, and governance layer.\n\nWe will clarify that our contribution is a domain-specific interface and verification pipeline that **complements**, rather than replaces, progress in general LLMs."}}, "id": "rzsSiaMf1T", "forum": "DCnYQ59zrt", "replyto": "DCnYQ59zrt", "signatures": ["ICLR.cc/2026/Conference/Submission19233/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19233/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19233/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763744972098, "cdate": 1763744972098, "tmdate": 1763760421010, "mdate": 1763760421010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MIND, a typed domain-specific language and toolchain for market design, paired with a natural-language copilot trained on synthetic data. It claims that MIND makes specifications simpler than writing them in Python, achieves high correctness on synthetic benchmarks across many domains, and leads to faster modifications in a preregistered user study."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The architectural separation and the different components is clean and auditable. The combination of LLM tools and a new language specialized in the market design domain is an interesting direction."}, "weaknesses": {"value": "In general, the current contribution reads as a well-engineered wrapper that does well on simple scenarios but does not yet demonstrate clear advantages or robustness guarantees on the messy, complex mechanisms practitioners or researchers in the community care about.\n\n- The primary workflow study uses three very tractable and arguably simple settings (second price, a two-stage reserve auction, and a basic compatibility matching). The main results show comparisons based on Python lines of code (e.g., ~40–60 for second price; ~100–120 for the two-stage variant) are used to argue a 79% reduction in 'complexity', but these tasks are easy to implement directly and lines of code is a weak, setup-dependent proxy for difficulty. Second price in particular is not a hard environment: given individual valuations, you allocate to the highest valuation and charge the second highest price. It is not clear why this is considered complex in Python to begin with. Performance results beyond the lines-of-code angle also feel thin - in Table 2, the highlighted results are 100% parse success, 96% 'IR equivalence'.  This is compared to 98%, 91% for gpt-4o. That is a small improvement for a fine-tuned system, and with widely adopted reasoning models like GPT-5 Thinking and Gemini 2.5 Pro that perform far beyond GPT-4o on coding tasks, the practical advantage of this method is unclear.\n\n- There is a real risk of overfitting to synthetic data and limited external validity. Nearly all results come from synthetic prompts and descriptions filtered by LLMs; this may overfit to templated distributions and prompt artifacts. Evidence on human-authored specifications, messy real-world tie-breaking, and irregular data feeds is minimal. The human audit is small, and details of the seventeen-participant user study are only in the abstract, with no information about venues, tasks, sampling, or statistics. In realistic use, people (practitioners or researchers) will want highly configurable and complex auctions: custom tie-breaking, clock speed and observability in clock auctions, missing or messy data, arbitrary allocation and payment rules with constraints, and other edge conditions. These are off-distribution for the training procedure, and it is not clear how the system would generalize, if at all.\n\n- The most interesting promise for a new language would be the ability to represent genuinely complicated auction formats and to perform meaningful checks that advise, correct, or interpret those specifications. The paper does not convincingly deliver on either. On representation, coverage appears narrow relative to the complexity seen in practice (see above point for detail). On checking, the focus is on basic items like unique names, nonnegative reserves, and simple reference integrity, plus mapping known aliases for auction types and rules. Those are easy to implement in ordinary languages and the alias mapping underlines limited versatility. In real workflows, if the data format from a live auction differs from what this system expects, it often seems faster to write a small script directly than to adapt everything into this syntax and toolchain. There are no strong robustness guarantees to justify that translation cost. Given that strong general-purpose models like GPT-5 Thinking can already produce end-to-end Python or optimization code from natural language with high quality, the concrete gain from moving to this synthetic data focused constrained syntax is not established."}, "questions": {"value": "- Is there a head-to-head comparison against state of the art reasoning models that generate Python or optimization code directly from natural language (for example, GPT-5 Thinking or Gemini 2.5 Pro)? In addition, can the authors clarify steps they take to prevent overfitting and testing on training data? For example, is the test set structurally different from the training set, and do the fine-tuned models show generalization to settings they were not trained on?\n\n- What exactly does “intermediate representation equivalence” mean in your evaluation? When you say you “compare the generated intermediate representation to ground truth using graph isomorphism on the abstract syntax tree,” what differences are allowed or ignored? Do you treat variable renaming, order of commutative operations, defaulted parameters, or refactorings as equivalent? How do you handle cases where two specifications compile to the same behavior but the trees differ?\n\n- Beyond second price and the multi-stage reserve auction, what concrete auction formats does the language support today without workarounds? Can a user specify arbitrary allocation and payment rules with constraints, or are they limited to a named menu of rules?\n\n- Real data rarely matches a clean schema. How does the system ingest and normalize arbitrary auction logs and bidder data with missing fields, extra columns, or format drift? Or is this left to ad-hoc preprocessing outside the framework?\n\n- The user study is mentioned briefly and lacks details. Could you provide high level details on venue, recruitment details, tasks, and statistical analysis? Also, do you have results on human-authored specifications sourced from practitioners, not synthetic prompts?\n\n- It might help to highlight where this is genuinely the better tool. Can the authors point to specific realistic settings or published academic research papers in the field where the framework would clearly and significantly save time, make analysis easier, or reduce errors compared to writing a script or using a general model? Does the framework enable researchers to do more than they can today?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qOguaXjRjH", "forum": "DCnYQ59zrt", "replyto": "DCnYQ59zrt", "signatures": ["ICLR.cc/2026/Conference/Submission19233/Reviewer_yi5a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19233/Reviewer_yi5a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937329080, "cdate": 1761937329080, "tmdate": 1762931214588, "mdate": 1762931214588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Author Overall Response (3/3)"}, "comment": {"value": "---\n\n### 7. Reviewer-specific clarifications\n\n- **R1 (ia69, rating 4):** We will (i) emphasize IR’s central role and refine the “one engine” line into a practical backend-routing argument; (ii) define “AI Copilot” precisely as the NL→DSL assistant; and (iii) state that Table 1’s Python/AnyLogic baselines are hand-written to reflect human workflow complexity.  \n\n- **R2 (yi5a, rating 2):** We will (i) frame our three tasks as canonical sanity checks, (ii) make the dataset pipeline, de-duplication, and audit explicit, (iii) add a clear scope table (what we support vs. out of scope), (iv) move user-study details into the main text, and (v) clarify that our main claim is about the architecture (DSL + IR + validators + dataset) rather than beating the latest closed-source models.  \n\n- **R3 (N77G, rating 8):** We will move a motivating end-to-end example earlier (currently in Appendix G), reference Figure 1 when it appears, separate “IR equivalence” from “end-to-end correctness,” and briefly explain our use of cosine similarity for data de-duplication. We will also expand discussion of extensibility (adding rules/validators/backends) and connect to formal-methods work on auction verification as complementary tools.  \n\n- **R4 (6kfJ, rating 4):** We will add the missing workflow-study details, a table with filtering counts at each dataset stage, a concise description of supported mechanism families, and a clearer definition of “governance artifacts” as the combination of readable DSL specs, typed IR + validator reports, and logged spec hashes / backend outputs that enable replay and audit.\n\nWe hope these clarifications address the reviewers’ concerns and better convey the novelty and practical value of MIND as a verifiable DSL copilot for market design."}}, "id": "YCjv1qYGY2", "forum": "DCnYQ59zrt", "replyto": "DCnYQ59zrt", "signatures": ["ICLR.cc/2026/Conference/Submission19233/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19233/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19233/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763745006337, "cdate": 1763745006337, "tmdate": 1763745281951, "mdate": 1763745281951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Market Interpretation DSL that allows LLM-based simulation to adhere to market-related constraints."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This is a good paper. The authors seem to have given enough thought to building scaffolding for the particular task of market simulation. For that, the choice of building a DSL makes sense. Cool (reaffirmation, really) to see that finetuning helps compared to few-shot ICL."}, "weaknesses": {"value": "The first confusion I have is about the IR:\nThe paper says, \"To solve this problem, we introduce an Intermediate Representation (IR) (Lattner et al., 2021) as a critical abstraction layer.\" So truly, the IR is introduced as an additional scaffold to help in parsing and compilation. However, in the results, the proposed method (MIND) only outperforms a standard LLM in IR (for completion and parsing, it remains negligibly different). This raises a question about the technical novelty and effectiveness of the work. It is true that additional scaffolding helps, but how this extends the knowledge of the field with regard to AI agents is unclear.\n\n\"AI Copilot\" -- I am not sure what that means. Is it the DSL? the framework? the LLM itself? and agent? For all academic purposes, it might be useful to precisely define a term like such in a paper. Not a rejection point, but strongly recommended for review. \n\n\"In practice, one execution engine cannot serve all market designs well.\" -- why? Does this apply to a different design, or does one need to build an application-specific execution engine? And the DSL is agnostic to such choices?\n\nTable 1: Does the Python code generated by an LLM? If so, why do we care about the flexibility if the pipeline is automated? I guess without this, the results from Table 2 (which seem to be the main results) don't add a lot of value."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KwezZNeMki", "forum": "DCnYQ59zrt", "replyto": "DCnYQ59zrt", "signatures": ["ICLR.cc/2026/Conference/Submission19233/Reviewer_ia69"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19233/Reviewer_ia69"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762747398233, "cdate": 1762747398233, "tmdate": 1762931214292, "mdate": 1762931214292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}