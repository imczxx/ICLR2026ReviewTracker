{"id": "SA8xDYrUYB", "number": 12228, "cdate": 1758206470024, "mdate": 1759897523980, "content": {"title": "Purrception: Variational Flow Matching for Vector-Quantized Image Generation", "abstract": "We introduce Purrception, a variational flow matching approach for vector-quantized image generation that provides explicit categorical supervision while maintaining continuous transport dynamics. Our method adapts Variational Flow Matching to vector-quantized latents by learning categorical posteriors over codebook indices while computing velocity fields in the continuous embedding space. This combines the geometric awareness of continuous methods with the discrete supervision of categorical approaches, enabling uncertainty quantification over plausible codes and temperature-controlled generation. We evaluate Purrception on ImageNet-1k $256 \\times 256$ generation. Training converges faster than both continuous flow matching and discrete flow matching baselines while achieving competitive FID scores with state-of-the-art models. This demonstrates that Variational Flow Matching can effectively bridge continuous transport and discrete supervision for improved training efficiency in image generation.", "tldr": "We apply variational flow matching to VQ latent image generation through a hybrid discrete-continuous approach for improved image generation.", "keywords": ["generative models", "flow matching", "vector quantized", "image generation", "computer vision", "variational flow matching"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa22578b307feae496d10734fdfda1bc6c96d9be.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigated flow-matching models for vector-quantized image generation. It pointed out the strengths and weaknesses of existing approaches: continuous flow models preserved geometric information but could not exploit categorical learning signals, while discrete models aligned with the quantized structure and allowed temperature-scaling control but failed to understand geometry. It then proposed Purrception, a variational flow-matching technique for vector-quantized image generation that combined the advantages of continuous and discrete flow models. Purrception learned categorical posteriors over codebook indices, which could be used for computing velocity fields in the continuous embedding space. Experiments showed that Purrception outperformed both continuous and discrete flow-matching baselines in both convergence speed and image generation quality on the ImageNet-1k 256 x 256 dataset."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper discussed the strengths and weaknesses of existing flow-matching approaches for vector-quantized image generation. It then proposed to employ a variational flow-matching technique to combine the advantages of mentioned approaches.\n- The paper is technically sound.\n- Experiments showed that Purrception outperformed both continuous and discrete flow-matching baselines in both convergence speed and image generation quality on the ImageNet-1k 256 x 256 dataset."}, "weaknesses": {"value": "- The middle side of Equation (13) seems to be incorrect. \n- Section 4.1: Missing information on the averge NFE and the used temperature (I guess it is 0.9 based on Table 1).\n- Section 4.2: From my understanding, this experiment only varies the temperature at inference while keeping the training temperature (possibly 0.9) unchanged.\n  + As expected, using the inference temperature similar to the training one (the straight-forward configuration) provides the best performance. Deviating the inference temperature from the training one causes performance degradation, particularly in color saturation. Hence, the experiment cannot prove the benefits of the temperature-scaling control\n  + A more reasonable experiment is to use the same temperature for training & inference, and compare models trained w/ different temperatures\n  + There is no evidence showing that the temperature affects image diversity\n- The results reported in Table 1 are far from state-of-the-art for Class-conditional generation on ImageNet-1k 256 x 256. The state-of-the-art ones, e.g., REPA, have FID less than 2. The reported result is only strong for vector-quantized image generation using standard DiT backbones. The authors should correct their claim.\n- Writting issues:\n  + Equation 12: $\\pi$ was used before being defined\n  + L265: The text should be in the same paragraph with the previous one\n  + Figure 5: Should add the temperature lable for each column"}, "questions": {"value": "- The middle side of Equation (13) seems to be incorrect. \n- Section 4.1: Missing information on the averge NFE and the used temperature (I guess it is 0.9 based on Table 1).\n- Section 4.2: From my understanding, this experiment only varies the temperature at inference while keeping the training temperature (possibly 0.9) unchanged.\n  + As expected, using the inference temperature similar to the training one (the straight-forward configuration) provides the best performance. Deviating the inference temperature from the training one causes performance degradation, particularly in color saturation. Hence, the experiment cannot prove the benefits of the temperature-scaling control\n  + A more reasonable experiment is to use the same temperature for training & inference, and compare models trained w/ different temperatures\n  + There is no evidence showing that the temperature affects image diversity"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9gvFQmoMx7", "forum": "SA8xDYrUYB", "replyto": "SA8xDYrUYB", "signatures": ["ICLR.cc/2026/Conference/Submission12228/Reviewer_8vtk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12228/Reviewer_8vtk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888287721, "cdate": 1761888287721, "tmdate": 1762923173668, "mdate": 1762923173668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes variational flow matching to train flow matching with discrete code from vq-vae. By jointly combining discrete and continuous, it shows that flow matching achieves better convergence than purely continuous or discrete."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper shows that under same discrete VQ-VAE, it outperforms the discrete flow matching training or the continuous flow matching training. The idea of uncertainty in sampling is quite interesting.\n\n2. The paper writing is easy to follow and clear."}, "weaknesses": {"value": "1. The motivation is not well-convincing to me. It is not clear why receiving categorical signal is better for continuous flow matching. What is the geometry structure in continuous here ?\n\n2. The theory about VFM seems to be out of place for me. To my understanding, this method is based on VFM framework but instead of inputing the codebook index and let the flow matching learn internal codebook embedding, they utilize the codebook embedding from vq-vae to create soft embedding $z_1$ and input continuous signal. This leads to limited novelty.\n\n3. The model underperforms with DiT using continuous VAE. Furthermore, this technique heavily depends on a good VQ-VAE which is limited the model performance. It would be better to choose other discrete VQ-VAE, which has better rFID and some generative model training on that achieves very good FID like [1,2] to see how good the proposed technique can reached given different VQ-VAE. \n\n[1]: An Image is Worth 32 Tokens for Reconstruction and Generation\n[2]: Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation"}, "questions": {"value": "Please see the weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4Ho3rSL9By", "forum": "SA8xDYrUYB", "replyto": "SA8xDYrUYB", "signatures": ["ICLR.cc/2026/Conference/Submission12228/Reviewer_woJ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12228/Reviewer_woJ8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979663676, "cdate": 1761979663676, "tmdate": 1762923172777, "mdate": 1762923172777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Purrception to address a dual nature of VQ-latents, which the authors argue that existing flow-based models make a poor trade-off: Continuous Flow Matching (CFM) respects the continuous geometry but ignores the discrete categorical structure, while Discrete Flow Matching (DFM) models the indices but discards the geometry.\n\nPurrception proposes a hybrid solution by adapting Variational Flow Matching (VFM). The model learns a categorical variational posterior over the discrete codebook indices, while the actual transport velocity is computed in the continuous embedding space.\n\nExperimental results on ImageNet-1k $256\\times256$ showing that Purrception converges significantly faster (1.7x-3.5x) and achieves a better final FID score than both CFM and DFM baselines when using the same DiT backbone88."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation \"discrete-continuous tradeoff\" is a real problem for VQ-latents, and the paper's solution directly addresses it without compromise.\n1. The proposed solution is simple and elegant.\n1. Exprimental results show faster convergence than other DFM and CFM baselines."}, "weaknesses": {"value": "1. Purrception failed to beat LlamaGen-XL, which has similar number of parameters, with respect to FID (Purrception 4.72 vs LlamaGen-XL 3.39), as well as other baselines.\n1. Purrception's encoder, decoder, and codebook are kept frozen during training, thus the entire method is fundamentally capped by the quality of the frozen vq-f8 tokenizer.\n1. Missing comparision with CDCD mentioned in related work, which also \"preserves the continuous-time formulation by operating on noisy embeddings while training with cross-entropy\".\n1. Table 1 does not include a CFM baseline. The paper only argues the converge curve \"strongly suggests\" Purrception would surpass CFM, but this is not convicing enough.\n1. Missing visual comparison with baselines."}, "questions": {"value": "1. As shown in Figure 5, the temperature does not affect the generated image too much. How is temperature useful in real application? And any tests on DFM baseline?\n1. Is z-loss just a specific patch that masks a deeper problem with scaling Purrception?\n1. Any reason why adapting VFM to a fixed, pretrained codebook is novel compared to the CDCD framework?\n1. Any evident that the final FID of 4.72 is not a property of Purrception, but just an inherent limitation of the vq-f8 latent space itself?\n1. Did you test how Purrception performs with any other VQ-VAE tokenizers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ozn2clvMDO", "forum": "SA8xDYrUYB", "replyto": "SA8xDYrUYB", "signatures": ["ICLR.cc/2026/Conference/Submission12228/Reviewer_BoRK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12228/Reviewer_BoRK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762170922391, "cdate": 1762170922391, "tmdate": 1762923172242, "mdate": 1762923172242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a latent variational flow matching approach, Purrception, for vector quantized generative models. The motivation for the approach is to propose a hybrid between discrete and continuous latent flows for the latent generative models. They exploit the variational formulation of the marginal vector field and propose to model the discrete latent tokens as a categorical distribution (probabilities) from the continuous latent embeddings. With the experiments on the ImageNet-256 dataset, they show training efficiency over discrete and continuous latent generative models and improved final accuracy over discrete latent generative models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea of a continuous flow path and categorical token prediction combined under the variational flow matching framework is interesting. The application to VQ-VAEs does show improvement in generative performance compared to latent discrete flow-based approaches."}, "weaknesses": {"value": "Major:\n\nImportant baseline missing - The main improvement compared to baselines is shown in the training efficiency. However, a comparison to the key baseline latent generative model SiT (Ma et al. 2024) is missing. I would request the authors to add this to their training efficiency analysis to improve thoroughness.\n\nOther:\n\nThe model doesn't reach competitive FID vs continuous models. This is not a major weakness, as it is known (empirically) that continuous latent models tend to perform better than the discrete ones in image generation. However, I would request the authors to add a discussion of why a 750M Purrception (discrete) will be more useful than a 675M DiT (continuous) model."}, "questions": {"value": "In line 294, what is e^{x_i}?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jdC8EawKXD", "forum": "SA8xDYrUYB", "replyto": "SA8xDYrUYB", "signatures": ["ICLR.cc/2026/Conference/Submission12228/Reviewer_i3Vq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12228/Reviewer_i3Vq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762221829842, "cdate": 1762221829842, "tmdate": 1762923171760, "mdate": 1762923171760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Thank you to all reviewers and updates on the second version of the paper"}, "comment": {"value": "We would like to thank you very much for all the constructive feedback we received from all reviewers. The questions and weaknesses you pointed out helped us improve the paper a lot!\n\nWe addressed every comment raised by each reviewer separately below. However, we expect to upload a second version of the paper in the next few days, where we aim to include *mostly* the followings:\n\n- improving the positioning of our work in literature\n- validating Purrception's performance with better VQ tokenizers\n- comparing Purrception with SiT baseline\n- rewriting Section 4.2 to enhance clarity about softmax temperature scaling\n- fixing small mathematical / writing inconsistencies"}}, "id": "96H6Y0u5Zo", "forum": "SA8xDYrUYB", "replyto": "SA8xDYrUYB", "signatures": ["ICLR.cc/2026/Conference/Submission12228/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12228/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission12228/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763744550714, "cdate": 1763744550714, "tmdate": 1763744550714, "mdate": 1763744550714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}