{"id": "ivaIwRZvTT", "number": 9190, "cdate": 1758114580264, "mdate": 1759897738755, "content": {"title": "Disentangled Pseudo-Labeling and Classification for Class-Imbalanced Semi-Supervised Learning", "abstract": "Although significant improvements have been made in addressing class-imbalanced semi-supervised learning (CISSL), many algorithms still suffer from confirmation bias. Inaccurate pseudo-labels hinder the learning of the classifier, which in turn leads to further inaccurate pseudo-labels—creating a self-reinforcing loop that amplifies bias, particularly toward majority classes. This bias arises because the classifier that generates pseudo-labels is simultaneously trained on the unlabeled data it labels. To address this issue, we propose a novel CISSL algorithm, Disentangled Pseudo-Labeling and Classification (DPC). DPC introduces an auxiliary classifier, dedicated solely to generating pseudo-labels, called a pseudo-labeler, which is attached to the representation layer of the backbone semi-supervised learning algorithm. To prevent confirmation bias, the pseudo-labeler is trained exclusively on labeled data, ensuring that pseudo-label generation remains unaffected by noisy unlabeled samples. Furthermore, to mitigate imbalanced feature representations—which are often biased toward majority classes and exacerbate confirmation bias—DPC propagates the classifier’s training loss to the shared representation layer to encourage balanced feature learning. Benefiting from high-quality pseudo-labels and balanced feature representations, DPC achieves state-of-the-art classification performance on CISSL benchmark datasets.", "tldr": "", "keywords": ["Class-Imbalanced Semi-Supervised Learning", "Pseudo-labeling"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/448363abb9a7c363d2efbac345a901596105bc01.pdf", "supplementary_material": "/attachment/fa7d482af25645ac616d159fc16ee4fe40769e74.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical challenge of class-imbalanced semi-supervised learning (CISSL).\nThe author(s) develop a framework that introduces an auxiliary pseudo-labeler (PSL) learned from labeled and unlabeled data to mitigate confirmation bias, together with a reweighted feature-level loss designed to alleviate representation imbalance.\nExtensive experiments on small scaled long-tailed semi-supervised benchmarks demonstrate competitive or superior performance compared with existing state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important and well-known problem in CISSL, which is confirmation bias caused by training the classifier on both labeled and unlabeled data jointly.\n\n2. The proposed idea is simple yet conceptually clear, and provides an intuitive way to decouple pseudo-label generation from classifier training."}, "weaknesses": {"value": "1. The main idea of separating pseudo label generating and classifier is relatively straightforward and overlaps to some extent with prior works, such as auxiliary or debiased classifier approaches.\n\n2. The writing style is highly uniform and repetitive,  showing a lack of natural variation and subtle reasoning. The presentation occasionally seems overly structured and mechanical, resembling auto-generated by GPT or other AI tools.\n\n3. It remains unclear why two classifiers (PSL and CLS) are both necessary if PSL also learned from labeled data and already provides pseudo-labels. What prevents simply using PSL predictions within the backbone? \nIn my opinion, providing more theoretical or experimental proof will be better.\nIn addition, the motivation and justification for the feature-level reweighting term are not sufficiently explained.\n\n4. Some parts of the paper, such as Eqs. (6), (8), and (9), appear overly formalized, where simple ideas are expressed through unnecessarily complex mathematical notation."}, "questions": {"value": "1. How does DPC scale to larger and more complex datasets, such as Small-ImageNet-127 or full ImageNet-LT?\n\n2. What is the essential benefit of separating the pseudo-labeler (PSL) and the classifier? Since both modules share the same feature extractor, how does training the PSL solely on labeled data effectively prevent confirmation bias when the shared representation is still updated using pseudo-labeled samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WagfFlRCSy", "forum": "ivaIwRZvTT", "replyto": "ivaIwRZvTT", "signatures": ["ICLR.cc/2026/Conference/Submission9190/Reviewer_xPhz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9190/Reviewer_xPhz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638316286, "cdate": 1761638316286, "tmdate": 1762920862073, "mdate": 1762920862073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Disentangled Pseudo-Labeling and Classification (DPC) method to address confirmation bias observed in class-imbalanced semi-supervised learning (CISSL). DPC consists of two modules: a pseudo-labeler (PSL) trained exclusively on labeled data to generate pseudo-labels, and a classifier (CLS) trained with a rebalanced loss on shared features to address feature-level imbalance. Both modules use a simple class-reweighting scheme, assigning weights inversely proportional to class frequency. Comprehensive experiments validate the superiority of the method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "+ This paper is well-motivated and easy to follow.\n+ This proposed method performs well on commonly used benchmark datasets CIFAR and STL-10."}, "weaknesses": {"value": "+ The novelty of the proposed method is limited. The main contribution of this paper lies in disentangling the pseudo-labeling and classification framework. However, the strategy of decoupling representation learning from classifier training has already been explored in recent works [1, 2], on both imbalanced supervised learning [1] and imbalanced semi-supervised learning [2]. Besides, this paper introduces an auxiliary classifier, called a pseudo-labeler, which is trained solely on labeled data to produce pseudo labels for unlabeled samples, similar to the recent method ABC [3].\n+ The reweighting of sample-wise losses based on the class distribution to mitigate class imbalance is a very common practice in the CISSL setting. Similar reweighting strategies have also been employed in RECD [3] and TAF [4].\n+ In addition to lacking strong technical novelty, the performance improvement appears marginal relative to the substantial computational overhead introduced by multiple forward passes. Moreover, the performance even underperforms recent methods [5, 6, 7, 8, 9].\n+ Furthermore, the experimental datasets (CIFAR and STL-10) are too small to convincingly demonstrate the effectiveness of the proposed method. It would be more convincing to include evaluations on more realistic benchmarks, such as ImageNet-127 [10] or Semi-Aves [6].\n+ Minor Comments:\n   - In line 143, the prediction range of the model $f_{\\theta}(x)$ should be $\\mathbb R$, If the intention is to denote the output range of the function $\\xi$, please make this distinction explicitly.\n   - In Equation (6), the summation subscript M is not defined.\n   - There is a typo in Line 144: \"mpas\" → \"maps\".\n- - -\n**Reference:**   \n[1] Decoupling Representation and Classifier for Long-Tailed Recognition. In ICLR, 2020.   \n[2] Rethinking Re-Sampling in Imbalanced Semi-Supervised Learning. In Arxiv, 2020.   \n[3] ABC: Auxiliary Balanced Classifier for Class-Imbalanced Semi-Supervised Learning. In NeurIPS, 2021.   \n[4] Rebalancing Using Estimated Class Distribution for Imbalanced Semi-Supervised Learning under Class Distribution Mismatch. In ECCV, 2024.   \n[5] Triplet Adaptation Framework for Robust Semi-Supervised Learning. In TPAMI, 2024.   \n[6] DASO: Distribution-aware semantics-oriented pseudo-label for imbalanced semi-supervised learning.   \n[7] Towards realistic long-tailed semi-supervised learning: Consistency is all you need. In CVPR, 2023.   \n[8] Twice class bias correction for imbalanced semi-supervised learning. In AAAI, 2024.   \n[9] SimPro A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning.   \n[10] COSSL: Co-learning of representation and classifier for imbalanced semi-supervised learning. In CVPR, 2022."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vROkgpq9Bo", "forum": "ivaIwRZvTT", "replyto": "ivaIwRZvTT", "signatures": ["ICLR.cc/2026/Conference/Submission9190/Reviewer_5L6V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9190/Reviewer_5L6V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827024122, "cdate": 1761827024122, "tmdate": 1762920861540, "mdate": 1762920861540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Disentangled Pseudo-Labeling and Classification (DPC) for class-imbalanced semi-supervised learning. DPC separates pseudo-label generation from classifier training to reduce confirmation bias and employs reweighted and feature-level losses to handle imbalance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**: The paper presents a clear and original idea by explicitly disentangling pseudo-label generation from classifier training, effectively addressing the confirmation bias issue in class-imbalanced semi-supervised learning.\n\n**quality**: The experimental quality is strong, with comprehensive comparisons, ablation studies, and sensitivity analyses on multiple benchmarks.\n\n**Clarity**: The structure of this paper is clear, and the language is well-articulated.\n\n**Significance**: The work provides a significant and practical contribution to improving robustness and fairness in semi-supervised learning under class imbalance."}, "weaknesses": {"value": "1.The robustness of the pseudo-labeling module (PSL) under extreme imbalance or very few labeled samples is not fully analyzed—PSL may overfit when labeled data are highly limited.\n\n2.The EMA-based estimation of unlabeled data distribution could be unstable when the unlabeled set is highly biased, yet the paper lacks a sensitivity or robustness analysis for such cases.\n\n3.The masking rule (“top 50% most frequent classes” and class-specific threshold ρₖ) appears empirical. The authors should clarify the rationale for these design choices and provide sensitivity studies"}, "questions": {"value": "1.Both the masking rule and the class reweighting scheme are crucial to DPC’s success, yet their design choices are largely empirical and lack sufficient analysis.\n\n2.Since the initial unlabeled distribution $p_u^{0}$​ is initialized from the labeled set, how would DPC behave if the labeled data were biased?\n\n3.The notation of the subscript $x_b^{m} \\in MB_{x}$​ in Equation (6) is ambiguous and requires clarification.\n\n4.The expression for $\\gamma_{u}$ on line 139 is not entirely correct and could easily confuse readers. $M_k$ is the number of unlabeled samples of class k. However, $\\gamma_{u}$ should be calculated based on the $M_k$ sequence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sXeUYHRPhD", "forum": "ivaIwRZvTT", "replyto": "ivaIwRZvTT", "signatures": ["ICLR.cc/2026/Conference/Submission9190/Reviewer_E2LY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9190/Reviewer_E2LY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827956489, "cdate": 1761827956489, "tmdate": 1762920861182, "mdate": 1762920861182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address confirmation bias in class-imbalanced semi-supervised learning (CISSL). The authors argue that confirmation bias arises because the classifier that generates pseudo-labels is simultaneously trained on the unlabeled data it labels, forming a self-reinforcing loop. To break this loop, the paper proposes the DPC framework, whose core idea is to disentangle pseudo-label generation from the final classification task. DPC introduces a “pseudo-labeler” (PSL) trained only on labeled data to produce higher-quality pseudo-labels, and a “classifier” (CLS) that is trained using these pseudo-labels. In addition, DPC designs a feature loss to mitigate imbalance at the representation level. The authors conduct extensive experiments on multiple CISSL benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Adding two heads (PSL/CLS) and loss terms on top of an existing SSL backbone entails minimal engineering changes and offers good reusability.\n- The authors perform comprehensive experiments on standard datasets (CIFAR-10-LT, CIFAR-100-LT, STL-10-LT) and across both distribution-matched and mismatched imbalance settings, comparing against many recent baselines. The experimental coverage is broad and the comparisons are thorough.\n- CISSL is a real and highly challenging core problem, making the topic timely and important."}, "weaknesses": {"value": "- Decoupling or adding auxiliary modules in class-imbalanced learning is a common strategy; methodologically, the innovative contribution of this paper is limited.\n\n- The DPC framework integrates a backbone SSL algorithm, PSL, and CLS—three classification modules—plus multiple losses (L_back, L_psl, L_cls, L_feature). The system is overly complex. The intricate interactions among modules (e.g., both L_back and L_feature update the shared feature extractor) are not clearly explained or analyzed, giving the impression of a carefully tuned ensemble rather than a simple, elegant solution. The authors should further clarify the role of each module and how the losses are balanced. There are additional ambiguities that need explanation, such as the difference between L_back and L_psl, their respective purposes, why both are necessary, and which classifier is used at inference time.\n\n- The loss functions rely on sample-wise reweighting based on W. By contrast, many SOTA methods use Logit Adjustment (LA)-based losses, which have stronger theoretical foundations and, in my experience, outperform sample-wise weighting. This has also been discussed in many supervised learning papers. Please justify why sample-wise weighting is preferable in the semi-supervised setting. Moreover, the paper does not provide code; please release reproducible open-source code during rebuttal.\n\n- The paper claims \"DPC also addresses feature representation imbalance,\" yet it is widely accepted in the long-tailed literature (e.g., Decoupling representation and classifier for long-tailed recognition) that reweighting harms representation learning. The authors should explain why reweighting here improves representation learning. In addition, the motivation for squaring the sample weights (W^2) is vague and ad hoc. Why square rather than use, for example, 1.5-power or 3-power? This key design lacks theoretical or strong empirical support.\n\n- In Table 5, removing the core PSL module (“Without PSL”) on STL-10-LT (γl=10) only drops performance from 81.6 to 80.9. Such a small decrease severely weakens the central claim that “PSL is key to solving confirmation bias.” If a complex PSL module yields only a negligible gain, its necessity is questionable. Similarly, removing the feature loss (“Without feature loss”) or masking (“Without masking”) results in very small drops, suggesting that the practical contributions of these components may be limited.\n\n- Although the method is claimed to be SOTA, some presented results do not surpass existing SOTA approaches; under certain settings, performance is worse than ACR (published two years ago)."}, "questions": {"value": "Refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uOR3qL8XKl", "forum": "ivaIwRZvTT", "replyto": "ivaIwRZvTT", "signatures": ["ICLR.cc/2026/Conference/Submission9190/Reviewer_ft2t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9190/Reviewer_ft2t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831855168, "cdate": 1761831855168, "tmdate": 1762920860704, "mdate": 1762920860704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}