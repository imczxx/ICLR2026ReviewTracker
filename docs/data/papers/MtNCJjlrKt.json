{"id": "MtNCJjlrKt", "number": 10474, "cdate": 1758172962503, "mdate": 1763646885783, "content": {"title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research", "abstract": "This paper tackles \\textbf{open-ended deep research (OEDR)}, a complex challenge where AI agents must synthesize vast web-scale information into insightful reports. Current approaches are plagued by dual-fold limitations: static research pipelines that decouple planning from evidence acquisition and monolithic generation paradigms that include redundant, irrelevant evidence, suffering from hallucination issues and low citation accuracy. To address these challenges, we introduce \\textbf{WebWeaver}, a novel dual-agent framework that emulates the human research process. The planner operates in a dynamic cycle, iteratively interleaving evidence acquisition with outline optimization to produce a comprehensive, citation-grounded outline linking to a memory bank of evidence. The writer then executes a hierarchical retrieval and writing process, composing the report section by section. By performing targeted retrieval of only the necessary evidence from the memory bank via citations for each part, it effectively mitigates long-context issues and citation hallucinations. Our framework establishes a new state-of-the-art across major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and DeepResearchGym. These results validate our human-centric, iterative methodology, demonstrating that adaptive planning and focused synthesis are crucial for producing comprehensive, trusted, and well-structured reports.", "tldr": "A novel agent framework to achieve SOTA for open-ended deep research", "keywords": ["Large Language Models", "Agent", "Deep Research"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c986dc210593dc7234f78da3f4b447400aa68538.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes WebWeaver, a dual-agent “planner + writer” framework for open-ended deep research. The planner dynamically produces a structured outline with linked evidence; the writer then retrieves the required evidence section by section and composes the content hierarchically based on the outline. The authors conduct experiments and ablations across multiple benchmarks, and the results demonstrate the effectiveness of the approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is easy to follow, the figures are clear, and the experimental details are thorough. The final results look solid.\n\n2. It folds “dynamic outline optimization + evidence binding” into the planning loop and uses section-wise retrieval + writing, which mirrors how humans actually do deep research.\n\n3. It releases a 3k SFT dataset and shows gains on smaller models, which supports the method’s effectiveness."}, "weaknesses": {"value": "1. The planning stage parses 100+ pages and burns tens of thousands of tokens, but there’s no apples-to-apples report of wall-clock latency or API cost vs. baselines.\n2. It is recommended to include parameter-matched comparisons across different frameworks built on the same model backbone, or reproduction experiments against other strong baselines on the identical model."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r4UEVLUnpH", "forum": "MtNCJjlrKt", "replyto": "MtNCJjlrKt", "signatures": ["ICLR.cc/2026/Conference/Submission10474/Reviewer_W3Xh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10474/Reviewer_W3Xh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907045243, "cdate": 1761907045243, "tmdate": 1762921769923, "mdate": 1762921769923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes WebWeaver, a dual-agent “planner + writer” framework for open-ended deep research. The planner dynamically produces a structured outline with linked evidence; the writer then retrieves the required evidence section by section and composes the content hierarchically based on the outline. The authors conduct experiments and ablations across multiple benchmarks, and the results demonstrate the effectiveness of the approach."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is easy to follow, the figures are clear, and the experimental details are thorough. The final results look solid.\n\n2. It folds “dynamic outline optimization + evidence binding” into the planning loop and uses section-wise retrieval + writing, which mirrors how humans actually do deep research.\n\n3. It releases a 3k SFT dataset and shows gains on smaller models, which supports the method’s effectiveness."}, "weaknesses": {"value": "1. The planning stage parses 100+ pages and burns tens of thousands of tokens, but there’s no apples-to-apples report of wall-clock latency or API cost vs. baselines.\n2. It is recommended to include parameter-matched comparisons across different frameworks built on the same model backbone, or reproduction experiments against other strong baselines on the identical model."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r4UEVLUnpH", "forum": "MtNCJjlrKt", "replyto": "MtNCJjlrKt", "signatures": ["ICLR.cc/2026/Conference/Submission10474/Reviewer_W3Xh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10474/Reviewer_W3Xh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907045243, "cdate": 1761907045243, "tmdate": 1763696910377, "mdate": 1763696910377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents WebWeaver, a dual-agent system for deep research questions. They distinguish their approach from previous methods like “search-then-generate” or “outline-then-write”  by  introducing a planner–writer loop in which the planner iteratively refines a dynamic outline and the writer performs hierarchical writing. They run experiments on DeepResearch Bench, DeepConsult, and DeepResearchGym and showed they outperformed baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important and and timely problem.\n\n- The dual-agent design  is very tangible and human-inspired very well motivated.\n\n- Comprehensive experiments across three strong benchmarks\n\n- comprehensive ablation studies"}, "weaknesses": {"value": "- Missing cost analysis compared to baselines \nThe paper does not clearly specify when the planner decides to terminate the outline optimization process—what the stopping criteria or thresholds are.\n\n- Several important open-source baselines are missing, such as DeepResearcher, OpenScholar, and open sourced LLM + tool-use systems. Including these would strengthen claims of generality and fairness.\n\n- The WebWeaver-3k dataset is also said to be built from “diverse queries crawled from the web,” yet the crawling criteria, domains, or filters are unspecified."}, "questions": {"value": "- Did you do any analysis on what actiona cause the most problem of answering wrong? for example things like early termination?\n-  how were the web queries for WebWeaver-3k crawled—were they topic-balanced or randomly collected?\n - How would webweaver as a dual agent do in terms of cost compared to single agent architechture"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wQQej1vPks", "forum": "MtNCJjlrKt", "replyto": "MtNCJjlrKt", "signatures": ["ICLR.cc/2026/Conference/Submission10474/Reviewer_yb9V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10474/Reviewer_yb9V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946738440, "cdate": 1761946738440, "tmdate": 1762921769489, "mdate": 1762921769489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **WebWeaver**, a dual‑agent system (Planner + Writer) for “open‑ended deep research” (OEDR). The Planner iteratively interleaves web search and outline optimisation; the Writer then synthesises a long‑form report section‑by‑section using a citation‑grounded memory bank. The authors claim that this human‑inspired workflow eliminates the “loss‑in‑the‑middle” problem, reduces hallucinations, and yields state‑of‑the‑art results on three recently released OEDR benchmarks (DeepResearch Bench, DeepConsult, DeepResearchGym). They also present a 3 k‑example SFT dataset (WebWeaver‑3k) that purportedly enables a 30 B‑scale model to match the performance of much larger proprietary agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear, human-inspired, end-to-end system design with an explicit evidence chain: planning → evidence extraction into a memory bank → outline with citation IDs → retrieval-grounded writing. The qualitative case study is coherent and illustrates the intended workflow.\n- Across three reasonably comprehensive benchmarks, results indicate competitive to SOTA performance. On DeepResearch Bench, the reported citation accuracy (93.37%) and effective citations are particularly strong. On DeepConsult and DeepResearchGym, the win rate and average scores are strong relative to listed systems.\n- Ablations address two core hypotheses: (1) iterative outline optimization improves depth/breadth/support and end-to-end scores, and (2) hierarchical, citation-driven writing outperforms brute-force context stuffing. These analyses are aligned with the proposed mechanism of action.\n- Useful operational statistics (number of searches, outline size, number of saved pages, evidence tokens) convincingly argue that memory-centric, targeted retrieval is not just helpful but necessary under the reported scale (>100 pages, >60k evidence tokens).\n- The SFT dataset and fine-tuning experiments show that the framework can act as a data engine for transferring agentic skills to smaller models, with large gains in citation accuracy. This is a practical contribution for accessibility."}, "weaknesses": {"value": "- Cost and budget parity are not controlled or reported. WebWeaver averages ~100+ pages parsed, ~67k evidence tokens, and ~26k-token outputs. Many baseline systems (especially proprietary) may have stricter rate/token limits. Without a controlled per-task budget (time, API calls, tokens), performance advantages may partly reflect higher spend rather than method.\n- Baseline comparability is unclear. WebWeaver is evaluated with multiple powerful base LLMs (Qwen-235B, Claude Sonnet 4), and even when the “agent model” is Claude, a separate model (GPT-oss-120B) is used for URL selection and evidence extraction. This multi-model, auxiliary-LLM design could materially boost performance yet is not mirrored for baselines. It is therefore difficult to isolate method gains from model capability/tooling differences.\n- Statistical analysis is thin. No confidence intervals, no statistical tests, and no repeated runs are reported. Improvements on RACE overall scores are modest versus strong baselines (e.g., ~1 point vs Gemini 2.5), and the absence of variance estimates undermines claims of reliable superiority. DeepConsult reports win/tie/lose counts but no significance testing despite pairwise comparisons being amenable to binomial or bootstrap CIs.\n- Reliance on LLM-as-judge across all benchmarks creates evaluation circularity risks; although the paper follows “official” judges per benchmark, additional human evaluation on a representative subset would strengthen claims, especially for “insight,” “readability,” and “support.”\n- Effective citations metric may be confounded by output length. WebWeaver outputs very long reports; if “Eff. c.” counts raw validated citations, longer outputs can inflate the score. There is no normalization for output length or per-claim basis reported. The very large Eff. c. numbers (200+) relative to baselines raise the possibility that the metric rewards verbosity rather than calibrated evidence use.\n- The ablation “hierarchical vs brute-force writing” lacks sufficient detail to be fully persuasive: context limits, chunking strategy for the brute-force baseline, and exact parity of planner outputs across conditions are not specified. Without these, it’s unclear if the baseline is disadvantaged by context overflow or suboptimal chunking rather than an inherent weakness of the paradigm.\n- The outline optimization ablation uses samples with three rounds of optimization and then evaluates earlier rounds; however, the prompt template prescribes “at least three” updates but the reported average outline optimization steps are ~2.2. This mismatch between prescribed behavior and observed behavior should be clarified.\n- No code, no executable release, and no links to run-time infrastructure are provided. Prompts are included, but the system depends on multiple LLMs, tools, and scraping steps. Critical parameters (stop criteria, deduplication policies, cache behavior, search engine settings, rate limits, tie-breaking rules, etc.) are not fully specified. This reduces reproducibility.\n- The WebWeaver-3k dataset creation lacks overlap analysis against evaluation prompts and lacks licensing audits of scraped content. Without an overlap/contamination check, the risk of implicit test exposure cannot be discounted.\n- The ethics section is a little generic at the moment and should also address website terms-of-service compliance, robots.txt adherence, or how PII in web pages is handled during evidence extraction and dataset curation. For an agent that crawls and stores web-scale evidence, these omissions are material."}, "questions": {"value": "1. Code/data release: Will you release the full WebWeaver code (planner/writer, tools, memory) and the WebWeaver-3k dataset? If not, what components will be made public and when?\n2. Fairness controls: Can you provide a controlled comparison where WebWeaver and a strong baseline agent both use the same backbone LLM and judge, to isolate framework gains? Any cross-judge robustness (e.g., alternate judges or human eval subsets)?\n3. Cost/latency: What are average wall-clock time, token counts, and dollar costs per task on each benchmark, for both planning and writing? How does it compare to baselines?\n4. Evidence extraction specifics: How are quote spans selected, how is evidence deduplicated/normalized, and what is the granularity for memory entries? How do you handle noisy, contradictory, or paywalled sources?\n5. Failure and correction: How do the planner/writer detect and correct citation errors (e.g., broken links, drifted IDs, semantically mismatched evidence)? Any automatic validation?\n6. WebWeaver-3k licensing and distribution: What is the licensing status of the dataset, given it is derived from web materials and proprietary teacher outputs? Can it be redistributed?\n7. Safety and misuse: What safeguards are in place to prevent the agent from citing low-quality or harmful sources? Do you filter by source credibility or domain allowlists/blacklists?\n8. Outline-to-retrieval mapping: How robust is the citation-ID mapping across multiple optimization rounds (IDs added/removed)? How do you maintain referential integrity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OV3AhDySAt", "forum": "MtNCJjlrKt", "replyto": "MtNCJjlrKt", "signatures": ["ICLR.cc/2026/Conference/Submission10474/Reviewer_4PPn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10474/Reviewer_4PPn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966034033, "cdate": 1761966034033, "tmdate": 1762921769107, "mdate": 1762921769107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **WebWeaver**, a dual‑agent system (Planner + Writer) for “open‑ended deep research” (OEDR). The Planner iteratively interleaves web search and outline optimisation; the Writer then synthesises a long‑form report section‑by‑section using a citation‑grounded memory bank. The authors claim that this human‑inspired workflow eliminates the “loss‑in‑the‑middle” problem, reduces hallucinations, and yields state‑of‑the‑art results on three recently released OEDR benchmarks (DeepResearch Bench, DeepConsult, DeepResearchGym). They also present a 3 k‑example SFT dataset (WebWeaver‑3k) that purportedly enables a 30 B‑scale model to match the performance of much larger proprietary agents."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Clear, human-inspired, end-to-end system design with an explicit evidence chain: planning → evidence extraction into a memory bank → outline with citation IDs → retrieval-grounded writing. The qualitative case study is coherent and illustrates the intended workflow.\n- Across three reasonably comprehensive benchmarks, results indicate competitive to SOTA performance. On DeepResearch Bench, the reported citation accuracy (93.37%) and effective citations are particularly strong. On DeepConsult and DeepResearchGym, the win rate and average scores are strong relative to listed systems.\n- Ablations address two core hypotheses: (1) iterative outline optimization improves depth/breadth/support and end-to-end scores, and (2) hierarchical, citation-driven writing outperforms brute-force context stuffing. These analyses are aligned with the proposed mechanism of action.\n- Useful operational statistics (number of searches, outline size, number of saved pages, evidence tokens) convincingly argue that memory-centric, targeted retrieval is not just helpful but necessary under the reported scale (>100 pages, >60k evidence tokens).\n- The SFT dataset and fine-tuning experiments show that the framework can act as a data engine for transferring agentic skills to smaller models, with large gains in citation accuracy. This is a practical contribution for accessibility."}, "weaknesses": {"value": "- Cost and budget parity are not controlled or reported. WebWeaver averages ~100+ pages parsed, ~67k evidence tokens, and ~26k-token outputs. Many baseline systems (especially proprietary) may have stricter rate/token limits. Without a controlled per-task budget (time, API calls, tokens), performance advantages may partly reflect higher spend rather than method.\n- Baseline comparability is unclear. WebWeaver is evaluated with multiple powerful base LLMs (Qwen-235B, Claude Sonnet 4), and even when the “agent model” is Claude, a separate model (GPT-oss-120B) is used for URL selection and evidence extraction. This multi-model, auxiliary-LLM design could materially boost performance yet is not mirrored for baselines. It is therefore difficult to isolate method gains from model capability/tooling differences.\n- Statistical analysis is thin. No confidence intervals, no statistical tests, and no repeated runs are reported. Improvements on RACE overall scores are modest versus strong baselines (e.g., ~1 point vs Gemini 2.5), and the absence of variance estimates undermines claims of reliable superiority. DeepConsult reports win/tie/lose counts but no significance testing despite pairwise comparisons being amenable to binomial or bootstrap CIs.\n- Reliance on LLM-as-judge across all benchmarks creates evaluation circularity risks; although the paper follows “official” judges per benchmark, additional human evaluation on a representative subset would strengthen claims, especially for “insight,” “readability,” and “support.”\n- Effective citations metric may be confounded by output length. WebWeaver outputs very long reports; if “Eff. c.” counts raw validated citations, longer outputs can inflate the score. There is no normalization for output length or per-claim basis reported. The very large Eff. c. numbers (200+) relative to baselines raise the possibility that the metric rewards verbosity rather than calibrated evidence use.\n- The ablation “hierarchical vs brute-force writing” lacks sufficient detail to be fully persuasive: context limits, chunking strategy for the brute-force baseline, and exact parity of planner outputs across conditions are not specified. Without these, it’s unclear if the baseline is disadvantaged by context overflow or suboptimal chunking rather than an inherent weakness of the paradigm.\n- The outline optimization ablation uses samples with three rounds of optimization and then evaluates earlier rounds; however, the prompt template prescribes “at least three” updates but the reported average outline optimization steps are ~2.2. This mismatch between prescribed behavior and observed behavior should be clarified.\n- No code, no executable release, and no links to run-time infrastructure are provided. Prompts are included, but the system depends on multiple LLMs, tools, and scraping steps. Critical parameters (stop criteria, deduplication policies, cache behavior, search engine settings, rate limits, tie-breaking rules, etc.) are not fully specified. This reduces reproducibility.\n- The WebWeaver-3k dataset creation lacks overlap analysis against evaluation prompts and lacks licensing audits of scraped content. Without an overlap/contamination check, the risk of implicit test exposure cannot be discounted.\n- The ethics section is a little generic at the moment and should also address website terms-of-service compliance, robots.txt adherence, or how PII in web pages is handled during evidence extraction and dataset curation. For an agent that crawls and stores web-scale evidence, these omissions are material."}, "questions": {"value": "1. Code/data release: Will you release the full WebWeaver code (planner/writer, tools, memory) and the WebWeaver-3k dataset? If not, what components will be made public and when?\n2. Fairness controls: Can you provide a controlled comparison where WebWeaver and a strong baseline agent both use the same backbone LLM and judge, to isolate framework gains? Any cross-judge robustness (e.g., alternate judges or human eval subsets)?\n3. Cost/latency: What are average wall-clock time, token counts, and dollar costs per task on each benchmark, for both planning and writing? How does it compare to baselines?\n4. Evidence extraction specifics: How are quote spans selected, how is evidence deduplicated/normalized, and what is the granularity for memory entries? How do you handle noisy, contradictory, or paywalled sources?\n5. Failure and correction: How do the planner/writer detect and correct citation errors (e.g., broken links, drifted IDs, semantically mismatched evidence)? Any automatic validation?\n6. WebWeaver-3k licensing and distribution: What is the licensing status of the dataset, given it is derived from web materials and proprietary teacher outputs? Can it be redistributed?\n7. Safety and misuse: What safeguards are in place to prevent the agent from citing low-quality or harmful sources? Do you filter by source credibility or domain allowlists/blacklists?\n8. Outline-to-retrieval mapping: How robust is the citation-ID mapping across multiple optimization rounds (IDs added/removed)? How do you maintain referential integrity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OV3AhDySAt", "forum": "MtNCJjlrKt", "replyto": "MtNCJjlrKt", "signatures": ["ICLR.cc/2026/Conference/Submission10474/Reviewer_4PPn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10474/Reviewer_4PPn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966034033, "cdate": 1761966034033, "tmdate": 1763651970548, "mdate": 1763651970548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces WebWeaver, a dual-agent framework for open-ended deep research (OEDR).\nThe system consists of a Planner that dynamically do evidence acquisition and outline optimization, and a Writer that performs memory-grounded, hierarchical synthesis by section. The human-centric design simulates real research workflows.\nWebWeaver is claimed to achieve SOTA on DeepResearch Bench, DeepConsult, and DeepResearchGym against Gemini-2.5, OpenAI DeepResearch and Claude Research."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clear and well-motivated problem framing: tackling OEDR beyond static pipelines.\n- Dual-agent design (Planner + Writer) inspired by real-wold human research process.\n- Extensive experiments across 3 major benchmarks with detailed breakdowns with strong quantitative evidence.\n- Thorough ablation and statistical analysis showing the strength of outline optimization and hierarchical writing.\n- Clear figures, intuitive workflow, and solid writing throughout."}, "weaknesses": {"value": "- Heavy rely on LLM-as-judge evaluations that lacks human verification to some extend\n- I'm curious what's the computational cost and latency? \n- Some redundancy between Sections 3.2 and 3.3, the method flow could be slightly more concise.\n- How scalable is WebWeaver for real-world web-scale research (e.g., 1k+ page retrieval)?\n- How sensitive is performance to the backbone model (Qwen vs. Claude vs. GPT)?\n- Minor formatting issue: Fig. 1 move to page 2 looks better"}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ulUilDdOwb", "forum": "MtNCJjlrKt", "replyto": "MtNCJjlrKt", "signatures": ["ICLR.cc/2026/Conference/Submission10474/Reviewer_21rJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10474/Reviewer_21rJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762302077666, "cdate": 1762302077666, "tmdate": 1762921768637, "mdate": 1762921768637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response Summary"}, "comment": {"value": "Dear ACs and Reviewers,\n\nWe extend our deepest gratitude to the reviewers for their dedicated time and insightful feedback throughout the review process. We are particularly encouraged by their recognition of our work's **strong motivation** (@21rJ, @W3Xh), **novel human-inspired design** (@21rJ, @4PPn, @yb9V, @W3Xh), **well presentation** (@W3Xh), and **SOTA performance** (@21rJ, @4PPn, @W3Xh) validated through **extensive experiments** (@21rJ, @yb9V) on widely-used benchmarks.\n\nThe major clarifications are summarized as follows:\n\n- **Human Evaluation:** We have conducted a new human evaluation, confirming the superior quality and coherence of WebWeaver's generated reports over baselines (@21rJ, @4PPn).\n- **Cost Analysis:** We now provide a detailed cost analysis, demonstrating that WebWeaver's slightly increased computational cost is a deliberate trade-off for its significant gains in report quality and accuracy (@21rJ, @4PPn, @yb9V, @W3Xh).\n- **Statistical Analysis:** To ensure the robustness of our results, we have performed statistical analysis across three independent runs, confirming the stability and significance of WebWeaver's performance gains (@21rJ, @4PPn).\n- **Expanded Baselines:** We have expanded our experiments to include more strong baselines (including open-source systems and a strictly controlled ReAct setup), further solidifying WebWeaver's state-of-the-art performance (@4PPn, @yb9V, @W3Xh).\n- **Controlled Ablation Studies**: New ablation studies have been added to isolate the architectural gains of our framework, proving that WebWeaver's core components—not just the underlying LLM—are key to its success (@4PPn, @yb9V).\n- **Data & Ethics Statement:** We have significantly expanded our Ethics Statement to provide full transparency on our data processing, including details on PII anonymization, adherence to `robots.txt`, and terms of service, ensuring our work aligns with the highest ethical standards (@4PPn, @yb9V).\n\nWe provided point-by-point responses to each reviewer as below. We have also included all these modifications into our revised manuscript with blue text."}}, "id": "7f67Jfuksn", "forum": "MtNCJjlrKt", "replyto": "MtNCJjlrKt", "signatures": ["ICLR.cc/2026/Conference/Submission10474/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10474/Authors"], "number": 15, "invitations": ["ICLR.cc/2026/Conference/Submission10474/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763646777583, "cdate": 1763646777583, "tmdate": 1763646777583, "mdate": 1763646777583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}