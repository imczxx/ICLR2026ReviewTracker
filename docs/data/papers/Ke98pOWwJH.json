{"id": "Ke98pOWwJH", "number": 23047, "cdate": 1758338784632, "mdate": 1759896834515, "content": {"title": "DANCE: Difficulty and Novelty Co-driven Exploration", "abstract": "Reinforcement learning in environments with sparse rewards presents a formidable conundrum. Numerous exploration techniques strive to surmount this challenge by inciting agents to explore novel states. However, as familiarity with the environment burgeons, the novelty of states wanes, yielding an unguided exploration trajectory during later phases of learning. To surmount this quandary, this study posits that the difficulty of attaining a state functions as a more potent intrinsic motivational beacon, guiding the agent throughout the learning process. This difficulty signal encapsulates pivotal insights into the environment's underlying structure and the task's trajectory, a facet transcending the exclusive purview of state novelty. Subsequently, we introduce a reward prediction network to acquire a hybrid reward sourced from both state difficulty and novelty. Initially elevated for novel states, this reward progressively converges toward the state's inherent difficulty as visitations accumulate. This dynamic formulation assuages the scourge of catastrophic forgetting, shepherding the agent precisely across the learning odyssey. We establish the theoretical underpinnings of this reward mechanism as a distinct manifestation of reward shaping. It ensures the consistency between the learned policy and the original policy and additionally transforms the sparse reward problem into a dense reward problem, consequently accelerating the entire learning process. We evaluate the proposed Difficulty and Novelty Co-driven Exploration agent on several tasks with sparse rewards, and it consistently achieves satisfactory results.", "tldr": "A novel exploration method in Reinforcement Learning", "keywords": ["Exploration"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f13e311d84ad5d9ea7b85581ae6db8ca78a36829.pdf", "supplementary_material": "/attachment/ad6b7abd5a41926868cc464505a3b8898c93d8e9.zip"}, "replies": [{"content": {"summary": {"value": "DANCE is an intrinsic-motivation method that blends novelty with enduring difficulty: a random-target vs. learned-predictor RPN yields intrinsic reward as prediction error, which is high for new states and, with revisits, converges to the state‚Äôs steps-to-reach (difficulty), thus sustaining exploration. A reward-shaping view argues policy invariance while explaining learning speed-ups; experiments show better maze coverage and stronger returns on hard-exploration Atari than RND/PPO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Recasts intrinsic motivation by combining novelty with a persistent, learned notion of state difficulty so that exploration remains guided even after novelty fades\n\n- Presents a reward-shaping analysis showing that DANCE‚Äôs intrinsic reward is potential-based and thus policy-invariant to the original MDP optimum (despite being asymptotically inconsistent), strengthening correctness.\n\n- The method is simple to implement"}, "weaknesses": {"value": "- Conceptual mismatch between ‚Äúdifficulty‚Äù and its proxy, the paper motivates difficulty as the hardness of reaching a state, yet uses normalized time step $d_s(s_t) \\infty t$ as the proxy. This approximates hitting time only in special cases (fixed start, near-deterministic dynamics, minimal loops, consistent episode horizons). The paper does not characterize when this proxy fails (e.g., stochasticity, partial observability, multiple paths with different lengths, or environments with exogenous timers), beyond a brief ‚Äúclock‚Äù limitation note \n\n- The chosen proxy (step count) is heuristic; it may not always capture true task complexity or generalize well to all domains.\n\n- The difficulty signal can be hijacked by irrelevant but predictable features (e.g., clocks in the environment), leading to misleading intrinsic rewards."}, "questions": {"value": "- What is the relative contribution of novelty vs. difficulty? Would the method still be effective if one of the signals is removed?\n\n- You argue the RPN error is high for novel states and, with revisits, converges to a state‚Äôs difficulty (steps-to-reach). Can you formalize conditions under which the prediction error e(s) decomposes as e(s)=novelty(s,t)+difficulty(s) and the difficulty component is identifiable (e.g., mixing/coverage assumptions on trajectories)?\n\n- The reward-shaping analysis claims policy invariance despite DANCE‚Äôs intrinsic reward not vanishing asymptotically. Could you restate the exact potential $\\phi(s_t)$ and clarify the scope of the invariance result: which choices of Œ≤ (intrinsic weight), discount Œ≥, and predictor training schedules still guarantee invariance? Any edge cases where off-policy updates or stale targets break the shaping argument?\n\n- Table 1 compares DANCE to several exploration methods. Could you also include modern novelty/difficulty-style baselines (e.g., NovelD/NGU/Plan2Explore/E3B/DEIR where applicable) or clarify why they are out of scope? This would sharpen where DANCE‚Äôs ‚Äúpersistent difficulty‚Äù advantage matters most.\n\n- In the shaping analysis and curriculum theorem, what assumptions on the behavior policy, stationarity, and predictor convergence are required? Can you provide a bounded-error statement: if the predictor tracks difficulty with error ùúñ, what‚Äôs the induced performance gap relative to the policy optimal under true difficulty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vf0wYPOlWQ", "forum": "Ke98pOWwJH", "replyto": "Ke98pOWwJH", "signatures": ["ICLR.cc/2026/Conference/Submission23047/Reviewer_ksRn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23047/Reviewer_ksRn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400053814, "cdate": 1761400053814, "tmdate": 1762942489909, "mdate": 1762942489909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes¬†DANCE (Difficulty and Novelty Co-driven Exploration), an intrinsic motivation mechanism for reinforcement learning in sparse-reward environments. The core idea is to combine state¬†novelty¬†and¬†difficulty¬†to maintain exploration signals throughout training. The method introduces a¬†Reward Prediction Network (RPN), structurally similar to Random Network Distillation (RND), consisting of a fixed random target network and a predictor network trained to minimise prediction error. Unlike RND, the target embedding in DANCE is augmented with a ‚Äúdifficulty‚Äù signal proportional to the number of steps taken to reach each state. The resulting intrinsic reward is high for novel states early in training and converges to the state‚Äôs difficulty value as familiarity increases, thus preventing the intrinsic signal from vanishing. The authors provide theoretical analysis interpreting DANCE as a form of potential-based reward shaping that preserves policy invariance, and claim that the approach transforms sparse-reward problems into dense-reward ones. Empirical results on a maze domain and several hard-exploration Atari games suggest that DANCE improves exploration efficiency and learning stability compared to RND and other curiosity-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method is simple and understandable. \n- The writeup is clean and correct. \n- The visualisations show the exploration behaviour clearly.\n- The algorithm seems to work reasonably well compared to other baselines."}, "weaknesses": {"value": "* While a contribution doesn't have to be entirely novel to provide value, in my view DANCE is Random Network Distillation with an additional conditioning term attached to the target network based on the assumption that states further in time are harder to reach. This however I feel is a very simplistic assumption.\n* The step index is not necessarily correlated with how hard a state is to reach. In many environments (e.g., stochastic, looping, or partially observable ones), a later timestep might correspond to:\n\t- wandering aimlessly in a known region,\n\t- revisiting trivial states, or getting stuck in local cycles.\n* Also,¬†difficult¬†states may appear early (e.g., near traps, narrow passages, or high-risk transitions).\n* One major issue I see is that by tying the difficulty to the¬†time index¬†rather than the¬†structural properties of the state, DANCE seems to¬†destroy the notion of reusability:\n\t- The¬†same¬†visual or semantic state encountered at different timesteps will yield¬†different difficulty signals.\n\t- Consequently, the reward prediction network cannot learn reusable embeddings or abstractions.\n\t- This breaks¬†compositionality¬†and¬†transferability, the model can‚Äôt generalise difficulty knowledge across trajectories or episodes.\n\tIn contrast, a true difficulty measure (e.g., based on reachability, energy, or transition entropy) would be invariant to when the state was encountered.\n\n* I believe the only truly difficult sparse exploration environment tested here was Montezuma's Revenge. I would have liked to see exploration on other sparse reward exploration domains such as MiniGrid or MiniHack, since they may uncover the shortcomings mentioned above (repeating states in partial observability).\n\n\nMinor weaknesses:\n* The abstract uses formulations that in my view make comprehension difficult. I would suggest the authors formulate the abstract in a more streamlined way."}, "questions": {"value": "1. How exactly is the ‚Äúdifficulty‚Äù signal computed in practice, and does it adapt beyond being proportional to the timestep?\n2. How would DANCE behave in partially observable or aliased environments (e.g., MiniGrid) where the same observation can appear at different timesteps with conflicting difficulty values?\n3. Did the authors compare DANCE to a simpler baseline such as RND with a time-based or decaying intrinsic reward to isolate the effect of the difficulty term?\n4. Can the claimed ‚Äúcurriculum formation‚Äù be demonstrated quantitatively rather than only through qualitative heatmaps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QKqCIisufV", "forum": "Ke98pOWwJH", "replyto": "Ke98pOWwJH", "signatures": ["ICLR.cc/2026/Conference/Submission23047/Reviewer_duoK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23047/Reviewer_duoK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909548033, "cdate": 1761909548033, "tmdate": 1762942489227, "mdate": 1762942489227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DANCE, an intrinsic-motivation mechanism that combines novelty with a state-difficulty signal implemented via a Reward Prediction Network (RPN). The predictor is trained to match a target embedding and a scalar ‚Äúdifficulty‚Äù term tied to step count and similar to previous work such as RND, the intrinsic reward is the predictor‚Äìtarget discrepancy, which is high on novel states and converges to per-state ‚Äúdifficulty.‚Äù The authors also provided a theoretical analysis which frames the mechanism as potential-based reward shaping and argues policy equivalence with the original MDP."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Mixing novelty with a non-vanishing intrinsic signal to prevent late-stage drift is well motivated and easy to implement (RPN structure is close to RND).\n2. This paper provides empirical results on hard exploration problems with an intuitive visualization. DANCE improves coverage in the pure-exploration Maze and shows competitive/strong results on several Atari benchmarks compared to baseline algorithms."}, "weaknesses": {"value": "1. ‚ÄúDifficulty‚Äù lacks a formal, state-only definition. The paper sets difficulty proportional to the (normalized) step index t needed to reach a state and trains RPN so that the reward converges to that quantity. Acorss the paper does not present a formal and clean definition on the difficulty ds(s) beyond ‚Äúproportional to steps.‚Äù in the text.\n2.   Results are reported on one Maze and seven Atari games, but MiniGrid hard-suite} and ProcGen, which are natural tests for exploration and generalization are not included. The paper itself notes background-similarity issues (e.g., Solaris/Frostbite) that can degrade the step-learning signal, yet it does not demonstrate robustness beyond Atari.\n3. Theorems 2 \\& 3 are largely restate definitions (``the optimal policy maximizes the stated objective'') rather than offering new insight. If retained, they should be framed as preliminaries, not contributions. Convergence assumptions (for both the policy and the predictor) are not made explicit but are essential to the arguments.\n4. Theorem 1 and Theorem 4 are not convincing: \n        4.1): A ``fixed/converged'' policy may be only locally optimal and fail to visit many states; unvisited states invalidate the claim.\n        4.2): Policy convergence does not imply the RPN has converged to its MSE optimum.\n        4.3): Even if both converge, with a \\(k\\)-dimensional embedding and a scalar difficulty will broadcast to all dimensions,\n        $r^{\\text{int}}(s)=\\|\\hat g(s)-g(s)\\|^2 = k\\,\\big[ds(s)\\big]^2$\n5. Theorem 4 may not hold as it depends on Theorem 1. It would benefit more from a clear existence proof."}, "questions": {"value": "1. Can you compare NovelD on Atari and extend to MiniGrid/ProcGen to evaluate generality beyond Atari?\n2. Is it guaranteed that the intrinsic reward will not dominate the training and mislead the policy learning?\n3. Adding an explicit mathematical definition on difficulty measurement would be helpful for understanding the paper\n4. Hyperparameter $\\beta$ seems to be quite different in Maze and Atari game (from 0.01 to 0.5). How sensitive is your method to this hyperparameter?\n5. It would be desirable to see the performance bound if the paper wants to focus on theoretical contributions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Udnq5XS4h", "forum": "Ke98pOWwJH", "replyto": "Ke98pOWwJH", "signatures": ["ICLR.cc/2026/Conference/Submission23047/Reviewer_6rN7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23047/Reviewer_6rN7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958328643, "cdate": 1761958328643, "tmdate": 1762942488955, "mdate": 1762942488955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors consider the problem of intrinsic motivation in sparse-reward RL problems requiring substantial exploration. To complement state novelty (a signal which can lead agents to ignore ‚Äúgood‚Äù states if familiar, later in training), the authors propose to learn the ‚Äúdifficulty‚Äù of states. They show that if a difficulty metric can be defined, that policies learned based on this metric are a subset of optimal policies. They then show superior exploration in toy and several sparse reward Atari tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* the idea of proposing a more stable metric for intrinsic exploration (other than novelty) is reasonable and commendable objective\n* The authors do a decent job demonstrating that the metric they‚Äôre learning improves exploration on the tasks shown\n* The authors provide several proofs which appear to be correct under the assumptions made."}, "weaknesses": {"value": "* the paper is quite unclear in its description of the metric they are proposing\n* the concept of ‚Äúdifficulty‚Äù is never clearly defined. And seems impossible to define universally, since certain policies will consider certain states ‚Äúeasy‚Äù and other states ‚Äúhard‚Äù.\n* Using ‚Äúnumber of steps since start‚Äù or ‚Äúnumber of steps away from start‚Äù requires a lot of assumptions on the environment which may not be there.\n* not obvious how this would work with continuous control problems, where ‚Äúdifficulty‚Äù via distance is a lot murkier.\n* In general, difficulty seems to require additional oracle information (dramatically limiting its utility), approaching providing something close to a dense reward / proper value function."}, "questions": {"value": "* How does state novelty trade off with difficulty? No ablations were done here (e.g. tuning that tradeoff)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sEZSYDIijd", "forum": "Ke98pOWwJH", "replyto": "Ke98pOWwJH", "signatures": ["ICLR.cc/2026/Conference/Submission23047/Reviewer_R9fo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23047/Reviewer_R9fo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23047/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762990329094, "cdate": 1762990329094, "tmdate": 1762990329094, "mdate": 1762990329094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}