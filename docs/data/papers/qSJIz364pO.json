{"id": "qSJIz364pO", "number": 6774, "cdate": 1757995248697, "mdate": 1759897894863, "content": {"title": "Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation", "abstract": "Enabling embodied agents to effectively imagine future states is critical for robust and generalizable visual navigation. Current state-of-the-art approaches, however, adopt modular architectures that separate navigation planning from visual world modeling, leading to state–action misalignment and limited adaptability in novel or dynamic scenarios. To overcome this fundamental limitation, we propose UniWM, a unified, memory-augmented world model integrating egocentric visual foresight and planning within a single multimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly grounds action decisions in visually imagined outcomes, ensuring tight alignment between prediction and control. A hierarchical memory mechanism further integrates detailed short-term perceptual cues with longer-term trajectory context, enabling stable, coherent reasoning over extended horizons. Extensive experiments across four challenging benchmarks (Go Stanford, ReCon, SCAND, HuRoN) demonstrate that UniWM substantially improves navigation success rates by up to 30%, significantly reduces trajectory errors compared to strong baselines, and exhibits impressive zero-shot generalization on the unseen TartanDrive dataset. These results highlight UniWM as a principled step toward unified, imagination-driven embodied navigation.", "tldr": "", "keywords": ["World models", "Visual navigation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/817d0ca5a4e4a8d9bbf2ae41ff21dde253024209.pdf", "supplementary_material": "/attachment/b2c47a8e4ef43b716a6cb29d3b096119fee9194d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes UniWM, a unified multimodal autoregressive world model for goal-conditioned visual navigation. Unlike prior modular approaches that decouple planning and imagination, UniWM integrates both within a single backbone and augments inference with a hierarchical memory mechanism. The model alternates between predicting actions and visualizing future observations, leveraging intra-step and cross-step memory banks to stabilize long-horizon rollouts. Extensive experiments across four benchmarks and zero-shot generalization to TartanDrive show strong performance gains over existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Clarity and Motivation**: The paper is well-written, with a clear problem framing and motivation. It effectively critiques limitations of prior modular designs and positions UniWM as a principled alternative.\n\n- **Unified Architecture**: The integration of planning and imagination into a single autoregressive backbone is novel and addresses state–action misalignment, which is also validated in the ablation study.\n\n- **Memory-Augmented Inference**: The hierarchical memory mechanism (KV-cache based) is well-motivated and empirically shown to improve long-horizon stability.\n\n- **Comprehensive Experiments**: UniWM outperforms baselines across multiple datasets in both navigation and visualization metrics. Ablation studies are thorough and isolate contributions of memory, tokenization, and training strategies."}, "weaknesses": {"value": "1.  **Problem Formulation Violates MDP Principles**:\n\n    The world model is defined as $o_{t+1} = W(o_t, a_{t+1}, o_s, o_g)$, which violates the standard MDP formulation. In MDPs, the transition model should **only reflect environment dynamics** and **be independent of the task goal $o_g$**. Conditioning on $o_g$ risks conflating planning with dynamics modeling, and this theoretical inconsistency undermines the model’s interpretability and generalizability.\n\n2. **Information Leakage via Goal Conditioning**:\n\n    Conditioning the **transition model** on the final **goal observation $o_g$** introduces potential leakage, especially in **visualization metrics** like SSIM and LPIPS. This is particularly problematic for **state-action pairs near the goal**, where the model may trivially reconstruct the goal view. **Comparisons with NWM (which does not use $o_g$ during prediction) are  thus not entirely fair**. Additional experiments isolating the impact of $o_g$ conditioning —e.g., removing it or masking it—would strengthen the claims.\n\n3. **Unfair Dataset Usage in Benchmark Comparisons**:\n\n    The paper reports large gains on **Go Stanford**, but unlike NWM (Go stanford is only used for evaluation), UniWM uses this dataset during training. This undermines the fairness of the comparison and inflates the perceived contribution. \n\n4. **Training Details and Architectural Transparency**:\n    Several aspects of the training pipeline are under-specified:\n    - The label smoothing loss $L_{LS}$ is mentioned but not clearly differentiated from the main objectives.\n    - The implementation of cross-attention between the decoder and memory modules is not described. Did the authors modify standard Transformer blocks or insert new cross-attention layers between the decoder and memory module?"}, "questions": {"value": "1. Discretized Bin Token Loss: Why was this chosen over naive cross-entropy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WLXMYebw7N", "forum": "qSJIz364pO", "replyto": "qSJIz364pO", "signatures": ["ICLR.cc/2026/Conference/Submission6774/Reviewer_LfkC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6774/Reviewer_LfkC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761043781844, "cdate": 1761043781844, "tmdate": 1762919051226, "mdate": 1762919051226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified architecture for policy and world model, and a plug-in hierarchical memory mechanism using KV caches. The proposed method is evaluated on goal -conditioned navigation tasks and shows competitive performance compared to NWM and other baselines. However, the reported results are questionable given the discrepancies in training settings with the critical baseline. I will be willing to increase my score if my concerns regarding the experiment setting are addressed."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The proposed hierarchical memory mechanism is intriguing and shows some good potential for navigation tasks."}, "weaknesses": {"value": "1. The unified architecture of predicting both action and future images has been used in multiple other works [1][2] so novelty in this aspect is limited. As the authors claim this unified architecture and joint training will help align imagination with control, it will be helpful to have some experiments to support this claim. For example, separate planner and world models can be trained with same training data and compared to this proposed unified model.\n\n2. My primary concern is about the experiment setup. Clearly NWM is the strongest baseline that this work tries to beat, but there are some significant discrepancies in the experiment setup. NWM uses ReCon, SCAND, HuRoN and TartanDrive for training and in-domain evaluation, while using GO Stanford for OOD evaluation due to the low resolution images. However, this paper uses TartanDrive for OOD evaluation and the other four datasets for training. Is there any specific reason that you have to make this change? Did you train NWM yourself on the same training datasets as yours? Otherwise all the conclusions drawn from comparison to NWM's results are significantly compromised.\n\n3. I notice that NWM reported ATE 5.63 and RPE 1.18 in their paper as in-domain evaluation, but the out-of-domain results in Table 6 of the paper (which should be harder) shows that NWM achieves an ATE of 1.61 and a RPE of 0.62, both of which are much better. How is NWM evaluated in your paper and is there any explanation for this discrepancy?\n\n[1] Imagine while Reasoning in Space: Multimodal Visualization-of-Thought https://arxiv.org/abs/2501.07542\n[2] WorldVLA: Towards Autoregressive Action World Model https://arxiv.org/abs/2506.21539"}, "questions": {"value": "1. Experiment result of separate planner and world models trained with same data.\n2. It is quite surprising for me that the proposed hierarchical memory using KV caches works this well without any finetuning, considering they are from early layers (layer 5-7). Based on the description, this memory is used for both action and image generation. Do you know in which step (action or image generation) it has more significant impact? Do you think if this memory mechanism can be directly applied to other policy or world model networks? \n3. Other questions are listed in the weakness session."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "imV6OJ0ojk", "forum": "qSJIz364pO", "replyto": "qSJIz364pO", "signatures": ["ICLR.cc/2026/Conference/Submission6774/Reviewer_fGwF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6774/Reviewer_fGwF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857558409, "cdate": 1761857558409, "tmdate": 1762919049579, "mdate": 1762919049579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniWM, a unified, memory-augmented world model trying to address critical limitations in existing visual navigation.\nUniWM integrates egocentric visual foresight and navigation planning into a single multimodal autoregressive backbone, explicitly grounding action decisions in visually imagined outcomes to ensure prediction-control alignment. This paper introduces a hierarchical memory mechanism, where intra-step memory caches immediate perceptual cues, while cross-step memory accumulates long-term trajectory context. Experiments on four benchmarks (Go Stanford, ReCon, SCAND, HuRoN) show UniWM boosts navigation success rates (SR) by up to 30% and reduces trajectory errors (ATE/RPE) versus baselines (e.g., GNM, VINT, NoMaD, NWM)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) By bridging visual imagination and navigation planning into a single multimodal autoregressive backbone, this paper demonstrates that both tasks can be mutually beneficial to tighten alignment between prediction and control, which sets a promising scheme for future visual navigation approaches.\n\n(2) The proposed hierarchical memory bank mechanism is an interesting and efficient idea to incorporate historical inputs into the visual planning and imagination process. By reusing the key-value (KV)-cache stored in selected decoder layers (intra-step memory) and across temporal steps (cross-step memory), the scheme avoids redundant computation of past perceptual cues and trajectory context.\n\n(3) All the proposed components are fairly evaluated and analyzed with clear ablation studies, making it easy to quantify and understand the technical contribution of each part of the paper. \n\n(4) I appreciate the authors’ efforts to support reproducibility, including the code in the supplementary materials"}, "weaknesses": {"value": "(1) The planning results are evaluated within an open-loop evaluation protocol, which limits the ability to fully assess UniWM’s long-horizon performance and generalization.\n\n(2) The demonstrated results do not include scenarios with dynamic obstacles, a critical gap given that dynamic elements (e.g., moving pedestrians, vehicles) are common and challenging in real-world navigation tasks. \n\n(3) The paper lacks efficiency analysis compared to state-of-the-art navigation methods like NoMaD—an oversight given that real-time planning frequency is a critical requirement for visual navigation tasks."}, "questions": {"value": "(1) The paper states that UniWM is fine-tuned on the GAIR Anole-7B backbone, but the rationale for selecting Anole-7B specifically remains unclear. Could the authors elaborate on the key factors that led to the choice of Anole-7B over other existing multimodal large language models (MLLMs) (e.g., Qwen-VL-2.5) for visual navigation tasks? \n\n(2) There are other world models also support imagining the planning process given start and target images—aligning with UniWM’s goal of integrating visual imagination and navigation planning. How does the UniWM performance compares with the Aether in the navigation benchmarks?\n\n(3) The paper emphasizes UniWM’s practical value for real-world navigation where inference speed directly impacts deployment viability. However, it provides no details on the time cost of decoding entire navigation trajectories.  Could the authors specify the average inference time per trajectory on the evaluated datasets?\n\n[1] Team, Aether, et al. \"Aether: Geometric-aware unified world modeling.\" arXiv preprint arXiv:2503.18945 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hTSyW48aB9", "forum": "qSJIz364pO", "replyto": "qSJIz364pO", "signatures": ["ICLR.cc/2026/Conference/Submission6774/Reviewer_zu2f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6774/Reviewer_zu2f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891843486, "cdate": 1761891843486, "tmdate": 1762919049301, "mdate": 1762919049301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper proposes a novel unified, memory-augmented world model, UniWM, which integrates egocentric \"visual imagination\" and route planning within a single model. The unified design allows the information to be tightly integrated between the 2 tasks (prediction and control). A hierarchical memoty module combines the sensor embeddings with the world context for long-term planning. In 4 benchmarks, the proposed UniWM improves success rates, and reduces trajectory errors. The zero-shot setup in the unseen TartanDrive also shows the generalizability of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Method proposes a novel architecture which significantly outperform existing SOTA for 4 benchmarks, and a zero-shot setting.\n\n2. Quality of paper is high in term of architecture design and experimental results.\n\n3. Paper is clear, but requires some background knowledge of the field.\n\n4. Paper is highly significant in the significant improvements (up to 30%) for the ego-centricity navigation task."}, "weaknesses": {"value": "Paper highlights domain shift and fixed token budget as 2 limitations. \n\nFor the domain shift issue, the proposed method does not seems to have a good potential solution."}, "questions": {"value": "1. Paper highlights domain shift and fixed token budget as 2 limitations. But only proposes some possible solutions for Fixed token budget, and not domain shift. Please suggest some possible solutions/mitigation methods for the domain shift issue."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HVgAqv6W0z", "forum": "qSJIz364pO", "replyto": "qSJIz364pO", "signatures": ["ICLR.cc/2026/Conference/Submission6774/Reviewer_Vagp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6774/Reviewer_Vagp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971798390, "cdate": 1761971798390, "tmdate": 1762919048819, "mdate": 1762919048819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}