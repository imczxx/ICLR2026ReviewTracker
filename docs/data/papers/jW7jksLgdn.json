{"id": "jW7jksLgdn", "number": 17833, "cdate": 1758281039635, "mdate": 1759897151285, "content": {"title": "Visual Cues-Induced Jailbreak Attack on Large Vision-Language Models", "abstract": "Although large vision-language models (LVLMs) demonstrate powerful capabilities across various tasks, their generated content still poses significant safety risks. Jailbreak attacks against LVLMs help uncover potential safety vulnerabilities in these models, guiding developers to build more robust safety guardrails. Existing black-box jailbreak attacks primarily exploit the weak capability of LVLMs to detect harmful information in the visual modality. These attacks transfer harmful intent from text to images, constructing \"benign text + harmful image'' combinations to bypass LVLMs' safety guardrails. In this paper, we reveal a novel safety vulnerability: LVLMs' responses are highly susceptible to visual information manipulation. Leveraging this property, we demonstrate that even when explicit harmful questions are present in the textual modality, it is still possible to effectively bypass LVLMs' safety guardrails. To this end, we propose a novel black-box jailbreak method called visual cues-induced attack (VCI). Different from prior methods that typically disguise harmful intent, VCI directly inputs complete harmful questions in the textual modality and requires LVLMs to infer answers based on the provided image, exploiting the visual cues embedded in the image to induce LVLMs to generate relevant harmful responses. Our method achieves an average attack success rate (ASR) of 77.0\\% on eight popular open-source LVLMs and 78.5\\% on four mainstream closed-source commercial LVLMs, outperforming existing state-of-the-art (SOTA) methods.", "tldr": "We propose a novel straightforward yet effective black-box jailbreak method against LVLMs.", "keywords": ["Jailbreak", "AI Safety", "Large Vision-Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/947b82481f3ae8fb6ae9db25ac0ae71a4179db7c.pdf", "supplementary_material": "/attachment/ab5826cd7509f74b51fe41cf8e0a0f1e786fd701.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies jailbreak attack on large vision-languange models. Existing black-box attack methods tend to hide harmful semantics from text input. Based on the observation that responses generated by LVLMs are susceptible to visual modality information, the authors show that by using visual cues to induce LVLMs, it is possible to effectively bypass the safety guardrails of LVLMs while explicitly including harmful instructions in the text input. Experiments on six open-source models and three commercial closed-source models demonstrate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper studies an important topic of jailbeak attack on LVLMs. It has drawn more and more attention due to popularity of vision-language foundation models.\n- Different from previous works focusing on manipulating text input only, this paper proposes to use visual cues and demonstrates its effectiveness without hiding any harmful semantics. This observation is helpful to researchers in this field to develop more effective attacking methods and understanding the vulnerability of LVLMs in visual input.\n- The proposed attack method is black-box, thus can be used to different models including close-source models.\n- The authors did extensive experiments on nine models. Ablation study on visual cues utilization instruction and analysis on model training method towards safety guardrails are helpful."}, "weaknesses": {"value": "- Since the proposed method does not hide harmful semantics, it is more prone to be detected by safety gaurdrails when image is not considered.\n- The method has a step of generating base image, leading to additional cost. The quality of the generated image may also affect the attack effectiveness. When T2I model cannot generate image for some particular question, the attack will fail."}, "questions": {"value": "- In the proposed method, harmful contents are not hidden from text input. If a similar strategy like previous works is used to hide harmful contents, will VCI still be helpful to the attack?\n\n- When creating images with character identities through typography, the demonstration example uses two words. Will it be helpful to include more detailed descriptive words? Could the authors show a set of commonly used identities?\n\n- In 4.4 Table 4, VCI shows better ASR than Q4. Have the authors tried other images and observed how images quality is related to ASR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zMdixtxHp5", "forum": "jW7jksLgdn", "replyto": "jW7jksLgdn", "signatures": ["ICLR.cc/2026/Conference/Submission17833/Reviewer_B8yN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17833/Reviewer_B8yN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761020918715, "cdate": 1761020918715, "tmdate": 1762927672128, "mdate": 1762927672128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VCI (Visual Cues-Induced Attack), a novel black-box jailbreak attack targeting Large Vision-Language Models (LVLMs). Unlike prior methods that hide harmful intent by embedding it into images, VCI keeps the complete harmful question in text and instead uses visual cues embedded in the image (through typography) to induce the model to generate harmful responses. Experiments across 8 open-source and 4 closed-source LVLMs show average ASRs of 77.0% and 78.5%, respectively, outperforming prior SOTA attacks like FigStep and VRP. The study also explores defenses and ablation results, finding that simple defenses (like noise or perplexity filters) are ineffective. Overall, the paper claims to reveal a previously underexplored vulnerability: that LVLMs’ safety alignment can be bypassed not by obscuring harmful intent, but by exploiting the visual modality’s inductive influence on model behavior."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel perspective: The attack does not rely on disguising harmful content, unlike prior visual jailbreaks. Instead, it exploits cross-modal susceptibility, revealing a new failure mode of LVLMs.\n- Strong empirical results: Consistent and high ASR across both open- and closed-source models.\n- Comprehensive evaluation: Includes ablations, topic-level breakdowns, and defense analysis."}, "weaknesses": {"value": "1. Interpretation of Figure 4: The authors show that embeddings for harmful and benign queries remain separable under VCI, yet the model still produces harmful outputs. This is interpreted as “visual cues overriding safety alignment.” However, when a defender knows this, they might be able to reverse engineer to filter out the harmful queries.\n2. Attack design assumptions (redundancy and missing controls):\nThe paper asserts that including the harmful query in text and image form strengthens the attack. However, this redundancy is questionable.\n- If removing harmful text increases ASR (as suggested but not explicitly tested), including it would be unnecessary and potentially counterproductive.\n- Conversely, if textual inclusion is required for attack success, that contradicts the claim that visual cues alone induce jailbreak behavior.\nAn ablation testing removal of textual harmful content would clarify the mechanism and strengthen the paper’s conclusions.\n3. Lack of mechanistic analysis:\nWhile the empirical vulnerability is evident, the paper does not probe why visual inputs dominate—e.g., whether this arises from cross-modal attention, gradient alignment, or pretraining data biases."}, "questions": {"value": "1. Clarification: Does Figure 4’s separation of embeddings imply that harmfulness is detectable but not mitigated? Could embedding-space separation be leveraged for defense?\n2. Design rationale: Why include harmful queries both in the text and the image? Would typography alone (without textual repetition) still succeed? Did you test removing the harmful text from prompts to isolate the effect of visual cues?\n3. Mitigation: Can you discuss strategies to mitigate VCI? For example\n- Pretraining/fine-tuning: Incorporating safety-aligned multimodal data, etc.\n- Inference-time defenses: Visual-textual consistency checks, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5WFuML2fSR", "forum": "jW7jksLgdn", "replyto": "jW7jksLgdn", "signatures": ["ICLR.cc/2026/Conference/Submission17833/Reviewer_M6Dw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17833/Reviewer_M6Dw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809122397, "cdate": 1761809122397, "tmdate": 1762927671724, "mdate": 1762927671724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a new failure mode in LVLM safety: model outputs are highly susceptible to visual cues even when the text explicitly contains a harmful request. Building on this, the authors propose VCI, a black-box jailbreak attack that (i) embeds an attacker-chosen identity and keywords as typographic elements in an image, and (ii) explicitly instructs the LVLM to rely on those visual cues to answer a list-style, paraphrased version of the harmful question. Across 8 open-source LVLMs and 4 commercial LVLMs, VCI reports high attack success rates (ASR) and outperforms prior black-box baselines. The paper also studies defenses (self-reminder, perplexity filter, image noise)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1: Previous black-box LVLM jailbreaks primarily “move harm into the image” to bypass text-side safety mechanisms. In contrast, VCI preserves the harmful textual content but leverages visual cue–driven compliance to transform a model’s refusal into a direct response, representing a distinct and insightful attack mechanism.\n\n2: The attack uses readily available components (text-to-image, typography, paraphrasing) and a straightforward prompt template. This design not only facilitates replication but also provides a valuable foundation for developing effective defensive strategies."}, "weaknesses": {"value": "1: The paper lacks a clear analysis of the underlying mechanism behind the proposed attack. In particular, it remains unexplained why the introduction of a seemingly innocuous blank image substantially increases the success rate of jailbreak attempts (as shown in Figures 1 and 2). Similarly, the paper does not provide sufficient insight into why the VCI method is able to bypass the safety guardrails of LVLMs. A deeper examination of these mechanisms, such as how visual cues influence the model’s alignment layers or decision boundaries, would greatly strengthen the paper’s technical contribution and interpretability."}, "questions": {"value": "1: See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pxr8zTuN4y", "forum": "jW7jksLgdn", "replyto": "jW7jksLgdn", "signatures": ["ICLR.cc/2026/Conference/Submission17833/Reviewer_TsXZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17833/Reviewer_TsXZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962562987, "cdate": 1761962562987, "tmdate": 1762927671354, "mdate": 1762927671354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a black-box jailbreak attack against Large Vision-Language Models (LVLMs) called Visual Cues-Induced Attack (VCI). The method provides an explicit harmful question in the text modality while simultaneously embedding visual cues, such as negative character identities and keywords, into the image modality. VCI aims to exploit a vulnerability where LVLMs, when processing such inputs, are induced to bypass their safety guardrails and generate harmful content."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's primary strength is its extensive experimental evaluation, covering 12 popular open-source and closed-source LVLMs and comparing against 7 baseline methods.\n2. The VCI method achieves high ASR in the experiments, proving that this specific combination of explicit harmful text and visual cues is an effective attack configuration."}, "weaknesses": {"value": "1. The paper's core motivation—that visual information \"overrides\" internal knowledge —is built on a flawed argument (Figure 2). The model's behavior in the \"1840\" example is better explained as instruction-following rather than a fundamental override of its safety/factual training.\n2. As detailed in the \"Contribution\" section, the core mechanism of VCI (embedding cues in images) heavily overlaps with existing methods like FigStep and VRP. The paper fails to clearly articulate a fundamental conceptual innovation beyond an incremental change in the attack setup (i.e., making the text prompt explicitly harmful).\n3. The paper does not sufficiently disentangle why the attack succeeds. The high ASR could be induced by the specific content of the visual cues (e.g., \"Organ Trafficker\"), or it could be that the task structure itself (i.e., \"infer based on the image\") pushes the model into a less safety-aligned state. The ablation Q2 (blank image) hints at the latter but is not deeply explored."}, "questions": {"value": "1. Given that the prompt explicitly directs the model to answer \"Based on the image\" , how can the authors justify using this example as evidence of visual cues \"overriding\" internal knowledge, rather than the model simply \"following\" a task instruction?\n2. FigStep also embeds typographic text (e.g., \"Methods to... illegal human organ trade\") into the image. What is the fundamental mechanistic difference between VCI embedding \"Organ Trafficker\" and \"illegal\" and the prompts used by FigStep? Both seem to leverage the model's visual understanding to introduce harmful concepts.\n3. How can the authors prove that the attack's success is due to the specific content of the visual cues (e.g., \"Organ Trafficker\") rather than the task structure itself (i.e., the \"infer from image\" meta-instruction) disabling the safety guardrails? What is the ASR if benign visual cues (e.g., \"Doctor\" and \"health\" in the image) are used with the same harmful text prompt from VCI?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F3RA3RGFiT", "forum": "jW7jksLgdn", "replyto": "jW7jksLgdn", "signatures": ["ICLR.cc/2026/Conference/Submission17833/Reviewer_tUYe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17833/Reviewer_tUYe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995342902, "cdate": 1761995342902, "tmdate": 1762927670581, "mdate": 1762927670581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}