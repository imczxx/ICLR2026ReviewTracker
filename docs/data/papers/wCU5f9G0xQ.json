{"id": "wCU5f9G0xQ", "number": 22289, "cdate": 1758329089609, "mdate": 1763344402062, "content": {"title": "Connections between Schedule-Free Optimizers, AdEMAMix, and Accelerated SGD Variants", "abstract": "Recent advancements in deep learning optimization have introduced new algorithms, such as Schedule-Free optimizers, AdEMAMix, MARS and Lion which modify traditional momentum mechanisms. In a separate line of work, theoretical acceleration of stochastic gradient descent (SGD) in noise-dominated regime has been achieved by decoupling the momentum coefficient from the current gradient's weight. In this paper, we establish explicit connections between these two lines of work. We substantiate our theoretical findings with experiments on 300m and 150m scale language modeling task. We find that AdEMAMix, which most closely resembles accelerated versions of stochastic gradient descent, exhibits superior performance. Building on these insights, we introduce a modification to AdEMAMix, termed Simplified-AdEMAMix, which maintains the same performance as AdEMAMix across both large and small batch-size settings while eliminating the need for two different momentum terms.", "tldr": "", "keywords": ["AdEMAMix", "Schedule Free", "Accelerated SGD"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a965d575ae0b84d84e99a2f50f62a4c99cf9649b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper connects recent schedule-free optimizers with accelerated SGD, showing that methods like Lion, MARS, and AdEMAMix can be viewed as special cases of accelerated SGD with preconditioning or averaging. Based on this insight, the authors propose a simplified AdEMAMix that removes redundant momentum terms and achieves similar performance on Transformer language models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper analyzes the relationship between schedule-free optimizers and accelerated SGD, showing their mathematical equivalence under certain formulations. It derives a simplified version of AdEMAMix that removes one momentum term and compares it empirically on Transformer models, focusing on efficiency and reduced complexity."}, "weaknesses": {"value": "The experimental section is limited, with only a few validation-loss curves presented and no analysis of convergence speed, stability, or downstream task performance.\n\nThe experiments focus on medium-sized Transformer models, lacking broader evidence across different architectures or optimization settings.\n\nThe claimed equivalence among optimizers is shown mainly through algebraic reformulation without deeper empirical verification.\n\nThe proposed simplified AdEMAMix is evaluated only against a narrow set of baselines, making it unclear whether the improvement holds under diverse training regimes."}, "questions": {"value": "Can the authors provide more comprehensive experiments beyond validation-loss curves, such as convergence analysis or downstream performance?\n\nHow sensitive is the proposed simplified AdEMAMix to hyperparameters like learning rate or batch size compared to the original version?\n\nHave the authors tested whether the claimed theoretical equivalence holds empirically under different optimizers and training setups?\n\nWould adding comparisons on larger-scale or more diverse tasks change the observed trends?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GgxiR2ZAjg", "forum": "wCU5f9G0xQ", "replyto": "wCU5f9G0xQ", "signatures": ["ICLR.cc/2026/Conference/Submission22289/Reviewer_DFkx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22289/Reviewer_DFkx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760481043106, "cdate": 1760481043106, "tmdate": 1762942152522, "mdate": 1762942152522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper establishes a connection between two distinct lines of research: recently proposed optimizers for deep learning and the theory of accelerated stochastic gradient descent. The authors demonstrate that these modern optimizers can be framed as variants of accelerated SGD. Based on this insight authors propose  a new method - Simplified-AdEMAMix, which removes one of the momentum from the AdEMAMix and achieves the same performance in Language Modeling task\n\nStrengthes\n1) Insightful Theoretical Connection. The primary strength of this work is the bridge it builds between the practical success of modern optimizers and the formal theory of accelerated SGD. By showing that modern optimizers like MARS and AdEMAMix are mathematically related to specific accelerated SGD formulations, the paper provides a perspective that helps explain why these methods can be effective.\n2) Simplified-AdEMAMix. The proposed \"Simplified-AdEMAMix\" is promissing method. It addresses the significant memory overhead of AdEMAMix, which requires storing three momentum terms. This is particularly relevant for training large models where optimizer state size can be a bottleneck. The empirical results convincingly show that this simplified version maintains the performance of the original AdEMAMix.\n3) Strong Empirical Validation. The experiments are conducted on relevant language models (150m and 300m parameters) with substantial training data (up to 15B tokens)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) Insightful Theoretical Connection. The primary strength of this work is the bridge it builds between the practical success of modern optimizers and the formal theory of accelerated SGD. By showing that modern optimizers like MARS and AdEMAMix are mathematically related to specific accelerated SGD formulations, the paper provides a perspective that helps explain why these methods can be effective.\n2) Simplified-AdEMAMix. The proposed \"Simplified-AdEMAMix\" is promissing method. It addresses the significant memory overhead of AdEMAMix, which requires storing three momentum terms. This is particularly relevant for training large models where optimizer state size can be a bottleneck. The empirical results convincingly show that this simplified version maintains the performance of the original AdEMAMix.\n3) Strong Empirical Validation. The experiments are conducted on relevant language models (150m and 300m parameters) with substantial training data (up to 15B tokens)."}, "weaknesses": {"value": "1) Poorly Supported Claim. The paper hypothesizes that the poor performance of Schedule-Free AdamW at large batch sizes is due to the \"coupling between weight averaging and momentum coefficients.\" While this is a interesting explanation, it is presented without direct evidence. A more convincing approach would involve a targeted ablation study to isolate this coupling effect, rather than just postulating it as the cause for the observed performance gap.\n2) The paper claims that \"Adam with momentum scheduling can match the performance of AdEMAMix\". This is concluded from the observation that the best-performing Simplified-AdEMAMix run corresponds to the case where α=0 in the large batch-size regime, which reduces it to Adam with momentum scheduling. However, this claim raises several questions: why the α=0 is not used in small-batch setting? Why the AdEMAMix does not have this value in the sweep? Does this mean that momentum scheduling can be the real reason, why AdEMAMix performs better than other methods? How Simplified-AdEMAMix performs with α!=0?\n3) Hyperparameter Search Spaces. Appendix A reveals that the search spaces for hyperparameters (especially learning rate and α) are vastly different across optimizers. For instance, the learning rate space for Simplified-AdEMAMix ([1e-6, ..., 3.16e-5]) is orders of magnitude different from that for AdEMAMix ([3.16e-4, ..., 3.16e-3]), and their respective α search spaces are non-overlapping. The paper provides no justification for these choices.\n4) Simplified-AdEMAMix design. Simplified-AdEMAMix use theory-style momentum, which is uncommon for the DL community. The paper will benefit from discussion, how one should update his AdamW/AdEMAMix hyperparameters to use Simplified-AdEMAMix \n4) Lack of Reproducibility: The paper suffers from major reproducibility issues. The authors do not provide source code. Additionally, authors do not report best-performing hyperparameters for each experiment, which with experiment cost (~10k H100 GPU hours as noted in Appendix A) makes reproduction from scratch prohibitively expensive."}, "questions": {"value": "1) The hypothesis regarding the performance degradation of Schedule-Free AdamW due to the coupling of momentum and weight averaging is compelling. Have you considered or run any targeted experiments to isolate and confirm this effect?\n2) Could you elaborate on the rationale behind the selection of different hyperparameter search spaces for different optimizers in Appendix A? Specifically, why does Simplified-AdEMAMix require a much smaller learning rate and a different range for α compared to AdEMAMix?\n3) Could you please provide the exact best-performing hyperparameters used to generate the plots in Figures 1, 2, and 3? Furthermore, would you consider releasing the source code to allow the community to reproduce and build upon your findings?\n4) Why the α=0 is not used in small-batch setting? Why the AdEMAMix does not have this value in the sweep? How Simplified-AdEMAMix performs with α!=0?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yRD1BkJzmX", "forum": "wCU5f9G0xQ", "replyto": "wCU5f9G0xQ", "signatures": ["ICLR.cc/2026/Conference/Submission22289/Reviewer_HLyQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22289/Reviewer_HLyQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907649765, "cdate": 1761907649765, "tmdate": 1762942152161, "mdate": 1762942152161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a generalized accelerated-SGD framework and establishes connections between several recent optimizers, including Schedule-Free, Lion, MARS, AdEMAMix. The framework decouples the momentum coefficient from current gradient weight. It further proposes Simplified-AdEMAMix that use two momentums instead of three that can achieve comparable performance as AdEMAMix, outperforming other methods in the high batch size regims. Experiments on 150M/300M LLMs are provided to support the claims."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The unifying view is clear and compact. \n\n2. Experimental details are clearly provided."}, "weaknesses": {"value": "1. Limited theoretical novelty and significance. Much of the connection is pure algebraic rewriting of algorithms, and the unification seems superficial. The paper does not provide formal performance guarantee or new convergence results that explains the different convergence behavior of different methods. \n\n2. The proposed Simplified-AdEMAMix seems incremental. As shown in Algorithm 2, it removes the $(1-\\beta_1)$ factor before gradient for first momentum (step 5), and add another gradient term in the update (step 7). It is unclear why this brings significant improvement. \n\n3. Experiments are limited. The size of the model is relatively small. It is unclear whether the claims can be transferred to larger models, e.g., 1B size."}, "questions": {"value": "1. Is there any theoretical justification on why AdEMAMix, \"which most closely resembles accelerated stochastic gradient descent\", works the best? Why AdamW deviates from this framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6l2ObxW07V", "forum": "wCU5f9G0xQ", "replyto": "wCU5f9G0xQ", "signatures": ["ICLR.cc/2026/Conference/Submission22289/Reviewer_HVmQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22289/Reviewer_HVmQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953708708, "cdate": 1761953708708, "tmdate": 1762942151841, "mdate": 1762942151841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper unifies a handful of recent optimizers (Schedule-Free, AdEMAMix, MARS, Lion) into an accelerated SGD framework. Whereas heavy-ball and Nesterov momentum accelerate deterministic gradient descent; accelerated SGD methods that decouple the momentum term from the gradient term in the optimizer update have been proposed to accelerate in the presence of noise. They show that Schedule-Free SGD can be rewritten as accelerated SGD followed by weight averaging. Lion, Schedule-Free AdamW, AdEMAMix (with beta1=0), and LaProp can be rewritten as preconditioners followed by accelerated SGD.\n\nThey perform experiments on 150M and 300M Transformers and note several results that are consistent with their theoretical observations. In particular, they note that compared to AdamW, the accelerated methods perform well at small batch sizes but their advantages diminish at large batch sizes. This aligns with the notion that accelerated SGD methods should improve performance only in the noise-dominated regime. They also note that Schedule-Free AdamW performs especially poorly with large batch sizes and note this could be due to the inherent coupling of the momentum and weight averaging coefficients whose optimal values should deviate at large batch size. They also propose a simplification of AdEMAMix with a single momentum term that improves performance at large batch size."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This is an excellent paper. The literature on optimizers tends to proliferate closely related methods leaving a lack of understanding and comparison via empirical results that depend heavily on who had more compute to tune hyperparameters. It is refreshing to read a paper that consolidates a handful of interesting optimizers into one framework that sheds light on empirical results (particularly the diminished performance gains of accelerated SGD methods on large batch sizes compared to AdamW).\n\nSection 4.1 that rewrites Schedule-Free as an accelerated SGD update rule with weight averaging is an especially nice insight."}, "weaknesses": {"value": "- Minor comment: the paragraph in the related work that starts with “Over the years, several optimizers have been proposed … “ would be easier to read if you included the name of each optimizer, e.g. “Chen et al (2023) proposed the Lion optimizer discovered via a genetic search algorithm…”"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tqOHYbHg2U", "forum": "wCU5f9G0xQ", "replyto": "wCU5f9G0xQ", "signatures": ["ICLR.cc/2026/Conference/Submission22289/Reviewer_psGb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22289/Reviewer_psGb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762264957501, "cdate": 1762264957501, "tmdate": 1762942151382, "mdate": 1762942151382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}