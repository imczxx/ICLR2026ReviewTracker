{"id": "1tUjir7nub", "number": 12537, "cdate": 1758208453059, "mdate": 1759897503400, "content": {"title": "Reimagining Federated Knowledge Graph Embedding with One-Shot Adaptive Semantic Alignment", "abstract": "Knowledge graphs (KGs) are widely used in various knowledge-driven applications, such as the recent surge in large language models. Yet, its adoption across different organizations is often hindered by data silos and privacy concerns, resulting in fragmented knowledge and limited performance. Federated learning (FL) has recently emerged as a potential solution for KG sharing while preserving data privacy. However, most federated KG embedding methods incur high communication and computation costs due to iterative updates and heavy synchronization, yet yield only limited performance gains, with additional privacy leakage risk via multiple communications. To address these challenges, we propose OFKGE, an efficient One-Shot Federated Knowledge Graph Embedding framework. Clients independently train local KGE models and upload them once, eliminating iterative communication. The server distills and aggregates semantic patterns from submitted models to form a generalized global representation via the Adaptive Semantic Alignment (ASA), which is then redistributed as optimized auxiliary knowledge. Clients locally fine-tune their models using this distilled global guide, enhancing inference performance while preserving personalized structural information. OFKGE achieves competitive representation quality with minimal communication, making it suitable for resource-constrained and privacy-sensitive federated settings. Experimental results on the MKG-W, MKG-Y, DB15K, FB15K-237, and NELL-995 datasets demonstrate that OFKGE significantly reduces resource overhead while outperforming state-of-the-art methods on most metrics. The code is available at https://anonymous.4open.science/r/OFKGE-CACB.", "tldr": "", "keywords": ["Knowledge Graph", "Federated Learning", "One-Shot"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a58246e969058426b65282af414275431e70d18c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper utilizes one-shot federated learning paradigms (OFL) to scenarios in collaborated training of knowledge graph embeddings. The training procedure operates as a three-stage process: Starting with a somewhat conventional client side upload, the server then use a dynamic teacher selection with picks the most powerful client-side model that guides the server side training of a global model. The procedure is finished by a client-side fine-tuning leveraging the server-side global model. Experimental investigations show promising performance of the proposed OFKGE method"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The authors identified the communication overhead and privacy leakage as an important challenge in contemporary developments in federated knowledge graph embedding. One-shot FL might be a potential solution to this challenge.\n- The overall writing of the paper is clear"}, "weaknesses": {"value": "- Limited novelty: While the authors claimed OFKGE to be the first attempt in applying OFL methodology to federated KGE problems, the adaptation of OFL seems pretty trivial for me. The primary contribution of OFKGE seems to be the multi-teacher distillation method used in the second stage of the training pipeline (server-side distillation). However, such kinds of procedures have been previously studied in federated learning context such as [1], while [1] did not specifically handles OFL problems, I believe such kinds of extensions is pretty straightforward. Moreover, the so-called **adaptive teacher selection** method as detailed in equation (6) in the paper seems to be a *dynamic* selection method instead of an *adaptive* selection method, as it merely uses client-side uploaded weights to determine the per-step best teacher, but no statistics of previous server-side training are deviced------which for me would then indicate a formal sense of adaptivity. \n- Privacy guarantees: As the authors also pointed out in the intro section that privacy leakage of FKGE is a significant concern [2], it would be then interesting to investigate the actual privacy protection level of OFKGE. While it seems that OFKGE does not utilize strong privacy protocols like DP, it should provide empirical evidence of protection. I think in addition to performance reports, the authors should also state the privacy protection capabilities of OFKGE."}, "questions": {"value": "In addition to the weaknesses, I have one additional question:\n- **On relation-aware performance matrices**: During the first stage of OFKGE, the clients commute *relation-aware performance matrices* using their local datasets and the server uses this metric to guide the distillation process in the second stage of OFKGE. Therefore it makes me wonder that whether the distributional shifts of client-local datasets may severely affect the performance metrics------As the server do NOT have access to those evaluation datasets, it seems that relying on such types of matrices is somewhat unreliable. Please elaborate more on this design choice.\n\n\n[1]. Li, Mingyi, et al. \"Resource-aware federated self-supervised learning with global class representations.\" Advances in Neural Information Processing Systems 37 (2024): 10008-10035.  \n[2]. Hu, Yuke, et al. \"Quantifying and defending against privacy threats on federated knowledge graph embedding.\" Proceedings of the ACM Web Conference 2023. 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qH1rtsPtfG", "forum": "1tUjir7nub", "replyto": "1tUjir7nub", "signatures": ["ICLR.cc/2026/Conference/Submission12537/Reviewer_jtAW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12537/Reviewer_jtAW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760842382419, "cdate": 1760842382419, "tmdate": 1762923399141, "mdate": 1762923399141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce a novel one-shot federated leanring framework for knowledge graph embedding to balance performance gain with communication/computation cost. The proposed methd, named OFKGE, is evaluated on 5 datasets to show the effectivenss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Generally, this paper is easy to follow. \n\n2. The authors focuses on an important problem in federated learning, reducing comunication cost while mainting performance."}, "weaknesses": {"value": "1. The authors states that this is the first OFL framework tailored for knowledge graph embedding. However, to my knoweledge, some studies in this field have emerged. It would be better for the author to distinguish and compare the proposed method with existing ones. For example: \n\n[1] Personalized One-shot Federated Graph Learning for Heterogeneous Clients. \n\nIn addition, existing years have witnessed a great development of one-shot learning. Some of them are desgined for federated learning setting. It is beneficial to adapt them methods in KGE to fully validate the effectivenss of the proposed method. \n\n2. The paper lacks theoretical justification for why one-shot learning should work well for KGE. There is no convergence analysis or theoretical bounds on the performance gap between one-shot and multi-round approaches.\n\n3. All experiments use only 3 clients by default, which is quite limited for federated settings. Figure 3 shows results up to 7 clients, but real federated scenarios may involve hundreds or thousands of clients. \n\n4. The method requires either unlabeled data or synthetic data generation capability on the server, which may not always be available or practical.\n\n5. The teacher selection (Eq. 4) is greedy and based on relation-wise performance. What if different clients excel at different entity types rather than relations?"}, "questions": {"value": "1. How sensitive is OFKGE to the quality of synthetic data?\n\n2. How were multi-round baselines (FedE, FedLU, etc.) adapted to the one-shot setting? Did you simply run them for one round, or were there other modifications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HQC3ykv9Lu", "forum": "1tUjir7nub", "replyto": "1tUjir7nub", "signatures": ["ICLR.cc/2026/Conference/Submission12537/Reviewer_XiuL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12537/Reviewer_XiuL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806793047, "cdate": 1761806793047, "tmdate": 1762923398562, "mdate": 1762923398562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses critical limitations of existing Federated Knowledge Graph Embedding methods. It proposes One-Shot Federated Knowledge Graph Embedding, a framework built on the One-Shot Federated Learning paradigm, which streamlines training into three core stages:\n\ni) One-Shot Model Distillation: Each client independently trains a local Knowledge Graph Embedding model using its private knowledge graph and uploads the model parameters and relation-aware performance weights to the server only once, eliminating iterative communication.\n\nii) Server-Side Adaptive Knowledge Integration: The server leverages an Adaptive Semantic Alignment mechanism to dynamically select the most informative client model as a teacher for each batch of distillation data. It then performs knowledge distillation to fuse multi-source semantics into a generalized global embedding, which is distributed back to clients as auxiliary knowledge.\n\niii) Client-Side Local Fine-Tuning: Clients refine their local models using the global auxiliary knowledge and a regularization-based strategy, preserving personalized local KG structures while enhancing inference performance with global semantics."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "In innovation aspect, OFKGE is the first work to adapt OFL to FKGE, addressing the root cause of high communication costs in multi-round FKGE methods. This innovation fills a gap in applying OFL to knowledge graph tasks, where structural and semantic complexity differs from computer visio.\n\nIn experimental aspect, covers multimodal (MKG-W, MKG-Y) and standard (FB15K-237) KGs to test adaptability across data types. Includes ablation studies (validating ASA, aggregation initialization, and fine-tuning), cost analysis, hyperparameter sensitivity tests, and low-overlap scenario evaluations, providing rigorous evidence for OFKGE’s effectiveness.\n\nIn application aspect, OFKGE is well-suited for resource-constrained (low bandwidth) and privacy-sensitive scenarios (e.g., healthcare/finance KGs) and supports future extension to multimodal KGs, offering clear real-world value ."}, "weaknesses": {"value": "i) The baseline methods used for comparison are somewhat outdated and fail to cite recent approaches such as \"Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation.\"\n\nii) Line 466, the paper states, \"Despite this semantic gap and the presence of some factual errors, training with synthetic triples still enhances the distillation process.\" Intuitively, data containing grammatical and factual errors would negatively impact model distillation, particularly for embedding models. However, the paper presents a completely opposite viewpoint without providing explanations or references.\n\niii) The experimental analysis section is relatively weak, primarily emphasizing that the author's model achieved strong results in evaluations, but lacking in-depth analysis of the experimental findings and insightful conclusions that could guide the field forward."}, "questions": {"value": "As I mentioned in the weaknesses section:\n\nCould you add citations, comparisons, and analyses of some recent methods, such as the method \"Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation.\"?\n\nI have a question regarding “Despite this semantic gap and the presence of some factual errors, training with synthetic triples still enhances the distillation process”—could you provide a further explanation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FLE4eQC1ad", "forum": "1tUjir7nub", "replyto": "1tUjir7nub", "signatures": ["ICLR.cc/2026/Conference/Submission12537/Reviewer_jcKT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12537/Reviewer_jcKT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912300849, "cdate": 1761912300849, "tmdate": 1762923398029, "mdate": 1762923398029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces OFKGE, an efficient One-Shot Federated Knowledge Graph Embedding framework that tackles the high communication and privacy costs of traditional iterative methods. Clients train local knowledge graph models and upload them a single time; the server then intelligently distills a global knowledge representation using a novel Adaptive Semantic Alignment (ASA) mechanism that dynamically selects the best \"teacher\" model from the clients. This distilled global model is then sent back to clients, who fine-tune their local embeddings with it, balancing generalized knowledge with local personalization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's primary strength is its one-shot approach. By eliminating iterative communication rounds, it drastically reduces the communication overhead and shrinks the attack surface for privacy violations like gradient inversion attacks. This makes the framework highly practical for real-world scenarios with limited bandwidth or strict privacy requirements.\n- The server-side is a sophisticated alternative to naive model averaging. Instead of simply blending all client models, it dynamically selects the most informative client model (the \"best teacher\") for specific relations during distillation. This allows the server to build a more nuanced and accurate global model by drawing on the specialized strengths of different clients, effectively mitigating the negative impact of data heterogeneity.\n- The framework concludes with a client-side fine-tuning stage that uses a dedicated regularization term to balance two objectives: integrating the powerful distilled global knowledge and preserving the unique, personalized characteristics of the client's original local model."}, "weaknesses": {"value": "- The effectiveness of the server-side distillation, particularly the ASA \"best teacher\" selection, is highly dependent on the quality of the uploaded client models and their self-reported performance metrics. If a client's model is poorly trained, biased, or overfitted to its local data, it can be selected as a teacher and subsequently degrade the quality of the global model for all other clients. The framework assumes all clients provide high-quality, reliable models, which may not hold true in practice.\n- The one-shot nature of the framework is also its main weakness. Real-world knowledge graphs are dynamic and constantly evolving with new entities and relations. OFKGE provides no mechanism for clients to incorporate new knowledge or for the global model to be updated after the single communication round. This static \"snapshot\" approach makes it unsuitable for long-term deployments or real-time systems where continuous learning is essential."}, "questions": {"value": "- How could the OFKGE framework be extended to handle dynamic KGs where new entities, relations, or triples are added over time? Would this require periodic re-uploads from clients, a mechanism for incremental updates to the global model, or an entirely different approach that moves beyond the one-shot paradigm?\n- In a massively federated scenario with hundreds or thousands of clients, how do you expect the ASA mechanism to perform? Would the process of selecting a single \"best teacher\" per batch remain effective, or would a more complex aggregation of multiple top-performing teachers be necessary to capture the diverse knowledge present across a large client population?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bpPR1kugLs", "forum": "1tUjir7nub", "replyto": "1tUjir7nub", "signatures": ["ICLR.cc/2026/Conference/Submission12537/Reviewer_jYhd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12537/Reviewer_jYhd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986778024, "cdate": 1761986778024, "tmdate": 1762923397645, "mdate": 1762923397645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}