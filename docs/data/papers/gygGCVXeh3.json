{"id": "gygGCVXeh3", "number": 16618, "cdate": 1758266789136, "mdate": 1759897229208, "content": {"title": "Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments", "abstract": "Despite substantial progress in video understanding, most existing datasets are limited to Earth’s gravitational conditions. However, microgravity alters human motion, interactions, and visual semantics, revealing a critical gap for real-world vision systems. This presents a challenge for domain-robust video understanding in safety-critical space applications.\nTo address this, we introduce MicroG-4M, the first benchmark for spatio-temporal and semantic understanding of human activities in microgravity. Constructed from real-world space missions and cinematic simulations, the dataset includes 4,759 clips covering 50 actions, 1,238 context-rich captions, and over 7,000 question–answer pairs on astronaut activities and scene understanding. MicroG-4M aims to support three core tasks: fine-grained multi-label action recognition, temporal video captioning, and visual question answering, thereby enabling a comprehensive evaluation of both spatial localization and semantic reasoning in microgravity contexts. We establish baselines using state-of-the-art models. All data, annotations, and code will be made publicly available upon decision.", "tldr": "MicroG-4M is the first benchmark for human action and scene understanding in microgravity, offering clips, captions, and QA pairs to reveal domain gaps and guide robust vision-language models.", "keywords": ["Microgravity", "Action Recognition", "Vision-Language Understanding"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48c99ea5498c8686d0143d2cdefac74d04f8a117.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new benchmark on video understanding. \nThe benchmark address the novel scenario of astronautic environment in space.\nThe benchmark contain 5k clips of short videos (3 seconds) with rich annotations. \nThis paper then benchmark state-of-the-art models using their dataset, results suggest the evaluated models do not work as good as they are in normal earth-based videos."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses a novel scenario for video understanding. \n\nThe paper is well motivated. \n\nThe paper clearly documented the data collection pipeline.\n\nThe evaluation setting of baseline methods are also clearly written."}, "weaknesses": {"value": "The main issue with this paper is that its distinction from the Earth-Video benchmark is not clearly presented. \nThe reported performance indicates that the benchmark is difficult, but it is unclear whether any unique aspects of the astronautic videos contribute to this difficulty. \nCurrently, the presentation of experiment section 5 feels like “just another video benchmark”, as it does not provide much surprising findings. \n\n\nRegarding the baselines of human action recognition task, I’m curious whether the authors have tried VLMs like Gemini 2.5 Pro? I feel advanced VLMs like Gemini 2.5 Pro may be able to answer this. It would be nice to have them."}, "questions": {"value": "This paper is clearly written, thus I don’t have questions regarding clarity. \n\nI expect more qualitative results like Figure 1, 3 and 4. \n\nThe videos.zip in the supplementary does not work for me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C8F8gbGZEN", "forum": "gygGCVXeh3", "replyto": "gygGCVXeh3", "signatures": ["ICLR.cc/2026/Conference/Submission16618/Reviewer_Dmaz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16618/Reviewer_Dmaz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777225209, "cdate": 1761777225209, "tmdate": 1762926687847, "mdate": 1762926687847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MicroG-4M, the first benchmark dataset designed for spatio-temporal and semantic understanding of human activities in microgravity environments. It addresses a critical gap in current vision research, as most existing datasets for video captioning and action recognition are recorded on Earth under normal gravity, whereas microgravity significantly alters human motion, interactions, and visual semantics. Constructed from real-world space mission footage and cinematic simulations, MicroG-4M contains 4,759 3 second video clips at 30fps covering 50 actions, 1,238 captions, and over 7,000 question–answer pairs centered on astronaut activities and scene understanding. The dataset supports three core tasks: fine-grained multi-label action recognition, temporal video captioning, and visual question answering."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  The contribution of MicroG-4M is highly interesting and original, as it introduces the first large-scale benchmark for understanding human actions, captions, and question answering in microgravity environments. The dataset offers significant potential for advancing research in vision-language modeling, domain adaptation, and embodied AI under extreme physical conditions.\n\n2. The strength of this paper lies in the detailed presentation of dataset statistics and discussion with limitation. The authors provide a clear breakdown of the distribution across broad action types, the number of persons per clip, and fine-grained action frequencies, highlighting important patterns such as the dominance of single-person clips and the long-tail distribution of actions. Additionally, the per-class AP results convincingly demonstrate the value of MicroG-4M for microgravity-specific action recognition, while the high-density, VQA annotations further support rich semantic understanding."}, "weaknesses": {"value": "1. In the collection methodology section, the author’s writing style is precise, formal, and research-oriented, making it suited for submission in conference. However, it leans toward being dense and information-heavy, which could benefit from slight simplification or the inclusion of visual aids (such as tables or flow diagrams) to enhance clarity and readability.\n\n2. The author does not explicitly define the categories “Object Manipulation,” “Person Interaction,” and “Person Movement.” Instead, it appears that the reader is expected to infer their meaning from common sense or from the constituent fine-grained actions. It would improve clarity if the author briefly described each category with examples. For instance, \"Object Manipulation could be defined as actions where a person interacts with objects in the environment, including picking up, carrying, holding, pushing, pulling, operating equipment, or using tools. Examples include carry/hold object, push object, operate spaceship, or using a computer. In microgravity, these actions are particularly important because the dynamics of motion and object handling differ from those on Earth.\"\n\n3. While the authors state that all captions and QA annotations were created manually by annotators with “domain guidance,” they do not specify what type of guidance was provided or give concrete examples. It would be helpful to clarify whether this guidance included official space agency documents, astronaut manuals, mission reports, or expert review, and to briefly describe what information from these sources was used (e.g., spacecraft layout, standard operating procedures, or typical astronaut activities). Providing such details would improve transparency and help readers better assess the quality and reliability of the annotations."}, "questions": {"value": "1. For video captioning and VQA, the authors mention using large language models (LLMs) but do not describe the prompts or prompting strategy employed. Providing information about the prompts, including their format, instructions, or examples, would improve reproducibility and allow readers to better understand how LLMs contributed to annotation quality.\n\n2. It is unclear how video input is processed to generate captions. Do the authors first extract keyframes, or do they read video frames sequentially? If keyframes are used, the authors should specify the algorithm or criteria for keyframe selection. Additionally, the caption generation process using Visual-Language Models (VLMs) or Multimodal Large Language Model (MLLM) is not described in detail, clarifying whether captions are generated per frame, per keyframe, or for the entire clip would improve reproducibility.\n\n3. Employing VLMs may introduce computational overhead, especially for high-resolution frames or sequential multi-frame processing. For practical deployment, it would be helpful if the authors reported the hardware configuration (GPU type, CPU, RAM), memory usage, and average execution time per frame or video clip. This information would provide readers with a clearer understanding of the method’s efficiency and scalability.\n\n4. The authors state that each three-second clip contains six QA pairs; however, it is unclear how these questions were generated or selected. It would be helpful to clarify whether the questions were derived directly from the video content or generated based on the captions. The paper should also describe the question selection process, including examples of question types and how balance was maintained across reasoning categories such as temporal reasoning. The author could explain such as Six QA pairs covering different aspects: (1) Identity (“Who is the astronaut?”), (2) Action detail (“What does she do with her hands?”), (3) Body motion (“How do her head and shoulders move?”), (4) Spatial context (“What items can be seen behind?”), (5) Static background recognition (“What remains stationary?”), (6) Unanswerable / implicit reasoning. However, the authors do not indicate whether such systematic consideration guided their question design. A clearer explanation of the QA generation and selection methodology would greatly enhance reproducibility and transparency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4FJbLOmVQf", "forum": "gygGCVXeh3", "replyto": "gygGCVXeh3", "signatures": ["ICLR.cc/2026/Conference/Submission16618/Reviewer_fpxM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16618/Reviewer_fpxM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811552184, "cdate": 1761811552184, "tmdate": 1762926687463, "mdate": 1762926687463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces MicroG-4M, the first large-scale dataset specifically curated for human action recognition and vision-language understanding in microgravity environments. MicroG-4M includes 4,759 clips covering 50 actions, 1,238 context-rich captions, and over 7,000 question–answer pairs on astronaut activities and scene understanding. MicroG-4M aims to support three core tasks: fine-grained multi-label action recognition, temporal video captioning, and visual question answering, thereby enabling a comprehensive evaluation of both spatial localization and semantic reasoning in microgravity contexts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper is well-written and easy to understand.\n\n2.This work introduces MicroG-4M dataset, which is a valuable contribution to fill the gap of video understanding benchmarks under microgravity scenarios."}, "weaknesses": {"value": "1.One concern for this work is the technical contribution. From my point of view, the major contribution of this work comes from its data collection and organization, while the methodological contributions are missing, i.e., there is no specifically designed baselines for video understanding under microgravity scenarios nor novel insights from this work. This decreases the overall contributions of this work."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UJsD2feISv", "forum": "gygGCVXeh3", "replyto": "gygGCVXeh3", "signatures": ["ICLR.cc/2026/Conference/Submission16618/Reviewer_baMz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16618/Reviewer_baMz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999462274, "cdate": 1761999462274, "tmdate": 1762926687059, "mdate": 1762926687059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}