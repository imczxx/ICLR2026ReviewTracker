{"id": "RhQ1BzUppB", "number": 3896, "cdate": 1757562962487, "mdate": 1759898063938, "content": {"title": "FD-Bench: A Modular and Fair Benchmark for Data-driven Fluid Simulation", "abstract": "Data-driven modeling of fluid dynamics has advanced rapidly with neural PDE solvers, yet a fair and strong benchmark remains fragmented due to the absence of unified PDE datasets and standardized evaluation protocols. Although architectural innovations are abundant, fair assessment is further impeded by the lack of a clear disentanglement between spatial, temporal and loss modules.\nIn this paper, we introduce FD-Bench, the first fair, modular, comprehensive and reproducible benchmark for data-driven fluid simulation. \nFD-Bench systematically reviews and decomposes 89 baseline models reported across recent publications, extracting and standardizing their key architectural and training components for fair, unified comparison across 10 representative flow scenarios.\nIt provides four key contributions: (1) a modular design enabling fair comparisons across spatial, temporal, and loss function modules; (2) the first systematic framework for direct comparison with traditional numerical solvers; (3) fine-grained generalization analysis across resolutions, initial conditions, and prediction time window]; \nand (4) a user-friendly, extensible codebase to support future research. Through rigorous empirical studies, FD-Bench establishes the most comprehensive leaderboard to date, resolving long-standing issues in reproducibility and comparability, and laying a foundation for robust evaluation of future data-driven fluid models. The code is open-sourced at https://anonymous.4open.science/r/FD-Bench-15BC.", "tldr": "", "keywords": ["AI for PDE"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d34078f964379449afc5bf0358883f5d9cfad7fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces FD-Bench, a benchmark for data-driven fluid simulation aimed at addressing the lack of standardized evaluation in the field. The authors propose a modular decomposition of neural PDE solvers into spatial, temporal, and loss components, and evaluate 89 baseline models across 10 fluid flow scenarios. They also include comparisons with traditional numerical solvers and analyze generalization across resolutions, initial conditions, and rollout horizons."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The modular design is a step toward disentangling the contributions of different components in neural PDE solvers.\n\n- The effort to unify and standardize evaluation across a large number of models is commendable.\n\n- The inclusion of traditional numerical solvers as baselines is a valuable addition.\n\n- The codebase is publicly released, which may facilitate future research."}, "weaknesses": {"value": "- Lack of Theoretical or Algorithmic Novelty: The paper presents a benchmarking effort rather than a methodological advance. While useful, it does not introduce new models, theoretical insights, or algorithmic improvements. The decomposition into spatial/temporal/loss modules is intuitive but not novel, and the paper does not justify why this particular decomposition is the most meaningful or complete.\n\n- Superficial Model Decomposition: The decomposition of 89 models into modular components is largely based on a post-hoc categorization rather than a unified theoretical framework. This risks oversimplifying architectural differences and may misrepresent the original contributions of the models being compared. For instance, grouping models under “self-attention” or “Fourier” ignores important nuances in their design and implementation.\n\n- Limited Justification for Dataset Selection: While the authors curate 10 flow scenarios, the rationale for their selection is not deeply motivated from a fluid dynamics perspective. The paper does not adequately address whether these scenarios are representative of real-world challenges or if they cover the full spectrum of fluid behaviors (e.g., multi-phase flows, high Mach number regimes, complex geometries).\n\n- Evaluation Metrics Lack Physical Interpretability: The reliance on RMSE and nRMSE as primary metrics is insufficient for fluid dynamics, where quantities like vorticity preservation, energy spectra, or divergence errors are often more informative. The paper does not justify why these standard fluid diagnostics are omitted, limiting the utility of the benchmark for the CFD community.\n\n- Insufficient Discussion of Limitations in Neural vs. Traditional Solvers: The comparison with traditional solvers focuses on error and runtime but overlooks critical aspects such as stability, convergence guarantees, and physical consistency. Neural solvers are known to suffer from error accumulation and lack of long-term stability—issues that are not sufficiently addressed in the experiments or discussion.\n\n- Generalization Claims Are Overstated: The generalization studies (e.g., zero-shot initial conditions, resolution shifts) are limited to narrow variations and do not convincingly demonstrate robustness to realistic distribution shifts. The paper does not test on truly out-of-distribution scenarios such as different geometries, boundary conditions, or physical parameters outside the training range.\n\n- Reproducibility and Scalability Concerns: Although the code is released, the work does not provide sufficient details on computational budgets, hyperparameter tuning procedures, or the feasibility of reproducing all 89 models. The scalability of the benchmark to 3D or more complex PDE systems is also not demonstrated or discussed."}, "questions": {"value": "- How does your modular decomposition account for models that inherently couple spatial and temporal processing (e.g., neural ODEs or recurrent architectures)?\n\n- Why were more physically meaningful metrics (e.g., enstrophy, divergence, spectral decay) not included in the evaluation?\n\n- Can you provide evidence that the selected flow scenarios are sufficiently diverse to draw general conclusions about model performance?\n\n- What steps were taken to ensure that the hyperparameter tuning for each model was fair and computationally feasible across 89 baselines?\n\nShould you be able to satisfactorily address the points I've raised above, I will accordingly provide a positive rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6fSWzlOu8a", "forum": "RhQ1BzUppB", "replyto": "RhQ1BzUppB", "signatures": ["ICLR.cc/2026/Conference/Submission3896/Reviewer_HKxd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3896/Reviewer_HKxd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482623375, "cdate": 1761482623375, "tmdate": 1762917087783, "mdate": 1762917087783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FD-Bench, a modular benchmark for data-driven fluid simulation that standardizes datasets, model components, and evaluation protocols. The core contribution is to decouple neural PDE solvers into three orthogonal modules (spatial representation, temporal representation, and loss) and to compare 89 baselines across 10 flow scenarios under a unified codebase and tuning budget. The benchmark also includes head-to-head comparisons with traditional numerical solvers, generalization tests (OOD initial conditions, resolution shifts, and rollout length), and discretization studies (Eulerian vs. mesh vs. particle). Empirically, the paper reports that self-attention/Transformers tend to yield the best accuracy, while Fourier-based operators are competitive and computationally efficient; temporal bundling is consistently strong among evolution strategies. The authors release an extensible codebase and claim improved fairness and reproducibility of comparisons for data-driven fluid simulation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The modularization of neural PDE solvers into spatial/temporal/loss components, along with benchmarking controlled cross-comparisons of these choices, is a useful organizing principle that helps attribute where gains come from, rather than treating methods as monoliths. The inclusion of direct comparisons to classical solvers at coarser grids and lower-order schemes (with matched error targets) is a thoughtful step toward practicality.\n\nThe benchmark scope is broad (reported 89 baselines, 10 scenarios) with standardized tuning and clear metrics (RMSE/nRMSE, fRMSE bands, memory, GFLOPs). The paper transparently documents setups and seeds, and provides a public, modular codebase for reproduction.\n\nThe paper is well structured and clear to read. I enjoyed reading this paper!\n\nA fair, reproducible reference point for data-driven CFD is valuable. The finding that Transformers dominate on accuracy while Fourier operators offer excellent cost-accuracy trade-offs, and that temporal bundling is a robust evolution strategy, can rationalize design choices in subsequent work. The numerical-solver comparisons and run-time/speed-up evidence increase the benchmark’s practical relevance."}, "weaknesses": {"value": "*Novelty of questions vs. synthesis*: The three headline questions (which neural architecture; can neural replace numerical; which discretization; how well do models generalize) are useful but not conceptually new; several prior surveys/benchmarks have articulated similar axes. FD-Bench’s novelty is primarily scope and modular rigor, not new task formulations. (This aligns with the authors’ own positioning against prior benchmarks in Table 1.)\n\n*Insights largely confirm established intuitions*: The empirical takeaways (Transformers (self-attention) excel in accuracy, Fourier operators in efficiency/long-range mixing) are directionally consistent with recent literature and community experience (e.g., work by Mishra group at ETH, reviews from Brown, etc); FD-Bench consolidates rather than surprises here.\n\n*Geometry and boundary-condition coverage feel limited for real-world deployment*: Despite listing heterogeneity and even “irregular geometries” among selection criteria, much of the benchmark centers on canonical, mostly rectangular/periodic domains (e.g., incompressible/Compressible N-S on uniform grids, Burgers, advection, Kolmogorov flow). The Lagrangian subsets (Taylor–Green, lid-driven cavity, reverse Poiseuille) add variety, but the dominant training/evaluation remains on boxes with simple/periodic BCs, and several Lagrangian cases are aggregated back to regular grids for evaluation. This limits conclusions about performance under complex geometries (curved walls, obstacles, multi-body systems), mixed/moving boundaries, and non-Cartesian meshes, which are typical in engineering applications.\n\n*Boundary-condition diversity*: The protocol does not yet stress diverse BC types (Dirichlet/Neumann/Robin inflow–outflow mixes, no-slip vs. partial slip, pressure outlets), CFL-critical transient regimes near complex boundaries, or wall models and source terms relevant to real devices. This weakens claims about readiness for practical deployment, even if trends are robust on boxes.\n\n*Limited 3D and multiphysics breadth*: The benchmark mix skews toward 2D and single-physics PDEs; many industrial use cases hinge on 3D, moving interfaces, or coupled physics (e.g., conjugate heat transfer, reacting flows). As a result, the reported rankings may not carry over."}, "questions": {"value": "I suggest these questions for consideration (with the understanding that many will be difficult to execute in the short response time, but with the hope that this will help the authors strengthen this good paper for the community)\n\n*Geometry and BC realism*: Could the authors extend FD-Bench with non-rectilinear geometries (curved ducts, bluff-body wakes) and mesh-based Eulerian data (unstructured tri/tet, boundary-layer refinement) to stress spatial inductive biases outside regular grids? Can you introduce tasks with mixed boundary conditions (e.g., no-slip on walls, specified pressure at the outlet, and inflow profiles at the inlet) and report the per-boundary condition breakdown of errors? This would test if attention/Fourier advantages persist with wall physics.\n\n*Generalization beyond periodic boxes*:  Several scenarios appear periodic or effectively box-confined. How do models fare under obstacle insertion or domain deformation (e.g., adding a cylinder/airfoil) without finetuning? \n\n*3D scaling/memory*: The efficiency section reports GFLOPs/memory on 2D. Do the memory/runtime advantages for Fourier vs. attention hold in 3D at comparable accuracy (e.g., (128^3), (256^3))? A 3D micro-suite could meaningfully extend the benchmark.\n\n*Rollout step size & stability claims*: The paper argues neural solvers tolerate larger (\\Delta t) with minor degradation compared to classical solvers constrained by CFL. Could you report absolute wall-clock vs. effective physical time advanced per step curves, plus failure cases where large (\\Delta t) breaks stability/accuracy? This would help practitioners set safe schedules.\n\n*Task difficulty calibration*: How sensitive are rankings to turbulence intensity (e.g., higher Reynolds for cavity/Poiseuille), shock strength in compressible cases, or stiff reaction rates? A difficulty-graded version of each task could probe robustness of the reported ordering."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cTuhZp9lxs", "forum": "RhQ1BzUppB", "replyto": "RhQ1BzUppB", "signatures": ["ICLR.cc/2026/Conference/Submission3896/Reviewer_8HCY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3896/Reviewer_8HCY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899893448, "cdate": 1761899893448, "tmdate": 1762917087574, "mdate": 1762917087574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The current field of data-driven fluid simulation lacks unified datasets and standardized evaluation protocols, and the highly coupled design of spatial, temporal, and loss modules makes fair comparisons between methods difficult. This paper proposes FD-Bench, which establishes a comprehensive leaderboard, addressing long-standing issues of repeatability and comparability, and laying the foundation for robust evaluation of future data-driven fluid models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This paper accurately identifies the core pain point of \"fragmented evaluation\" in the current field of neural PDE solvers, and the proposal of FD-Bench has clear practical significance.\n2.This paper unifies and abstracts various existing methods into a quadruple of \"spatial encoding + temporal encoding + loss function + tricks\", and designs controlled variable experiments based on this to achieve \"structure-performance\" decoupling.\n3.This paper provides an easy-to-use and reproducible codebase that addresses the long-standing lack of standardized evaluation protocols and helps to facilitate fair and reproducible evaluations in the future."}, "weaknesses": {"value": "1.This paper emphasizes comparisons across 89 baselines. However, Table 3 appears to extract components from existing methods, categorize them, and then compare the methods after reorganizing the components, which doesn't seem entirely consistent with the description.\n2.FD-Bench’s primary metrics are RMSE, nRMSE, and frequency‑domain RMSE. These capture “fit” but do not directly assess physical consistency."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OVC3bWoVA4", "forum": "RhQ1BzUppB", "replyto": "RhQ1BzUppB", "signatures": ["ICLR.cc/2026/Conference/Submission3896/Reviewer_7tPR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3896/Reviewer_7tPR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065713960, "cdate": 1762065713960, "tmdate": 1762917087347, "mdate": 1762917087347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}