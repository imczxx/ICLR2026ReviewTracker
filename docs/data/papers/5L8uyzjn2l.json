{"id": "5L8uyzjn2l", "number": 14950, "cdate": 1758245995825, "mdate": 1759897339644, "content": {"title": "AgentPO: Enhancing Multi-Agent Collaboration via Reinforcement Learning", "abstract": "Multi-Agent Systems (MAS) offer a powerful paradigm for solving complex problems through distributed reasoning and collaboration. However, their effectiveness is often hindered by the challenge of optimizing interactions among agents. To address this, we introduce AgentPO, a novel framework that directly optimizes agent collaboration. AgentPO employs reinforcement learning to train a specialized Collaborator agent, which refines its interaction policy to enhance overall system performance within a fixed multi-agent topology. We evaluated AgentPO on multiple mathematical reasoning tasks, where it consistently outperformed strong baselines. With Llama-3.2-3B-Instruct as the actor model, AgentPO achieves accuracy improvements of +1.8\\% and +7.2\\% over strong baselines like Role Assignment and EvoAgent, respectively. When using the larger Llama-3.1-8B-Instruct model, these gains increase to +5.6\\% and +11.3\\%. Crucially, AgentPO achieves these results with remarkable efficiency: it requires only 500 training samples and operates at just 7.8\\% of EvoAgent's inference cost, highlighting its superior scalability and practicality.", "tldr": "We propose AgentPO, a reinforcement learning framework that trains a Collaborator agent to optimize multi-agent cooperation.", "keywords": ["Multi-Agent Systems", "Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c10401f695b464c72e178d5b7550ab3f3a926a0c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes AgentPO, a framework that trains a lightweight Collaborator agent via GRPO to assist a frozen Actor model. Two collaboration topologies are explored: Hint-Actor, where the Collaborator provides guiding hints upfront, and Actor-Critic, where the Collaborator critiques initial outputs for refinement. Experiments on mathematical reasoning benchmarks demonstrate improvements over baselines such as Self-Refine and Multi-Agent Debate, with reduced training data requirements and lower inference costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive empirical evaluation: Multiple benchmarks with diverse baselines and thorough ablations on data efficiency, model selection, and computational cost provide convincing evidence of practical effectiveness.\n2. Strong efficiency gains: Demonstrates substantial inference cost reduction while achieving superior accuracy with minimal training samples compared to direct Actor fine-tuning—valuable findings for resource-constrained scenarios.\n3. Hybrid system innovation: Successfully demonstrates that locally-trained Collaborators can enhance black-box API models, addressing real deployment constraints where model parameters are inaccessible."}, "weaknesses": {"value": "1. Insufficient novelty and missing related work: The core ideas are not new. The Actor-Critic topology closely resembles Reflexion's generate-critique-refine loop, with the main difference being use of a separate model rather than self-reflection. Shepherd trained critics for textual feedback; RLAIF used RL to optimize AI judges; OPRO optimized prompts via LLMs. The paper fails to cite Reflexion or adequately differentiate from these closely related works, overstating the novelty of the proposed framework.\n\n2. Misleading multi-agent framing: Despite emphasizing \"Multi-Agent Collaboration,\" all experiments use exactly two agents with no exploration of scaling to larger systems or complex interaction patterns. This is essentially a helper model setup rather than a genuine multi-agent system.\n\n3. Limited scope and analysis: Evaluation is restricted to mathematical reasoning without evidence of generalization to other domains. The paper lacks qualitative analysis of what collaboration strategies the Collaborator learns—only quantitative metrics are provided without interpretable examples of learned patterns."}, "questions": {"value": "1. Clarification on core contribution: I would appreciate if the authors could articulate more clearly what the central methodological contribution is beyond training a smaller Collaborator model. While both Hint-Actor and Actor-Critic represent reasonable interaction patterns, they appear to be fairly standard approaches (upfront guidance and iterative refinement). Could you help me understand what novel design principles or insights AgentPO contributes to multi-agent system design?\n\n2. Missing comparison with trained reflection models: The paper positions the Actor-Critic topology as a novel contribution, but recent work in the Reflexion family has also explored training separate smaller models for reflection and critique. Could you clarify how your approach differs from these existing methods and provide direct experimental comparisons? \n\n3. Justification for fixed Actor assumption: A central design choice is keeping the Actor frozen while training the Collaborator. However, Table 2 shows that direct Actor optimization baselines (e.g., Prime-Zero-7B at 48.0%) are competitive with AgentPO (49.4%). Could you provide more analysis on when collaborative optimization is preferable to direct Actor fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cwnyy1Jziq", "forum": "5L8uyzjn2l", "replyto": "5L8uyzjn2l", "signatures": ["ICLR.cc/2026/Conference/Submission14950/Reviewer_B4gj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14950/Reviewer_B4gj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879602999, "cdate": 1761879602999, "tmdate": 1762925287624, "mdate": 1762925287624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AgentPO, a two-model collaboration scheme where a small Collaborator LLM produces a hint or critique to guide a stronger, frozen Actor LLM. The collaborator alone is optimized with a PPO-style objective (GRPO) using solution-level rewards (e.g., correctness), while the actor remains fixed. Two topologies are evaluated: feed-forward Hint-Actor and feedback Actor-Critic. Experiments focus on math-reasoning benchmarks with Pass@1, comparing against prompting and multi-agent baselines and reporting token-cost trade-offs and limited ablations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear modular setup: freeze the solver; optimize only a lightweight helper policy.\n\n2. Practical objective: GRPO removes the value critic and uses group baselines, simplifying training.\n\n3. Cost-aware evaluation with token usage plots highlights a plausible efficiency advantage over multi-sample prompting/debate.\n\n4. Method slots into common LLM stacks (works with open-source LLMs; actor can be API-only)."}, "weaknesses": {"value": "1. Statistical rigor is thin. Many benchmark slices (e.g., AIME) are small. Improvements like +1.8 pts could be noise. There are no confidence intervals, no multi-seed runs, no significance tests. Even with temperature 0, variance should be quantified.\n\n2. Several baselines are re-implemented with details pushed to the appendix. Without exhaustive hyperparameter parity (sampling strategy, steps, stopping, prompt templates), the risk of unknowingly weakened baselines is high.\n\n3. Cost–accuracy plots are welcome, but you also need equal-budget comparisons (fixed tokens or wall-clock) per dataset. Otherwise claims of \"superior scalability\" are too sweeping.\n\n4. Terminology ambiguity. Using \"Actor\" and especially \"Critic\" is easily conflated with actor–critic (where \"critic\" means a value function). Here the \"critic\" is a text generator providing feedback, not a value model. This should be made explicit and a notation table is needed.\n\n5. Limited external validity. Evaluations are math-only with verifiable end rewards. It's unclear how the approach fares with noisy/non-binary rewards (e.g., tool use, code exec flakiness, factual QA)."}, "questions": {"value": "1. Statistical rigor is thin. Many benchmark slices (e.g., AIME) are small. Improvements like +1.8 pts could be noise. There are no confidence intervals, no multi-seed runs, no significance tests. Even with temperature 0, variance should be quantified.\n\n2. Several baselines are re-implemented with details pushed to the appendix. Without exhaustive hyperparameter parity (sampling strategy, steps, stopping, prompt templates), the risk of unknowingly weakened baselines is high.\n\n3. Cost–accuracy plots are welcome, but you also need equal-budget comparisons (fixed tokens or wall-clock) per dataset. Otherwise claims of \"superior scalability\" are too sweeping.\n\n4. Using \"Actor\" and especially \"Critic\" is easily conflated with actor–critic (where \"critic\" means a value function). Here the \"critic\" is a text generator providing feedback, not a value model. This should be made explicit and a notation table is needed.\n\n5. External validity is limited. Evaluations are math-only with verifiable end rewards. It's unclear how the approach fares with noisy/non-binary rewards (e.g., tool use, code exec flakiness, factual QA)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s9CXwgwgbS", "forum": "5L8uyzjn2l", "replyto": "5L8uyzjn2l", "signatures": ["ICLR.cc/2026/Conference/Submission14950/Reviewer_z8GS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14950/Reviewer_z8GS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897525864, "cdate": 1761897525864, "tmdate": 1762925287198, "mdate": 1762925287198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AgentPO introduces a RL framework to directly optimize agent collaboration for a fixed topology multi-agent system. The Collaborator Agent's objective is to optimize the joint performance of a group of agents using a frozen high-performance policies (decoupling collaboration and execution). The authors use GRPO to train the Collaborator Agent to provide key contexts to LLMs to solve math based reasoning tasks. The authors show that AgentPO's performance scales with Actor model performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Collaborator agent optimization shows improved result, using context distillation and joint reasoning. The agent is trained using GRPO to accommodate multiple goals.\n2. AgentPO's short training horizon at 500 samples highlights the efficacy and low overhead of the framework.\n3. Empirical evaluations, sample efficiency and inference costs are tested showcasing the cost effectiveness and viability of the framework for rapid prototyping."}, "weaknesses": {"value": "1. The framework focuses on fixed topology systems, however, further discussions into how AgentPO would perform on less structured or open-ended collaborative tasks would be beneficial."}, "questions": {"value": "It seems the Collaborator agent holds the contextual memory and history of the agents, the aggregation constructs a ToM-like model that distills domain-specific knowledge and reasoning to the LLM policies. An interpretability analysis/discussion with the Collaborator agent's contexts over the policies actions would be beneficial to show the efficacy of the agent."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1SReVlPBUD", "forum": "5L8uyzjn2l", "replyto": "5L8uyzjn2l", "signatures": ["ICLR.cc/2026/Conference/Submission14950/Reviewer_g4rv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14950/Reviewer_g4rv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016427213, "cdate": 1762016427213, "tmdate": 1762925286817, "mdate": 1762925286817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AgentPO, a framework that optimizes multi-agent system collaboration through reinforcement learning. Rather than searching for optimal agent topologies, AgentPO trains a lightweight \"Collaborator\" agent to improve interactions with a fixed, powerful \"Actor\" agent within predefined topologies. The approach uses GRPO (Group Relative Policy Optimization) to train the Collaborator on mathematical reasoning tasks, demonstrating improvements over baselines while requiring minimal training data and inference cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Well-motivated problem formulation**: The shift from topology search to collaborative optimization is compelling. The distinction between architectural search and interaction optimization effectively reframes the research question.\n\n2. **Clear presentation**: The paper is well-written and easy to follow, with clear diagrams (Figure 1) and illustrative examples (Figure 2).\n\n3. **Practical and efficient**: Training only a small Collaborator model (3B parameters) instead of fine-tuning large Actor models is resource-efficient. The data efficiency (strong performance with only 500 training samples) seems impressive."}, "weaknesses": {"value": "**1. Limited Scope and Generalization Concerns:**\n\n- **Limited topology coverage**: The paper only examines two specific two-agent topologies (Hint-Actor and Actor-Critic). How would AgentPO extend to other two-agent configurations or multi-agent systems with three or more agents? The framework's applicability to more complex collaboration structures remains unclear.\n\n- **Collaborator transferability**: The paper does not investigate whether a Collaborator trained with Actor A can effectively assist Actor B. This is fundamental for practical deployment: If Collaborators are actor-specific, then every new/upgraded Actor requires retraining the Collaborator, undermining the claimed scalability and efficiency advantages\n\n- **Domain-specific evaluation**: All experiments focus exclusively on mathematical reasoning tasks. No evidence is provided for generalization to other domains such as code generation, creative writing, question answering, or multi-modal tasks.\n\n**2. Unclear and Inconsistent Experimental Setup:**\n\n- The experimental configuration in Section 3.1 (line 210) lists model choices but does not clearly specify which models serve as Actors versus Collaborators. \n- Subsequent sections show inconsistent model assignments without adequate justification:\n    - Main results (Table 1) use \"Qwen2.5-3B-Instruct\" as the Hint model with Llama as Actor, testing only Hint-Actor topology\n    - Actor optimization comparison (Table 2) switches to \"Qwen2.5-Math-7B\" as the Actor model\n    - Topology comparison (Table 4) uses \"Qwen2.5-Plus\" as Actor\n- Why do the base models keep changing across experiments? What is the rationale for these specific model selections? This makes it difficult to assess whether performance differences stem from the method or from model choices.\n\n**3. Insufficient Analysis of Performance Drops:**\n\nTable 1 shows AgentPO sometimes underperforms baselines (e.g., AIME24: 6.7% vs. 10.0% for several baselines with Llama-3.2-3B), but these failure cases are not analyzed. When and why does AgentPO fail? What characteristics of problems or model combinations lead to degraded performance?\n\n**4. Overfitting Concerns Underexplored:**\n\nFigure 4 shows performance drops at 700-1000 training samples, suggesting potential overfitting. However, this is only briefly mentioned without thorough investigation. What regularization strategies or early stopping criteria should be used? This is particularly concerning for low-resource deployment scenarios.\n\n**5. Topology Choice vs. Optimization:**\n\nTable 4 reveals a critical issue: Critic-Actor performs well even without AgentPO optimization (57.9% vs. 56.6% baseline), while Hint-Actor degrades performance before optimization (55.1% vs. 56.6%). This suggests topology selection may matter more than the optimization itself, contradicting the paper's core claim that optimizing collaboration within a fixed topology is the key to success.\n\n**6. Limited Scaling Analysis:**\n\nThe paper demonstrates AgentPO scales with Actor capability (Llama-3.2-3B → 3.1-8B), but does not explore:\n- What happens with even larger Actor models?\n- How does performance change when the capability gap between Collaborator and Actor grows very large?\n- Are there diminishing returns or failure modes at extreme scales?"}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pFkTK7KsXZ", "forum": "5L8uyzjn2l", "replyto": "5L8uyzjn2l", "signatures": ["ICLR.cc/2026/Conference/Submission14950/Reviewer_P6Hn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14950/Reviewer_P6Hn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762387409910, "cdate": 1762387409910, "tmdate": 1762925286312, "mdate": 1762925286312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}