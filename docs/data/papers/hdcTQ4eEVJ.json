{"id": "hdcTQ4eEVJ", "number": 6388, "cdate": 1757977644034, "mdate": 1763163273891, "content": {"title": "Multi-Objective Markov Games: Theoretic Foundations and Learning Algorithms", "abstract": "In practical multi-agent systems, agents often have diverse objectives, which makes the system more complex, as each agent's performance across multiple criteria depends on the joint actions of all agents, creating intricate strategic trade-offs. To address this, we introduce the  Multi-Objective Markov Game (MOMG), a framework for multi-agent reinforcement learning with multiple objectives. We propose the Pareto-Nash Equilibrium (PNE) as the primary solution concept, where no agent can unilaterally improve one objective without sacrificing performance on another. We prove existence of PNE, and establish an equivalence between the PNE and the set of Nash Equilibria of MOMG's corresponding linearly scalarized games, enabling solutions of MOMG by transferring to a standard single-objective Markov game. However, we note that computing a PNE is theoretically and computationally challenging, thus we propose and study weaker but more tractable solution concepts. Building on these foundations, we develop online learning algorithm that identify a single solution to MOMGs. Furthermore, we propose a novel two-phase, preference-free algorithm that decouples exploration from planning. Our algorithm enables computation of a PNE for any given preference profile without collecting new samples, providing an efficient methodological characterization of the entire Pareto-Nash front.", "tldr": "", "keywords": ["Multi-objective learning", "Markov games", "reinforcement leaning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1fef0a99c7a7990ea35b78fe34d357d75116b66.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies PNE and WPNE in $n$-player multi-objective Markov games. It first establishes the existential proof of PNE and WPNE; and discusses their connections with Nash equilibrium in standard Markov games. Further, the paper proposes algorithms that learn $\\epsilon$-WPNE and $\\epsilon$-WPCE in pseudo-polynomial time. However, I believe there are several critical issues with this submission."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper establishes connections between PNE and WPNE in multi-objective Markov games and provides proof for their existence.\n\n- The paper proposes learning algorithm that learns an $\\epsilon$-PCE in time polynomial to $\\epsilon, S, A, H$."}, "weaknesses": {"value": "- The critical issue of this paper lies in claiming to provide an algorithm that learns an $\\epsilon$-WPNE in multi-objective general-sum Markov games in pseudo-polynomial time, which contradicts the Exponential Time Hypothesis (ETH) of PPAD [1]. Specifically, the algorithm can be viewed as a no-regret type algorithm, which is known to only output a coarse correlated equilibrium in $n$-player games. However,  the author appear to treat the output distribution (average of past joint strategies) to be a product policy and thus argue it constitutes a Nash equilibrium. \n\n- Regarding the existential proofs of PNE and WPNE, I found the argument to be similar to those given in [2], which essentially follows from [3]. For example, Theorem 1,2 and 3 in this paper can be viewed as the extensions of Theorem 2.2 in [2] to the Markov games settings. I believe the authors should properly cite those works and highlight the distinctions and contributions beyond existing results.\n\n- No numerical results are provided.\n\n----\n\n[1] Aviad Rubinstein. 2016. Settling the complexity of computing approximate two player Nash equilibria. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS). IEEE, 258–265.\n\n[2] Shaojian Qu, Ying Ji, and Mark Goh. The robust weighted multi-objective game. PloS one, 10(9):e0138970, 2015.\n\n[3] S. Y. Wang Existence of a pareto equilibrium. J Optim Theory Appl 79, 373–384 (1993)."}, "questions": {"value": "See Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ElY2lxtWeJ", "forum": "hdcTQ4eEVJ", "replyto": "hdcTQ4eEVJ", "signatures": ["ICLR.cc/2026/Conference/Submission6388/Reviewer_ucqT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6388/Reviewer_ucqT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702836227, "cdate": 1761702836227, "tmdate": 1762918673599, "mdate": 1762918673599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced Multi-Objective Markov games (MOMGs). Pareto Nash equilibria (PNE) are introduced as the main solution concept and it is shown that they always exist in finite MOMGs similarly to Nash equilibria in Markov games. Then it is shown that the set of PNE coincides with the set of NE of linearly scalarized games. Using this characterization, the paper proposes online learning algorithms with regret guarantees to establish convergence to approximate weak PNE and Pareto Correlated Equilibria (PCE). The paper concludes with a two-phase approach for full Pareto front characterization with an exploration phase followed by a PNE computation phase for any given preference profile without recollecting new samples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and easy to follow, the paper is nicely motivated in the introduction. \n2. Multi-objective multi-agent RL is relatively less explored in the literature, the formulation in this paper with $N$ players appears to be new, to the best of my knowledge although Vector-valued Markov Games have been introduced in Yu et al. 2021 as briefly mentioned in l. 162. I think the study of multi-objective multi-agent RL is an interesting topic given that multiplicity of objectives is common in applications. \n3. The paper generalizes Markov games and multi-objective RL and multi-objective normal-form games to multi-objective Markov games."}, "weaknesses": {"value": "**1. Technical novelty:** Essentially, after the reduction to classical Markov games by scalarization, existing results for Markov games can be applied. \n\n--  For instance, the proof of theorem 8 (existence of PNE)  follows from existence of Nash equilibria in Markov games, note that Fink 1964 should be cited in the proof instead of Shapley 1953 who only considered 2-player zero-sum Markov games.  See also questions below regarding technical novelty. \n\n**2. Computation.** Computational complexity is not discussed, hardness of Nash equilibrium computation follows in general but it would be useful to comment more on this beyond statistical efficiency.\n\n**3. Equilibrium concepts.** The paper introduces several other equilibrium concepts inspired from the normal form setting in section 3.3, but they are then abandoned. I think the paper could better motivate why these alternative solution concepts could be interesting if there is any positive reason to consider them."}, "questions": {"value": "1. Can you comment more on the technical novelty compared to existing results in static multi-objective normal-form games? In particular, what are the challenges faced when proving Theorems 1, 2, 3 due to the Markov setting? It seems that the dynamic programming structure is not needed for the proofs (beyond the fact of existence of Nash equilibria in Markov games). \n\n2. Same question for the comparison to Markov games and multi-objective RL. It seems that under scalarization, after the reduction to Markov games, one can just apply existing results. The paper mentions a central challenge in l. 337-338, around random reward vectors. I am not sure how substantial is this challenge.  Why can’t you directly use existing results for Markov games and need to reserve concentration inequalities to establish regret bounds? \n\n3. The developments in sections 4.1, 4.2 rely on the results of Liu et al. al 2021, and Jin et al. 2021. Can you justify the choice of optimistic Nash value iteration over other alternatives? Are the obtained regret bounds (for Markov games) optimal or state of the art? \n\n4. Can you discuss computational efficiency of the proposed algorithms? They seem to require some Nash equilibrium computation (in Algorithm 1). \n\n5. Can you for instance consider multi-agent policy gradient methods similarly to their use in Markov games and single-agent multi-objective RL (Agarwal et al. 2022)? Perhaps only for special classes of Markov games with additional structure (potential, zero-sum …) to show convergence to approximate PNE with convergence rates and sample complexity guarantees?  \n\n6. What is $s_1$ in lines 124 (definition of Pareto optimal policy) and l. 144 (definition of NE). Is it any state so that it is a Markov Perfect equilibrium of just a fixed initial state (i.e. a Dirac initial state distribution)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WnjYGxMxUS", "forum": "hdcTQ4eEVJ", "replyto": "hdcTQ4eEVJ", "signatures": ["ICLR.cc/2026/Conference/Submission6388/Reviewer_qVid"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6388/Reviewer_qVid"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924429186, "cdate": 1761924429186, "tmdate": 1762918672950, "mdate": 1762918672950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Multi-Objective Markov Game (MOMG) framework to extend standard Markov games to settings where each agent optimizes multiple (possibly conflicting) objectives. The authors propose the Pareto-Nash Equilibrium (PNE) as the core solution concept, analyze its existence, and show its equivalence to the union of Nash equilibria of linearly scalarized games. To address computational challenges, they define weaker notions—Weak Pareto-Nash Equilibrium (WPNE) and Pareto Correlated Equilibrium (PCE)—and develop algorithms to approximate these equilibria. They further propose a two-phase “preference-free” approach to characterize the full Pareto-Nash front without re-sampling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The topic is timely and relevant, bridging multi-objective reinforcement learning and multi-agent game theory. The PNE–scalarization equivalence theorem is a clean theoretical result that could help connect multi-objective analysis with classical single-objective Markov game solvers. The paper is well-written and in general easy to follow."}, "weaknesses": {"value": "My major concern is with regard to the technical contribution and novelty. The ONVI-MG and MO-V-Learning algorithms effectively reduce the multi-objective problem to a scalarized Markov game with a fixed preference distribution, and thus inherit results from existing Nash learning algorithms rather than introducing genuinely new algorithmic mechanisms. This makes the contribution feel incremental—mainly a rephrasing of scalarized MARL with multi-objective terminology.\n\nAlso the results are purely theoretical and algorithmic, with no experimental validation or discussion of applicability to real multi-objective domains.\n\nOther minor comments:\n\nthe input symbol {M} in the definition of the MOMG is never properly introduced, and the meaning of $\\Delta_M^o$ in Eq. (1) is vague—especially since the only difference between PNE and WPNE is replacing $\\Delta_M^o$ with $\\Delta_M$, but the implications of this distinction are never clearly explained.\n\nIn Definition 2 the definition of MOMG lacks on input argument $\\{M\\}$ which is inconsistent with the notation on Line 192-193"}, "questions": {"value": "What is $\\Delta_M^o$ in Eq. (1) and how is it chosen? Is it true that the only difference between PNE and WPNE is replacing $\\Delta_M^o$ with $\\Delta_M$? Would be better if the author can illustrate more on the differences and when is better to consider one over another.\n\nThe proposed algorithms relies on a pre-specified preference distribution—how does this differ practically from solving a standard scalarized Markov game?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pkcQjvBhQp", "forum": "hdcTQ4eEVJ", "replyto": "hdcTQ4eEVJ", "signatures": ["ICLR.cc/2026/Conference/Submission6388/Reviewer_avgz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6388/Reviewer_avgz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959433539, "cdate": 1761959433539, "tmdate": 1762918672384, "mdate": 1762918672384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies primal-dual approach to the constrained low-rank MDPs. Specifically, for both soft-constraint and hard-constraint problems, this work proposes primal-dual based algorithms with provable guarantee."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The theoretical regret and violation bounds for the primal-dual algorithm in constrained low ranks MDPs are mostly sound, and bridges the gap between representation learning and safe RL."}, "weaknesses": {"value": "The contribution of this work is incremental. There lacks novel design of the algorithm as well as technical contribution regarding constrained low-rank RL."}, "questions": {"value": "1 See Weaknesses.\n\n2 In remark 7, I could not find the lower bound result for unconstraint low-rank MDPs could be found in [21], but such result did appear in [q1]. Please specify the lower bound result in [21] you are referring to. \n\n[q1] Improved Sample Complexity for Reward-free Reinforcement Learning under Low-rank MDPs\n\n3 In Corollary 9, \"replacing $b=b+\\eta$\" -> \"replace $b\\leftarrow b+\\zeta$\". Similarly, $\\xi\\leftarrow \\frac{4}{(1-\\gamma)\\theta}$.\n\n4  In Line 461, $Y_k<\\sqrt{K}$ is not the same condition as shown earlier in line 446 $K^{1/4}$.\n\n5 In Line 474, it is not convincing to claim \"the $K^{3/4}$ regret ... dynamics\". As Table 2 suggests, in tabular and linear scenario, the dependency on the number of episodes $K$ in both regret and violation is $K^{1/2}$. Further considering Table 1, it seems the dependency on $K$ does not increase in hard constraint setting, while the only exception is the low-rank setting in this work. \n\n6 Line 1423 and 1436, remove \"(author?)\".\n\n7 The idea of using soft-max policy in hard-constraint MDP and the associated analysis, as far as I know, appeared in [q2], and should be explicitly and clearly stated in the work. \n\n8 In [q2], they identify $Y_k<\\sqrt{K}$ or $Y_k=\\sqrt{K}$ while this work selects $Y_k<K^{1/4}$ or $Y_k=K^{1/4}$. Explain the rationale for selecting the threshold. Further, emphasize the novelty or difference in handling hard constraint in low-rank MDPs compared to [q2] if any.\n\n[q2] Towards Achieving Sub-linear Regret and Hard Constraint Violation in Model-free RL\n\n9 When comparing results (in Table 1 and Table 2), it would better to convert all results to episodic setting (or discounted setting) to make a fair comparison. In most cases, the conversion is simply $H\\sim\\frac{1}{1-\\gamma}$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5DmnxvsQfs", "forum": "hdcTQ4eEVJ", "replyto": "hdcTQ4eEVJ", "signatures": ["ICLR.cc/2026/Conference/Submission6388/Reviewer_vZX8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6388/Reviewer_vZX8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961021463, "cdate": 1761961021463, "tmdate": 1762918671970, "mdate": 1762918671970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Reviewers,\n\nThank you again for your constructive feedback on our submission. We are writing to gently follow up on our responses posted last week. We have updated the paper to address your major concerns, specifically by adding numerical experiments, providing comparisons with existing work, and clarifying the scope of our contributions and novelty.\n\nCould you please let us know if these updates and our responses have resolved your concerns? We are committed to improving the paper further and would value any additional feedback you might have.\n\nBest regards, The Authors"}}, "id": "4uqdxucgzK", "forum": "hdcTQ4eEVJ", "replyto": "hdcTQ4eEVJ", "signatures": ["ICLR.cc/2026/Conference/Submission6388/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6388/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission6388/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763422300833, "cdate": 1763422300833, "tmdate": 1763422300833, "mdate": 1763422300833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}