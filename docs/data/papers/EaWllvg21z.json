{"id": "EaWllvg21z", "number": 19052, "cdate": 1758293136977, "mdate": 1759897063741, "content": {"title": "Learning Universal Adversarial Perturbations for Ordered Top-K Targeted Attacks", "abstract": "Universal adversarial perturbations (UAPs) have deepen concerns regarding the vulnerability of Deep Neural Networks (DNNs) under the white-box attack setting. While most success with UAPs has been observed in untargeted attack settings, achieving effective top-1 targeted UAPs has proven challenging. In this paper, we address this challenge by demonstrating that ordered Top-K targeted UAPs  can be learned aggressively along the label target axis (tested up to Top-6), and transfer very well along the data axis (i.e., across images from the seen training images to the unseen test images). They also show strong double-transferability across unseen test models and unseen test images, when learned from an ensemble of disparate train models. Our method, named **AllAttacK**, simultaneously targets three axes: images, models, and label targets, and is posed as a maximum satisfiability (MAXSAT) problem. We evaluate AllAttacK on the ImageNet-1k classification task using 27 diverse models with more than 500 UAPs learned, showing that the resulting perturbations not only exhibit strong transferability but also display intriguing, interpretable characteristics.", "tldr": "a large-scale analyses of more than 500 ordered top-K targeted universal adversarial perturbations learned for 27 DNNs", "keywords": ["Adversarial Attack", "Universal Adversarial Perturbation", "Ordered Top-K Attack", "Quadratic Programming"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8793c2a99e6bde3420086f5e3a757f55bf1787f7.pdf", "supplementary_material": "/attachment/bb57a82b8adc290459c0a163f97cb3e2d7c93b30.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a universal attack framework for ordered Top-K target, demonstrating that ordered Top-K target labels can transfer effectively across both datasets and models. Specifically, the proposed AllAttacK approach extends previous single-dimensional analyses by jointly exploring universal adversarial perturbations (UAPs) across three dimensions, including model, data, and target. Built upon two single-model-single-image ordered Top-K attack methods (i.e., Adversarial Distillation and QuadAttack), this paper generalizes these techniques to the universal attack setting. Experimental results show that AllAttacK effectively generates ordered Top-K UAP with strong transferability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed approach demonstrates strong model-agnostic and image-agnostic properties, enhancing the transferability of universal adversarial perturbations (UAPs).\n\n2. The paper adopts a stochastic mini-batch and mini-model optimization strategy, effectively balancing practicality and generalization.\n\n3. Experimental results show that AllAttack achieves notable transferability, generating perturbations that are both transferable and interpretable."}, "weaknesses": {"value": "1.  In Section 1, the explanation of the data axis lacks clarity. The authors mention that only 1,000 ImageNet training images are used to learn the UAP, which deviates from most prior UAP studies that typically utilize much larger datasets. However, the paper does not provide a rationale for this choice, which reduces the clarity and completeness of the corresponding analysis.\n\n2. In Section 4, most experimental results are confined to the white-box setting, with no discussion of black-box scenarios. Since UAPs are generally expected to demonstrate effectiveness in black-box attacks, the lack of corresponding evaluations limits the completeness and generality of the experimental analysis.\n\n3. In Section 2, under the discussion of the target axis, the condition requiring the sampling list $T$ to exclude segments of any of the model predictions is introduced without sufficient explanation.\n\n4. In Section 3, the manuscript omits the intermediate derivations connecting Equation (4) to Equations (5) and (7). Moreover, the motivation for introducing the adversarial distillation (AD) and quadratic programming (QP) methods is insufficiently discussed, and it remains unclear how the proposed approaches satisfy the stated constraints. \n\n5. In Section 3, the paper clearly formulates an optimization problem with more restrictive constraints compared to the standard UAP setting. However, it does not discuss the associated computational cost or the practical feasibility of the proposed method.\n\n6. In Equation (7), the symbols and definitions are unclear. Specifically, $z_{i,s}$ is introduced without a proper definition, and its relationship to $x$ is not explained.\n\n7. In Section 3, the authors state that the method builds on two single-model, single-image ordered Top-K attack techniques. However, the paper does not provide a detailed procedure for generating UAPs. For example, it is unclear how the one-step gradient descent defined in Eq. (8) guarantees that the associated constraints are satisfied.\n\n8. In Section 3.3, the authors adopt stochastic mini-batch and mini-model learning to reduce memory requirements. It remains unclear how these choices affect the generation process and the effectiveness of the resulting UAP.\n\n9. In Section 4, the authors present comparisons across different models for the adversarial distillation (AD) and quadratic programming (QP) methods under varying numbers of target labels and datasets. However, the paper does not include comparisons with baseline methods, such as extending CW^{K} and RisingAttacK [1] to the UAP setting. \n\n10. In Section 4, the number of target labels is restricted to 1–6 without explanation. Moreover, the paper does not provide a theoretical justification for why the KL method outperforms QP under the Top-1 perturbation setting.\n\n11. In Table 9, the paper compares computational efficiency but does not include any baseline methods for reference, and it remains unclear why the runtime for K = 5 is nearly identical to that for K = 2.\n\n[1] Adversarial Perturbations Are Formed by Iteratively Learning Linear Combinations of the Right Singular Vectors of the Adversarial Jacobian. ICML 2025."}, "questions": {"value": "1. Most of the experimental comparisons are limited to white-box scenarios and do not address black-box settings. Given that UAPs are expected to be effective in black-box attack scenarios, could the authors clarify this limitation and provide corresponding analysis or discussion?\n\n2. The motivation for introducing adversarial distillation (AD) and quadratic programming (QP) to generate ordered Top-K targeted UAP is insufficiently explained, and it remains unclear how the proposed approach satisfies the stated constraints through the one-step gradient descent update. Could the authors clarify these points in greater detail?\n\n3. The authors adopt stochastic mini-batch and mini-model learning to reduce memory requirements. However, it remains unclear how these design choices affect the UAP generation process and the practical effectiveness of the resulting perturbations. Could the authors further discuss the impact of different batch sizes and model selections on UAP quality and transferability, and provide empirical sensitivity analyses or guidance for selecting these parameters?\n\n4. The authors compare adversarial distillation (AD) and quadratic programming (QP) methods across different models under varying numbers of target labels and dataset conditions. How does the attack performance of the proposed method compare with other baseline approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "X4EbdbFiI7", "forum": "EaWllvg21z", "replyto": "EaWllvg21z", "signatures": ["ICLR.cc/2026/Conference/Submission19052/Reviewer_DGam"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19052/Reviewer_DGam"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993378066, "cdate": 1761993378066, "tmdate": 1762931086559, "mdate": 1762931086559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AllAttacK, a method to learn universal (image-agnostic) and model-agnostic ordered Top-K targeted perturbations on ImageNet-1k. Two formulations are instantiated:\n\nAllKLAttacK: an unconstrained surrogate minimizing a KL divergence to a hand-crafted Top-K target distribution (adversarial distillation).\n\nAllQuadAttacK: a QP on intermediate features (per model) enforcing linear Top-K ordering constraints, followed by a gradient step in the pixel space to update the universal δ.\n\nTraining uses stochastic mini-batches of images and models, enabling ensembles up to 18 diverse DNNs (ResNets, DenseNets, ConvNeXt, ViTs/DEiTs, MLP-Mixer) and evaluation on 27 models in total, including robust and foundation-style encoders (CLIP ViT-B, EVA-2, ConvNeXtV2-H). Using 1000 train and 1000 test images (one per class), the paper reports high ASRs for Top-1 through Top-6 targets and demonstrates double transferability (to unseen images and unseen models). See Fig. 2 for qualitative UAPs; Table 1 for 18-model ensemble ASRs; Table 2 for transfer to unseen models; Table 3–5 for single-model Top-K (K≤6), including robust models; Table 6 for small ensembles; Table 7 for PCC evidence that δ dominates logits; Fig. 5 for visual examples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1- New attack axis: ordered Top-K universals at scale. Prior UAP work largely focused on untargeted or Top-1 targeted settings; demonstrating ordered Top-K (K up to 6) with universal δ on ImageNet-1k and across many architectures is novel and practically relevant for rank-based decision pipelines. Results in Table 3/4/6 show substantial Top-K ASRs, with QP clearly advantaged when K>1.\n\n2- Model- and data-transferability. The study convincingly shows double transfer: δ learned on 18 models transfers to held-out models (e.g., Swin-B, ConvMixer-768, ConvNeXtV2-H) and to unseen images (see Table 2, 13, 24–25, heatmaps and norms).\n\n3- Two complementary formulations and a practical training recipe. The KL surrogate works best for Top-1, while the QP ordering constraints dominate for Top-K; the mini-model/mini-data batching is a pragmatic contribution. Eq. (5–8) and Fig. 3 make the pipeline clear.\n\n4- Scale & breadth. Over 500 UAPs learned; 27 models spanning CNNs, ViTs, Mixers, robust checkpoints, and CLIP/EVA-2. The breadth increases confidence in external validity. See Sec. 4, Tables 1–6, F-section tables.\n\n5- Ordered-target semantics. The paper studies both random Top-K orders (stress-tests) and semantically coherent lists (Glove neighbors), showing the latter are easier (see Table 8 and Appx Tables 10–11), which matches intuition and informs downstream risk analyses.\n\n6- Diagnostic evidence. The PCC analysis (Eq. 9, Table 7) suggests δ steers logits so strongly that image content becomes “negligible noise,” supporting the “data-agnostic control” interpretation."}, "weaknesses": {"value": "1- Perceptibility / constraint regime. Although δ is described as “quasi-imperceptible,” reported ℓ₂ norms are often large (e.g., 50–150 in Table 4–5, 12, 14, 25), and ℓ∞ values approach 0.9–1.0 in some settings (see heatmaps). Without perceptual metrics (LPIPS/SSIM), PSNR, or human studies, the claim of imperceptibility is not substantiated.\n\n2- Threat-model clarity & detectability. The paper demonstrates white-box universals with logit access and allows image-range clipping only. It does not analyze defensive preprocessing, rank-consistency detectors, or semantic-coherence checks it motivates in §1–2. Top-K-ordering attacks might be detectable by rank-stability tests—no evaluation given. \n\n3- Evaluation protocol can inflate ASR. The Best/Mean/Worst aggregation over five randomly sampled target lists (Sec. 4) may bias results upward (especially “Best”). Per-image success distributions and statistical CIs are not reported.\n\n4- Energy–success trade-off governance. λ is grid-searched and the paper selects the lowest-energy δ with non-negligible ASR (Appx B), but the frontier (ASR vs energy) is not systematically characterized across K, models, and ensembles—hard to judge efficiency.\n\n5- Top-K exact-order vs set membership. The work enforces strict order; many applications care about set-based Top-K or partial orders. How sensitive is the method when evaluation relaxes to unordered Top-K or allows ties? Not explored.\n\n6- Generalization beyond ImageNet-1k. All results are on ImageNet-1k (one image per class). No experiments on non-natural domains, larger label spaces, or multi-label/ranking tasks. The method’s reliance on class-wise sampling (and removing images when GT=t₁) slightly biases the data protocol.\n\n7- Limited analysis against strong evaluation checklists. There is no EOT for stochastic components, no black-box query-budget study, no physical-world tests, and no transfer-from-surrogates vs white-box comparison beyond model ensembles.\n\n8- Foundational models: partial success. Transfer to CLIP ViT-B and ConvNeXtV2-H is notable but lower for larger K and for EVA-2 (Table 2 & 24–25); the paper does not analyze why (e.g., pretraining objectives, feature spectrum, or logit scale)."}, "questions": {"value": "1- Perceptibility. For the main ASR tables (e.g., Table 1–6), can you report LPIPS/SSIM and PSNR for (δ, x+δ) over Dtest, and provide human-rated visibility at multiple viewing scales? Also show ASR vs ℓ₂ / ℓ∞ curves for K=1…6 across key models.\n\n2- Ordered vs unordered. How do ASRs change if evaluation is unordered Top-K or allows partial order constraints (e.g., only top-m fixed, rest any order)? Please add ablations.\n\n3- Confidence intervals. For Mean/Best/Worst protocols, report per-image success distributions and 95% CIs, and test sensitivity to the number of target-list samples (5→20).\n\n4- QP details & optimality. In Eq. (7–8) the QP is solved on feature variables $z_{i, s}$  with linear Top-K constraints, then one gradient step updates δ. Please characterize feasibility rates of the QP per batch, KKT residuals, and show convergence (or cyclic behavior) of the alternating scheme. Any observed mode collapse to a few classes?\n\n5- Sensitivity to λ and batch sizes. Provide sweeps of λ (and smax if used inside the KL target construction) and mini-model/mini-data batch sizes, reporting ASR–energy frontiers.\n\n6- Feature-space choice. You set $F(x)=f(x) W+b$  and enforce order at logits. What if the linear constraints are enforced earlier (e.g., pre-classifier feature margins), or you use logit temperature scaling to control numerical conditioning—does that improve energy efficiency?\n\n7- Why EVA-2 is harder? Provide spectral/semantic analyses (Fourier energy, token attention patterns) explaining lower transfer to EVA-2 vs CLIP/ConvNeXtV2-H; test whether in-distribution CLIP fine-tuning or logit scaling reduces the gap.\n\n8- Cross-norm transfer. If δ is trained under ℓ₂ but evaluated under ℓ∞ (and vice versa), how do ASRs change? Include ε-sweeps like Fig.10 across norms.\n\n9- Stronger evaluations. Add EOT (noise/resize-crop), JPEG/bit-depth compressions, and rank-stability detectors; test simple defenses (randomized smoothing, small denoisers) and non-differentiable checks (e.g., semantic-coherence filters) against your δ.\n\n10- Black-box budgets. Can your δ be approximated from queries only (no gradients) given the same ordered Top-K targets? Report query counts to reach ASR comparable to white-box"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vF8zPrOVdF", "forum": "EaWllvg21z", "replyto": "EaWllvg21z", "signatures": ["ICLR.cc/2026/Conference/Submission19052/Reviewer_Q89G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19052/Reviewer_Q89G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996233342, "cdate": 1761996233342, "tmdate": 1762931086067, "mdate": 1762931086067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AllAttacK, a novel framework for learning Ordered Top-K Targeted Universal Adversarial Perturbations (UAPs). Unlike prior work focused on untargeted or top-1 attacks, AllAttacK jointly optimizes adversarial perturbations along three independent axes — the data axis (generalization across images), the model axis (transferability across architectures), and the target axis (control over ordered Top-K label sequences). The problem is formulated as a Maximum Satisfiability (MAXSAT) optimization task, instantiated via two scalable variants: the KL-divergence-based AllKLAttacK and the quadratic-programming-based AllQuadAttacK. Large-scale experiments on ImageNet-1k across 27 diverse models demonstrate strong double transferability of the learned perturbations — across unseen data and unseen models, and show that ordered Top-K targeted UAPs expose systematic vulnerabilities in modern deep neural networks’ ranking robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is the first to systematically study ordered Top-K targeted universal adversarial perturbations (UAPs), extending the scope of adversarial attack research beyond traditional untargeted or Top-1 targeted settings. By enforcing ordered Top-K constraints, the work aligns with emerging robustness evaluation standards (e.g., NIST SP800-226, 2025) and provides a new benchmark for model ranking stability.\n2.  Extensive experiments on ImageNet-1k involving 27 diverse models (including CNNs, Vision Transformers, CLIP, EVA2, and adversarially trained networks) demonstrate that the learned perturbations achieve high double-transferability across unseen data and unseen models. The experimental coverage is significantly broader than prior UAP studies."}, "weaknesses": {"value": "1. The MAXSAT formulation (Eq. 4) is conceptually sound but lacks formal solvability or approximation guarantees. No complexity or convergence analysis is provided.\n\n2. The quadratic-programming relaxation implicitly assumes local convexity of the logit-ordering constraints. However, the paper does not analyze feasibility margins or address potential degeneracy when logits become nearly tied, which could impact optimization stability and reproducibility.\n\n3. The paper focuses exclusively on attack performance without considering possible countermeasures, robustness training, or defense evaluation. Including even a preliminary analysis of model responses or defense sensitivity would strengthen the practical significance.\n\n4. The precise norm bound (e.g., ε under L∞ or L2 constraints) used for the universal perturbations is not explicitly stated. Clarifying these values is important for reproducibility and for fair comparison with prior UAP baselines.\n\n5. The PCC (Pearson correlation) experiment in Section 3.3 seems to analyze general UAP behavior rather than the specific ordered Top-K objective introduced by AllAttacK. The connection between this analysis and the proposed method should be clarified.\n\n6. The exploration of transferability across data and model axes largely follows prior UAP literature, offering incremental rather than conceptual novelty relative to existing universal attack frameworks."}, "questions": {"value": "1. What is the computational complexity of AllQuadAttacK compared to AllKLAttacK when trained on large model ensembles? Does the QP-based method scale linearly, quadratically, or worse with respect to the number of models and target classes?\n\n2. The paper focuses exclusively on ordered Top-K targeted attacks. How would the proposed framework behave under unordered Top-K settings, where only the presence of target classes in the Top-K predictions matters, rather than their specific order?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DRyoJJbkUR", "forum": "EaWllvg21z", "replyto": "EaWllvg21z", "signatures": ["ICLR.cc/2026/Conference/Submission19052/Reviewer_V4wR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19052/Reviewer_V4wR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088581617, "cdate": 1762088581617, "tmdate": 1762931085190, "mdate": 1762931085190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper present AllAttacK to generate ordered topk targeted universal adversarial perturbations across different models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is very well motivated.\n\n2. Extensive list of model coverage, including model families, and standard and adversarially trained models."}, "weaknesses": {"value": "Despite the extensive experiments, the paper lacks insightful analysis. In Sec4, large tables are presented with only brief two-liner summaries. The cramped spacing before/after tables blends them into the text, and it overwhelms readers with numbers rather than insight. Please highlight a few key results and discuss them in depth."}, "questions": {"value": "1. Can you clarify 173-175, specifically on how $\\delta_T$ is an energy-minimized points?\n\n2. Ln313 states that \"test the learned UAPs on 6 DNNs \", then what is the x-axis (resnet18, resnet34, ..., MlpMixer-B) in Table 1?\n\n3. Ln 152-157: What is the significance of separately considering Dtrain and Dtest on the data axis? Is generalization from training images to test images a non-trivial finding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dJBr87inom", "forum": "EaWllvg21z", "replyto": "EaWllvg21z", "signatures": ["ICLR.cc/2026/Conference/Submission19052/Reviewer_MG42"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19052/Reviewer_MG42"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762281964841, "cdate": 1762281964841, "tmdate": 1762931084765, "mdate": 1762931084765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}