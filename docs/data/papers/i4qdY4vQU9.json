{"id": "i4qdY4vQU9", "number": 17154, "cdate": 1758272827605, "mdate": 1759897193510, "content": {"title": "PROPGCL: Unleashing the Power of Propagation in Graph Contrastive Learning", "abstract": "Graph contrastive learning (GCL) has recently gained substantial attention, leading to the development of various methodologies. In this work, we reveal that a simple training-free propagation operator PROP achieves competitive results over dedicatedly designed GCL methods across diverse node classification benchmarks. We elucidate PROPâ€™s effectiveness by drawing connections with established graph learning algorithms. By decoupling the propagation and transformation phases of graph neural networks, we find that the transformation weights are inadequately learned in GCL and perform no better than random. When the contrastive and downstream objects are misaligned, the attendance of transformation causes the overfitting to the contrastive loss and harms downstream performance. In light of these insights, we remove the transformation entirely and introduce an efficient GCL method termed PROPGCL. We provide theoretical guarantees for PROPGCL and demonstrate its effectiveness through a comprehensive evaluation of node classification tasks.", "tldr": "", "keywords": ["Graph Contrastive Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0d2c37dbe9538764d5d64590d7d436244bb49c54.pdf", "supplementary_material": "/attachment/3555809f9632ebfb24135e8f23211e4fca067b23.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates simple propagation operators in graph contrastive learning. The authors introduce PROP, a training-free propagation method that achieves competitive performance across node classification benchmarks. Through decoupling analysis, they reveal that existing GCL methods struggle to learn meaningful transformation weights while showing potential in learning propagation coefficients. Based on these insights, they propose PROPGCL, which eliminates transformation layers and only learns graph-adaptive propagation coefficients. The method achieves strong results on heterophilic datasets with significant computational advantages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The core finding that simple propagation matches complex GCL methods is surprising and well-demonstrated. PROP consistently performs well across benchmarks, particularly on heterophilic graphs where many GCL methods struggle.\n\n2. Thm. 4.1 now formally connects propagation to Dirichlet energy minimization with proper mathematical rigor. Thm. 6.1 provides theoretical guarantees for PROPGCL's advantage when contrastive and downstream objectives are misaligned. These additions address major theoretical gaps from prior versions.\n\n3. The ablation studies in sec. 5.2 are particularly convincing. Tab. 3 shows that (1) GCL struggles to learn effective transformation weights even with optimal propagation coefficients, and (2) GCL can learn informative propagation coefficients when paired with well-trained transformation weights. This provides clear evidence for the main claims.\n\n3. Experiments compare against diverse architectures including spectral-based models etc. The addition of large-scale experiments on ogbn-products addresses scalability. The efficiency analysis in sec. 6.4 shows 99% memory reduction and substantial training time improvements."}, "weaknesses": {"value": "1. The title and narrative discuss \"graph contrastive learning\" broadly, but the method focuses on node classification. App. B shows a 2.82% performance gap on graph classification tasks. This suggests fundamental limitations when global graph representations are needed. The paper should either narrow the title and claims to node-level tasks, or provide deeper analysis of why propagation works for node classification but not graph classification.\n\n2. Thm. 6.1 assumes the downstream-relevant component corresponds to low-frequency signals with smoothness. However, heterophilic graphs violate this assumption since connected nodes have different labels. Yet PROPGCL shows its strongest improvements on exactly these datasets. The paper should discuss when these assumptions hold and provide empirical validation of the decomposition in def. 6.1.\n\n3. The paper frames PROP as performing implicit alignment in contrastive learning (thm. 4.2). Yet PROP requires no negative samples and no learned parameters. This blurs what constitutes contrastive learning versus effective preprocessing. Is the finding that simple propagation works well a contribution to GCL methodology, or evidence that the GCL paradigm is unnecessary? The paper should address this more explicitly."}, "questions": {"value": "1.Fig. 2 shows training loss rapidly approaching zero for GCL with transformation. Could you also show validation or downstream performance curves? This would directly visualize the negative transfer effect. Does early stopping based on downstream validation help?\n\n\n2. Tab. 2 shows random weights achieve 73.4% vs 72.8% for GCL-learned weights. Could you provide statistical significance tests? Have you tried other random initialization schemes beyond Gaussian?\n\n3. The paper uses Chebyshev basis functions. How sensitive is PROPGCL to this choice? Do the conclusions hold across different filter families?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sRTzttuoAr", "forum": "i4qdY4vQU9", "replyto": "i4qdY4vQU9", "signatures": ["ICLR.cc/2026/Conference/Submission17154/Reviewer_8KxH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17154/Reviewer_8KxH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668799248, "cdate": 1761668799248, "tmdate": 1762927141783, "mdate": 1762927141783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper claimed that matrix transform in traditional GCL degrades model performance. So, the authors removed it, purely relied on raw feature propagation. Moreover, they learned the coefficients to combine different order of graph Laplacian, enhancing propagation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper gave a comprehensive analysis of each component in GCL.\n2. The authors gave enough experiments and theorems to support their claims."}, "weaknesses": {"value": "1. The experimental results on PROP is very strange. In table 1, PROP, a pure $A^kX$, can perfrom extremely well on heterophilic graphs. The authors explained this that $A^kX$ can smooth all k-hop neighbors. That's ture, but we know Dirichlet energy $H^TLH=\\sum A_{ij}||h_i-h_j||^2$. For $A^k$, it becomes to $\\sum A^k_{ij}||h_i-h_j||^2$. Therefore, minimizing Dirichelet norm just equals to make all nodes within k-hops have similar embeddings. This is against the fact that we have to differentiate neighbor nodes embeddings to work on heterophilic graphs. From graph spectrum, $A^kX$ is a definitely a low-pass filter, while only high-pass filter can perform well on heterophilic graphs.\n\n2. In table 1, GSSL methods performed severely worse than supervised methods. This is also against consensus that GSSL methods are better than vanilla supervised GNN, like GCN."}, "questions": {"value": "In figure 2 GCLw/o transformation, the loss nearly did not decrease, or the the model was not trained. So, if GCL is without transformation, which paramters are learnt in it? Since the propation part is just H_PROP, also without paramters."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jN3PMP6EJk", "forum": "i4qdY4vQU9", "replyto": "i4qdY4vQU9", "signatures": ["ICLR.cc/2026/Conference/Submission17154/Reviewer_rEYN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17154/Reviewer_rEYN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822504282, "cdate": 1761822504282, "tmdate": 1762927141533, "mdate": 1762927141533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PROP, a training-free propagation operator that aggregates k-hop neighbor features, establishing it as a strong baseline for self-supervised node classification by showing its competitive performance across homophilic and heterophilic datasets, even outperforming many dedicated GCL methods on heterophilic benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper challenges the prevailing paradigm in graph contrastive learning (GCL) that complex parameterized encoders are indispensable for high performance.\nBy decomposing contrastive loss into downstream-relevant and irrelevant components, it proves that PROPGCL outperforms both PROP and traditional GCL when objectives misalign.\nThe paper conducts multiple experimental evaluations to validate its claims."}, "weaknesses": {"value": "PROP and PROPGCL depend on the propagation step K, but the paper lacks a systematic analysis of how K optimally adapts to diverse graph structures and provides no heuristic for K selection without brute-force tuning.\nThe paper omits comparisons with recent lightweight GCL methods (e.g., SimGCL, LiteGCL) that also prioritize efficiency.\nPROPGCL does not integrate with graph attention mechanisms to prioritize important neighbors, limiting flexibility for graphs with unevenly relevant neighbors. \nThe paper compares PROPGCL with established GCL methods but omits newer lightweight GCL approaches that also prioritize efficiency. \nThe paper does not test PROPGCL on inductive benchmarks with heterogeneous node types or dynamic graph splits."}, "questions": {"value": "For the training-free PROP operator, how does its performance scale with graph size and density?\nThe paper reveals that GCL-learned transformation weights perform no better than random weights, but only tests Gaussian, Uniform, Kaiming, and Xavier random initializations. Do other randomization strategies yield different results, and could they potentially outperform GCL-learned weights more significantly?\nHow does Fix-prop SL compare to dedicated few-shot GCL methods, and can further tuning of hyperparameters improve its performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EbLQRGXGE0", "forum": "i4qdY4vQU9", "replyto": "i4qdY4vQU9", "signatures": ["ICLR.cc/2026/Conference/Submission17154/Reviewer_3DvS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17154/Reviewer_3DvS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955118800, "cdate": 1761955118800, "tmdate": 1762927141275, "mdate": 1762927141275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper claims that the feature transformation component of GNNs is detrimental in Graph Contrastive Learning, performing no better than random weights. The authors propose PROPGCL, which removes the transformation layer entirely and only learns propagation coefficients, and can outperform complex GCL encoders. The method is shown to be both effective, particularly on heterophilic graphs, and computationally efficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper analyzes a problem in GCL where complex transformation layers are often poorly learned and perform no better than random weights.\n2. The proposed method is evaluated on heterphily graph benchmarks."}, "weaknesses": {"value": "1. The novelty is a bit limited - the propagation of graph node features serves as a good representation is well studied in the literature\n2. The paper excels at showing that the transformation fails but provides little insight into why. Why is the GCL objective sufficient for learning propagation coefficients $\\theta$ but not transformation weights $W$?"}, "questions": {"value": "1. Why is the GCL objective sufficient for learning propagation coefficients $\\theta$ but not transformation weights $W$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Tz2nPXaw1A", "forum": "i4qdY4vQU9", "replyto": "i4qdY4vQU9", "signatures": ["ICLR.cc/2026/Conference/Submission17154/Reviewer_os8L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17154/Reviewer_os8L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963678148, "cdate": 1761963678148, "tmdate": 1762927140685, "mdate": 1762927140685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}