{"id": "a195V8Qp4Z", "number": 6462, "cdate": 1757985768304, "mdate": 1759897912990, "content": {"title": "Low-frequency and Robust Image Information Hiding", "abstract": "Existing image information hiding methods commonly lack robustness to varying degrees of distortion on container images. This paper proposes a low-frequency and robust image information hiding method, LRIIS, to overcome current challenges. To emphasize robustness, instead of hiding high-frequency subbands, we propose a novel wavelet contrastive loss to constrain so that most secret information is hidden in the low-frequency subbands. Compared to high-frequency subband hiding, low-frequency subband embedding achieves enhanced robustness. To alleviate the varying degrees of distortion influence, we build an unsupervised Attacked Image Enhancement Module (AEM) to generate the de-attacked image that is close to the corresponding container image. Notably, thanks to the pseudo-class label of AEM, the proposed method can recover the secret image from the attacked image without requiring a specific attack label. Experimental results demonstrate the superior performance of the proposed LRIIS model on the COCO and DIV2K datasets compared to existing state-of-the-art image information hiding methods.", "tldr": "", "keywords": ["Information hiding", "Contrastive Learning", "Invertible Neural Network"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e90c1426b596f94258ffdd96421d80903c1c3fec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes LRIIS, a low-frequency robust image steganography method that solves two main problems in existing methods: poor robustness to image distortion and the need for specific attack labels. The method uses a wavelet contrast loss to hide secret information in low-frequency subbands, which reduces high-frequency artifacts. For the problem of previous methods requiring attack labels, LRIIS introduces an Adaptive Enhancement Module (AEM). This module divides the task into many-to-one class enhancements, creates pseudo-class labels without supervision for each attacked image, and applies these labels to remove distortion. This makes the processed images similar to the original ones without using any attack labels. Tests on COCO and DIV2K datasets show that LRIIS performs better than current SOTA methods."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The method uses a wavelet contrast loss to hide secret information in low-frequency subbands, which reduces high-frequency artifacts. For the problem of previous methods requiring attack labels, LRIIS introduces an Adaptive Enhancement Module (AEM). This module divides the task into many-to-one class enhancements, creates pseudo-class labels without supervision for each attacked image, and applies these labels to remove distortion."}, "weaknesses": {"value": "(1) What is the rationale for employing a dual-encoder structure (style and classified encoder) within the AEM? Is it feasible for a single encoder to generate valid pseudo-class labels independently? \n\n(2) What motivates the selection of SimCLR as the pretext task for clustering in the AEM module? What are the key benefits of using SimCLR compared to other contrastive learning frameworks like MoCo and CUT?\n\n(3) The “robust” claimed in the title and contribution is not evident from the presented experiments. The robustness of LRIIS against other standard attacks (e.g., noise, rotation, JPEG compression) and its applicability to non-global mask scenarios remain unverified. Conversely, the related RIIS method successfully handles various distortions like Gaussian noise, Poisson noise, and JPEG compression. \n\n(4) Despite the incorporation of multiple configurable settings within the Adaptive Enhancement Module (AEM), the article omits a comprehensive ablation study to validate their individual and collective contributions. For instance, it remains unclear whether valid pseudo-class labels can be generated using the class contrastive loss alone. Furthermore, the methodology for determining the module's internal settings is not explained. The overall contribution of the AEM itself has not been validated.\n\n(5) The article does not address the core requirement of security in steganography, which is resistance to steganalysis. The absence of any security analysis in this regard represents a major limitation of the presented work.\n\n(6) The visual representations in the manuscript, including the structural and comparative diagrams, are not clear."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "E29CV8YUsc", "forum": "a195V8Qp4Z", "replyto": "a195V8Qp4Z", "signatures": ["ICLR.cc/2026/Conference/Submission6462/Reviewer_W2hc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6462/Reviewer_W2hc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760701898390, "cdate": 1760701898390, "tmdate": 1762918852488, "mdate": 1762918852488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Low-Frequency and Robust Image Information Hiding method (LRIIS) to address the poor robustness of existing image information hiding methods to distortion. It introduces a wavelet contrastive loss to constrain secret information in low-frequency subbands for enhanced robustness. An unsupervised Attacked Image Enhancement Module (AEM) is constructed to generate de-attacked images close to original container images, enabling secret image recovery from attacked images without specific attack labels. Experiments on COCO and DIV2K datasets show it outperforms mainstream methods in metrics such as PSNR and SSIM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Proposes a novel wavelet contrastive loss, creatively confining secret information to low-frequency subbands and breaking the conventional high-frequency hiding paradigm.\n\n2.Constructs an unsupervised AEM, realizing attack-label-free secret image recovery and addressing a key limitation of prior robust steganography methods.\n\n3.Conducts comprehensive evaluations on COCO and DIV2K datasets, outperforming mainstream SOTA methods in multiple metrics and demonstrating excellent robustness and practicality."}, "weaknesses": {"value": "1.Narrow Scope of Distortion Types: The evaluation focuses mainly on global mask attacks, and it is unclear how the method performs against other common distortions like JPEG compression, Gaussian noise, or geometric transformations, limiting the assessment of its general robustness.\n\n2.The paper is generally well-written and the language is smooth. However, there are typographical errors. For example, \"attacked\" is mistakenly written as \"attached\", which impairs the accuracy of technical terms and requires careful proofreading to ensure the correctness of the content.\n\n3.Lack of comparative fairness:No comparison of computational and parameter quantities between different methods"}, "questions": {"value": "1. Could you provide experimental results or theoretical analysis to illustrate how LRIIS performs under other common image distortions such as JPEG compression, Gaussian noise, or geometric transformations? This would help clarify the general robustness of the method beyond global mask attacks.\n\n2. Please conduct a thorough proofreading of the manuscript to correct errors like the mistyping of \"attacked\" as \"attached\" and other potential typos, ensuring the accuracy and professionalism of the technical terms and content.\n \n3.Could you provide a detailed comparison of the computational complexity (e.g., FLOPs) and the number of model parameters between LRIIS and the baseline methods (HiNet, HIIDM, RIIS, CrossNET)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "y7iaZUz6Ld", "forum": "a195V8Qp4Z", "replyto": "a195V8Qp4Z", "signatures": ["ICLR.cc/2026/Conference/Submission6462/Reviewer_o4pf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6462/Reviewer_o4pf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643821385, "cdate": 1761643821385, "tmdate": 1762918852038, "mdate": 1762918852038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses two critical drawbacks of existing image information hiding methods, namely insufficient robustness to container image distortions and reliance on specific attack labels, by proposing LRIIS, a low-frequency and robust invertible image information hiding framework. Its core designs include: 1) a novel wavelet contrastive loss, which forces most secret information into low-frequency wavelet subbands (narrowing cross-subbands between high and low frequencies to mitigate high-frequency artifacts and boost robustness); 2) an unsupervised Attacked Image Enhancement Module (AEM), which generates de-attacked images close to the original container via unsupervised semantic clustering (for pseudo-class label generation) and a many-to-one class enhancement network. Experiments are conducted on the DIV2K and COCO datasets, with LRIIS compared against four SOTA methods (HiNet, HIIDM, RIIS, CrossNET). Both quantitative metrics (PSNR, SSIM, MAE, RMSE) and qualitative results confirm that LRIIS outperforms SOTA counterparts in terms of container image imperceptibility and extracted secret image fidelity.\n\nThe paper's key contributions are threefold: 1) it introduces the wavelet contrastive loss to enable low-frequency secret embedding, solving the poor robustness of high-frequency embedding; 2) it develops the unsupervised AEM to alleviate distortion impacts from diverse attacks; 3) it is the first robust steganography model that recovers secret images from attacked containers without specific attack labels."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1）The paper demonstrates originality by addressing key limitations of existing image information hiding methods: it proposes a wavelet contrastive loss to solve high-frequency artifacts in low-frequency embedding and an unsupervised AEM to eliminate reliance on specific attack labels. It also creatively combines contrastive learning with wavelet domain embedding and unsupervised clustering with image enhancement, forming a novel framework. \n2）The loss function of this work is comprehensively designed, integrating AEM loss (to mitigate the impact of distortion), HR loss (to ensure the fidelity of hiding and extraction), and wavelet contrast loss (to constrain low-frequency embedding), effectively balancing the three core task goals of robustness, imperceptibility, and embedding stability; the experimental design is fair, conducted on two publicly licensed datasets (DIV2K and COCO), with all the most advanced baseline models retrained under uniform experimental conditions, and the results are supported by both quantitative metrics and qualitative visualizations."}, "weaknesses": {"value": "1) Theoretical justification for core designs is insufficient, weakening the credibility of its innovations. The proposed wavelet contrastive loss is claimed to solve high-frequency artifacts in low-frequency embedding, but it is not compared to traditional frequency-constraint methods to prove its necessity—there is no analysis of why contrastive learning outperforms these simpler approaches in narrowing cross subbands. Similarly, the AEM module's core assumption (that \"style-similar attacked images belong to the same distortion type\") is unvalidated: the paper does not use tools to visualize style feature clustering across different distortions (e.g., global masic vs. Gaussian noise) or test how pseudo-label errors from mismatched style-distortion correlations impact extraction accuracy. To improve, the authors should add a theoretical analysis section quantifying the wavelet contrastive loss’s advantage over traditional methods and supplement experiments validating the style-distortion correlation (e.g., showing that style clustering accuracy aligns with distortion type classification accuracy).\n2) Experimental scope is narrow, failing to verify real-world robustness and core domain metrics. The paper only tests robustness against \"global masic attacks,\" ignoring common distortions that container images face in practice: basic distortions (Gaussian noise with σ=0.01/0.05, JPEG compression with quality factors 50/70, Poisson noise), geometric distortions (15°/30° rotation, 10%/20% cropping), and composite distortions (JPEG compression + Gaussian noise). This makes it impossible to assess LRIIS’s generalizability. Additionally, the paper omits \"embedding capacity (bpp)\"—a core metric in image information hiding (traditional methods often cap at <0.4 bpp, per Section 2.1)—with no tests of LRIIS’s maximum capacity or the trade-off between capacity and robustness (e.g., whether low-frequency embedding reduces capacity, and if so, by how much). \n3) Ablation studies are incomplete, leaving key module necessity unproven. The paper only validates the wavelet contrastive loss (by removing it to form LRIIS₁) but ignores other critical components: it does not test if AEM’s sub-modules (style encoder Estyle and class encoder Eclass) are indispensable (e.g., whether removing Estyle degrades pseudo-label quality), nor does it verify the rationality of loss weights (λ₁=6, λ₂=6, λ₃=10) via grid search (e.g., how λ₃=5/15 affects low-frequency embedding and robustness). \n4) Contribution boundaries are ambiguous, lacking clear differentiation from prior work. The paper claims to advance low-frequency embedding (Xiang et al. 2008) and robust label-free recovery, but it does not quantify these advances: it does not specify how much LRIIS reduces high-frequency artifacts compared to Xiang et al. 2008, nor does it clarify how AEM differs from generic unsupervised enhancement models in adapting to information hiding scenarios."}, "questions": {"value": "Please refer to \"Weaknesses\" for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dKCZQVJqcv", "forum": "a195V8Qp4Z", "replyto": "a195V8Qp4Z", "signatures": ["ICLR.cc/2026/Conference/Submission6462/Reviewer_pfd4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6462/Reviewer_pfd4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877237114, "cdate": 1761877237114, "tmdate": 1762918851403, "mdate": 1762918851403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a low-frequency and robust image information hiding method, LRIIS, to overcome current challenges. The authors build an unsupervised Attacked Image Enhancement Module (AEM) to generate the de-attacked image that is close to the corresponding container image."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose a novel wavelet contrastive loss to force most secret information to be hidden in low-frequency subbands, which can alleviate the distortion influence. \n2. The authors build an unsupervised Attacked Image Enhancement Module to generate the de-attacked image that is close to its corresponding container image. \n3. The authors claim that this is the first robust steganograph model to recover the secret image from the attacked image without giving its corresponding attack label."}, "weaknesses": {"value": "1. The paper’s title emphasizes “information hiding,” yet the authors do not report the bits per pixel (bpp), which is a key metric in this domain. Moreover, the experiments primarily focus on the quality of recovered images, suggesting that the actual goal might be to propose a de-attack or image recovery technique rather than an information hiding approach. If this is the case, the title is misleading and should be revised to better reflect the paper’s true focus.\n\n2. The cited works on traditional image information hiding techniques are outdated, with most references being over five years old. There have been more recent and relevant developments in this area, including methods such as [1], [2], among others, which achieve over 0.4 bpp. The authors are encouraged to conduct a more comprehensive and up-to-date literature review to better position their work within the current research landscape.\n\n3. The results section lacks statistical or significance testing, making it difficult to determine whether the proposed method offers measurable improvements over competing approaches. Incorporating quantitative comparisons and statistical analyses would strengthen the validity of the conclusions.\n\n[1] P. Puteaux and W. Puech, \"A Recursive Reversible Data Hiding in Encrypted Images Method With a Very High Payload,\" in IEEE Transactions on Multimedia, vol. 23, pp. 636-650, 2021, doi: 10.1109/TMM.2020.2985537.\nkeywords: {Encryption;Payloads;Image reconstruction;Decoding;Servers;Cloud computing;Image security;image encryption;reversible data hiding;recursive process;bit-plane prediction;signal processing in the encrypted domain},\n\n[2] Y. Puyang, Z. Yin and Z. Qian, \"Reversible Data Hiding in Encrypted Images with Two-MSB Prediction,\" 2018 IEEE International Workshop on Information Forensics and Security (WIFS), Hong Kong, China, 2018, pp. 1-7, doi: 10.1109/WIFS.2018.8630785.\nkeywords: {Encryption;Data mining;Correlation;Image reconstruction;Cloud computing;Streaming media},"}, "questions": {"value": "The paper would benefit from a discussion of failure cases. It is recommended that the authors include representative examples of failed or suboptimal results in the Results section, accompanied by figures. Analyzing these cases would help clarify the limitations of the proposed method and provide valuable insights into potential areas for improvement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "srGMBjEcoz", "forum": "a195V8Qp4Z", "replyto": "a195V8Qp4Z", "signatures": ["ICLR.cc/2026/Conference/Submission6462/Reviewer_2fhZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6462/Reviewer_2fhZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123061689, "cdate": 1762123061689, "tmdate": 1762918850038, "mdate": 1762918850038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}