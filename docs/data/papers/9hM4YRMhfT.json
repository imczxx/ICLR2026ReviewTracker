{"id": "9hM4YRMhfT", "number": 2533, "cdate": 1757138611228, "mdate": 1759898142580, "content": {"title": "GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence GUI Tasks", "abstract": "Autonomous agents for long-sequence Graphical User Interface tasks are hindered by sparse rewards and the intractable credit assignment problem. To address these challenges, we introduce GUI-Shepherd, a Process Reward Model that provides dense, step-by-step feedback to guide agents. GUI-Shepherd is trained on a diverse large-scale data set of 52k interactions that features human-annotated scores and GPT-4o generated rationales, enabling it to serve both as a reward provider for RL training and as a verifier for inference. As far as we know, we are the first to conduct a systematic study of process supervision in GUI agents, across diverse settings from online long-horizon tasks to offline single-step prediction. On the online AndroidWorld benchmark, GUI-Shepherd improves success rate by 7.7 points via multi-turn online PPO, significantly outperforming Outcome Reward Model based competitors. When used as an inference verifier, it brings 5.1 points improvements. The benefits generalize to the offline AndroidControl benchmark, with gains of 2.2 points as a reward provider and 4.3 points as a verifier. Collectively, our results establish that high-fidelity process supervision is critical for building more capable GUI agents and present a generalizable solution.", "tldr": "We propose a PRM and perform the first systematic study of process supervision in GUI agents from online long-horizon tasks to offline single-step prediction, from RL training to inference verification.", "keywords": ["long-sequence gui tasks", "multi-turns RL", "process reward model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9852bb13c5e6178e36959ddbed8b725044301ac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GUI-Shepherd, a process reward model (PRM) that offers dense feedback to guide agents in long-horizon Graphical User Interface (GUI) tasks. It marks the first successful application of PRM to online Reinforcement Learning (RL) in this domain. To address the lack of high-quality training data, the authors also present a scalable dual-pipeline approach for creating process supervision datasets and offer systematic validation of PRM in the GUI domain."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is the first to apply PRM to online RL in long-horizon GUI tasks and the experimental results show the effectiveness of the proposed model.\n\n\n2. The proposed data generation pipeline can generate high-quality training data for the GUI Agent tasks."}, "weaknesses": {"value": "The proposed model still exhibits a significant performance gap compared to state-of-the-art (SOTA) models on the Android World Leaderboard. For instance, \"GUI-Owl-7B\" achieves a 66.4% Success Rate (SR), while the proposed method only reaches 40.5% SR. Can you explain why this discrepancy occurs? Is the primary reason related to the foundation model, the training algorithm, or the training data?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4bBOk0Ckw9", "forum": "9hM4YRMhfT", "replyto": "9hM4YRMhfT", "signatures": ["ICLR.cc/2026/Conference/Submission2533/Reviewer_Fhi3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2533/Reviewer_Fhi3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381716219, "cdate": 1761381716219, "tmdate": 1762916270133, "mdate": 1762916270133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GUI-Shepherd using process reward model (PRM) that delivers step-level supervision and verification for agents performing long-sequence tasks in mobile GUI environments. The PRM is trained on a dataset created by LLM and human annotations, and it is intended to be used in training LLMs during RL training and acting as an action selector during inference time. The trained LLMs with PRM are evaluated in AndroidWorld and offline one-step AndroidControl benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe experiment covers both online and offline environments, showing improvements in both settings.\n-\tThe hybrid annotation process leverages both human expertise for correctness and LLM-as-judge for efficiency."}, "weaknesses": {"value": "-\tStrong related works are not included as comparison, for example, DigiRL in both online and offline settings.\n-\tThe contribution of applying PRM in training LLM is limited. Despite the equations in Section 4, the contribution does not extend beyond standard RL objective adaptations, and there are no new theoretical analyses of convergence or limitations of PRMs.\n-\tThe temporal and UI diversity mentioned in training data process are unclear. For example, how to ensure data quality (there seems no postprocessing or filtering)? How to measure diversity?\n-\tIt is unclear how the human annotation is performed. How many humans and how to ensure their expertise? What is their annotation instruction? What is the cost? How exactly is the human annotation process performed?\n-\tThe hybrid annotation is confusing. Figure 5 shows the human annotation interface with ‘thought’, which seems to be generated from LLMs. However, the prompt of LLM annotation in Appendix E shows that generating thoughts requires human annotation/ground truth."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ljW5VezNU0", "forum": "9hM4YRMhfT", "replyto": "9hM4YRMhfT", "signatures": ["ICLR.cc/2026/Conference/Submission2533/Reviewer_CgsL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2533/Reviewer_CgsL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901495945, "cdate": 1761901495945, "tmdate": 1762916269892, "mdate": 1762916269892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GUI-sheppard which is an early instantiation of process-reward models in ui-based tasks. The main contribution is using a reasoning LLM itself as the critic which I found quite interesting. Overall they show many ablations and interesting experiments showing their methodology is sound."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper leans more on the empirical contribution in my mind rather than the methodological one (which is good in my books), the idea of using a PRM for web tasks which are innately multi-turn is intuitive. The approach clearly works which is nice I have some problems with the evaluations but will discuss that later. I really like the ablation that discussed training on human-ratings vs GPT-4 ratings, that was interesting, makes me wonder how far off current OS models are from human annotator , since the difference was not very large."}, "weaknesses": {"value": "While I am largely positive on the general premise of the works and some of the experiments, I have significant issues with the evaluation which prevent me from recommending acceptance at this stage. \n\nOnly one model and one benchmark is evaluated, I would like to see at least two for each. \n\nNo error bars are presented, I understand the cost of running things multiple times but I think this is a necessity where each things should at least report standard errors. \n\nAs far as I can tell no valiantly PPO is reported, RLHF style value function, this is an important baseline to see if the generative process of the PRM is actually really important. \n\nI would kind of like to see some minimal analysis on the data required to start up the PRM, similar to [1]\n\nOverall i am flexible with my score but I think there are significant things missing for me to raise my score to an acceptance. \n\n[1] https://arxiv.org/pdf/2507.04103"}, "questions": {"value": "I wonder how other OS models behave as labelers (more so than humans to be honest). \n\nIs there any qualitative analysis that can be done on the reasoning of the PRM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3ikiHAN552", "forum": "9hM4YRMhfT", "replyto": "9hM4YRMhfT", "signatures": ["ICLR.cc/2026/Conference/Submission2533/Reviewer_8a8L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2533/Reviewer_8a8L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762104315266, "cdate": 1762104315266, "tmdate": 1762916269681, "mdate": 1762916269681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}