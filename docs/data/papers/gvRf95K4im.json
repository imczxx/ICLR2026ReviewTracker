{"id": "gvRf95K4im", "number": 9472, "cdate": 1758123649364, "mdate": 1759897718501, "content": {"title": "K-Prism: A Knowledge-Guided and Prompt Integrated Universal Medical Image Segmentation Model", "abstract": "Medical image segmentation is fundamental to clinical decision-making, yet existing models remain fragmented. They are usually trained on single knowledge sources and specific to individual tasks, modalities, or organs. This fragmentation contrasts sharply with clinical practice, where experts seamlessly integrate diverse knowledge: anatomical priors from training, exemplar-based reasoning from reference cases, and iterative refinement through real-time interaction. We present $\\textbf{K-Prism}$, a unified segmentation framework that mirrors this clinical flexibility by systematically integrating three knowledge paradigms: (i) $\\textit{semantic priors}$ learned from annotated datasets, (ii) $\\textit{in-context knowledge}$ from few-shot reference examples, and (iii) $\\textit{interactive feedback}$ from user inputs like clicks or scribbles. Our key insight is that these heterogeneous knowledge sources can be encoded into a dual-prompt representation: 1-D sparse prompts defining $\\textit{what}$ to segment and 2-D dense prompts indicating $\\textit{where}$ to attend, which are then dynamically routed through a Mixture-of-Experts (MoE) decoder. This design enables flexible switching between paradigms and joint training across diverse tasks without architectural modifications. Comprehensive experiments on 18 public datasets spanning diverse modalities (CT, MRI, X-ray, pathology, ultrasound, etc.) demonstrate that K-Prism achieves state-of-the-art performance across semantic, in-context, and interactive segmentation settings. Code will be released upon publication.", "tldr": "We propose K-Prism, a unified segmentation framework that integrates semantic priors, in-context examples, and interactive feedback into a dual-prompt MoE decoder, achieving state-of-the-art performance across 18 diverse medical imaging datasets.", "keywords": ["Medical Image", "Image Segmentation", "Universal Model", "Prompt Integration", "Knowledge-Guided"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3faa3891dcbc3cef8511d1118d5a65472dab4f67.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work propose K-Prism, which is a unified framework for medical image segmentation that innovatively integrates three knowledge paradigms into a single model: semantic priors (Mode-1), in-context exemplars (Mode-2), and interactive feedback (Mode-3). Its core technique encodes heterogeneous knowledge into a unified dual-prompt representation (1D sparse prompts for \"what\" and 2D dense prompts for \"where\") , which is then processed by a Mixture-of-Experts (MoE) decoder with dynamic routing. Experiments on 18 public, multi-modal datasets demonstrate SOTA performance across all three segmentation settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Significant Problem: Directly addresses the long-standing problem of model fragmentation in medical segmentation.\n\n- Novel Architecture: The proposed \"dual-prompt\" and MoE decoder design is elegant, successfully unifying three heterogeneous knowledge sources.\n\n- Comprehensive Experiments: Extensively validated on 18 public datasets covering diverse modalities.\n\n- SOTA Performance: Outperforms existing state-of-the-art methods in all three tracks: semantic, in-context, and interactive segmentation."}, "weaknesses": {"value": "- 2D Limitation: The model currently processes 3D medical data (like CT/MRI) based on 2D slices, which loses 3D contextual information.\n- In-context Bottleneck: For complex multi-organ segmentation tasks, the in-context mode (Mode-2) performs noticeably worse than the semantic mode (Mode-1).\n- Computational Cost: The MoE architecture, while effective, introduces a higher parameter count and computational overhead, potentially hindering real-time deployment."}, "questions": {"value": "- 3D Extension: What are the main challenges in extending this framework to 3D? Specifically, can the dual-prompt and MoE mechanisms be efficiently scaled computationally?\n\n- In-context Bottleneck: Could using k-shot (k>1) references in Mode-2 help alleviate the performance drop observed in multi-organ tasks?\n\n- MoE Expert Specialization: What specific functions have the MoE experts specialized in? (e.g., are some experts dedicated to positive clicks and others to negative clicks?)\n\n- Reference Quality: How sensitive is the performance of Mode-2 (in-context) to the quality of the reference exemplar (e.g., inaccurate mask) or domain shift (e.g., different scanner)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Oyz7Wt9moE", "forum": "gvRf95K4im", "replyto": "gvRf95K4im", "signatures": ["ICLR.cc/2026/Conference/Submission9472/Reviewer_GsDa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9472/Reviewer_GsDa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893755406, "cdate": 1761893755406, "tmdate": 1762921060882, "mdate": 1762921060882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes K-Prism, a unified medical image segmentation framework that integrates three knowledge sources—semantic priors, few-shot in-context examples, and interactive user feedback. Using a dual-prompt and Mixture-of-Experts design, K-Prism flexibly adapts across tasks and modalities. Tested on 18 public datasets, it achieves state-of-the-art performance in semantic, in-context, and interactive segmentation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Unified design integrating semantic, in-context, and interactive segmentation.\n2. Consistent performance gain across diverse datasets and modalities."}, "weaknesses": {"value": "The novelty of the paper is somewhat unclear. It appears to mainly combine existing ideas from interactive segmentation models (e.g., MedSAM) and in-context segmentation models (e.g., UniverSeg). It is not fully evident what new conceptual or methodological insight the proposed framework contributes beyond this integration. The authors should clarify the unique innovation or theoretical advancement that distinguishes K-Prism from prior works.\n\nThe three proposed “knowledge paradigms” — semantic priors, in-context examples, and interactive feedback — are somewhat loosely defined and may not convincingly qualify as distinct forms of knowledge. They are more simply like working mode instead of knoledge.The authors should better justify why these modes are conceptualized as “knowledge sources”.\n\nThe motivation for adopting a Mixture-of-Experts (MoE) design is insufficient. In medical segmentation, computational efficiency is typically not a critical bottleneck, so replacing a dense model with MoE seems unnecessary and possibly superficial."}, "questions": {"value": "1. Authors should consider adding MedSAM for comparison\n2. As shown in Table 2 and 3, the improvement is limited"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xn4cdPgHX5", "forum": "gvRf95K4im", "replyto": "gvRf95K4im", "signatures": ["ICLR.cc/2026/Conference/Submission9472/Reviewer_3nT9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9472/Reviewer_3nT9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932186645, "cdate": 1761932186645, "tmdate": 1762921060564, "mdate": 1762921060564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose K-Prism, a unified segmentation framework capable of performing (1) standard segmentation on in-class labels, (2) in-context learning–based segmentation using reference exemplars, and (3) interactive segmentation through user feedback. This is achieved by learning a 1-D sparse prompt that encodes what to segment and a 2-D dense prompt that encodes where to attend, which are jointly processed through a Mixture-of-Experts decoder for dynamic, task-aware feature routing. Experiments conducted on 18 public datasets spanning CT, MRI, X-ray, pathology, and ultrasound demonstrate that K-Prism achieves state-of-the-art performance across all three segmentation paradigms, with generalization to unseen classes and external datasets."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. K-Prism unifies semantic, in-context, and interactive segmentation within a single framework through a novel design that leverages 1-D and 2-D prompts to encode complementary information about what and where to segment. \n2. The use of a Mixture-of-Experts decoder is well-motivated, enabling task-aware specialization and effective fusion of different prompt types while maintaining a shared representation space.\n3. The paper is well-written and easy to follow, with clear methodological explanations and thorough experimental validation that make the work accessible and reproducible."}, "weaknesses": {"value": "1. While K-Prism performs well on in-distribution and cross-dataset tests, its accuracy drops notably for unseen anatomical classes, suggesting limited generalization beyond trained label types.\n2. The paper does not compare against nnU-Net trained under equivalent conditions—either as a single generalist model trained on all datasets or as individual task-specific models. Such comparisons are important to validate that the proposed unification truly offers an advantage over established supervised baselines."}, "questions": {"value": "1. The model shows noticeably lower accuracy on unseen anatomical classes compared to in-distribution results. Could the authors provide an analysis explaining why this gap occurs?\n2. To better contextualize the proposed framework, can the authors include experiments comparing K-Prism to nnU-Net trained (a) jointly on all datasets as a generalist model, and (b) separately on each dataset as task-specific specialists? \n3. The paper mentions that all competing methods are trained on the same dataset, but it is unclear how interactive baselines (e.g., SAM2, MultiverSeg) were handled. Were these models trained or fine-tuned from scratch, or initialized from existing pretrained weights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VobQXM9KKM", "forum": "gvRf95K4im", "replyto": "gvRf95K4im", "signatures": ["ICLR.cc/2026/Conference/Submission9472/Reviewer_MRgi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9472/Reviewer_MRgi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965086836, "cdate": 1761965086836, "tmdate": 1762921059886, "mdate": 1762921059886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents K-Prism, a unified framework for medical image segmentation that systematically integrates three clinically relevant knowledge paradigms: (i) semantic priors learned from annotated datasets, (ii) in-context knowledge from few-shot reference examples, and (iii) interactive feedback from user inputs.\nThe key design lies in a dual-prompt representation—1-D sparse prompts encoding “what to segment” and 2-D dense prompts encoding “where to attend”—together with a Mixture-of-Experts (MoE) decoder for dynamic, task-aware routing.\nK-Prism supports three operational modes (semantic, in-context, interactive) within a single architecture and achieves state-of-the-art results across 18 datasets spanning CT, MRI, X-ray, ultrasound, pathology, and fundus imaging. The model demonstrates strong generalization to external and unseen-class datasets and enables efficient refinement through user interaction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a clear and practical gap: the fragmentation of current medical segmentation models across different knowledge paradigms.\n- The dual-prompt + MoE design is conceptually elegant and technically sound, enabling unified training and inference across tasks without architectural modifications.\n- Experiments are comprehensive and convincing, covering 18 public datasets, multiple modalities, and both in-distribution and cross-dataset evaluations.\n- K-Prism achieves consistent SOTA performance across semantic, few-shot, and interactive settings, with particularly impressive efficiency in interactive segmentation (lowest NoC90 and NoC95).\n- Ablation and analysis are thorough, confirming the contribution of each component and the dynamic specialization of experts.\n- The work has practical clinical potential, as it can streamline deployment and annotation pipelines."}, "weaknesses": {"value": "- All experiments are conducted on 2D slices, while many clinical workflows require 3D volumetric segmentation.\n- The method relies on the quality and availability of reference examples for in-context mode, which may constrain performance in data-scarce or noisy scenarios.\n- Discussion on model interpretability or failure cases is relatively limited."}, "questions": {"value": "- Could the framework be extended to 3D volumetric segmentation without major architectural changes?\n- How significant is the computational cost of the MoE decoder compared to standard transformer-based decoders (e.g., in inference latency)?\n- Is there any strategy to automatically balance or select the optimal knowledge source (semantic / in-context / interactive) during inference, especially when user feedback is unavailable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "si7V8h1zjm", "forum": "gvRf95K4im", "replyto": "gvRf95K4im", "signatures": ["ICLR.cc/2026/Conference/Submission9472/Reviewer_ZEBx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9472/Reviewer_ZEBx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966075420, "cdate": 1761966075420, "tmdate": 1762921059461, "mdate": 1762921059461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}