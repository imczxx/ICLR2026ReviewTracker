{"id": "gr0Z4kWUdC", "number": 10868, "cdate": 1758183651996, "mdate": 1759897623618, "content": {"title": "How Do You Watch a Movie? HourHDVC: Hour-Long Hierarchical Dense Video Captioning", "abstract": "While existing Dense Video Captioning (DVC) research has shown promise for short video clips, current approaches struggle with hour-long videos due to a critical lack of datasets that capture long-term context and models capable of managing extensive temporal dependencies. To address these challenges, we introduce Hierarchical Dense Video Captioning (HDVC), a novel task designed for long-form videos that involves both scene-level and video-level narrative captioning. For this task, we propose HourHDVC, a new dataset providing comprehensive annotations for hour-long videos. We also present LOng COntext memory-based hierarchical dense video captioning (LOCO), an end-to-end model explicitly designed to manage extensive temporal dependencies by modeling the scene-to-narrative structure inherent in HDVC. LOCO leverages a two-tier memory system, Context-aware Memory and Long-term Context Memory, to maintain narrative coherence across extended durations. Experiments on HourHDVC demonstrate that LOCO establishes strong baselines for the HDVC task, while highlighting the remaining challenges of modeling long-form video narratives.", "tldr": "We present HourHDVC, a benchmark and model for hour-long dense video captioning that leverage scene-to-narrative structure and long-context memory, setting a new standard for coherent, paragraph-level video descriptions.", "keywords": ["Dense Video Captioning", "Hour-long Video Understanding", "Paragraph Captioning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1ed10c3e26edbc9d69bbfaae7f41953b162dadc.pdf", "supplementary_material": "/attachment/9d114e2fc76ee32754d2ba066d8a8b30c06f61b8.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Hierarchical Dense Video Captioning (HDVC), a new task requiring both timestamped scene descriptions and an overall narrative for hour-long videos. To support HDVC, the authors present the HourHDVC dataset with 498 richly annotated movie videos, and propose LOCO—an end-to-end model that uses dual memory mechanisms to maintain coherence across long videos. LOCO significantly outperforms existing baselines on both scene and narrative captioning, as measured by the proposed ConSimLlama3 metric, which shows strong human correlation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This work demonstrates significant strengths across multiple dimensions. It introduces a novel and well-defined task (HDVC) supported by the first large-scale, richly annotated dataset (HourHDVC) for hour-long videos. The proposed LOCO model is technically innovative, featuring an elegant two-tier memory design that effectively handles long-range dependencies, which is validated through extensive ablations. LOCO achieves substantial and consistent empirical improvements, establishing a new state-of-the-art on both scene and narrative captioning. The claims are further solidified by thorough evaluation, including strong baseline comparisons and a careful validation of both the dataset quality and the proposed ConSimLlama3 metric. Finally, the paper itself is exceptionally well-written and clearly structured, enhancing its overall impact and accessibility."}, "weaknesses": {"value": "This work has several notable limitations. The HourHDVC dataset relies on GPT-4 for annotation, which may introduce LLM-specific biases or errors despite its high overall quality. The dataset and method are also primarily validated on movie content, leaving their generalizability to other long-form video genres less certain. Computationally, the complexity and cost of LOCO's multi-component architecture are not analyzed, raising scalability concerns. While the proposed ConSimLlama3 metric is well-validated, the near-zero BLEU scores and reliance on this single LLM-based metric suggest the evaluation scope could be broader. Finally, the main text lacks an explicit discussion of these limitations, which would provide a more balanced presentation."}, "questions": {"value": "1. The dataset pipeline relies on movie transcripts and audio descriptions. How would the approach handle videos without transcripts or with noisy speech (e.g. background music, crowd noise)? Could the model still perform well if only visual features are available?\n\n2. The paper uses a three-act structure for narrative generation. How robust is this scheme across different movie genres (e.g. non-fiction documentaries or experimental films)? Did the authors try alternative narrative segmentations?\n\n3. What is the computational cost of training and inference with LOCO, compared to baseline models? Can it run in reasonable time on a single long video?\n\n4. For scene segmentation and captioning, how sensitive is the performance to the quality of the initial transcript? (E.g. if ASR transcripts have errors, does LOCO degrade gracefully?)\n\n5. Do the authors plan to release HourHDVC publicly? If so, will they provide the raw video clips, transcripts, or only the annotations?\n\n6. Could the authors provide qualitative examples of scene-level captions and the generated narrative, especially highlighting cases where LOCO succeeds or still fails? This would help illustrate the model’s behavior.\n\n7. Has LOCO been compared to using a large end-to-end multimodal model (e.g. GPT-4V) directly on the long video, perhaps with sliding windows? It would be interesting to see how a powerful MLLM stacks up with external memory.\n\n8. In the controlled experiment on noise (Table 6), the precision is relatively low (~52%). Can the authors clarify whether missing or extra scene segments affect narrative quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OPTFDMiVGe", "forum": "gr0Z4kWUdC", "replyto": "gr0Z4kWUdC", "signatures": ["ICLR.cc/2026/Conference/Submission10868/Reviewer_jmSY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10868/Reviewer_jmSY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736611103, "cdate": 1761736611103, "tmdate": 1762922083161, "mdate": 1762922083161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Previous work struggles with hour-long videos because existing datasets fail to capture long-range context, and current models are not designed to handle such extensive temporal dependencies.\nTo address this limitation, the authors introduce a new task called Hierarchical Dense Video Captioning (HDVC) for long-form video understanding. HDVC requires generating both scene-level captions and a global narrative caption for the entire video. They also release HourHDVC, a new dataset that supports this task.\nIn addition, the authors propose LOCO (LOng COntext memory-based hierarchical dense video captioning), a model that uses a two-tier memory system, Context-aware Memory and Long-term Context Memory, to maintain coherent narratives over extended time spans."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors introduce a new task and dataset with multi-level annotations, providing scene-level and video-level narrative captions. This dataset is likely to be valuable for future research on long-form video understanding.\n2. They present a new model, LOCO, which outperforms existing methods on the HourHDVC dataset.\n3. They also propose a new evaluation metric, ConSim, designed to measure how well models capture contextual information."}, "weaknesses": {"value": "1. The paper lacks sufficient detail regarding the LOCO architecture. In particular, it is unclear which backbone models are used for each component (e.g., the temporal encoder), whether the framework generalizes to alternative model choices, and what hyperparameters are used during training. More implementation details would help clarify the method and improve reproducibility.\n2. It is also unclear why the performance of Scene Dense Video Captioning decreases when context-aware memory is added in Table 4. Since this component is intended to enhance the modeling of long-range context, additional explanation is needed to understand why it does not consistently improve performance. \n3. It seems like the comparison setup is unfair for Table 2: the proposed method is trained in-domain, whereas some baselines may not be. \n4. More detail is needed about the human refinement of the evaluation set. For example, it is unclear what specific changes annotators made, how frequently edits were applied, who the annotators were, and what procedures were used to ensure annotation quality.\n5. The description of the ConSim metric is also incomplete. How ConSim is computed? Is it a form of LLM-as-a-judge–style evaluation?\n6. The paper does not report results from other methods on the Ego4D-HCap dataset (Table 5). Could you also add the results of different methods?"}, "questions": {"value": "1. It appears that the context-aware memory is not updated across scenes, and instead each scene has its own separate context-aware memory. Is this correct? If so, if the video gets long, wouldn't it need large storage?\n2. In line 290, when you mention parameter sharing, does this mean that the same LoRA weights are shared, or that it is the same base model with different LoRA adapters for each component? The figure only shows LoRA applied to the video-narrative captioning module, so clarification would be helpful.\n3. In Equation (2), I assume M represents memory, but it would improve readability if all symbols were explicitly defined in the text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ob6OwAzYNd", "forum": "gr0Z4kWUdC", "replyto": "gr0Z4kWUdC", "signatures": ["ICLR.cc/2026/Conference/Submission10868/Reviewer_1GSZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10868/Reviewer_1GSZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864340104, "cdate": 1761864340104, "tmdate": 1762922082729, "mdate": 1762922082729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HourHDVC, a benchmark for hierarchical dense video captioning of hour-long videos, and proposes the LOCO model, which leverages a two-tier memory system to generate both scene-level and narrative-level captions. The dataset is constructed using LLM-generated captions, with human refinement for evaluation splits. Experiments show that LOCO achieves strong results on this benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. System integration: The benchmark and model are well-integrated and address a technical gap in long-form video captioning.\n2. Experimental thoroughness: The experiments are comprehensive for the proposed benchmark, including ablations and human evaluations."}, "weaknesses": {"value": "1. Motivation unclear: Given that LLMs can already generate high-quality captions for long videos (as shown by the authors’ own quality control), the necessity of this benchmark and the proposed modeling innovations is questionable. The paper does not convincingly show that existing methods (e.g., clip-wise captioning and merging) are insufficient.\n2. Algorithmic novelty: The modeling techniques are incremental adaptations of existing ideas, not fundamentally new algorithms.\n3. Synthetic data risks: Heavy reliance on LLM-generated captions may introduce artifacts or biases, which are not deeply analyzed.\n4. Generalization and practical impact: The work is evaluated only on the proposed benchmark, with limited exploration of broader applicability or real-world utility."}, "questions": {"value": "1. Necessity of the task: Can the authors provide evidence or analysis showing that simple baselines (e.g., sampling short clips, captioning with LLMs, and merging) are insufficient for hour-long video captioning? What unique challenges does HourHDVC address that cannot be solved by existing methods?\n2. Real-world motivation: Are there real-world applications or user studies that demonstrate a need for hierarchical dense captioning of hour-long videos?\n3. Comparison to simple pipelines: How does the proposed approach compare to a pipeline that uses LLMs to caption short segments and then merges or summarizes them?\n4. Synthetic data quality: Are there systematic errors or artifacts in LLM-generated captions that affect model training or evaluation?\n5. Generalization: Can the model and dataset be applied to other domains or tasks, or is the contribution limited to the proposed benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DsBhYZP0S1", "forum": "gr0Z4kWUdC", "replyto": "gr0Z4kWUdC", "signatures": ["ICLR.cc/2026/Conference/Submission10868/Reviewer_AHWX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10868/Reviewer_AHWX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920168027, "cdate": 1761920168027, "tmdate": 1762922082400, "mdate": 1762922082400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors address a key limitation in Dense Video Captioning (DVC), i.e., the scale from short, minute-level clips to hour-long videos like movies. This failure is attributed to a lack of both appropriate benchmarks and models capable of handling extensive temporal dependencies and narrative context. To tackle this technical gap, the authors introduce a task, named Hierarchical Dense Video Captioning (HDVC), which requires both (i) scene-level dense captions with timestamps and (ii) a single video-level narrative paragraph for hour-long videos. To this end, HourHDVC, a new large-scale dataset, is proposed with comprehensive, hierarchical annotations for both scene-level and narrative-level captioning across hour-long video content. A new framework, LOCO (LOng COntext memory-based hierarchical dense video captioning), an end-to-end hierarchical DVC model, is also proposed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is, in general, well written in a good structure and is easy to follow.\n\n2. The proposed problem is indeed a key challenge in dense video captioning, i.e., most prior DVC datasets target seconds–minute clips. HourHDVC fills an important gap (hour-scale, paragraph annotations, hierarchical structure). The dataset statistics and motivation are clearly presented.\n\n3. The proposed LOCO model is well designed to directly tackle the new HDVC task. The two-tier memory system is a reasonable solution that explicitly maps to the problem's two-level (intra-scene and inter-scene) context, which the authors clearly define. The inclusion of a gradient flow mechanism across windows is a technically sound detail to improve learning over long sequences.\n\n4. A new metric is proposed, which the authors claim is a better fit for the proposed task compared to traditional metrics(BLEU, CIDER). The proposed model has good performance in general. It is also good that lots of ablation tests are included."}, "weaknesses": {"value": "1. The dataset is built upon the MAD-v2. This dataset is based on movie audio descriptions, which specifically focus on visual information for the visually impaired and intentionally do not overlap with dialogue. Hence, the \"ground truth\" annotations, even when processed by an LLM, are likely heavily biased toward visual actions rather than narrative points driven by dialogue, which is a critical part of movie narratives.\n\n2. Although the authors include a user study for data quality, which compares LLM-generated captions to expert-refined LLM-generated captions, this is just to check the refinement step. It is still quite possible that the LLM pipeline introduces systematic biases in the benchmark already, which also indicates that the scalability and quality of the annotation pipeline are fundamentally limited by the accuracy, objectivity, and generalization of large language models, especially under challenging visual or audio ambiguity.\n\n3. The proposed metric ConSimLlama3 uses an LLM evaluator (Llama3/GPT-4o style). While the human correlation numbers look good, LLM-based metrics can be unstable depending on model/version/prompt."}, "questions": {"value": "Please check the \"Weaknesses\" section and the other questions as follows:\n\n1. I wonder what exact LLM and temperature/chain-of-thought steps were used for ConSimLlama3? Can you provide an ablation where ConSim is computed with several models (e.g., Llama, GPT-style) to show metric robustness?\n\n2. As mentioned, the proposed ConSimLlama3 applied an LLM. If LLM has pre-existing knowledge of the evaluation movies (which is quite possible), how can you be sure it isn't inflating the scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ezsIleVgQl", "forum": "gr0Z4kWUdC", "replyto": "gr0Z4kWUdC", "signatures": ["ICLR.cc/2026/Conference/Submission10868/Reviewer_WWFW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10868/Reviewer_WWFW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762103294298, "cdate": 1762103294298, "tmdate": 1762922082031, "mdate": 1762922082031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}