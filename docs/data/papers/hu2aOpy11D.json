{"id": "hu2aOpy11D", "number": 23654, "cdate": 1758346798384, "mdate": 1759896802983, "content": {"title": "Generalist Scanner Meets Specialist Locator: A Synergistic Coarse-to-Fine Framework for Robust GUI Grounding", "abstract": "Grounding natural language queries in graphical user interfaces (GUIs) presents a challenging task that requires models to comprehend diverse UI elements across various applications and systems, while also accurately predicting the spatial coordinates for the intended operation. To tackle this problem, we propose GMS: Generalist Scanner Meets Specialist Locator, a synergistic coarse-to-fine framework that effectively improves GUI grounding performance. GMS leverages the complementary strengths of general vision-language models (VLMs) and small, task-specific GUI grounding models by assigning them distinct roles within the framework. Specifically, the general VLM acts as a \"Scanner\" to identify potential regions of interest, while the fine-tuned grounding model serves as a \"Locator\" that outputs precise coordinates within these regions. This design is inspired by how humans perform GUI grounding, where the eyes scan the interface and the brain focuses on interpretation and localization. Our whole framework consists of five stages and incorporates hierarchical search with cross-modal communication to achieve promising prediction results. Experimental results on the ScreenSpot-Pro dataset show that while the \"Scanner\" and \"Locator\" models achieve only $2.0\\%$ and $3.7\\%$ accuracy respectively when used independently, their integration within \\textit{GMS} framework yields an overall accuracy of $35.7\\%$, representing a $10 \\times$ improvement. Additionally, GMS significantly outperforms other strong baselines under various settings, demonstrating its robustness and potential for general-purpose GUI grounding.", "tldr": "", "keywords": ["Synergistic", "GUI Grounding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c495f88d1b45e365ae5c936220b4c55ee515c583.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the GUI grounding problem, one of the key challenges for GUI agent models. The authors propose a coarse-to-fine grounding approach that integrates two modules, a scanner and a locator. Experiments are conducted on GUI grounding benchmarks across diverse UI environments, and three different models are employed for the scanner component."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The proposed method is clear and the motivation is intuitive.\n- Improvments specially using Gemini as Scanner model seems clear."}, "weaknesses": {"value": "- The major concern lies in the novelty of the work. The coarse-to-fine grounding concept has already been proposed in prior work (R-VLM ACL 2025), yet this paper neither cites nor discusses the differences. The overall idea appears to overlap significantly, except for the use of a different vision-language model (VLM) for the scanner component.\n\n- Considering the performance gap between Qwen2.5-VL and Gemini, the improvement observed when using Gemini as the scanner seems rather trivial and expected, as it primarily reflects the stronger base model rather than methodological innovation.\n\n- The experiments are limited to GUI grounding benchmarks and do not include evaluations on broader GUI agent tasks. It remains unverified whether the proposed approach generalizes to realistic interactive settings such as AITW, Multimodal-Mind2Web, or MiniWob."}, "questions": {"value": "Please see the Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wYlpNJLnsJ", "forum": "hu2aOpy11D", "replyto": "hu2aOpy11D", "signatures": ["ICLR.cc/2026/Conference/Submission23654/Reviewer_5eGP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23654/Reviewer_5eGP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994602278, "cdate": 1761994602278, "tmdate": 1762942748721, "mdate": 1762942748721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GMS, a training-free multi-agent framework that emulates human-like grounding by assigning complementary roles to generalist and specialist models, achieving substantial gains without additional fine-tuning. Extensive experiments demonstrate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental evaluation is generally comprehensive.\n\n2. The method section is fairly complete, and the approach yields performance improvements."}, "weaknesses": {"value": "1. The paper is not clearly written. For example, the motivation in the introduction is somewhat confusing and lacks a clear statement of the specific problem being addressed.\n\n2. The novelty is limited. It lacks comparisons with similar test-time scaling approaches, such as [1].\n\n3. Some experiments are still missing. For instance, the method section introduces several components to implement the full agent— including the verification mechanism and multi-agent debate, yet the overhead attributable to each stage (token consumption and time cost) does not appear to be quantified.\n\n[1] Visual Test-time Scaling for GUI Agent Grounding. ICCV 2025."}, "questions": {"value": "1. The method section mentions “verification” multiple times; however, the ablation study does not clearly specify which verification component is being evaluated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2002nXJLKb", "forum": "hu2aOpy11D", "replyto": "hu2aOpy11D", "signatures": ["ICLR.cc/2026/Conference/Submission23654/Reviewer_CZan"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23654/Reviewer_CZan"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994968609, "cdate": 1761994968609, "tmdate": 1762942748490, "mdate": 1762942748490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GMS (Generalist Scanner Meets Specialist Locator), a training-free, multi-agent framework for grounding natural language queries in graphical user interfaces (GUIs). The method integrates a generalist vision-language model (Scanner) for broad semantic perception with a specialist grounding model (Locator) for fine-grained coordinate prediction in a coarse-to-fine manner. Experiments on the ScreenSpot-Pro benchmark show that while each model performs poorly in isolation, their integration boosts grounding accuracy (e.g., from below 4% to 36% for OS-Atlas-4B)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a straightforward and modular design that separates generalist perception and specialist localization, making the overall idea easy to follow and potentially adaptable to other multimodal grounding tasks.\n2. The experiments, though limited in scope, demonstrate that integrating a generalist and a specialist model can lead to performance improvements."}, "weaknesses": {"value": "*Method\n1. The contributions are limited. The proposed framework mainly combines two existing large models (i.e., a generalist vision-language model and a specialist grounding model) through iterative refinement. While the design is functional, it feels largely engineering-driven and lacks deeper methodological insight to justify why such a combination leads to substantial improvements.\n2. Deploying two large models within a GUI agent pipeline raises significant concerns about cost and inference efficiency. However, the paper does not provide any quantitative analysis of runtime, memory consumption, or scalability. Without such an evaluation, it is difficult to assess the practicality of the proposed approach for real-world GUI interaction systems.\n\n*Experiments\n\nExperiments are conducted only on a single dataset (ScreenSpot-Pro), which limits the generalizability of the conclusions. As there are some related datasets such as ScreenSpot and ScreenSpot-v2, it would be better to include results on these benchmarks to demonstrate the superiority.\n\nIn general, I do not think this paper is ready for publication yet, as both the methodology and experiments remain relatively underdeveloped."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iWsiDJN8HR", "forum": "hu2aOpy11D", "replyto": "hu2aOpy11D", "signatures": ["ICLR.cc/2026/Conference/Submission23654/Reviewer_E2qj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23654/Reviewer_E2qj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004745997, "cdate": 1762004745997, "tmdate": 1762942748164, "mdate": 1762942748164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper primarily focuses on the GUI Grounding task. The authors propose a coarse-to-fine framework, which includes several modules: Hierarchical attention allocation, Iterative focus refinement, Cross-modal verification, Multi-agent consensus, Adaptive resolution enhancement. The experiments and comparisons were only conducted on the screenspot-pro dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The writing is easy to follow."}, "weaknesses": {"value": "1. The proposed method requires multiple rounds of image cropping into smaller sub-regions for the MLLM to perform repeated inference and decision-making for a single Grounding task (i.e., the Scanner module).\n  - First, the strategy of cropping the original image based on a preset grid is query-unaware. The MLLM receives only local information and lacks global context, which is likely to impair the decision quality.\n  - Second, the time and economic cost associated with these repeated inferences are significant and cannot be ignored, especially since a complex task often requires multiple grounding steps to complete. The proposed method, therefore, appears highly impractical.\n2. The proposed method relies on numerous heuristic hyperparameters, such as the $125 \\times 125$ pixels (L232-233), the $3 \\times 3$ subgrid (L260-261), and the $\\times 5$ upscale factor for $C^*$, among others. The paper must include an ablation study to validate the necessity and rationality of these specific choices.\n3. Based on the experiments, the Locator agent module selected DiMo-GUI as the grounding model. The paper needs to justify why only this specific model was chosen and whether other potential grounding models could be used instead.\n4. The proposed method was only evaluated on a single benchmark (screenspot-pro). It is essential to validate its performance on multiple benchmarks, such as ScreenSpot-v2 and OSWorld, to demonstrate generalizability."}, "questions": {"value": "Please find the question in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9roJXKkr1s", "forum": "hu2aOpy11D", "replyto": "hu2aOpy11D", "signatures": ["ICLR.cc/2026/Conference/Submission23654/Reviewer_3PJU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23654/Reviewer_3PJU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005751184, "cdate": 1762005751184, "tmdate": 1762942747857, "mdate": 1762942747857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}