{"id": "P8klg2CLMG", "number": 14761, "cdate": 1758243192908, "mdate": 1759897350632, "content": {"title": "Neural Logistic Bandits", "abstract": "We study the problem of neural logistic bandits, where the main task is to learn an unknown reward function within a logistic link function using a neural network. Existing approaches either exhibit unfavorable dependencies on $\\kappa$, where $1/\\kappa$ represents the minimum variance of reward distributions, or suffer from direct dependence on the feature dimension $d$, which can be huge in neural network–based settings. In this work, we introduce a novel Bernstein-type inequality for self-normalized vector-valued martingales that is designed to bypass a direct dependence on the ambient dimension. This lets us deduce a regret upper bound that grows with the effective dimension $\\widetilde{d}$, not the feature dimension, while keeping a minimal dependence on $\\kappa$. Based on the concentration inequality, we propose two algorithms, NeuralLog-UCB-1 and NeuralLog-UCB-2, that guarantee regret upper bounds of order $\\widetilde{O}(\\widetilde{d}\\sqrt{\\kappa T})$ and $\\widetilde{O}(\\widetilde{d}\\sqrt{T/\\kappa})$, respectively, improving on the existing results. Lastly, we report numerical results on both synthetic and real datasets to validate our theoretical findings.", "tldr": "We propose provable neural network-based UCB‐style algorithms for logistic contextual bandits.", "keywords": ["Logistic bandits", "neural network", "neural bandits", "upper confidence bound"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a29dff4e3b2b2619e68bdfa000139180f02dffb.pdf", "supplementary_material": "/attachment/030403bcf4aef2af88c020645b62e28141251c64.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes NeuralLog-UCB, a framework for neural logistic bandits. NeuralLog-UCB utilizes a tail inequality for self-normalized vector-valued martingales to bypass a direct dependence on the ambient dimension. Theoretical results show an improvement in the cumulative regret bound for existing neural logistic bandits. Empirical results validate these findings using both synthetic and real-world datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is generally well-written and clearly explains the algorithm's key aspects.\n- The theoretical results are clear with mathematical notation, assumptions, statements, and proof of the proposed method.\n- The theory is also confirmed by experimental evidence, e.g., cumulative regrets in Fig.1 and 2."}, "weaknesses": {"value": "- The proposed method requires pre-defining and fine-tuning several parameters, such as a **known** minimum variance of reward distributions $1/\\kappa$, a reward upper bound $R$, norm parameter $S$, exploration rate $\\lambda$, etc. This overall raises concern about its robustness and usefulness in practice.\n- Bersterin-type self-normalized inequality for vector-valued martingales is not new in bandits literature [1,2]. This reduces the soundness of this paper regarding theoretical contributions. Note that I still appreciate your analysis in the logistic bandits setting.\n- Empirical results may need to be improved with a standard synthesis dataset in neural-contextual bandits (e.g., [3]) and more UCI datasets."}, "questions": {"value": "1. In Alg.1 & 2, while $\\lambda$, $\\nu$ are updated by Eq.4 and Eq.2 (5), why do the authors fix these with the best parameter values using grid search in experiments?\n\n2. How realistic of the assumption about a large enough width of the neural network $m$ in Condition C.2.? In practice, $T^4K^4L^6$ is often much higher than $m$, where $T$ is the time horizon, $K$ is the number of arms, and $L$ is the number of layers.\n\n---\nReferences:\n\n[1] Zhou et al., Nearly minimax optimal reinforcement learning for linear mixture markov decision processes, CLT, 2021.\n\n[2] Faury et al., Improved optimistic algorithms for logistic bandits, ICML, 2020.\n\n[3] Zhou et al., Neural Contextual Bandits with UCB-based Exploration, ICML, 2021."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PklnVHa30M", "forum": "P8klg2CLMG", "replyto": "P8klg2CLMG", "signatures": ["ICLR.cc/2026/Conference/Submission14761/Reviewer_igeJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14761/Reviewer_igeJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589503724, "cdate": 1761589503724, "tmdate": 1762925117797, "mdate": 1762925117797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies neural logistic bandits, a setting where an agent must choose actions over time and observe binary (Bernoulli) feedback whose probability is modeled by a logistic link on top of an unknown nonlinear reward function approximated with an overparameterized neural network. The authors develop a new Bernstein-style self-normalized concentration inequality that is both variance-adaptive (it avoids relying on worst-case logistic variance constants like \\kappa) and data-adaptive (it depends on an effective dimension \\tilde{d}, not the full parameter dimension), and use it to design two UCB-based algorithms, NeuralLog-UCB-1 and NeuralLog-UCB-2, with regret bounds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Improved variance dependence for neural logistic bandits\n\nThe paper introduces two UCB-style algorithms (NeuralLog-UCB-1 and NeuralLog-UCB-2) that achieve regret bounds with better dependence on the logistic variance parameters, improving over prior $\\tilde{O}(\\kappa \\tilde{d}\\sqrt{T})$ in neural logistic bandits.\n\n- Data-adaptive exploration design\n\nNeuralLog-UCB-2 estimates per-arm uncertainty using a learned, curvature-weighted design matrix rather than a crude global worst-case variance. This yields tighter confidence sets and, in experiments, significantly lower cumulative regret.\n\n- Empirical support\n\nOn both synthetic nonlinear rewards and real datasets (MNIST, mushroom, shuttle), the proposed methods — especially NeuralLog-UCB-2 — consistently achieve the lowest regret among strong baselines, suggesting practical benefit and not just a theoretical improvement."}, "weaknesses": {"value": "Weaknesses\n-  Removing d is not new\n\nThe paper highlights that its regret bounds no longer depend directly on the ambient dimension d, but instead on the effective dimension \\tilde{d}. However, this “$d \\rightarrow \\tilde{d}$” replacement has already been standard since NeuralUCB / NeuralTS–style analyses in neural bandits, where NTK-based arguments control regret via an effective log-det complexity term rather than the raw parameter dimension. \n\n- The regret bounds are not strictly stronger than recent neural bandit results\n\nThe improvements mainly target neural logistic bandits, and still rely on strong NTK-style assumptions such as Assumption 4.1 (well-conditioned NTK / non-degeneracy). Some recent neural bandit work [1] weakens or removes such assumptions, so the structural assumptions here are not obviously milder.\n\n[1] Robust neural contextual bandit against adversarial corruptions\n\nThe bounds still scale with a global effective dimension \\tilde{d} defined using all arms / all rounds. Recent work [2] has started reducing or even removing this global \\tilde{d} factor in the leading term.\nIn short, while the paper improves the $\\kappa$ or $\\kappa^*$ dependence for logistic neural bandits, it is not a general domination of existing neural bandit theory.\n\n[2] Contextual bandits with online neural regression\n\n- Regret still depends polynomially on S\nBoth main theorems include factors like $S^2 \\tilde{d}\\sqrt{\\kappa T}$ and even higher-order terms in S (up to $S^5$ in NeuralLog-UCB-2). S measures how far the optimal parameter is from initialization."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hzZXZH13sH", "forum": "P8klg2CLMG", "replyto": "P8klg2CLMG", "signatures": ["ICLR.cc/2026/Conference/Submission14761/Reviewer_F7Bv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14761/Reviewer_F7Bv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742357781, "cdate": 1761742357781, "tmdate": 1762925117346, "mdate": 1762925117346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the neural logistic bandit problem and proposes two algorithms with high-probability regret guarantees. The first algorithm achieves a regret of order $\\tilde O(\\tilde d\\sqrt{\\kappa T})$, improving the $\\kappa$-dependence over prior neural logistic work (notably Verma et al. 2025); the second algorithm achieves a bound of order $\\tilde O(\\tilde d\\sqrt{T/{\\kappa^*}})$, i.e. a second order bound where the leading term is independent of the worst-case $\\kappa$. To make these results dimension-adaptive (i.e. depend on the effective dimension $\\tilde d$ rather than the ambient parameter dimension $p$, the paper introduces a new variance- and data-adaptive self-normalized martingale inequality, which can be viewed as combining the variance-aware idea of Faury et al. (2020) with a Freedman-style argument to remove the explicit $d$ tern in the confidence radius. This new concentration tool is then plugged into a NeuralUCB-style NTK analysis to obtain the final regret bounds. \n\nIn general I would give a weakly accept. The reasons are: strong and interesting concentration tool + improved regret guarantees in a neural logistic setting; assumptions and width requirement are heavy, and the algorithm needs a non-observable radius $S$, but these are common pain points in NTK-based bandit papers, and the contribution is still significant for ICLR."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Technically solid and interesting concentration result.\nThe new Bernstein-type, self-normalized martingale inequality is elegant and, as far as I can check, correct. It simultaneously achieves (i) variance adaptivity (using the true logistic variance instead of the worst-case $1/\\kappa$) and (ii) data adaptivity (via the log-det / effective-dimension term), thereby bridging the gap between the variance-aware inequality of Faury et al. (2020), which still kept an explicit $d$, and the neural logistic analysis of Verma et al. (2025), which still carried a $\\sqrt{\\kappa}$ factor. The proof via Freedman is clearly laid out.\n\n2. Improved regret guarantees.\nThe paper gives, to my knowledge, the first regret bounds for neural logistic bandits that (a) match the best-known $\\kappa$-dependence from the logistic bandit literature and (b) replace the ambient dimension by the effective dimension $\\tilde d$. This is a meaningful step because naïvely lifting logistic-bandit analysis to the neural setting reintroduces dependence on the number of parameters $p$, which is exactly what the authors avoid here.\n\n3. Nontrivial adaptation of previous techniques.\nEven though the analysis is clearly inspired by Faury et al. (2020), Abeille et al. (2021), and Jun et al. (2021), extending those ideas to the NTK-based neural setting, and making the variance-adaptive matrix compatible with the evolving, data-dependent regularization $\\lambda_t$, is nontrivial. The proofs for Algorithms 1 and 2 look technically careful."}, "weaknesses": {"value": "1. Algorithm requires knowing $S$.\nThe algorithm needs the learner to input the norm parameter $S$ (Condition 4.4: “set $S$ as a norm parameter satisfying $S \\ge \\sqrt{2 h^\\top H^{-1} h}$”) but this quantity is defined in terms of the true latent reward vector $h$ and the NTK matrix over all future contexts — not something the learner can observe or compute online. So in practice this is an assumption, not an implementable step. The paper itself later notes that removing the dependence on $S$ is an open direction.\n\n2. Network width requirement is unrealistically large.\nThe NTK part uses the usual “wide network $\\Rightarrow$ linearization” argument, but the concrete requirement on $m$ grows polynomially in $T$, $K$, and $L$ (see Condition C.2), which makes the guarantee more asymptotic than finite-sample. In realistic settings one would not take $m = \\Omega(T^4 K^4 L^6 / \\lambda_H^4)$ just to run a bandit algorithm.\n\n3. Empirical section is simplified.\nTo make things practical, the experiments diagonalize the design matrices and simplify baselines (dropping projection steps that are actually needed for the theory), so the empirical comparison is a bit informal and doesn’t fully test the theoretical contribution in a high-dimensional or large-$m$ regime. I won’t hold this strongly against the paper, but it means the experiments currently play more of an illustrative than a validating role.\n\n4. Computational cost is high.\nAs written, the algorithm repeatedly computes gradients $g(x;\\theta_0) \\in \\mathbb{R}^p$ and maintains design matrices / inverses in the $p$-dimensional tangent space. Even with diagonal approximations (as in the experiments) this is at least linear in $p$, and the theoretical version is more like $O(p^2)$–$O(p^3)$ per update because of the log-det / inverse terms. This contrasts with some recent logistic bandit algorithms whose per-round cost is only $O(d^2)$ or even linear in $d$.\n\n5. Minor but real issue in Theorem 3.1 statement.\nIn the current draft, the statement/proof of Theorem 3.1 seems to interchange $N$ and $L$ in a couple of places — e.g., the bound uses $|x_t|_2 \\le N$ but later the displayed inequality has an $L$-like symbol in the multiplicative constant. This is probably a notational slip, but it should be fixed since Theorem 3.1 is a main technical contribution."}, "questions": {"value": "1. I'm not familiar with the NTK analyses but I find it intuitively hard to understand why would the requirement on the width of the network would scale with the number of layers $L$ and the horizon size. Intuitively I believe if one has wide networks in each layer then the number of layers could be small. My intuition for NTK-style bandits is that once the NTK is “frozen” at initialization, the width should depend on the richness of the context set, not on the time horizon — but here you have a dependence like $m \\gtrsim T^4 K^4 L^6$ (Condition C.2).\n2. The train–update subroutine uses a number of gradient descent steps $J$ that depends on $T$ (and on $\\lambda_t$). This makes the algorithm non-anytime. Is this purely for proof convenience (i.e., could we make it depend on current time $t$ instead and keep the same order), or is there a real obstacle to making it fully online? What do Zhou et al. (2020) and Verma et al. (2025) do here?\n\n3. Could you provide a short intuition for why your Freedman-based argument manages to keep both the variance term (like in Faury et al. 2020) and the log-det term (like in Abbasi-Yadkori et al. 2011) without reintroducing an explicit $d$? Right now the proof is clear but a bit “black-boxy”; adding intuition would make it more readable. \n\n4. Can you compare the per-round (or per-update) computational complexity of NeuralLog-UCB-1/2 with (i) Faury et al. (2020), (ii) Abeille et al. (2021), and (iii) Verma et al. (2025)? Right now the proposed method seems to require operations in $\\mathbb{R}^p$, while part of the motivation was to avoid explicit dependence on $p$ — so a short discussion of how to implement the algorithm efficiently (e.g., diagonal / low-rank approximations, periodic updates, NTK feature caching) would strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tjE34ZWm9A", "forum": "P8klg2CLMG", "replyto": "P8klg2CLMG", "signatures": ["ICLR.cc/2026/Conference/Submission14761/Reviewer_NggP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14761/Reviewer_NggP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132287972, "cdate": 1762132287972, "tmdate": 1762925116996, "mdate": 1762925116996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the neural logistic bandits problem, where the rewards are binary and depend on a latent, non-linear reward function that is estimated using neural networks.\nExisting methods depend on the minimum reward variance and feature dimension, making them impractical for high-dimensional settings. This paper proposes a novel Bernstein-type concentration inequality for self-normalized vector-valued martingales and then uses it to derive regret upper bounds that scale with the effective dimension (a data-dependent, typically much smaller quantity than the feature dimension) rather than the full network size. \n\nThe authors propose two new algorithms, NeuralLog-UCB-1 and NeuralLog-UCB-2, both achieving improved or matching best-known regret rates for neural logistic bandits, while fully removing dependence on worst-case variance and feature dimension. The authors also empirically validated the superior performance of the proposed algorithms on synthetic and real-world datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**Strengths of the paper:**\n1. This paper considers the neural logistic bandits problem, where an unknown latent non-linear reward function is estimated using neural networks.\n\n2. This paper proposes a novel Bernstein-type concentration inequality for self-normalized vector-valued martingales and then uses it to derive tighter regret upper bounds.\n\n3. The authors propose two new algorithms, NeuralLog-UCB-1 and NeuralLog-UCB-2, both achieving improved existing regret bounds, closing the gap between theory and practice.\n\n4. The authors also validate the empirical performance of the proposed algorithms on synthetic and real-world datasets, showing superior cumulative regret performance over key baselines."}, "weaknesses": {"value": "**Weaknesses of the paper:**\n1. It is unclear how one can choose the right architecture of a neural network (NN) to estimate the underlying unknown reward function. If the NN architecture (too small or too large) is good enough for estimating the reward function, it may lead to mis-specification.\n\n2. The empirical results can also include Thompson sampling-based variants of the proposed algorithms. Also, the authors can mention the key challenges to integrating Thompson sampling with the proposed algorithms."}, "questions": {"value": "Please address the paper's weaknesses. I am open to changing my score based on the authors' responses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Since this work is a theoretical paper, I do not find any ethical concerns."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oMk7x1aYYZ", "forum": "P8klg2CLMG", "replyto": "P8klg2CLMG", "signatures": ["ICLR.cc/2026/Conference/Submission14761/Reviewer_tUn9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14761/Reviewer_tUn9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14761/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762967494175, "cdate": 1762967494175, "tmdate": 1762967494175, "mdate": 1762967494175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}