{"id": "sE8DCSJTzd", "number": 35, "cdate": 1756728181673, "mdate": 1763696388167, "content": {"title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward", "abstract": "This paper examines the exploration–exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: \\textit{spurious rewards}, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and \\textit{entropy minimization}, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.", "tldr": "", "keywords": ["Reinforcement Learning with Verifiable Rewards", "Group Relative Policy Optimization", "LLM Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/729f670ccd80f9010b8ee41cef9daea4f72db397.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the exploration-exploitation trade-off in RLVR for LLMs. It examine the paradoxical finding that random rewards can improve LLM reasoning performance. The core contributions are a theoretical and empirical reframing of the underlying mechanisms. This paper argues that clipping bias, previously thought to be the driver of these gains, provides a negligible learning signal. Instead, it demonstrates that clipping acts as a regularizer that deterministically reduces policy entropy. Through experiments on multiple model series, it shows that these gains are not exclusive to potentially contaminated models. Finally, this paper proposes a reward-misalignment model to explain why stronger models are more likely to benefit from this training regime."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper provides a  theoretical argument that refutes the prevailing view that upper-clipping bias is the primary driver of performance gains under random rewards.\n- This paper establishes a relationship between the application of clipping and the reduction of policy entropy, effectively reframing clipping as an entropy-minimizing regularizer rather than a source of learning signal.\n- This paper introduces a  probabilistic model  to explain which models are likely to benefit from random reward training. This framework provides an explanation for the empirical observation that stronger models tend to improve more."}, "weaknesses": {"value": "- Several key figures presenting experimental results are difficult to interpret due to missing legends and insufficient detail in captions. Figures 1, 2, 3 (Middle), and 4 all contain plots with multiple colored lines, but none include a legend to explain what each line represents. The review is left to assume they correspond to independent training runs, but this is not stated. The captions often lack sufficient detail. For example, the caption for Figure 1 does not explain that the faint lines in the Left and Middle panels are likely individual runs.\n- The paper's mathematical exposition is dense, and the notation is not always clear or consistent.  The definitions for the clipping analysis in Theorem 3.3  are imprecise. $N_t^{clip} := \\bar{r}_t \\hat{A}_t$ is defined, but $\\bar{r}_t$ itself is not. The proof of Proposition 3.2  appears to rely on unstated assumptions. The expansion $log~Z(h) = \\frac{1}{2}\\eta^2 + \\mathcal{O}(\\eta^3)$ requires $\\mathbb{E}[\\hat{A}] = 0$ and $\\mathbb{E}[\\hat{A}^2] = 1$ .\n- The paper's refutation of contamination does not address a key alternative. The low clipping activation rate ($\\approx 0.001$) for Qwen2.5-Math-7B (Remark 3.5) is used to show the clipping *signal* is negligible. However, this low rate could *itself* be an *effect* of contamination (i.e., the model is already so confident on the benchmark that $r_t$ rarely exceeds $1+\\epsilon$). This possibility is not discussed."}, "questions": {"value": "see Weaknesses and:\n\n- The explanation for the entropy *increase* in unclipped training (Fig 2 Left) relies on the policy being \"skewed\" (Remark 4.3 , Fig 5). Can this paper provide any empirical measure of the skewness of the initial Qwen2.5-Math-7B policy to confirm it is in the regime described by Remark B.7?\n\n- The paper implies (via Thm 4.2 and Fig 3 Right) that clipping should be detrimental on harder datasets like AIME because it forces entropy minimization. Did this paper tests the performance of *unclipped* training on the AIME dataset? This seems like a critical experiment to connect the paper's theoretical sections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fGFD4vqICi", "forum": "sE8DCSJTzd", "replyto": "sE8DCSJTzd", "signatures": ["ICLR.cc/2026/Conference/Submission35/Reviewer_tUmE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission35/Reviewer_tUmE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission35/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643753157, "cdate": 1761643753157, "tmdate": 1762915440670, "mdate": 1762915440670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of requested changes"}, "comment": {"value": "Again, we thank reviewers for your time to provide constructive suggestions. We provide a summary of requested changes from all reviewers across paper sections and additional experiments. Non-trivial changes are reflected in green highlights.\n\n---\n\n*Paper*\n\n- Section 2: we reorganize the preliminary section by (i) clearly define and introduce all the notations and setup and (ii) moving important lemmas and definitions that used in the later analysis to this section, for better clarity.\n  \n- Section 3: we add Definition 3.1 for a clearer definition and Remark 3.3 for better explanation of theorem to the readers.\n\n\n- Section 4: we add Remark 4.2 and Remark 4.4 to better explain their correspdoning theorems.\n\n\n- Appendix A.1: a systematic review of experiment results and rationale.\n\n\n- Appendix A.2: discussions over additional ablation analysis.\n\n\n- Appendix A.3: discussions over additional unclipped experiment and model skewness.\n\n---\n\n*Experiments*\n\n- Page 16-17, Figure 5-6: ablation on clipping ratio threshold $\\varepsilon$ and GRPO group size $G$.\n\n- Page 17, Figure 7: additional experiment on Qwen2.5-Math-7B with unclipped training over AIME set.\n\n- Page 8, Figure 3: additional experiments of DeepSeek-R1-Distill-Llama-8B over AIME set & more trials over QwQ-32B.\n\n- Page 18, Figure 8, further demonstration of policy skewness.\n\n- Page 10, Figure 4: additional experiment and figure presentation on the base models' initial strength and their corresponding final average improvement.\n\n---\n\n\nSincerely,\n\n\nAuthors of Paper 35"}}, "id": "uiQLc8GmHg", "forum": "sE8DCSJTzd", "replyto": "sE8DCSJTzd", "signatures": ["ICLR.cc/2026/Conference/Submission35/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission35/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission35/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763696510394, "cdate": 1763696510394, "tmdate": 1763696510394, "mdate": 1763696510394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the exploration–exploitation dynamics in reinforcement learning with verifiable rewards (RLVR), a method used for training reasoning-focused large language models. The authors analyze the roles of clipping, policy entropy, and spurious (random) rewards, arguing that prior explanations of performance gains under random rewards are incomplete. They propose theoretical analyses and empirical studies suggesting that clipping primarily reduces policy entropy, and that observed gains arise from reward–entropy interactions and model–data regime effects rather than contamination or genuine learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "•\tInteresting theoretical framing: The paper offers a formal treatment linking clipping bias and policy entropy, contributing to a better conceptual understanding of RLVR optimization dynamics.\n•\tRelevance to ongoing discourse: Given recent debates about “spurious reward” effects and entropy minimization in reasoning LLMs, this paper addresses a timely and relevant topic for the ICLR community.\n•\tClarity of theoretical results: The analytical sections (Theorems 3.3–4.2) are clearly presented and mathematically rigorous, providing useful insight into when and why entropy changes during GRPO updates."}, "weaknesses": {"value": "•\tExperimental design inconsistency: Although the paper claims “extensive experiments across multiple model families and sizes,” not all experiments are conducted uniformly; rather, different subsets of experiments use different models and data. This fragmented setup makes it difficult to assess the generality of the conclusions.\n•\tLimited empirical depth: The experiments do not convincingly support the claim of “reconciling conflicting reports” in the literature. The evaluation scope remains narrow, and results on random rewards or entropy minimization are mostly qualitative or limited to a few benchmarks (e.g., MATH500, AIME).\n•\tUnclear novelty of findings: The conclusion that entropy minimization acts as a regularizer, or that clipping primarily affects entropy, is intuitive and has been suggested in prior work. The contribution beyond existing studies remains unclear.\n•\tIncomplete closure of central questions: While the paper lists three core research questions, the final sections do not directly or conclusively answer them."}, "questions": {"value": "1.\tYou mention that experiments were run across several model families and sizes, but not all settings overlap. Could you clarify how these choices were made and whether comparable setups were used across models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "zkP9q0hiXE", "forum": "sE8DCSJTzd", "replyto": "sE8DCSJTzd", "signatures": ["ICLR.cc/2026/Conference/Submission35/Reviewer_1ufZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission35/Reviewer_1ufZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission35/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855235601, "cdate": 1761855235601, "tmdate": 1762915440556, "mdate": 1762915440556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the exploration–exploitation dynamics in reinforcement learning with verifiable rewards (RLVR), a method used for training reasoning-focused large language models. The authors analyze the roles of clipping, policy entropy, and spurious (random) rewards, arguing that prior explanations of performance gains under random rewards are incomplete. They propose theoretical analyses and empirical studies suggesting that clipping primarily reduces policy entropy, and that observed gains arise from reward–entropy interactions and model–data regime effects rather than contamination or genuine learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Interesting theoretical framing:** The paper offers a formal treatment linking clipping bias and policy entropy, contributing to a better conceptual understanding of RLVR optimization dynamics.\n- **Relevance to ongoing discourse:** Given recent debates about “spurious reward” effects and entropy minimization in reasoning LLMs, this paper addresses a timely and relevant topic for the ICLR community.\n- **Clarity of theoretical results:** The analytical sections (Theorems 3.3–4.2) are clearly presented and mathematically rigorous, providing useful insight into when and why entropy changes during GRPO updates."}, "weaknesses": {"value": "- **Experimental design inconsistency:** Although the paper claims “extensive experiments across multiple model families and sizes,” not all experiments are conducted uniformly; rather, different subsets of experiments use different models and data. This fragmented setup makes it difficult to assess the generality of the conclusions.\n- **Limited empirical depth:** The experiments do not convincingly support the claim of “reconciling conflicting reports” in the literature. The evaluation scope remains narrow, and results on random rewards or entropy minimization are mostly qualitative or limited to a few benchmarks (e.g., MATH500, AIME).\n- **Unclear novelty of findings:** The conclusion that entropy minimization acts as a regularizer, or that clipping primarily affects entropy, is intuitive and has been suggested in prior work. The contribution beyond existing studies remains unclear.\n- **Incomplete closure of central questions:** While the paper lists three core research questions, the final sections do not directly or conclusively answer them."}, "questions": {"value": "1.\tYou mention that experiments were run across several model families and sizes, but not all settings overlap. Could you clarify how these choices were made and whether comparable setups were used across models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "zkP9q0hiXE", "forum": "sE8DCSJTzd", "replyto": "sE8DCSJTzd", "signatures": ["ICLR.cc/2026/Conference/Submission35/Reviewer_1ufZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission35/Reviewer_1ufZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission35/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855235601, "cdate": 1761855235601, "tmdate": 1763111135732, "mdate": 1763111135732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how policy entropy influences reinforcement learning with verifiable rewards and whether spurious rewards can yield gains through clipping bias and model contamination. Through rigorous mathematical analysis supported by empirical experiments, the authors establish a direct connection between clipping and entropy, showing that clipping primarily reduces policy entropy. They argue that clipping does not provide a genuine learning signal and discuss when reduced entropy is beneficial. The authors introduce a reward-misalignment model to explain under which conditions spurious rewards enhance performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important and timely question in reinforcement learning for large language models.\n- Rigorous theoretical analysis linking clipping and entropy, extending prior accounts.\n- The reward-misalignment model offers a probabilistic explanation for the benefits of random rewards.\n- The paper is well motivated, interesting, and clearly presented."}, "weaknesses": {"value": "- The evaluations are concentrated on MATH500, and ablations on hyperparameters (e.g., clipping ratio, group size) are missing.\n- Some findings, while formalized, may only confirm intuitively expected behaviors (e.g., entropy minimization failing when incorrect trajectories are the peak of the distribution)."}, "questions": {"value": "- Why did you not stick to the same model for the experiments or use them all?\n- How do empirical results change with different clipping thresholds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zVEOP7dAvv", "forum": "sE8DCSJTzd", "replyto": "sE8DCSJTzd", "signatures": ["ICLR.cc/2026/Conference/Submission35/Reviewer_PiCo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission35/Reviewer_PiCo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission35/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995522092, "cdate": 1761995522092, "tmdate": 1762915440292, "mdate": 1762915440292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate seemingly paradoxical phenomena with RLVR: spurious rewards, which should decrease exploitation, and entropy minimization, which should increase exploitation, seem to both yield improvements for LLM's trained on MATH 500. The authors present a series of theoretical and empirical analyses to explain this phenomena. \n\nFirst, following previous hypotheses that clipping induces a bias towards high probability tokens under the initial model, they derive a bound on the magnitude of the effect of clipping on the gradient and perform empirical experiments to show that improvement still occurs under spurious rewards without clipping.\n\nSecond, they analyze the effect of clipping on policy entropy and present empirical results that Qwen2.5-Math-7B undergoes entropy increase with spurious rewards without clipping and entropy decrease with clipping on MATH 500, while both improve in validation performance. They discuss how entropy minimization may not always be useful, depending on the performance of the initial model, and perform empirical experiments with models of different sizes.\n\nThird, they analyze when spurious rewards can be useful in terms of the success rate of the true correctness rate of the initial policy. The basic intuition is that stronger base models will tend to have correct responses reinforced by spurious rewards, and they show experiments with a fine-tuned Llama3 model to show that improvement under spurious rewards becomes more likely as the model gets stronger."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors carefully investigate open questions related to RLVR and present interesting insights. The combination of empirical and theoretical analysis is compelling."}, "weaknesses": {"value": "1. The presentation of the motivation for each analysis and key takeaways could be more clear. A few general suggestion would be to explicitly state the goal of each analysis at the beginning of each section, to lead with the empirical results that the theory aims to explain, and to use more descriptive figure captions that discuss the key conclusion from each figure. At a few points reading through the paper, it was not clear to me why an analysis was being conducted and what conclusion the authors were justifying with the theoretical result.\n\n2. Notation is not always clear (e.g. r is used to represent importance ratio before defining in section 3)\n\n3. Overall, could do a better job of guiding the reader through the analysis as a story. Current flow seems to be 1) clipping does not explain learning from spurious rewards 2) clipping leads to entropy reduction and acts as a regularizer 3) entropy decrease from clipping does not always lead to improvements 4) models that have high success rates before RL are more likely to improve with spurious rewards. Motivating each of these points and connecting them back to the initial questions would be helpful"}, "questions": {"value": "Section 3 questions / comments:\n* what is the main takeaway from the upper bound on the clipping bias? The implication seems to be that the clip correction having a small magnitude makes it insignificant, but the later results show that it has meaningful effects (e.g. on entropy). The empirical results are much more clear to me (clipping not required for learning under spurious rewards)\n* r as the importance ratio is used in remark 3.1 without defining it (r up to this point had been reward?)\n* would be helpful to state why the law of clipping is important\n* would be helpful to state the key conclusion from the figure in the caption\n\nSection 4 questions / comments\n* it would be helpful to lead with the results in figure 2 to motivate the theoretical analysis (these are actually mentioned in the theory section motivation before they are presented, which is a bit confusing)\n* it would be helpful to discuss the implications of the derived expression for entropy change more explicitly (e.g. after theorem 4.2, explain how this indicates that training with clipping leads to entropy decrease). The notation is very dense and proof is in the appendix, so it is not obvious how to interpret this analysis as one reads through. Currently the paragraph after the proof refers to the empirical results, rather than the implications of the theorem\n* Remark 4.3 does a better job of discussing implications, but again it refers to the empirical results that haven't been introduced\n* section 4.3 felt a bit out of place given that the whole section up to this point had been about the relationship between clipping and entropy, not entropy and performance. Useful analysis, but it would be helpful to set it up a bit more for the reader (e.g. mention this question in the intro paragraph for this section)\n\nSection 5\n* Figure could be made more clear. If the key point is that initial model performance explains how likely a model is to improve with spurious rewards, a plot comparing initial model performance to improvement would be helpful"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2zqvO4ogtT", "forum": "sE8DCSJTzd", "replyto": "sE8DCSJTzd", "signatures": ["ICLR.cc/2026/Conference/Submission35/Reviewer_9GLK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission35/Reviewer_9GLK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission35/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111759075, "cdate": 1762111759075, "tmdate": 1762915440129, "mdate": 1762915440129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}