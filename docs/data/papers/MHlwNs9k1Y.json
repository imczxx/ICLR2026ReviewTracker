{"id": "MHlwNs9k1Y", "number": 4014, "cdate": 1757584447641, "mdate": 1763651244287, "content": {"title": "PoliCon: Evaluating LLMs on Achieving Diverse Political Consensus Objectives", "abstract": "Achieving political consensus is crucial yet challenging for the effective functioning of social governance. However, although frontier AI systems represented by large language models (LLMs) have developed rapidly in recent years, their capabilities in this scope are still understudied. In this paper, we introduce PoliCon, a novel benchmark constructed from 2,225 high-quality deliberation records of the European Parliament over 13 years, ranging from 2009 to 2022, to evaluate the ability of LLMs to draft consensus resolutions based on divergent party positions under varying collective decision-making contexts and political requirements. Specifically, PoliCon incorporates four factors to build each task environment for finding different political consensus: specific political issues, political goals, participating parties, and power structures based on seat distribution. We also developed an evaluation framework based on social choice theory for PoliCon, which simulates the real voting outcomes of different political parties to assess whether LLM-generated resolutions meet the requirements of the predetermined political consensus. Our experimental results demonstrate that even state-of-the-art models remain undersatisfied with complex tasks like passing resolutions by a two-thirds majority and addressing security issues, while uncovering their inherent partisan biases and revealing some behaviors LLMs show to achieve the consensus, such as prioritizing the stance of the dominant party instead of uniting smaller parties, which highlights PoliCon's promise as an effective platform for studying LLMs' ability to promote political consensus.", "tldr": "We introduce PoliCon, a benchmark constructed from 2,225 European Parliament's real-world deliberation records, which can evaluate the ability of LLMs to craft consensus within varying collective decision-making contexts and political requirements.", "keywords": ["Political Consensus Finding", "Collective Decision Making", "Benchmark on Societal Considerations"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3efbd17173ccb446112aee613bfbd40ed7f13b81.pdf", "supplementary_material": "/attachment/b422b1d9dcc9e19677ffd3016e68eae461c8d69e.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a benchmark to evaluate whether LLMs can draft consensus resolutions under realistic political-committee constraints. The dataset is built from European Parliament records (2009–2022) and, per the paper, comprises 2,225 cleaned issues, each with (issue, topic, debates, resolution, votes). Each evaluation scenario specifies: (1) a topic (5 coarse / 19 fine categories), (2) a political goal (passing a resolution with different voting rules; Rawlsianism; Utilitarianism), (3) participating parties, and (4) a power structure (randomized seat shares and, optionally, a veto party).\nSix LLMs are benchmarked. The benchmarked models often succeed in simple-majority settings but struggle with two-thirds, veto, security topics, and Rawls. They also exhibit partisan biases aligned with real EP voting patterns. The authors claim the judge correlates with “ground-truth” party votes with Pearson 0.83, and >72% of simulations fall within 2 SD of ground truth."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Evaluating LLMs on stakeholder consensus formation (with explicit power structures and collective-choice objectives) is a significant step beyond standard dialogue or persuasion tasks\n2. Tying tasks to EP issues, debates, and resolutions is realistic (and much better than synthetic setups)\n3. The topic taxonomy is broad and policy-relevant\n4. Detailed scraping pipelines, prompt templates, and task descriptions are provided (high evaluation transparency)\n5. Multiple models, tasks, and topic analyses expose nuanced failure modes (e.g., difficulty with Security and Rawls and specific biases)"}, "weaknesses": {"value": "1. LLM-as-judge with same-family system under test (GPT-4o judge vs GPT-4o candidate) risks systematic biases. A human (expert) verification, cross-judge, or calibration would be helpful.\n2. The core of the aggregation, i.e., u_i = JUDGE(· | background, s_i, resolution) remains undefined (clearly under-specified). How exactly is alignment and feasibility integrated into this score?\n3. It is not clear how the SD in Fig. 4 is exactly calculated. Can you please provide the exact error formula you are using?\n4. Obviously, some scenarios have been omitted from the analyses. Can you please state how exactly you proceeded in the selection phase? In addition, you state that there were 2,225 \"high-quality\", i.e., complete records, but in Table 6 (last row), you report more than 2,700 total valid records. Can you please clarify?\n5. Your vetoing rule seems to be contradictory. Does rejection by the vetoing party happen if support is under 60% or at least 60%? Please clarify.\n6. DeepSeek-R1 is used to summarize, clean, and to extract party stances. How do you avoid errors or potential hallucinations? Human-based quality assessment, preferably with inter-annotator checks would be helpful. \n7. It is not clear how the number of seats per party affects the veto robustness. For each issue, you could sample 20 to 50 random seat distributions (and veto assignments where applicable) and report the mean with CI per model and task.\n8. It is difficult to put the LLM performance into relation to a baseline because there is no simple (i.e., interpretable) baseline in the paper. Would it be possible to provide such a baseline? For example, if a proposal fails to meet the threshold (e.g., simple majority, 2/3), iteratively sacrifice the interests of the party that contributes least to passing (i.e., with smallest w_i). This would be a greedy strategy to pass the threshold."}, "questions": {"value": "See comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wUnlBoUTmj", "forum": "MHlwNs9k1Y", "replyto": "MHlwNs9k1Y", "signatures": ["ICLR.cc/2026/Conference/Submission4014/Reviewer_u21o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4014/Reviewer_u21o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396574179, "cdate": 1761396574179, "tmdate": 1762917135684, "mdate": 1762917135684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PoliCon, a new benchmark to evaluate how large language models (LLMs) handle political consensus building. The authors collect real parliamentary data to create diverse decision-making scenarios and define several types of consensus goals. They design an LLM-based evaluation framework that measures whether a model’s generated resolutions can reach agreement among simulated parties. Experiments show that current LLMs perform well on simple majority decisions but struggle with more complex or fairness-oriented objectives. The results also reveal clear topic sensitivity and political bias, indicating that existing models still lack balanced reasoning in social and political contexts. Overall, the work introduces a valuable and well-structured framework for assessing LLMs’ ability to reason about group decisions and fairness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a fresh and meaningful problem and it builds a solid and realistic benchmark using real parliamentary data, making the evaluation credible and grounded.\n\n2. The evaluation setup is thoughtfully designed and connects well with social choice theory.\n\n3. The experiments are thorough and provide clear insights into where current models perform well and where they fail."}, "weaknesses": {"value": "1. The evaluation still depends on another LLM as a judge, which could introduce hidden bias.\n\n2. The dataset only covers European Parliament data, so it might not generalize to other regions or political systems.\n\n3. There’s no human validation to confirm that the evaluation results truly match real consensus reasoning.\n\n4. Using LLMs in both dataset creation and evaluation could lead to subtle data leakage."}, "questions": {"value": "1. How do you make sure the LLM-based judge is fair and not favoring certain model types?\n\n2. Did you involve any human experts to check whether the model’s “consensus” decisions make real-world sense?\n\n3. How could this benchmark be extended to other political or cultural settings beyond Europe?\n\n4. What steps did you take to avoid overlap or leakage between LLM-generated data and evaluation content?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WwyvvV4FlG", "forum": "MHlwNs9k1Y", "replyto": "MHlwNs9k1Y", "signatures": ["ICLR.cc/2026/Conference/Submission4014/Reviewer_XoDH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4014/Reviewer_XoDH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683267352, "cdate": 1761683267352, "tmdate": 1762917134892, "mdate": 1762917134892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PoliCon, a novel benchmark for evaluating LLMs' ability to achieve political consensus objectives. Built from 2,225 European Parliament deliberation records (2009-2022), PoliCon tests LLMs across diverse collective decision-making scenarios incorporating political issues, goals, participating parties, and power structures. The benchmark defines five distinct political consensus objectives (Simple Majority, Two-thirds Majority, Veto Power, Rawlsianism, and Utilitarianism) and employs an evaluation framework based on social choice theory that simulates real voting outcomes. The authors evaluate six state-of-the-art LLMs, revealing significant performance gaps in complex consensus scenarios and uncovering inherent partisan biases."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. First benchmark specifically designed to evaluate LLMs' political consensus-building capabilities across diverse objectives\n2. 2,225 high-quality parliamentary records with extensive cleaning and processing, integrating multiple sources \n3. Diverse task settings: 15 different configurations combining party numbers, voting mechanisms, and political goals, creating 28,620 distinct scenarios"}, "weaknesses": {"value": "1. Using GPT-4o-mini as evaluator could introduce circular biases when testing other LLMs"}, "questions": {"value": "Could the authors clarify whether the GPT-4o-mini evaluator was temperature-controlled (e.g., deterministic setting) during scoring, and whether variance across seeds was analyzed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z0TY0AVhcW", "forum": "MHlwNs9k1Y", "replyto": "MHlwNs9k1Y", "signatures": ["ICLR.cc/2026/Conference/Submission4014/Reviewer_cGY7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4014/Reviewer_cGY7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814190538, "cdate": 1761814190538, "tmdate": 1762917134688, "mdate": 1762917134688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates LLMs' capabilities to generate consensus statements on political issues. The authors created a novel benchmark constructed from 2,225 high-quality deliberation records of the European Parliament over 13 years. With an automatic evaluation pipeline, this paper demonstrates that LLMs have varied capabilities on generating consensus statements and most of the models remain undersatisfied with complex tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper tackles an important topic \n2. The results are presented clearly"}, "weaknesses": {"value": "1. One of the major weaknesses of the paper is the reliability of the LLM-as-judge pipeline. LLM-as-judge has long been criticized for its generalizability, which applies to this paper as well. If I understand it correctly, the LLM-as-judge evaluation heavily relies on existing statements and voting data. However, it is really unclear whether this pipeline could create generalizable results for new statements. I believe this is the major issue of this paper. I'm happy to discuss with the authors about this.\n\n2. A second weakness is regarding the motivation and practical value of this work. In parliament deliberations, it is the deliberation process that leads to the final voting results instead of the statement itself. Therefore, this task setting may not reflect real-world settings where the consensus statements are actually needed."}, "questions": {"value": "please see weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nFsHqSSXn8", "forum": "MHlwNs9k1Y", "replyto": "MHlwNs9k1Y", "signatures": ["ICLR.cc/2026/Conference/Submission4014/Reviewer_CDif"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4014/Reviewer_CDif"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762315061325, "cdate": 1762315061325, "tmdate": 1762917134474, "mdate": 1762917134474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}