{"id": "i0Nno5MgXt", "number": 13038, "cdate": 1758212968069, "mdate": 1763746052938, "content": {"title": "Robust Multi-Objective Controlled Decoding of Large Language Models", "abstract": "We introduce Robust Multi-Objective Decoding (RMOD), a novel inference-time algorithm that robustly aligns Large Language Models (LLMs) to multiple human objectives (e.g., instruction-following, helpfulness, safety) by maximizing the worst-case rewards. RMOD formulates the robust decoding problem as a maximin two-player game between adversarially computed reward weights and the sampling policy, solvable through a Nash equilibrium. We demonstrate that this game reduces to a convex optimization problem to identify the worst-case reward weights, with the optimal sampling policy analytically derived. For practical applications, we propose an efficient algorithm of RMOD tailored for contemporary LLMs, introducing minimal computational overhead compared to standard non-robust Controlled Decoding methods. Experimental results across the range of popular alignment datasets with up to 10 objectives show the effectiveness of RMOD and its distilled version, consistently outperforming baselines in worst-case rewards and win rates.", "tldr": "We propose a multi-objective decoding algorithm that generates robust responses without knowing the exact weights over the objectives.", "keywords": ["Large Language Models", "Inference-time Alignment", "Robustness"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1539bc60bb9006e9637da464bfb978b6cc2a4d64.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper *“Robust Multi-Objective Controlled Decoding of Large Language Models”* extends the multi-objective decoding framework by introducing a robustness criterion. Instead of optimizing for a fixed set of user-specified objective weights, the proposed approach seeks a **decoding policy that remains optimal under the worst-case combination of weights**. This effectively ensures robustness to uncertainty or variability in user preferences at inference time. The formulation begins as a bilevel optimization problem, which is then relaxed into a single-level equivalent and further simplified into a closed-form solution, yielding a practical inference-time algorithm (tRMOD) that can efficiently generate robust aligned outputs without retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- **Comprehensive experiments:** The evaluation spans multiple datasets and baselines, demonstrating consistent improvements in worst-case rewards relative to previous approaches.  \n- **Clear theoretical pathway:** The derivation from the bilevel to single-level formulation, and eventually to a closed-form expression, is well presented and technically sound.  \n- **Practicality:** The reported latency and computational results show that the proposed RMOD algorithm is feasible for real-time inference, making it suitable for deployment in alignment-sensitive applications."}, "weaknesses": {"value": "**Philosophical concern about the problem setup:** While the mathematical formulation is solid, the motivation for a *universal robust inference-time policy* is debatable. Inference-time policies are typically **user-specific**, reflecting individual objective preferences. Each user can specify new weights or objectives, and the decoding process adapts accordingly. Designing a single robust policy to handle all users’ worst-case preferences might conflict with the personalized spirit of inference-time alignment.  \n\nThe paper would benefit from explicitly clarifying this conceptual distinction relative to *SitAlign* (Chehade et al., 2025), which also explores user-conditioned value weighting at inference time.  \n\nThe notion of “worst-case” robustness could be better motivated—what kind of real-world variability in user preferences is being modeled, and why is the minimax setup the most appropriate solution?  \n\n\n### Reference\n\n- Chehade, Mohamad, et al. *\"Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time.\"* arXiv preprint arXiv:2505.23729 (2025)."}, "questions": {"value": "1. How do you justify the need for a single robust policy that optimizes for the worst-case user preference, instead of learning user-conditioned inference-time policies as in SitAlign?  \n2. What practical scenarios justify assuming adversarial or worst-case user weights, given that preferences can typically be queried or provided interactively?  \n3. How sensitive is tRMOD’s performance to the assumed weight uncertainty set? Could over-conservatism lead to underperformance in typical (non-worst-case) users?  \n4. Can this framework be extended to dynamically update the robust policy as user preferences change, rather than precomputing a single static one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "axQSf5yaPs", "forum": "i0Nno5MgXt", "replyto": "i0Nno5MgXt", "signatures": ["ICLR.cc/2026/Conference/Submission13038/Reviewer_uMCr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13038/Reviewer_uMCr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851776372, "cdate": 1761851776372, "tmdate": 1762923771941, "mdate": 1762923771941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on multi-objective controlled decoding to perform robust inference-time alignment across multiple objectives. A key challenge with multiple, often conflicting, objectives is determining how to balance them effectively. To address this, the authors propose a maximin-style approach that maximizes the worst-case reward combination rather than relying on fixed or pre-learned weights. They formulate the decoding objective as a max–min problem and demonstrate a clean convex structure in the objective. The paper further introduces an efficient decoding algorithm that leverages value functions trained per objective, iteratively updating the weights during decoding to achieve robust alignment without requiring further retraining. Empirical results are promising across the alignment datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is a most natural extension to the Maxmin RLHF training time approaches which is interesting.\n- The observation of convexity of the objective due to the Logsumexp function is interesting (although similar has been leveraged in certain past works)\n- Multiple value functions are trained in CD-Fudge style with objective-specific rewards using a reference model which is crucial.\n- Experimental results are clean and shows gains with multiple objectives.\n- The observation and experimental setting by increasing the difficulty and increasing number of objective to worst case reward is particularly interesting"}, "weaknesses": {"value": "- One of the key weakness lies in the way how the value function are trained. The value function are trained with data from reference policy which means its restricted to V^pi and not V^pi* which will result in sub-optimality (check Transfer Q*, Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time  for reference) otherwise needs to meet certain coverage assumptions on the policy.]\n- Whats the data on which the value functions are trained? They have been generated under which policy? Its extremely crucial to understand.\n- How are the value functions trained? Do they use the linear head with the same backbone? Since, for conflicting objective these structure can have some issues? \n- Experimental comparisons with training time Maxmin objectives needs to be performed and highlighted why the gain is coming? or if not why it performs better. Due to Test-time exploration, inference objectives can do better than training also (Transfer Q^*). Will be helpful to provide details.\n\nOne important thing is to provide a detailed discussion of the value function training, challenges, uniqueness in this setting of multiple policies and coverage aspects."}, "questions": {"value": "Check in Weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H7RUUARI7t", "forum": "i0Nno5MgXt", "replyto": "i0Nno5MgXt", "signatures": ["ICLR.cc/2026/Conference/Submission13038/Reviewer_FxTQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13038/Reviewer_FxTQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868827531, "cdate": 1761868827531, "tmdate": 1762923771386, "mdate": 1762923771386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce Robust Multi-Objective Decoding (RMOD), an inference-time algorithm to align LLMs with multiple, competing objectives like helpfulness and safety. RMOD addresses the need to balance these objectives without manual tuning. To be specific, RMOD formalizes the problem as a maximin two-player game between the sampling policy and the objective weights, with the goal of maximizing the reward of the worst-performing objective. To support their hypothesis, the authors theoretically show that this maximin game reduces to a convex optimization problem over the weights, which can be solved to find the optimal weights. The optimal sampling policy is then analytically derived from these weights. Experiments on the HH, UltraFeedback, and ValuePrism datasets show that RMOD achieves a higher worst-case reward and win rate than baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written in general. The research problem of multi-reward alignment has been well-articulated to the reader. The proposed RMOD algorithm is based on a maximin formulation, which is a principled and well-motivated approach to this problem\n\n2. The theoretical analysis of formulating the max-min two-player game and its reduction to a single, convex optimization problem (Eq. 7) is interesting.\n\n3. Across three standard benchmarks: Helpfulness-Harmless, UltraFeedback, and ValuePrism, RMOD outperforms all the compared baselines (MOD, MO-DPO, CD)."}, "weaknesses": {"value": "1. The primary concern lies in the assumption that value functions are available for all objectives. Although Section 5.1 outlines the loss function used to train these value models, this requirement somewhat undermines the inference-time nature of the framework, as it necessitates additional training to obtain the value functions."}, "questions": {"value": "Please refer weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pE05gAaApY", "forum": "i0Nno5MgXt", "replyto": "i0Nno5MgXt", "signatures": ["ICLR.cc/2026/Conference/Submission13038/Reviewer_GQqL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13038/Reviewer_GQqL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989043459, "cdate": 1761989043459, "tmdate": 1762923770869, "mdate": 1762923770869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Robust Multi-Objective Decoding (RMOD), an inference-time alignment method that aims to robustly satisfy multiple reward objectives. RMOD formulates decoding as a max–min game between a sampling policy and an adversarial distribution over objective weights, and shows this game can be reduced to a convex optimization over the weight simplex with a closed-form best-response policy. For practical use, the authors design a blockwise controlled-decoding algorithm that approximates the equilibrium using K samples from a reference model and iteratively updated weights, plus a distillation scheme that trains a single policy to imitate RMOD outputs. Experiments on Anthropic HH, UltraFeedback, and ValuePrism show that RMOD improves worst-case reward and worst-case win rate over controlled decoding with fixed weights, Best-of-K, and several fine-tuning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. RMOD provides a principled max–min formulation of multi-objective decoding and reduces it to a convex optimization over objective weights, giving a clear theoretical underpinning for worst-case alignment at inference time.\n2. The blockwise controlled-decoding algorithm is practical: it reuses K samples from a reference model, introduces less than ~4.5% additional latency over standard controlled decoding on HH, and includes a distillation procedure that recovers most robustness benefits with a single-response policy.\n3. Experiments on HH, UltraFeedback, and ValuePrism consistently show improved worst-case rewards and worst-case win rates compared to fixed-weight controlled decoding, Best-of-K, and multi-objective fine-tuning baselines (GRPO, DPO, Rewarded Soups), while keeping average performance competitive."}, "weaknesses": {"value": "1. The method relies heavily on pre-trained reward models and learned value functions, but the paper provides little analysis of how calibration or misspecification across objectives affects the claimed robustness, which may limit the interpretability of “worst-case” guarantees.\n2. Although related robust alignment and group-robust RLHF methods are discussed, the empirical comparisons are restricted to non-robust baselines (fixed-weight controlled decoding and a few fine-tuning approaches), so it remains unclear how RMOD compares to other robustness-oriented techniques on similar datasets."}, "questions": {"value": "How sensitive is RMOD to the relative scaling and calibration of different reward heads, and have the authors tried alternative normalization schemes or diagnostics to detect when one objective dominates due to miscalibration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pRh1nQ7EN2", "forum": "i0Nno5MgXt", "replyto": "i0Nno5MgXt", "signatures": ["ICLR.cc/2026/Conference/Submission13038/Reviewer_k3rX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13038/Reviewer_k3rX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13038/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763119677614, "cdate": 1763119677614, "tmdate": 1763119677614, "mdate": 1763119677614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}