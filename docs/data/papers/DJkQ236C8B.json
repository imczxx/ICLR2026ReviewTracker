{"id": "DJkQ236C8B", "number": 1450, "cdate": 1756883547430, "mdate": 1759898208526, "content": {"title": "JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models", "abstract": "Audio Language Models (ALMs) have made significant progress recently.\nThese models integrate the audio modality directly into the model, rather than converting speech into text and inputting text to Large Language Models (LLMs). \nWhile jailbreak attacks on LLMs have been extensively studied, the security of ALMs with audio modalities remains largely unexplored. \nCurrently, there is a lack of an adversarial audio dataset and a unified framework specifically designed to evaluate and compare attacks and ALMs. \nIn this paper, we present JALMBench, a comprehensive benchmark to assess the safety of ALMs against jailbreak attacks. \nJALMBench includes a dataset containing 11,316 text samples and 245,355 audio samples with over 1,000 hours. \nIt supports 12 mainstream ALMs, 4 text-transferred and 4 audio-originated attack methods, and 5 defense methods. \nUsing JALMBench, we provide an in-depth analysis of attack efficiency, topic sensitivity, voice diversity, and architecture.\nAdditionally, we explore mitigation strategies for the attacks at both the prompt level and the response level.", "tldr": "", "keywords": ["AudioLM", "ALM", "Benchmark", "Dataset", "Jailbreak Attacks"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cec83c61cdf48f6489a56a79fd81d690430223de.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors:\n\n- create a benchmark for assessing the robustness of audio language models (ALMs) to text and audio jailbreaks\n- benchmark the attack success rates (ASRs) of various jailbreaks against various models\n- analyze ASRs along various dimensions of interest: efficiency, topics, languages\n- analyze t-SNE embeddings to see how architecture affects embeddings of attacks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Very solid, very thorough paper\n- I thought Figure 1 is cute\n- The models, attacks, and analyses seem sensible"}, "weaknesses": {"value": "Please begin with my line-by-line notes under \"Questions\".\n\nOverall, I think this is a solid and thorough paper. I see two primary weaknesses that, if modified, would lead me to increase my score.\n\n1. *All* of this uses a single judge model (gpt-4o-2024-11-20). I cannot find mention of another judge model (ideally, multiple other judge models) or any analysis of how accurate / reliable / trustworthy this particular judge model is. Thus, rather than understanding these results as describing the robustness of ALMs to jailbreaks, I must interpret the results as describing gpt-4o-2024-11-20's judgement of the robustness of ALMs to jailbreaks. This is especially important because the data that gpt-4o is judging is data designed to bamboozle (A)LMs, so we need to be certain that gpt-4o isn't similarly getting bamboozled! Fixing the bullet points under \"Questions\" and addressing this shortcoming would raise my score to an 8.\n\n2. To me, I feel like the paper doesn't offer many (any?) conceptual insights or information to drive decision-making. It's more akin to: Here are tables and figures reporting robustness scores of ALMs and also looking at tSNE embeddings. That's reasonable, but what would make this paper impactful is if the authors could provide actionable recommendations for ALMs and robustness. Depending on the significant of the insights, if the authors fix the bullet points under \"Questions\", add more judges and analyze the judges, and provide meaningful insights, I could be persuaded to raise my score to a 10."}, "questions": {"value": "## Title\n\n- Succinct, clear, accurate! And easy to pronounce. I like the title.\n\n## Abstract\n\n- nit: since ALM stands for Audio Language Model, it’s a bit odd to call Language Models “Large”; otherwise, why are the ALMs not ALLMs? I feel like we’ve moved to Language Models as LMs. But feel free to disagree.\n- The paper would be stronger if you could provide any insights or takeaways from evaluating models on JALMBench here in the abstract. I feel what separates mediocre benchmarks from great benchmarks is that great benchmarks don’t just provide scores but also drive insights and decision-making.\n\n## Section 1 Introduction\n\n- I like Figure 1! Very cute, very simple, very understandable. If I can make one minor suggestion, is “Dataset” a smaller size than “Statistics”? The two words appear to have different font sizes.\n- Figure 1: I’m unclear on why there are two arrows from Attack to ALM, but only one arrow from Harmful Query to ALM and from ALM to Defense?\n- Line 102: Numbers like “improve the average safety performance by 11.3%” mean little when the reader doesn’t have a strong prior over these numbers. It’d be helpful if you could add a sentence commenting on what this means practically. Does 11.3% mean that all attacks are blocked? That almost all attacks get through? etc.\n\n## Section 2 Related Work\n\n- No comments - looks good!\n\n## Section 3 JALMBench\n\n- nit: In general, I think methodology should be written in *past* tense. This is because the methodology is you (the authors) describing what you previously did.\n- Line 163: “We manually curate and deduplicate” -> How did the authors actually validate that the dataset was properly deduplicated? Performing this task by hand seems tedious and error prone. Why not use something like n-grams or semhash (https://github.com/MinishLab/semhash) or other deduplication methods?\n- Line 167: For comprehensiveness, it might be good to generate the speech using other methods/libraries e.g,. https://elevenlabs.io/. Otherwise, we don’t know whether the benchmark results are particular to Google TTS’s current model.\n- Line 192-193: For BoN, with 600 independent variations of each harmful audio sample, how do you determine the ASR? Do you take a similar definition that the attack is successful if any of the variations are successful?\n\n## Section 4 Evaluation\n\n- Line 215: “We use GPT-4o … as the judge model”. How did you validate the correctness/accuracy/reliability of the judge model? How consistent are its judgements with other candidate judge models? Just like my above bullet point, we don’t otherwise know whether the benchmark results are merely GPT-4o’s judgements.\n- Table 1: Without uncertainty quantification (e.g., 95% confidence intervals), it’s hard to know which (if any) of these values overlap\n- Table 1: I personally find tables difficult to intuitively understand, and strongly prefer visualizations. Here’s one way you could convert this into a nice figure that communicates exactly the same information. I’ll use matplotlib+seaborn terminology, but feel free to use whatever library you prefer: Have two columns, one for text modality, one for audio modality. The y values should be the attacks (THarm, ICA, DI, DAN, PAP, etc.), the x values should be the ASRs, and each hue should be a different model. You can achieve this easily using seaborn’s catplot https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot or implement it yourself\n- Lines 251-252: I think we need to be careful about comparing different methods if a method takes multiple attempts. Here, PAP is using 40 attacks and is considered successful if any attempt succeeds. If we want to make comparative statements about the efficacy of different methods, we should compare on a fair basis. I’m not sure what exactly that should look like, but the naive choice is that every method gets 40 tries and is considered successful if any attempt jailbreaks the model.\n- Figure 2: Shouldn’t PAP be \"PAP (N-40)\" in the legend?\n- Figure 2: For methods that take multiple attempts (PAP, BoN, maybe others), it might be useful to plot different “budgets”. How does BoN perform with N=10 attempts? 100 attempts?\n- Lines 306-307;324-326: I’m not sure I agree with this characterization. As best as I can tell, these statements are about the average success per category. But I’m not sure that _averages_ are what we care about. If Method A and Method B aren’t successful jailbreaks on a topic, but Method C is, then an attacker can succeed. It’s not as if an attacker must use the “average” attack.\n- Line 332: nit: there is a leading “,” by itself. It belongs on the line above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1exKDnKa2R", "forum": "DJkQ236C8B", "replyto": "DJkQ236C8B", "signatures": ["ICLR.cc/2026/Conference/Submission1450/Reviewer_k8Vy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1450/Reviewer_k8Vy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534105210, "cdate": 1761534105210, "tmdate": 1762915772546, "mdate": 1762915772546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents JALMBench, a large-scale benchmark designed to evaluate jailbreak vulnerabilities in Audio Language Models (ALMs). The authors compile a dataset of over 1,000 hours of audio and 11k text samples, covering 12 ALMs and 8 attack methods (4 text-transferred, 4 audio-originated), and 5 defenses. The benchmark measures attack success rate (ASR), efficiency, topic sensitivity, voice diversity, and architectural robustness. It also includes analyses of alignment gaps between modalities and evaluates prompt- and response-level mitigations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Comprehensiveness.** The scale and coverage of JALMBench may position the work as canonical testbed akin to JailbreakBench or AdvBench for text-based. It covers 12 ALMs, 8 distinct attacks, 5 defenses, and multidimensional analyses (efficiency, topic, voice, architecture).\n\n2. **Reproducibility.** The paper provides an anonymous GitHub repository that unifies interfaces for ALMs and defenses, documenting generation pipelines that reflect high implementation quality.\n\n3. **Empirical Findings.** a) Continuous-feature ALMs (e.g., SALMONN, LLaMA-Omni) exhibit cross-modal safety misalignment, whereas discrete tokenization (e.g., GLM-4-Voice) better preserves safety transfer. b) Audio-originated attacks (e.g., AdvWave) reach near ASR of 96%, showing ALMs are substantially less safe than text-only LLMs."}, "weaknesses": {"value": "1. **Limited Novelty Beyond Benchmark Construction**. The paper primarily aggregates existing attack/defense methods without introducing fundamentally new algorithms or method. The main novelty only lies on integration and scale rather than the concept. Also, cite existing ALM jailbreaking attacks that shows overlap in both evaluation and method to provide the difference of this work compared to existing ones [1, 2]\n\n2. **Overreliance on LLM-as-a-Judge**. The evaluation relies solely on GPT-4o judging outputs on a 5-point scale. Without human validation or inter-annotator calibration, this introduces potential bias and raises questions about consistency across attack modalities (text vs. audio). \n\n3. **Defense Evaluation Lacks Audio-Specific Techniques**. All defenses are adapted from text settings. The conclusion admits this limitation. However, this paper would be stronger if it proposed even a preliminary audio-native defenses (e.g., perturbation detection or phoneme-level filtering), considering that this is a Audio LM benchmark and that the paper's main novelty comes with the comprehensiveness.\n\n4. **Ambiguities in Methodology**\n\n- The paper does not specify how success thresholds (score greater than 4) were validated.\n\n- Details on human-recorded data IRB approval are given, but the sample size (6 speakers) is too small to support the claims about \"diversity\"\n\n-----\n*References*\n\n[1] Gupta, Isha, David Khachaturov, and Robert Mullins. \"\" I am bad\": Interpreting Stealthy, Universal and Robust Audio Jailbreaks in Audio-Language Models.\" arXiv preprint arXiv:2502.00718 (2025).\n\n[2] Roh, Jaechul, Virat Shejwalkar, and Amir Houmansadr. \"Multilingual and multi-accent jailbreaking of audio llms.\" arXiv preprint arXiv:2504.01094 (2025)."}, "questions": {"value": "1. What is the reason behind choosing those specific ALMs? Because some of those models were already considered broken or not safely aligned (e.g., SALMONN and LLaMA-Omni)\n\n2. Could the authors provide per-model refusal rates and false positives (\"benign\" classified as \"unsafe\" misclassification)?\n\n3. Could the authors provide any metric or quantitative evaluation of transcription capability of the selected ALMs? As a reviewer, I am curious whether these models accurately understood the input and rather they are not answering non-sense.\n\n4. What are the benign speech question answering capabilities of these chosen ALMs? \n\n5. What is the reason that for majority of the models text modality has a slightly / significantly lower ASRs compared to audio modality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TnDuuMy12b", "forum": "DJkQ236C8B", "replyto": "DJkQ236C8B", "signatures": ["ICLR.cc/2026/Conference/Submission1450/Reviewer_yBr8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1450/Reviewer_yBr8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761596091924, "cdate": 1761596091924, "tmdate": 1762915772349, "mdate": 1762915772349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce what seems to genuinely be the first benchmark for audio language model robustness that fully takes into account, in particular, the variety of model architectures (as well as ASR variation across language & accents). They evaluate 12 state of the art audio models (some where audio is converted into text tokens, some where the audio gets passed as audio tokens simultaneously with the text input) against 8 attacks and 5 defenses."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "I think overall the paper gives us valuable insight into the differences between discrete tokenization and continuous encoding approaches, though the defense evaluation could be strengthened with multi-objective analysis.\n\nI'm convinced that the paper's contribution is overall original, and in particular it seems important to be able to compare across different architectures for audio models. The benchmark itself is pretty extensive on the fronts of both the models and the attack methods evaluated. The paper overall represents substantial engineering effort to generate all of the audio attacks. \n\nThe empirical evidence of the t-SNE visualisation from fig 5 makes me quite confident in the author's conclusion that models trained on interleaved audio & text data are likely more robust against audio attacks. This seems like a valuable insight that model developers should take into account. \n\nThe authors also provide useful insights in identifying the different axes on which attacks are more or less successful, especially in the variability of robustness across categories of harm (e.g. misinformation requests being much more successful than explicit hate speech), and across the diversity of voices (e.g. how attacks with non-US accents seem to have higher ASR). The section on attack efficiency (by query length in seconds) is also useful in understanding the variability of different types of attack.\n\nThe paper is overall well written and clearly presented, and the claims made are well-calibrated. For example, they rightly frame their defense work as more preliminary."}, "weaknesses": {"value": "As above, I think the paper is overall quite useful. However, in the analysis & framing of the defense results, I'd be interested in the authors making some aspects a little clearer. In particular, while table 4 is helpful for understanding the raw ASR reduction from defense methods, I have to jump all the way to the appendix to get the full breakdown of ASR reduction vs capability retention. And, even then, the authors don't emphasise this tradeoff much in their analysis of the defenses, which makes it hard for me to know what the takeaways really are for defense methods. I also think a more impressive benchmark would have deeper things to say about other differences across defenses, like which attacks are particularly successful against which defenses/architectures and why, but perhaps this was out of scope. \n\nI understand and appreciate that the defense part of the paper is framed as more exploratory, but given that you have the data for this, I'd appreciate more depth of analysis on which defense methods are pareto-optimal across safety & utility. I think having something like a pareto plot and a few more comments here would help me draw more useful conclusions about the state of how to go about defending audio-llms.\n\nAnother significant area where I wish the paper had more depth was in the analysis of the differences between architectures. I appreciate that this is first large-scale analysis of how differences in architecture affect ALM robustness. Further, it's helpful that ground their hypothesis that discrete tokenization leads to more consistent safety in the resultant model by empirically visualising the last hidden layer’s representation in the backbone LLM with t-SNE. However, I think the authors could have gone into even more depth. First of all, it'd be helpful to get this visualisation for more models, rather than just one representative model for each architecture. Secondly, we only see inputs for one attack, PAP (admittedly the most successful). Overall, I'm left not totally confident that we understand where the gaps between different model architectures come from - especially in the differences between Quen2-audio and LLaMA-Omni (since the audio encoder are similar, which to my understanding was the hypothesis). It'd be helpful for the field to know why Qwen2-audio is more robust than LLaMA-Omni, and I'm not totally sure where the author's claim - that joint alignment objectives might be the reason - comes from.\n\nAlso, more minor, but I was a little surprised/confused by the multilingual attack results - it looks like ASR drops in table 10 for attacks in non-English languages. The authors claim this is due to \"reduced model proficiency due to limited training data in non-English languages\" - but I'm not sure if the authors really have the evidence to claim this. I'd expect it could be about as likely that the drop is due to imperfect translation, and I'd appreciate any extra detail that the authors could provide to support either hypothesis here."}, "questions": {"value": "1. Can you provide more commentary on, and perhaps a Pareto plot for, the safety vs. utility for defense methods? Which defenses are Pareto-optimal? I think this would make me satisfied with Section 5.\n2. Can you go into the architectural tradeoffs in more depth? What are the performance costs (latency, accuracy on benign tasks) of discrete vs. continuous architectures? Can you quantify the safety-performance tradeoff?\n3. What is the baseline ASR for vanilla harmful requests (no jailbreak) across different languages? This would address my minor confusion/concern about the claims around translation of attacks.\n4. More of a clarification, but can you provide more detail on the 6,000 GPU hour cost breakdown? What portion was dataset generation vs. evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BD6xXN3AE2", "forum": "DJkQ236C8B", "replyto": "DJkQ236C8B", "signatures": ["ICLR.cc/2026/Conference/Submission1450/Reviewer_5Ah4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1450/Reviewer_5Ah4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851682112, "cdate": 1761851682112, "tmdate": 1762915772115, "mdate": 1762915772115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}