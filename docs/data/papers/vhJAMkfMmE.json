{"id": "vhJAMkfMmE", "number": 21278, "cdate": 1758315731779, "mdate": 1759896931137, "content": {"title": "Latent Point Collapse on a Low Dimensional Embedding in Deep Neural Network Classifiers", "abstract": "The topological properties of latent representations play a critical role in determining the performance of deep neural network classifiers. In particular, the emergence of well-separated class embeddings in the latent space has been shown to improve both generalization and robustness. In this paper, we propose a method to induce the collapse of latent representations belonging to the same class into a single point, which enhances class separability in the latent space while making the network Lipschitz continuous.\nWe demonstrate that this phenomenon, which we call \\textit{latent point collapse} (LPC), is achieved by adding a strong $L_2$ penalty on the penultimate-layer representations and is the result of a push-pull tension developed with the cross-entropy loss function.\nIn addition, we show the practical utility of applying this compressing loss term to the latent representations of a low-dimensional linear penultimate layer.\nLPC can be viewed as a stronger manifestation of \\textit{neural collapse} (NC): while NC entails that within-class representations converge around their class means, LPC causes these representations to collapse in absolute value to a single point. As a result, the network improvements typically associated with NC—namely better generalization and robustness—are even more pronounced when LPC develops.", "tldr": "We characterize latent point collapse, a stronger manifestation of neural collapse that is is achieved by adding a strong $L_2$ penalty on the penultimate-layer representations making the network Lipschitz continuous.", "keywords": ["Classification", "Latent Space", "Neural Collapse"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f51666e78b1a6236eb0c4936298818bfa006ffd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Latent Point Collapse (LPC): by adding a strong L2 penalty on a low-dimensional linear penultimate layer, latent representations from the same class collapse to (nearly) a single point near the origin. The authors argue this is a stronger form than standard Neural Collapse, gives Lipschitz continuity guarantees, and improves robustness and generalization. They provide a theoretical explanation (cross-entropy pushes outward; L2 pulls inward), and experiments on CIFAR-10/100 and ImageNet-1K compare LPC variants to SCL, ArcFace, and stronger weight decay baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method is just CE + γ‖z‖² on a linear, low-dimensional penultimate layer, so it is easy to add in practice. \n- The paper explains why CE alone tends to unbounded feature norms and how a strong L2 term creates stable equilibrium points (collapse near origin).  \n- DeepFool robustness improves by orders of magnitude for LPC vs. non-LPC baselines on CIFAR, with lower input-gradient norms and higher class-separation ratios. \n- The narrow/wide variants and “L2 on backbone but no penultimate layer” controls are informative, showing the bottlenecked linear layer is important."}, "weaknesses": {"value": "- The paper explicitly positions very strong γ (e.g., γ≈10⁶) as crucial, but there is no systematic γ-sensitivity study (trade-off vs. accuracy/robustness; stability across datasets). This may limit generality if γ must be extreme.\n- The improvement on CIFAR-10 and CIFAR-100 is marginal in Table 2.\n- The overall mechanism—combining a strong L2 penalty with a linear bottleneck—is conceptually simple and similar to known ideas in feature normalization, Neural Collapse regularization, or weight decay.\n- All experiments are on balanced datasets (CIFAR-10/100, ImageNet-1K). Since the paper discusses generalization and collapse dynamics, it would be important to test on imbalanced or long-tailed datasets to see whether LPC remains stable.\n- Key training details (learning rate, batch size, normalization, early stopping, random seed averaging) should be summarized in the main text for transparency."}, "questions": {"value": "Main concern refers to the Weakness section. \n\nThe improvement of LPC on ImageNet-1K appears noticeably larger than on CIFAR-10/100 in accuracy (Table 2). Could the authors provide some insights or hypotheses on why the benefit scales up with dataset complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AMlqWfEypl", "forum": "vhJAMkfMmE", "replyto": "vhJAMkfMmE", "signatures": ["ICLR.cc/2026/Conference/Submission21278/Reviewer_3zcj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21278/Reviewer_3zcj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761769861444, "cdate": 1761769861444, "tmdate": 1762941666699, "mdate": 1762941666699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose penalizing the embedding norms in the penultimate layer of DNNs. They suggest that this causes the latent representations to separate better along class boundaries and therefore improves classification performance. They evidence this with experiments on cifar10, cifar100 and imagenet, along with theoretical derivations in the appendix."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents an interesting idea -- that significantly penalizing the latent embedding norms leads to improved class separation -- and performs thorough scientific evaluations of the properties of their method. I particularly appreciate that the authors did a sweep over many configurations of their method, including the LPC, LPC-Wide, LPC-SCL, etc. Finally, I commend the authors' thorough theoretical work in the appendix.\n\nWhile my list of weaknesses is long, I want to emphasize that I really like the ideas in this paper and the execution. My constructive criticisms are simply oriented at helping the authors improve their presentation and experiments to make the paper successful."}, "weaknesses": {"value": "The paper has two primary weaknesses in my view. First, the presentation spends a lot of time focusing on pieces which are not particularly relevant while missing pieces which *would* be relevant. Second, the paper makes confident claims without them being sufficiently validated.  I discuss these both below:\n\n## Presentation choices\n\nWhile the paper does a good job motivating the study into latent separability, the presentation is lop-sided in what it chooses to focus on:\n\n- In the main body of the paper, it is never explained why bringing the points to the origin would *also* cause them to separate. I see that the theoretical analysis in the appendix discusses this, but it leaves the main body quite incomplete. The main body should therefore have an extensive section (at least one page!) describing these theoretical results and pointing to the appendix. This section should give all the intuition for this (very unintuitive) result that adding a penalty on the norms *simultaneously* forces the latent representations to have large margins. For example, I found the summary in A.8 helpful towards this and wish it was in the main body with references to the corresponding theorems.\n  - I will also note that this result is sufficiently unintuitive that I actually still do not believe it... but this is a point for my second criticism.\n- It is quite surprising that the entire main body of the paper has 0 figures! These would make it much nicer to digest the ideas being presented and nicer to read the paper. I suggest having 4-5 figures at least for help with providing intuition and verifying claims.\n- The paper spends 3 pages on the introduction and related work, with related work then taking up additional space during the \"method\" section. I believe this is way too much space dedicated to the set-up. I am also unsure what the references to the information bottleneck add, nor why the authors choose to present 7 key contributions (of which 3 are simply relegated to the appendix!). This presentation of the paper's story makes it difficult to follow the authors' key results and takeaways.\n   - Nitpick: I would not call adding a linear layer with a penalty an architectural \"innovation\"...\n- The analysis of various architectural combinations takes up too much space in the main body of the paper, in my opinion: it is not useful towards actually understanding the main point that adding this penalty induces large margins. Specifically, the results across the various architectural components are quite consistent with one another, so the actual information being gained is \"LPC does X, not having LPC does Y\". Thus, I would suggest moving the architectural analysis into the appendix and simply reporting a single variant of the method for the main body of the paper, with a reference to the fact that many variants were tested.\n\nThese modifications to the presentation would make space for the figures and additional experiments that I believe the paper requires in order to be compelling and which are documented in the next section of this review.\n\n## Experimental gaps\n\nWhile the authors make claims that LPC leads to improved performance, there are many questions that are left untested. Specifically, here are the things which I am still unconvinced of:\n- To my understanding, there is nearly 0 experimental analysis which supports the \"Summary\" section in Appendix A.8. Since these are stated as the primary notion by which the authors' proposed method works mechanistically, a thorough experimental validation seems in order. Specifically, I would like to see, over the course of training, how the embeddings develop. How do they respond to varying strengths of the penalty hyperparameter? For each of the theorems and propositions, I would like to see experimental evidence.\n- Many of the statistics tested in the main body of the paper do not seem appropriately scaled. Specifically, the authors' method scales the embeddings to have tiny norm. The metrics they use to evaluate the embedding properties should therefore be invariant to this scaling; otherwise they are not informative. For example, the avg. grad norm seems like it is just a function of the embedding norms. It is therefore difficult to draw any conclusion from it. The same goes for the $\\Sigma_W$ term. I would instead prefer to see scaled variants of these metrics (unless I'm misunderstanding).\n- It would be extremely helpful to understand how this interacts with learning rate. I see in the code that the gamma has a complicated schedule and that the authors also use a learning rate schedule. So does the training take significantly longer with the gamma term due to needing to slowly incorporate all the things in without ruining the convergence? I am left with 0 idea of how this dynamic plays out.\n\nIn essence, the experiments suggest that what the authors say is happening is happening. But they are incomplete so as to leave doubts in the reader's mind.\n\nI also finally want to note that there is a line of work studying the effect of embedding norms on neural network performance, which the authors may find relevant [1], [2], [3].\n\n[1]: Draganov, Andrew, et al. \"On the Importance of Embedding Norms in Self-Supervised Learning.\" arXiv preprint arXiv:2502.09252 (2025).\n\n[2]: Kirchhof, M., Roth, K., Akata, Z., and Kasneci, E. A nonisotropic probabilistic take on proxy-based deep metric learning. In European Conference on Computer Vision, pp. 435–454. Springer, 2022.\n\n[3]: Zhang, D., Li, Y., and Zhang, Z. Deep metric learning with spherical embedding. Advances in Neural Information Processing Systems, 33:18772–18783, 2020."}, "questions": {"value": "Please see the above section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wTMD6RqMJR", "forum": "vhJAMkfMmE", "replyto": "vhJAMkfMmE", "signatures": ["ICLR.cc/2026/Conference/Submission21278/Reviewer_iDT1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21278/Reviewer_iDT1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839484248, "cdate": 1761839484248, "tmdate": 1762941666360, "mdate": 1762941666360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Latent Point Collapse (LPC), a phenomenon achieved by applying a strong $L_2$ regularization penalty to the outputs of a low-dimensional, linear penultimate layer. \nThe authors provide a rigorous theoretical framework to demonstrate that this technique forces within-class representations to collapse to unique points near the origin, a stronger condition than standard Neural Collapse (NC). \nThey prove that this mechanism guarantees global Lipschitz continuity, independent of input distance. \nEmpirically, the paper shows that LPC leads to significant improvements in class separability, robustness proxies (measured by DeepFool), and generalization on CIFAR-10, CIFAR-100, and ImageNet."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Strong Theoretical Foundation: \nThe paper offers a rigorous and extensive theoretical analysis in its appendix, proving that strong L2 regularization on features reshapes the loss landscape to be strongly convex, thereby guaranteeing convergence to a state of absolute, rather than relative, collapse.\n- Novel Lipschitz Continuity Guarantees: \nA key contribution is the proof that LPC ensures global Lipschitz continuity with a constant that is independent of input distance. \nThis provides a principled theoretical underpinning for the observed robustness improvements without requiring architectural constraints.\n- Methodological Simplicity: \nThe proposed method is simple and practical, requiring only the addition of a linear layer and a standard L2 regularization term, making it easy to implement and integrate into existing architectures.\n- Information-Theoretic Grounding: \nThe work successfully connects the LPC phenomenon to the Information Bottleneck principle, providing a theoretical justification for the observed generalization benefits by showing that LPC implicitly minimizes the entropy of the latent representations."}, "weaknesses": {"value": "- Insufficient Baselines: \nThe paper's novelty and empirical significance are not well-contextualized due to the omission of several key baselines. \nMethods like Center Loss, L2-Softmax, or NormFace share the goal of regularizing latent feature geometry and should be compared against to fairly assess the contribution of LPC.\n- Overstated Robustness Claims: \nThe paper claims dramatic robustness improvements based on the DeepFool algorithm and average gradient norms. \nThese are proxies for robustness and do not reflect performance against standard, stronger adversarial attacks (e.g., PGD, AutoAttack). Without such an evaluation, the practical robustness of the method remains unverified.\n- Extreme Regularization: \nThe use of an extremely large regularization coefficient (γ = 10^6) raises concerns about its impact on model expressiveness and capacity. \nThe paper argues this creates a new optimization regime but does not sufficiently explore the potential downsides or the sensitivity to this critical hyperparameter.\n- Limited Scope and Unexplained Phenomena: \nThe study is confined to balanced datasets, a significant limitation acknowledged by the authors. \nFurthermore, the intriguing observation that collapse points align with hypercube vertices (binary encoding) is presented without a theoretical explanation, representing a gap in understanding.\n\n---\n- General Limitations:\n    - Certified Robustness Analysis:\nThe paper does not provide a certified robustness analysis, which is the gold standard for provable defense claims. \nThe Lipschitz guarantee is theoretical but not translated into a practical certificate.\n    - Analysis of Computation Cost:\nThe computational cost of the proposed regularization schedule, especially when coupled with the long training times (1000 epochs for CIFAR), is not analyzed, making it difficult to assess the practical trade-offs of the method.\n    - Limited utility:\nThe work does not explore the utility of the learned representations for downstream tasks (e.g., transfer learning), where an extremely compressed feature space might have different properties compared to representations from standard models."}, "questions": {"value": "- Could you justify the choice of baselines and explain why more directly related methods for feature regularization, such as Center Loss or L2-Softmax, were not included in the comparison? \nHow would you expect LPC to perform against them?\n- To substantiate the strong robustness claims, would it be possible to evaluate your method against standard adversarial attacks like PGD or AutoAttack, even on a smaller scale? \nThis would provide a much clearer picture of its practical utility for adversarial defense.\n- What is the sensitivity of the model's performance (both accuracy and robustness) to the final regularization strength γ? \nIs there a risk of harming model expressiveness or 'over-collapsing' the features with such an extreme value, and how can practitioners find an optimal value without the extensive training schedule used in the paper?\n- Regarding the binary encoding phenomenon, have you performed any analysis to quantify this alignment with hypercube vertices beyond visual inspection? \nFor instance, by measuring the cosine similarity between class mean vectors and the closest hypercube vertex?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CrSEApW3Il", "forum": "vhJAMkfMmE", "replyto": "vhJAMkfMmE", "signatures": ["ICLR.cc/2026/Conference/Submission21278/Reviewer_fek9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21278/Reviewer_fek9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976971672, "cdate": 1761976971672, "tmdate": 1762941666084, "mdate": 1762941666084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}