{"id": "lINWtlpvSF", "number": 4876, "cdate": 1757781630598, "mdate": 1763679367463, "content": {"title": "Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs", "abstract": "Large language models demonstrate impressive results across diverse tasks but are still known to hallucinate, generating linguistically plausible but incorrect answers to questions. Uncertainty quantification has been proposed as a strategy for hallucination detection, requiring estimates for both global uncertainty (attributed to a batch of responses) and local uncertainty (attributed to individual responses). While recent black-box approaches have shown some success, they often rely on disjoint heuristics or graph-theoretic approximations that lack a unified geometric interpretation. We introduce a geometric framework to address this, based on archetypal analysis of batches of responses sampled with only black-box model access. At the global level, we propose Geometric Volume, which measures the convex hull volume of archetypes derived from response embeddings. At the local level, we propose Geometric Suspicion, which leverages the spatial relationship between responses and these archetypes to rank reliability, enabling hallucination reduction through preferential response selection. Unlike prior methods that rely on discrete pairwise comparisons, our approach provides continuous semantic boundary points which have utility for attributing reliability to individual responses. Experiments show that our framework performs comparably to or better than prior methods on short form question-answering datasets, and achieves superior results on medical datasets where hallucinations carry particularly critical risks. We also provide theoretical justification by proving a link between convex hull volume and entropy.", "tldr": "We introduce a black-box geometric framework that detects and corrects LLM hallucinations by analyzing the convex geometry of response embeddings, offering both global and local uncertainty estimates.", "keywords": ["LLMs", "hallucination detection", "uncertainty quantification", "black-box models", "LLM reliability"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cebac86532ade77542d30b2562d17bb895c16b9d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a geometric framework for understanding uncertainty in large language models, distinguishing between global and local uncertainty. The authors propose a black-boxed method to both detect and mitigate hallucinations by modeling uncertainty from a holistic perspective. The paper evaluates the approach on several benchmarks, showing moderate performance improvements in some cases."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a novel and conceptually interesting perspective on hallucination detection and mitigation through geometric modeling of uncertainty. It introduces a distinction between global and local uncertainty, which offers a potentially useful framework for understanding LLM confidence. The paper also propose an appealing high-level idea to address hallucination detection and mitigation in a unified framework."}, "weaknesses": {"value": "(1) My main concern is the lack of essential ablations. As a sampling based method, key hyperparameters such as sampling temperature and number of samples are not studied, but rather fixed to the same value through out the paper. This makes it difficult to assess the generalizability of the proposed approach.\n\n(2) While the authors highlight the importance of medical data, the proposed method on MedicalQA underperforms baselines in AUROC on most models in Table 1.  Also, there's typo in the color code in table 1-- on CLAMBER dataset llama3.1 8B, performance of proposed method should not be highlighted for AUROC.   \n\n(3) Some simple baselines are missing. On the detection task on open-source model, how does the performance compared to simple perplexity based detection? On the mitigation task, how does the performance compared to majority vote among the generated answers? -- this is particularly interesting since the design of detection score is to find consensus."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0giwqE0gao", "forum": "lINWtlpvSF", "replyto": "lINWtlpvSF", "signatures": ["ICLR.cc/2026/Conference/Submission4876/Reviewer_nh9Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4876/Reviewer_nh9Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548229244, "cdate": 1761548229244, "tmdate": 1762917628805, "mdate": 1762917628805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a hallucination detection method that lies within a geometric framework that uses black-box model access. The framework includes both global uncertainty estimates through measuring geometric volume, while local uncertainty estimates use geometric suspicion. In doing this, the framework is able to capture both global and local uncertainty in the LLM responses, quantifying the reliability of the response through semantic boundary points."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The theoretical framework (Appendix A)  to justify allows for a higher-level understanding of the ideas proposed in the paper. I would suggest you find a way to include this in the main paper. \n- The benchmarks used to evaluate the framework are adequate and diverse. Although most are focused on medical data, there are more general purpose datasets used as well."}, "weaknesses": {"value": "- The performance, as reported in table 1, is not consistently higher than other baselines. More experiments need to be conducted to understand why this is the case. It seems that P(True), the simplest baseline out of all of them, outperforms in certain scenarios, so considering the complexity associated with the proposed approach in comparison with P(True), there needs to be better justification by the others. \n- A complexity and/or time analysis of the framework is missing from the paper."}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IPHWqKY6RR", "forum": "lINWtlpvSF", "replyto": "lINWtlpvSF", "signatures": ["ICLR.cc/2026/Conference/Submission4876/Reviewer_gaV1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4876/Reviewer_gaV1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755523893, "cdate": 1761755523893, "tmdate": 1762917627745, "mdate": 1762917627745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript presents a new framework to quantify the uncertainty of LLM responses from a geometric perspective. The framework provides both a group-wise uncertainty score and a response-wise uncertainty score based on archetypal analysis. Experiments on several QA benchmark are performed to validate the effectiveness of the approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tUsing archetypal analysis to quantify the uncertainty of LLM responses provides a new perspective.\n2.\tProviding response-wise uncertainty is of more practical values than existing prompt-wise uncertainty metrics."}, "weaknesses": {"value": "1.\tThe main claim “this is the first sampling-based black-box method that differentiates high and low uncertainty responses within a batch.” is not true. See [1], where the proposed semantic density metric also measures sampling-based response-wise confidence/uncertainty without accessing the internal LLM state.\n2.\tThe design of the local uncertainty, i.e., the sum of ranks in three simple heuristic metrics, is not supported by any solid mathematical justifications. \n3.\tThe interpretation of the experimental results need to be further clarified. See “Questions” below.\n4. Several related baseline methods are missing in the current experiments. See “Questions” below for more details."}, "questions": {"value": "1.\tCan you add more discussions and compare to Semantic density [1], which is also a sampling-based response-wise confidence/uncertainty metric without access to the internal LLM state? Moreover, please also consider comparing to Degree [2] and the length-normalized likelihood [3].\n2.\tWhen converting the responses into embedding space, the prompt is not considered as a context. This may cause problems for measuring the semantic relationships among the responses. One example: Q: “What is the capital of France?” A1: “Paris”, A2: “the capital of France is Paris”. A1 and A2 actually mean the same thing under this context, but they will have very different embeddings without considering the original questions as context. Have you considered this limitation? \n3.\tIn the \"Convex Hull Approaches\" part of “Related Works”, existing works are criticized to oversimplify the problem by using PCA to reduce the embedding dimensionality to two. However, the proposed approach also uses PCA to reduce the embedding dimension (to 15 dimensions). How do you make sure 15 dimensions are sufficient to preserve important information? Do you have an ablation study with different PCA dimensions?\n4.\tThe likelihood of each sampled response is not utilized in the local uncertainty calculations. Using this information can potentially reduce the cost of sampling, i.e., we don’t need to sample the same response multiple times to estimate the output distribution. What is your consideration here?\n5.\tIn the experimental setup, it is stated “to classify response sets as reliable or hallucinated…”. What do you exactly mean here? Are you using the default answer to represent the response set?\n6.\tDid you handle the long-form question/answer in medicalQA in a different way than other shorter QA benchmarks? If not, do you think one uncertainty score for the entire long answer, which may include multiple claims, is sufficient?\n7.\tDo you have an explanation or analysis about why the answer selection does not work well in Qwen3-8B? What makes the performance difference so different across different model?\n\n[1] Xin Qiu, Risto Miikkulainen. Semantic density: Uncertainty quantification for large language models through confidence measurement in semantic space, Advances in Neural Information Processing Systems (NeurIPS), 2024\n\n[2] Zhen Lin, Shubhendu Trivedi, Jimeng Sun, Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models, Transactions on Machine Learning Research, 2024\n\n[3] Kenton Murray, David Chiang. Correcting Length Bias in Neural Machine Translation. In Proceedings of the Third Conference on Machine Translation, 2018."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "51dsz8zkjN", "forum": "lINWtlpvSF", "replyto": "lINWtlpvSF", "signatures": ["ICLR.cc/2026/Conference/Submission4876/Reviewer_nv83"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4876/Reviewer_nv83"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893946590, "cdate": 1761893946590, "tmdate": 1762917627050, "mdate": 1762917627050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Author summary response to all reviewers"}, "comment": {"value": "We thank all reviewers for their feedback, and note the consensus that our work represents a \"new\", \"conceptually interesting\" perspective for LLM uncertainty modelling, whilst offering \"practical value\" with our hallucination mitigation application. We are also pleased to see reviewer support for the unified framework for prompt-wise and response-wise uncertainty measures, and the diversity of our benchmark datasets.\n\nThe review process has been highly productive. In particular, the discussion regarding prior graph-based baselines (e.g., Degree and Eccentricity) has helped us sharpen our core contribution: moving from a claim of \"the first black-box local method\" to \"the first unified geometric framework\" that derives both global dispersion and local reliability from a single continuous space.\n\nWe have updated the manuscript to reflect this refined positioning and included comprehensive new experiments to address reviewer requests:\n\n1. **Addition of baselines**: We implemented and compared against Degree and Eccentricity. These experiments confirm that while graph heuristics perform well in some settings, our Geometric Suspicion method demonstrates superior robustness, particularly in complex domains like MedicalQA.\n2. **Positioning and grounding**: We revised the Abstract and Introduction to explicitly contrast our unified continuous framework against prior graph-theoretic approaches. We moved our key theorem (linking convex hull volume to entropy) from the Appendix to the Methods section to better highlight the theoretical justification of our approach.\n3. **Ablations**: We added ablations for the key hyperparameters in our framework, justifying our setting selection. \n4. **Further Analysis**: We added experiments investigating the efficacy of our hallucination correction mechanism across models and datasets. We also add complexity analysis confirming our method is computationally feasible.\n\nWe reply to all reviewers individually below."}}, "id": "627TJx6tU0", "forum": "lINWtlpvSF", "replyto": "lINWtlpvSF", "signatures": ["ICLR.cc/2026/Conference/Submission4876/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4876/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4876/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763679501122, "cdate": 1763679501122, "tmdate": 1763679501122, "mdate": 1763679501122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}