{"id": "1NZ3DHF9nT", "number": 2544, "cdate": 1757141419189, "mdate": 1759898142078, "content": {"title": "Fast-dLLM v2: Efficient Block-Diffusion LLM", "abstract": "Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that efficiently adapts pretrained AR models into dLLMs for parallel text generation—requiring only ∼1B tokens of fine-tuning. This represents a 500× reduction in training data compared to full-attention diffusion LLMs such as Dream (580B tokens), while preserving the original model’s performance. Our approach introduces a novel training recipe that combines a block diffusion mechanism with a complementary attention mask, enabling blockwise bidirectional context modeling without sacrificing AR training objectives. To further accelerate decoding, we design a hierarchical caching mechanism: a block-level cache that stores historical context representations across blocks, and a sub-block cache that enables efficient parallel generation within partially decoded blocks. Coupled with our parallel decoding pipeline, Fast-dLLM v2 achieves up to 2.5× speedup over standard AR decoding without compromising generation quality. Extensive experiments across diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs—marking a significant step toward the practical deployment of fast and accurate LLMs. Code and model will be publicly released.", "tldr": "Fast-dLLM v2 transforms pretrained autoregressive LLMs into efficient block diffusion models, matching accuracy while delivering up to 2.5× faster decoding with minimal data and training cost.", "keywords": ["Diffusion LLM", "Efficient AI"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88be351069a45367e1acc688a63112f02d777487.pdf", "supplementary_material": "/attachment/f13b274894cef886d359b4bbe0876255b02266cd.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Fast-dLLM v2, a block diffusion language model designed to address the inherent inefficiency of sequential decoding in standard autoregressive LLMs. Its core contribution is a highly data-efficient adaptation method that converts pre-trained AR models into efficient parallel decoders, requiring around ~1B tokens for the entire fine-tuning process. This represents a 500x reduction in required training data compared to full-attention dLLMs such as Dream, while successfully preserving the original model's performance. The method employs a novel training recipe combining a block diffusion mechanism with a complementary attention mask to enable block-wise bidirectional context modeling. To accelerate inference, it introduces a hierarchical caching mechanism: a \"block-level cache\" for inter-block context reuse, and a DualCache-based \"sub-block cache\" to facilitate efficient parallel refinement within each block. Extensive experiments demonstrate that Fast-dLLM v2 achieves up to a 2.5x inference speedup compared to standard AR decoding. Crucially, this acceleration is achieved without sacrificing generation quality; the model's performance generally matches or exceeds that of strong AR baselines, while varying among different tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.The paper proposes a novel and ingenious method. The paper's design of the hierarchical caching mechanism is ingenious, maximizing the dLLM's intra-block parallel decoding potential while preserving the AR model's inter-block causal dependencies.\n\n2.The paper's results are outstanding. The proposed training method is highly efficient and achieves SOTA inference speed without sacrificing quality. This greatly reduces training costs and holds significant practical value.\n\n3.The experimental analysis is thorough and detailed. For instance, it provides a practical trade-off analysis of the \"confidence threshold,\" which is highly instructive for the technology's real-world deployment.\n\n4.The technical details and experimental settings are clearly articulated, facilitating reproducibility.\n\n5.The paper's writing and organizational structure are very clear, demonstrating excellent readability.\n\nOverall, this is a solid and good paper."}, "weaknesses": {"value": "The paper claims that token shift strategy is necessary for preserving the pretrained AR model’s representation quality. However, this claim is not empirically validated with a dedicated ablation study. As shown in Table 2, the most basic baseline, 'naive token shift,' already includes this strategy. Therefore, the actual necessity of this design is unclear."}, "questions": {"value": "1.The proposed method includes a hierarchical caching mechanism and a specialized hybrid attention mask; will this lead to a more complex engineering implementation?\n\n2.The adaptation is performed using the LLaMA-Nemotron dataset, which consists of high-quality SFT data. This raises the question of whether such high-quality instruction data is a necessary component for successfully learning the block diffusion mechanism, or if the method could be equally effective using general pre-training corpora.\n\n3.Could the same impressive results be achieved if other mainstream AR architectures were used for the pre-trained initialization? Or is the success of this method dependent on the specific architecture or quality of the AR model used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mqzJouCUuT", "forum": "1NZ3DHF9nT", "replyto": "1NZ3DHF9nT", "signatures": ["ICLR.cc/2026/Conference/Submission2544/Reviewer_aB3q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2544/Reviewer_aB3q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869849986, "cdate": 1761869849986, "tmdate": 1762916274784, "mdate": 1762916274784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Fast-dLLM v2, a block diffusion language model (dLLM) designed for efficient inference. The core idea is to adapt a pretrained autoregressive (AR) LLM into a block-wise diffusion model using a data-efficient finetuning process (requiring only ~1B tokens). The method introduces a novel training recipe with a complementary attention mask to enable this adaptation. For inference acceleration, the paper proposes a hierarchical caching mechanism: a block-level cache for reusing context from previous blocks and a sub-block cache (adopting DualCache from Fast-dLLM v1) for parallel generation within the current block. Experiments show that Fast-dLLM v2 can achieve up to a 2.5x speedup over its AR baseline (Qwen2.5-7B) while maintaining comparable or slightly improved accuracy on several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Significant Problem: The paper addresses the critical problem of LLM inference latency. The goal of achieving parallel decoding speedups while preserving the quality of strong AR models is highly relevant.\n\n2. Strong Empirical Results: The method demonstrates impressive practical results. A 2.5x speedup over a strong AR baseline like Qwen2.5-7B-Instruct with no loss in generation quality is a significant engineering achievement.\n\n3. Data and Training Efficiency: A major strength is the data efficiency of the finetuning process. Adapting a 7B model with only ~1B tokens is a practical and accessible approach compared to training full-attention dLLMs, which the paper notes can require orders of magnitude more data (e.g., 580B tokens for Dream).\n\n4. Clarity: The paper is well-written, and the proposed architecture, training process, and inference pipeline are explained clearly."}, "weaknesses": {"value": "The primary weakness of this paper is its perceived lack of conceptually new contribution, as it appears to be a skillful integration of several existing lines of work.\n\n1. Combination of Existing Caching Techniques: The \"hierarchical caching mechanism\" seems to be a straightforward combination of two existing ideas.\n- The block-level cache, which stores representations of previously decoded blocks, is an inherent feature of the block-wise autoregressive structure, as seen in \"Block Diffusion\" (Arriola et al., 2025).\n- The sub-block cache for parallel refinement within a block is explicitly stated to \"adopt the DualCache in Fast-dLLM (Wu et al., 2025)\" (Section 3.3).\n- While effective, combining these two caches (inter-block from Block Diffusion, intra-block from Fast-dLLM v1) feels more like an incremental engineering step than a novel caching design.\n\n2. Finetuning Concept is Not New: The core idea of adapting a pretrained AR model into a diffusion model has been explored before.\n\n- Previous work Dream (Ye et al., 2025b;a) is already a model that was \"adapted from the existing Qwen-2.5 7B\" (Section 2.1), and the paper also points to \"SDAR (Cheng et al., 2025)\" as concurrent work that \"successfully finetuned a block diffusion model from a pretrained autoregressive model\" (Section 2.2).\n- The main claim over Dream is data efficiency (1B vs. 580B tokens). However, this comparison may not be direct, as Dream adapts to a full-attention dLLM, whereas this work adapts to a more structured block-wise dLLM. The efficiency gain may stem more from the architectural choice (block-wise vs. full-attention) than from a fundamentally new finetuning recipe.\n\nOverall, while the final system is fast and effective, the contributions seem to be more in the successful engineering and combination of existing components rather than the introduction of new, fundamental concepts."}, "questions": {"value": "1. Could the authors please elaborate on the novelty of the hierarchical caching mechanism? Is there more to it than the combination of the inter-block cache from Block Diffusion and the intra-block DualCache from Fast-dLLM v1?\n\n2. The data efficiency (1B vs. 580B tokens) is a key claim over Dream. How much of this 500x reduction is due to the proposed training recipe (complementary masks, etc.) versus the simpler, more AR-like target architecture (block-wise dLLM) compared to Dream's full-attention dLLM?\n\n3. Could the authors clarify the primary novel contributions of their finetuning approach and inference pipeline compared to the concurrent SDAR (Cheng et al., 2025), which also finetunes a pretrained AR model into a block diffusion model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "blwnpzdG4V", "forum": "1NZ3DHF9nT", "replyto": "1NZ3DHF9nT", "signatures": ["ICLR.cc/2026/Conference/Submission2544/Reviewer_9tWZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2544/Reviewer_9tWZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945829953, "cdate": 1761945829953, "tmdate": 1762916274584, "mdate": 1762916274584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Fast-dLLM v2, which turns regular AR language models into faster diffusion models that can generate multiple tokens at once, using only ~1B tokens for fine-tuning. The key trick is a block-based approach where tokens within each block can look at each other while still respecting the left-to-right flow between blocks. With smart caching and parallel decoding, it runs up to 2.5× faster than standard models while keeping the same quality on benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is elegantly simple yet effective. It adapts pretrained AR models into block diffusion models with minimal modifications, requiring only a complementary masking strategy and block-wise attention that can be easily integrated into existing training and inference frameworks.\n2. The paper provides thorough experimental validation across multiple model scales and diverse benchmarks, including code generation, mathematical reasoning, and knowledge tasks, with comprehensive ablation studies examining the impact of block sizes, sub-block sizes, and caching strategies.\n3. The presentation is easy to follow, with concrete experimental details that make the work solid."}, "weaknesses": {"value": "1. Although the average performance remains the same compared with Nemo-FT models, there are consistent patterns in performance change in specific benchmarks. Eg. HumanEval has a significant increase, while Math experiences a large decrease in both 1.5B and 7B comparison, which is unlikely to be random fluctuations.\n2. While causing some performance drops in some of the benchmarks, the proposed method only achieves 2.5x acceleration compared with AR models. The speed-up is similar to many existing acceleration methods for AR models, like speculative decoding, making the method less competitive."}, "questions": {"value": "1. What could be causing the consistent and significant discrepancy mentioned in \"Weakness\"?\n2. Can this block diffusion process be used together with existing methods to accelerate AR models? eg. speculative decoding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0PM6rwMDFf", "forum": "1NZ3DHF9nT", "replyto": "1NZ3DHF9nT", "signatures": ["ICLR.cc/2026/Conference/Submission2544/Reviewer_3sof"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2544/Reviewer_3sof"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994123182, "cdate": 1761994123182, "tmdate": 1762916274398, "mdate": 1762916274398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a block-diffusion adaptation to convert a pretrained AR model into a block-wise parallel generation model, achieving up to 2.5× speed-up over standard AR decoding with minimal loss in accuracy.\n\nContributions include: introducing complementary masking during training, a \"hierarchical\" KV caching mechanism to enable efficient block decoding, and large-scale evaluations on models upto 7B in size."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- requires light weight finetuning to convert AR to diffusion models\n- better and faster than DREAM style approaches, that utilizes full sequence diffusion for generation\n- there is a configuration in Fast-dLLM v2 that offers speed ups compared to an AR model, with similar accuracy -- fulfiling the promise of diffusion models"}, "weaknesses": {"value": "My main concern is limited conceptual novelty: there is only a single change from the previous iteration of Fast-dLLM-v1, which is adding blockwise training and generation. This weakness is not a bad thing in itself.\nPlease look at the questions :)\n\nOther weakness is: the cache is still an approximation."}, "questions": {"value": "- i'm still confused on how the hierarchical caching mechanism works, could the authors provide more details?\n- are different sub-blocks decoded autoregressively? or in a given block, different sub-blocks can be decoded in any order?\n- why do you need sub-blocks? can't dual cache work within a block, without defining sub-blocks?\n- how would the latency change when the context length increases? i think authors tested on sequence lengths upto 2K only, i'm very curious to see how the latency scales with increasing context length\n\n\nSince i'm not an expert in this field, i'll be looking at feedback of other reviewers too to gain a better understanding.\nThat being said, i'm definitely willing to improve my score during our discussion :)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fWSibuZIbS", "forum": "1NZ3DHF9nT", "replyto": "1NZ3DHF9nT", "signatures": ["ICLR.cc/2026/Conference/Submission2544/Reviewer_nsrh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2544/Reviewer_nsrh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017569921, "cdate": 1762017569921, "tmdate": 1762916274064, "mdate": 1762916274064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}