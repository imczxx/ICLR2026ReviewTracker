{"id": "F5sQPP7Swa", "number": 21206, "cdate": 1758314947413, "mdate": 1759896934813, "content": {"title": "Measuring  Distribution Shifts in Inverse Problems without clean data", "abstract": "Diffusion models are widely used as priors in imaging inverse problems. However, their performance often degrades under distribution shifts between the training and test-time images. Existing methods for identifying and quantifying distribution shifts typically require access to clean test images, which are never available at test time when solving inverse problems. We propose a flexible framework for measuring distribution shift using  *only* corrupted test measurements and candidate diffusion model scores.  Our framework enables three complementary capabilities. First, in the general case with only a pool of diffusion models, it supports a principled model selection by identifying the model whose prior best matches the test data. Second, when an in-distribution model is available, our metric provides a theoretically guaranteed estimator of KL divergence that closely matches the image-domain KL. Third, the metric serves as a tool for adaptation guidance: aligning score functions with corrupted measurements reduces the estimated shift and improves reconstruction quality. Experiments on inpainting and MRI confirm that our method (i) achieves robust model selection, (ii) reliable estimates KL divergence in the presence of an in-distribution model, and (iii) enables effective adaptation to mitigate distribution shift.", "tldr": "We propose a metric to identify the best diffusion model prior for solving imaging inverse problems in test-time,provide adaptation based on the proposed metric, and measure KL divergence.", "keywords": ["Imaging inverse problems", "distribution shift", "model selection."], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6bbe1f7971282c124ec4c73484b4ae39faedfb51.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The author deal with the important problem of measuring distribution shift using only measurements. They use their approach for model selection, estimation of the KL divergence and for fine-tuning / adaptation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written \n- The main result in Theorem 1 is interesting and also holds in the numerical experiments \n- I think in particular the experiments on the effect of the model selection and adaptation on downstream tasks (page 7, starting line 365) are interesting and important"}, "weaknesses": {"value": "See questions.\n\n\n\nI have some minor issues with the formating: \n- Figure 1: It is hard to see the difference of the plot for \"image domain\" and \"meas. domaine\" without zooming in on the PDF. In a printed version it is even harder to tell apart. \n- Figure 1 is on page 3, but only discussed on page 6\n- Figure 3 is discussed on page 6 (line 309), but is only displayed on page 8 (after Figure 6)\n- Page 7, line 377 the subsection title \"4.4 Ablation Studies\" it at the end of the page, without any text below it \n\nOther minor points:\n- In Assumption 2 you use $\\hat{D}$, but $\\hat{D}$ is only defined  later in Theorem 1\n- page 7, line 348 missing space between \"OOD denoiser\" and $\\hat{D}$"}, "questions": {"value": "- You write that your approach even works for JPEG compression, a setting not supported by the theory. What are inverse problems where your method would not work? Do you have some neccessary conditions on a problem?\n- In Figure 2: Is this also the integrand of Eq. (4) and Eq. (9) up to $\\sigma$?\n- In Table 1 we see that MetFaces is closer to FFHQ than AFHQ. So, using your model selection criterion I would choose MetFaces. However, in Table 2 the image reconstruction results for MetFaces are worse than for AFHQ. Why? \n- What is the difference between your adaptation loss in Equation (10) and the objective in Ambient Diffusion [1] or [2]\n- Why do you choose AFHQ in the adaptation in 4.3. and not MetFaces (MetFaces was closer to FFHQ according to Table 1)?\n\n[1] Daras et al. \"Ambient Diffusion: Learning Clean Distributions from Corrupted Data\" (2023)\n\n[2] Kawar et al. \"GSURE-Based Diffusion Model Training with Corrupted Data\" (2024)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GMFHSNMQHr", "forum": "F5sQPP7Swa", "replyto": "F5sQPP7Swa", "signatures": ["ICLR.cc/2026/Conference/Submission21206/Reviewer_hv1x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21206/Reviewer_hv1x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761556769861, "cdate": 1761556769861, "tmdate": 1762941612562, "mdate": 1762941612562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method for detecting distribution shifts between measurements and priors to (1) select the best prior for regularization (2) mitigate distribution shift effects at inference time when using an OOD prior. The authors make an important point that often in inverse problem settings we do not have access to clean images to measure distribution shifts with the prior and thus there is a need for a unsupervised approach. Experimental results show good alignment between their method and image-based estimates for OOD detection. Additionally, the authors show that using an augmented loss to correct for out of distribution samples they can adapt OOD models with few samples and show improvement for said models on downstream recovery."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper solves an interesting problem that is important for deploying pre-trained generative models to image recovery in potentially new settings. As far as I know this is the first works to look into this for inverse problems in the self-supervised setting. This setting is crucial because adapting generative priors to new scientific data will likely require pre-trained models that havent been explicitly trained on the target distribution due to data scarcity. The paper has nice experiments showing the ability of their method to reliably detect OOD models from partial measurments. Additionally, the authors show encouraging results of using their technique to adapt their OOD models to the target distribution using only a limited number of samples which is really great to see."}, "weaknesses": {"value": "The results are convincing for the most part, however, it would be nice to potentially see a few more experiments with more samples of the measurement data to get a better idea of how the method scales with more measurement data when adapting OOD models to the target distribution. Additionally, if it’s possible to compare their measurement only adaption approach to image-based adaptation approaches with with the same number of samples that would be a helpful baseline which can serve as the upper bound on performance for their measurement adaption otherwise its a bit more difficult to appreciate the performance gains they are getting."}, "questions": {"value": "1. how does the method scale in performance with a higher # of measurement examples?\n2. how does the method compare to image based adaption techniques with the same number of samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NP7B1ndarQ", "forum": "F5sQPP7Swa", "replyto": "F5sQPP7Swa", "signatures": ["ICLR.cc/2026/Conference/Submission21206/Reviewer_AU8b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21206/Reviewer_AU8b"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921890472, "cdate": 1761921890472, "tmdate": 1762941611914, "mdate": 1762941611914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an unsupervised framework to measure distribution shifts in inverse problems using only corrupted measurements, removing the need for clean test images. It reformulates the KLD between in-distribution and out-of-distribution data through diffusion model score functions evaluated in the measurement domain. The derived estimator links shift magnitude to denoiser residuals across noise levels, enabling model selection, divergence estimation, and adaptation that aligns out-of-distribution scores with measurement data. Experiments on image inpainting and magnetic resonance imaging show strong correspondence between image- and measurement-domain estimates and improved reconstruction after adaptation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents an interesting and timely idea by proposing a framework to quantify distribution shifts in inverse problems without access to clean data. The formulation is conceptually clear, and the paper is generally well written and structured, with good alignment between theory and experiments. The method shows potential practical value by enabling model selection and adaptation using only measurement data. However, while these aspects are promising, the overall contribution remains somewhat limited in scope and especially experimental depth."}, "weaknesses": {"value": "My main concern lies in the experimental setup, which appears overly simplified and somewhat artificial. The chosen tasks, such as low-resolution inpainting and small-scale fastMRI tests, do not capture the real challenges of distribution shift in medical imaging. The out-of-distribution settings, based on different anatomical regions or synthetic corruptions, are relatively easy to separate and may overstate the method’s performance. A more convincing validation would use realistic shifts, for example differences in scanner field strength (1.5 T versus 3 T MRI), acquisition protocols, or vendor-specific pipelines, where data differ in subtle but meaningful ways and clean reference images are not available. This would better demonstrate the method’s practical value and robustness.\n\nAnother weakness is the strong reliance on assumptions that rarely hold in practice, such as randomized measurement operators and independence between measurements and denoiser residuals. These conditions are violated in realistic setups like fixed MRI masks, making the theoretical guarantees and practical reliability of the method uncertain."}, "questions": {"value": "1. The experiments appear simplified and artificial. How would the proposed framework perform under more realistic domain shifts, such as differences in scanner field strength (e.g., 1.5 T vs 3 T MRI), acquisition protocols, or vendor-specific reconstruction pipelines?\n\n2. Given that the out-of-distribution scenarios used in the paper (different anatomies or synthetic corruptions) are relatively easy to distinguish, can the authors provide evidence that their metric remains reliable when the domain shift is subtle but clinically meaningful?\n\n3. The theoretical analysis assumes randomized measurement operators whose span covers the full signal space. How critical is this assumption in practice, and what happens if one uses fixed or structured operators as in real MRI or CT acquisition?\n\n4. Since the proposed metric depends on expectations over many random operators, how feasible is this in realistic imaging pipelines where only a single, fixed acquisition model is available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "peYHnnS38c", "forum": "F5sQPP7Swa", "replyto": "F5sQPP7Swa", "signatures": ["ICLR.cc/2026/Conference/Submission21206/Reviewer_wiAN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21206/Reviewer_wiAN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935301037, "cdate": 1761935301037, "tmdate": 1762941610831, "mdate": 1762941610831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposes a method to measure distributional shift in inverse problems at test-time."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Use diffusion model to detect OOD problems is interesting.\n2. The mathematics in this paper looks correct to me."}, "weaknesses": {"value": "1. This paper should also compare with domain/test-time adaptation methods for inverse problems. such as [1], [2]. \n2. There are literatures on using diffusion models to detect OOD samples, e.g. perturbing intermediate noise or so on. Authors should mention them.[3]\n3. The novelty of this work is lacking since OOD detection with diffusion model is well known, and adapting a pretrained model to OOD inverse problem is also well-known. The authors contribution in this field is obscure.\n\n\n[1] Deep Diffusion Image Prior for Efficient OOD Adaptation in 3D Inverse Problems\n\n[2] Patch-based diffusion models beat whole-image models for mismatched distribution inverse problems\n\n[3] Denoising diffusion models for out-of-distribution detection"}, "questions": {"value": "Instead of selecting the best pretrained model, is there a way to improve the pretrained model for inverse problem solving given your OOD detection information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4wukYEmpUH", "forum": "F5sQPP7Swa", "replyto": "F5sQPP7Swa", "signatures": ["ICLR.cc/2026/Conference/Submission21206/Reviewer_DSpX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21206/Reviewer_DSpX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996834633, "cdate": 1761996834633, "tmdate": 1762941610382, "mdate": 1762941610382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}