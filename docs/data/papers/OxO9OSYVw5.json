{"id": "OxO9OSYVw5", "number": 4870, "cdate": 1757781098230, "mdate": 1763565884412, "content": {"title": "Time-to-Move: Training-Free Motion-Controlled Video Generation via Dual-Clock Denoising", "abstract": "Diffusion-based video generation can create realistic videos, yet existing image and text-based conditioning fails to offer precise motion control. Prior methods for motion control typically rely on displacement-based conditioning and require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations, obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection, as direct motion guidance, analogous to using coarse layout input in image editing. To integrate these signals, we adapt SDEdit to the video domain while anchoring the  appearance with image conditioning. We further propose dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions and grants flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting.", "tldr": "We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models.", "keywords": ["Computer vision", "Generative models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e1bbd2393f949024396afdfe1e1506095d06c8d7.pdf", "supplementary_material": "/attachment/84d51d5963f1f2acf2f4bb3ada82e10a0358d606.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes TTM, a training-free framework for motion control in image-to-video (I2V) models. The core of the method is a \"dual-clock denoising\" strategy: it uses less noise ($t_{strong}$) in user-specified motion regions to strongly constrain motion, while using more noise ($t_{weak}$) in other regions (like backgrounds) to allow for natural dynamics. Experiments show this training-free method achieves results comparable to, or even better than, state-of-the-art (SOTA) methods that require training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Practicality and Novelty:** The paper addresses a key pain point in video generation (training-free motion control). \"Dual-clock denoising\" is a concise and effective innovation that elegantly solves the SDEdit trade-off between motion fidelity and background artifacts. The framework can be used as a plug-and-play module for different I2V models.\n2.  **Experimental Validation:** The experiments are thorough, covering object motion, camera motion, and joint appearance control. Comparisons against SOTA methods like DragAnything and GWTF are convincing, demonstrating the method's effectiveness both quantitatively (Tables 1, 2) and qualitatively (Figs 4, 6). The ablation study in Appendix A also strongly supports the necessity of the dual-clock design."}, "weaknesses": {"value": "1.  **Relation to RePaint:** The paper should more clearly articulate its distinction from RePaint. The blending operation during the $t_{strong} \\le t < t_{weak}$ phase appears very similar to RePaint's. The core innovation seems to be the **temporal control** of this blending (i.e., stopping at $t_{strong}$ for joint refinement) rather than the blending mechanism itself, which needs clarification.\n2.  **Hyperparameter Sensitivity:** The key hyperparameters $(t_{weak}, t_{strong})$ require manual tuning for different models (e.g., (36, 25) for SVD vs. (46, 41) for CogVideoX), which somewhat reduces its \"plug-and-play\" convenience. An analysis of sensitivity to these parameters is recommended.\n3.  **Dependence on Input Quality:** The method's effectiveness depends on the quality of the \"crude animation\" $V^w$ and the mask $M$. Particularly for the camera motion task, it relies on external depth estimation models and complex 3D reprojection (as detailed in Appendix B.2), which adds a \"hidden cost\" to its application."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OFZcWxEjJe", "forum": "OxO9OSYVw5", "replyto": "OxO9OSYVw5", "signatures": ["ICLR.cc/2026/Conference/Submission4870/Reviewer_X1Wj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4870/Reviewer_X1Wj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844178447, "cdate": 1761844178447, "tmdate": 1762917624733, "mdate": 1762917624733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper Introduces a training-free method for controllable video generation. It firstly generates a crude edited video according to user's input. Then the edited video serves as the guiding signal for a pretrained video diffusion model and inferences in a SDEdit-style. To achieve better performance, authers modify SDEdit and propose a Dual-clock inference pipeline which apply different denoising timestep for mask and unmasked region."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method adopts a user-friendly setup that allows users to drag the object they want to move. Studying this problem plays an important role in advancing controllable generation toward practical applications. \n2. The proposed training-free method is straight-forward and efficient. The method also shows board application for various pretrained video diffusion models, extending the function of video foundation model.\n3. The author’s writing and figures in the paper are clear and easy to understand."}, "weaknesses": {"value": "1. The warping method. The quality of this method highly depends on the quality of the directly warped video. However, the author only briefly mentioned that the warped video is generated through forward warping, without providing detailed steps or references for the process.\n2. The dual-clock denoising strategies. The dual-clock is a slight adaptation of SDEdit and lacks insight or improvement of SDEdit. And I think it's highly depend on the choice of $t_{weak}$ and $t_{strong}$.\n3. Experiments. The authors do not provide ablation study. I think the author should ablate different warping method and the dual-clock denoising strategy to see which part contributes more to the final result."}, "questions": {"value": "1. The warping method. Can you describe more about the forward warping method? Also, in the provided Object Control demos (second column, fifth row), the warped video not only warps the white ball but also warps the two other balls it collides with. I’m curious how this was achieved. Additionally, I wonder how the method performs on actions that are difficult to warp, such as complex interactions or occlusions between objects. If the warping quality is poor in such cases, would it significantly affect the subsequent generation?\n2. The dual-clock denoising strategies. Could the authors provide some empirical guidelines or example values for selecting  $t_{weak}$ and $t_{strong}$? Is there a generally applicable choice for these parameters, or do they need to be tuned separately for each video?\n3. Experiments. This paper lacks ablation study and the authors should provide more ablation studys on different warping method and the dual-clock denoising strategy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ocWNczkmAL", "forum": "OxO9OSYVw5", "replyto": "OxO9OSYVw5", "signatures": ["ICLR.cc/2026/Conference/Submission4870/Reviewer_4gbG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4870/Reviewer_4gbG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901943791, "cdate": 1761901943791, "tmdate": 1762917624495, "mdate": 1762917624495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Time-to-Move (TTM), a training-free framework for controllable video generation using image-to-video diffusion models. TTM introduces the concept of using crude reference animations (e.g., cut-and-drag edits or depth-based warping) as direct motion guidance signals. TTM uses dual-clock denoising, which assigns different noise schedules to masked (motion-controlled) and unmasked regions, allowing strict enforcement of motion guidance where specified and natural evolution elsewhere. Experiments across multiple diffusion backbones show that TTM offers strong performance compared to state-of-the-art training-based and training-free baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The training-free and plug-and-play natures of TTM are valuable. No fine-tuning or architectural changes are required, making the method broadly applicable across various existing models and lightweight.\n- Region-dependent noising is effective, balancing fidelity to user-specified motion with natural background dynamics.\n- Generalization results across backbones look good."}, "weaknesses": {"value": "- Objects introduced later rather than in the first frame cannot be anchored, limiting use cases involving scene entry, new dynamic object, or sudden occlusion.\n- The dual-clock denoising scheme requires tuning specific settings per backbone, which could hinder plug-and-play deployment in practice.\n- Strict full-object masks must be provided. Control signals relying on sparse cues are more user-friendly in some workflows."}, "questions": {"value": "- What is the trade-off in inference time and computational overhead? What is the average inference time increase (or decrease) when using TTM on a base model compared to its vanilla inference process?\n- How robust is the method to segmentation errors? For example, when there is motion blur or non-rigid motion, it is difficult to provide an accurate mask."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sI7CikjK4b", "forum": "OxO9OSYVw5", "replyto": "OxO9OSYVw5", "signatures": ["ICLR.cc/2026/Conference/Submission4870/Reviewer_Bb3Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4870/Reviewer_Bb3Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942162104, "cdate": 1761942162104, "tmdate": 1762917624077, "mdate": 1762917624077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We thank the reviewers for their time and thoughtful feedback. We are encouraged that they highlight TTM as training-free, plug-and-play, and broadly applicable across backbones [**bHQm, Bb3Y, 4gbG, X1Wj**]. We appreciate their recognition that our work addresses a key practical need in controllable video generation [**X1Wj**] and offers a simple, user-friendly interface [**4gbG**]. We are glad they view our dual-clock, region-dependent denoising as effective and well supported by ablations [**bHQm, Bb3Y, X1Wj**], and we thank them for noting the strength of our experimental evaluation against competing methods [**bHQm, X1Wj**].\n\nWe address the shared reviewers’ concerns below and the reviewer-specific comments in their respective sections. All feedback has been incorporated into the revised paper (changes in red). Additional material is available on our **rebuttal supplementary page**: https://taupe-pixie-aa7944-rebut.netlify.app.\n\n---\n\n## Masking\nAddressing the questions raised by all reviewers concerning masks, we provide the following clarifications:\n1. **Robustness in current experiments [bHQm, Bb3Y, X1Wj]**\n\n   We acknowledge that this aspect was not sufficiently clear in the original text. Our object-control evaluation on MC-Bench already implicitly measures robustness to inaccurate masks: the dataset provides human-annotated brush masks that are coarse, include background regions, and often miss fine object details, rather than pixel-accurate segmentations. Examples illustrating this appear in **“MC-Bench Samples - Coarse Masks” section** of our *rebuttal page* (https://taupe-pixie-aa7944-rebut.netlify.app), and we now clarify this explicitly in *Sec. 4.1*. \n\n2. **Mask Quality Ablation [bHQm, Bb3Y, X1Wj]**\n\n   Complementing current experiments, we add a **new ablation study** that explicitly perturbs the input masks. We follow the same experimental setup as in *Sec. 4.1*, but apply morphological erosion and dilation with varying kernel sizes to the MC-Bench masks, simulating under- and over-segmented masks with different boundary characteristics. As shown in *Sec. B* and *Tab. 4* In the paper revision appendix, these perturbations lead to only minor changes in all metrics, indicating that our method is robust to mask inaccuracies.\n\n3. **Ease of use [bHQm, 4gbG, Bb3Y, X1Wj]**\n\n   The “cut-and-drag” design is explicitly meant to keep user interaction simple: the user roughly indicates what should move, and where, and TTM handles the rest. Following the reviews, we now introduce a **new lightweight GUI** in which the user selects one or more polygons around the object(s) and drags them along the desired trajectory. The GUI automatically constructs the warped reference video and corresponding masks. In addition, it supports rotation, scaling, recoloring, and insertion of new objects. We include an example and an explanation of this interface in *Sec. 5* of the revised paper, together with *Fig. 8*, and provide a video demonstration at the **“Cut-and-Drag GUI Interface” section** of our *rebuttal page* (https://taupe-pixie-aa7944-rebut.netlify.app/).  \n\n   To further reduce the need for precise manual masks and address the concern about sparse control signals raised by reviewer **[Bb3Y]**, we additionally integrate Segment Anything (SAM). With a single click, the user can mark an object, and SAM generates a full-object mask that can then be dragged and used directly by TTM.\n\n---\n\n## Warping\n[**bHQm, 4gbG**] \n\nWe thank the reviewers for highlighting the need for greater clarity regarding the warping operation and acknowledge that our initial explanation was insufficient. To facilitate easy application of our method and avoid computational overhead, we chose the most straightforward approach for warping. We have now fully clarified how it is performed in practice in **the new** *Sec. 5.1*.\n\n---\n\n## Hyperparameter Sensitivity (dual-clock denoising)\n[**bHQm, Bb3Y, 4gbG, X1Wj**]\n\nThe necessity of inference-time hyperparameter tuning is standard for diffusion-based methods, analogous to the guidance scale in classifier-free guidance or the timestep schedule in SDEdit. While this is a valid consideration, we found that optimal parameters for our method are readily determined with minimal trial runs.\nIn direct response to the reviewers’ request, we provide a **new sensitivity analysis**, explained in the *Sec. C* of the revised supplementary, by varying $t_{weak}$​ and $t_{strong}$​ around their defaults on MC-Bench. The results are detailed in *Tab. 5* in the appendix. Smaller $t_{strong}$​  values generally increase conditional motion adherence and reduce CoTracker distance, but they slightly degrade imaging quality due to a more rigid object representation. Conversely, higher $t_{weak}$​ values (introducing more initial noise) tend to lead to a higher dynamic degree. This analysis shows how these hyperparameters affect the results and demonstrates that our chosen settings lie in a **stable operating range**."}}, "id": "CO0Ia68xoV", "forum": "OxO9OSYVw5", "replyto": "OxO9OSYVw5", "signatures": ["ICLR.cc/2026/Conference/Submission4870/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4870/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4870/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763569684174, "cdate": 1763569684174, "tmdate": 1763593767034, "mdate": 1763593767034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Time-to-Move (TTM), a training-free, plug-and-play motion control method for image-to-video diffusion models. Core ideas: (i) generate a crude reference animation (cut-and-drag or depth reprojection) and adapt SDEdit to inject its motion, and (ii) a region-dependent “dual-clock” denoising that uses a lower noise level inside a motion mask and a higher one elsewhere to balance adherence vs. realism. The method claims strong object/camera motion control across SVD, CogVideoX, and Wan backbones with no extra training, with quantitative gains on MC-Bench and DL3DV."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Training-free & backbone-agnostic; integrates with multiple I2V backbones.\n\n2. Clear articulation of the dual-clock idea with ablations.\n\n3. Competitive metrics and clean qualitative examples for object and camera motion control."}, "weaknesses": {"value": "1. Missing related-work discussion: structural-noise initalization like in video-MSG [1]\n\n2. Sensitivity to the applied template (reference animation + mask + schedule). TTM critically depends on: (a) how the warped video is produced (cut-drag vs. depth reprojection, inpainting strategy), (b) the mask quality, and (c) the chosen (t_weak, t_strong) schedule. The paper acknowledges per-model tuning and that identity is anchored only by the first frame, but doesn’t deeply analyze robustness to template variance \n\n3. Even though figures highlight cleaner results vs. baselines, the pipeline can inherit issues from the template (e.g., depth reprojection tearing/holes) and from hard masking. The paper partly recognizes this but artifact analysis is light.\n\n[1] Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization, 2025"}, "questions": {"value": "please see weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lG3HcNcTaN", "forum": "OxO9OSYVw5", "replyto": "OxO9OSYVw5", "signatures": ["ICLR.cc/2026/Conference/Submission4870/Reviewer_bHQm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4870/Reviewer_bHQm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137560790, "cdate": 1762137560790, "tmdate": 1762917623725, "mdate": 1762917623725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}