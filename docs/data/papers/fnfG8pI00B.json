{"id": "fnfG8pI00B", "number": 21806, "cdate": 1758322110376, "mdate": 1763691595921, "content": {"title": "When Machine Learning Gets Personal: Evaluating Prediction and Explanation", "abstract": "In high-stakes domains like healthcare, users often expect that sharing personal information with machine learning systems will yield tangible benefits, such as more accurate diagnoses and clearer explanations of contributing factors. However, the validity of this assumption remains largely unexplored. We propose a unified framework to fairly quantify if personalizing a model improves both prediction and explanation for every group who provide personal data. We show that its impacts on prediction and explanation can diverge: a model may become more or less explainable even when prediction is unchanged. For practical settings, we study a standard hypothesis test for detecting personalization effects on demographic groups. We derive a finite-sample lower bound on its probability of error as a function of group sizes, number of personal attributes, and desired benefit from personalization. This provides actionable insights, such as which dataset characteristics are necessary to test an effect, or the maximum effect that can be tested given a dataset. We apply our framework to real-world tabular datasets using feature-attribution methods, uncovering scenarios where effects are fundamentally untestable due to the dataset statistics. Our results highlight the need for joint evaluation of prediction and explanation in personalized models and the importance of designing models and datasets with sufficient information for such evaluation.", "tldr": "", "keywords": ["Fairness", "explainability", "personalization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ca733014a11f3c87eac833a171ab16f4a2f0b0fe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a unified framework to evaluate how *personalization* in machine learning affects both **prediction accuracy** and **explanation quality** across groups.  \nIt extends the *Benefit of Personalization (BoP)* from binary classification to regression and explanation metrics (sufficiency, incomprehensiveness), proving that personalization can improve or harm interpretability even when accuracy is unchanged.  \nThe authors derive finite-sample limits on the reliability of hypothesis tests for personalization and apply their framework to healthcare datasets (MIMIC-III, UCI Heart), showing that personalization effects are often untestable given dataset size and attribute structure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Extends BoP theory to continuous and explanation-based settings, bridging personalization, fairness, and explainability.  \n- Offers clear theoretical results (Thms. 4.1–4.3) showing prediction and explanation gains can diverge; provides a rare link between them under additive models.  \n- Derives practical finite-sample error bounds that guide when personalization tests are statistically valid.  \n- Empirical study (MIMIC-III) effectively illustrates the theoretical limits, highlighting cases where high empirical gains are statistically meaningless.  \n- Well-motivated and relevant to fairness, interpretability, and trustworthy AI."}, "weaknesses": {"value": "- The explanation method is left vague in the main text; it becomes clear only in Appendix A. Stating early that the framework is demonstrated with Integrated Gradients, DeepLIFT, and SHAP would help.  \n- Heavy notation and scattered definitions (Tables 1 & 3) slow reading; concise examples or visual summaries could clarify.  \n- Some theorems (e.g., 4.3) claim necessity (“always”) rather than possibility (“can”)—the text should match this (compare lines 232 vs 242)\n- Too much of the intuition (e.g., Fig. 6) is tucked into appendices, reducing accessibility. You have some nice plots!\n- Limited discussion of connections to fairness of recourse (implying/not implying fairness of prediction) literature, despite thematic overlap."}, "questions": {"value": "1. Does the framework apply beyond feature-attribution explainers (e.g., to counterfactual or surrogate explanations)?  \n2. Could the additive-model alignment in Thm. 4.3 generalize to linear or kernel settings?  \n3. How robust are the lower-bound results to distributional misspecification (e.g., Laplace vs Gaussian)?  \n4. Can the framework test *absence of harm* rather than *presence of benefit*?\n\n\n### Comment on Table 3 (Appendix B):\nTable 3 could be made more consistent with Table 1, particularly regarding the inclusion of s∖Js_{\\setminus J}s∖J​ in the definition of incomprehensiveness. Clarifying this alignment would improve coherence across the theoretical and empirical sections.\nIt would also help readers if the caption or surrounding text explicitly stated that higher BoP values in each row indicate greater benefit for subgroup sss, reinforcing the interpretability of the metrics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3N0ulBIQsg", "forum": "fnfG8pI00B", "replyto": "fnfG8pI00B", "signatures": ["ICLR.cc/2026/Conference/Submission21806/Reviewer_k2Uy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21806/Reviewer_k2Uy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924647251, "cdate": 1761924647251, "tmdate": 1762941937290, "mdate": 1762941937290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper assesses the impact of personal information on explanation fidelity, fairness and model performance claiming that impact on predictions and explanations can be different."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* XAI is a hot topic with many open questions and investigating personalization and faithfulness is an interesting angle\n* The paper aims at a strong mathematical underpinning paired with real-world examples"}, "weaknesses": {"value": "* The paper should early on state that they rely on importance scores, which covers only a subset of XAI methods though arguably the most important one (as the authors claim). Also the emphasis is on tabular data, which is a further narrowing down. Both should be clear from the abstract.\n\n* The setting is not fully clear. On the one hand, the title should contain fairness rather than prediction (pointing towards overall prediction accuracy) as fairness seems to be the focus and not prediction accuary as also stated \"We propose a framework to evaluate whether this weaker fairness criterion is satisfied, both theoretically and empirically, rather than proposing corrective algorithms.\"   . In turn, also some of the abstract and introduction might require rephrasing. On the other hand, H1 points towards an improvement of all groups. It is better to make one clear case on either of the two.\n\n* There is no passage in the appendix (or main part) for theorm 5.1. called like \"proof of theorem 5.1\" ( it might have been called proof for theorem D.3 / 4.3 (which exist) or so), but I could not find the proof. While this might be a minor issue, it essentially makes the work non-verifyable.\n\n* Given the inaccessibility of the proof (prior comment), I focused on the intution provided, which could be expanded. Essentially, the paper appears to say that if there is colinearity among features (i.e., the personalization ones and others) , faithfulness can be jeopardized.  While this sounds reasonable, it is not clear, why that must be really the case. It seems not really specific to personalization but a rather general issue with colinearity. Do you agree?\n\n* The case-study style evaluation is not appreciated. It seems to jeopardize generalization and also makes an assessment clear given by all the practioners advice - a reference to an Appendix is of little help here, but some kind of convincing overview would be better. The promise of providing clear datasets characteristics (mentioned in abstract) to see when personalization hurts seems to be missing - or where is it? \n\nDetail: use faithfulness/fidelity in the main part of the paper, not just in the Appendix, as it is a well-known term and clarifies some of our complex arguign, ie.  the sentence \"this is by design:....\" is obsolete if you clearly speak of fidelity."}, "questions": {"value": "see above.\n\n* Why do you need your framework and cannot just assess fairness using one of the many existing ways, ie.:\n1) Add personalization features -> compute fairness metrics\n2) Remove personalization features -> compute fairness metrics\n3) Compare"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yZH7H5HYJ3", "forum": "fnfG8pI00B", "replyto": "fnfG8pI00B", "signatures": ["ICLR.cc/2026/Conference/Submission21806/Reviewer_BgiC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21806/Reviewer_BgiC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939118664, "cdate": 1761939118664, "tmdate": 1762941936950, "mdate": 1762941936950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to measure how personalisation in machine learning impacts model performance and explainability. The authors show that, for the Bayes optimal classifier, accuracy and explainability are not necessarily aligned. However, when the data distribution follows an additive model, they show that there is a strong relationship between accuracy and explainability. The paper presents a statistical test to determine whether personlisation leads to improvements in accuracy and/or explainability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Paper is well written and clear for the most part\n- Figure 3 and example scenario in Section 6 may be helpful for practitioners who might use the framework"}, "weaknesses": {"value": "**Weak Problem Statement**\n\nThere are two facets here:\n1. Does this actually occur in practice?\n2. If yes, what are the ramifications?\n\nThe authors don't really address these questions. I'd imagine the most salient case here is when we see a benefit in predictive accuracy but not in explainability (which kind of seems to be the case in Table 2?). For example, Monteiro Paes et al. (2022), demonstrate that personalisation may lead to harm on the second page (Table 1). Even still, we need to **infer** why a decrease in incomprehensiveness (or sufficiency) is undesirable. The authors should explicitly discuss the harm here.\n\nI know that the authors state that the BoP framework is metric agnostic, but they should note that they have chosen these metrics to use throughout the paper, including in theoretical results.\n\nThe main motivation in this paper is:\n> A common intuition is that if personalization improves prediction, it must also improve explanations (Del Giudice, 2024) (lines 189-190).\n\nHowever, this doesn't seem to be the point of Del Giudice (2024). \"Explanation quality\" in this paper is different. Del Giudice is talking about the **model's** ability to represent real-world phenomena:\n> The cost is that maximizing the predictive accuracy of a model tends to sacrifice its ability to represent the underlying phenomenon in an accurate and interpretable fashion (page 24).\n\nThis differs from the explanation metrics one might use in XAI, which aims to measure whether explanations faithfully explain **the model** (not nature).\n\n**Results**\n> Insight: The choice of improvement threshold $\\varepsilon$ is key.\n\nThe paragraph here is just repeating the theoretical result in words; its quite trivial. The authors need to elaborate on this further.\n\nMoreover, I can't seem to find the results that accompany the first and last insight.\n\n**Overall Takeaway**\n\nThe authors conclude that there are limits to testing whether there is a benefit to personlisation. However, Monteiro Paes et al. have already demonstrated this in their work. Instead, if the authors have shown a tighter bound, they should frame the takeaway in light of those results."}, "questions": {"value": "- Is it easy to construct distributions that show Theorem 4.1 and 4.2? I would be interested in how often this arises in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ufpLYFeaUm", "forum": "fnfG8pI00B", "replyto": "fnfG8pI00B", "signatures": ["ICLR.cc/2026/Conference/Submission21806/Reviewer_dhvG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21806/Reviewer_dhvG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762201920669, "cdate": 1762201920669, "tmdate": 1762941936713, "mdate": 1762941936713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discusses personalization and explainability of machine learning models and prove the relationship between these two. The paper then discusses hypothesis testing, lower bound, practical aspects of the theory and concludes with experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I am not really an expert in this field so just offering my best evaluation here. \n\nThe paper is well written and provides important insights on the personalization and explainability. The sections on their relationship are well-thought and the theoretical aspects are sound."}, "weaknesses": {"value": "I am not familiar with the relevant literature but the paper is quite original and contains insightful theories."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cQdKwtSLWI", "forum": "fnfG8pI00B", "replyto": "fnfG8pI00B", "signatures": ["ICLR.cc/2026/Conference/Submission21806/Reviewer_bsq1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21806/Reviewer_bsq1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762406380716, "cdate": 1762406380716, "tmdate": 1762941936370, "mdate": 1762941936370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We are genuinely grateful for the quality and depth of feedback provided by all reviewers. Their constructive criticism and insightful suggestions have improved our paper's clarity, rigor, and impact. Based on the common concerns raised, we have made revisions to the manuscript (highlighted in blue for easy identification) that we hope strengthen our contribution.\n\nWe welcome continued discussion and remain committed to further revisions that strengthen our contribution to the field. Detailed responses to individual reviewer comments follow below."}}, "id": "mmNOSUs6uI", "forum": "fnfG8pI00B", "replyto": "fnfG8pI00B", "signatures": ["ICLR.cc/2026/Conference/Submission21806/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21806/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission21806/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763691711047, "cdate": 1763691711047, "tmdate": 1763691711047, "mdate": 1763691711047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}