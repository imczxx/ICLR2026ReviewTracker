{"id": "f2RSY0sNii", "number": 10134, "cdate": 1758161625079, "mdate": 1759897671847, "content": {"title": "FedANC: Adaptive Sparse Noise Scheduling for Federated Differential Privacy", "abstract": "Federated Learning (FL) enables multiple clients to collaboratively train a shared model without sharing raw data. Although this reduces direct exposure of local data, model updates can still leak sensitive information through gradient-based attacks. Differential Privacy (DP) mitigates this risk by adding calibrated noise to updates, providing formal guarantees. However, most existing DP-FL methods adopt fixed noise scales and uniform injection across all gradient dimensions, without adapting to client heterogeneity or training dynamics. This often results in poor privacy-utility trade-offs. To overcome these limitations, we propose FEDANC, an adaptive differential privacy framework for FL. It consists of three components: (i) an Adaptive Noise Controller (ANC) with an LSTM-based design that generates client-specific noise scales and sparsity ratios from local training feedback; (ii) a Selective Noise Injection mechanism that perturbs only the most sensitive gradient entries; and (iii) a Privacy Budget Regularization term that aligns per-round updates with a predefined privacy target. For stability, the ANC is pretrained with synthetic feedback that simulates typical training behavior. We provide theoretical guarantees on both convergence and differential privacy. Extensive experiments demonstrate that FEDANC achieves higher accuracy, faster convergence, and stronger privacy protection compared with existing approaches.", "tldr": "", "keywords": ["Federated Learning", "Differential Privacy", "Adaptive Noise Controller", "Sparse Gradient Perturbation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d181ec946eb99aa2974a2eab9c7e5067eeb2caa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a new method for federated learning with differential privacy. An LSTM controller adaptively adds calibrated noise to certain components of the gradients during client training in a federated averaging framework. The goal is to reduce the total noise by only adding it to the larger components, and adapting it to each user's data."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The method is original, and the writing is mostly clear, although some of the figures are too small."}, "weaknesses": {"value": "The proposed FedANC framework is highly complex, requiring a pretrained LSTM controller on each client, a selective top-k noise mechanism, and a novel privacy regularization term. While Figure 6 shows utility gains, it's unclear if these benefits outweigh the significant implementation, pretraining, and (modest) computational overhead.\n\nTo support a claim of improved privacy-utility trade-off, I think it would be much stronger to compare model utility (e.g., test accuracy) while holding the total privacy budget ($\\epsilon$, $\\delta$) constant across all methods. The current experiments (e.g., Figure 6) compare utility against communication rounds, which is insufficient to demonstrate a superior trade-off.\n\nMost concerningly, the privacy guarantee presented in Theorem 2 and Section 3.3 appears to be invalid. The \"Selective Noise Injection\" mechanism is data-dependent, as the choice of which top-k gradient components to perturb is a function of the private data. The paper's analysis only accounts for the privacy cost of noising the values of these components, while ignoring the information leaked by selecting them. This data-dependent selection process, which leaves other components un-noised, would seem to break the formal definition of differential privacy leading to a formal $\\varepsilon$ of $\\infty$."}, "questions": {"value": "Consider this counterexample to the formal privacy guarantee claim. Take two neighboring datasets $D$ and $D'$ that differ in a single user $u$. Let all users in $D$ have a gradient of 0 for a specific component $j$. Let $D'$ be the same except let $u$ have a large-magnitude gradient for component $j$.\n* On dataset $D$: Component $j$ will always be 0. It will never be in the top k and will never receive noise. The aggregated update for $j$ will be exactly 0.\n* On dataset $D'$: The large gradient will place component $j$ in the top k, causing it to be perturbed with noise. The aggregated update for $j$ will be non-zero with high probability.\n\nAn observer can distinguish $D$ from $D'$ with near certainty by checking if component $j$ is zero, so there is no finite $\\epsilon$ that satisfies the definition of differential privacy."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)", "Yes, Other reasons (please specify below)"]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "This is very similar to another paper I have just reviewed: \"10194\tFedMAP: Meta-Driven Adaptive Differential Privacy for Federated Learning\". It is another flawed method using a deep learning \"controller\" to adjust DP parameters during FL. Much of the text, particularly in the introduction, is copied verbatim. The flavor of the intended \"contribution\" is similar, and they suffer from the same fatal flaw (not accounting for privacy loss of passing private data through the controller and releasing the result). I'm not certain whether this counts as dual submission, but I feel that the authors are intentionally submitting a set of very similar papers with the hopes that one of them will get a favorable set of reviewers. I would advise the program committee to check any other papers submitted by these authors."}}, "id": "FeiuMssgBI", "forum": "f2RSY0sNii", "replyto": "f2RSY0sNii", "signatures": ["ICLR.cc/2026/Conference/Submission10134/Reviewer_e2Yr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10134/Reviewer_e2Yr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780509446, "cdate": 1761780509446, "tmdate": 1762921508592, "mdate": 1762921508592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FEDANC, a federated learning framework that incorporates adaptive differential privacy through three main components: (i) an LSTM-based Adaptive Noise Controller (ANC) that generates client-specific noise scales and sparsity ratios from local training feedback, (ii) a selective noise injection mechanism that perturbs only top-k gradient entries, and (iii) a privacy budget regularization term that aligns per-round updates with a predefined privacy target. The ANC is pretrained on synthetic data to ensure stability. The authors provide theoretical convergence and privacy guarantees, and evaluate the framework against gradient inversion attacks on CIFAR-10, FashionMNIST, and HARBox datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Novel integration approach: The paper presents an interesting combination of adaptive privacy parameter generation, sparse gradient perturbation, and budget regularization within a unified framework.\nTheoretical analysis: The authors provide formal convergence guarantees (Theorem 1) for both convex and non-convex objectives, as well as differential privacy guarantees (Theorem 2) using the Moments Accountant.\nComprehensive experimental evaluation: The paper evaluates against multiple gradient inversion attacks (DLG, IG, GI) across three datasets with different model architectures, demonstrating broad applicability.\nPractical consideration of heterogeneity: The framework addresses client heterogeneity through personalized ANC instances, which is relevant for real-world federated learning deployments."}, "weaknesses": {"value": "W1. The ANC takes a 4-dimensional input (|\\mathbf{g}t|2, \\ell_t, \\beta{t-1}, \\gamma{t-1}) and outputs 2 values (\\beta_t, \\gamma_t). The pretraining uses synthetic data where both inputs (Equation 6) and outputs (Equation 7) are sampled from independent uniform distributions with no inherent relationship. Training an LSTM to fit random noise to random targets lacks principled justification. Figure 3 shows minimal variation in output parameters, suggesting the controller may output near-constant values rather than performing meaningful adaptation. The paper does not provide a clear rationale for why this random pretraining would lead to effective adaptation during actual federated training.\nW2. While the paper cites neural architecture search (Zoph & Le, 2017), meta-learning (Jiang et al., 2019), and adaptive DP (Li et al., 2022) to motivate pretraining (lines 193-198), none of these works employ or validate pretraining on independently generated random inputs and random targets. This gap significantly weakens the theoretical foundation of the proposed pretraining strategy.\nW3. Based on Equations (9-10), \\gamma_t and \\beta_t are fundamentally coupled through the privacy constraint \\hat{\\epsilon}t = \\sqrt{2\\gamma_t d \\ln(1.25/\\delta)} / \\beta_t. Given a target privacy budget \\epsilon{\\text{target}}, specifying one parameter determines the other. The paper does not acknowledge or address this coupling, making it unclear how the controller provides independent adaptation of two parameters that are mathematically constrained.\nW4. The privacy regularization loss \\mathcal{L}^{(t)}_{\\text{privacy}} = (\\hat{\\epsilon}t - \\epsilon{\\text{target}})^2 can theoretically be set to zero by appropriately choosing \\beta_t given \\gamma_t (or vice versa), indicating only one degree of freedom exists rather than two. This redundancy is not discussed, raising questions about what the controller actually learns and whether both parameters are necessary.\nW5. Table 1 and accompanying text do not report privacy budget (\\epsilon, \\delta) values for FEDANC or baseline DP methods. Without these values, the evaluation measures only empirical attack difficulty, not formal differential privacy guarantees under equivalent privacy constraints. \nW6. Table 1 reports only attack metrics (MSE, PSNR, SSIM) without including model test accuracy for each defense method. \nW7.  Algorithm 1 contains critical ambiguities in mask generation. Lines 8-15 iterate over multiple batches per epoch, each producing different sparse patterns through top-k selection. Line 17 states \"Generate binary mask \\mathbf{M}_k; send \\mathbf{W}_k \\odot \\mathbf{M}_k to server\" without specifying which batch's mask is used or how masks from multiple batches are aggregated. \nW8. Lines 456-457 mention \"DPSA-FL strategy\" which is never defined elsewhere in the paper and has no corresponding citation."}, "questions": {"value": "Q1- The paper claims \"low-magnitude gradients usually carry less sensitive information\" (Section 3.2, lines 216-219) based on empirical attack studies. Can the authors provide formal theoretical justification or proofs demonstrating that low-magnitude gradient components inherently contain less sensitive information? Without theoretical grounding, this remains an empirical assumption that may not hold universally.\n\nQ2- How many synthetic samples are used for pretraining the ANC module? \n\nQ3- The paper states that pretraining \"guides the controller toward stable parameter regions\" (lines 189-192). Given that inputs (Equation 6) and outputs (Equation 7) are independently sampled from uniform distributions with no functional relationship, what is the mechanism by which fitting random inputs to random targets produces stable and reliable parameters for actual federated learning?\n\nQ4- What specific (\\epsilon, \\delta) values were used for FEDANC and baseline DP methods in Table 1? How were these budgets allocated across rounds, and were all methods compared under equivalent total privacy budgets?\n\nQ5- Given the coupling between \\gamma_t and \\beta_t described in W3, how does the controller provide meaningful independent adaptation of these two parameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9ohB6iUOhA", "forum": "f2RSY0sNii", "replyto": "f2RSY0sNii", "signatures": ["ICLR.cc/2026/Conference/Submission10134/Reviewer_eQXG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10134/Reviewer_eQXG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859210610, "cdate": 1761859210610, "tmdate": 1762921505652, "mdate": 1762921505652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedANC, a differentially private federated learning (FL) algorithm. The proposed method first uses an LSTM to decide privacy parameters, then add privacy noise to the top-k entries of the gradient, and update a masked model update to the server.\n\nThe paper provides theoretical convergence and privacy guarantee for the proposed method and numerical results show than the proposed algorithm can defend against privacy attacks while achieving better model performance."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The numerical reuslts of the model shows that the proposed method can defend against different privacy attacks compared with non-DP algorithms.\n\n2. The numerical comparision shows that the proposed method achieves better performance than other privacy preserving algorithms."}, "weaknesses": {"value": "1. On the presentation level, the paper failed to provide a clear algorithm in the main paper. It is hard to follow the steps of the algorithm.\n\n2. The paper failed to provide a solid privacy analysis to the algorithm. It is hard to understand why the privacy noise is only added to the top-k entries and still protects privacy. A more rigorous privacy analysis on the mechanism should be provided.\n\n3. It is unclear which privacy level the paper is trying to achieve. In FL, there are different levels of privacy protection, including client/user-level, local and server level. The paper should provide a formal statement to the setting.\n\n4. The ANC network uses the true gradient information to decide the privacy parameter, which may already leak privacy. The paper should provide further justification on how the ANC network involves in privacy protection.\n\n5. The numerical comparision is incomplete. In the privacy defence, the paper failed to include DP based method."}, "questions": {"value": "Please address the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ddlFaYOrYW", "forum": "f2RSY0sNii", "replyto": "f2RSY0sNii", "signatures": ["ICLR.cc/2026/Conference/Submission10134/Reviewer_WroX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10134/Reviewer_WroX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934937990, "cdate": 1761934937990, "tmdate": 1762921502000, "mdate": 1762921502000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedANC, a differentially private federated learning (FL) algorithm. The proposed method first uses an LSTM to decide privacy parameters, then add privacy noise to the top-k entries of the gradient, and update a masked model update to the server.\n\nThe paper provides theoretical convergence and privacy guarantee for the proposed method and numerical results show than the proposed algorithm can defend against privacy attacks while achieving better model performance."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The numerical reuslts of the model shows that the proposed method can defend against different privacy attacks compared with non-DP algorithms.\n\n2. The numerical comparision shows that the proposed method achieves better performance than other privacy preserving algorithms."}, "weaknesses": {"value": "1. On the presentation level, the paper failed to provide a clear algorithm in the main paper. It is hard to follow the steps of the algorithm.\n\n2. The paper failed to provide a solid privacy analysis to the algorithm. It is hard to understand why the privacy noise is only added to the top-k entries and still protects privacy. A more rigorous privacy analysis on the mechanism should be provided.\n\n3. It is unclear which privacy level the paper is trying to achieve. In FL, there are different levels of privacy protection, including client/user-level, local and server level. The paper should provide a formal statement to the setting.\n\n4. The ANC network uses the true gradient information to decide the privacy parameter, which may already leak privacy. The paper should provide further justification on how the ANC network involves in privacy protection.\n\n5. The numerical comparision is incomplete. In the privacy defence, the paper failed to include DP based method."}, "questions": {"value": "Please address the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ddlFaYOrYW", "forum": "f2RSY0sNii", "replyto": "f2RSY0sNii", "signatures": ["ICLR.cc/2026/Conference/Submission10134/Reviewer_WroX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10134/Reviewer_WroX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934937990, "cdate": 1761934937990, "tmdate": 1763660739180, "mdate": 1763660739180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an algorithm to using a learned controller to adaptively control noise added to the client-side gradient in FedAvg framework, with the target being improving algorithm performance under fixed privacy budget."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper covers convergence guarantee, privacy guarantee, and experiment."}, "weaknesses": {"value": "1. The major weakness I feel is correctness of privacy guarantee. The paper use gradient dependent sparsity operator to get top-k largest coordinate of gradient, but this step is not privatized, only final top-k coordinates are privatized. This indicate the algorithm may not actually be private.\n2. The paper does not have any pseudo code of algorithm. It is unclear whether the algorithm is updating based on sparse gradient or dense gradient. In figure 1 it seems sparse gradients are used implied by the \"sparse avg aggerate\" below server figure, but I don't see anything formally mentioned or introduced the algorithm steps in a clear manner."}, "questions": {"value": "1. Is the algorithm private? I am concerned that the top-k operator is not privatized.\n2. Do server update parameters using sparsified gradient?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WlKSEjAMX6", "forum": "f2RSY0sNii", "replyto": "f2RSY0sNii", "signatures": ["ICLR.cc/2026/Conference/Submission10134/Reviewer_PxPp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10134/Reviewer_PxPp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762110090971, "cdate": 1762110090971, "tmdate": 1762921501000, "mdate": 1762921501000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Area Chair and Program Committee,\n\nWe strongly reject Reviewer e2Yr’s (Submission #10134) accusations of plagiarism, dual submission, and technical flaws. The same reviewer, identified as P77G in Submission #10194, appears to have reviewed both of our papers — FedANC: Adaptive Sparse Noise Scheduling for Federated Differential Privacy and FedMAP: Meta-Driven Adaptive Differential Privacy for Federated Learning — and provided nearly identical reviews. These reviews contain false allegations, factual mistakes, and reflect malicious reviewing behavior rather than an objective evaluation. We respectfully request that the Program Committee investigate this reviewer’s conduct and potential conflicts of interest.\n\n1. On the Relationship Between the Two Papers\n\nWe acknowledge that both papers are indeed authored by our team. However, they address entirely different research questions and are based on distinct problem formulations, algorithmic designs, and technical contributions. It is misleading and unprofessional to equate them based on superficial similarities in background or structure, which are common across works in this research area.\n\n| **Aspect**           | **FedANC**                                                   | **FedMAP**                                                   |\n| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Controller type      | LSTM-based Adaptive Noise Controller (ANC)                   | Lightweight MetaNet (BERT-tiny encoder)                      |\n| Controlled variables | Noise scale $β_t$ and sparsity ratio $\\gamma_t$              | Clipping threshold $C_t$ and noise scale $\\sigma_t$          |\n| Mechanism            | Sparse Top-k noise injection on selected gradient coordinates | Full-dimensional adaptive DP-SGD                             |\n| DP accounting        | Local privacy regularization $\\hat{\\varepsilon}_t = \\frac{\\sqrt{2\\gamma_t d\\ln(1.25/\\delta)}}{\\beta_t}$ | Server-side Rényi DP accountant tracking $\\varepsilon_{\\text{global}}$ |\n| Feedback design      | Local-only adaptation                                        | Closed-loop global feedback                                  |\n| Aggregation          | Sparsity-aware aggregation                                   | Standard FedAvg                                              |\n\nThese differences are clear, verifiable, and explicitly described in both manuscripts. The reviewer’s claim that the two papers are \"nearly identical\" is factually false.\n\n2. On the Misunderstanding of Privacy Accounting\n\nThe reviewer claims both papers \"ignore the privacy loss of passing private data through the controller\".\nThis is incorrect and shows a fundamental misunderstanding.\n\nFedANC:\nThe ANC runs only on the client side. Inputs $(|g_t|2, \\ell_t, \\beta_{t-1}, \\gamma_{t-1})$ are local and never transmitted. Outputs $(\\beta_t, \\gamma_t)$ only determine local Gaussian noise; controller outputs are not uploaded. By the post-processing property of DP, privacy guarantees remain valid. Theorem 2 bounds cumulative privacy loss as $\\varepsilon_R = O(\\beta_{\\min}^{-1}\\sqrt{R\\gamma_{\\max}d})$ under bounded parameters.\n\nFedMAP:\nThe MetaNet outputs $(C_t, \\sigma_t)$ locally to adjust clipping and noise. The server’s Rényi DP accountant integrates these into total user-level $(\\varepsilon, \\delta)$-DP. Both analyses are mathematically sound and complete.\n\nTherefore, the claim of a \"fatal flaw\" is entirely unfounded.\n\n3. Pattern of Repeated and Biased Reviews\n\nBoth submissions received nearly identical reviews, repeating claims such as \"copied introduction text\", \"flawed controller design\", and \"recommend checking all papers by these authors\", without evidence or engagement with technical content.\nThis repetition demonstrates a pattern of bias and possibly malicious intent, rather than independent evaluation. Such behavior undermines the fairness and integrity of peer review.\n\n4. Request for Investigation\n\nGiven the seriousness of these issues, we respectfully ask the Program Committee to:\n\n- Investigate potential conflicts of interest for Reviewer e2Yr/P77G, including any connections to competing FL–DP research.\n- Audit the reviewer’s activities to verify whether identical or template reviews were submitted across our papers.\n- Remind reviewers that ethical accusations such as plagiarism or dual submission must be supported by evidence and factual basis.\n- Examine whether the reviewer or their collaborators have submissions overlapping with ours, which could create an incentive for biased reviewing to improve their own acceptance chances.\n\nWe believe that scientific evaluation must be based on technical merit and factual accuracy, not speculation or personal bias.\nWe trust the Program Committee will uphold fairness, transparency, and academic integrity in handling this matter.\n\nSincerely,\n\n10134 Authors"}}, "id": "736VEtlD80", "forum": "f2RSY0sNii", "replyto": "f2RSY0sNii", "signatures": ["ICLR.cc/2026/Conference/Submission10134/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10134/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10134/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762953714239, "cdate": 1762953714239, "tmdate": 1762955502883, "mdate": 1762955502883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their time and effort. The comments help us understand the weaknesses of our paper, and we will revise the work with care. We also understand that this paper is unlikely to be accepted, and we respect the review process.\n\nWe would like to express one serious concern. One review includes an ethics accusation that targets the personal integrity of the authors. This accusation is not supported by evidence in the submission and goes beyond a fair evaluation of the scientific content. As researchers, we cannot accept a statement that harms our dignity.\n\nWe support a fair and transparent review process. We welcome the Area Chair to examine all submissions from our team. We are confident that such an examination will confirm that our work follows normal academic practice. We again thank all reviewers for their comments and will continue to improve our research."}}, "id": "zD0KJUCWdx", "forum": "f2RSY0sNii", "replyto": "f2RSY0sNii", "signatures": ["ICLR.cc/2026/Conference/Submission10134/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10134/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission10134/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762992382697, "cdate": 1762992382697, "tmdate": 1762992382697, "mdate": 1762992382697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}