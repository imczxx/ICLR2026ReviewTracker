{"id": "yADIMxHaXW", "number": 20315, "cdate": 1758304693546, "mdate": 1759896984443, "content": {"title": "Fast Training Dataset Attribution via In-Context Learning", "abstract": "We investigate the use of in-context learning and prompt engineering to estimate the contributions of training data in the outputs of instruction-tuned large language models (LLMs). We propose two novel approaches: (1) a similarity-based approach that measures the difference between LLM outputs with and without provided context, and (2) a mixture distribution model approach that frames the problem of identifying contribution scores as a matrix factorization task. Our empirical comparison demonstrates that the mixture model approach is more robust to retrieval noise in in-context learning, providing a more reliable estimation of data contributio", "tldr": "We investigate the use of in-context learning and prompt engineering to estimate the contributions of training data in the outputs of instruction-tuned large language models (LLMs)", "keywords": ["Data influence"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f38a6656b1da242a2eb51f82b9c43b10bbf2b8ad.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates an important and interesting topic: the contributions of training data to the outputs of instruction-tuned large language models (LLMs). \nThe authors propose two novel and easy-to-use methods: the Shapley Context Method and Context Mixture Factorization.\nExperiments across various datasets show that the Context Mixture Factorization method consistently estimates data contributions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces two novel approaches (SCM and CMF) to address an important and interesting problem: estimating the contribution of training data to the outputs of instruction-tuned LLMs.\n\n2. The paper is generally well-structured and easy to follow. The methodology is described in clear detail.\n\n3. Comprehensive empirical analysis and ablation studies in the experiment section."}, "weaknesses": {"value": "1. Missing comparison: The paper only compares SCM and CMF with the TRAK method proposed in 2023. The paper should compare SCM and CMF with the advanced and recent baseline methods from other studies. Such an analysis could offer valuable insights into the strengths and weaknesses of the proposed approaches.\n\n2. Limited Improvement: Tables 1â€“3 show that the results of CMF and the baseline TRAK are quite similar, especially for the Phi-3 model. Moreover, SCM performs worse than TRAK. It would be helpful if the authors could further clarify the advantages of the proposed methods.\n\n3. Lacks an analysis of its limitations. The success of the proposed methods relies on the assumptions shown in Remarks 1 and 2. It would be valuable for the authors to discuss specific scenarios in which these assumptions may not hold and the proposed methods are likely to fail. \n\n4. Missing analysis on the selection strategy and the number of demonstrations used in ICL: It would be useful if the authors also studied how different selection strategies (e.g., EPR [1] and DPP [2]) and varying numbers of demonstrations (e.g., K = 4, 8, 16) affect the performance of the proposed methods.\n\n5. Notation issues: The notation system of this paper can be further improved. For example, the definition of $c_S$ is unclear. \n\n6. Citation issues: The author needs to correct the citation format of the whole paper. For example, in line 25, the citation format should be in the form of (Xia et al., 2024; Qin  et al., 2025).\n\n7. The absence of code and generated datasets makes it difficult for the broader research community to reproduce the results claimed in the paper or verify the method's effectiveness on the tasks.\n\n[1] Learning to retrieve prompts for in-context learning.\n\n[2] Compositional exemplars for in-context learning"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aClArQ7qiZ", "forum": "yADIMxHaXW", "replyto": "yADIMxHaXW", "signatures": ["ICLR.cc/2026/Conference/Submission20315/Reviewer_SXoQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20315/Reviewer_SXoQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760698057687, "cdate": 1760698057687, "tmdate": 1762933779040, "mdate": 1762933779040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of Training Data Attribution for large language models, specifically focusing on the black-box setting where internal model gradients and parameters are inaccessible. The authors note that existing TDA methods, such as retraining-based or influence function-based approaches, are often computationally expensive or require white-box access, making them unsuitable for many modern LLMs . To address this limitation, the paper proposes a framework that utilizes In-Context Learning and Retrieval-Augmented Generation. The core idea is to use RAG to provide dataset-specific context within a prompt and then measure the resulting change in the LLM's output to estimate the dataset's contribution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "It introduces the use of In-Context Learning and Retrieval-Augmented Generation as a non-intrusive \"probe\" to estimate dataset contributions in a black-box setting . This conceptual reframing of ICL/RAG from a performance-enhancement mechanism to a diagnostic tool for attribution is a new approach to the black-box TDA problem."}, "weaknesses": {"value": "1) The paper's primary proposed method, CMF, is built on theoretically weak foundations. The core assumption (Eq. 4), which models how ICL context intervenes in the mixture distribution, appears to be entirely heuristic. The paper provides no theoretical derivation or empirical validation for this specific functional form; it seems to be a mathematical construct specifically chosen to make the subsequent matrix factorization (Eq. 5) tractable, rather than a principled model of the LLM's actual behavior during ICL.\n2) The paper's experimental validation is entirely limited to binary question-answering Yes/No tasks. This is a low-dimensional, simplified task setting. It completely avoids the core challenge of TDA for modern LLMs: attributing free-form, generative text. The paper provides no evidence that its framework, which relies on the output probability distribution p(y|q,c), can be generalized to high-dimensional and complex generative tasks, which severely limits the claimed robustness and applicability of the findings.\n3) The methodology's heavy reliance on a RAG system introduces a critical confounding variable. The resulting attribution score is an ambiguous measure that conflates the LLM's reliance on its training data with the quality and effectiveness of the RAG retriever. For instance, a low attribution score for the Olympic2024 dataset could be attributed to the RAG's embedding model failing to retrieve relevant context, rather than purely reflecting the LLM's lack of training exposure. The experiments do not adequately disentangle this ambiguity."}, "questions": {"value": "1) Could the authors please elaborate on how the CMF framework, particularly the probability matrix P in Eq. 5, would be applied to free-form text generation tasks? What would the elements of P represent in that scenario sequence perplexity, specific token probabilities, or embedding similarities?\n2) The proposed method tightly couples attribution scores with the performance of the RAG system, making it difficult to distinguish whether a low attribution score reflects the LLM's training or simply the RAG system's retrieval quality. The authors need to provide an ablation study on RAG quality. For example, if RAG quality for a seen dataset is intentionally degraded , how much does the CMF score decrease? Conversely, if an oracle context is provided for an unseen dataset, how much does its CMF score increase?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pjMnPBiJOU", "forum": "yADIMxHaXW", "replyto": "yADIMxHaXW", "signatures": ["ICLR.cc/2026/Conference/Submission20315/Reviewer_p8pQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20315/Reviewer_p8pQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721115753, "cdate": 1761721115753, "tmdate": 1762933778605, "mdate": 1762933778605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes two black-box, no-retraining methods to estimate how much specific training datasets contribute to a LLM's outputs. The SCM treats dataset context as in-context learning and measures residual changes in outputs, aggregating over context combinations with Shapley values. The CMF models observed outputs as a mixture of latent dataset-specialized components and recovers mixture weights by a constrained, regularized alternating least-squares factorization. Experiments on BoolQ, FakeQ, Olympic2024 set show that CMF is more robust to retrieval noise and that both metrics increase monotonically under controlled fine-tuning that raises exposure to Olympic2024. The metrics are further used to rank machine unlearning methods, where adversarial-sample unlearning yields the largest attribution drops, and CMF is substantially more time- and resource-efficient than SCM and far more practical than Trak."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method can be applied to black-box LLMs and does not require re-training.\n\n2. Thoughtful dataset design covering likely seen, synthetic unseen but similar, and guaranteed unseen settings.\n\n3. Demonstrated downstream use for evaluating unlearning methods and strong runtime efficiency for CMF."}, "weaknesses": {"value": "1. The presentation of the proposed methodology is not smooth, which hinders comprehension. If the author intends to use mathematical symbols, it is necessary to clearly define them beforehand. Furthermore, many of the mathematical notations appear to be inaccurate. For example, denoting $p(y|q,c)$ as $M(q|c)$ is unconventional, and the notation used in Eq. (1) is imprecise, which may lead to misinterpretation of the author's intended meaning.\n\n2. The rationale behind the described methods, SCM and CMF, does not appear to be well-founded. The exposition relies heavily on assumptions, and since the algorithms are presented without supporting evidence or intuitive justification, the methodology lacks persuasiveness.\n\n3. As shown in Tables 1, 2, and 3, the baselines other than SCM and CMF appear insufficient. A more comprehensive comparison with additional baselines is necessary to empirically validate the effectiveness of the proposed methods.\n\n4. The main experiments focus on binary yes or no outputs."}, "questions": {"value": "1. Following Eq. (1), what exactly does $c_S$ denote?\n2. Is Eq. (3) an assumption proposed by the authors? If so, please explain the rationale for this assumption in greater detail.\n3. Please provide a more detailed derivation of the step from Eq. (3) to Eq. (4)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aXVigbUUDG", "forum": "yADIMxHaXW", "replyto": "yADIMxHaXW", "signatures": ["ICLR.cc/2026/Conference/Submission20315/Reviewer_x1MU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20315/Reviewer_x1MU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893322725, "cdate": 1761893322725, "tmdate": 1762933777834, "mdate": 1762933777834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes two data attribution methods that can estimate the relative importance of different datasets to a model's prediction. Compared with other approaches such as Trak, these methods are claimed to be more efficient and accurate."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The potential to create a reliable data attribution method that is computationally efficient is good."}, "weaknesses": {"value": "I found the explanation of the methods in this paper highly confusing. For 2.1, the most salient issues are:\n* Line 074: \"context\" is not clearly defined. Based on the datasets used, it seems that \"context\" is a passage that is relevant for answering the question, but in other places \"context\" appears to refer to in-context demonstrations from a dataset.\n* Line 092/Equation 1: While claimed to be a definition of $s_k$, this is not actually a definition as \"sim\" is also not defined. As a more minor point, I think this should say something like $sim(y_1, y_2 | c_k)$ instead of $sim(y, y | c_k)$ since the two y's should presumably be different.\n* Line 098: Similarly, the definition of $s_S$ is unclear, both for the reason above (\"sim\" is not defined) and also because $S$ and $c_S$ are not defined. Given Equation 2, S probably denotes a set of datasets, so maybe $c_S$ is the concatenation of contexts from examples in $S$?\n* Line 107: Points to an Algorithm that is not present\n\nFor 2.2, the most salient issues are:\n* Line 136: The paper points out that the context terms are dropped from the RHS of equation 4, but the significance and reasonability of this assumption is not adequately discussed. This again relates to the unclear definition of \"context\". If \"context\" refers to passages required to answer the question, dropping the context-dependence seems like it would greatly alter the model's outputs. This may be less of an issue if \"context\" only refers to in-context examples, but still warrants justification.\n* $\\pi_0$ is assumed to be fixed, which seems like a big assumption. Intuitively, the prompt can do a lot to shift how much the model relies on this prior.\n* Line 142: It is unclear what a \"query\" is. Do you make a separate query for each example? What is the result of a query--is it the model's probability assigned to the correct answer? The model's answer distribution?\n* Line 144/Equation 5: The variables in this equation are not defined (only the dimensions are specified, not their semantics).\n* Line 154/Remark 3: It's not clear why 2^K is the maximum number of prompts. 2^K is the number of subsets of datasets. But for each dataset, one can presumably form many prompts by choosing different examples from that dataset. How are datasets/subsets of datasets mapped to prompts?\n\nOther presentation issues include:\n* Line 195: It is claimed that \"calculating similarities was straightforward\" but it still needs to be specified how they were calculated.\n* Line 198: The comment about 1000 characters was confusing.\n\nThis paper compares with Trak but does not compare with MAGIC, a more recent work from some of the same authors https://arxiv.org/pdf/2504.16430 . MAGIC claims to greatly outperform Trak.\n\nPerhaps most importantly, the evaluation in this paper is extremely coarse-grained. The evaluation relies on testing whether the relative attribution strength of different datasets follows expected trends. While acceptable as a sanity check, this is not sufficient for evaluating a method like this. Papers like Trak or MAGIC compute much more fine-grained correlations between their attributions and empirical importance, and those best practices should be followed by this work.\n\nLine 360: The paper claims that CMF can identify \"Latent associations in linguistic or contextual patterns\", but it seems that the evidence for this general claim is minimal.\n\nLine 369: The paper claims that CMF \"provides a more detailed and accurate attribution of dataset contributions\", but I did not see any justification for why it is more accurate. The observed \"details\" could just be random fluctuations. In the same paragraph, the paper says that since \"CMF consistently assigns higher attribution values\" than other methods, this reflects \"robustness in capturing alignment\" but it's very unclear why a higher value is considered better. The goal of the attribution method should be to reflect the model's reliance on different data, not to produce a high value on the dataset *we* think should be most relied on."}, "questions": {"value": "Minor points/suggestions for writing:\n* Line 025: Should be \\citep\n* Line 075: The notation $M(q|c)$ seems strange to me, as it highly suggests that M assigns a probability to a question conditioned on a context. I think $M(q, c)$ is more appropriate.\n* Line 079: You can just write \"we have $k=1, \\dotsc, K$\"\n* Line 250: Redundant explanation of Olympic2024, it was already described previously"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3AHewVtYqF", "forum": "yADIMxHaXW", "replyto": "yADIMxHaXW", "signatures": ["ICLR.cc/2026/Conference/Submission20315/Reviewer_qMTm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20315/Reviewer_qMTm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925362654, "cdate": 1761925362654, "tmdate": 1762933777525, "mdate": 1762933777525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two dataset attribution methods that utilizes in-context learning. The first one measures the dataset contribution by the marginal change between with and without the dataset in the context. The second method uses a mixture model to estimate the contribution of each dataset. Empirically, the methods are evaluated on 4 models with 3 evaluation datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Using in-context learning to measure the dataset contribution is an interesting idea that has efficiency advantages.\n- The authors conducted extensive experiments for empirical evaluation.\n- The paper is easy to follow."}, "weaknesses": {"value": "- The proposed methods have strong assumptions that are not explicitly validated.\n- For the Shapley Context Model, a key (implicit) assumption is that adding the dataset to the context would achieve a similar effect as if the model is finetuned on this dataset. This could be validated empirically (e.g., by comparing the outputs of a base model + dataset context vs a model finetuned on the dataset).\n- For the Context Matrix Factorization, the assumptions that in-context learning (and finetuning) can be modeled as mixture distributions are not convincing to me. These assumptions should also be empirically validated."}, "questions": {"value": "Could the authors validate the assumptions in a more explicit way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x7Q3t3LQhF", "forum": "yADIMxHaXW", "replyto": "yADIMxHaXW", "signatures": ["ICLR.cc/2026/Conference/Submission20315/Reviewer_4V1H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20315/Reviewer_4V1H"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762360919614, "cdate": 1762360919614, "tmdate": 1762933777088, "mdate": 1762933777088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}