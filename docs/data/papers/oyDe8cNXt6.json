{"id": "oyDe8cNXt6", "number": 6433, "cdate": 1757983567823, "mdate": 1759897914878, "content": {"title": "AttriCtrl: A Generalizable Framework for Controlling Semantic Attribute Intensity in Diffusion Models", "abstract": "Diffusion models have recently become the dominant paradigm for image generation, yet existing systems struggle to interpret and follow numeric instructions for adjusting semantic attributes. \nIn real-world creative scenarios, especially when precise control over aesthetic attributes is required, current methods fail to provide such controllability. \nThis limitation partly arises from the subjective and context-dependent nature of aesthetic judgments, but more fundamentally stems from the fact that current text encoders are designed for discrete tokens rather than continuous values. \nMeanwhile, efforts on aesthetic alignment, often leveraging reinforcement learning, direct preference optimization, or architectural modifications, primarily align models with a global notion of human preference. While these approaches improve user experience, they overlook the multifaceted and compositional nature of aesthetics, underscoring the need for explicit disentanglement and independent control of aesthetic attributes.\nTo address this gap, we introduce AttriCtrl, a lightweight framework for continuous aesthetic intensity control in diffusion models. \nIt first decomposes relevant aesthetic attributes, then quantifies them through a hybrid strategy that maps both concrete and abstract dimensions onto a unified $[0,1]$ scale. A plug-and-play value encoder is then used to transform user-specified values into model-interpretable embeddings for controllable generation.\nExperiments show that AttriCtrl achieves accurate and continuous control over both single and multiple aesthetic attributes, significantly enhancing personalization and diversity.\nCrucially, it is implemented as a lightweight adapter while keeping the diffusion model frozen, ensuring seamless integration with existing frameworks such as ControlNet at negligible computational cost.", "tldr": "", "keywords": ["Diffusion; Control Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d5793b3bd32ab1aa63196fffcb9264554a3cbb8a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "While diffusion models have been showing really good results in image generation, there might still be cases where they are unable to follow instructions for adjusting visual inputs. This issue arises because text encoders handle words, not continuous values, and current aesthetic alignment methods only capture broad preferences. The proposed framework, AttriCtrl, enables fine-grained, continuous control of aesthetic features by mapping both concrete and abstract traits onto a [0, 1] scale. It uses a plug-and-play value encoder to translate user inputs into interpretable embeddings, allowing smooth and accurate adjustments. AttriCtrl integrates with existing systems like ControlNet while keeping the base model unchanged and computationally efficient."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I think the paper has the following strengths:\n\n1) Most of the results seem to be visually nice, which shows that the method can have some good usability.\n\n2) It is very nice that the method can be combined with the likes of ControlNet and Eligen.\n\n3) The paper is quite readable, and a motivated user should be able to understand it."}, "weaknesses": {"value": "I think the paper can be further improved in these aspects.\n\n1) By far my main issue with the paper is the lack of extensive comparisons with the other methods. I understand that for some tasks, this is very hard to do, considering that there aren't clear comparisons. However, for this reason, the authors should have done more evaluations in the task of nudity erasure where there are many other methods and benchmarks. In particular, the authors could have compared with:\n\n[A] Gong et al., Reliable and efficient concept erasure of textto-image diffusion models, 2024\n\n[B] Schramowski et al.,  Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models, 2023\n\n[C] Yoon et al., SAFREE: training-free and adaptive guard for safe text-to-image and video generation, 2024\n\n[D] Lyu et al., One-dimensional adapter to rule them all: Concepts, diffusion models and erasing applications, 2024\n\n[E] Gaintseva et al., CASteer: Steering Diffusion Models for Controllable Generation, 2025\n\nI understand that erasure is not the main focus of the paper, but considering how well-established (and important) it is, it can serve as a proxy on quantitative evaluation of the method.\n\n2) Similarly, for erasure, the authors could have done a user study.\n\n3) Some parts of the paper feel like padding. In particular, almost all equations seem trivial such as doing a normalization, or computing entropy. This does not make the paper be more sophisticated, in fact, it almost have the opposite effect.\n\n4) The authors should have tested their method with more diffusion backbones, currently it is only in Flux. In particular, I would have been very interested to see how it works in SDXL, SD-3/3.5 and SANA."}, "questions": {"value": "I do not have particular questions for the authors, but I would be very interested to see if they can properly address the mentioned weaknesses. If so, I might be willing to increase my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a7plR144JS", "forum": "oyDe8cNXt6", "replyto": "oyDe8cNXt6", "signatures": ["ICLR.cc/2026/Conference/Submission6433/Reviewer_3gSF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6433/Reviewer_3gSF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760714068451, "cdate": 1760714068451, "tmdate": 1762918824294, "mdate": 1762918824294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the task of controlling specific attributes (e.g., brightness, detail) in image generation. It proposes a method that defines and manipulates these attributes, presenting comparative experiments against other methods like AID. The main contribution lies in formulating this control task and providing a comparative analysis based on the proposed attribute metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "*   **Interesting Problem:** The core problem identified in Figure 1 is valid and represents an interesting challenge in controllable generation.\n*   **Initial Validation:** The experimental results indicate that the proposed method shows some advantages in controlling the defined attributes compared to certain baselines."}, "weaknesses": {"value": "*   **Lack of Conceptual Rigor and Novelty:**\n    *   **Inconsistent/Terminological Flaws:** A significant weakness is the lack of conceptual clarity. The title mentions \"semantic attributes\" while the abstract discusses \"aesthetic attributes,\" which are incorrectly applied to concepts like `detail` and `safety`. The definition of `brightness` (changing object color vs. illumination) is problematic and not aligned with standard understandings in image processing or aesthetics.\n    *   **Unnovel Technical Core:** The technical approach for calculating and utilizing the attribute metrics is acknowledged as being based on existing, relatively straightforward techniques (e.g., average pixel intensity for brightness, entropy for detail). The paper fails to clearly articulate what the specific novel technical contribution is beyond the application of these existing metrics.\n*   **Inadequate Experimental Validation and Comparisons:**\n Based on Figure 4, I believe the AID method performs the best. AID's results demonstrate precise control over the `brightness`, `detail`, and `realism` attributes, while excellently maintaining content consistency during attribute manipulation.\n    *   Regarding `brightness`, AID modifies the ambient lighting rather than the cow's color. This discrepancy is inherently linked to the brightness calculation method used in this paper (average pixel intensity), which does not correspond to illumination in the imaging process.\n    *   Using entropy as a complexity metric leads to excessively small objects in low-complexity controls, which is unreasonable from either an aesthetic or user need perspective.\n    *   Concerning `realism`, the first result from the proposed method has low realism, whereas all AID results appear highly realistic. The `Diff` metric also does not accurately reflect the corresponding quality.\n    *  The experiments in the appendix primarily demonstrate AID's capability in attribute control and semantic consistency. Although cases where AID fails are listed, based on my knowledge of generative models, the proposed method likely also has failure cases. Isolated examples cannot fully substantiate the proposed method's superiority.\n\n    *   **Metrics Lead to Artifacts:** The chosen attribute metrics themselves are a source of limitations, as evidenced by Figure 4, where they cause undesirable effects (e.g., unnaturally small images for low complexity, unrealistic brightness changes). This suggests the core formulation may be flawed.\n    *   **Insufficient Evidence for Superiority:** The claim of superiority over AID is not fully convincing. The appendix shows AID failures but does not provide a fair comparative analysis of failure cases for the proposed method. The visual results in Figure 4 suggest AID may perform better in terms of control precision and content preservation.\n*   **Issues with Presentation and Reproducibility:**\n    *   **Clarity:** The explanation is sometimes unclear, such as the definition and derivation of the final embedding `v`. Variable naming is non-standard (`$normalized_i$`).\n    *   **Reproducibility:** The provided code link is broken.\n    *   **Scholarly Rigor:** Section 3.2 lacks necessary citations for the techniques used."}, "questions": {"value": "1.  **Conceptual Foundation:** Could the authors clarify the fundamental concept? Are you controlling \"semantic\" or \"aesthetic\" attributes? How do your definitions of attributes like `brightness` and `detail` align with or differ from established definitions in image analysis and aesthetic assessment? Specifically, why was average pixel intensity chosen over illumination-aware models for brightness?\n2.  **Technical Contribution:** Given that the attribute calculations are based on existing techniques, what is the specific novel technical contribution of the method proposed in this paper? Is it primarily the composition and application pipeline?\n3.  **Experimental Comparisons:**\n    *   Could you provide a more balanced comparison with AID, including a statistical analysis of performance and failure cases for both methods, rather than just selected examples? The results in Figure 4 seem to favor AID; can you comment on this?\n4.  **Metric Design:** The results in Figure 4 show that your metrics can lead to unrealistic outputs (e.g., small image size for low detail, color change for brightness). Do you agree that these metrics have inherent limitations? Have you considered designing or incorporating more sophisticated, perceptually-aligned metrics to address these issues?\n5.  **Clarification and Reproducibility:**\n    *   How exactly is the final aesthetic intensity embedding `v` obtained? Please provide a clear mathematical formulation or description."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HHlxc1Y8lH", "forum": "oyDe8cNXt6", "replyto": "oyDe8cNXt6", "signatures": ["ICLR.cc/2026/Conference/Submission6433/Reviewer_LHsn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6433/Reviewer_LHsn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636363614, "cdate": 1761636363614, "tmdate": 1762918823937, "mdate": 1762918823937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AttriCtrl is a small add-on that lets you directly control how a text-to-image model renders certain qualities—like brightness, detail, realism, or safety—by giving it a numeric value (e.g., brightness = 0.7). It turns that number into a few tokens and appends them to the normal text prompt, while keeping the big diffusion model frozen, so it’s easy to plug in. For concrete attributes, it uses simple measurable definitions (e.g., image brightness); for abstract ones, it relies on CLIP similarity. The method works for one or multiple attributes at once, shows clear, smooth control in examples, and beats prior “prompting” baselines on a main accuracy metric and user preference tests. Overall, it’s a practical, lightweight way to get consistent, continuous control without retraining the main model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear, modular mechanism (frozen base, tiny encoder) that composes with ControlNet/T2I-Adapter; qualitative demos on multi-attribute control are convincing (Figs. 6–7). \n- Simple, explicit quantification of attributes (eqs. 1–6) with visualized normalization; practical and reproducible.\n- Positive headline numbers: Table 1 shows lower AvgDiff vs AID/Kontext/W-Emb and strong user preference; safety shows higher RR than NP/SLD/ESD (Fig. 5)."}, "weaknesses": {"value": "- Robustness under conflicting prompts is acknowledged but not quantified. Appendix G notes degradation when prompts contain strong attribute modifiers (e.g., “hyper-realistic…”) and shows Fig. 11, but lacks a quantified analyses.\n- Safety depends on SD checker prior. Safety is defined relative to the Stable Diffusion safety checker; authors note bounded effectiveness (Appendix G). This is a limitation if the checker’s coverage/bias shifts.\n- Unrelated-concept quality is only lightly assessed. Table 4 (COCO-10K: CLIP/FID) is encouraging, but the comparison set is safety baselines (NP/SLD/ESD), not control baselines."}, "questions": {"value": "- Could you add an “oracle” numeric-prompt tuning baseline (e.g., prompt templates tuned per attribute) and a CLIP-guided latent interpolation variant to situate AttriCtrl’s gains?\n- Since Safety(I) depends on the SD checker prior (eq. 5), can you add an alt-prior (e.g., LAION NSFW classifier) to test sensitivity? Also report RR per category (as in Fig. 5) with confidence intervals. \n- Please add control baselines that also aim to steer generation while preserving content regarding Table 4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E1LyW0KLX6", "forum": "oyDe8cNXt6", "replyto": "oyDe8cNXt6", "signatures": ["ICLR.cc/2026/Conference/Submission6433/Reviewer_VVmk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6433/Reviewer_VVmk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932011232, "cdate": 1761932011232, "tmdate": 1762918823454, "mdate": 1762918823454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces *AttriCtrl*, a novel method which allows fine-grained control over the strength of a set of textual attributes (specifically *brightness, detail, realism, safety*) in the context of text-to-image diffusion models. Given one attribute a user can decide in a 0-1 scale how strongly that attribute should be present in the final generation. For instance, by selecting the attribute *realism* the user can smoothly interpolate from less realistic to more realistic generations. The method relies on the training of specialized encoders (one for each attribute) which, given a 0-1 score, output a set of tokens which are appended to the textual ones to condition the generation. Experiments are conducted using the FLUX model and prompts from GenEval. Given the novelty of the task of fine-grained attribute control, AttriCtrl is compared to some baselines proposed by this same work and new evaluation metrics are proposed for the task. A user study is conducted to further evaluate the performances. Quantitative and qualitative results as well as the user study generally prove the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. **Original and relevant task.** The paper clearly defines and tackles the underexplored task of *continuous aesthetic intensity control*, introducing a principled framework that goes beyond binary or discrete conditioning.\n2. **Sound and well-motivated design.** The use of independent value encoders for each attribute is simple yet effective, allowing plug-and-play control without retraining the diffusion backbone.\n3. **Comprehensive evaluation.** The paper presents quantitative, qualitative, and user study evidence to compare over baselines, along with ablations clarifying key design choices. It also provides a clear experimental framework to facilitate future research in this direction.\n4. **Clarity and reproducibility.** The paper is clearly written, well structured, and supported by released code and detailed appendices."}, "weaknesses": {"value": "1. **Scalability.** Each attribute requires a separate encoder; while this modularity is elegant, scaling to a larger number of attributes could be computationally expensive as a new encoder must be trained for each attribute.\n2. **Content preservation.** In some qualitative examples (e.g., Fig. 4, brightness adjustment of the cow), changes in attribute intensity alter semantic content (the cow becomes white for higher brightness values), which may not be desirable.\n3. **Limited backbone diversity.** Experiments are conducted only on FLUX; demonstrating transferability to other architectures (e.g., Stable Diffusion) would strengthen the work."}, "questions": {"value": "1. In line 26, the phrasing *decompose relevant aesthetic attributes* may suggest that AttriCtrl can disentangle and control an arbitrary number of attributes, while in practice it focuses on four (*brightness*, *detail*, *realism*, *safety*). Could this part be clarified or rephrased?\n2. How long does the training of each attribute encoder take?\n3. Have you experimented also with different models other than FLUX? It would be interesting to understand whether an encoder learnt with FLUX can be used with another model for instance. This would make the method easy to apply by only training the encoders once.\n4. Minor notes:\n    - The *safety* attribute seems to be missing in the list of line 158.\n    - *Ours* should not be capitalized in line 416."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UINHp7hG68", "forum": "oyDe8cNXt6", "replyto": "oyDe8cNXt6", "signatures": ["ICLR.cc/2026/Conference/Submission6433/Reviewer_jx4y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6433/Reviewer_jx4y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987135642, "cdate": 1761987135642, "tmdate": 1762918822744, "mdate": 1762918822744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}