{"id": "HlQtHRjh5O", "number": 18638, "cdate": 1758289677741, "mdate": 1759897090328, "content": {"title": "ChebMoE: A Spectral-Aware and Expert-Adaptive Framework for Graph Anomaly Detection", "abstract": "Graph anomaly detection is critical for applications such as social networks, cybersecurity, and finance, yet remains challenging due to the unique spectral signatures of anomalies. In particular, anomalous nodes often exhibit high-frequency spectral patterns—a phenomenon known as \\textit{spectral shift}—which are easily suppressed by the low-pass nature of standard Graph Neural Networks (GNNs), resulting in \\textit{spectral washing} and poor anomaly detection.\nIn this work, we present ChebMoE, a novel and principled framework that directly addresses the spectral limitations of existing GNN-based anomaly detectors. Our key contributions are as follows:\n(1) We introduce a Chebyshev polynomial-based spectral feature extractor that efficiently preserves and amplifies high-frequency components, enabling the model to capture subtle spectral shifts associated with anomalies without requiring costly eigendecomposition.\n(2) We design a Mixture of Experts (MoE) anomaly detector with a learnable gating mechanism, allowing the model to adaptively aggregate diverse expert subnetworks and flexibly model complex anomaly patterns.\n(3) We propose a contrastive anomaly feature generator that leverages self-supervised contrastive learning to further enhance the discriminative power of node representations, improving robustness in the absence of labeled anomalies.\nExtensive experiments on seven real-world dynamic graph datasets demonstrate that ChebMoE consistently outperforms state-of-the-art baselines. For example, it achieves ROC-AUC of 0.9906 on Wiki, 0.8791 on Reddit, and 0.9812 on UCI, along with consistently high F1-scores, effectively counteracting spectral washing and substantially advancing the state of graph anomaly detection.", "tldr": "", "keywords": ["Anomaly Detection"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ab93d4f52544d436bf7d1614deadc1129e845c1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present CHEBMOE, a framework that tries to address the spectral limitations of existing GNN-based anomaly detectors for dynamic graph anomaly detection. Experiments show the effectiveness to some extent."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors present CHEBMOE, a framework that tries to address the spectral limitations of existing GNN-based anomaly detectors for dynamic graph anomaly detection.\n2. Experiments show the effectiveness to some extent."}, "weaknesses": {"value": "1. The novelty is limited. The right shift of spectral space is already utilized in several previous works in the graph anomaly detection field, which means the observation can not be a contribution. Besides, the design of the framework is a combination of exiting work, instead of a carefully designed one for the observation, further degrading the novelty of the work. \n2. The experimental comparison is not convincing. Since the training objective of the MoE part requires ground truth labels, the comparison with several unsupervised frameworks can be unfair. Furthermore, most of the compared baselines are relatively old. The authors should consider new baselines in this area, such as [1-4]. \n3. The hyperparameters can lead to instability of the framework. As shown in Figures 12, 13, and 14, the performance of some datasets can vary significantly when changing the hyperparameters, which raises the question of how to choose appropriate hyperparameters for new examples without tedious grid search. \n\n[1] Xiao Yang, Xuejiao Zhao, Zhiqi Shen. A Generalizable Anomaly Detection Method in Dynamic Graphs. AAAI 2025. \n\n[2] Yifan Hong, Muhammad Asif Ali, Huan Wang, Junyang Chen, Di Wang. ABNet: Mitigating Sample Imbalance in Anomaly Detection Within Dynamic Graphs. IJCAI 2025. \n\n[3] Jianhao Guo, Siliang Tang, Juncheng Li, Kaihang Pan, Lingfei Wu. RustGraph: Robust Anomaly Detection in Dynamic Graphs by Jointly Learning Structural-Temporal Dependency. TKDE 2024. \n\n[4] Dong Chen, Xiang Zhao, Weidong Xiao. Fine-grained Anomaly Detection on Dynamic Graphs via Attention Alignment. ICDE 2024."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k37vMlQLZX", "forum": "HlQtHRjh5O", "replyto": "HlQtHRjh5O", "signatures": ["ICLR.cc/2026/Conference/Submission18638/Reviewer_fhdD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18638/Reviewer_fhdD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760805959333, "cdate": 1760805959333, "tmdate": 1762928348973, "mdate": 1762928348973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Chebyshev-based Network with Mixture-of-experts, namely ChebMoE, a spectral-aware and expert-adaptive method for graph anomaly detection that addresses the “spectral washing” problem in conventional GNNs, where low-pass filtering suppresses high-frequency anomaly signals. ChebMoE integrates Chebyshev polynomial filtering to preserve spectral information, contrastive learning to enhance anomaly feature representation, and a mixture-of-experts mechanism to adaptively detect diverse anomalies. Theoretical analysis supports its spectral design, and experiments are conducted on several real-world dynamic datasets to evaluate the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-motivated, which clearly identifies a meaningful limitation of existing GNN-based anomaly detectors that the loss of high-frequency anomaly information due to spectral smoothing. \n\n2. The proposed CHEBMOE is theoretically sound, it uses Chebyshev polynomial filters to approximate spectral responses without requiring eigendecomposition. Meanwhile, the integration of spectral modeling, contrastive learning, and the mixture-of-experts design is creative, well-motivated, and technically consistent.\n\n3. This paper presents experiments across multiple benchmark datasets, demonstrating extensive experimental coverage."}, "weaknesses": {"value": "1. The paper is not well written, it is hard to follow at times. For example, in lines 278-279, it is stated that “We select features of nodes flagged as anomalous, which serve as input to the contrastive generator.” , but it is not explicit what these “flags” are.    \n\n2. Results are based on 20-run averages but tables lack standard deviations or confidence intervals , making it difficult to assess stability or whether reported improvements exceed random fluctuations.\n\n3. Missing direct comparisons to other spectral-aware filters in ablation study."}, "questions": {"value": "1. Is the contrastive encoder trained separately (pretrain) or jointly end-to-end with the MoE classifier? \n\n2. How to generate two augmentations per node? In Fig 2, two augmentations are enhanced first and then used as the input for MLP. However, when described in the text, the augmentations seem to be derived from the embedding after MLP.\n\n3. Is the SimCLR encoder trained only on the subset labeled as anomalies, or on all nodes to construct a good discriminative space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EVgkG3C68Y", "forum": "HlQtHRjh5O", "replyto": "HlQtHRjh5O", "signatures": ["ICLR.cc/2026/Conference/Submission18638/Reviewer_1SE7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18638/Reviewer_1SE7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568576215, "cdate": 1761568576215, "tmdate": 1762928348378, "mdate": 1762928348378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ChebMoE, a framework for graph anomaly detection that combines Chebyshev polynomial-based spectral filtering, contrastive learning, and mixture-of-experts (MoE) architecture. While the paper addresses the important problem of spectral washing in GNN-based anomaly detection, it suffers from significant weaknesses in novelty, experimental evaluation, and theoretical rigor."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem identification: The paper clearly articulates the spectral washing phenomenon where standard GNNs suppress high-frequency components critical for anomaly detection.\n\n2. Comprehensive theoretical appendix: Appendix B provides detailed mathematical formulations of spectral washing effects.  \n\n3. Consistent empirical improvements: The method shows performance gains across multiple datasets."}, "weaknesses": {"value": "1. The paper's main contribution amounts to a relatively rigid combination of existing techniques without deep integration: \n- Chebyshev filtering has been extensively studied for graph signal processing (Zhou 2010; Shuman et al. 2018; Defferrard et al. 2016's ChebNet). The application here is straightforward without novel theoretical insights specific to anomaly detection.\n- Contrastive learning via SimCLR/InfoNCE is standard practice (Chen et al. 2020). The \"contrastive anomaly feature generator\" (Section 3.2) simply applies existing contrastive loss to anomaly features without addressing why this specific combination is necessary.  \n\nThe paper essentially stacks these modules (Chebyshev → Contrastive → MoE → Classifier) without demonstrating why this particular pipeline is superior to alternatives or how these components synergistically address the core spectral washing problem. The modules solve different problems (spectral preservation, feature augmentation, adaptive detection) rather than being designed as an integrated solution to spectral washing.\n\n\n2. The paper has a conspicuous omission of BWGNN (Tang et al., 2022), which directly addresses high-frequency signal preservation for anomaly detection using adaptive spectral filtering. This is particularly problematic because: BWGNN explicitly tackles the same spectral shift problem cited by this paper (Tang et al., 2022 is referenced for defining spectral shift); The absence of BWGNN from baselines undermines the credibility of performance comparisons\n\n3. Missing GADBench methods: The paper omits recent strong baselines from GADBench (Tang et al., 2023), a comprehensive benchmark suite specifically for graph anomaly detection. This selective baseline comparison raises concerns about cherry-picking to favor the proposed method.\n\n4. No fine-grained ablations: What happens with ChebNet alone without MoE? What is the individual contribution of contrastive learning to the final performance?\n\n5. No formal guarantee that Chebyshev filtering improves anomaly detection: Corollary 7 provides an energy preservation bound, but does not establish that preserved high-frequency energy translates to better anomaly detection performance\n\n6. The method processes each snapshot G_t independently (Algorithm 1, lines 1-6). How is temporal information leveraged? The paper claims to address \"dynamic graphs,\" but the temporal modeling is unclear"}, "questions": {"value": "1. Why is BWGNN not included as a baseline, given its direct relevance to spectral preservation for anomaly detection?\n\n2. What is the computational overhead of your method compared to baselines? Please provide wall-clock time comparisons.\n\n3. How does the method perform when anomalies do not exhibit high-frequency characteristics (e.g., low-degree anomalous nodes in dense regions)?\n\n4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eRWuTcQyoj", "forum": "HlQtHRjh5O", "replyto": "HlQtHRjh5O", "signatures": ["ICLR.cc/2026/Conference/Submission18638/Reviewer_Bo1a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18638/Reviewer_Bo1a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663001893, "cdate": 1761663001893, "tmdate": 1762928347831, "mdate": 1762928347831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addressed spectral washing in graph anomaly detection, where prior works suppressed high-frequency signals critical for identifying anomalies. The authors proposed CHEBMOE, a three-component framework: (1) Chebyshev-based Feature Extractor (ChebNet) to preserve high-frequency spectral information; (2) Contrastive Anomaly Feature Generator (CAFG) to address the scarcity of anomaly labeled samples and (3) Mixture-of-Experts Anomaly Detector (MoE-AD) to adaptively capture evolving temporal anomaly patterns."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(S1) Insightful problem observation and targeted solution: The paper identified a limitation in GNN-based anomaly detection. The low-pass nature of GCN suppressed high-frequency anomaly signals. The adoption of ChebNet directly targeted this issue by preserving high-frequency components."}, "weaknesses": {"value": "(W1) The core problem to be resolved and the proposed method are inconsistent. The abstract and introduction focus on spectral washing as the primary challenge, with brief mention of temporally dynamic anomalies. However, when introducing the proposed framework, a third issue--scarcity of labeled anomalies--suddenly appeared to justify the design of CAFG, despite never being discussed earlier. This creates a narrative disconnect: the paper established spectral washing as the central problem but proposed a three-component solution where two components (CAFG and MoE-AD) address issues (label scarcity and temporal dynamics) that are inadequately motivated upfront.\n\n(W2) The methodology design lacks novelty and customization to graph data. The proposed components are largely direct adoptions of existing techniques without innovation or graph-specific adaptations. The Chebyshev Feature Extractor is essentially ChebNet without modifications; CAFG basically follows the SimCLR pipeline; and MoE-AD is a simple adoption of vanilla MoE architecture. Critically, both CAFG and MoE-AD fail to leverage graph-specific characteristics: CAFG treated node embeddings as independent vectors, ignoring structural relationships, while MoE-AD operated without any customization for graph topology.\n\n(W3) The writing lacks clarity. For example, the description of MoE-AD’s input is ambiguous. Based on Figure 2 and the provided source code, MoE-AD appeared to take two types of inputs: (1) embeddings from the ChebNet and (2) embeddings from the CAFG. However, Section 3.3 fails to explicitly clarify this dual-input design. The authors vaguely introduced the input as \"x\" without specifying where \"x\" comes from.\n\n(W4) The provided code is incomplete for reproducibility. It only provided methodology scripts while missing several critical components: (1) dataset preprocessing pipeline scripts, (2) environment specifications (e.g., requirements.txt), and (3) README file with proper documentation. These omissions significantly hinder reproducibility."}, "questions": {"value": "1. Is the Chebyshev-based Feature Extractor a direct adoption of ChebNet, or are there any differences between them?\n\n2. What is the input to MoE-AD? What is the architect of experts in MoE?\n\n3. Are there any customized designs in CAFG and MoE-AD that are specifically tailored for graph data?\n\n4. Since the paper framed \"high-frequency removal\" as an underexplored core issue in graph anomaly detection, why not addressing it in static graph settings where MoE (a simple adoption) would be unnecessary? If the authors mainly aimed to resolve \"high-frequency removal\", a static setting seems adequate; introducing dynamic graphs only adds unnecessary complexity without deepening the core contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RWuJKK1Id1", "forum": "HlQtHRjh5O", "replyto": "HlQtHRjh5O", "signatures": ["ICLR.cc/2026/Conference/Submission18638/Reviewer_3rxb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18638/Reviewer_3rxb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741826487, "cdate": 1761741826487, "tmdate": 1762928347460, "mdate": 1762928347460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}