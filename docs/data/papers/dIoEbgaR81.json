{"id": "dIoEbgaR81", "number": 1728, "cdate": 1756912615928, "mdate": 1759898191436, "content": {"title": "SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges", "abstract": "Evaluating the capabilities and risks of frontier AI models is paramount, yet current methods demand extensive domain expertise, hindering their scalability as these models rapidly evolve. We introduce SKATE: a novel evaluation framework in which large language models (LLMs) compete by generating and solving verifiable tasks for one another. Our core insight is to treat evaluation as a game: models act as both task-setters and solvers, incentivized to create questions which highlight their own strengths while exposing others' weaknesses. SKATE offers several key advantages, balancing scalability, open-endedness, and objectivity. It is fully automated, data-free, and scalable, requiring no human input or domain expertise. By using verifiable tasks rather than LLM judges, scoring is objective. Unlike domain-limited programmatically-generated benchmarks (e.g. chess-playing or spatial reasoning), having LLMs creatively pose challenges enables open-ended and scalable evaluation. As a proof of concept, we introduce LLM-set code-output-prediction (COP) challenges as a verifiable and extensible framework in which to test our approach. Using a TrueSkill-based ranking system, we evaluate six frontier LLMs and find that:\n(1) weaker models can score stronger ones consistently, reliably differentiating between them, and\n(2) LLM-based systems are capable of self-preferencing behavior, generating questions that align with their own capabilities, and\n(3) SKATE automatically surfaces fine-grained capability differences between models.\nOur findings are an important step towards general, scalable evaluation frameworks which can keep pace with LLM progress.", "tldr": "We introduce SKATE: a novel evaluation framework in which large language models (LLMs) compete by generating and solving verifiable tasks for one another.", "keywords": ["automated evaluation", "scalable evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f55301698e90f35b9b78fec4648421c69b1b10ae.pdf", "supplementary_material": "/attachment/1e04781f971c9e31e3333fa0d43030fdb6c2815b.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a judge-free evaluation where models set and solve verifiable tasks (i.e., code output prediction MCQs) and are then ranked with TrueSkill while controlling option/order noise and near duplicate questions. Experiments show stable rankings, that weak models can consistently score stronger ones, and demonstrate measurable self-preference when models author questions that they can answer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The peer-generated, verifiable tournament evaluation framework is an interesting approach towards a scalable and (hopefully reliable) evaluation framework. \n2) The work carefully controls MCQ option/order noise with re-sampling and convergence criteria, adds guardrails that reduce reward hacking, and attempts to enforce question uniqueness via embedding based clustering."}, "weaknesses": {"value": "1) The results are limited to COP. The framework would be stronger with at least one additional verifiable task family.\n2) It seems like only one embedding model is used for uniqueness filtering (my apologies if I am mistaken). What happens if a different model is used? How robust is uniqueness filtering to the choice of embedding model?\n3) Rankings may be dependent on TrueSkill mapping (eg relative vs absolute), p-threshold, number of distractors, number of rounds, etc."}, "questions": {"value": "1) Would it be possible to add a second verifiable task family?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vDw7HUNPQM", "forum": "dIoEbgaR81", "replyto": "dIoEbgaR81", "signatures": ["ICLR.cc/2026/Conference/Submission1728/Reviewer_oN5s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1728/Reviewer_oN5s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682634370, "cdate": 1761682634370, "tmdate": 1762915871542, "mdate": 1762915871542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a methodology for LLMs collaboratively and interactively judge each other's performance, by having the LLMs play a game wherein they generate questions whose answers can be verifiably answered. The aim is to pose questions they can answer while the other LLMs cannot distinguish between true and incorrect answers. The LLMs are ranked by their abilities to both correctly answer other LLM's questions, and to stump the other LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the evaluations are verifiable, differing from common LLM-as-Judge frameworks like Alpaca-Eval, where the biases of a judge LLM influences the evaluation\n- no human input is required in generating the evaluation data sets\n- the resulting rankings (on code output prediction) are demonstrated to be stable to the addition of more LLMs\n- the methdology is shown to elicit some fine-grained performance differences between LLMs,. and incorporate varying levels of prior knowledge to assist the LLMs in generating difficult questions\nThe primary significance is in the verifiability of the methodology."}, "weaknesses": {"value": "the methodology is limited to automatedly verifiable tasks, while many tasks we would like to understand the performance of LLMs on (e.g. alignment), cannot be automatically verifiable\n- the methodology similarly is limited to tasks that can be posed as multiple choice questions, so it cannot be used to e.g. compare the summarization abilities of LLMs\n- although the methodology encourages diversity, it does not ensure coverage, so e.g. in the specific case studied in the paper, it could be the case that LLMs all share a common blind spot for code output prediction that is not identified by this methodology.\n- this methodology was only instantiated and tested on a code output prediction task; it would have been more persuasive to see it perform on at least one other, significantly different task"}, "questions": {"value": "- please address the statistical issues in using adaptive sampling to determine p(correct): this means that models with less certainty are given a larger budget of samples.\n- please be more comprehensive with your discussion of how your work compares to related gamified LLM-vs-LLM evaluation methodologies, e.g. the GTBench paper of Duan et al. in NeurIPS 2024 and the ZeroSumEval paper of Alyahya et al. in ACL 2025; in particular, how does your use and emphasis placed on verifiability differ from theirs? Making this more clear would help me better judge the novelty and significance of your contribution.\n- the author list in the bibliographic entry for Humanity's Last Exam spans several pages, it should be shortened! Several other author lists should be similarly be shortened."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5OWMgd1qAW", "forum": "dIoEbgaR81", "replyto": "dIoEbgaR81", "signatures": ["ICLR.cc/2026/Conference/Submission1728/Reviewer_JWwY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1728/Reviewer_JWwY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923540701, "cdate": 1761923540701, "tmdate": 1762915871218, "mdate": 1762915871218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SKATE, a novel LLM evaluation framework where models compete by generating and solving verifiable tasks for one another. Models act as both \"task-setters\" and \"solvers\" in a tournament format, creating Code-Output-Prediction (COP) challenges. The system uses TrueSkill ranking and tests 6 frontier LLMs, finding that: (1) weaker models can differentiate stronger ones, (2) models exhibit self-preferencing in question generation, and (3) automatic discovery of differentiating questions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The whole idea is novel and well-motivated.The paper identifies two major bottlenecks in LLM evaluation: (1) evaluation requires costly, non-scalable human-annotated ground truths, or (2) relies on LLM-as-judge which is easily manipulated. SKATE attempts to address both through a peer-challenge multi-model system.\n2. Methodologies are sound and considerate, such as robust scoring algorithm to adress the multiple-choice biases and question clustering to increase the diversity of questions."}, "weaknesses": {"value": "1. While the authors claim that COP tasks provide \"a general substrate for evaluating model capabilities,\" this paper provides zero empirical evidence that SKATE can work with other task types. The COP tasks tested here don't even resemble a typical coding evaluation where models' generation or algorithmic problem-solving capabilities are tested. Any model with code execution tools could solve COP tasks. Without demonstrating SKATE generalisation ability on more diverse verifiable tasks (such as mathematical proofs, puzzles), the claim is questionable.\n\n2. Although COP tasks can be verified through code execution easily, the authors provide no evidence that general verifiable tasks can have similar guarantees. For instance, if a model generates a mathematical problem as a challenge (the task itself is verifiable with ground-truths but extremely hard with only models), verifying the correctness of the answer may be non-trivial or even intractable.\n\n3. While collective knowledge from multiple models works well for training scenarios, using it for evaluation may be problematic because it provides no ground-truth anchorâ€”evaluation results depend entirely on which specific models are included in the cohort. The experiments in Section 6.2 only test adding models sequentially by capability order, which is insufficient to establish robustness. A rigorous validation would require testing with diverse model combinations to show rankings remain consistent, or anchoring the evaluation with human-verified questions to provide external validation.\n\n4. The empirical findings presented in Section 6, while technically sound, lack novelty and offer limited new insights into LLM capabilities. The observation that weaker models can reliably score stronger ones, and models prefer their own questions, have been found two years ago."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wC8UD21Q5T", "forum": "dIoEbgaR81", "replyto": "dIoEbgaR81", "signatures": ["ICLR.cc/2026/Conference/Submission1728/Reviewer_3DNS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1728/Reviewer_3DNS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953088999, "cdate": 1761953088999, "tmdate": 1762915870918, "mdate": 1762915870918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}