{"id": "dIoEbgaR81", "number": 1728, "cdate": 1756912615928, "mdate": 1763640859864, "content": {"title": "SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges", "abstract": "Evaluating the capabilities and risks of frontier AI models is paramount, yet current methods demand extensive domain expertise, hindering their scalability as these models rapidly evolve. We introduce SKATE: a novel evaluation framework in which large language models (LLMs) compete by generating and solving verifiable tasks for one another. Our core insight is to treat evaluation as a game: models act as both task-setters and solvers, incentivized to create questions which highlight their own strengths while exposing others' weaknesses. SKATE offers several key advantages, balancing scalability, open-endedness, and objectivity. It is fully automated, data-free, and scalable, requiring no human input or domain expertise. By using verifiable tasks rather than LLM judges, scoring is objective. Unlike domain-limited programmatically-generated benchmarks (e.g. chess-playing or spatial reasoning), having LLMs creatively pose challenges enables open-ended and scalable evaluation. As a proof of concept, we introduce LLM-set code-output-prediction (COP) challenges as a verifiable and extensible framework in which to test our approach. Using a TrueSkill-based ranking system, we evaluate six frontier LLMs and find that:\n(1) weaker models can score stronger ones consistently, reliably differentiating between them, and\n(2) LLM-based systems are capable of self-preferencing behavior, generating questions that align with their own capabilities, and\n(3) SKATE automatically surfaces fine-grained capability differences between models.\nOur findings are an important step towards general, scalable evaluation frameworks which can keep pace with LLM progress.", "tldr": "We introduce SKATE: a novel evaluation framework in which large language models (LLMs) compete by generating and solving verifiable tasks for one another.", "keywords": ["automated evaluation", "scalable evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9bf4d9ee576554ffb0932b82b2f61c2bc581c3b4.pdf", "supplementary_material": "/attachment/1e04781f971c9e31e3333fa0d43030fdb6c2815b.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a judge-free evaluation where models set and solve verifiable tasks (i.e., code output prediction MCQs) and are then ranked with TrueSkill while controlling option/order noise and near duplicate questions. Experiments show stable rankings, that weak models can consistently score stronger ones, and demonstrate measurable self-preference when models author questions that they can answer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The peer-generated, verifiable tournament evaluation framework is an interesting approach towards a scalable and (hopefully reliable) evaluation framework. \n2) The work carefully controls MCQ option/order noise with re-sampling and convergence criteria, adds guardrails that reduce reward hacking, and attempts to enforce question uniqueness via embedding based clustering."}, "weaknesses": {"value": "1) The results are limited to COP. The framework would be stronger with at least one additional verifiable task family.\n2) It seems like only one embedding model is used for uniqueness filtering (my apologies if I am mistaken). What happens if a different model is used? How robust is uniqueness filtering to the choice of embedding model?\n3) Rankings may be dependent on TrueSkill mapping (eg relative vs absolute), p-threshold, number of distractors, number of rounds, etc."}, "questions": {"value": "1) Would it be possible to add a second verifiable task family?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vDw7HUNPQM", "forum": "dIoEbgaR81", "replyto": "dIoEbgaR81", "signatures": ["ICLR.cc/2026/Conference/Submission1728/Reviewer_oN5s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1728/Reviewer_oN5s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682634370, "cdate": 1761682634370, "tmdate": 1762915871542, "mdate": 1762915871542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a methodology for LLMs collaboratively and interactively judge each other's performance, by having the LLMs play a game wherein they generate questions whose answers can be verifiably answered. The aim is to pose questions they can answer while the other LLMs cannot distinguish between true and incorrect answers. The LLMs are ranked by their abilities to both correctly answer other LLM's questions, and to stump the other LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the evaluations are verifiable, differing from common LLM-as-Judge frameworks like Alpaca-Eval, where the biases of a judge LLM influences the evaluation\n- no human input is required in generating the evaluation data sets\n- the resulting rankings (on code output prediction) are demonstrated to be stable to the addition of more LLMs\n- the methdology is shown to elicit some fine-grained performance differences between LLMs,. and incorporate varying levels of prior knowledge to assist the LLMs in generating difficult questions\nThe primary significance is in the verifiability of the methodology."}, "weaknesses": {"value": "the methodology is limited to automatedly verifiable tasks, while many tasks we would like to understand the performance of LLMs on (e.g. alignment), cannot be automatically verifiable\n- the methodology similarly is limited to tasks that can be posed as multiple choice questions, so it cannot be used to e.g. compare the summarization abilities of LLMs\n- although the methodology encourages diversity, it does not ensure coverage, so e.g. in the specific case studied in the paper, it could be the case that LLMs all share a common blind spot for code output prediction that is not identified by this methodology.\n- this methodology was only instantiated and tested on a code output prediction task; it would have been more persuasive to see it perform on at least one other, significantly different task"}, "questions": {"value": "- please address the statistical issues in using adaptive sampling to determine p(correct): this means that models with less certainty are given a larger budget of samples.\n- please be more comprehensive with your discussion of how your work compares to related gamified LLM-vs-LLM evaluation methodologies, e.g. the GTBench paper of Duan et al. in NeurIPS 2024 and the ZeroSumEval paper of Alyahya et al. in ACL 2025; in particular, how does your use and emphasis placed on verifiability differ from theirs? Making this more clear would help me better judge the novelty and significance of your contribution.\n- the author list in the bibliographic entry for Humanity's Last Exam spans several pages, it should be shortened! Several other author lists should be similarly be shortened."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5OWMgd1qAW", "forum": "dIoEbgaR81", "replyto": "dIoEbgaR81", "signatures": ["ICLR.cc/2026/Conference/Submission1728/Reviewer_JWwY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1728/Reviewer_JWwY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923540701, "cdate": 1761923540701, "tmdate": 1762915871218, "mdate": 1762915871218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SKATE, a novel LLM evaluation framework where models compete by generating and solving verifiable tasks for one another. Models act as both \"task-setters\" and \"solvers\" in a tournament format, creating Code-Output-Prediction (COP) challenges. The system uses TrueSkill ranking and tests 6 frontier LLMs, finding that: (1) weaker models can differentiate stronger ones, (2) models exhibit self-preferencing in question generation, and (3) automatic discovery of differentiating questions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The whole idea is novel and well-motivated.The paper identifies two major bottlenecks in LLM evaluation: (1) evaluation requires costly, non-scalable human-annotated ground truths, or (2) relies on LLM-as-judge which is easily manipulated. SKATE attempts to address both through a peer-challenge multi-model system.\n2. Methodologies are sound and considerate, such as robust scoring algorithm to adress the multiple-choice biases and question clustering to increase the diversity of questions."}, "weaknesses": {"value": "1. While the authors claim that COP tasks provide \"a general substrate for evaluating model capabilities,\" this paper provides zero empirical evidence that SKATE can work with other task types. The COP tasks tested here don't even resemble a typical coding evaluation where models' generation or algorithmic problem-solving capabilities are tested. Any model with code execution tools could solve COP tasks. Without demonstrating SKATE generalisation ability on more diverse verifiable tasks (such as mathematical proofs, puzzles), the claim is questionable.\n\n2. Although COP tasks can be verified through code execution easily, the authors provide no evidence that general verifiable tasks can have similar guarantees. For instance, if a model generates a mathematical problem as a challenge (the task itself is verifiable with ground-truths but extremely hard with only models), verifying the correctness of the answer may be non-trivial or even intractable.\n\n3. While collective knowledge from multiple models works well for training scenarios, using it for evaluation may be problematic because it provides no ground-truth anchor—evaluation results depend entirely on which specific models are included in the cohort. The experiments in Section 6.2 only test adding models sequentially by capability order, which is insufficient to establish robustness. A rigorous validation would require testing with diverse model combinations to show rankings remain consistent, or anchoring the evaluation with human-verified questions to provide external validation.\n\n4. The empirical findings presented in Section 6, while technically sound, lack novelty and offer limited new insights into LLM capabilities. The observation that weaker models can reliably score stronger ones, and models prefer their own questions, have been found two years ago."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wC8UD21Q5T", "forum": "dIoEbgaR81", "replyto": "dIoEbgaR81", "signatures": ["ICLR.cc/2026/Conference/Submission1728/Reviewer_3DNS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1728/Reviewer_3DNS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953088999, "cdate": 1761953088999, "tmdate": 1762915870918, "mdate": 1762915870918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Joint Response to All Reviewers"}, "comment": {"value": "We thank all three reviewers for their thoughtful and constructive feedback. We are pleased that the core motivations and methodological contributions came through clearly. In particular, we appreciate the reviewers’ recognition that our approach directly addresses major limitations of current evaluation methods: (i) the lack of scalability arising from the need for significant human involvement in question generation and, (ii) the well-documented challenges of LLM-as-judge frameworks.\n\n__Key strengths noted by the reviewers:__\n\nAcross reviews, multiple strengths of our contribution were highlighted. We thank R1 for noting that our work identifies two central bottlenecks in LLM evaluation and effectively addresses them through our peer-challenge framework. We also appreciate their positive assessment of our methodologies as “sound and considerate”, as well as their recognition that our clustering procedure supports more robust results. We are grateful for R2 for underscoring the importance of verifiability of our method and for highlighting that our pipeline surfaces interesting fine-grained capability differences between LLMs. We also thank R3 for recognizing scalability as a key contribution and for commending the care we took in handling multiple-choice noise and question clustering.\n\nThese strengths affirm our work’s main aim to provide a __scalable, verifiable and robust__ framework for evaluating LLMs without relying on human-data or subjective model judgements. We want to thank the reviewers again for their careful assessment and helpful suggestions. \n\n__Key revisions in response to the comments:__\n\nWe have revised the paper carefully, with major changes summarized below.\nBoth R1 and R3 raised concerns regarding our choice of code-output-prediction (COP) as the primary class of verifiable tasks used to demonstrate SKATE. We agree that our original presentation did not sufficiently emphasize the generality of COP-style questions. In response, we have added a new Appendix C (“Comments on Generalizability of COP”), which illustrates how a broad range of tasks such as mathematical reasoning, game-playing and spatial reasoning, can all be expressed in COP format. The majority of the examples included in this Appendix were written by LLMs: emphasizing that these tasks are a) representable as COP, and b) possible to generate automatically with current models.\n\nR1 also asked about the applicability of our results to other verifiable tasks. We appreciate this point and have clarified our claim in Section 3. Specifically, we argue that SKATE applies to all __automatically verifiable tasks__, meaning tasks whose solutions can be checked automatically even if an LLM cannot always generate the ground truth. We provide further discussion and concrete examples in a new Appendix B (“Comments on Other Verifiable Tasks”).\n\nWe also thank R2 for the comments on related work and formatting; these have been addressed in detail in the individual reviewer responses. We have also addressed R2’s point about whether or not our method was limited to multiple-choice questions. We restricted our experiments to this class of task, but in fact SKATE applies to open-ended questions also.  The key requirement is that it must be possible to automatically verify the correctness of a question / task. We include examples in Appendix B of open-ended questions which fit this requirement and discuss how models should be scored in this setting.\n\n[In this comment, we refer to Reviewer 3DNS as R1, Reviewer JWwY as R2, and Reviewer oN5s as R3.]"}}, "id": "8NpOVMVWCz", "forum": "dIoEbgaR81", "replyto": "dIoEbgaR81", "signatures": ["ICLR.cc/2026/Conference/Submission1728/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1728/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission1728/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763641444952, "cdate": 1763641444952, "tmdate": 1763641444952, "mdate": 1763641444952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}