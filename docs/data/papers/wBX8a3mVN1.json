{"id": "wBX8a3mVN1", "number": 21517, "cdate": 1758318420998, "mdate": 1763597926962, "content": {"title": "An uncertainty-aware framework for data-efficient multi-view animal pose estimation", "abstract": "Multi-view pose estimation is essential for quantifying animal behavior in scientific research, yet current methods struggle to achieve accurate tracking with limited labeled data and suffer from poor uncertainty estimates. We address these challenges with a comprehensive framework combining novel training and post-processing techniques, and a model distillation procedure that leverages the strengths of these techniques to produce a more efficient and effective pose estimator. Our multi-view transformer (MVT) utilizes pretrained backbones and enables simultaneous processing of information across all views, while a novel patch masking scheme learns robust cross-view correspondences without camera calibration. For calibrated setups, we incorporate geometric consistency through 3D augmentation and a triangulation loss. We extend the existing Ensemble Kalman Smoother (EKS) post-processor to the nonlinear case and enhance uncertainty quantification via a variance inflation technique. Finally, to leverage the scaling properties of the MVT, we design a distillation procedure that exploits improved EKS predictions and uncertainty estimates to generate high-quality pseudo-labels, thereby reducing dependence on manual labels. Our framework components consistently outperform existing methods across three diverse animal species (flies, mice, chickadees), with each component contributing complementary benefits. The result is a practical, uncertainty-aware system for reliable pose estimation that enables downstream behavioral analyses under real-world data constraints.", "tldr": "This paper introduces a framework for multi-view animal pose estimation that combines multi-view transformers, uncertainty-aware post-processing, and model distillation to achieve more accurate tracking with limited labeled data.", "keywords": ["animal behavior analysis", "multi-view pose estimation", "uncertainty quantification"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/888080b243f270328509d0668feb24ed895b3e2e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Multi-view animal pose estimation methods have several major limitations:\n(1) they typically process each camera view separately and only fuse information late, which wastes cross-view geometric information;\n(2) they provide poorly calibrated uncertainty; and\n(3) animal datasets often have very few labeled frames, so the amount of supervised data is limited.\nTo address these challenges, the authors propose a new architecture, an uncertainty-aware post-processing method, and a pseudo-labeling pipeline:\nArchitecture. The authors introduce a multi-view vision transformer (MVT) that jointly processes pixel patches from all camera views, enabling early fusion through attention. They also add a multi-view patch masking scheme that randomly masks patches in some views.\n\nUncertainty estimation. The authors extend Ensemble Kalman Smoothing (EKS) into a variance-inflated and nonlinear versions.\n\nPseudo-labeling / “distillation.” The authors then use these uncertainty estimates provided by EKS to select high-confidence frames: they keep frames with low predicted variance, filter near-duplicates via clustering, and treat those as pseudo-labels. They combine these pseudo-labels with the ground-truth annotations and retrain a single model. The practical benefit is that, at inference time, you can run one model instead of running an ensemble and post-processing with mvEKS every time, while still getting the gains from the ensemble+mvEKS pipeline.\n\nThe authors evaluate on three animal datasets (treadmill mouse, fly, chickadee) and show that the proposed MVT achieves lower pixel error than single-view baselines, and patch masking further improves performance under limited labels. For the mvEKS / variance inflation part and for the pseudo-labeling stage, I still have questions because I felt that some comparisons or curves were missing from the figures (see “Questions”).\n\nOverall, I think this is a valuable contribution for neuroscience / behavior analyses, where annotation is expensive and often inconsistent. However, I do have some doubts regarding the results and I would like clarification before I can fully judge impact (see “Questions” and “Weaknesses”). I am scoring this paper a 4 for now, and I will actively participate in the discussion, if the authors address these questions clearly,  I am inclined to raise my score."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important problem in neuroscience and behavior analysis. The experiments cover the claims made in the paper (the rest is in questions)."}, "weaknesses": {"value": "- I find the use of the term “distilled student” somewhat confusing. As I understand it, the pipeline is: (1) train multiple models $f_i$\n , (2) aggregate their predictions across views, (3) run the proposed mvEKS to get temporally consistent keypoints and uncertainty, (4) select only high-confidence frames and remove duplicates with PCA clustering, and (5) treat those frames as pseudo-labels and retrain a single model on GT + pseudo-labels. This looks more like pseudo-labeling with uncertainty filtering and dataset expansion than classical knowledge distillation (where a student is explicitly trained to match a teacher’s outputs/soft targets). That said, if “distillation” is the accepted terminology in this subcommunity, feel free to clarify and ignore this comment.\n- I found notation and labels confusing, I put all this into Questions."}, "questions": {"value": "- Figure 3. It looks like the plot does not include the baseline mvEKS variant for pixel error. Could the authors include that for completeness?\n- Figure 4. The legend is hard to interpret. For example, does “MVT++ + EKS distilled + Geometric Consistency” use the proposed variance-inflated mvEKS, or the baseline mvEKS? Please make this explicit.\nRelated to that: because proposed mvEKS + variance inflation seem like core contributions, it would really help to show separate curves for (a) MVT++ + mvEKS (the baseline), (b) MVT++ + mvEKS + variance inflation, (c) MVT++ + mvEKS + variance inflation distilled, and (d) MVT++ + mvEKS  (the baseline) distilled. \n- Figures 9, 10, 11, and 16. I did not see results for the mouse dataset in these figures. Why the results were omitted? \n- Figure 17. Appendix A suggests that each dataset has <500 labeled frames, but Figure 17 compares “MVT++”, “MVT++ (1000 frames)”, and “MVT++ (3000 frames)”. Are these curves matched for total training compute / number of optimization steps, or are the larger-data models also trained longer? If it’s the latter, could the authors normalize for wall-clock or epochs to make the comparison fair?\n- Figure 15. It would help to label clearly which line corresponds to baseline mvEKS and which line corresponds to mvEKS with variance inflation. Also, what is the difference between “Ensemble Median + Anipose” and just “Ensemble Median”? Is Anipose only doing triangulation / post-processing in that comparison? \n- Terms mvEKS and EKS are used interchangeably, for example:\n> The linear multi-view Ensemble Kalman Smoother (mvEKS), introduced in Biderman et al. (2024), leverages multi-view ...\n\nIn the same paragraph authors uses EKS instead. The same is applicable to labels in all the figures. I find it confusing. Could authors clarify if mvEKS and EKS describe the same method that was proposed in Biderman et al. (2024)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "knKxAMzryH", "forum": "wBX8a3mVN1", "replyto": "wBX8a3mVN1", "signatures": ["ICLR.cc/2026/Conference/Submission21517/Reviewer_rzyx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21517/Reviewer_rzyx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761435400235, "cdate": 1761435400235, "tmdate": 1762941814028, "mdate": 1762941814028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank the reviewers for their thoughtful evaluations and constructive feedback. We believe that addressing these points has strengthened our submission significantly.\n\nBelow we address the comments of each individual reviewer. We have organized our responses into three categories:\n\n1. **Minor revisions**: For straightforward concerns (e.g., adding missing references, clarifying writing), we acknowledge the feedback and indicate the changes we will make in the final manuscript.\n\n2. **Substantive responses with new results**: For more in-depth comments requiring additional experiments, we have added a new section to the appendix of our manuscript (Appendix F). We will post initial conclusions in our text responses here and direct you to specific sections of the updated PDF for detailed results and visualizations. We will continue to update this manuscript as we complete additional analyses.\n\n3. **Work in progress**: For comments we are still actively addressing, we list these at the end of each reviewer response and will post follow-up comments as new results become available.\n\nWe have already uploaded an initial manuscript revision to facilitate discussion. We plan to update this document iteratively over the coming weeks and will notify you when significant new results are added.\n\nWe hope this structure will assist in your evaluation and look forward to continuing discussions with each of you."}}, "id": "Cjv9zU0jGt", "forum": "wBX8a3mVN1", "replyto": "wBX8a3mVN1", "signatures": ["ICLR.cc/2026/Conference/Submission21517/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21517/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21517/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763597251848, "cdate": 1763597251848, "tmdate": 1763597251848, "mdate": 1763597251848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for data-efficient multi-view animal pose estimation that integrates: (1) A multi-view transformer (MVT) that processes all camera views jointly, introducing a patch masking scheme to encourage cross-view consistency. (2) A 3D augmentation and triangulation loss for calibrated setups, improving geometric consistency. (3) An enhanced variance-inflated, nonlinear Ensemble Kalman Smoother (EKS) for post-processing and uncertainty calibration. (4) A pseudo-label distillation pipeline, using EKS to generate high-quality labels from unlabeled frames to retrain a single efficient model.\nThe framework is tested on three datasets (Fly-Anipose, Treadmill Mouse, and Chickadee), showing improvements over single-view ViTs and some multi-view baselines."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### **Strengths:**\n- A clear motivation: data-efficient animal pose estimation is a relevant challenge.\n- The paper proposes a consistent framework that works across both calibrated and uncalibrated setups.\n- The variance-inflated EKS for uncertainty refinement is an interesting and potentially valuable extension.\n- Demonstrates integration of early-fusion transformers with geometric consistency losses, which could inspire further research."}, "weaknesses": {"value": "### **Weaknesses**\n\n* Lack of quantitative table and ablations to support claims.\n* The distillation is mischaracterized; it is pseudo-labelling, not teacher–student distillation.\n* No analysis of occlusion handling, despite claiming improvements.\n* Masking and augmentation strategies are introduced without proper experimental justification or hyperparameter analysis.\n* The paper uses outdated pretraining (DINO); comparison with DINOv2 or MAE-based ViTs is missing.\n* No computational efficiency metrics (runtime, model size, speed).\n* Terminology and vocabulary sometimes confusing and nonstandard (\"3D augmentation *scheme*\", \"patch masking *scheme*\")\n* Missing or unclear equations (3D loss, masking formulation).\n* A full related work section about MAE would be required, right now all the related work about this domain is overlooked even though it constitutes one of the main authors' claimed contribution."}, "questions": {"value": "### **Questions**\n\n1. How does the model handle occlusions?\n2. Why is DINO used rather than DINOv2 (or even v3) or MAE, which are known to produce stronger representations?\n3. How was the 10–50% masking ratio chosen (l.203)? How was the masking strategy (l.200) chosen? Any ablation results?\n4. Can the authors clarify whether EKS-based pseudo-labelling is equivalent to a true distillation process? \n5. How is the 3D keypoint loss implemented mathematically? Please include the full equation.\n6. Are there computation or inference benchmarks to show efficiency gains of the distilled model?\n7. 3D augmentation is not very clear, at least I'm not sure I understood what was performed. A figure might be more self-explanatory.\n8. l.161: Is the positional encoding $p_i$ a contribution or does it already exist? Please cite accordingly. Is the learnable view encoding $v_i$ a contribution or does it already exist? Please cite accordingly.\n\nThe paper combines several known ideas (multi-view ViTs, pseudo-labelling, EKS) into a coherent framework, but lacks depth in quantitative validation, novelty, and clarity. With clearer definitions, full ablations, and stronger baselines (e.g., DINOv2, MAE), it could reach the acceptance threshold. Based on the answers to my questions and other reviewer's judgment, I am open to increase my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tF9ohHYVaf", "forum": "wBX8a3mVN1", "replyto": "wBX8a3mVN1", "signatures": ["ICLR.cc/2026/Conference/Submission21517/Reviewer_vSia"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21517/Reviewer_vSia"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910134997, "cdate": 1761910134997, "tmdate": 1762941813779, "mdate": 1762941813779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an uncertainty-aware framework for data-efficient multi-view animal pose estimation, addressing key challenges in quantifying animal behavior for scientific research. The framework has three components: 1. Multi-view Vision Transformer (MVT): An architecture that processes image patches from all camera views simultaneously, enabling early fusion of cross-view information through self-attention. 2. Enhanced Post-processing: An improved Ensemble Kalman Smoother (EKS) featuring a nonlinear variant and a variance inflation technique for better spatiotemporal smoothing and uncertainty calibration. Distillation Pipeline: A method to transfer knowledge from the complex multi-model EKS pipeline into a single, efficient network using high-quality pseudo-labels. The method can work without camera calibration. It is validated on diverse species datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe proposed components provide complementary benefits. For example, patch masking and 3D loss address different aspects of robustness ( occlusion handling vs. geometric consistency). \n2.\tThe approach could adapt to different camera settings (with calibration or without calibration), enabling broader application scenarios. \n3.\tThe paper demonstrates superior performance with very limited labeled data. \n4.\tThis paper is easy to read."}, "weaknesses": {"value": "1. The technical contributions seem limited. (1) The patch masking operation is commonly seen in today’s vision transformer studies. (2) nonlinear MvEKS only involves standard camera distortion (with radial and tangent distortion parameters) in the Kalman filter. \n\n2. The complexity of MVT may increase with the number of camera views increasing. \n\n3. Why not directly apply Kalman filter to 3D data x? would the performance degrade compared with applying Kalman filter to 2d keypoints then deducing 3D? \n\n4. For typical triangulation, the accuracy of 3D point estimation would increase given more number of observed views. However, I do not know how the performance of MVT would be affected by the number of camera views. An ablation study would help understand the behavior of MVT. \n\n5. A previous paper named “Triangulation residual loss for data-efficient 3D pose estimation” also addresses data-efficient 3d pose estimation of diverse species. If possible, a comparison would strengthen the paper. \n\n6. The 3D baseline only includes Anipose, limiting the persuasiveness of the experiments. Including more baselines such as selfpose3d or dannce (trained with limited samples) would enhance the experiments."}, "questions": {"value": "See above. Currently the results in the paper are not very impressive for me, as the techniques used are not surperising. The paper writing is good. The integrity of experiments still has space to improve."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "29xTicscS1", "forum": "wBX8a3mVN1", "replyto": "wBX8a3mVN1", "signatures": ["ICLR.cc/2026/Conference/Submission21517/Reviewer_q2Cc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21517/Reviewer_q2Cc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986579501, "cdate": 1761986579501, "tmdate": 1762941813272, "mdate": 1762941813272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}