{"id": "Q9FCqzOo37", "number": 15658, "cdate": 1758253664056, "mdate": 1759897290729, "content": {"title": "SELF: A Robust Singular Value and Eigenvalue Approach for LLM  Fingerprinting", "abstract": "The protection of Intellectual Property (IP) in Large Language Models (LLMs) represents a critical challenge in contemporary AI research. While fingerprinting techniques have emerged as a fundamental mechanism for detecting unauthorized model usage, existing methods--whether behavior-based or structural--suffer from vulnerabilities such as false claim attacks or susceptible to weight manipulations. To overcome these limitations, we propose SELF, a novel intrinsic weight-based fingerprinting scheme that eliminates dependency on input and inherently resists false claims. SELF achieves robust IP protection through two key innovations: 1) unique, scalable and transformation-invariant fingerprint extraction via singular value and eigenvalue decomposition of LLM attention weights, and 2) effective neural network-based fingerprint similarity comparison based on few-shot learning and data augmentation. Experimental results demonstrate SELF maintains high IP infringement detection accuracy while showing strong robustness against various downstream modifications, including quantization, pruning, and fine-tuning attacks. Our code is available at \\url{https://anonymous.4open.science/r/SELF-BC5F}.", "tldr": "A secure weight-based fingerprinting scheme for Robust LLM Intellectual Property Protection.", "keywords": ["Large Language Model", "Intellectual Property Protection", "Fingerprinting", "Singular Values", "Eigenvalues"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c9ca3c8efd3f8a2a26a85fcfa151d33bca70d583.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a method for LLM intellectual property (IP) protection, aiming to overcome the vulnerabilities of current fingerprinting schemes, which are often susceptible to input-dependent false claim attacks or structural weight manipulations. The authors introduce SELF, a novel intrinsic weight-based fingerprinting scheme designed to be inherently resistant to false claim attacks by eliminating any dependency on model inputs. The method's core mechanism extracts transformation-invariant features by applying singular value (SVD) and eigenvalue (EVD) decomposition to specific matrices derived from the LLM's attention weights. This process yields a compact fingerprint that is theoretically robust to permutation and linear mapping attacks. To compare these fingerprints, the authors employ a neural network (SimNet), trained using a few-shot learning paradigm with data augmentation, to compute a final similarity score. Experimental results on Llama-7B, Llama2-7B, and their variants demonstrate that SELF maintains high detection accuracy and shows strong robustness against various modifications, including quantization, pruning, and fine-tuning attacks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper presents promising results.\n- Perfect resistance against the False Claim Attack by eliminating dependency on inputs.\n- This paper is generally easy to read."}, "weaknesses": {"value": "- **On the Insufficient Justification for Robustness via Eq. 6 and 10:** The paper's theoretical motivation for robustness relies on Eq. 6 and Eq. 10. However, this analysis is incomplete. The authors argue that small perturbations $\\Delta M$ lead to small changes in singular/eigenvalues. This overlooks the high dimensionality of LLM weight matrices (e.g., $a \\times a$). Even if each element of $\\Delta M$ is small ($k$), the spectral norm of the matrix ($||\\Delta M||_2$) can have an upper bound as large as $ak$, which could be substantial.\n\n- **On the Limited Applicability to Modern LLM Architectures:** The proposed invariant matrices in Eq. 12 and Eq. 14 are fundamentally dependent on the Multi-Head Attention (MHA) architecture, where $W_Q$ and $W_K$ have compatible dimensions. However, many modern and state-of-the-art LLMs (e.g., Llama 3, DeepSeek) have adopted more efficient attention mechanisms like Grouped-Query Attention (GQA) and Multi-Query Attention (MQA). In GQA/MQA, the key matrix has different dimensions from the query matrix, rendering Eq. 12 and 14 computationally invalid. This severely limits the method's applicability to many of the most relevant contemporary models.\n- **On the Limited Scope of Experimental Validation:** The empirical evaluation is limited in scope. The method is only tested on two target models (Llama-7B and Llama2-7B) and primarily considers variants from fine-tuning and quantization. To make a convincing case for the method's practical robustness, it may be better for the authors to evaluate it against a wider range of common model modification techniques, such as knowledge distillation and model merging, which are known to significantly alter model weights.\n\n- **On the Misleading Analysis of False Claim Attacks:** The paper's analysis of the \"False Claim Attack\" on HuRef (Appendix B) appears to be based on a non-standard definition. In fingerprinting literature, a False Claim Attack typically refers to the optimization of a transferable fingerprint that can be successfully claimed across many unrelated models. The attack demonstrated in the paper is a one-to-one attack, where an input is optimized to forge similarity between two specific models. This is more accurately described as an Ambiguity Attack, which is a known problem that can often be mitigated by methods like timestamping [1]. The paper provides no evidence that their attack on HuRef is transferable, thus weakening the claim that their input-independent method is necessary to solve this specific vulnerability.\n\n- **On the Confusing and Unfavorable Baseline Comparison:** The comparison to baseline methods is presented in a confusing manner. The main body of the paper (Table 4) only compares SELF to REEF on unrelated models, while the full robustness comparison is relegated to Appendix F. More importantly, these appendix results suggest that SELF's performance is actually weaker than REEF's. This contradicts the paper's narrative of superior robustness and makes the justification for the new method unclear.\n\n[1] False Claims against Model Ownership Resolution."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BQrorfNZEo", "forum": "Q9FCqzOo37", "replyto": "Q9FCqzOo37", "signatures": ["ICLR.cc/2026/Conference/Submission15658/Reviewer_HKUo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15658/Reviewer_HKUo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718863322, "cdate": 1761718863322, "tmdate": 1762925914564, "mdate": 1762925914564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SELF, a robust fingerprinting method for large language models (LLMs) based on model weight singular values and eigenvalue decomposition, aiming to facilitate intellectual property protection. SELF extracts transformation-invariant fingerprint features from the attention layer weights of Transformer models and employs the few-shot learning-based SimNet to assess model similarity, thereby detecting model tampering and piracy. Experimental results demonstrate that SELF performs well under a variety of attack scenarios, such as weight perturbation, pruning, quantization, and fine-tuning, outperforming existing structure-based fingerprinting methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work addresses the vulnerability of structure-based fingerprints against transformation attacks by introducing a purely weight-driven fingerprinting approach that does not rely on model inputs, fundamentally mitigating risks from adversarial model declaration attacks.\n2. The method builds on the inherent invariance properties of singular values and eigenvalues to craft fingerprints robust to parameter perturbations, with clear mathematical underpinnings and theoretical justification.\n3. The paper presents extensive experiments covering a wide range of practical adversarial scenarios in the LLM ecosystem—including pruning, quantization, merging, fine-tuning, and impersonation attacks—and conducts detailed comparisons with several baseline methods (PCS, ICS, REEF), providing comprehensive empirical validation."}, "weaknesses": {"value": "1. Practical limitations: The method assumes a white-box setting where full access to all parameters of the target and suspected models is available. This assumption may not hold in real-world commercial or API-based protection scenarios where only black-box access is feasible.\n2. It remains unclear whether SELF is robust to parameter perturbations in non-attention layers (e.g., MLP, LayerNorm). The authors could explore whether these components should be incorporated into fingerprint construction.\n3. It is uncertain how the method adapts to structural variations in the model, such as changes in the number of layers (either adding or removing layers)."}, "questions": {"value": "1. Possible data leakage: Although the training and testing sets are nominally separated, the augmented samples used in training are highly similar to the original models, and the evaluation extensively involves these same or similar models and their fingerprints. This raises concerns about potential overlap in the feature distribution between training and test sets, possibly leading to overfitting. Furthermore, the generalization ability of SELF to unseen models has not been convincingly demonstrated. The authors are encouraged to clearly demarcate the training and testing model spaces and to include cross-family generalization experiments involving structurally diverse models to enhance credibility.\n2. The method may be vulnerable to model distillation-based stealing attacks. In particular, results on FuseLLM-7B  in Appendix F suggest that SELF struggles to maintain robustness in such scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w3GGd13fwj", "forum": "Q9FCqzOo37", "replyto": "Q9FCqzOo37", "signatures": ["ICLR.cc/2026/Conference/Submission15658/Reviewer_QAmV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15658/Reviewer_QAmV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885038840, "cdate": 1761885038840, "tmdate": 1762925913991, "mdate": 1762925913991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SELF, a weight-based LLM fingerprinting method that defends against both permutation attacks and linear-mapping attacks by exploiting matrix invariances—using singular values (invariant under row/column permutations) and eigenvalues (invariant under similarity transformations) to build robust fingerprints."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's mathematical formulation is clear. The choice to use singular values and eigenvalues to counter specific transformation attacks (permutation and linear mapping) is well-justified with solid and reliable theoretical derivations. \n\n2. The proposed methodology demonstrates practical feasibility. The entire pipeline, from fingerprint extraction to similarity comparison, is well-defined."}, "weaknesses": {"value": "1. The experimental validation for related models feels insufficient for some categories. For instance, while the paper claims to effectively identify pruned, quantized, False Claim Attack, and merged models, the main results table only lists the similarity score for a single model or none in each of these classes. Broadening the evaluation to include more diverse examples for each modification type would provide stronger evidence for the method's generalizability.  \n\n2. The experiments are centered on the Llama and Llama2 families. The study would be more comprehensive and timely if it included an evaluation of newer, state-of-the-art models. For example, models such as Qwen2.5-7B and its many variants are now widely available and would serve as an excellent test case for the robustness and scalability of the SELF method.  \n\n3. The paper does not propose a clear or principled method for setting a suitable discrimination threshold for similarity scores. This becomes particularly problematic in cases like those shown in Appendix Table 11, where the scores for heavily fine-tuned models like Codellama-7B (500B Tokens) and Llemma-7B (700B Tokens) are significantly lower than other related models, yet are still considered correct detections. Without a defined threshold, it is difficult to assess the method's reliability in these challenging, borderline scenarios."}, "questions": {"value": "1. All reported results are based on raw similarity scores. How do SELF, REEF, and ICS compare when each method applies its own paper-defined decision threshold to convert similarity into binary judgments (related vs. unrelated)? Please report thresholded metrics such as accuracy, precision/recall/F1, and confusion matrices for a fair, apples-to-apples comparison.\n\n2. For routine model checks (e.g., those in Table 6 and the additional models you included), how does a simple classifier based on Fingerprint Distance (with a principled threshold or ROC/AUC analysis) perform relative to training a SimNet? Quantitatively, how large is the accuracy gap, and in which settings (fine-tuning, pruning, quantization) does SimNet provide the most gain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kqkoKLRBZI", "forum": "Q9FCqzOo37", "replyto": "Q9FCqzOo37", "signatures": ["ICLR.cc/2026/Conference/Submission15658/Reviewer_63A4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15658/Reviewer_63A4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920242413, "cdate": 1761920242413, "tmdate": 1762925913632, "mdate": 1762925913632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SELF, a novel intrinsic weight-based fingerprinting scheme that eliminates dependency on input and inherently resists false claims. Experimental results demonstrate that SELF maintains high IP infringement detection accuracy while showing strong robustness against various downstream modifications. The proposed method is simple and effective, but there are deficiencies in its experimental design and completeness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and highly readable.\n- The method is supported by theoretical guarantees.\n- The proposed method is computationally efficient, and its effectiveness and robustness are demonstrated experimentally."}, "weaknesses": {"value": "- Due to its reliance on model weights, the method is not applicable to closed-source models. This limits its practical application value, especially given that modern AI applications are primarily offered as services.\n- Missing details in the experimental setup:\n  - The rationale for choosing attention blocks over other modules (and a comparison of the results).\n  - The reason for selecting the first N layers, as opposed to using all layers or a different set of M layers.\n  - The source of the SimNet training data, especially considering the potential imbalance between \"unrelated\" and \"target\" models. It is also unclear if training SimNet with randomly generated fingerprints against the target model's fingerprint would be similarly effective.\n- Unfair comparisons:\n  - The comparison with other methods is limited to fingerprint size. However, the SELF method introduces a SimNet, which requires separate training for each target model, representing a significant overhead that is not accounted for.\n  - The numerical comparison in Table 4 is not objective, as the values being compared are derived from different methodological systems.\n- The models used (e.g., Llama 7B as the suspect model) are relatively old. It is uncertain whether the method is applicable to current mainstream Mixture of Experts (MoE) architectures or how it performs on other model families and larger-scale models.\n- There appears to be a typo in Lines 909-911 regarding the definition of \"fingerprint margin.\" It seems it should be defined by the minimum value for related models and the maximum value for unrelated models."}, "questions": {"value": "- As shown in Appendix D, L2 distance appears to be an effective metric. What is the necessity of training a SimNet?\n- The method extracts fingerprints from the first 8 layers. Would this method fail if the model undergoes layer-pruning techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fRKSzakDKx", "forum": "Q9FCqzOo37", "replyto": "Q9FCqzOo37", "signatures": ["ICLR.cc/2026/Conference/Submission15658/Reviewer_f8CQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15658/Reviewer_f8CQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986179384, "cdate": 1761986179384, "tmdate": 1762925913031, "mdate": 1762925913031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}