{"id": "x8tl75Yznn", "number": 6555, "cdate": 1757988752191, "mdate": 1759897908106, "content": {"title": "Optimizing Temporal and Spatial Efficiency for Chain-of-Thought Reasoning in Large Language Models", "abstract": "Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) achieves remarkable performance but suffers from significant computational overhead. CoT reasoning exhibits redundancy across two critical dimensions:  temporal redundancy, where reasoning steps may be unnecessary, and spatial redundancy, where computations can be performed at reduced precision. While existing approaches require expensive dataset construction and model fine-tuning to improve reasoning efficiency, we propose Temporal-Spatial Adaptive Reasoning (TSAR), a training-free framework that jointly exploits both redundancy dimensions through coordinated optimization. TSAR segments reasoning based on Dewey's reflective thinking model, employs progressive precision reduction that adapts to both reasoning phases and progress, and coordinates termination decisions through entropy-based confidence estimation. Our adaptive scheduler prevents precision-induced errors while enabling compound efficiency gains. Extensive evaluation on diverse reasoning tasks demonstrates up to 12.4× speedup while maintaining the accuracy, establishing coordinated multi-dimensional redundancy exploitation as superior to conventional optimization strategies.", "tldr": "We introduce TSAR, a training-free framework that dramatically accelerates LLM reasoning by adaptively reducing both unnecessary thinking steps and computational precision, achieving massive speedups without sacrificing accuracy.", "keywords": ["Reasoning Model", "Model Compression", "Efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/13368cb9938ca9c628da10401966099d5176e675.pdf", "supplementary_material": "/attachment/31103b6670e2c8569a5df869aa2d43912bcc1196.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Spatio-Temporal Adaptive Reasoning (TSAR), a training-free framework that enhances the efficiency of CoT reasoning by leveraging temporal and spatial redundancies. While the paper's motivation is timely and relevant, it suffers from conceptual ambiguity, methodological opacity, a limited theoretical foundation, and overstated contributions."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This work addresses the significant problem of reducing the inference cost of Chain-of-Thought (CoT) reasoning.\n- This work proposes a training-free optimization perspective.\n- This work claims strong empirical results, albeit unconvincing."}, "weaknesses": {"value": "- **Lack of Clarity in Core Mechanisms:** Key figures and equations are either missing or uninformative, which significantly limits reader comprehension. Crucially, the paper fails to describe the formulas for core computations such as Confidence and Stability. The calculation methods for key parameters, specifically **$r_{\\phi}(s_t)$** and **$H_{\\text{base}}$**, are not explained, leaving their implementation entirely unclear. The paper overuses high-level conceptual phrases without grounding them in low-level implementation details or pseudocode. Consequently, and due to the absence of crucial experimental details, the results are difficult to **reproduce or verify**. Even though providing supplementary material, it is still disorganized and fails to get the necessary implementation specifics.\n- **Unfair Experimental Comparisons:** The performance claims should be benchmarked against established quantization models, such as \"one-bit quantization\" methods.  For a fair comparison, the baselines (e.g., s1) must also undergo the same quantization during inference. The current comparison is inequitable. Moreover, baselines like s1 can operate under a fixed token budget. If the experimental settings for budget control are inconsistent between the proposed method and the baselines, the comparison is rendered meaningless.\n- **Lack of Analysis:** The \"progressive precision reduction\" strategy lacks clear visualization. A more in-depth case study is necessary to elucidate how this process is implemented in practice. The paper lacks critical ablation studies on the contribution of keywords or the impact of the priors, making it difficult to assess the individual components of the framework.\n- **Limited Generalizability and Robustness:** The method appears heavily reliant on pre-computed statistics and static reasoning patterns of specific models. Reasoning patterns are known to vary significantly across different model families (e.g., Claude vs. Grok), which raises serious concerns about the method's generalizability and portability. It is unclear whether different benchmarks require distinct prior thresholds. If so, the reported performance gains may not be genuinely innovative but rather an artifact of task-specific, manual tuning or \"cherry-picking\".\n- **Missing Related Work:**\n\n[1] Towards reasoning era: A survey of long chain-of-thought for reasoning large language models\n\n[2] From System 1 to System 2: A Survey of Reasoning Large Language Models\n\n[3] Wait, We Don't Need to\" Wait\"! Removing Thinking Tokens Improves Reasoning Efficiency\n\n[4] Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models\n\n[5] AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting"}, "questions": {"value": "- Could you re-clarify the core mechanisms of your method? The paper currently lacks the specific formulas for core computations like Confidence and Stability.\n- How are key parameters such as $r_{\\phi}(s_t)$ and $H_{\\text{base}}$ calculated?\n- To ensure a fair comparison, why wasn't the method benchmarked against established quantization models?\n- Why not consider to compare all baselines with the same quantization during inference?\n- How was the token budget controlled consistently between your method and the baselines? Without consistent settings, the comparison seems inequitable and potentially meaningless.\n- Could you provide a more in-depth analysis, like a visualization or case study would greatly clarify the \"progressive precision reduction\" strategy.\n- Have you performed ablation studies to isolate the impact of keywords/behaviors and the priors? Understanding the contribution of each reasoning behavior is crucial for evaluating the method's design.\n- How generalizable is your method across different model families (e.g., Claude vs. Grok), given its reliance on pre-computed statistics and static reasoning patterns?\n- Do different benchmarks require manually tuning distinct prior thresholds? If so, how can we be sure the performance gains are not simply an artifact of task-specific \"cherry-picking\" rather than a genuinely innovative contribution?\n- Adding more reference\n\nI am willing to revise my score if these questions are addressed and clarified in a revision."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cSNqkZASVm", "forum": "x8tl75Yznn", "replyto": "x8tl75Yznn", "signatures": ["ICLR.cc/2026/Conference/Submission6555/Reviewer_5DDz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6555/Reviewer_5DDz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640968570, "cdate": 1761640968570, "tmdate": 1762918898185, "mdate": 1762918898185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to improve the efficiency of chain of thought in LLMs by a strategy that combines adaptive quantization and a better stopping criterion. The main idea is to partition the CoT into phases, and adjust the precision individually for each phase, via a linear function. The termination criterion goes by a confidence, which combines a relative uncertainty with a quality measure. The strategy is not learned (or maybe the linear function is learned?), and fairly light-weight when combined with recent dynamic precision techniques. It is evaluated on a set of reasoning tasks, and compared to state of the art methods for quantization and termination."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is simple and lightweight, and should be relatively easy to apply.\n\n- The empirical results in Table 1 for two larger LLMs (7B, 8B parameters) are good (fewer tokens, less precision, decent accuracy), and the method seems to outperform purely quantization and purely termination rules.\n\n- The division of the reasoning process into phases and an adaptive mechanism for each phase makes intuitive sense and seems to be effective, too."}, "weaknesses": {"value": "- It is not fully clear where exactly most of the gains are coming from. For instance, PMPD (Cheng et al, 2025a) is also developing an adaptive quantization framework, but without the termination rule, if I understand correctly. However, the proposed method seems to be more \"efficient\" than PMPD. Is the gain coming from the earlier termination or from a slightly different quantization schedule? This is not explored; the ablations only include parameters of the termination criterion.\n\n- Novelty: the idea of the paper is interesting. Yet, PMPD has a somewhat similar motivation and adaptive quantization strategy: they also partition the reasoning sequence into phases (just different ones), and they also decrease precision towards the end. What exactly is the new insight / strategy of the quantization schedule? How does it differ? The derivation of the linear function is quite simple; is the Taylor series / linearization in any way specific to this problem?\nIn what way is the termination criterion different from prior work?\n\n- Clarity: several concepts and notations are not defined at all, or not defined when they are introduced. Some procedures are also not specified. A few examples:\n* when I first read the paper, I only understood towards the middle/end of the paper that \"temporal redundancy\" simply means that the model goes on after it has essentially found the answer. I was thinking of unnecessary \"fluff\" during the reasoning process, too, and was wondering how to identify it on the fly.\n* how exactly is \"efficiency\" in Table 1 defined? As a formula?\n* what is $\\phi_t$ (eqn (5)),$ \\phi(s_t)$ , $r_{\\phi(s_t)}$ (eqn (7))?\n* how are the parameters in eqn 5 selected? And the parameters and constants in eqns 7 and 8?\n* see also my questions below"}, "questions": {"value": "- Sec. 2: what model was used for these experiments? \n\n- line 150/151: \"to statistically analyze the scores across these stages\". The Qwen reward model seems to be scoring entire responses. How exactly did you score specific phases, i.e., assign a reward to each phase?\n\n- The keywords in equations 3 and 4 are hand-selected. How about learning a simple linear classifier? How would you select the keywords that characterize different phases?\n\n- Eqn 5: How are $p_{base}$, $\\alpha$, $\\beta$, $T_{exp}$ determined? Are they specific to an LLM model or applicable across different models?\n\n- what is $\\phi_t$ (eqn (5)),$ \\phi(s_t)$ , $r_{\\phi(s_t)}$ (eqn (7))? How is $r_{\\phi(s_t)}$ computed?\n\n- Sec 3.4: what is the reasoning behind selecting only transition vocabulary to estimate uncertainty?\n\n- Sec 4.2 (Main results): how exactly is \"efficiency\" defined, as a formula?\nThe table caption says it combines spatial and temporal efficiency. What exactly does this mean?\n\n- How are the parameters and constants in eqns 7 and 8 selected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GhcqtGOAth", "forum": "x8tl75Yznn", "replyto": "x8tl75Yznn", "signatures": ["ICLR.cc/2026/Conference/Submission6555/Reviewer_r8Rq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6555/Reviewer_r8Rq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751312399, "cdate": 1761751312399, "tmdate": 1762918897841, "mdate": 1762918897841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new training-free framework TSAR to solve two dimensions of the redundancy: temporal and spatial. Authors provides a classification method using the Dewey’s model to describe the sequential process. Enough performance evaluation was done on several mathematical and general reasoning benchmark(e.g., GSM8K, MATH-500, AIME-120, MMLU) for two types of models (DeepSeek-R1-Distill-Qwen-7B and Qwen-3-7B) which shows up to 5.3x-12.4x inference speedup for special datasets. Authors also optimize redundancy from the coordinate terminations and phase-aware quantization aspect with the ablation analysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The two-dimension redundancy optimization perspective is novel with the clear motivations and observations. The training-free framework also important for the CoT reasoning redundancy improvement.\n\n2.Comprehensive evaluation shows the efficiency improvements with maintaining accuracy. The selection of datasets can also include different fields."}, "weaknesses": {"value": "1.The two-dimension redundancy optimization perspective is novel with the clear motivations and observations. The training-free framework also important for the CoT reasoning redundancy improvement.\n\n2.Comprehensive evaluation shows the efficiency improvements with maintaining accuracy. The selection of datasets can also include different fields."}, "questions": {"value": "1.In the paper, both models have a scale of 7B, but in fact, LLMs of different scales have significant differences in reasoning ability, behavioral patterns, and redundancy performance. Do you think the effectiveness of TSAR, especially its keyword based stage classification and entropy driven termination strategy, can be directly extended to larger scale models?\n\n2.What do you think is the reason why the GPQA Diamond MC Dataset is significantly more efficient than other datasets? Is it because the combination of datasets and model classification methods are more compatible?\n\n3.I am concerned about the value of the Dewey model, perhaps it is necessary to compare different classification methods? For example, I only used 3 categories(question-solving-reflection) instead of 5 categories.\n\n4.Do the three challenges have corresponding references support?\n\n5.For the keyword mechanism, how does the system handle conflicts if a fragment contains keywords from multiple stages simultaneously? Is there a priority rule, or how does this ambiguity affect subsequent accuracy allocation and termination decisions?\n\n6.Whether the termination criteria are reliable and whether different models can effectively judge without training if the completion indicators are different.\n\n7.For Fig5, How is this T_exp estimated at the beginning of the inference? Is it a fixed hyperparameter at the dataset level, or a dynamically predicted value for each input?\n\n8.For the framework you proposed, if a critical computational step (such as a complex multiplication) occurs in the middle of a long segment that is judged as having low precision requirements overall, will this introduce errors?\n\n9. Section 3.3 mentions that the precision scheduler is a linear model. However, the inference process of LLM and its sensitivity to accuracy are highly likely to be highly nonlinear. Is the choice of linear model mainly based on considerations of computational efficiency, or is there stronger theoretical or empirical evidence that it is sufficient to describe this relationship? Have you tried simpler nonlinear functions?\n\n10.I am curious about the reasons for the gain of datasets related to mathematics and fundamental knowledge. Dewey's five stage cognitive model is actually more in line with mathematics and logical reasoning. But for unstructured reasoning tasks such as history, law, or philosophy in MMLU, will TSAR's performance gain in the latter only be due to the effective early stopping of short answers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FR4vTl0vND", "forum": "x8tl75Yznn", "replyto": "x8tl75Yznn", "signatures": ["ICLR.cc/2026/Conference/Submission6555/Reviewer_b83C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6555/Reviewer_b83C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916238973, "cdate": 1761916238973, "tmdate": 1762918897456, "mdate": 1762918897456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the computational inefficiency of CoT reasoning in LLMs caused by temporal and spatial redundancy. It proposes Temporal-Spatial Adaptive Reasoning (TSAR), a training-free framework that segments reasoning via Dewey’s reflective thinking model, applies progressive precision reduction, and coordinates termination with entropy-based confidence estimation. Experiments on 6 datasets and 2 models show TSAR achieves up to 12.4× speedup while maintaining accuracy, outperforming static quantization and early termination baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The multi-dimensional redundancy exploitation is innovative and well-grounded. The framework jointly optimizes temporal (unnecessary reasoning steps) and spatial (excessive precision) redundancy, which are ignored by existing methods.\n* The experimental evaluation is comprehensive. Evaluations cover multiple mathematical and general-purpose tasks, with ablation studies confirming the contribution of key components."}, "weaknesses": {"value": "* The method’s applicability to larger or smaller models is untested: experiments only use 7B and 8B models, while LLMs with different scales of parameters may exhibit different redundancy patterns. If the time is limited, the authors can provide some results on smaller models like DeepSeek-R1-Distill-Qwen-1.5B.\n* The keyword-based phase classifier may lack generalizability and it relies on domain-specific keywords, although the paper acknowledges potential fragility on out-of-domain tasks. Additionally, how can the authors ensure that the model's response **could be\" classifed into the five distinct phases? How does TSAR handle reasoning sequences with ambiguous phase transitions (i.e., lacking predefined keywords), and what is the accuracy of phase classification in such cases?"}, "questions": {"value": "See weaknesses. Also:\n* In resource-constrained edge devices with strict memory limits, does the Any-Precision quantization’s overhead offset TSAR’s efficiency gains?\n* For tasks with highly variable reasoning chain lengths (e.g., open-ended creative reasoning or code generation tasks), how does the termination threshold affect the balance between accuracy and efficiency, and is there an adaptive threshold adjustment mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vCihpHKjWK", "forum": "x8tl75Yznn", "replyto": "x8tl75Yznn", "signatures": ["ICLR.cc/2026/Conference/Submission6555/Reviewer_tC8f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6555/Reviewer_tC8f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762912558007, "cdate": 1762912558007, "tmdate": 1762918896991, "mdate": 1762918896991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}