{"id": "opuQH9Xyu9", "number": 14675, "cdate": 1758241434679, "mdate": 1759897355729, "content": {"title": "Multi-LLM Adaptive Conformal Inference for Reliable LLM Response", "abstract": "Ensuring factuality is essential for the safe use of Large Language Models (LLMs) in high-stakes domains such as medicine and law. Conformal inference provides distribution-free guarantees, but existing approaches are either overly conservative, discarding many true-claims, or rely on adaptive error rates and simple linear models that fail to capture complex group structures. To address these challenges, we reformulate conformal inference in a multiplicative filtering setting, modeling factuality as a product of claim-level scores. Our method, Multi-LLM Adaptive Conformal Inference MACI, leverages ensembles to produce more accurate factuality scores, which in our experiments led to higher retention, while validity is preserved through group-conditional calibration. Experiments show that MACI consistently achieves user-specified coverage with substantially higher retention and lower time cost than baselines. Our anonymized repository is available at https://github.com/Anonymous2026conf/MACI.git.", "tldr": "Develop a new conformal inference method to guarantee the factuality of LLM responses and maximize its practicality by using multi-LLMs.", "keywords": ["LLM Response Factuality", "Conformal Inference", "Multi-LLM"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4227308780e3517308b0e20cdac5aab7962c3911.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a conformal inference for the distribution-free uncertainty quantification. Overall, the paper is well-written and well-positioned. Two major contributions come to my attention: 1) the conformal inference is used to select the factual LLM-generated claims with group-conditional guarantees; 2) multi-LLM ensemble is used to get a preciser score estimator $\\hat{p}$ with higher retention rate. The proposed method is evaluated on three datasets for empirical benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Typically for conformal inference, the coverage guarantee is expected. The authors have shown the (marginal and group-conditional) coverage guarantees in Theorem 1 and Theorem 2. Also, authors provided Theorem 3, in which the improved accuracy in the estimated score $\\hat{p}$ will lead to more efficient retention. The motivation to multi-LLM ensemble is intuitive on top of Theorem 3. The flow of the paper is clear and reasonable. The empirical results align with the expectations."}, "weaknesses": {"value": "A shortlist of weaknesses that might be worthy to address\n\n1. The review of the related works on conformal inference seems not sufficient in my view. The authors do not mention too many papers addressing the group-conditional coverage. A incomplete list is\n* Conditional validity of inductive conformal predictors\n* [Neurips 2024] Class-conditional conformal prediction with many classes\n* [ICML 2025] Bridging Fairness and Efficiency in Conformal Inference\n\n2. Second, I am concerned about the conditional independence assumption in Definition 1. Since the claims are generated by LLM from **the same one** prompt, it does not intuitive to assume the factuality of each claim is independent. Did authors try to model the probability of claims being true jointly given one prompt? I understand it can break the way to obtain the calibration threshold for the conformal inference. But it might be a good sensitivity check, that the modeling the joint probability of the claims being true is similar to modeling them independently.\n\n3. Third, I am confused by the dataset used in Multi-LLM ensemble. In specific, what is the M here. From my understanding, the prompt is repeated M times. But how can you make sure that the claim $c$ is the same in the repeated prompts to get the ensemble scores?\nDo you consider Some of the atomic claims are considered the same?"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5RXgFVg5lY", "forum": "opuQH9Xyu9", "replyto": "opuQH9Xyu9", "signatures": ["ICLR.cc/2026/Conference/Submission14675/Reviewer_fF7o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14675/Reviewer_fF7o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760841419209, "cdate": 1760841419209, "tmdate": 1762925045240, "mdate": 1762925045240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles reliable factuality filtering for LLM outputs using conformal inference (CI). Prior CI approaches either (i) apply a single global threshold (achieving marginal coverage but aggressively filtering, hence low retention) or (ii) learn sample-wise thresholds with adaptive error rates which improves retention but weakening fixed-level guarantees. The authors propose MACI, which (1) reformulates factuality filtering via a multiplicative conformity score (cumulative probability product across claims), (2) introduces group-conditional calibration to retain finite-sample, distribution-free guarantees within predefined subgroups, and (3) increases retention by improving factuality-score quality through a multi-LLM ensemble whose weights are optimized. They also give retention analysis in this CI setting, linking deviations from an oracle factuality-score to true-claim preservation, which motivates the ensemble design. Empirically, MACI maintains user-specified coverage while substantially improving retention and reducing time cost relative to BCI and CCI baselines; an anonymized repository is provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1- Casting filtering as a cumulative probability product of claim-level factuality scores is natural yet non-obvious; it aligns the conformity score with joint ‚Äúno-false-claims‚Äù events and helps reconcile coverage with retention. \n\n2- The paper provides a retention analysis linking oracle-estimator deviations to preserved true claims, thereby justifying the multi-LLM ensemble design rather than treating it as a heuristic. \n\n3- Group-conditional calibration preserves finite-sample guarantees at user-specified risk levels while avoiding the heavy conservatism of global thresholds; it also sidesteps adaptive-Œ± issues in high-stakes settings. \n\n4- The method achieves higher retention at target coverage and shows favorable time-cost due to single-pass scoring and simpler calibration vs. conditional boosting/sampling methods."}, "weaknesses": {"value": "1- The method relies on exchangeability/i.i.d. between calibration and test; real deployments may face distribution shift (domain, style, model temperature). The paper would benefit from experiments that explicitly stress test group-conditional coverage under covariate shift (e.g., density-ratio buckets, temporal drift). \n\n2- Guarantees are group-conditional, so the definition of groups meaningfully affects validity/retention. More analysis on group granularity vs. sample size trade-offs (and automatic group discovery) would strengthen the story. \n\n3- Retention gains hinge on factuality-score quality and thus on ensemble diversity and calibration. The paper could add ablations on ensemble composition, weighting stability, and computational budget vs. benefit. \n\n4- While the multiplicative score is well-motivated, exploring alternatives (e.g., log-sum, powered products) and reporting sensitivity would clarify robustness to mis-calibrated claim probabilities. (This is partly implied by the theory but not fully evaluated.) \n\n5- Many high-stakes domains require retrieval/attribution. It would help to discuss compatibility with retrieval-augmented pipelines and very long contexts where claim segmentation and score independence may be more brittle. (Background mentions database-heavy approaches; MACI‚Äôs stance could be clarified.)"}, "questions": {"value": "1- How does MACI‚Äôs group-conditional coverage behave under covariate shift (prompt style, domain drift, temperature changes)? Could the authors report coverage/retention under controlled density-ratio bins or apply lightweight split-conformal covariate-shift corrections? \n\n2- What guidance can you offer for choosing group granularity ùêæ given a fixed calibration budget? Any results on automatic grouping (e.g., clustering claim/prompt features) vs. fixed datasets‚Äô categories? \n\n3- Please include diversity diagnostics (pairwise disagreement, Q-statistic) and an efficiency-quality frontier varying the number/type of models and weighting schemes. How sensitive is MACI to over-reliance on a single strong model? \n\n4- Did you compare the multiplicative conformity score to additive/log-sum or power-mean variants? A small ablation would help establish that the product is not only principled but also empirically dominant. \n\n5- Table 3 is helpful; could you add wall-clock numbers at larger evaluation scales and with streaming recalibration when groups shift or new domains appear?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SPEmp7GU9g", "forum": "opuQH9Xyu9", "replyto": "opuQH9Xyu9", "signatures": ["ICLR.cc/2026/Conference/Submission14675/Reviewer_p4aF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14675/Reviewer_p4aF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681043537, "cdate": 1761681043537, "tmdate": 1762925044793, "mdate": 1762925044793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a conformal method (MACI) for filtering LLM outputs. MACI leads to higher claim retention that prior work while still maintaining marginal/group covergage."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The paper is well-motivated and the proposed method is novel. The the empirical results (Table 1) are strong and clearly communicated."}, "weaknesses": {"value": "In general, I think this is a good paper. However, I think the empirical section does not clearly communicate some details. For example, I believe completing replicating the experiments from just reading the paper (without contacting the authors or looking through their code) would be challenging. While I understand that the authors are limited in space, I also did not find it easy to parse some of these details out of the appendix.  \n\nI am open to improving my score once some of my clarifying questions and suggestions are addressed, but currently, I am not comfortable giving a higher rating."}, "questions": {"value": "Questions (I apologize in advance if some of these questions were directly addressed in the main body):\n1. How exactly do you produce estimates $\\hat{p}$ / What is $\\hat{p}$ your experiments? It wasn't clear to me if you were using base uncertainty scores like in prior work (verbalized confidence, frequency scores, model probabilities, P(True), etc.) or if the \"black-box classifier\" is also something you train for each LLM.\n2. For the ensemble, do you mean that you are taking $\\hat{p}$ from each LLM: Llama-3.3-70B-Instruct, Qwen-2.5-72B-Instruct, DeepSeek-V3?\n3. Which model's output are you filtering? Or are you running these methods on outputs from all 3 and then taking an average in Table 1?\n\nSuggestions:\n1. I recommend citing a few other works related to group-conditional conformal prediction.\n- [1] Jung, Christopher, et al. \"Batch Multivalid Conformal Prediction.\" International Conference on Learning Representations.\n- [2] Liu, Terrance, and Steven Wu. \"Multi-group Uncertainty Quantification for Long-form Text Generation.\" The 41st Conference on Uncertainty in Artificial Intelligence.\n- [3] H√©bert-Johnson, Ursula, et al. \"Multicalibration: Calibration for the (computationally-identifiable) masses.\" International Conference on Machine Learning. PMLR, 2018.\n- [4] Detommaso, Gianluca, et al. \"Multicalibration for Confidence Scoring in LLMs.\" International Conference on Machine Learning. PMLR, 2024.\n\n    In contrast to Cherian et al. 2024 / Gibbs et al., 2024, [1] and [2] frame group-conditional conformal prediction more similarly to works on multicalibration (like [3], [4]), and they also typically consider more groups than in your work or that of Cherian et al. 2024. Potentially, you could also compare to the methods evaluated in [2] (group-conditional versions split conformal and conformal quantile regression) to further bolster your experimental results since at the end of the day, the goal of those methods are also to achieve better group coverage and retention rates. However, I think just mentioning these lines of works would be sufficient.\n2. I don't see the value of comparing to sampling-based methods (SelfCheck, FSC) since as you stated, the goal of those methods are not to achieve some target coverage. I think the point you're making could also be made with any conformal method like just using BCI, so the comparison doesn't highlight to me anything about MACI. Hearing your thoughts on this would be helpful. In my opinion, you could remove these experiments without hurting the paper at all, and it would leave more room to explain other details in the main body.\n\n    Also, since these methods are in some sense, proposing better options for $\\hat{p}$, perhaps a more sensible comparison might have been to see what happens if you use these sampling methods as the base uncertainty score $\\hat{p}$ (instead of the \"frequency score\" that prior work often use) for BCI, CCI, multi-valid split conformal, etc. However, maybe that would make sense if BCI/CCI perform better using SelfCheck/FSC compared to frequency scores and that MACI still performs the best.\n3. Perhaps Figure 3b and c should be bar charts instead of line graphs.\n4. I think 5.2 and 5.3 could be moved to the appendix so that some other experimental details from the appendix can be put into the main body."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mr3DHS2FbL", "forum": "opuQH9Xyu9", "replyto": "opuQH9Xyu9", "signatures": ["ICLR.cc/2026/Conference/Submission14675/Reviewer_eeqE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14675/Reviewer_eeqE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934456570, "cdate": 1761934456570, "tmdate": 1762925044219, "mdate": 1762925044219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a subgroup-aware selective prediction procedure that learns a filter and threshold per subgroup to control FPR while maximizing retention and attains valid subgroup-conditional coverage. An ensemble over base filters stabilizes performance, and the method reports strong empirical FPR control with competitive efficiency. I appreciate the clear motivation and the practical design choices (e.g., monotonicity of FPR in the tuning parameter, which enables efficient search). My main concerns are (i) statistical stability in underrepresented groups when learning per-group thresholds/filters, (ii) the strength and usefulness of Assumptions 1‚Äì2 (Theorem 3 hinges on them), and (iii) limited discussion of PAC-style guarantees despite the monotone FPR structure. I also encourage a more robust experimental protocol (more than 30 trials). More discussion on retention-aware, adaptive-Œ± strategy would be good."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Clear problem framing at the intersection of fairness (group FPR control) and efficiency (retention), with a practical ensemble that helps smooth variance across base filters.\n\n* Good use of the monotonic relation between the parameter and FPR, which opens the door to simple line or binary searches and possible RCPS-style risk control.\n\n* Careful numerical studies and sensible metrics; the approach feels implementable."}, "weaknesses": {"value": "* Learning a separate filter and threshold per subgroup can be statistically brittle when the sample size is small, such as in underrepresented subgroups; estimates of group-level score quantiles and FPR could swing widely, harming both fairness and efficiency. Consider clustering subgroups with similar score distributions (cf. Gao et al., 2025 or Ding et al, 2023) or hierarchical / shrinkage pooling of quantiles.\n\n* As written, C appears tunable or possibly loose enough that the assumption risks being vacuous, i.e., I could take C large enough to satisfy the assumption. More clarification on whether C is pop-determined, empirically verifiable, or enters the finite-sample bound in a way that tightness matters.\n\n* Assumption 1 and 2 seem potentially strong for minority groups; if score density at the operating quantile is small, variance explodes, and makes the bound useless. More discussion on failure modes when density at decision boundary is low is needed.\n\n* Because FPR is monotone in the parameter, RCPS techniques suggest that PAC coverage bounds could be obtained.\n\n* 30 trials used in Table 1 seems small. How sensitive are results to group size imbalance?"}, "questions": {"value": "1. If test and calibration data distributions differ or if the filter is learned on the same data as thresholding without sample splitting, nominal control may not hold. It would be good to see clear explicit cross-fitting across subgroups (which could be unstable without clustering those with similar conformal scores as suggested) or an accounting for potential dataset shift, e.g., covariate shift with bounds on the density ratio.\n\n2. If C upper bounds either the change in FPR as a function of the parameter near the operating point or the inverse density, then taking it to be arbitrarily large makes the bound true but uninformative. Can C be tied to an observable quantity, e.g., plug-in estimator of f_S at the quantile with high-prob bounds? Or can it be acknowledged that small density at the threshold leads to large constants and loose rates; can this be characterized and shown more clearly? \n\n4. Are there ever settings where we must achieve a particular retention group-wise while enforcing FPR per group to be less than some threshold? If so, how might one go about doing this? \n\n5. For a group-conditional score, the efficient influence function for the \\tau quantile in the nonparametric model can yield a one-step estimator. Can the authors comment on this and its feasibility?\n\n6. Is there a retention vs FPR frontier per group? This could make the fairness-efficiency trade-off visually clear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EfY9vbMqsy", "forum": "opuQH9Xyu9", "replyto": "opuQH9Xyu9", "signatures": ["ICLR.cc/2026/Conference/Submission14675/Reviewer_4aeR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14675/Reviewer_4aeR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968423741, "cdate": 1761968423741, "tmdate": 1762925043851, "mdate": 1762925043851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}