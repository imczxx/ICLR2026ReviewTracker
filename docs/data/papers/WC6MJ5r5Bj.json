{"id": "WC6MJ5r5Bj", "number": 9075, "cdate": 1758109640212, "mdate": 1759897745001, "content": {"title": "ReCAPA: Hierarchical Predictive Correction to Mitigate Cascading Failures", "abstract": "Vision–Language–Action (VLA) agents follow instructions to perform multi-step tasks in multimodal environments. To support planning and execution in such settings, many approaches typically adopt structured post-hoc or rely on fixed decomposition and rigid alignment to improve success rate. However, once an intermediate subgoal or action is mis-specified and without a flexible correction mechanism, local errors propagate through subsequent steps and eventually accumulate into cascading failures in long-horizon reasoning. To mitigate this compounding effect, we propose Reflective Contrastive Alignment and Planning Architecture (ReCAPA), a framework that uses predictive correction to anticipate deviations and adjust representations across three levels: actions, subgoals, and trajectories. Semantic alignment is enforced at all levels using a Sinkhorn-based module and a Score-field module. The corrective signals, derived from predictive correction and alignment mechanisms, jointly update the execution network during training, enabling it to flexibly adjust fine-grained steps to remain aligned with the overall intent. We further introduce two new metrics to quantify error propagation and recovery processes in tasks.    Experiments show that ReCAPA achieves competitive results on embodied agent benchmarks such as VisualAgentBench, MineDojo, and MAP-THOR, outperforming strong proprietary and open-source Large Language Model (LLM) baselines.", "tldr": "", "keywords": ["Vision-language-action", "embodied agent", "large language models", "Long-Horizon Planning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d65336a32883e5adc23f68a3ce890b8f0500e4c8.pdf", "supplementary_material": "/attachment/bc855ee61b95991ea3715aebe5ed433d6b96b973.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes ReCAPA (Reflective Contrastive Alignment and Planning Architecture), a hierarchical predictive correction framework designed to mitigate cascading failures in long-horizon reasoning for vision–language–action (VLA) agents. Unlike prior methods that rely on fixed task decomposition or post-hoc correction, ReCAPA proactively anticipates and corrects deviations across three hierarchical levels—actions, subgoals, and trajectories—using predictive contrastive learning and prompt–trajectory alignment modules based on Sinkhorn optimal transport and score-field gradients. The authors also introduce two diagnostic metrics, Error Propagation Rate (EPR) and Propagation Attenuation Coefficient (PAC), to evaluate how errors accumulate and dissipate during execution. Experiments on VisualAgentBench, MineDojo, and MAP-THOR demonstrate that ReCAPA achieves higher success rates and superior robustness compared to strong LLM baselines, effectively reducing error propagation in multi-step embodied tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important and underexplored issue—cascading failures in long-horizon reasoning—and proposes a technically sound hierarchical correction framework. The introduction of predictive alignment across multiple levels and the new diagnostic metrics (EPR and PAC) provide useful analytical tools for evaluating robustness in embodied agents. Experiments are comprehensive, spanning several major benchmarks, and the reported improvements are consistent across tasks."}, "weaknesses": {"value": "1. Conceptual novelty is moderate. The proposed hierarchical predictive correction (HPCC) framework largely builds upon existing ideas in self-reflective planning (e.g., Reflexion, ReAct, AdaPlanner) and hierarchical alignment (e.g., HiP, TrajPrompt). The distinction between ReCAPA’s “predictive correction” and prior feedback-based reflection mechanisms is not sharply articulated. The paper would benefit from a clearer theoretical or algorithmic differentiation beyond combining multi-level prediction and Sinkhorn-based alignment.\n\n2. Many components—such as how cross-level corrective gradients interact with the execution network or how the LLM’s decomposition is integrated during inference—are described only qualitatively. Key design choices (e.g., the dimensionality of embeddings, predictor architectures, and training stability) are omitted, making the method hard to reproduce or verify.\n\n3. The paper emphasizes “hierarchical correction,” yet no qualitative rollout visualizations or case studies are shown to illustrate how errors are detected and corrected in practice. Examples demonstrating the evolution of alignment or prediction across levels would make the mechanism more convincing.\n\n4. The new EPR and PAC metrics are interesting but lack correlation studies with task performance or user interpretability. It remains unclear whether improvements in these metrics genuinely indicate better planning robustness or simply reflect model-specific biases.\n\n5. Writing and organization issues. The text is dense and occasionally repetitive, with unclear boundaries between related work and method sections. Some equations (e.g., Eq. 1–4) are introduced abruptly without sufficient intuition. The overall presentation would benefit from clearer motivation and more concise explanation of the technical pipeline."}, "questions": {"value": "The proposed EPR and PAC metrics are intriguing, but how sensitive are they to task complexity or trajectory length? Have the authors verified that improvements in these metrics correlate with human-judged robustness or actual task success rates?\n\nI also notice some typo of writing:\n\n1. The appendix title: “Appendix C.3 ABLATION ON PREDICTION AND” seems incomplete.\n\n2. The first paragraph of Related Work has repeat sentence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7nw6TjWT68", "forum": "WC6MJ5r5Bj", "replyto": "WC6MJ5r5Bj", "signatures": ["ICLR.cc/2026/Conference/Submission9075/Reviewer_3npG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9075/Reviewer_3npG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904620596, "cdate": 1761904620596, "tmdate": 1762920784878, "mdate": 1762920784878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReCAPA, a predictive correction framework designed to mitigate cascading failures in multi-step reasoning in the context of VLA models. The main ides is that small errors in subgoal or action specification can compound over time, degrading overall performance.\nReCAPA addresses this by applying predictive correction mechanisms at multiple levels such as actions, subgoals, and trajectories with the aim of anticipating and correcting deviations before they propagate.\nThe proposed method demonstrates strong performance, reportedly outperforming both proprietary and open-source large language models on benchmark tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clear, well-structured, and easy to follow, with an interesting and well-motivated idea.\n- The method is explained clearly, with a logical progression from motivation to implementation."}, "weaknesses": {"value": "- The discussion of limitations lack details. While two limitations are mentioned, the proposed mitigation strategies are not empirically validated, which reads as somewhat unbalanced.\n- The statistical significance of results is unclear. Although the authors mention using three random seeds on VisualAgentBench (Fig. 4), it is not evident how consistent or significant the improvements are across other experiments.\n- The reproducibility of the results is limited: the authors do not provide the code, and the training details are not discussed.\n- The ablation studies are limited."}, "questions": {"value": "- Why did the authors not further investigate the proposed mitigation strategies for ReCAPA’s limitations?\n- Could the authors include or discuss ablation analyses to clarify the contribution of each module?\n- Can the authors elaborate on the statistical significance of their results beyond the limited VisualAgentBench trials?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UiGA47kzzF", "forum": "WC6MJ5r5Bj", "replyto": "WC6MJ5r5Bj", "signatures": ["ICLR.cc/2026/Conference/Submission9075/Reviewer_rzHw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9075/Reviewer_rzHw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933187577, "cdate": 1761933187577, "tmdate": 1762920784474, "mdate": 1762920784474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on the problem of proactive error prediction and corrections for VLA models. The method, Reflective Contractive Alignment and Planning Architecture (ReCAPA) incorporates a bottom-up approach for deviation detection, and a top-down approach for deviation correction. ReCAPA aims to tackle the challenges of both short-term action error correction as well as long-term plan adherence. The hierarchical framework breaks down an embodied task trajectory into action, subgoal, and trajectory levels. At high level, the model aims to achieve prompt-trajectory distributional alignment through the Sinkhorn-based module. At the action and subgoal level, the model aims to achieve fine-grained prediction and execution alignment through the Score-fieldSong module. The model goes through 2 training stages: a pre-training to align state-action sequences, and a joint training of hierarchical model through a combined loss of Optimal Transport between the task prompt embedding and the overall trajectory distribution, a denoting local objective to pull fine-grained actions and subgoals closer to the prompt semantic intent, and a contrastive corrective loss between each adjacent levels (e.g.. action-subgoal, subgoal-trajectory). The paper also proposed two new evaluation methods, error propagation rage (EPR), and propagation attenuation coefficient (PAC) to explicitly measure the error propagation and cascading degree during the long horizon process. The paper conducted experiments on 3 benchmarks, and compared aganist numerous open sourced and proprietary LLMs, and demonstrated improved overall success rate as well as some EPR and PAC curves. The paper also conducted ablation studies to evaluate the importance of each component."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is well motivated and well structured. It studies the problem of action deviation from both short term mistake correction and longer term task alignment two objectives.\n- The paper proposed an interesting method to decompose the trajectories into hierarchical 3 levels, and align the predicted states with prompts for deviation detection and correction.\n- The paper offers sufficient and concise explanations behind the method, loss function designs, and acknowledge the key limitations in the conclusion section while offering future directions. \n- The paper conducted thorough experimentations against multiple benchmarks and baseline models, offering sufficient evidence to support the advantages of the proposed method."}, "weaknesses": {"value": "1. Table 1: it would be helpful to explain how 'transport rate, coverage, and balance' these three metrics are calculated\n2. Questions below"}, "questions": {"value": "1. When an inconsistencies arises at state $S_i$, with action $a_i$, ($S_i \\rightarrow a_i$), and the model learns and updates its weight, does it learn a 'correction action' from the current state ($S_i \\rightarrow a_i \\rightarrow a_j \\rightarrow S_j$) or a hopefully better action $a_j$ from the same state $S_i$, ($S_i \\rightarrow a_j$)?\n\n2. Section 3.3.2: the prompt embeddings $p$ -- are these the same prompts as in Section 3.3.1 $v$ for the overall task, or are they action/substep specific prompts? \n\n3. Ln 257: how to generate negative state-action sequences from GPT-4o-mini? \n\n4. If a model performs consistently well throughout a task, will the PAC score be high or low? \n3. Ln 114-115: duplicates"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uUyoUXBUBZ", "forum": "WC6MJ5r5Bj", "replyto": "WC6MJ5r5Bj", "signatures": ["ICLR.cc/2026/Conference/Submission9075/Reviewer_iUSv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9075/Reviewer_iUSv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998163312, "cdate": 1761998163312, "tmdate": 1762920784139, "mdate": 1762920784139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}