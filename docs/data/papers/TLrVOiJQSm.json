{"id": "TLrVOiJQSm", "number": 20163, "cdate": 1758303198568, "mdate": 1759896997676, "content": {"title": "R1-Ranker: Teaching LLM Rankers to Reason", "abstract": "Large language models (LLMs) have recently shown strong reasoning abilities\nin domains like mathematics, coding, and scientific problem-solving, yet their\npotential for ranking tasks, where prime examples include retrieval, recommender\nsystems, and LLM routing, remains underexplored. Ranking requires complex rea-\nsoning across heterogeneous candidates, but existing LLM-based rankers are often\ndomain-specific, tied to fixed backbones, and lack iterative refinement, limiting\ntheir ability to fully exploit LLMs’ reasoning potential. To address these challenges,\nwe propose R1-Ranker, a reasoning-incentive framework built on reinforcement\nlearning, with two complementary designs: DRanker, which generates full rankings\nin one shot, and IRanker, which decomposes ranking into an iterative elimination\nprocess with step-wise rewards to encourage deeper reasoning. We evaluate unified\nR1-Rankers on nine datasets spanning recommendation, routing, and passage rank-\ning, showing that IRanker-3B consistently achieves state-of-the-art performance,\nsurpasses larger 7B models on some tasks, and yields a 15.7% average relative\nimprovement. Ablation and generalization experiments further confirm the critical\nrole of reinforcement learning and iterative reasoning, with IRanker-3B improving\nzero-shot performance by over 9% on out-of-domain tasks and reasoning traces\nboosting other LLMs by up to 22.87%. These results demonstrate that unifying\ndiverse ranking tasks with a single reasoning-driven foundation model is both\neffective and essential for advancing LLM reasoning in ranking scenarios.", "tldr": "IRanker is a reasoning-based ranking model that uses RL and iterative decoding to unify diverse ranking tasks, achieving state-of-the-art results and strong zero-shot generalization.", "keywords": ["Ranking tasks", "LLM reasoning", "Candidate elimination"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d46712605845c1c03ea3d9f247384ceb1e9de6e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes R1-Ranker, a reinforcement learning framework for teaching large language models (LLMs) to perform ranking tasks via reasoning-inspired strategies. There are two variants are introduced: 1) DRanker: directly outputs a ranked list in one step, trained with a MRR–based reward. 2) IRanker: performs iterative elimination by repeatedly excluding the least relevant candidate, assigning step-wise rewards.\n\nThe authors evaluate these models across nine tasks in three domains, recommendation, LLM routing, and passage retrieval, using datasets such as MovieLens, Amazon-CD/VG, RouterEval, and MS MARCO. They claim that IRanker (3B) achieves state-of-the-art performance, sometimes surpassing 7B baselines (e.g., LLaMARank), and that iterative reasoning contributes to improved ranking accuracy and interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper attempts to unify multiple ranking paradigms (recommendation, retrieval, and routing) under a reasoning-driven reinforcement learning framework. And Introducing the iterative elimination design (IRanker) is a conceptually creative attempt to bridge “reasoning steps” and “listwise ranking decisions.”\n\n2. The use of step-wise rewards and PPO-based optimization is methodologically sound and well-aligned with reinforcement learning for language models.\n\n3. The paper is generally well-written and structured, with clear separation between DRanker and IRanker. The proposed routing task extends ranking beyond traditional document or item relevance to model selection, a novel application direction."}, "weaknesses": {"value": "1. Benchmark Coverage and Evaluation Metrics: The evaluation relies mainly on MovieLens and Amazon for recommendation and MS MARCO for retrieval, datasets with binary relevance. Missing evaluations on standard IR benchmarks such as TREC-DL, BEIR or even recent BRIGHT dataset, which are essential for validating general ranking ability. In addition, using MRR as the sole main metric is limiting; nDCG@10 and nDCG@20 are more standard and informative for graded relevance (especially in main table rather than in appendix).\n\n2. Binary Relevance Simplification: All tasks assume only relevant/irrelevant items per query, reducing ranking to binary discrimination. Consequently, the reward functions (MRR, step-wise +1 reward) fail to capture graded or positional relevance, meaning the model optimizes classification accuracy rather than true listwise ranking quality. If multiple relevant items exist, the current reward design becomes invalid or misleading.\n\n3. Efficiency and Practicality: IRanker requires D separate LLM calls for D candidates, as it eliminates one candidate per iteration. This makes inference linearly dependent on the candidate set size, introducing substantial overhead for retrieval tasks (e.g., MS MARCO). The paper does not quantify this cost (token count, latency, or FLOPs). Compared to LLaMARank (single forward pass), the iterative approach is far less efficient despite smaller model size. No mechanism for early stopping (e.g., when top items stabilize) is proposed, further reducing real-world practicality.\n\n4. Scaling and Fairness of Comparison: The primary experiments use Qwen2.5-3B-Instruct, whereas baselines like LLaMARank and PRP are 7B-scale. This mismatch complicates claims such as “3B IRanker surpasses 7B baselines.” Without a 7B variant or a performance-per-token-cost analysis, it is unclear whether the improvements stem from the algorithm or simply tuning differences.\n\n5. Reasoning–Performance Disconnect: Despite the title “Teaching LLM Ranker to Reason,” the paper never isolates the reasoning effect. The method may thus enhance ranking accuracy, but it remains unclear whether this improvement comes from reasoning ability or merely from multi-step RL fine-tuning.\n\n6. Limited Novelty and Incremental Improvement: Beyond the iterative elimination mechanism, the proposed method largely overlaps conceptually with recent reasoning-based ranking frameworks \"RaCT: Ranking-aware Chain-of-Thought Optimization for LLMs\" and \"Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models\", Although the authors employ PPO-based reinforcement learning instead of the DPO-style preference optimization used in RaCT, both methods share the core idea of leveraging chain-of-thought reasoning to improve ranking ability. However, the paper does not clarify what specific advantages its PPO formulation brings over RaCT, nor present evidence that it yields better reasoning or ranking quality. Likewise, the IRanker’s iterative elimination of the least relevant candidate resembles the progressive filtering mechanism in PeRank, yet the paper neither discusses nor compares against these related methods, leaving its distinct contribution unclear. And empirically, performance gains on traditional ranking tasks (e.g., passage reranking) are modest and sometimes weaker than baselines (such as llamarank)."}, "questions": {"value": "1. For Reward Design: How would the current reward function adapt to datasets with multiple relevant documents (graded relevance)? Could a DCG- or pairwise-margin–based reward improve stability and ranking fidelity?\n\n2. For Efficiency: Please provide quantitative results on inference cost. e.g., latency or FLOPs. Have the authors considered reverse iteration (selecting most relevant first) or early-stop heuristics to reduce redundant steps?\n\n3. For Scaling and Model Size: How does the framework behave under larger backbones (e.g., Qwen2.5-7B, LLaMA-7B)? Can the authors show a cost-normalized comparison with LLaMARank under equal compute budgets?\n\n4. For Benchmark and Metrics: Do the authors plan to include BEIR, TREC-DL, or BRIGHT datasets with graded relevance and to report nDCG as the main evaluation metric? If not, please clarify why standard ranking benchmarks were not used for the ranking tasks.\n\n5. For more related works: Please clarify the conceptual and methodological differences from recent reasoning-based ranking frameworks.\n\nTypo: The section ID is missing on line 208."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nQSyzPlZGn", "forum": "TLrVOiJQSm", "replyto": "TLrVOiJQSm", "signatures": ["ICLR.cc/2026/Conference/Submission20163/Reviewer_NhDw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20163/Reviewer_NhDw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545292060, "cdate": 1761545292060, "tmdate": 1762933680148, "mdate": 1762933680148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes R1-Ranker, a reinforcement learning-based ranking framework designed to enhance model reasoning. The framework comprises two complementary designs: DRanker, which enables a large language model (LLM) to perform one-shot reasoning by directly generating a complete ranking list of candidates, and IRanker, which iteratively reasons by removing the least relevant candidate at each step and obtains the final ranking by reversing the exclusion order. Experiments were conducted on nine datasets across three major scenarios: recommendation systems, LLM routing, and document ranking.\n\nOverall, the paper has significant shortcomings regarding writing clarity, experimental completeness, and in-depth analysis"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Applying LLM’s powerful general reasoning ability to ranking is an important and cutting-edge research direction.\n\n2. Optimizing LLM reasoning for ranking tasks via reinforcement learning presents a promising technical approach."}, "weaknesses": {"value": "The discussion on model design and comparisons is insufficient. The relationship between DRanker and IRanker is unclear. Although presented as parallel methods, IRanker clearly outperforms DRanker, and all subsequent experiments are based on IRanker, making the necessity of DRanker uncertain.\n\nWhile the paper emphasizes reasoning, it does not clearly explain how the method improves the reasoning ability of LLMs, nor does it provide a detailed description of the modeling approach in the main text.\n\nExperimental scale and baseline comparisons need improvement. The paper does not compare with important LLM-based rankers such as RankGPT. Models like monoT5 are missing from the main experiment table (Table 2) and appear only in Figure 3, which weakens the clarity and completeness of performance comparisons. Additionally, on passage ranking tasks, the proposed method fails to outperform any state-of-the-art baselines in Figure 3. Comparisons with existing reasoning rankers such as Rank1, RankR1, and TFRank are also lacking. The core method IRanker is only tested on a 3-billion-parameter model, reflecting insufficient experimental scale. Likewise, Table 4 compares only a single baseline.\n\nThe absence of reasoning-specific benchmarks reduces the persuasive power of the results. Although the paper focuses on enhancing reasoning ability in ranking tasks, it does not evaluate performance on commonly used reasoning benchmarks such as BRIGHT.\nThe focus of the method and ablation studies are missing. It is unclear whether the key contribution lies in RL or the iterative modeling approach. Results suggest both aspects are important, but the lack of detailed ablation studies, such as evaluating the effect of sequential supervised fine-tuning (SFT) followed by RL on iterative exclusion, limits understanding of their individual contributions to reasoning enhancement.\n\nThe discussion of data processing assumptions is insufficient. In recommendation and document ranking tasks, negative samples are generated by random sampling. These “pseudo-negatives” are not guaranteed to be truly irrelevant to users and may include potential interests. The paper does not address noise introduced by this negative sampling strategy or how IRanker ensures excluded negatives are genuinely irrelevant.\n\nRelated work coverage is insufficient. The related work section is brief, and the authors should provide a comprehensive survey on LLM ranking and reasoning ranking literature, including corresponding baselines in experiments.\n\nOther issues:\nSection 4.1 (line 208) contains an unreplaced placeholder reference (Section ??).\n\nFormula definitions lack clarity; for example, symbols in Equation (4) are not explicitly defined. All formula components require further clarification.\n\nBaseline definitions are unclear. For instance, baselines such as Qwen2.5-3B-Instruct-iter represent the proposed framework applied to the base model without RL training. This should be explicitly stated in the experimental section."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XhLIIS3DpD", "forum": "TLrVOiJQSm", "replyto": "TLrVOiJQSm", "signatures": ["ICLR.cc/2026/Conference/Submission20163/Reviewer_jPfB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20163/Reviewer_jPfB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899299184, "cdate": 1761899299184, "tmdate": 1762933679595, "mdate": 1762933679595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reasoning ranker model that aims to work on recommendation, routing, and passage reranking. They train the model via RL and show improvements on some of these tasks (as well as on average). They do ablations to show that the iterative ranking and RL is helpful for this model. \n\nOverall, although the paper is in a promising direction, there is a lack of detail in the training/eval section that leaves some doubt, as well as some relatively low performance even compared to very old baselines."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is on an important topic and one that needs more exploration on.\n- The paper evaluates on several tasks and has a lot of baseline models. \n- The paper is clear to read, if missing details"}, "weaknesses": {"value": "I have two main concerns with this paper:\n\n(1) The evaluations (as well as training) are unclear and seem unconventional. I am not an expert in recommender systems but do have experience in passage ranking and the setup for MS MARCO is very unusual. I am also not sure how the training/test split is done: there are 9 datasets  but it seems the model is trained on all of them and then tested on them as well? This runs counter to the paper's premise to create a model that generalizes, as all the tests are done in-domain. If the authors were to test on more standard benchmarks that would give more credence to their claims\n\n(2) I am not sure the results show that this model is stronger than others. The baselines are often quite old (e.g. 2020 and before) and model performance is not much better than them. E.g. on retrieval, it barely beats RankBERT which is ~10x smaller and is an outdated BERT-era model. Similar on Routing, we see worse performance than other models in Figure 3 despite being larger. The strongest results are on recommendation - perhaps the authors should revise the paper to pitch in that direction and remove the other tasks.  This continues throughout the paper, in Table 4 the results are almost exactly the same for both models for nearly half of the tasks, etc.\n\n(3) The paper ignores already published related work on this topic (Rank1) and even uses nearly the exact same name for their models as previous work: there already exists Rank1, Rank-R1, R1-Rec etc. I would advise the authors to pick a new name, to help them sell the paper and distinguish it from others."}, "questions": {"value": "A: How were the training/eval splits done?\n\nB: What is performance on the regular MS MARCO task (e.g. reranking on DL19 or DL20)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pxSOWgTWOr", "forum": "TLrVOiJQSm", "replyto": "TLrVOiJQSm", "signatures": ["ICLR.cc/2026/Conference/Submission20163/Reviewer_ziMx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20163/Reviewer_ziMx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941141737, "cdate": 1761941141737, "tmdate": 1762933679181, "mdate": 1762933679181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}