{"id": "35HahPHrFG", "number": 11637, "cdate": 1758202735376, "mdate": 1759897563459, "content": {"title": "Point-MoE: Large-Scale Multi-Dataset Training with Mixture-of-Experts for 3D Semantic Segmentation", "abstract": "Massively scaling data and models has transformed NLP and 2D vision, but the impact on 3D point clouds remains limited. We take a first step toward scalable 3D learning under a realistic regime: large-scale, multi-dataset joint training for 3D semantic segmentation without dataset labels at inference. Point clouds span diverse sensors (depth cameras, LiDAR) and scenes (indoor, outdoor), leading to heterogeneous sampling and semantic biases; naïvely mixing such data degrades standard backbones. We introduce Point-MoE, a Mixture-of-Experts design that adds capacity via sparsely activated expert MLPs and a lightweight top-k router, letting tokens select specialized experts without dataset supervision. Trained jointly on a diverse mix of indoor and outdoor datasets and evaluated on seen datasets and in zero-shot settings, Point-MoE outperforms prior methods without using dataset labels for training or inference. Token routing promotes expert specialization with efficient inference and modest training overhead. This suggests a scalable path for 3D perception: let the model discover structure in heterogeneous 3D data rather than enforce it through manual curation or dataset-specific heuristics.", "tldr": "Point-MoE scales 3D semantic segmentation by routing tokens to sparse experts during multi-dataset training, improving accuracy without dataset labels at train or test time.", "keywords": ["3D Semantic Segmentation", "Mixture of Expert", "Point Cloud Understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a326120b5acd8b681ebe25a254f18799219a140.pdf", "supplementary_material": "/attachment/c42d39dd1876d3589fbf104e29c56374351f3e69.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a novel evaluation scheme -- multi-dataset 3d semantic segmentation without domain labels -- for existing indoor and outdoor 3d datasets. To solve this novel task authors presents Point-MoE -- and updated version of PTv3 with MoE layers inside transformer blocks. This model is trained on mixtures of indoor / outdoor / indoor + outdoor 3d semantic segmentation datasets. Point-MoE demonstrate state-of-the-art results on 6 datasets in its formulation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents novel formulation of multi-dataset 3d semantic segmentation. Namely, authors claim that previous 3d understanding works in multi-dataset settings use *dataset label* (information from which dataset it the current point cloud) for train and / or test stages. I agree that not using this *dataset label* during test is a useful formulation.\n\n - Authors improve existing PTv3 method with novel MoE layers. These experts are claimed to split information between datasets and dataset domains, improving overall accuracy and inference speed.\n\n - The model is trained in different scenarios: single-dataset, indoor multi-dataset, indoor+outdoor multi-dataset. After training it is tested on single datasets from train set, or even zero-shot ones.\n\n - Authors provide comprehensive ablations of their MoE architecture and training parameters."}, "weaknesses": {"value": "- Main weakness is the absence of mentioning of some important 3d understanding papers that push to exactly the same multi-dataset direction. The only 2 multi-dataset methods mentioned in the paper are PPT [CVPR 2024] for semantic / instance 3d segmentation, and One-for-All [NeurIPS 2024] for 3d detection. I believe this topic is much broader than just 2 papers, including ARKit LabelMaker [CVPR 2025], UniDet3D [AAAI 2025], Sonata [CVPR 2025], Mosaic3D [CVPR 2025], and Uni3D [CVPR 2023].\n  - Both PPT and One-for-All are claimed to use *dataset label* of each scene during train and test. And the main novelty of proposed Point-MoE is the elimination of this *dataset label*. However, I think it was already explored in UniDet3D, which is trained on a mixture of 6 datasets (incl. ScanNet and S3DIS) without *dataset label* during train or inference, merging all class labels to the same set. Also UniDet3D demonstrates the importance of multi-dataset training by achieving state-of-the-art results on 6 datasets. Which contradicts with the results of Point-MoE, which are actually far from state-of-the-art on all single datasets. Batch data mixing from Tab. 4 was also used in UniDet3D. Finally, I think Uni3D can be mentioned here, as trained on 3 outdoor datasets simultaneously.\n  - Regarding multi-dataset 3d semantic segmentation, it is very much progressed in recent ARKit LabelMaker, Sonata, and Mosaic3D. These papers combine the same set of indoor ScanNet+S3DIS+Structured3D or outdoor KITTI+nuScene+Waymo as is explored in Point-MoE. Mosaic3D even goes further, unifying class labels with VLM annotation, resulting in annotation-free solution without *dataset label*.\n\n - Second major weakness is that the proposed multi-dataset pipeline requires more data compared to existing single-dataset methods, however is significantly inferior to them in accuracy.\n   - In Tab. 1 single-dataset training of PTv3-S on Structured3D achieves 81.5 mIoU and multi-dataset training - only 69.0\n   - On ScanNet multi-dataset Point-MoE (based on PTv3) achieves 76.0 mIoU, however original PTv3 with just a subset of this data already achieves 77.5 mIoU. And this disadvantage is even bigger on all other datasets: S3DIS, nuScenes, and SemanticKITTI.\n   - Recent multi-dataset methods achieves even much higher mIoU on ScanNet: 78.6 for PPT, 79.1 for ARKit LabelMaker, 79.2 for Sonata, and 80.5 for DITR [CVPRw 2025]. This gaps remain significant on all other datasets: S3DIS, nuScenes, and SemanticKITTI.\n\n - Performance gains of MoE look limited. E.g. in Tab. 1 MoE brings 59M parameters to 52M active ones for PTv3-S and 100M to 59M for PTv3-L. This sounds not very significant for the case of using as much as 8 experts on MoE. E.g. with help of MoE GPT-OSS transforms 117B to 5.1B active, or 21B to 3.6B, which sound significant to practical applications. Also Tab. 3h doesn't show the benefit of large number of expert as there is no difference between 4 and 8 experts."}, "questions": {"value": "- How is the performance of Point-MoE compared to other multi-dataset 3d segmentation methods including: ARKit LabelMaker, Sonata, Mosaic3D? Can multi-dataset + MoE scheme be adapted to show actual single-dataset state-of-the-art results?\n\n - How is the proposed training without *dataset label* compared to the one from UniDet3D? Why UniDet3D shows state-of-the-art results on all benchmarks, and Point-MoE not?\n\n - Why in Tab. 1 single-dataset training on Structured3D is significantly better than multi-dataset?\n\n - Can the proposed MoE scheme reduce the number of active parameters several times like for other state-of-the-art transformers?\n\n - As there is no difference between 8 and 4 experts in MoE in Tab. 3h, why not to check 1 or 2 experts? Why the performance is not always correlate with number of active experts in Tab. 3b? Isn't it contradictory to result in recent LLM papers, like mentioned DeepSeek one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pJzyWRIk9U", "forum": "35HahPHrFG", "replyto": "35HahPHrFG", "signatures": ["ICLR.cc/2026/Conference/Submission11637/Reviewer_9WmA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11637/Reviewer_9WmA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760868342665, "cdate": 1760868342665, "tmdate": 1762922706433, "mdate": 1762922706433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles an underexplored but important problem—large-scale multi-dataset training for 3D semantic segmentation—using a Mixture-of-Experts (MoE) framework. The authors identify a clear limitation of existing 3D methods that rely on dataset-specific normalization or labels and convincingly argue for a unified, label-free approach. The proposed Point-MoE integrates a (router+ different F expert) layer into Point Transformer V3, enabling specialization across heterogeneous 3D datasets. The method is well-motivated and supported by strong experimental evidence: consistent improvements over PTv3 and PPT across multiple datasets (ScanNet, Structured3D, S3DIS, nuScenes, etc.), as well as solid zero-shot generalization to unseen domains.  Ablations are thorough—covering routing strategies, normalization, expert count, and position—and the visual clustering analysis (t-SNE, JSD) provides meaningful insights. The writing is mostly clear, and the results are strong and significant."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper clearly defines a practical setting—multi-dataset 3D semantic segmentation without dataset labels at inference—and proposes Point-MoE, which integrates sparse Mixture-of-Experts into Point Transformer V3. Each attention output projection is replaced with lightweight expert routing (top-k), enabling automatic expert specialization across heterogeneous 3D domains. The method achieves consistent gains over PTv3 and PPT on both indoor and mixed indoor–outdoor benchmarks, while maintaining better zero-shot generalization on unseen datasets such as Matterport3D and Waymo. The ablations are thorough—covering balancing loss, routing k, normalization, expert count/placement, and batch mixing—and the t-SNE/JSD analyses nicely reveal encoder–decoder specialization behavior. Moreover, Point-MoE reduces computation and memory compared with PPT, which strengthens its practical value."}, "weaknesses": {"value": "The innovation lies mainly in applying MoE mechanisms as a routing layer into Point-TransformerV3 to 3D multi-dataset training. \nThe interaction between CLIP-based language supervision(section 3.1 Language-guided classification) is somewhat unclear to me,  and the MoE router is under-explored, like the ambiguity about how semantic alignment influences expert selection?\n\nPerformance improvements, while steady (~2.5–3.6 mIoU in the large joint setup), are moderate relative to the engineering complexity. Discussion on scalability and convergence stability for larger-scale or longer training runs is limited. Overall, the contribution is more an engineering advance than a conceptual breakthrough."}, "questions": {"value": "1.Routing behavior – Could you clarify how the router behaves at inference when encountering unseen domains? Are the same gating logits used as during training, or is there any adaptation (e.g., temperature scaling, entropy regularization)?\n\n2.CLIP alignment – How exactly does the CLIP-based language supervision interact with the MoE gating? Does semantic alignment influence expert selection, or are these processes independent?\n\n3.Load balancing – You mention that removing the auxiliary balancing loss improves results (Tab. 3a). Could you elaborate on why this happens and whether it risks expert collapse in larger-scale settings? Is the hypothesis that this is due to the inherent imbalance in the distribution of samples across datasets in 3D point cloud datasets verified, or a pure heuristic claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hV25qKBaNS", "forum": "35HahPHrFG", "replyto": "35HahPHrFG", "signatures": ["ICLR.cc/2026/Conference/Submission11637/Reviewer_rFAr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11637/Reviewer_rFAr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761293479658, "cdate": 1761293479658, "tmdate": 1762922706110, "mdate": 1762922706110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Point-MoE, a multi-dataset training method for 3D point-cloud semantic segmentation. Multi-dataset training is challenging for this task because 3D point clouds datasets are heterogeneous due to the different ways in which the point clouds are captured, making naïve multi-dataset training ineffective. Point-MoE addresses this challenge by introducing a Mixture-of-Experts (MoE) block into existing 3D segmentation models. This block routes point-level tokens to different 'expert' MLPs, allowing the model to have specialized learned layers for tokens from different datasets. Notably, this approach allows for dataset-specific components without requiring that the model knows to which dataset an input sample belongs, unlike existing work. This makes it more suitable for real-world application and improves generalization. Through experiments, Point-MoE is shown to be more effective than other multi-dataset training methods on indoor datasets and a combination of indoor and outdoor datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Overall, the paper is well written. This makes it easy to understand how the method works and what the significance is of the findings.\n\n2. The idea of applying an MoE block in 3D segmentation models to facilitate multi-dataset training is original and well-motivated. By allowing tokens to be dynamically routed to expert MLPs, the model can learn to adaptively handle different types of data, which is required for multi-dataset 3D segmentation due to the heterogeneity of 3D point cloud datasets. Moreover, these MoE blocks allow for multi-dataset optimization without requiring explicit labels about the dataset that an input sample belongs to. As a result, Point-MoE puts fewer requirements on the information that should be available during inference, and generalizes much better to unseen data for which no dataset label is available (as demonstrated in Tab. 2).\n\n3. Experimental results (Tab. 1) show that Point-MoE outperforms existing multi-dataset training approach PPT when training on indoor data only, as well as when training on both indoor and outdoor data. This demonstrates the effectiveness of the approach in the considered experimental setting.\n\n4. The paper carefully analyzes the impact of many design choices that play a role when inserting a mixture-of-experts block into a 3D segmentation model (see Tab. 3), such as the number of experts that can be selected, the position of the MoE blocks, and the operations used for each expert. These analyses provide actionable insights into the effectiveness of various configurations, and validate the design choices made for Point-MoE."}, "weaknesses": {"value": "1. While Point-MoE obtains impressive results in Tab. 1, the performance of its baselines (PTv3 and PPT) is lower than reported in the original papers. For PTv3 single-dataset training, this paper reports scores of 75.0 mIoU on ScanNet val and 67.6 mIoU on S3DIS val (should this be Area5 instead of val?), while the original paper [a] reports scores of 77.5 mIoU on ScanNet val and 73.4 mIoU on S3DIS Area5. Additionally, for PPT-S multi-dataset training, this paper reports 74.7 mIoU on ScanNet and 64.7 mIoU on S3DIS while the original paper [b] reports 78.6 mIoU on ScanNet and 74.7 mIoU on S3DIS. Why are there such considerable gaps in performance? The paper (L256) mentions that PTv3 is re-benchmarked in this paper under a 'unified protocol', but claims that this increases the performance rather than decreasing it. Due to the differences between the baselines' reported performance in Tab. 1 and in the original papers, it is not clear if Point-MoE obtains such impressive results because it is truly so effective, or if it merely obtains these results because the baselines are weaker than they should be. This limits the value of the paper.\n\n2. While Point-MoE performs quite well on average across all datasets, training it on more data does not improve performance across the board, on each individual dataset. For instance, in Tab. 1, the Structured3D performance for single-dataset training is much better than for multi-dataset training. Similarly, training on outdoor data in addition to indoor data (Tab. 1b) does not greatly impact the performance on indoor datasets (compared to Tab. 1a), and even causes a performance drop for S3DIS. These results suggest that Point-MoE still cannot fully leverage the power of massive heterogeneous datasets, which is a limitation of the approach.\n\n3. It is not clearly explained why, in Tab. 1, the number of activated model parameters is so low for Point-MoE, compared the number of parameters of PTv3. If I understand the paper correctly, Point-MoE only replaces the projection layer after the self-attention operation with a router and experts, where each expert consists of an MLP (L075). Since two of these expert MLPs will actually be used (L347-L354), this means that the parameters from PTv3's original projection layer are replaced with the parameters from the router and the 2 MLPs, when considering only activated parameters. Since the original projection layer is quite lightweight, I assume that the router and 2 MLPs do not have fewer parameters than the projection layer. Beyond this change, all other model parameters remain the same for Point-MoE compared to PTv3. In this case, how is it possible that the total number of activated parameters for Point-MoE is so much lower than for PTv3? This should be explained better. The same applies to the drops in VRAM and FLOPs (L299).\n\n4. There are some inconsistencies in the mathematical notation, and some symbols are not explained. \n    * The symbol $N$ is used to represent three different things: the number of point clouds (L148), the number of experts (L168), and the number of points (L180). This is very confusing. The paper would be clearer if a different symbol was used to represent each of these three things.\n    * Similarly, $f$ represents both the overall model in L153 and the expert function in L168.\n    * In Sec. 3.2, it is not described what $\\mathbf{Y}$ (Eq. 7) represents, and what it is used for.\n\n[a] Wu et al., Point Transformer V3: Simpler, Faster, Stronger, CVPR 2024.\n\n[b] Wu et al., Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training, CVPR 2024."}, "questions": {"value": "This paper presents an idea that is original in the context of 3D segmentation, and presents a method that is effective and has clear benefits over existing approaches, but I mainly have some concerns about the low performance of the baselines. In the rebuttal, I would like to ask the authors to provide a compelling reason why the performance of the baselines is lower than what is originally reported. Additionally, I would like to ask the authors to explain why the number of activated model parameters is so low for Point-MoE, and to fix the issues with the mathematical notation. If the authors resolve these concerns, I am willing to upgrade my rating.\n\nIn addition to this, I have the following question:\n* Could Point-MoE also improve the performance for single-dataset training, or it only effective for multi-dataset training? I could imagine that adaptively using specialized model parameters for different data subsets would also be useful within a single dataset. While it is beyond the main scope of the paper, I think the paper would be even stronger if it it could give some more insights into the situations in which Point-MoE is effective or not."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cEjpt9Lxjm", "forum": "35HahPHrFG", "replyto": "35HahPHrFG", "signatures": ["ICLR.cc/2026/Conference/Submission11637/Reviewer_wdW6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11637/Reviewer_wdW6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848891686, "cdate": 1761848891686, "tmdate": 1762922705732, "mdate": 1762922705732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores the shift from dense to sparse Transformers for point cloud processing and investigates mixed indoor-outdoor training with a MoE framework. The approach aims to leverage heterogeneous data for improved generalization and domain adaptation. The results demonstrate notable accuracy gains across datasets and suggest that the MoE design effectively mitigates cross-dataset interference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "the authors present evidence that mixed-data training enhances performance on each constituent dataset. The MoE framework appears effective at isolating domain-specific patterns while suppressing cross-dataset interference, thereby amplifying the positive effects of data scaling. This insight is valuable."}, "weaknesses": {"value": "1. The shift from dense to sparse Transformers is a significant architectural choice; however, the paper would benefit from a more thorough analysis of its practical implications on point cloud data — particularly regarding training/inference latency overhead and training stability. These aspects are critical for assessing whether the reported accuracy gains come with hidden trade-offs (e.g., convergence difficulty, increased wall-clock time, or sensitivity to hyperparameters).\n2. The experiments focus on heterogeneous indoor-outdoor training and inference. But several key questions remain underexplored:  \n- Do different sensor modalities (or sensor configurations) benefit equally from the proposed approach?  \n- Are the learned experts equally capable of capturing and adapting to sensor-specific data distributions, or do some sensors dominate or degrade performance?\n3. Indoor and outdoor scenes differ substantially in object geometry, scale, density, and context. Training across both domains clearly leverages additional data — but what specific capabilities does this impart to the model? Is it improved generalization, better feature disentanglement, or simply regularization? \n4. Mixed-dataset training yields a performance gap between indoor and outdoor evaluation: outdoor performance lags significantly behind indoor gains. This raises an important concern — could indoor data be introducing noise or domain bias that hampers scaling benefits for outdoor scenes, rather than helping them? \n5.  The observed zero-shot gains are markedly stronger in indoor settings. Is this truly due to better scene or acquisition-mode discrimination — or might it instead reflect the MoE’s improved ability to specialize on object-level distinctions that happen to be more prevalent or cleanly separable indoors?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cLyQVhCKAm", "forum": "35HahPHrFG", "replyto": "35HahPHrFG", "signatures": ["ICLR.cc/2026/Conference/Submission11637/Reviewer_tPuN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11637/Reviewer_tPuN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872366676, "cdate": 1761872366676, "tmdate": 1762922705205, "mdate": 1762922705205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}