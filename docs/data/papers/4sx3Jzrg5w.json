{"id": "4sx3Jzrg5w", "number": 5870, "cdate": 1757942582693, "mdate": 1759897948477, "content": {"title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache", "abstract": "Large Language Models struggle with memory demands from the growing Key-Value (KV) cache as context lengths increase. Existing compression methods homogenize head dimensions or rely on attention-guided token pruning, often sacrificing accuracy or introducing computational overhead. We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions: lower dimensions prioritize local context, while upper ones capture long-range dependencies. By projecting the long-context-insensitive dimensions onto orthogonal Fourier bases, FourierAttention approximates their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models show FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel, FlashFourierAttention, is designed to optimize memory via streamlined read-write operations, enabling efficient deployment without performance compromise.", "tldr": "We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions.", "keywords": ["Large Language Model", "Long-Context LLM", "KV cache optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4751bc43b0365bbbdfcaaebe72a58b648e3b60ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work is based on an interesting observation that, within attention heads, some dimensions focus on local context while others capture long-range dependencies. To exploit this heterogeneity, the paper introduces FourierAttention, which compresses the locally focused dimensions of the KV cache into fixed-length states using the HiPPO framework combined with a translated Fourier transform, achieving efficient and training-free KV-cache compression."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Novel Observation.** This work presents an interesting finding that within a single attention head, some dimensions focus on local context while others capture global dependencies. This insight provides a new perspective on KV-cache compression, distinct from prior token-wise, head-wise, or precision-wise (quantization-based) methods.\n\n* **Solid System Implementation.** The paper adapts kernels from FlashAttention and FlashDecoding to build FlashFourierAttention, effectively integrating Fourier-based compression into the attention computation and avoiding extra memory movement during inference."}, "weaknesses": {"value": "* **Limited Latency Evaluation.** The experiments only report prefill latency, while the majority of KV-cache read and update operations occur during decoding. Including TPOT (Time per Output Token) or throughput comparisons would provide a more complete performance evaluation.\n* **Limited Model Generalization.** FourierAttention is built upon the finding of heterogeneous dimension roles within attention heads, yet the evaluation is limited to LLaMA-3.1-8B and LLaMA-3.2-3B. Models with different head sizes (e.g., LLaMA-3.2-1B with only 64 dimensions per head) may not exhibit the same behavior, raising questions about general applicability.\n* **Lack of Per-Head Analysis.**\n  Prior works such as MInference [1], RazorAttention [2], DuoAttention [3], and HeadKV [4] have shown that sparsity and attention patterns vary across heads. This paper provides limited analysis on whether the observed dimension heterogeneity holds consistently across all attention heads, leaving this aspect unclear.\n\n\n\n[1] Jiang, Huiqiang, et al. \"Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention.\" *Advances in Neural Information Processing Systems* 37 (2024): 52481-52515.\n\n[2] Tang, Hanlin, et al. \"Razorattention: Efficient kv cache compression through retrieval heads.\" *arXiv preprint arXiv:2407.15891* (2024).\n\n[3] Xiao, Guangxuan, et al. \"Duoattention: Efficient long-context llm inference with retrieval and streaming heads.\" *arXiv preprint arXiv:2410.10819* (2024).\n\n[4] Fu, Yu, et al. \"Not all heads matter: A head-level kv cache compression method with integrated retrieval and reasoning.\" *arXiv preprint arXiv:2410.19258* (2024)."}, "questions": {"value": "* Regarding the main finding — why can the locally focused and globally focused dimensions be separated by contiguous index ranges? Wouldn’t one expect them to be more interleaved or randomly distributed across dimensions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nZOL7uwNV6", "forum": "4sx3Jzrg5w", "replyto": "4sx3Jzrg5w", "signatures": ["ICLR.cc/2026/Conference/Submission5870/Reviewer_wpka"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5870/Reviewer_wpka"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761131125835, "cdate": 1761131125835, "tmdate": 1762918316366, "mdate": 1762918316366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FourierAttention, an online compression method for the KV cache. FourierAttention uses Hippo-FourierT to compress the KV cache, significantly reducing the memory overhead of the KV cache while maintaining better performance compared to baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ This paper discovers that the different dimensions of Q and K in attention computation play different roles. Initially, this finding seemed counterintuitive to me, as I typically assumed that different dimensions were homogeneous—this was because I had overlooked the effect of ROPE. The paper innovatively leverages this insight by applying different compression strategies to different dimensional ranges, achieving better compression efficiency.\n\n+ The paper also implements the proposed method's corresponding FlashFourierAttention operator, which is an important contribution to the open-source community.\n\n+ The experiments provide thorough comparisons in terms of performance and latency, effectively demonstrating the superiority of the proposed method."}, "weaknesses": {"value": "+ The paper only conducts experiments on LLaMA. It would be better to include comparisons with other open-source models as well."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "awRcnadOrS", "forum": "4sx3Jzrg5w", "replyto": "4sx3Jzrg5w", "signatures": ["ICLR.cc/2026/Conference/Submission5870/Reviewer_iCAS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5870/Reviewer_iCAS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991044686, "cdate": 1761991044686, "tmdate": 1762918314826, "mdate": 1762918314826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FourierAttention, a training-free KV cache compression method. The idea is that KV cache tokens have long-context-insensitive and long-context sensitive channels -- the former can be projected onto translated Fourier bases, while the latter can be ;eft uncompressed. Authors also propose a Triton kernel for on-the-fly reconstruction of the tokens. Experiments on long-context benchmarks with LLaMA-3.* models show improvements over baseline KV cache compression methods like SnapKV and PyramidKV. One concern I have is that results in Tab. 1 often show very marginal improvements, and results are often worse than those of SnapKV and PyramidKV -- are those statistically significant? Also, authors only experiment with Llama-3.1-8B and Llama-3.2-3B -- what happens on models not in the Llama3 family (e.g. any of the recent Qwens)? On the other hand, the baselines are very competitive (according to e.g. KVPress, https://github.com/NVIDIA/kvpress) so any improvements on those is more than welcome.\n\nTiny typo: line 189 -- \"We denode\""}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Interesting analysis about how latent dimensions can be characterised into long-context-sensitive/insensitive (also from a mechinterp point of view)\n- Strong results compared with competitive baselines like SnapKV, PyramidKV, and Palu at comparable budgets on LongBench and NIAH\n- Interesting custom FlashFourierAttention kernel"}, "weaknesses": {"value": "- Absolute gains are *very* modest -- are they statistically significant?\n- I was not able to find quantitative results on latency, please let me know if I missed those\n- Experiments only on Llama3-based backbones"}, "questions": {"value": "- What happens on other families on models?\n- Are results statistically significant?\n- How sensitive are the results to errors in identifying the two types of latent dimensions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iRlm5LvD74", "forum": "4sx3Jzrg5w", "replyto": "4sx3Jzrg5w", "signatures": ["ICLR.cc/2026/Conference/Submission5870/Reviewer_sDbS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5870/Reviewer_sDbS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762177158313, "cdate": 1762177158313, "tmdate": 1762918314244, "mdate": 1762918314244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free KV cache compression method that exploits heterogeneous roles across transformer head dimensions. The authors observe that lower dimensions capture local context while upper dimensions capture long-range dependencies, and compress the former using Fourier basis functions from the HiPPO framework. A custom Triton kernel (FlashFourierAttention) is implemented for efficient deployment. The observation and the custom kernel are nice, but the evaluation is not comprehensive enough. Both the choice of the baselines (compression vs quantization) and the range of performed experiments and models selected are not enough. Also, the paper would benefit from a better framing of the method with respect to the literature about KV Cache compression."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The empirical finding that different head dimensions serve distinct roles (local vs. global context) is interesting and well-validated through ablation studies\n- Leveraging the HiPPO framework provides mathematical rigor, and the adaptation from complex to real-valued representations is sensible for practical implementation.\n- The custom Triton kernel shows practical engineering effort and is very nice."}, "weaknesses": {"value": "- (General) The title and abstract claim this is \"KV cache compression,\" but the actual mechanism is better described as lossy approximation or dimensionality reduction. For this reason, while comparing to Palu (dimensionality reduction) makes sense, comparing to SnapKV and PyramidKV is kind of strange as these perform KV Cache eviction in a different setting. The authors should (a) discuss the differences between KV Cache compression, quantization, reduction in a clear way and (b) include quantization in the baselines.\n- (Method) The observation about dimension heterogeneity is a bit over-claimed and not entirely new, as prior work has noted similar patterns. This is cited by the authors themselves.\n- In Table 1. The comparison shows that the methods have been tested with different compression ratios. Why not use the same compression ratio for fair comparison? \n- In Table 1 and in general across the paper, for a fair and comprehensive evaluation one should consider a range of compression rations for all methods instead of a specific one.\n- The choice of L-init=4, L-local=1024, N=512 appears arbitrary with no ablation study or principled justification. How sensitive is performance to these choices?\n- The method is only evaluated on Llama models. This strongly hinders the generalization of the method to different architectures.\n- I am not sure I understood how is the dimension selection actually performed? The authors say \"we directly compress and decompress all KV caches, prioritizing dimensions with smaller mean-squared error\" but doesn't specify the algorithm. Is this done once offline or adaptively?"}, "questions": {"value": "- Could you frame the contribution wrt KV Cache compression, Quantization and Low Rank reduction.\n- Could you provide exact numerical results for latency and memory consumption rather than just plots?\n- Why aren't other architectures included in the evaluation ? Could you include them ? \n- Could you provide results in Table1 for different compression ratios ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pYpn7A5PRx", "forum": "4sx3Jzrg5w", "replyto": "4sx3Jzrg5w", "signatures": ["ICLR.cc/2026/Conference/Submission5870/Reviewer_smj6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5870/Reviewer_smj6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762533735226, "cdate": 1762533735226, "tmdate": 1762918313979, "mdate": 1762918313979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}