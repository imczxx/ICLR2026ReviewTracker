{"id": "93rk0PEp92", "number": 21640, "cdate": 1758319968205, "mdate": 1759896911036, "content": {"title": "Understanding Adam through the Lens of Duality: A Unified Theory of Normalized Gradient Methods", "abstract": "This paper presents a fresh mathematical perspective on Adam, whose empirical success is in stark contrast with its analytic intractibility. We derive Adam via duality, showing that many of its design choices such as coordinate-wise normalization and exponential moving averages emerge naturally from a unified framework. Using this framework, we first analyze two normalized gradient descent methods in the setting of linearly separable data which favor different solutions with differing geometries: SignGD, which converges to a $\\ell_{\\infty}$-max-margin classifier at a rate of $\\mathcal O( \\frac{1}{\\sqrt{t}})$, and \\emph{Normalized GD}, which instead converges to a $\\ell_2$-max-margin classifier at a rate of $\\mathcal O( \\frac{1}{t})$, vastly improving upon the $\\mathcal O(\\frac 1 {\\ln t})$ rate for gradient descent. Next, we show that Adam, which replaces the solitary gradients within SignGD with exponential moving averages, achieves margin maximization at a rate of $\\mathcal O(\\frac 1 {\\sqrt{t}} )$, whereas prior work requires additional assumptions and has a rate of $\\mathcal O(\\frac{1}{t^{1/3}})$. In the stochastic setting, this duality approach gives the first high probability convergence guarantee for low test error with standard empirical choices of the momentum factors $0<\\beta_1<\\beta_2<1$, improving upon prior work which can only establish bounds in expectation, and has a slower rate of $\\mathcal O(\\frac{1}{t^{1/4}})$.", "tldr": "We present a robust framework for analyzing various normalized first order gradient methods such as Adam, obtaining theoretical guarantees for convergence rates for both full batch and stochastic Adam.", "keywords": ["Adam", "Optimization", "Normalized Gradient Methods"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a93a7ab099936d99ca72d234d661d9a170e5621e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the implicit bias of normalized steepest descent, including the SignGD and normalized gradient descent, and generalizes their results to Adam, which shares similar properties with SignGD under some scenarios. However, all these conclusions have been rigorously established in the previous works. [1, 2, 3, 4]\n\n[1] Gunasekar et al. Characterizing implicit bias in terms of optimization geometry. ICML 2018.\n\n[2] Ji and Telgarsky. Characterizing the implicit bias via a primal-dual analysis. ALT 2021.\n\n[3] Zhang et al. The implicit bias of Adam on separable data. Neurips 2024.\n\n[4] Fan et al. Implicit bias of spectral descent and muon on multiclass separable data. Neurips 2025."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Improve one previous convergence rate."}, "weaknesses": {"value": "As I have stated in the summary section, all the conclusions regarding the implicit bias of different optimizers in this paper have been rigorously established in the previous studies [1, 2, 3, 4]. Specifically, [1] demonstrated that under logistic regression settings, all the steepest descent w.r.t. $\\\\|\\cdot\\\\|$-norm will converge to the maximum $\\\\|\\cdot\\\\|$ norm, and  SignGD is exactly the normalized version of steepest descent w.r.t. the $\\ell\\_\\\\infty$. Notably, the proof of this conclusion exactly utilizes the duality between the objective norm and its dual. Given these results, [3, 4] establish the implicit bias of Adam, SignGD, and Muon by demonstrating they are normalized steepest descent w.r.t. specific norms, or share similar properties to normalized steepest descent with specific norms. **I believe these existing works have clearly and rigorously demonstrated why Adam and SignGD have the implicit bias toward the $\\ell_\\infty$ norm. I fail to see any novel motivation for this study, and feel surprised about the authors' claim that \"Unfortunately, it is unclear from prior work why Adam exhibits a $\\ell_\\infty$ max margin bias'', especially given that they have cited [3, 4] in this paper.** In addition, I am also very curious why the authors could claim they established an improved convergence rate regarding the implicit bias of normalized gradient descent, where the entirely identical conclusion is demonstrated in [2], especially given that the technical framework of this paper is established upon [2] as acknowledged in Section 3, and similarly, the entirely identical convergence rate of SignGD is also established in [4]. \n\nIn summary, this study does not provide any interesting novel results. More severely, authors overclaimed many existing results as their own contributions, especially given that these existing works are known to the authors."}, "questions": {"value": "I suggest the authors consider investigating the existing works."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4cGDZhHayg", "forum": "93rk0PEp92", "replyto": "93rk0PEp92", "signatures": ["ICLR.cc/2026/Conference/Submission21640/Reviewer_wEgr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21640/Reviewer_wEgr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760462140913, "cdate": 1760462140913, "tmdate": 1762941866809, "mdate": 1762941866809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies generalization and implicit bias of Adam, SignGD, and Normalized GD in linear classification. The analysis relies on duality. The ($\\ell_\\infty$) margin maximization rate that is shown for Adam is better than what was previously shown. For stochastic Adam with mini batches, they prove minimization of the population loss at a rate faster than previous work, and their result is with high probability, unlike previous work that proved an in-expactation result."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Understanding the implicit bias of Adam, SignGD and Normalized GD, as well as population loss minimization for stochastic Adam, are important questions. The paper improves the previously known bounds. Also, the analysis relies on a duality approach, which is of independent interest."}, "weaknesses": {"value": "Overall, I think that the contribution is nice and above the acceptance threshold. But:\n\nI have a question/concern regarding Theorem 3: \nThe formal theorem implies margin maximization if $\\beta_1 \\approx \\beta_2$, while in practice we often have $\\beta_1 \\neq \\beta_2$. In the discussion after the theorem, the authors note that the claim also holds without the $\\beta_1=\\beta_2$ condition, if we add an additional assumption similar to the one from Zhang et al. (2024). The assumption from Zhang et al. requires that at initialization (or at some time $t_0$) all coordinates of the gradient are bounded away from zero by some parameter $\\rho$. In their paper, they remark that this assumption is mild since for all non-degenerate datasets, with probability 1, the gradient at initialization does not have zero coordinates, and the dependence of their result $\\rho$ is logarithmic. Given the fact that in practice we generally have $\\beta_1 \\neq \\beta_2$, I view their assumption as milder. So, my questions are: (1) Can the authors specify how Theorem 3 changes if we use the assumption form Zhang et al.? (2) May the authors provide a proof or a proof sketch or an explanation for how the proof changes when relying on the assumption of Zhang et al.?\n\nMoreover, Figure 1 is unclear. As far as I can see, the authors do not explain what algorithm each curve represents. For example, what is $\\ell_2$-signGD? (is it really signGD or maybe the authors meant normalized GD here?). Also, what is SignGD with non-zero epsilon? (is it full-batch Adam without momentum, as opposed to “Adam” which represents here stochastic Adam?) \n\nFinally, I should say that I found some of the technical arguments hard to follow. More detailed derivations would make the paper easier to read."}, "questions": {"value": "Please see the “weaknesses” section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "51YpHRh4Or", "forum": "93rk0PEp92", "replyto": "93rk0PEp92", "signatures": ["ICLR.cc/2026/Conference/Submission21640/Reviewer_TLVt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21640/Reviewer_TLVt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761229242664, "cdate": 1761229242664, "tmdate": 1762941866588, "mdate": 1762941866588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the duality approach and mirror descent analysis to study normalized gradient descent, signGD and Adam. With this framework, it is clear that normalized GD maximizes $\\ell_2$ norm margin and signGD maximizes $\\ell_\\infty$ norm margin. They improve the rate of Adam maximizing $\\ell_\\infty$ margin with this framework. And there is a novel high probability convergence rate for Adam."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The duality framework clearly shows the difference between GD, normalized GD and signGD and possibly gives a unified prespective to understand any steepest descent.\n2. The effect of $\\epsilon$ is explicitly discussed and verified by experiments, which is often overlooked or not fully emphasized in previous works. \n3. There are novel results such as the improved rate of Adam and high probability convergence rate."}, "weaknesses": {"value": "1. It is unclear how this framework directly helps understand Adam better. In section 3, the dual objectives are clearly mentioned for GD, signGD and normalized GD but not for Adam. In section 4, the result of Adam is obtained by drawing connection to signGD. However, the connection between signGD and Adam is already mentioned in Xie and Li(2024) and Zhang et al., (2024) to help answer why Adam exhibits $\\ell_\\infty$-max margin bias. There are more literature on the implicit bias of steepest descent and Adam, which are given in the questions below. Therefore, this insight is not a novel contribution. \n2. The theoretical results are not satisfying enough in terms of the dependence on hyperparameters, which is also admitted by the authors. The theorem 5 also requires a very large batch size $b=\\sqrt{n}$, which is impractical in practice. \n3. Section 5 is disconnected to the unified framework. Also it is unclear how the lower bound justifies the dependence on $d$ in the upper bound. See questions below."}, "questions": {"value": "1. There are two important missing literatures Characterizing Implicit Bias in Terms of Optimization Geometry, Gunasekar et al. 2020 and Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks, Tsilivis et al. 2024. Can you compare with them?\n2. I don’t understand the statement of theorem 5. $\\delta$ doesn’t appear in the test error term and there is no constraint on $\\delta$. If it is correct, then we can choose $\\delta=0$ and the result is too good to be true. Also it relies on $\\ell_2$ norm margin, which is not satisfying to me when we want to connect Adam to $\\ell_\\infty$ norm margin. Can you explain the intuition of using $\\ell_2$ norm margin here?\n3. I didn’t understand the paragraph in line 393-398 when you mention the dependence on $d$ in the test error. In theorem 5, there is no $d$ in the error term explicitly. Are you saying $R$ and $\\gamma_2$ can have some dependence on $d$? Can you explain why the sample complexity is $\\Omega(d)$ rather than $\\Omega(d^2)$?\n\nTypos:\n1. Page 2 line 104, the test error should be $O(\\frac{d}{\\sqrt{n}})$. \n2. In the introduction section, there are wrong references several times when you mentioned Xie and Li 2024 proved the $\\ell_\\infty$ max margin result while it should be Zhang et al. 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fSH0dTv1qH", "forum": "93rk0PEp92", "replyto": "93rk0PEp92", "signatures": ["ICLR.cc/2026/Conference/Submission21640/Reviewer_G2uE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21640/Reviewer_G2uE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624409121, "cdate": 1761624409121, "tmdate": 1762941866355, "mdate": 1762941866355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an explanation of Adam through the lens of duality and derives a unified theoretical framework. In addition, the authors consider the deterministic setting and analyze signGD and normalized GD within this framework. Furthermore, they provide a theoretical guarantee on the test error of Adam in the stochastic setting."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides a novel perspective for understanding Adam.\n2. The paper establishes improved convergence rates compared with previous work.\n3. The paper presents a unified theoretical framework that can be applied to analyze various optimization methods."}, "weaknesses": {"value": "1. This paper is poorly written. Although I can follow the main motivation, I am still confused by many details.\n   - First, the discussions on stochastic and deterministic settings should be clearly separated. For example, in Figure 1(b), the stochastic and deterministic results should not be plotted together.\n   - Second, I am confused why Section 6 is titled “Future Related Work.” Normally, the related work section should appear before the preliminaries, and the authors should also discuss other papers that aim to understand Adam, such as [1] and [2].\n   - Third, the authors repeatedly refer to Lemma 20 in the main text, but Lemma 20 itself only appears in the appendix.\n2. In Assumption 1, $Z u$ is a vector. Does the condition $Z u > 0$ mean that all its elements are positive? If so, I suggest the authors use the notation $\\succ 0$.\n3. In Line 183, could the authors explain why $\\mathcal{L}(Zw)=\\hat{\\mathcal{R}}(w)$.\n4. For Adam in the online stochastic setting, could the author drive the regret bound for this problem?\n5. Can the authors conduct experiments to further validate their analysis?\n6. The duality framework of this paper follows previous work and the technical contribution may be the analysis under the stochastic setting. So could the authors summarize the contribution and challenges in deterministic setting?\n\nReference:\n\n[1] Understanding Why Adam Outperforms SGD: Gradient Heterogeneity in Transformers.\n\n[2] Understanding the Generalization of Adam in Learning Neural Networks with Proper Regularization."}, "questions": {"value": "Please refer to __Weaknesses__."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2PBZrpIPzK", "forum": "93rk0PEp92", "replyto": "93rk0PEp92", "signatures": ["ICLR.cc/2026/Conference/Submission21640/Reviewer_5Mqu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21640/Reviewer_5Mqu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762428248919, "cdate": 1762428248919, "tmdate": 1762941865958, "mdate": 1762941865958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}