{"id": "fkyebMiRHv", "number": 24819, "cdate": 1758360651107, "mdate": 1759896747080, "content": {"title": "Are Small Language Models the Silver Bullet to Low-Resource Languages Machine Translation?", "abstract": "Small language models (SLMs) represent parameter-efficient variants of large language models, designed to achieve computational efficiency while retaining core linguistic competencies. This study investigates the persistent challenges associated with translation performance in low-resource languages (LRLs) through a systematic evaluation of SLMs across 200 languages. In contrast to prior research, which has only marginally addressed LRL-oriented distillation, this work provides empirical evidence that transferring knowledge from large-scale teacher models to compact SLMs (2B/3B parameters) using predominantly monolingual LRL data yields substantial translation improvements, at times even surpassing models of up to 70B parameters. The primary contributions of this work can be summarized as follows: (1) the introduction of the first comprehensive quantitative benchmark evaluating SLMs over 200 languages with explicit emphasis on LRL limitations; (2) the demonstration that knowledge distillation for LRLs enhances translation quality without provoking catastrophic forgetting, while also elucidating key design priorities—prioritizing full-scale models over LoRA-based strategies, privileging data quality over data volume, and favoring decoder-only architectures as teachers over encoder–decoder frameworks; and (3) the confirmation of the robustness and transferability of these improvements across a wide spectrum of LRLs, thereby establishing a scalable and cost-effective methodology for addressing fairness disparities in multilingual translation. Overall, this study offers a rigorous validation of the feasibility and methodological best practices for applying SLMs in the context of LRLs, thereby laying an empirical foundation for their reliable deployment in low-resource language scenarios.", "tldr": "", "keywords": ["Low-resource language", "Small language models", "Luxembourgish", "Monolingual distillation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38b16480cd6f775fc943d53bd0dd9051b9e8ec98.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors first show how effectively decoder-only models perform when it comes to low-resource translation. How it varies for different models and languages. After this they focus on Luxembourgish and finetune decoder-only LLMs (gemma and llama). They show how careful data collection and augmentation can help create data to perform SFT which can do better than pretrained 70B models. They also perform LoRA finetuning to show it’s not that effective. They evaluate their models on Val 300 and FLORES200."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and is easy to follow. \n- The experimentation is quite thorough and multiple metrics are used like spBLEU, chRF++, etc.\n- Figure 2 shows the current disparity when it comes to translation in LLMs.\n- The data collection and augmentation task is done very carefully. Creating fake targets is a good way to find out which model is good enough for distillation. Results are also good for Val 300 test set authors created.\n- Catastrophic forgetting is really important and helpful to show that the model doesn’t forget the world knowledge."}, "weaknesses": {"value": "- The abstract of the paper looks much AI-generated. I sincerely request authors to re-write the abstract without heavily using AI.\n- I couldn’t see any word properly in Figure 2 as its font is too small. Legends are blurred in Figure 3. Figure 5 needs to have its size increased as it’s too small as well (I'm reading on a printed paper).\n- The authors mention in Table 1 how most of the LLMs support ~25-30 languages then how come they rely on LLMs for judging translation quality of all 200 languages?\n- The authors use gemma3-27b as a judge but it’s pretrained on those languages (140) but all languages are not officially supported. Only 35 languages are supported. I’ll take LLM-as-judge results with a bucket of salt.\n- There is no comment in the results section that their finetuned models are unable to beat the NLLB200-3.3B model in FLORES200 in both en-lb and lb-en for chRF++\n- Line 358-359: Selecting teacher of same decoder-only family during finetuning helps. I can see only Llama3.2-3B is the one example here. I’d request the authors to not generalise it without enough results.\n- LoRA comparison: In section 4.3.2, the authors mention LoRA is not recommended. I’d like to disagree with this as their data setting is not suitable for LoRA. LoRA is effective only if you have ~100k samples or less. In low data regime, LoRA will do better when compared to full finetuning. Ideally authors should have shown Figure 5 for LoRA models too (rank 256, 512) where they compete well with full finetuning. The authors utilise 615k data samples and LoRA won’t be able to generalise well. This doesn’t imply that LoRA is ineffective for LRLs."}, "questions": {"value": "- Do authors know about FLORES+ by The Open Language Data Initiative? It has more languages than FLORES200 and also fixes some problems with FLORES200.\n- For figure 2, did you use gemma3-27b as a judge or something else?\n- Can you please explain lines 156-158\n- Is there any inter-annotator agreement for Lines 233-234?\n- Why didn’t you use Gemini2.5 Pro as a judge? It supports way more languages and is quite good when it comes to translation.\n- Appendix E.4: When NLLB was not translating sentences properly was it due to context length?\n- Why is it sometimes DGDC underperforms DG? I assume a dictionary check approach should always do well?\n- Did you try finetuning NLLB2001.1b model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rDpHi5jzNs", "forum": "fkyebMiRHv", "replyto": "fkyebMiRHv", "signatures": ["ICLR.cc/2026/Conference/Submission24819/Reviewer_2ipC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24819/Reviewer_2ipC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760629693631, "cdate": 1760629693631, "tmdate": 1762943207220, "mdate": 1762943207220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates the performance of large language models (LLMs) and small language models (SLMs) on low-resource languages (LRLs), with a particular focus on Luxembourgish–English translation tasks. The authors use the FLORES-200 benchmark to systematically assess multilingual translation quality and explore a knowledge distillation framework where a teacher model transfers capabilities to smaller student models using primarily monolingual LRL corpora. The work will be positioned as an empirical analysis rather than a technical proposal of a new training or fine-tuning method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is generally easy to follow, with a clear structure and logical flow.\n- The approach addresses realistic low-resource settings by relying primarily on monolingual corpora, which aligns with real-world LRL data constraints."}, "weaknesses": {"value": "- While the study claims coverage of multiple languages, the analysis seems heavily centered on Luxembourgish–English pairs. It’s unclear how representative the selected languages are of the broader “low-resource” spectrum. We'd like to see a few more LRL cases that is linguistically distant from English.\n- The proposed approach assumes the existence of a sufficiently sized monolingual corpus. For languages with very limited text availability, it might be uncertain whether this method works.\n- Minor - Figures and tables are relatively small and difficult to read; they should be enlarged in the camera-ready version."}, "questions": {"value": "- I'd appreciate how were the “low-resource” languages defined and selected from Flores200, to better understand that those selected languages are sufficiently representative of typical LRL challenges.\n- Does the proposed approach remain effective for extremely low-resource cases where even monolingual data are scarce?\n- Can the authors clarify what “fake targets” refer to? Are these simply the pseudo-translations generated by the teacher models?\n- The distillation setup seems underspecified. Can the authors provide more details on finetuning steps?\n- What explains the As-En outlier behavior observed in Figure 6? Was there any follow-up analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rHXY176cXV", "forum": "fkyebMiRHv", "replyto": "fkyebMiRHv", "signatures": ["ICLR.cc/2026/Conference/Submission24819/Reviewer_LwNZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24819/Reviewer_LwNZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968797893, "cdate": 1761968797893, "tmdate": 1762943206629, "mdate": 1762943206629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents work on adapting small LLMs to a low resource language, Luxembourgish. It motivates this work with the need to build such models for any low resource language."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper analyses the problem of low resource well, and has some interesting results on model size vs. translation quality (albeit different models),\n\nThe approach to adapt an LLM for Luxembourgish follows the state of the art in the field."}, "weaknesses": {"value": "There is nothing really new in the paper. That translation quality degrades for languages with less data and when using LLMs of smaller size is well known. \n\nThe paper sets out to make general claims about low resource languages, but then only experiments with one language, Luxembourgish, which is a German dialect and hence an unusual case - in contrast to the languages that are currently a real challenge (African etc.).\n\nAdaptation is done only with LLMs, both of similar size, and not exactly tiny, especially when compared to dedicated MT models such as NLLB.\n\nThe experiment on 200 languages (Figure 2) uses an unconventional method - since the evaluation is done on English when comparing against English reference, standard methods such as BLEU or COMET would have worked here as well (and these numbers already exist). Later in the paper, for the adaptation experiments, these metrics are used (and there are skeptical comments about LLMaaJ in lines 324-326.\n\nTable 2 misses GPT4 BM - it would be good to know how good the best model is that you distill from. The use of the Val 300 test set is also highly questionable, since it has machine translated references. It also shows much higher adapation gains than the more traditional Flores 200 test set."}, "questions": {"value": "Table 3 - can you add the non-LoRA adaptation results into the table?\n\nFigure 3 would be easier to interpret, if the colors could reflect model sizes.\n\nLine 243 Typo in Lëtzebuerg\n\nLine 276 \"monolingual corpus is used primarily...\" -> it is only used for that, right?\n\nLine 281 \"fake\" -> \"synthetic\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PKYdOHGE8i", "forum": "fkyebMiRHv", "replyto": "fkyebMiRHv", "signatures": ["ICLR.cc/2026/Conference/Submission24819/Reviewer_YKNe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24819/Reviewer_YKNe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969709671, "cdate": 1761969709671, "tmdate": 1762943206289, "mdate": 1762943206289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}