{"id": "gkjYmREgzi", "number": 6423, "cdate": 1757982411830, "mdate": 1759897915564, "content": {"title": "Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation", "abstract": "Modern long-context large language models (LLMs) perform well on synthetic \"needle-in-a-haystack\" (NIAH) benchmarks, but such tests overlook how noisy contexts arise from biased retrieval and agentic workflows. We argue that haystack engineering is necessary to construct noisy long contexts that faithfully capture key real-world factors---distraction from heterogeneous biased retrievers and cascading errors in agentic workflows---to test models' long-context robustness. We instantiate it through HaystackCraft, a new NIAH benchmark built on the full English Wikipedia hyperlink network with multi-hop questions. HaystackCraft evaluates how heterogeneous retrieval strategies (e.g., sparse, dense, hybrid, and graph-based) affect distractor composition, haystack ordering, and downstream LLM performance. HaystackCraft further extends NIAH to dynamic, LLM-dependent settings that simulate agentic operations, where models refine queries, reflect on their past reasonings, and decide when to stop. Experiments with 15 long-context models show that (1) while stronger dense retrievers can introduce more challenging distractors, graph-based reranking simultaneously improves retrieval effectiveness and mitigates more harmful distractors; (2) in agentic tests, even advanced models like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated distractors or struggle to perform early stops. These results highlight persistent challenges in agentic long-context reasoning and establish HaystackCraft as a valuable testbed for future progress.", "tldr": "", "keywords": ["benchmark", "long-context language models", "retrieval-augmented generation", "context engineering", "agentic workflows"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6dd78450bc90c106fac627013ededca355801a4e.pdf", "supplementary_material": "/attachment/d3644aca485bd98a89bea1e8726c1ebd08dedf08.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces \"haystack engineering\" to create more realistic benchmarks for long-context LLMs, arguing that synthetic \"Needle-in-a-Haystack\" (NIAH) tests fail to capture real-world noise. The authors present HaystackCraft, a new benchmark built on Wikipedia, which evaluates models in two settings: (1) Static NIAH, testing how performance is affected by distractors from heterogeneous retrieval strategies (sparse, dense, graph-based), and (2) Dynamic NIAH, a novel agentic setting that tests robustness to cascading errors from multi-step query refinement. Key findings show that graph-based (PPR) reranking can mitigate harmful distractors in the static setting. More importantly, in the dynamic setting, even SOTA models like Gemini 2.5 Pro and GPT-5 are brittle, with performance degrading over multiple reasoning rounds, demonstrating that models are currently more robust to \"width\" (long contexts) than \"depth\" (reasoning iterations)."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper develops more realistic evaluations (dynamic, LLM-dependent, agentic) for long-context models to tackle the limitations of synthetic NIAH benchmarks. The \"haystack engineering\" concept is a clear and valuable contribution to this effort.\n2. This paper has a thorough experiments among the retrieval strategies under static NIAH settings."}, "weaknesses": {"value": "1. The figures in this work is confusing and didn't show the main point. In Figure3, the authors want to compare different retrieval strategy then it is better to show one figure with different strategies and the scores are average on all 12 models.  In Figure4, the authors compare retriever-ranked and random haystack ordering, but there is no comparison in the figure and we need to compare Figure3 and Figure4 to understand.\n2. The paper complicated its simple idea with many unnecessary explanations. For example, the idea of using retrieved documents as haystack is already well-used by most of works, but this paper used section 3.1, 3.2, 3.4 to explain this. On the other hand, the paper should discuss novel dynamic NIAH in details. The paper should be well written with concise data/task introduction and detailed analysis without exceeding 9 page limit."}, "questions": {"value": "1. In Figure2, the authors claim reranking with PRR consistently boosts performance, especially for multi-hop questions. However, 4-hop shows opposite results. Can you explain why?\n2. Can you show some examples that how graph-based reranking mitigates harmful distractors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wk88KRnSmb", "forum": "gkjYmREgzi", "replyto": "gkjYmREgzi", "signatures": ["ICLR.cc/2026/Conference/Submission6423/Reviewer_L4UT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6423/Reviewer_L4UT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761333883322, "cdate": 1761333883322, "tmdate": 1762918817377, "mdate": 1762918817377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper first identifies the gap between synthetic and real-world long-context evaluation settings. To address the Sim2Real gap, the authors propose a new benchmark mimicing real-world long-context reasoning tasks, including RAG and multi-round agentic reasoning.\n\nThe authors study heterogeneous retrieval and ranking methods, including sparse (BM25), dense (embedding-based), hybrid (sparse+dense), and graph-based (e.g., PPR) approaches. The benchmark includes (1) a document corpus, constructed from the entire English Wikipedia hyperlink network and (2) a QA dataset, extracted from single-hop (NQ) and multi-hop (MuSiQue) benchmarks.\n\nThe paper reports that:\n\n1. The models' performance is degraded with longer context windows. Dense retrieval methods introduce \"harder\" distractor documents under larger context sizes and graph-based reranking improves downstream LLM performance. The authors also observe that ordering of the documents matters (not conclusive in which direction).\n\n2. Under multi-round agentic settings, the models are sensitive to (1) cascading amplified errors from their own reasoning traces, (2) deviations from the original query's intent. An interesting insight is that the models are more robust to larger context windows (width) compared to more iterations (depth)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper identifies a critical gap between synthetic and real-world NIAH tasks. The **problem setup** is the main strength and the key contribution of the paper. Additionally,\n\n1. The paper considers two tasks inspired from real-world long-context use cases: RAG and multi-round agentic reasoning (e.g., deep research).\n\n2. The paper thoroughly studies the effect of different retrieval strategies and context engineering design choices (e.g., ordering) on the downstream performance.\n\n3. Given that the dataset is released, the benchmark can be valuable for long-context NIAH-style evaluation."}, "weaknesses": {"value": "**Agentic context engineering evaluation**\n\n1. Why doesn't the model see the original question and its all previous queries as part of the input context? This can potentially mitigate the observed failure case of deviating from the original query and is more aligned with how the current models are used in multi-round tool-calling scenarios.\n\n2. The current deep search pipelines often utilize tool-calling capabilities of the models: the models formulate queries to a search engine / retrieval module. This setup is more realistic and can change the reported results, as the models will differentiate between the user's query and queries to the retrieval module, resolving some of the observed failure cases.\n\n**Presentation**\n\nThe plots are hard to interpret. Instead of reporting all the models in a single plot with multiple subplots, I would recommend:\n\n1. Having a separate plot(s) for scale ablation (how does the scale affect the results). This ablation can be done on the Qwen models.\n\n2. Having a separate plot(s) for number of hops: select 2-3 models and report the results broken down by the number of hops.\n\n3. Select the main models for the main paper and move the rest to a model family ablation in the appendix.\n\n**Ordering**\n\nAre the documents ordered in ascending / descending order based on the retriever scores? How does this affect the performance?\n\nWhen comparing ordered vs random context, do the authors observe lost-in-the-middle phenomenon reported in prior work? Additional ablations on the document ordering can clear the unconclusive (model-specific) results (Figure 4).\n\n**Data contamination**\n\nThe authors report non-zero performance with no context (i.e., models relying on their parametric knowledge). Removing the questions that can be answered without search can improve the reliability of the results: I suspect that the effect of context on parametrically answerable and non-answerable questions would be different.\n\n**Literature Review**\n\nEven though the authors reference related work throughout the paper, the main literature review section seems limited, specifically for long-context agentic reasoning applications (e.g., search engine contamination, deep research under noisy retrieval, failure modes) and search benchmarks (e.g., BrowseComp, SearchArena, etc.)"}, "questions": {"value": "**Hop count**\n\nThe authors report variation in retrieval efficiency across questions requiring different number of hops. Additionally, it is reported that multi-hop questions are more robust to data contamination. These observations lead to a natural questions: how does this affect the downstream LLM performance? Do the current results still hold if we only look at single-hop or 4-hop question?\n\n**RAG Models**\n\nWhy don't the authors evaluate stronger models on the RAG task (e.g., GPT-5, Gemini-2.5-Pro)? Do the current findings hold for current SOTA models?\n\n**Ordering**\n\nAre the documents ordered in ascending / descending order based on the retriever scores? How does this affect the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ANoHsQDB45", "forum": "gkjYmREgzi", "replyto": "gkjYmREgzi", "signatures": ["ICLR.cc/2026/Conference/Submission6423/Reviewer_SV16"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6423/Reviewer_SV16"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861850288, "cdate": 1761861850288, "tmdate": 1762918817022, "mdate": 1762918817022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark HaystackCraft built on Wikipedia hyperlink network to evaluate how heterogeneous retrieval strategies affect distractor composition, haystack ordering and LLM performance. Evaluation results using 15 long-context models highlight robust agentic long-context reasoning is far from solved."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ a new benchmark designed to evaluate how heterogeneous retrieval strategies affect distractor composition, haystack ordering and LLM performance\n+ interesting findings from the evaluation using 15 long-context models"}, "weaknesses": {"value": "- lack of deep analysis on the underlying reasons of the performance of the models over the HaystackCraft benchmark"}, "questions": {"value": "It is interesting that the current LLMs are more robust to noisy long contexts than noisy reasoning iterations. What are the possible reasons for such an observation? How can it be leveraged to improve the performance of LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IcnOdKZevO", "forum": "gkjYmREgzi", "replyto": "gkjYmREgzi", "signatures": ["ICLR.cc/2026/Conference/Submission6423/Reviewer_Q1KQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6423/Reviewer_Q1KQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997822418, "cdate": 1761997822418, "tmdate": 1762918816680, "mdate": 1762918816680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that standard \"Needle-in-a-Haystack\" (NIAH) benchmarks are insufficient for evaluating long-context LLMs. The authors claim these synthetic tests fail to model two critical, real-world sources of noise: (1) biased distractors from heterogeneous retrieval systems (like in RAG) and (2) cascading, self-generated errors from multi-step agentic workflows.\n\nTo address this, they propose \"Haystack Engineering,\" a principle for constructing more realistic noisy contexts. They instantiate this with a new benchmark, \"HaystackCraft,\" built on the Wikipedia hyperlink network and multi-hop questions. The benchmark includes both \"static\" tests (evaluating different retrievers like sparse, dense, and graph-based) and \"dynamic\" tests (evaluating multi-round agentic reasoning).\n\nThe paper's main finding is that even SOTA models (e.g., GPT-5, Gemini 2.5 Pro) are brittle in these dynamic, agentic settings, suffering from cascading errors and an inability to self-correct."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The critique of synthetic NIAH tests is spot-on. We need evaluations that look more like real-world RAG and agent systems.\n\n2. The dynamic test cleverly simulates how an agent can create its own errors.\n\n3. Comparing sparse, dense, and graph-based retrievers is practical and shows how much the \"haystack\" composition matters.\n\n4. The results, especially showing that graph-based reranking (PPR) helps a lot, are genuinely useful for practitioners."}, "weaknesses": {"value": "1. Is this \"Agent\" too simple? The \"agent\" here is just a simple \"summarize + refine query\" loop. It's not clear if the findings about \"cascading errors\" apply to all agentic reasoning, or just to this one very simple design. A more robust agent architecture might not fail this way.\n\n2. The paper concludes \"width is safer than depth\" (more context > more steps). But is the model failing because of the steps (depth), or because the underlying multi-hop question is just too hard? It's hard to tell if this is a failure of agentic reasoning or just a failure at a complex task.\n\n3. The benchmark is only based on Wikipedia. Real-world systems run on messy, domain-specific data (like legal, medical, or internal wikis). It's a big question mark whether these findings generalize beyond a clean, well-structured corpus."}, "questions": {"value": "1. How sensitive are these \"cascading errors\" to your specific prompt? Could a better-worded prompt that, for example, tells the model to \"doubt its previous findings\" or \"consider alternatives\" prevent these failures?\n\n2. What happens if you run this dynamic test on a simple single-hop task? If the models still fail, you've proven \"depth\" is the problem. If they succeed, it suggests the failure is just a mix of \"depth\" and \"task complexity.\"\n\n3. Why do you believe models failed to stop early? Is this a failure of in-context learning (they didn't understand the \"stop\" instruction), or is it a more fundamental failure of the model's self-assessment (i.e., it thought it needed more info, even when it didn't)? Could this be \"fixed\" with a different prompt or a small number of few-shot examples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pdfMYprOxD", "forum": "gkjYmREgzi", "replyto": "gkjYmREgzi", "signatures": ["ICLR.cc/2026/Conference/Submission6423/Reviewer_7Rcq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6423/Reviewer_7Rcq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179375694, "cdate": 1762179375694, "tmdate": 1762918816367, "mdate": 1762918816367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}