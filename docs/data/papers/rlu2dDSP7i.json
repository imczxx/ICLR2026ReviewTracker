{"id": "rlu2dDSP7i", "number": 24066, "cdate": 1758352328799, "mdate": 1759896783254, "content": {"title": "Spoken Named Entity Localization as a Dense Prediction task: End-to-end Frame-Wise Entity Detection", "abstract": "Precise temporal localization of named entities in speech is crucial for privacy-preserving audio processing. However, prevailing cascaded pipelines propagate transcription errors and end‐to‐end models lack the temporal granularity required for reliable frame‐level detection. To address these limitations, we introduce DEnSNEL (Dense End‐to‐end Spoken Named Entity Localizer), the first end-to-end model to perform direct, frame-level spoken named entity localization, without intermediate character or text representations. We reformulate the task as a dense, frame-wise binary classification (\"entity\" vs. \"non-entity\"), employing a lightweight encoder-classifier architecture. To improve boundary delineation, DEnSNEL incorporates a learnable complex filter bank to capture phonetic information, and employs a boundary-focused loss that explicitly optimizes span precision. On the SLUE Phase 2 benchmark, DEnSNEL outperforms state-of-the-art methods in frame-level spoken named entity localization, while requiring substantially fewer parameters. With its lightweight architecture and precise frame-level entity detection, DEnSNEL offers a practical and efficient solution for real-world privacy-sensitive speech applications. Our code and models will be released publicly.", "tldr": "DEnSNEL is a lightweight end-to-end model that directly detects and localizes spoken named entities at the audio-frame level for precise, privacy-preserving redaction.", "keywords": ["Speech", "Named Entity Recognition", "Audio", "Entity Detection"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d464a149ad5ce2b6d13b486ff3620568a4031e4.pdf", "supplementary_material": "/attachment/0ca4cddae0a2206a33e59861d3e04d3fa4dcdcab.zip"}, "replies": [{"content": {"summary": {"value": "The proposed DEnSNEL reconstructs spoken NEL into a binary classification task, determining ``entity/non-entity'' on the frame sequence and outputting results directly at the audio frame level. This approach fuses a pre-trained audio encoder with a learnable filter to enhance phoneme boundary cues. It also proposes the TBAL loss, a combination of the tIoU loss and the BCE loss, to simultaneously optimize semantic correctness and temporal accuracy in classification. It achieved state-of-the-art results on NEL in SLUE Phase-2."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed TBAL loss, a combination of tIoU loss and BCE loss, simultaneously considers the semantic correctness of NEL and the temporal precision of boundary prediction. The model achieves state-of-the-art performance on the SLUE Phase-2 NEL benchmark, while using fewer parameters compared to previous approaches."}, "weaknesses": {"value": "* For the second contribution \"Phonetic-enhanced feature integration for precise entity boundary detection\", the paper lacks a complete and detailed ablation study. The paper only compares the performance score with previous methods, providing coarse-grained evidence of the model’s effectiveness, but lacks fine-grained analysis to validate the contribution of the proposed filter. It is recommended to include ablation experiments that remove the filter and conduct deeper validation across different loss functions, such as:(1) Whisper-encoder + IoU-loss,(2) Whisper-encoder + BCE-loss,(3) Whisper-encoder + TBAL-loss.\n* Lacking in-depth analysis. It is recommended to further analyze the proposed method, such as how it compares to previous methods in different audio lengths, how sensitive it is to environmental noise, etc. Alternatively, the effect of adding filters to the previous task implementation method could be added.\n* Many important details are unclear. For example, The ablation experiments on TBAL-loss in the paper demonstrate that neither tIoU-loss nor BCE-loss alone are optimal. However, the paper does not provide a detailed explanation of the $\\beta$ hyperparameter in TBAL-loss. Specifically, it is unclear which loss should be favored during actual training, or whether there are any trends when setting the two losses at different ratios. We recommend providing more comprehensive hyperparameter settings, along with reasonable experiments and explanations. The paper also does not provide a detailed rationale for fine-tuning only the deeper 1/6 layers of the audio-encoder during training. Is there a better choice for this ratio, or is there a trend? We recommend providing additional examples of other ratios, as well as two boundary conditions: full fine-tuning and full freezing."}, "questions": {"value": "The following questions mainly come from weakness. I hope the author will explain them in detail in the rebuttal stage.\n\n* Q1: For the second contribution of the paper, the authors emphasize the importance of \"Phonetic-enhanced feature integration,\" but no in-depth ablation analysis is conducted in the experiment. What is the performance of the model after removing the filter component?\n* Q2: In section 3.4, the authors proposed TBAL-loss, which combines tIoU-loss and BCE-loss, and conducted an ablation experiment on loss in the experimental section. It can be seen that there are differences in the effects of the two losses. However, the authors did not explain the $\\beta$ parameter in TBAL-loss. In actual training, are the two treated equally or will one be biased towards the other? At the same time, is there any trend change with different $\\beta$?\n* Q3: In section 4.2, the authors mention \"maintain an approximately constant ratio of 1:6 between fine-tuned and total encoder layers across all variants.\" How is this figure of 1:6 obtained? Is there a more optimal option? Are there any trend changes in different ratios? What are the effects for full freezing and full fine-tuning?\n* Q4: How the model performs under different audio lengths? How it performs when faced with noisy audio?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GLKuW3acLt", "forum": "rlu2dDSP7i", "replyto": "rlu2dDSP7i", "signatures": ["ICLR.cc/2026/Conference/Submission24066/Reviewer_M8NX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24066/Reviewer_M8NX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761211188710, "cdate": 1761211188710, "tmdate": 1762942920114, "mdate": 1762942920114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DEnSNEL, the first fully end-to-end and text-free model for frame-level spoken Named Entity Localization (NEL). Instead of cascading ASR and NER or augmenting CTC with entity tokens, DEnSNEL reformulates NEL as a dense binary classification task over 20 ms audio frames. A lightweight encoder–classifier processes mel-spectrogram frames in parallel with a learnable complex Gabor filterbank that operates directly on the raw waveform to capture phonetic transients; the two streams are concatenated and fed to a 3-layer MLP. Training is driven by the proposed Temporal Boundary Alignment Loss (TBAL), which combines frame-wise BCE with a 1-D IoU term to sharpen entity boundaries. On the SLUE Phase-2 benchmark, DEnSNEL achieves 78.4 frame-F1 with a Whisper-medium encoder, outperforming the previous best result by +5.8 F1 while using fewer parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s key strength lies in its elegant reformulation of spoken named-entity localization as a fully end-to-end, frame-level binary prediction task, eliminating any reliance on intermediate text or CTC decoding. By equipping a lightweight encoder with a learnable complex Gabor filterbank, the model captures millisecond-scale phonetic transients that mel-spectrograms smooth away, yielding a +2.3 F1 boost with only 8 k extra parameters. Coupled with the proposed Temporal Boundary Alignment Loss, this design achieves a new state-of-the-art 78.4 frame-F1 on SLUE Phase-2 while running in ≈0.1 s on a single GPU, demonstrating that high accuracy and on-device efficiency can be obtained simultaneously."}, "weaknesses": {"value": "A notable architectural weakness is that the phonetic Gabor features and the contextual encoder outputs are simply concatenated without any channel-wise normalization or learnable projection; the raw Gabor magnitudes can be one to two orders of magnitude larger than the log-mel/logit features, so the subsequent layer-norm only rescales per-frame vectors and does not eliminate the risk of gradient dominance or sensitivity to the number of filters. This scale mismatch, omitted in Figure 2, could explain the saturation (and even drop) in F1 when more than 512 filters are used and should be addressed by separate layer-norm or bottleneck fusion blocks to ensure stable and interpretable feature blending."}, "questions": {"value": "1.  the phonetic Gabor features and the contextual encoder outputs are simply concatenated without any channel-wise normalization or learnable projection; the raw Gabor magnitudes can be one to two orders of magnitude larger than the log-mel/logit features\n2.  No statistical significance tests are reported: the +5.8 F1 gain is quoted from a single run, and variance across random seeds or bootstrap resampling is unknown.\n3.  Hyper-parameter sensitivity (TBAL weight β, Gabor filter count K, threshold τ) is only explored in isolation; joint tuning and confidence intervals are absent.\n4.  All experiments are conducted on English data; cross-lingual or cross-domain generalisation (e.g., French, clinical speech) is not examined.\n5.  The promised code and pre-trained models are not yet available, impeding reproducibility and fair comparison by future work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nA6PV5uSRT", "forum": "rlu2dDSP7i", "replyto": "rlu2dDSP7i", "signatures": ["ICLR.cc/2026/Conference/Submission24066/Reviewer_G3FA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24066/Reviewer_G3FA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533744122, "cdate": 1761533744122, "tmdate": 1762942919876, "mdate": 1762942919876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DEnSNEL (Dense End-to-end Spoken Named Entity Localizer), a novel approach for frame-level named entity localization in speech audio. The method reformulates the task as dense binary classification (entity vs. non-entity) rather than text-based sequence labeling, employing a lightweight encoder with a learnable complex filter bank for phonetic features and a boundary-focused loss function. On the SLUE Phase 2 benchmark, DEnSNEL achieves state-of-the-art results (78.4 F1)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Novel Problem Formulation: The reformulation of spoken NEL as frame-wise dense prediction without intermediate text representations is innovative and well-motivated for privacy applications. This direct audio-to-mask approach is more suitable for on-device processing.\n\n2. Technical Contributions:\nThe learnable complex filter bank with jointly learned center frequencies and Q-factors is a solid extension of prior work (SincNet, Gabor filters)\nThe temporal boundary alignment loss (TBAL) combining BCE and temporal IoU is intuitive and effective\nThe integration of phonetic and contextual features is well-designed\n\n\n3. Strong Empirical Results: State-of-the-art performance on SLUE Phase 2 (+5.8 F1 over previous best)"}, "weaknesses": {"value": "1. Limited Scope:\nThe privacy motivation is compelling but insufficiently validated. Real PII masking requires entity-type–aware classification (e.g., masking PERSON but not ORG), which this binary approach cannot achieve. This limitation should be discussed and addressed in greater depth in the paper. I believe this limitation limits the contribution and applicability of this work.\n\n2. Generalization Concerns:\nIt remains unclear whether a model trained to detect a fixed set of entity types can generalize to unseen categories, such as works of art, not encountered during training. This is a realistic and important scenario for practical deployment. \n\n3. Single-Source Evaluation:\nThe reliance on a single dataset for evaluation is a fundamental weakness. The BLAB test set is relatively small, limiting generalizability. The paper would be stronger if the authors included evaluation on an independently annotated dataset or another NER corpus (e.g., SLURP) with alignments generated via MFA. Incorporating some form of zero-shot evaluation would significantly strengthen the paper’s claims about model applicability.\n\n4. Incomplete Ablation Analysis:\nThe phonetic filter ablation (Section 4.4, Figure 6) only varies the number of filters. A crucial missing ablation is performance without any phonetic features. Including this comparison would help clarify the true contribution of phonetic information."}, "questions": {"value": "Please check weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c5zgwUwKI4", "forum": "rlu2dDSP7i", "replyto": "rlu2dDSP7i", "signatures": ["ICLR.cc/2026/Conference/Submission24066/Reviewer_AiW7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24066/Reviewer_AiW7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971054084, "cdate": 1761971054084, "tmdate": 1762942919551, "mdate": 1762942919551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript proposes DEnSNEL, an end-to-end system for spoken named entity localization that predicts entity presence directly at the audio frame level, bypassing intermediate text representations common to cascaded ASR–NER pipelines. The method reframes NEL as dense binary classification (\"entity\" vs. \"non-entity\"), using a lightweight encoder–classifier augmented with a learnable complex filter bank to capture phonetic cues crucial for accurate span boundaries. Training is guided by a temporal boundary alignment loss (TBAL) that combines binary cross-entropy with a temporal IoU term to explicitly optimize boundary precision. On the SLUE Phase 2 benchmark, DEnSNEL reportedly surpasses state-of-the-art frame-level localization performance while using substantially fewer parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- lightweight architecture\n- learnable complex filter bank + a boundary-focused loss for span precision"}, "weaknesses": {"value": "- evaluation is limited\n- missing practitioner baselines"}, "questions": {"value": "## Practitioner baselines\n\n> DEnSNEL achieves state-of-the-art performance on the SLUE Phase 2 benchmark (Shon et al., 2023) for spoken NEL.\n\n> BLAB (Ahia et al., 2025) introduces a challenging long-form audio benchmark, reporting results using both open-source and proprietary audio LMs, including Gemini 2.0 Pro and GPT-4o. This differs from our motivation, which focuses on lightweight, on-device named entity detection and masking prior to sending audio to those large MLLMs.\n\nThe manuscript argues for a different goal—lightweight, on-device NER masking prior to sending audio to large MLLMs. That’s a possible deployment case, but it does not totally remove the need to benchmark against what practitioners / real-world products actually run today.\n\n1. There is no head-to-head comparison with a Whisper/WhisperX (or NeMo) alignment + strong text NER pipeline on **the same SLUE Phase-2 split**. This is the de-facto production baseline for timestamped NER; without it, it’s unclear whether DEnSNEL’s frame-level approach is competitive in accuracy and efficiency.\n\n2. Promptable audio-LLM baseline. There is no comparison with prompted audio LLMs (e.g., GPT-4o, Gemini, Qwen-Audio) instructed to return entity spans with timestamps. Even if the objectives differ, practitioners weigh trade-offs across these options. The BLAB discussion’s claim that MLLMs are “not comparable” is insufficient for a 2025 audience; the field needs calibrated, side-by-side results.\n\n3. Trade-off vs. audio-LLMs: For GPT-4o/Gemini/Qwen-Audio prompted for timestamped NER on SLUE, what is the **quality/latency/cost trade-off**, and in which regimes (device class, latency budget, privacy precision) does DEnSNEL win or lose?\n\n## Limited Evaluation\n\n> Our evaluation does not correspond to BLAB-MINI, the short-audio subset of BLAB with audio segments under 30 seconds, which is not given in detail. Instead, we process long recordings in 30-second chunks, resulting in a hybrid evaluation setting.\n\n> The entity categories in BLAB (Event, Location, NORP, Organization, Person, TV Show, Temporal, and Work of Art) do not fully correspond to those used in our approach. We therefore restrict our analysis to the 49 recordings in the “All Entities” category, which contains all nine entity types, while the remaining files consist of single-category recordings.\n\n4. BLAB comparison might not be apples-to-apples. The “zero-shot BLAB” analysis might be methodologically misaligned on at least two fronts:\n\n    4.1 Segmentation protocol. BLAB-MINI is short-audio (<30s), whereas the paper processes long recordings in 30-second chunks. This creates a hybrid evaluation regime that may alter boundary distributions, pause statistics, and error modes. Any reported performance should be prefaced with an explicit caveat and, ideally, replicated under BLAB’s native segmentation.\n\n    4.2 BLAB’s categories might not fully align with the paper’s entity set. Restricting analysis to the \"All Entities\" subset partially mitigates this but also changes the underlying class priors and may inflate or deflate difficulty. Without a principled mapping or macro-averaged reporting across matched classes, the comparison does not constitute a fair head-to-head.\n\n    The authors should consider providing a deterministic mapping between BLAB and your entity set (one-to-one, one-to-many, or \"dropped\" classes). Report macro- and micro-averaged metrics on the intersection, plus per-class breakdowns to show where performance shifts.\n\n5. The empirical evidence is concentrated almost entirely on SLUE Phase 2. With only one primary benchmark, it is difficult to disentangle overfitting to dataset artifacts from genuine generalization. External validity across accents, domains, recording conditions, and entity taxonomies is therefore not established.\n\n## Potential Leakage\n\n> We use the Whisper encoder (Radford et al., 2023) as the audio encoder in our DEnSNEL architecture.\n\n6. Are pretrained encoders (e.g., Whisper encoder) exposed to SLUE/BLAB audio or transcripts during pretraining? Evidence of deduplication or near-duplicate removal?\n\n> We evaluate our approach on the SLUE-VoxPopuli NEL test set from the SLUE Phase 2 benchmark (Shon et al., 2023). As no official training data is available, we construct our own training dataset by augmenting the SLUE-VoxPopuli NER training set (Shon et al., 2022) with word-level timestamps.\n\n7. How could you avoid data leakage when constructing your own training dataset? If there is not any standard training/eval/test split, how could you make sure your comparison is fair enough?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tOlDWmW4Fy", "forum": "rlu2dDSP7i", "replyto": "rlu2dDSP7i", "signatures": ["ICLR.cc/2026/Conference/Submission24066/Reviewer_TZxi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24066/Reviewer_TZxi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977973461, "cdate": 1761977973461, "tmdate": 1762942919237, "mdate": 1762942919237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}