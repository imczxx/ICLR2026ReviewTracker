{"id": "TeyHNq4WlI", "number": 846, "cdate": 1756820781701, "mdate": 1759898239030, "content": {"title": "InfBaGel: Human-Object-Scene Interaction Generation with Dynamic Perception and Iterative Refinement", "abstract": "Human–object–scene interactions (HOSI) generation has broad applications in embodied AI, simulation, and animation. Unlike human–object interaction (HOI) and human–scene interaction (HSI), HOSI generation requires reasoning over dynamic object–scene changes, yet suffers from limited annotated data. To address  these issues, we propose a coarse‑to‑fine instruction‑conditioned interaction generation framework that is explicitly aligned with the iterative denoising process of a consistency model. In particular, we adopt a dynamic perception strategy that leverages trajectories from the preceding refinement to update scene context and condition subsequent refinement at each denoising step of consistency model, yielding consistent interactions.\nTo further reduce physical artifacts, we introduce a bump‑aware guidance that mitigates collisions and penetrations during sampling without requiring fine‑grained scene geometry, enabling real‑time generation. To overcome  data scarcity, we design a hybrid training startegy that synthesizes pseudo‑HOSI samples by injecting voxelized scene occupancy into HOI datasets and jointly trains with high‑fidelity HSI data, allowing interaction learning while preserving realistic scene awareness. Extensive experiments demonstrate that our method achieves state‑of‑the‑art performance in both HOSI and HOI generation, and strong  generalization to unseen scenes. Code and datasets will be released upon acceptance.", "tldr": "", "keywords": ["Interaction Generation", "Consistency Model", "Human Motion"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fbb4ba606e1edd4391e55988dd943c99974e1db6.pdf", "supplementary_material": "/attachment/a0bde508cf51f508a7c8a59e0deb2b6ab0d15a87.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents InfBaGel, a unified framework for human–object–scene interaction generation that combines a coarse-to-fine consistency model with bump-aware guidance for physically plausible motion (mostly for collision free). It also introduces a hybrid data training strategy that mixes real and synthetic data to reduce annotation needs and improve generalization across diverse scenes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation and contribution is clear: to generate higher quality HOSI data and to tackle the data scarcity of this field.\n2. Experiment shows that the method is significantly better than previous works in terms of collision avoidance."}, "weaknesses": {"value": "1. Limited qualitative results. Only two action sequences are presented in the paper, which makes it difficult to fully assess the model’s performance.\n2. The method part confuse me a bit. Specifically, for Section 3.2 (“Motion Consistency Model”), does this section correspond to the Auto-regressive Consistency Model shown in the figure? Additionally, in line 277, how are human joints and object points constructed from human motion only? Is this perhaps a typo?\n3. In Figure 2(c), there is a noticeable penetration between the hand and the box. Is this due to training solely on the OMOMO dataset?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WtStJTdslP", "forum": "TeyHNq4WlI", "replyto": "TeyHNq4WlI", "signatures": ["ICLR.cc/2026/Conference/Submission846/Reviewer_73R8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission846/Reviewer_73R8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530361187, "cdate": 1761530361187, "tmdate": 1762915625787, "mdate": 1762915625787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The problem this paper aims to tackle is the (1) limited annotation in dataset and the (2) dynamic scene variation (the movement of objects changes the scene as well), for a task termed as human-object-scene interaction generation. The proposed approach consists of several contributions/effective modules, termed as the dynamic perception, bump-aware guidance, the consistency guided iterative refinement, with the help of hybrid-data-training strategy. It seems that using the proposed method achieves good results in the new benchmark HOSI and the HOI as well."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strength\n-\tThe technical method seems correct\n-\tThe motivation is well-presented"}, "weaknesses": {"value": "Weakness \n\n-\tIn table 3, it seems that the C+B provides inferior results. It does not convince me that all components achieve better performance, esp in the task accuracy metric. Maybe more explanations should be provided. \n\n-\tIt seems that the baseline methods in comparison seem limited. More evaluations should be performed comparing with more methods. \n\n\n-\tThe proposed contributions seem a lot. These makes the contributions diluted for some perspectives. The relationship among these contributions, and evaluation of their effectiveness one by one is necessary. Otherwise we cannot understand how each component tackles which portion of data."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BeuAGimGbU", "forum": "TeyHNq4WlI", "replyto": "TeyHNq4WlI", "signatures": ["ICLR.cc/2026/Conference/Submission846/Reviewer_f1br"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission846/Reviewer_f1br"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884714659, "cdate": 1761884714659, "tmdate": 1762915625660, "mdate": 1762915625660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a coarse-to-fine, instruction-conditioned generation framework using a consistency model that iteratively refines the human-object-scene interaction (HOSI). \n\nA dynamic perception strategy is key, where scene context is continuously updated using trajectories from the preceding refinement step to condition the subsequent refinement, ensuring consistent interactions.\n\nExperiment results show that the hybrid data training strategy overcomes data limitations by combining\nreal-scene HSI data with synthesized HOI data, achieving zero-shot scene generalization."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The core idea is to integrate a dynamic perception strategy and a coarse-to-fine iterative refinement scheme, which is aligned with a consistency model's denoising process. \n\nThis novel approach ensures that the scene context (which changes due to object and human movement) is updated at each step, leading to more physically consistent and realistic HOSI.\n\nSuch a method is robust, allowing the model to learn diverse and complex interactions without the dependence on a massive, fully annotated HOSI dataset, enabling strong generalization to unseen scenes."}, "weaknesses": {"value": "Need to show some benefits on the downstream tasks such as robot learning, gaming, humanoid motion learning, e.t.c.\n\nHow is the model integrated with real physics? \n\nAnd how is the model transfer to different simulation socially-inteactive environments (indoor vs outdoor, household, store, hospital, e.t.c.)?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ebkN5j3m9n", "forum": "TeyHNq4WlI", "replyto": "TeyHNq4WlI", "signatures": ["ICLR.cc/2026/Conference/Submission846/Reviewer_9B2s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission846/Reviewer_9B2s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930722542, "cdate": 1761930722542, "tmdate": 1762915625540, "mdate": 1762915625540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to generate paired human-object interaction motion in the context of a static scene. The input is the static scene geometry, interaction object geometry, text instruction, and goal position of the object. The dynamic human-object-scene interaction is represented as a sequence of five voxel grids, representing the scene and object occupancy at the start position, goal position, and (optionally) 3 intermediate timesteps along the human-object interaction trajectory. Voxel grids at intermediate timesteps can be masked out for unconditional trajectry generation. Then, an auto-regressive consistency model is trained via consistency distillation to generate human and object motion conditioned on these inputs. To ensure strict collision avoidance, the denoised trajectories outputted by the consistency model are iteratively refined to avoid colliding with the static scene geometry, by introducing guidance from each colliding voxel to the nearest free-space. The method is trained on a combination of real-world HSI data (LINGO) and synthetic HOI data (OMOMO)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Human-object-scene interaction generation is important and challenging problem.\n- The paper achieves substantially improved results compared to LINGO and TRUMANS.\n- The method is technically sound and the proposed components are effective: the dynamic perception encoder helps improve task success rate, the consistency model improves generation speed, and the bump-aware guidance reduces scene penetration."}, "weaknesses": {"value": "- Despite the dynamic scene encoder and bump-aware guidance, the method does not seem to generate physically plausible manipulation motions. In Fig2(c) and Fig2(e), the crate is at a strange angle, hand poses do not seem to be predicted, and the hands are either penetrating with the crate or not in contact.\n- The results are shown for a single task of moving an object from the start location to the goal location through a static scene. Therefore, it is not clear to me that the text instruction is necessary or useful in the problem formulation. More qualitative or quantitative examples for diverse interaction types (such as those seen in OMOMO - kicking and dragging objects, lifting over your head or in different manners) would strengthen the applicability of the method"}, "questions": {"value": "- The exact output format of the dynamic perception encoder has many missing details and is not fully clear in Sec. 3.2. It would be helpful to refer to the appendix in the main text and provide mathematical notations with the shape of each variable. My understanding is, each \"voxel grid\" is a 3D array {0,1,2}^(NxNxN) where N is the size of the voxel grid? And there are five voxel grids, corresponding to the start position, goal position, and three intermediate timestamps? How are the intermediate timestamps sampled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CPqeWTZVVh", "forum": "TeyHNq4WlI", "replyto": "TeyHNq4WlI", "signatures": ["ICLR.cc/2026/Conference/Submission846/Reviewer_2xtj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission846/Reviewer_2xtj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946330935, "cdate": 1761946330935, "tmdate": 1762915625378, "mdate": 1762915625378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}