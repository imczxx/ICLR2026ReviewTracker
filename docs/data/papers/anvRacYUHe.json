{"id": "anvRacYUHe", "number": 20061, "cdate": 1758302002417, "mdate": 1759897003800, "content": {"title": "Learning equivariant tensor function representations via covariant algebra of binary forms", "abstract": "Representing tensor-valued functions of tensor arguments is fundamental in many modeling problems. Tensor functions play a central role in constructing reduced-order approximations and are particularly useful for nonlinear anisotropic constitutive modeling of physical phenomena, such as fluid turbulence and material deformation among others. By imposing equivariance under the orthogonal group, tensor functions can be finitely and minimally generated by using the isomorphism between binary forms and symmetric trace-free tensors. After determining minimal generators, their coefficients can be learned as functions of the invariants of the tensor arguments by training on data which facilitates  generality of the models. The algebraic nature of the learned models makes them interpretable by revealing underlying dynamics, and it keeps the models economical as they contain the theoretically minimum required number of terms.  Determining minimal representations of higher-order tensor functions has remained computationally intractable in many cases of interest until now. The current work overcomes this limitation. Numerically efficient algorithms for generating tensor functions and reducing them to minimal sets are presented. A few classical tensor function representations and an approach to a bottleneck in modeling turbulence are worked out to showcase the practical applicability of our framework.", "tldr": "", "keywords": ["Representation Learning", "Tensor Functions", "Equivariance", "Covariant Algebra", "Invariant Theory", "Closure Modeling"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/27e84981c8d2218e647c9d5a6be46f3e1a71ccfe.pdf", "supplementary_material": "/attachment/98ae0d81817b096565dd0bfa15b125924d6a6325.zip"}, "replies": [{"content": {"summary": {"value": "The authors look to establish a framework for creating SO(3,R) equivariant models from tuples of tensors to a tensor output that is based on two separate parts. The first part comes from deriving the algebraic structure (a complete and minimal generating set of tensor monomials) using ideas from classical invariant theory. The second part uses neural networks on domain-specific data (from problems in physics) to learn certain scalars that results in a model that is consistent with the physics (equivariant to SO(3,R))."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "I would like to commend the authors on producing a strong piece of work. Particular strengths I see include:\n\n- the use of classical invariant theory to pose their problem in terms of two separate parts: the ability to build the tensor functions out of minimal generators for binary forms using an isomorphism via harmonic tensors is very nice, as well as using neural nets to learn the physics (the scalars). I think it is clever to create models that have a fundamental algebraic structure underneath them so that the overall model is grounded in solid mathematical theory, with the neural nets essentially tweaking the model to the problem at hand.\n- the creation of a novel algorithm to obtain minimal generators for Inv_G and Cov_G.\n- the demonstration of their framework on a practical problem, obtaining SOTA results."}, "weaknesses": {"value": "I also think there are some weaknesses in the paper, mostly relating to clarity of presentation, hence my score above. I think\n\n- given that the mathematical theory is \"advanced\" and may not be very well known, the presentation given here, in my view, is far too \"slick\". I strongly recommend that the authors include, in the Appendix, a full background of definitions used in Section 2 (from Definition 2.2 onwards). Examples for each definition would also be very welcome, otherwise the paper is in danger of being intractable to the uninitiated. \n- it took me a while to understand, from their introduction, what problem they were actually trying to solve - I think this should be made much clearer. One thing that might be helpful is to give the domain and codomain of each of the functions involved in (1), at worst in the Appendix. I'd even reorder the paragraphs in the Introduction to 1) state what has been done before *first* before saying 2) what their approach is to tackle the problem, otherwise it isn't obvious what the issues are, and their relevance (in relation to what has gone before). I did get there in the end, though.\n- I also think the authors need to reread the paper for typos, lack of spaces between words, full stops coming before a reference (e.g Table. 2), consistency (e.g equation vs Equation vs Equ. vs (Equ)), and also some poor grammar (e.g lines 273-282). I appreciate for the last one that the authors may not be native English speakers but I felt that all of these things detracted a bit from the overall presentation. It would be a shame if these weren't fixed in a camera ready version since the idea itself is particularly brilliant."}, "questions": {"value": "1) Does the framework only work for SO(3, R), or could it be extended to other groups beyond SO(3,R)? How general could the framework be?\n2) What is the runtime of the Algorithm 1 that is presented? It seems to me like there may be scenarios where this wouldn't work (if the invariant ring is too large), but this is an opinion I loosely hold, so I would welcome any comments from the authors on this.\n3) How does your approach compare to something like Villar et al. 2022 (Scalars are universal: Equivariant machine learning, structured like classical physics) which finds universally approximating polynomial functions to learn equivariant functions (by learning scalars)? I would suggest it might even fall under related work.\n\nFinally a minor point: in definition 2.3 shouldn't it be n choose i not n-i choose i?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xuLTb4dLal", "forum": "anvRacYUHe", "replyto": "anvRacYUHe", "signatures": ["ICLR.cc/2026/Conference/Submission20061/Reviewer_sKrU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20061/Reviewer_sKrU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761309769388, "cdate": 1761309769388, "tmdate": 1762932953442, "mdate": 1762932953442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a hybrid framework for learning equivariant tensor-valued functions, which are common in physical sciences like fluid dynamics. The core idea is to decompose the modeling problem into two parts: 1) an analytical part that uses classical invariant theory (specifically, the algebra of binary forms) to a priori determine a minimal and complete set of equivariant basis tensors (monomials) and scalar invariants; and 2) a learning part where a neural network is used to learn the coefficients for these basis tensors as functions of the invariants.The authors claim that previous attempts at this were computationally intractable for complex problems. Their key technical contribution is a set of \"numerically efficient algorithms\" (notably Algorithm 1) that can find this minimal basis, overcoming this limitation. The framework is validated in two ways: first, by correctly reproducing known classical results for simpler tensor functions, and second, by applying it to a \"well-known bottleneck\" in turbulence modeling (the RSPR tensor) and achieving results one to two orders of magnitude better than existing models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper’s main strength is its hybrid design, which combines the rigor of analytical methods (guaranteed equivariance, interpretability, and a provably minimal basis) with the flexibility of neural networks (learning complex, non-linear relationships from data).\n2. The application to the RSPR turbulence model is a strong, non-trivial test case. The proposed models achieve a one to two order-of-magnitude error reduction over established models, a highly significant improvement on a real-world problem.\n3. The authors wisely validate their framework by first reproducing known, classical results from the literature (Section 4). This is a commendable step that builds significant trust in their complex mathematical machinery.\n4. The paper claims to solve a long-standing computational bottleneck in this field. The development of efficient algorithms (like Algorithm 1) to find minimal generators is a key technical contribution that enables this framework to be applied to problems of practical interest"}, "weaknesses": {"value": "The paper claims its models are \"economical\" and that standard Equivariant Neural Networks (ENNs) are \"computationally expensive\". However, these claims are not substantiated with a direct quantitative comparison. The paper does not compare against an ENN baseline in terms of parameter count, training time, or inference cost for the RSPR problem. This makes it difficult to assess the full practical advantage of this method beyond its impressive accuracy."}, "questions": {"value": "To help substantiate the claims of an \"economical\" model, could the authors provide a quantitative comparison of their final model's complexity? Specifically, what is the parameter count of the coefficient-learning neural network for the Degree 3 RSPR model, and how does this compare to a standard ENN baseline designed for the same task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BADIe7ppgq", "forum": "anvRacYUHe", "replyto": "anvRacYUHe", "signatures": ["ICLR.cc/2026/Conference/Submission20061/Reviewer_JCzB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20061/Reviewer_JCzB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642542760, "cdate": 1761642542760, "tmdate": 1762932952798, "mdate": 1762932952798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a mathematically interpretable framework for equivariant tensor function representation.\nThe core idea is to establish a connection between harmonic tensors and binary forms (homogeneous polynomials), which enables the efficient identification of a minimal generating set for tensor function spaces using algebraic tools derived from Nakayama’s Lemma.\nThe authors further demonstrate the approach through numerical experiments, including both illustrative examples and a more practical case study on turbulence modeling."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a clear and mathematically rigorous contribution, offering an interpretable method to derive minimally complete tensor function representations under symmetry constraints.\n2. The work broadens the application scope of machine learning by bridging classical invariant theory and modern data-driven modeling, enriching cross-disciplinary understanding."}, "weaknesses": {"value": "The main weakness is readability.\nWhile the mathematical formulation is elegant, the presentation may be difficult to follow for readers without a strong background in algebraic geometry or invariant theory.\nThis is not a major flaw but may limit accessibility for the broader ML community.\nI do not have strong critical concerns—mostly clarifying questions. \n\nI am inclined to give a borderline positive score, primarily due to the novel connection established between harmonic tensors and binary forms (assuming this is the first work to do so). However, my final evaluation will depend on the authors’ responses, particularly regarding the clarity and readability of the presentation."}, "questions": {"value": "1. On Eq. (1): Could you clarify the definition of “joint invariants” \\lambda?\nSpecifically, what does the phrase “to express any other joint invariant” refer to?\nDoes it mean that all scalar invariants of multiple tensors can be expressed as functions of these \\lambda_i’s?\n2. Importance of minimal representation:\nCould you elaborate on why finding a minimal representation is crucial?\nDoes it primarily improve computational efficiency, interpretability, generalization, or reduce overfitting?\nSome concrete justification would help.\n3. Notation clarity:\n\n\t•\tLine 113: Please define GL(W) explicitly (general linear group of W?). This notation might not be immediately clear to ML readers.\n\n\t•\tLine 124: Does “K-representations” refer to representations over a field K?\n\n\t•\tLine 145: The symbol V should be bold, consistent with Eq. (6).\n\n\t•\tLine 162: The notation SL should be in italics to maintain consistency.\n\n4. Clarifications on algebraic statements (around line 196):\nIt would strengthen the paper to provide brief justification or reference for the following key claims:\n\n\t•\t“\\lambda_i’s must generate the invariant algebra…”\n\n\t•\t“If the \\lambda_i’s and B_i’s are minimal generators, then…”\n\n\t•\t“Once the model is developed, the coefficient functions C_i’s must be learned….”\n\n5. On the neural network implementation:\nCould you discuss how factors such as the number of layers, neurons, initialization, or the scale of training data influence results?\nDo different architectures affect the learned coefficient functions C_i(\\lambda)?\n\n6. On non-uniqueness:\nSince neural networks typically do not yield unique solutions due to nonconvex optimization, how does this non-uniqueness or local convergence behavior affect the learned tensor function representation?\n\n7. Minor issue:\nIn line 417, there is a missing blank space.\n\n__Disclosure of AI Assistance__\n\nPortions of this review (including wording refinement, grammar correction, and structural organization) were assisted by OpenAI’s GPT-5 language model.\nThe content, opinions, and evaluations are entirely my own, while the language model was used solely for improving clarity, coherence, and presentation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tDwhkyBJ7b", "forum": "anvRacYUHe", "replyto": "anvRacYUHe", "signatures": ["ICLR.cc/2026/Conference/Submission20061/Reviewer_o5pb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20061/Reviewer_o5pb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731574006, "cdate": 1761731574006, "tmdate": 1762932952314, "mdate": 1762932952314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a mathematically grounded framework for learning equivariant tensor function representations under the orthogonal group SO(3). The approach leverages the isomorphism between symmetric trace-free tensors (harmonic tensors) and binary forms to derive minimally complete and equivariant tensor function representations. By combining tools from invariant theory (notably Gordan’s algorithm and covariant algebra) with a learning framework for coefficient functions, the paper constructs models that are interpretable, efficient, and theoretically minimal. The method is validated through the 1) recovery of classical tensor function results for symmetric 2nd-order tensors and 2) a challenging application in turbulence modeling that is learning nonlinear models for the Rapid Pressure Strain Rate (RPSR) tensor, where it outperforms traditional and linear models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**\n\nThe paper introduces a novel synthesis of algebraic invariant theory (via binary forms and covariant modules) and modern learning frameworks to obtain minimal equivariant tensor representations.\n\nThis bridges a notable gap between symbolic invariant methods (e.g., Gordan/Hilbert theory) and equivariant neural networks (ENNs), which are data-driven but often opaque and computationally heavy.\n\n**Quality**\n\nThe mathematical treatment is sound, detailed, and references foundational results (Hilbert, Olive, Spencer, etc.).\n\nAlgorithm 1 (for minimal generator reduction using graded Nakayama’s lemma) is an elegant and efficient contribution addressing a nontrivial computational bottleneck.\n\nEmpirical validation on turbulence data is convincing, achieving significant error reductions compared to classical models (LRR and IP) and linear model.\n\n**Clarity**\n\nThe exposition of theoretical sections is rigorous and precise, though dense.\n\n**Significance**\n\nPotentially highly relevant to physics-informed machine learning, with potential applications in materials science, continuum mechanics, and turbulence modeling."}, "weaknesses": {"value": "Its originality is primarily mathematical, not algorithmic or conceptual in the machine learning sense (no new architecture, loss, or learning paradigm).\n\nComputations using Gordan’s algorithm and transvectants may explode combinatorially, no empirical evidence that this scales beyond low-order tensors.\n\nThe empirical validation is narrow and domain-specific (turbulence modeling only). There are no results across multiple datasets, tasks, or noise regimes, and no ablation studies on the learned coefficients or architecture choices. It is also lacking comparisons to modern equivariant neural networks (E(3)-equivariant GNNs, Tensor Field Networks, etc.)"}, "questions": {"value": "Can the authors benchmark their method against modern equivariant deep learning baselines on public datasets? Even a small-scale comparison (accuracy, runtime, interpretability) could help establish the framework’s competitive or complementary nature.\n\nWhat is the computational complexity of generating covariant modules for larger tensor orders?\n\nCan the framework extend beyond SO(3) to SE(3) or other transformation groups relevant in ML?\n\nHow is the “learning” part concretely implemented? How can it be integrated into modern ML pipelines? (architecture design, demonstration of end-to-end differentiability, etc.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tGTqlNOSbF", "forum": "anvRacYUHe", "replyto": "anvRacYUHe", "signatures": ["ICLR.cc/2026/Conference/Submission20061/Reviewer_YY2F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20061/Reviewer_YY2F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995115533, "cdate": 1761995115533, "tmdate": 1762932951690, "mdate": 1762932951690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}