{"id": "m1hrR1Ji4e", "number": 8530, "cdate": 1758089635979, "mdate": 1759897778283, "content": {"title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation", "abstract": "Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Lookahead Anchoring, which leverages keyframes from future timesteps ahead of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely.  We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures.", "tldr": "", "keywords": ["Audio-Driven Human Animations", "Talking Head Generation", "Long Video Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7221f05a902517cae4dfc3abbd962ec4a9edb08.pdf", "supplementary_material": "/attachment/bda1aee69c465c225c9a8f583f51022fc6feda05.zip"}, "replies": [{"content": {"summary": {"value": "This paper finds that the DiT model is limited by the quadratic complexity of the Transformer, allowing it to process only short segments of approximately 5 seconds at a time. To generate longer videos, a segment-based autoregressive generation approach is adopted, but this is prone to character identity drift. New segments rely on previously generated frames, and error accumulation causes the character's appearance to gradually deviate from the original reference image. To address this, the role of keyframes is modified in this paper: keyframes are shifted from being \"boundary constraints for the currently generated segment\" to \"directional guidance for future time steps.\" While responding to real-time audio signals, the model continuously tracks future keyframes, achieving a balance between identity consistency and motion naturalness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe thinking of sync-free keyframes is reasonable. And the “Do video DiTs understand distant frames” shows how reference frames influence video clip generation clearly.\n2.\tThe long auto-regressive generation results are generally satisfactory, and the motion intensity and identity-preserving abilities are well balanced.\n3.\tThe code and weights will be open sourced, which benefits the reproducing abilities. And it is convincing that several baselines are tested with proposed method.\n4.\tThe writing is clear and easy to understand."}, "weaknesses": {"value": "1.\tThe proposed distant keyframe conditioning method is not novel enough. Similar methods have been proposed in Section 3.3 of OmniHuman-1.5[1], which are not cited in this paper. As a core part of this work, the originality should be very clear, otherwise it will damage the novelty of this paper.\n2.\tIn the provided demos, the head motions seem to be restricted around a limited area, compared to the baseline methods. I am wondering why this happens. And will the proposed lookahead anchoring restrict the expressiveness ability of motion generation of DiT models, especially in half-body or full-body generation settings?\n\n\n[1] Jiang J, Zeng W, Zheng Z, et al. Omnihuman-1.5: Instilling an active mind in avatars via cognitive simulation[J]. arXiv preprint arXiv:2508.19209, 2025."}, "questions": {"value": "1.\tDiscuss the novelty of this paper, compared to OmniHuman-1.5.\n2.\tExplain the restricted motion shown in demos.\n3.\tDiscuss the proposed method whether could be used to full-body animation setting and the impacts of method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bAG5AxxVpk", "forum": "m1hrR1Ji4e", "replyto": "m1hrR1Ji4e", "signatures": ["ICLR.cc/2026/Conference/Submission8530/Reviewer_R6Tg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8530/Reviewer_R6Tg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761055419761, "cdate": 1761055419761, "tmdate": 1762920390989, "mdate": 1762920390989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Lookahead Anchoring approach to address the problem of identity drift in long-form, audio-driven human animation. \nInstead of forcing the model to meet specific keyframes at segment boundaries, keyframes are placed at a future time step (ahead of the current generation window), pushing the model to \"chase\" them. Such a design enables the preservation of character identity while allowing for more expressive motion dynamics. Experiments show that by using the Lookahead Anchoring strategy, the model can  maintain consistent identity over time while generating plausible audio-driven human animation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The method is demonstrated to generalize across multiple DiT-based human animation models including Hallo3, OmniAvatar, HunyuanAvatar (Sec. 4.1), which showcases its broad applicability and the potential for integration into other architectures.\n\n* The paper presents both quantitative and qualitative results showing the superiority of the proposed approach. In experiments with long video generation, Lookahead Anchoring outperforms traditional methods in terms of character consistency and overall video quality."}, "weaknesses": {"value": "* The concept of Lookahead Anchoring is not a new thing. Similar ideas have been explored in prior works like Omnihuman-1.5 (released on arXiv one month before the ICLR paper deadline), which also introduces a  Pseudo Last Frame design to anchor the given reference frame  at future timesteps ahead of the current generation window. Unfortunately, the paper does not cite or discuss these existing methods. \n\n* The results in the supplemental video (02:56-04:35) suggest that the Lookahead Anchoring strategy limits the motion range of the character. This restriction may hinder the model's ability to generate highly dynamic and expressive animations, which could be a significant drawback for certain use cases requiring more fluid motion.\n\n* Even if the Lookahead Anchoring strategy is completely novel, it is relatively simple and may not be novel enough to support a paper at ICLR, which typically expects more advanced and intricate contributions. The simplicity of the method, though effective for certain scenarios, may not meet the high standards of innovation and complexity expected at this level of the conference.\n\n* While the paper does a good job focusing on identity preservation and lip synchronization for simple scenarios, more experiments on scene dynamics, such as handling large environmental changes (e.g, view changes or moving background) are necessary. Current discussion on these cases is relatively brief."}, "questions": {"value": "See [Weaknesses]"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9oum1aFuyF", "forum": "m1hrR1Ji4e", "replyto": "m1hrR1Ji4e", "signatures": ["ICLR.cc/2026/Conference/Submission8530/Reviewer_DiP9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8530/Reviewer_DiP9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822594781, "cdate": 1761822594781, "tmdate": 1762920390638, "mdate": 1762920390638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Audio-driven human animation suffers from identity drift in long videos. Existing fixes either need extra keyframe models or restrict natural motion, failing to balance identity consistency and motion freedom.\n\nThis paper proposes Lookahead Anchoring: using future-timestamp keyframes as \"guides\" instead of current-window ones. It uses the reference image directly as the future anchor and adjusts lookahead distance to balance identity and motion.\n\nTests on three DiT-based models and datasets show it boosts identity consistency, maintains lip synchronization, and improves video quality. It also supports narrative-driven generation, serving as a practical solution for long audio-driven animations."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a new keyframe logic, which differs from traditional methods like KeyFace that rely on rigid boundary constraints or other reference-net-based designs. It converts keyframes into future-oriented guides, named self-keyframing, aiming to maintain character identity and address error accumulation.\n2. The approach designs temporal distance as a controllable parameter: smaller D values prioritize identity adherence, larger D values focus on motion expressivity.\n3. The method is integrated with three DiT-based audio-driven models (Hallo3, HunyuanVideo-Avatar, OmniAvatar) through a fine-tuning strategy. This integration is meant to show that the method can be applied to multiple architectures, not just a custom model framework.\n4. The work explores narrative-driven long video generation by combining text-based image editing models to create story-specific keyframes, thereby enhancing the solution’s extensibility to meet varied scenario-based requirements."}, "weaknesses": {"value": "1. The method mentioned in Section 3.3 of the OmniHuman1.5[1] is almost identical to this work, so I have some doubts about the innovativeness—nevertheless, this work features more detailed experiments compared to that paper.\n2. The method's visualizations do demonstrate its capability in generating long-duration videos, yet it lacks performance in high-dynamic scenarios: character dynamics remain relatively monotonous, with limited upper-body and hand movements.\n3. It would be good to visualize the ablation study for the distant keyframe conditioning.\n\n[1] Jianwen Jiang, Weihong Zeng, Zerong Zheng and et.al. OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation"}, "questions": {"value": "1. The first part of the qualitative comparison section in the supplementary materials (featuring a woman with wavy curly hair in a blue outfit), the dynamic effect of the result with lookahead anchoring is weaker than that of the baseline without it. This raises the question of whether this method might reduce dynamic performance？\n2. The paper mentions that the strategy of directly using anchors without training will produce artifacts. If you directly discard the final latent with artifacts in longer video generation tasks, can this serve as a training-free method?\n3. Three different baselines are used in this paper, which all perform well in fixed scenarios. However, have you tried using models with camera movement capabilities to verify the effectiveness of this method? And could the anchor frame possibly restrict the range of camera movement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CdtSxQWbRc", "forum": "m1hrR1Ji4e", "replyto": "m1hrR1Ji4e", "signatures": ["ICLR.cc/2026/Conference/Submission8530/Reviewer_aq1t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8530/Reviewer_aq1t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896239993, "cdate": 1761896239993, "tmdate": 1762920390229, "mdate": 1762920390229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Lookahead Anchoring (LA), a method designed to preserve character identity in audio-driven human animation. Existing methods rely on keyframe generation producing intermediate frames  and identify specific feature inject to prevent identity drift. But these explicit keyframes can overly constrain the motion dynamics, limiting natural expressivity.\n\nTo overcome this, LA introduces future keyframe conditioning: rather than generating anchors within the current sequence, the model leverages future latent keyframes as soft temporal guidance. \n\nKey observations include:\n\t•\tThe temporal lookahead distance directly balances expressivity and consistency; larger distances produce more dynamic motion, smaller ones yield stronger identity adherence.\n\t•\tLA integrates seamlessly into existing DiT-based architectures and improves both identity stability and lip synchronization across long sequences."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Conceptual innovation: The idea of using future latent frames as temporal anchors instead of rigidly generated keyframes is elegant and conceptually clear. It shifts the paradigm from hard constraints to soft temporal guidance.\n2. Interpretability: The empirical finding that lookahead distance controls a trade-off between motion expressivity and identity consistency is intuitive and well-supported (Fig. 6).\n3. Model-agnostic integration: LA can be attached to existing transformer or diffusion-based animation models with minimal architectural change.\n4. Quantitative gains: Across HDTF and AVSpeech datasets, LA consistently improves lip synchronization (Sync-D ↓, Sync-C ↑), face/subject consistency, and perceptual quality (FID ↓, FVD ↓), without harming motion smoothness.\n5. Perceptual preference: User studies show strong preference for LA-enhanced videos in terms of synchronization and identity stability.\n6. Practical significance: The approach removes the need for an explicit keyframe generation stage, simplifying pipelines for identity-preserving video generation."}, "weaknesses": {"value": "* Missing justification for “bounded generation” argument\nThe introduction claims that “bounded” keyframe-based methods are limited by the quality and expressiveness of their generated keyframes. While this is plausible, the paper does not provide quantitative or visual evidence demonstrating this limitation.\n\n* Ambiguity in “self-keyframing” explanation\nThe statement that “the keyframe no longer needs to match the exact lip movements and expressions required by the audio … enabling self-keyframing” is conceptually interesting but under-explained. It’s unclear how a distant or reference-based anchor can substitute for synchronised keyframes in guiding expression or pose accuracy.\n\n* Lack of comparison with KeyFace (Bigata et al., 2025)\nKeyFace is currently a strong state-of-the-art method for identity-preserving talking heads, explicitly designed to address identity degradation. Its absence from the qualitative comparisons leaves a significant gap. A side-by-side video comparison would substantially strengthen the evaluation.\n\n*  Limited clarity on latent-space interpretation\nAs I understand it, each latent token represents a spatiotemporally compressed patch, not a full frame. Appending the lookahead latent z_{n-1+d} therefore adds only one additional patch-level token, not a holistic future-frame reference. It is unclear how this single patch provides global temporal guidance or identity stabilization across the entire sequence.\n\n* Inference-time mechanism under-specified\nDuring inference, when only one keyframe or reference image is available, it remains unclear how the lookahead mechanism functions in practice. Does the model still benefit from a meaningful anchor signal? If not, this could contradict the paper’s claim that LA allows “sync-free keyframes” capable of matching poses and expressions."}, "questions": {"value": "1.\tCould you clarify what “bounded generation” refers to in practice, and show evidence that conventional keyframe methods limit expressiveness or quality?\n2.\tHow exactly does “self-keyframing” function; does the model reuse its own generated frames, or the original reference frame, as recursive anchors?\n3.\tWhy was KeyFace excluded from qualitative comparisons, given its strong relevance to identity preservation?\n4.\tGiven that each latent token represents a spatiotemporal patch, how can a single appended future latent provide meaningful global identity anchoring?\n5.\tDuring inference with only one keyframe/reference image, how is the lookahead mechanism applied, and does it still contribute to identity consistency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W10OiIwodo", "forum": "m1hrR1Ji4e", "replyto": "m1hrR1Ji4e", "signatures": ["ICLR.cc/2026/Conference/Submission8530/Reviewer_fgaf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8530/Reviewer_fgaf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944582533, "cdate": 1761944582533, "tmdate": 1762920389747, "mdate": 1762920389747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}