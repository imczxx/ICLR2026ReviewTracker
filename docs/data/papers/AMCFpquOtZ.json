{"id": "AMCFpquOtZ", "number": 1307, "cdate": 1756869674076, "mdate": 1759898216511, "content": {"title": "Certifying the Full YOLO Pipeline: A Probabilistic Verification Approach", "abstract": "Object detection systems are essential in safety-critical applications, but they are vulnerable to object disappearance (OD) threat, in which valid objects become undetected under small input perturbations, creating serious risks. This paper addresses the problem of verifying the robustness of YOLO networks against OD by proposing a three-step probabilistic verification framework: (1) estimating output ranges under a distribution of input perturbations, (2) formally verifying the Non-Maximum Suppression (NMS) process within these ranges, and (3) iteratively refining the results to reduce over-approximation. The framework scales to practical YOLO models. Both theoretical analysis and experimental results demonstrate that our method achieves comparable probabilistic guarantees and provides tighter Intersection-over-Union (IoU) lower bounds while requiring significantly fewer samples than existing methods.", "tldr": "This paper presents a probabilistic framework to verify the entire YOLO pipeline against perturbations, uniquely incorporating a formal analysis of  previously under-explored Non-Maximum Suppression (NMS) post-processing stage.", "keywords": ["Probabilistic Verification", "Formal Verification", "Object Detection", "Safety Guaranteen"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c902a92a9617ed6c3eef4a3073d3fe85cc5ac76a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a three-step probabilistic verification framework to assess the robustness of YOLO object detection systems against object disappearance attacks. By estimating output ranges, verifying NMS, and refining results, the method provides strong probabilistic guarantees and tighter IoU lower bounds with fewer samples, scaling effectively to practical models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors propose the first scalable probabilistic verification framework for YOLO models, effectively addressing the object disappearance threat in safety-critical scenarios.\n2. Combines solid theoretical analysis with comprehensive experiments, showing tighter IoU lower bounds and improved robustness.\n3. Achieves strong probabilistic guarantees with significantly fewer samples compared to existing methods."}, "weaknesses": {"value": "1. The method is very focused on object disappearance, which is an important threat, but also a pretty narrow one. It’s not clear how well the framework would generalize to other attack scenarios or tasks.\n\n2. The experimental comparison feels a bit limited. Most of the analysis is against one baseline (RCPN), and a broader set of baselines would make the claims more convincing.\n\n3. There’s little discussion on actual computational cost. While the paper shows faster verification times, it doesn’t really explain how the method would scale in a real deployment or resource-constrained setting."}, "questions": {"value": "1. How does the method generalize under different perturbation distributions?\n2. Could this verification framework be integrated with robustness training methods to further improve security?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xThfxE4dlr", "forum": "AMCFpquOtZ", "replyto": "AMCFpquOtZ", "signatures": ["ICLR.cc/2026/Conference/Submission1307/Reviewer_ji71"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1307/Reviewer_ji71"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761303023504, "cdate": 1761303023504, "tmdate": 1762915731208, "mdate": 1762915731208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a PAC-based local verification procedure for YOLO networks---object detection networks that produce annotation boxes---specially for object disappearance (OD) threats (see, e.g., [Eykholt et al., 2018](https://www.usenix.org/conference/woot18/presentation/eykholt)).\nThese networks have very high-dimensional input and especially output spaces and use non-trivial post-processing, which makes verification with off-the-shelf methods difficult.\n\nThe approach combines multiple sampling-based probabilistic techniques to solve the task in multiple steps.\n1. Approximate (from a sample) a hyper-rectangle as a high probability output \"bounding box\" for the relevant region in the input space.\n2. (Repeatedly):\n    - Detect unsafe output points $\\mathbf y$ with Quadratic programming, if none found, return **safe**.\n    - Attempt (with random sampling) to find inputs $\\mathbf x$ such that $\\lVert F(\\mathbf x) - \\mathbf y\\rVert$ is minimal\n    - If no close enough input found, refine output space, else return unsafe\nThe resulting guarantees are relaxed depending on multiple probabilistic hyperparameters, that dictate various sample sizes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **(S1) Significance and Motivation**\n    - The problem is important for gauging the robustness of YOLO networks, and existing methods cannot be trivially adapted to this domain.\n    - The Introduction and Related Work motivate the work well and give a nice overview of the neural net verification landscape, although some additional sources for more scalable PAC approaches can be discussed (e.g., [Baluta et.al., 2021](https://ieeexplore.ieee.org/abstract/document/9402111), [Blohm et al., 2025](https://openreview.net/forum?id=UKHlXpiFMy)).\n\n- **(S2) Theoretical Contribution**\n    - The obtained sample complexities are realistic, and proofs of the main results seem correct after surface-level checks.\n    - Neural network verification with PAC methods has been widely explored, yet the unique output modality and post-processing seem to require more involved methods for verification.\n    - The authors leverage multiple probabilistic techniques in tandem, leading to low sample complexity for the overall procedure; samples appear quasi-independent of the network dimensionality.\n    - The general idea of integrating PAC methods with counterexample-guided refinement in verification is interesting and implemented in a novel way.\n\n- **(S3) Empirical Results**\n    - The procedure seems able to deal with model sizes common in related literature and issues robustness certificates that are close to the results of adversarial methods.\n\nWhile I have several questions regarding the details of the procedure, I think they can be addressed, and their inclusion in the manuscript would strengthen the contribution.\n\n I would be happy to see a refined version of this submission accepted."}, "weaknesses": {"value": "The main weakness of this manuscript is the complicated presentation of an already complex procedure.\nImportant details of the probabilistic procedure are difficult to gauge from the manuscript. \n\nMany theoretical issues may be easily clarified or adapted by the authors and do not greatly impact the theoretical contribution. \nHowever, a clear statement of the limitations and failure modes is necessary for a probabilistic procedure like this.\n\n- **(W1) Readability and Notation**: The notation is very dense, which is partially difficult to avoid in this output modality. \nWhile not the main concern of the manuscript, the readability of the manuscript would improve a lot if:\n  - Abbreviations were introduced more consistently (e.g., YOLO, RCP, PGD, ...).\n  - Citations were wrapped in parentheses when they are not part of a sentence (i.e., use `\\citep{}`-style formatting if you use LaTeX/natbib).\n  - Tables avoided heavy rules and used a cleaner booktabs style, as is typical for ML conferences like ICLR.\n\n- **(W2) Opaque Sample Complexity**: The procedure combines sampling-based approaches in a “nested” way, resulting in four hyperparameters $\\alpha,\\beta,\\delta,\\epsilon$, as well as empirically chosen sample sizes for parts of the procedure. \nThis makes it difficult to gauge how precisely the sample complexity and runtime scale with each parameter, and what trade-offs different choices bring to the overall procedure. \n\n  The fact that the likelihoods of failure events depend on multiple parameters amplifies this, as multiple choices of different parameters can lead to the same sample sizes.\n\n  **Actionable request:** Can you provide a mapping of each parameter to its failure event, sample complexity (big-O), or its impact on the overall runtime?\n  This might not be necessary to include in the main text, but it potentially gives a nice overview of the procedure.\n\n- **(W3) Partially Empirical Network Output Approximation**: The output domain of the network is estimated by scaling $\\mathbf v_{\\max}$, which, in my understanding, is essentially an empirically chosen proposal vector.\n  The scaling coefficient is chosen with a probabilistic guarantee with $O(\\frac{1}{\\alpha} (\\ln \\frac{1}{\\beta}+\\ln\\frac{1}{\\alpha}))$ samples. \n  While this scaling coefficient is chosen optimally, the proposal vector seems to be chosen based on a heuristic, with the statement in Proposition 3 in Appendix D being opaque.\n\n  There is a lot of established theory on obtaining bounding rectangles (see alternatives below). \n  These methods do not give the dimension-independent sample complexities that are presented here.\n  However, the presented method consequently relies on a scaling trick and might overapproximate the tightest bounding rectangle if the proposal vector was not chosen well.\n\n  The potential looseness of the initial output-space approximation is amplified by eliminating $L\\_2$ balls from counterexamples, instead of using $L_\\infty$.\n   Over-approximations in a single dimension may require many refinement steps to eliminate, especially in very high-dimensional spaces.\n  **Actionable request:** see **(Q1)**\n\n - **(W4) $L_2$ Counterexample Refinement and MIQP Runtime in Higher Dimensions**: Refinement steps of the output space are performed with a Mixed Quadratic Integer Program. \n  While this idea is novel and interesting, it is unclear how effective the refinement steps are, and whether the found counterexamples, in general, significantly shrink the output domain to be searched.\nA discussion/investigation of this seems important, as each refinement step brings the cost of increased uncertainty.\n- **(W5) Unclear Empirical Results in Table 2 and Figure 5**:\n\n  - It is unclear what results precisely are presented in Table 2. The Baseline Selection and Appendix N state that 37,000 samples are used to certify the proposed method, with the baseline $\\mathrm{RCP}_N$ on $10^6$ samples. However, the caption of Table 2, as well as *Safety Guarantee*, states $10^6$ uniform perturbations. In either scenario, the results are presented in a slightly confusing manner: either the runtime is compared to a method using significantly more samples (clarify in Table 1), or Table 2 shows results for significantly larger sample complexity (and thus tighter parameters).  \n  - The caption states $\\epsilon = 1/255$, yet $\\epsilon$ is varied in the table.  \n  - The false negative rate appears high (up to 15%). It is not stated on how many robust/non-robust boxes these results were computed; standard deviations are also missing.  \n  - Figure 5’s x-axis is not labelled; it is not stated which model/experiment produced this data. Code does not appear to be available, so details cannot be checked."}, "questions": {"value": "I would appreciate it if the authors briefly addressed my concerns in **$W2$** as well as answered my questions below to address the remaining weaknesses.\n\n - **(Q1) Initial Approximation of Output Domain**:\nMany methods exist to give an \\(\\alpha,\\beta\\) guarantee on estimating a hyper-rectangle. How does your Part 1 method compare in terms of the tightness of the obtained rectangle? How much of an issue do over-approximations of the output domain present in practice?\n\n   There is established theory for estimating non-parametric confidence regions from samples, e.g., the **DKW inequality** ([Dvoretzky–Kiefer–Wolfowitz, 1956](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-27/issue-3/Asymptotic-Minimax-Character-of-the-Sample-Distribution-Function-and-of/10.1214/aoms/1177728174.full); [Massart, 1990](https://projecteuclid.org/journals/annals-of-probability/volume-18/issue-3/The-Tight-Constant-in-the-Dvoretzky-Kiefer-Wolfowitz-Inequality/10.1214/aop/1176990746.full); for multivariate exact constants, see [Naaman, 2021](https://www.sciencedirect.com/science/article/pii/S016771522100050X)). \n   Similarly, one can learn the smallest hyper-rectangle with empirical risk minimization (ERM).\n\n   Alternatively, there exist dimension-dependent bounds with **$\\epsilon$-nets**—see [Haussler & Welzl, 1987](https://link.springer.com/article/10.1007/BF02187876) and the standard reference [Mitzenmacher & Upfal, 2017](https://www.cambridge.org/core/books/probability-and-computing/3A5B47DB315FC64B9256C5C8131C5EFA). One could consider the class of *inverse* (half-)hyperrectangles with VC-dimension $d$. After a sample of size $\\tilde{O}(\\frac{d}{\\epsilon})$, one could use $\\mathbf v_{\\max}$ directly as a bound, without the need to scale by a constant. \n   These alternatives trade dimension-independence for transparency; a short discussion comparing your scaling heuristic to empirical rectangles/$\\epsilon$-nets would help position your choice.\n\n- **(Q2) Counterexample Refinement with MIQP**: From reading the manuscript, it is not obvious that refinement steps significantly reduce the volume of the output domain (i.e., that the produced $\\mathbf y$ will be far from the actual codomain of $F$). Consequently, refinement might weaken the probabilistic guarantees in Theorem 2 without real advantage. Why eliminate an $L_2$ neighborhood of each counterexample rather than, for example, an $L_\\infty$ hypercube (or hyper-rectangles)? \nWouldn’t that allow eliminating much larger volumes in high dimensions, especially for images?\n\n   It would be very useful to investigate how much each successive refinement step not only increases the bounds but actually decreases the size of the output domain, as well as how much it relaxes the resulting bounds. A discussion of when further refinement is “not worth it” in terms of cost in confidence would be valuable.\n\n- **(Q3) Counterexample Validation and Refinement**: The theoretical idea of Theorem 1 is opaque from the main text. A brief mention of the probabilistic idea behind the “more conservative estimates” in §5.3 (with a citation or name of the invoked bound/inequality) would help communicate the approach. \n  The proof of Theorem 1 in the appendix mentions Hoeffding’s inequality; explaining its role in one sentence in the main text would help.\n\n- **(Q4) Motivation versus PGD**: If I understand correctly, Figure 5 shows that PGD often finds tighter lower bounds than probabilistic verification. \n   Is this an issue for the motivation of the procedure? \n\n   With an attack that is presumably cheaper than the proposed method, one can seemingly get tighter bounds. \n   What advantage does the proposed method offer over performing a PGD attack over an \\(\\tilde{O}(1/\\epsilon)\\) sample and reporting the tightest counterexample as a bound (cf. [Blohm et al., 2025](https://openreview.net/forum?id=UKHlXpiFMy))?\n\n   In general, can adversarial methods be integrated into the approach instead of relying on uniform random samples? If such integration is out of scope, a motivation for using a probabilistic procedure when a cheap attack can immediately provide a counterexample would help.\n\n- **(Q5)Clarification of Experimental Setup**:\n  - What is the precise sample complexity of the results in Table 2---37,000 or $10^6$?\n  - What is the precise certificate that the procedure issues for the instances, in terms of the probabilistic bounds and their interpretation?\n  - Which column in the table reflects actual robustness behaviour?\n  - Approximately, what is the certificate (confidence) provided by $\\mathrm{RCP}_N$ at the used sample complexity?\n\n---\n\n## Minor Recommendations\n\n**Self-Contained Theorems** In §5.3–5.4, including Theorems 1–2, some notation is not reintroduced (e.g., $A', B', C$). In the theorems, restate the meanings of $N, M, M_2$ for self-containment. In Theorem 2, “the algorithms defined above” should be referenced specifically (reffing Algorithm 1 might suffice if others are subroutines).\n\n**Naming of Subroutines.** The manuscript would be easier to follow if “Part 1/2/3” were replaced by names (e.g., *Output-Box Estimation*, *Unsafe-$\\mathbf y$ Search*, *Counterexample Validation/Refinement*). The algorithm captions would read more cleanly without the repeated “Part Y” phrasing.\n\n**Typos.** One of the OpenReview keywords reads “guaranteen.” In Algorithm 4, use $\\mathbb{Z}^+$ rather than $Z^+$.\n\n---\n\n## References\n\n- Blohm, P.; Indri, P.; Gärtner, T.; Malhotra, S. (2025). *Probably Approximately Global Robustness Certification.* ICML 2025. OpenReview: <https://openreview.net/forum?id=UKHlXpiFMy>  \n- Baluta, T.; Chua, Z. L.; Meel, K. S.; Saxena, P. (2021). “Scalable Quantitative Verification for Deep Neural Networks.” Proceedings of the 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), 312–323. <https://doi.org/10.1109/ICSE43902.2021.00039>  \n- Dvoretzky, A.; Kiefer, J.; Wolfowitz, J. (1956). “Asymptotic Minimax Character of the Sample Distribution Function and of the Classical Multinomial Estimator.” *Annals of Mathematical Statistics*, 27(3), 642–669. <https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-27/issue-3/Asymptotic-Minimax-Character-of-the-Sample-Distribution-Function-and-of/10.1214/aoms/1177728174.full>  \n- Massart, P. (1990). “The Tight Constant in the Dvoretzky–Kiefer–Wolfowitz Inequality.” *Annals of Probability*, 18(3), 1269–1283. <https://projecteuclid.org/journals/annals-of-probability/volume-18/issue-3/The-Tight-Constant-in-the-Dvoretzky-Kiefer-Wolfowitz-Inequality/10.1214/aop/1176990746.full>  \n- Naaman, M. (2021). “On the Tight Constant in the Multivariate Dvoretzky–Kiefer–Wolfowitz Inequality.” *Statistics & Probability Letters*, 173, 109088. <https://www.sciencedirect.com/science/article/pii/S016771522100050X>  \n- Haussler, D.; Welzl, E. (1987). “\\(\\varepsilon\\)-nets and Simplex Range Queries.” *Discrete & Computational Geometry*, 2, 127–151. <https://link.springer.com/article/10.1007/BF02187876>  \n- Mitzenmacher, M.; Upfal, E. (2017). *Probability and Computing: Randomization and Probabilistic Techniques in Algorithms and Data Analysis* (2nd ed.). Cambridge University Press. <https://www.cambridge.org/core/books/probability-and-computing/3A5B47DB315FC64B9256C5C8131C5EFA>"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u5BBTfTzV4", "forum": "AMCFpquOtZ", "replyto": "AMCFpquOtZ", "signatures": ["ICLR.cc/2026/Conference/Submission1307/Reviewer_YCar"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1307/Reviewer_YCar"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694441845, "cdate": 1761694441845, "tmdate": 1762915731074, "mdate": 1762915731074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a probabilistic verification framework (ODPV) to certify YOLO object detectors against the Object Disappearance (OD) problem. The framework consists of three modules: output range approximation, NMS verification, and probabilistic refinement, providing experiments on multiple YOLO variants showing strong robustness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe work presents a scalable verification framework for YOLO object detectors, incorporating the non-differentiable NMS post-processing into the certification process.\n2.\tThe method requires far fewer samples to achieve strong probabilistic guarantees, enabling the verification of large-scale models.\n3.\tThe experimental evaluation is comprehensive, covering diverse YOLO object detectors and testing robustness under various configurations. Results demonstrate the effectiveness and superiority of ODPV compared to existing probabilistic baselines, establishing a solid empirical foundation for detection verification research."}, "weaknesses": {"value": "1.\tThe PAC guarantees rely on a uniform sampling distribution over the perturbation set. However, the paper does not explore how this assumption might affect the robustness of the guarantees in more realistic scenarios with non-uniform perturbations.\n2.\tThis framework is developed and evaluated only on the YOLO family of detectors, and its robustness certification is limited to the OD threat. The generality of the approach to other detection architectures or to other robustness concerns remains unverified."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XHAVlJZ3Dv", "forum": "AMCFpquOtZ", "replyto": "AMCFpquOtZ", "signatures": ["ICLR.cc/2026/Conference/Submission1307/Reviewer_HJ5S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1307/Reviewer_HJ5S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734288628, "cdate": 1761734288628, "tmdate": 1762915730899, "mdate": 1762915730899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a probabilistic method to verify the full YOLO pipeline, from a specified input set C (a hypersphere of images around a nominal image) through non-maximum suppression (NMS). The goal is to check that, for all x in C, the post-NMS output y does not exhibit object disappearance: there is no perturbation such that,   after NMS on x, the best box with the correct class has the IoU below a fixed detection threshold.\n\nThe procedure has three parts. First, over-approximate the detector outputs F(C) by building Z. In practice, Z is an axis-aligned hyperrectangle estimated from samples x ~ C, with a PAC guarantee: with probability at least 1 - beta over the construction, a random x ~ C satisfies F(x) in Z with probability at least 1 - alpha. Second,  verify NMS over all y in Z. This is framed via a safe set Q that identifies boxes which, across Z, both meet the IoU and class requirements and cannot be suppressed into failure by NMS; if Q is nonempty, disappearance cannot occur. Third, if a candidate y in Z appears to violate safety, refine Z by trimming unreachable regions until either a real counterexample is confirmed or the candidate is shown unreachable. Then they provide the end-to-end probabilistic guarantee: for the chosen perturbation set C, object disappearance does not occur with the specified confidence and coverage parameters."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses the difficult and relevant problem of assuring end-to-end robustness of YOLO under perception noise, including the NMS stage. This is especially relevant to safety-critical systems that employ these black box detectors at runtime.\n\n- The formalism encodes object disappearance but is general enough to express other anomaly types (e.g., misclassification, spurious appearances, duplicate suppression)\n\n- Synthesizing Z only requires the ability to draw samples from C and does not assume a parametric form for the perturbations\n\n- The PAC-style guarantees are nice because they provide calibrated confidence and coverage claims as opposed to simple binary claims, so the guarantees are generally more interpretable and can help inform upstream design decisions.\n\n- The paper states definitions precisely, proves lemmas and propositions (including the soundness of the NMS safe set argument), and relates the algorithms to the formal guarantees they provide."}, "weaknesses": {"value": "- The method certifies robustness only within a small epsilon-ball around a single image, and doesn't necessarily reflect practical YOLO deployments (e.g., traffic monitoring) where scenes change continuously and unpredictably across frames.\n\n- If ground truth is already available for the target image, the value of verifying that the detector recovers it is questionable; this makes the result feel more like a labeled-scene sanity check than a deployment-relevant guarantee.\n\n- The safety specification is narrow and ignores other important failure modes under perturbations, such as false appearances (spurious detections), class misidentification, or other anomalies."}, "questions": {"value": "1. For a deployment like traffic monitoring, where the scene evolves continuously (new vehicles appear, others leave), how tractable is it to verify formal robustness guarantees over a short temporal horizon (multiple frames)?\n\n2. What shape would C take on to make such guarantees meaningful? For instance, naively stretching the hypersphere C would begin to include semantically broken images that always invalidate safety.\n\n3. Is it possible for verification to work with weaker supervision? E.g., rather than having complete ground truth bounding boxes, you have some sort of a priori map over time."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HMh84diuuI", "forum": "AMCFpquOtZ", "replyto": "AMCFpquOtZ", "signatures": ["ICLR.cc/2026/Conference/Submission1307/Reviewer_UDS2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1307/Reviewer_UDS2"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927726852, "cdate": 1761927726852, "tmdate": 1762915730633, "mdate": 1762915730633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new method for PAC-based verification of the YOLO object detection network. Importantly, the method accounts for the Non-Maximum Suppression (NMS) post-processing step that is often used in practice. The first contribution is formalizing the verification problem. The second contribution is a certification pipeline based on the sample-based scenario approach [Campi, 2009]. Results show certification results that are faster than baselines and not too conservative."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an ambitious problem, as YOLO is a large object detection model.\n- The method accounts for the NMS post-processing stage, which appears to be novel and practically useful.\n- By using a PAC-based sample-based analysis, the proposed verification method is less conservative and more practical than deterministic formal verification techniques (at the expense of weaker guarantees).\n- The method is substantiated with a theoretical analysis.\n- Results show that the error bounds are not too conservative."}, "weaknesses": {"value": "- The sample complexity derived for the $RCP_N$ method (Appendix C.1) is incorrect: It should be computed with $d=1$ and not with $d_0=640 \\times 640 \\times 3$ (Appendix N), so $RCP_N$ likely requires fewer than 560'000'000 samples. The dimension $d$ corresponds to the optimization variable dimension, which is scalar for $RCP_N$, see (4). This error affects the sample efficiency and speedup claims.\n\n- The appendix and proofs of the theoretical results are long, yet they are sometimes not polished, unclear, and have typos. Given the emphasis on theoretical results, this is a serious limitation. In particular:\n1) Section D would greatly benefit from clearer exposition, e.g. \"Then main result\" (line 788). Also, \"by algorithm should not far beyond\" (line 789) does not specify what algorithm is considered and misses a verb.\n2) Section G (proof of lemma 2) is unclear and not rigorous: The proof starts with \"There are ...\", but how it leads to the conclusion is unclear, the sentences on lines 903-905 and 909-910 are unclear and miss verbs and nouns.\n3) In Section J.1., the proof relies on the sets $\\mathcal{Q}_k$ and $\\mathcal{T}$ whose definitions are unclear (the $\\mathcal{Q}_k$ are only subsets of $2^{\\mathcal{C}}$, and the definition of $\\mathcal{T}$ is not rigorously written), and on an independence assumption of the events $\\mathcal{T}\\in\\mathcal{Q}_k$ that is unclear.\n\n- In Definition 1, $P_{x\\sim\\mathcal{C}}$ is unclear. The probability distribution $P$ is undefined. Also, $\\mathcal{C}$ is a set, not a distribution, so $x\\sim\\mathcal{C}$ is unclear. This notation should be clarified before Section 5."}, "questions": {"value": "- Please clarify the sample complexity of the $RCP_N$ method.\n\n- Please revise the appendix and its proofs that are sometimes unclear or suffer from poor grammar."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T64KFDdifI", "forum": "AMCFpquOtZ", "replyto": "AMCFpquOtZ", "signatures": ["ICLR.cc/2026/Conference/Submission1307/Reviewer_Es96"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1307/Reviewer_Es96"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission1307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945342903, "cdate": 1761945342903, "tmdate": 1762915730110, "mdate": 1762915730110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}