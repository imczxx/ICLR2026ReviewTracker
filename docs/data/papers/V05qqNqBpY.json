{"id": "V05qqNqBpY", "number": 11001, "cdate": 1758186544418, "mdate": 1763214293988, "content": {"title": "Finite-Time Analysis of Actor-Critic Methods with Deep Neural Network Approximation", "abstract": "Actor–critic (AC) algorithms underpin many of today’s most successful reinforcement learning (RL) applications, yet their finite-time convergence in realistic settings remains largely underexplored. Existing analyses often rely on oversimplified formulations and are largely confined to linear function approximation. In practice, however, nonlinear approximations with deep neural networks dominate AC implementations, leaving a substantial gap between theory and practice. In this work, we provide the first finite-time analysis of single-timescale AC with deep neural network approximation in continuous state-action spaces. In particular, we consider the challenging time-average reward setting, where one needs to simultaneously control three highly-coupled error terms including the reward error, the critic error, and the actor error. Our novel analysis is able to establish convergence to a stationary point at a rate $\\widetilde{\\mathcal{O}}(T^{-1/2})$, where $T$ denotes the total number of iterations, thereby providing theoretical grounding for widely used deep AC methods. We substantiate these theoretical guarantees with experiments that confirm the proven convergence rate and further demonstrate strong performance on MuJoCo benchmarks.", "tldr": "This paper provides the first finite-time analysis of single-timescale AC with deep neural network approximation in continuous state–action spaces -- a setting ubiquitous in modern RL.", "keywords": ["finite-time analysis", "actor-critic", "deep neural network"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3fd4c44e10f96f08f1ff52f8b79eecd6ee8dd337.pdf", "supplementary_material": "/attachment/b678058f654629bb4288aba4bba4b315f74e042d.zip"}, "replies": [{"content": {"summary": {"value": "This paper established a finite-time convergence result for a proposed DNN-based actor-critic reinforcement learning algorithm.\n\nSpecifically, the authors considered a challenging RL setting with 1) continuous state and action spaces, 2) Markovian samplings, and 3) average reward model. To address this problem, they developed a *single-timescale DNN-based* actor-critic algorithm, and, under some assumptions, proved a convergence rate of $\\tilde{\\mathcal{O}}(T^{-0.5})$. Finally, this paper presented some numerical results to corroborate their theoretical findings."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**The general RL setting is impressive.** As most works on RL theory focus on tackling the discount reward model and discrete (even if can be infinite) state and action spaces, this paper extends the results to a more general setup.\n\n**The theoretical analysis is concrete.** Even though some of their techniques and analytical methods are similar to those found in existing research, the authors demonstrate commendable effort in handling a more practical and challenging scenario. I appreciate this theory-driven work, particularly in an era where empirical performance usually overshadows theoretical significance.\n\nIn addition, the paper is well-organized and the writing is pretty good."}, "weaknesses": {"value": "My main concern is: **The contributions and insights are not highlighted.** For example, the authors claim that the \"single-timescale\" design is a primary contribution, yet their analysis lacks a discussion of the associated challenges and the techniques employed to resolve them. Likewise, for continuous spaces, the difficulties introduced by this setup and the authors' solutions remain unexplained.\n\nWhile these points could represent the main contributions, the authors merely enumerate their findings without offering in-depth discussion or critical analysis.\n\nBesides, I find some claims could be incorrect or inaccurate. (See Questions 2,4.)"}, "questions": {"value": "**Please try to answer the following questions:**\n\n1. While the single-timescale method offers practical advantages, its convergence rate is inferior to some two-timescale algorithms [1,2]. This raises a question of potential tradeoff: could the noisier observations inherent in strongly coupled actor-critic methods contribute to this performance disparity?\n\n2. I disagree with the claim regarding $m$-dependence in Lines 70~78. Numerous studies, including [3-4], present $m$-dependent (and depth-dependent) convergence results to emphasize the influence of DNNs. Actually, this paper's findings also connect to the width $m$, though it is implicitly contained within $\\epsilon_{app}$. (To some extent, the authors are encouraged to better characterize the $\\epsilon_{app}$ with the parameters of the DNNs.) Thus, I think the $m$-dependence is supposed to be a strength rather than a drawback.\n\n3. Can the authors provide the key differences in the analysis among MLP, CNN and ResNet? It could have been interesting, but it is regrettable that the authors do not elaborate on it in sufficient detail.\n\n4. The authors state that a \"stationary policy\" is optimal due to non-convexity. However, given that numerous studies demonstrate global convergence results [1,2,5,6], should this also be acknowledged by the authors?\n\n5. The numerical experiments are kind of limited in two ways: 1) No other baselines are included; 2) The impact of depth and width remains unclear.\n\n[1] Closing the gap: Achieving global convergence (last iterate) of actor-critic under markovian sampling with neural network parametrization. ICML’24.\n\n[2] Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning. ICML’25.\n\n[3] Neural Temporal-Difference Learning Converges to Global Optima. NeurIPS’19.\n\n[4] Convergence of Actor-Critic Methods with Multi-Layer Neural Networks. NeurIPS’23.\n\n[5] Sample and Communication-Efficient Decentralized Actor-Critic Algorithms. ICML’22.\n\n[6] Improving sample complexity bounds for (natural) actor-critic algorithms. NeurIPS’20."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "l0YX0Hu4qs", "forum": "V05qqNqBpY", "replyto": "V05qqNqBpY", "signatures": ["ICLR.cc/2026/Conference/Submission11001/Reviewer_qM9u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11001/Reviewer_qM9u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760464946573, "cdate": 1760464946573, "tmdate": 1762922185606, "mdate": 1762922185606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an actor–critic algorithm with finite-time analysis under a neural network function approximation setting. Compared with previous studies, the paper establishes a sample convergence rate for environments with continuous action spaces. In addition, the authors present simulation results to demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The study derives theoretical guarantees by establishing the sample convergence rate of the Actor-Critic algorithm for MDP with continuous action spaces. \n\nIn addition, the authors include simulation experiments that demonstrate the empirical validity of the theoretical results."}, "weaknesses": {"value": "1. For Table 1, I am confused whether the comparison is fair, especially for the (Tian et.al. 2024). \n(a) Firstly, the sampling process is Markovian, both in Actor and Critic part. The restart setting is just to achieve different distribution for policy gradient under the discount finite horizon setting.  Please carefully check this part and correct the table.\n(b) Besides, the width of the Neural Network is for the $\\epsilon$-order approximation error of the value function. Your work choose to avoid this width but will lead to approximation error.   Finally, previous works with neural approximation will converge to $ \\mathcal{O}(\\epsilon)$ accurate set but your work will converge to $ \\mathcal{O}(\\epsilon+\\epsilon_{approx})$. \n\n2. Compared with (Tian et.al. 2024) and other previous works, could the author detailed  explain where are the techique novelty or improvement. I went through the proof sketch but analysis looks like standard."}, "questions": {"value": "See weaknesses, please."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WXUheQXLnr", "forum": "V05qqNqBpY", "replyto": "V05qqNqBpY", "signatures": ["ICLR.cc/2026/Conference/Submission11001/Reviewer_8pXP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11001/Reviewer_8pXP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699797117, "cdate": 1761699797117, "tmdate": 1762922185192, "mdate": 1762922185192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides the first finite-time convergence analysis for single-timescale actor-critic (AC) algorithms utilizing deep neural network approximation in continuous state-action spaces under the time-average reward setting. The authors prove convergence to a stationary point at a rate of  $\\tilde{O}({T}^{-1/2})$ for the coupled reward, critic, and actor errors. The theoretical claims are substantiated with experiments on the Pendulum task and MuJoCo benchmarks, demonstrating the superior approximation capability of neural critics over linear ones and empirically validating the predicted convergence rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper  provides a finite-time analysis for the challenging single-timescale neural AC setting, with continuous spaces and Markovian sampling, is a substantial theoretical advance.\n2. The paper goes beyond pure theory by including comprehensive experiments. The empirical confirmation of the $\\tilde{O}({T}^{-1/2})$ convergence rate on Pendulum and the demonstration of strong performance on MuJoCo benchmarks provide crucial support for the theoretical results and highlight the practical relevance of the analyzed algorithm.\n3. The paper is well-structured and easy to follow."}, "weaknesses": {"value": "The analysis operates in the neural tangent kernel (NTK) or overparameterized regime, where the network is wide enough to be well-approximated by its linearization around initialization. This regime, while theoretically fruitful, does not fully capture the feature learning dynamics that are believed to be crucial for the success of deep learning in practice."}, "questions": {"value": "1. How is $m$ avoided in the convergence result? Does it depend on the assumption that the network is wide enough?\n2. What is the convergence rate guarantee for the discounted reward setting, as it is more common in RL formulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s1xQbcoril", "forum": "V05qqNqBpY", "replyto": "V05qqNqBpY", "signatures": ["ICLR.cc/2026/Conference/Submission11001/Reviewer_Duep"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11001/Reviewer_Duep"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790868512, "cdate": 1761790868512, "tmdate": 1762922184691, "mdate": 1762922184691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies single-timescale actor--critic (AC) with deep neural network function approximation in continuous state--action spaces under the average-reward objective. It analyzes a practical AC loop that jointly updates a reward estimator, a TD(0)-style critic, and a policy-gradient actor with Markovian on-policy samples. The main theoretical result is a finite-time convergence guarantee to a stationary point at a $\\tilde{O}(T^{-1/2})$ rate (up to logarithmic factors), simultaneously controlling reward-estimation, critic, and actor errors. Experiments (Pendulum, MuJoCo-style tasks) illustrate empirical trends, including a measured slope near $-1/2$ on log--log plots and improvements from neural critics over linear baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Realistic setting: single-timescale updates with Markovian sampling in continuous spaces---closer to practice than idealized double-loop or two-timescale analyses.\n\n2.  Finite-time $\\tilde{O}(T^{-1/2})$ rate: matches the best known dependence on $T$ (up to logs) for this setting; jointly tracks three coupled sources of error.\n\n3. Empirical checks: (i) Pendulum results where the neural critic better aligns with an RVI baseline than a linear/RBF critic; (ii) empirical slope $\\approx -0.51$ consistent with theory; (iii) MuJoCo ablations show depth/width benefits over linear critics.\n\n4. Assumptions documented and motivated: geometric mixing/ergodicity, an exploration inequality, and smoothness/Lipschitz properties for policy and dynamics, with discussion of when exploration can fail."}, "weaknesses": {"value": "1.  The theory assumes sufficiently wide networks and projects critic updates to remain near initialization. It is unclear how necessary/tight this is or how it maps to common unconstrained training with Adam/weight decay.\n\n2. Guarantees include an $O(\\varepsilon_{\\text{app}})$ term from critic approximation, but there is limited guidance for architectures/regularization that make $\\varepsilon_{\\text{app}}$ small in practice; experiments do not quantify this floor or test misspecification."}, "questions": {"value": "Is projection onto a radius constraint essential for the analysis, or could similar guarantees hold for unconstrained (Adam/SGD) updates with weight decay?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TYLJehcspT", "forum": "V05qqNqBpY", "replyto": "V05qqNqBpY", "signatures": ["ICLR.cc/2026/Conference/Submission11001/Reviewer_VBtF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11001/Reviewer_VBtF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976133793, "cdate": 1761976133793, "tmdate": 1762922184206, "mdate": 1762922184206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}