{"id": "zGtTQTD1zu", "number": 12232, "cdate": 1758206487980, "mdate": 1759897523745, "content": {"title": "OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation", "abstract": "Embodied navigation is a foundational challenge for intelligent robots, demanding the ability to comprehend visual environments, follow natural language instructions, and explore autonomously. However, existing models struggle to provide a unified solution across heterogeneous navigation paradigms, often yielding low success rates and limited generalization. We present OmniNav, a unified framework that handles instruct-goal, object-goal, point-goal navigation, and frontier-based exploration within a single architecture. First, we introduce a lightweight, low-latency policy that predicts continuous-space waypoints (coordinates and orientations) with high accuracy, outperforming action-chunk methods in precision and supporting real-world deployment with control frequencies up to 5 Hz. Second, at the architectural level, OmniNav proposes a fast-slow system design: a fast module performs waypoint generation from relatively short-horizon visual context and subtasks, while a slow module conducts deliberative planning using long-horizon observations and candidate frontiers to select the next subgoal and subtask. This collaboration improves path efficiency and maintains trajectory coherence in exploration and memory-intensive settings. Notably, we find that the primary bottleneck lies not in navigation policy learning per se, but in robust understanding of general instructions and objects. To enhance generalization, we incorporate large-scale general-purpose training dataset including those used for image captioning and visual into a joint multi-task regimen, which substantially boosts success rates and robustness. Extensive experiments demonstrate state-of-the-art performance across diverse navigation benchmarks, and real-world deployment further validates the approach. OmniNav offers practical insights for embodied navigation and points to a scalable path toward versatile, highly generalizable robotic intelligence.", "tldr": "This paper introduces a visual end-to-end navigation framework that integrates multi-task models and active exploration strategies to efficiently handle object-goal, instruct-goal, and map-free frontier-based tasks.", "keywords": ["Object-Goal Navigation", "Instruct-Goal Navigation", "Active Exploration", "Policy-Diffusion"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bce5b014169fbedac4f8da1b2891ff811febf964.pdf", "supplementary_material": "/attachment/9c07e5c95fecc52af4f2d0aeba411abfb062d61e.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents OmniNav, a unified framework that integrates instruction-goal, object-goal, point-goal, and frontier-based exploration within a single embodied navigation architecture. The core design is a fast–slow dual-system: a fast module generates continuous-space waypoints at high frequency using a flow-matching policy on top of a vision–language backbone, while a slow module conducts long-horizon planning with frontier reasoning, visual memory, and explicit chain-of-thought outputs for interpretability. A two-stage training strategy combines discrete autoregressive alignment with continuous control fine-tuning, and further incorporates large-scale generic multimodal data (captioning, grounding, embodied QA). Experiments across R2R-CE, RxR-CE, HM3D-OVON, and CityWalker benchmarks show consistent state-of-the-art results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a highly relevant and challenging goal of unifying multiple embodied navigation paradigms and delivers a coherent architecture that balances long-horizon reasoning with real-time control.\n2. The dual-system design is conceptually grounded and practically validated: the fast module ensures low-latency precision, while the slow module brings reasoning, memory, and semantic exploration, jointly achieving both agility and deliberation.\n3. The training and evaluation are extensive, including multi-task datasets, ablation over all core components, and real-world deployment, demonstrating strong empirical support."}, "weaknesses": {"value": "1. The framework relies on multiple specialized modules (frontier reasoning, CoT-based subgoal selection, flow matching), yet their interaction lacks a clear learning-based coordination mechanism. The boundary between symbolic reasoning (slow system) and learned policies (fast system) appears manually tuned rather than jointly optimized.\n2. The scalability and computational trade-offs of the dual-system—especially the latency and frequency of the slow module during real-world deployment—are not quantitatively analyzed."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zv0ODQq0EV", "forum": "zGtTQTD1zu", "replyto": "zGtTQTD1zu", "signatures": ["ICLR.cc/2026/Conference/Submission12232/Reviewer_ZJRF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12232/Reviewer_ZJRF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761680351743, "cdate": 1761680351743, "tmdate": 1762923177199, "mdate": 1762923177199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OmniNav, a unified navigation framework for embodied agents that can perform instruction-goal, object-goal, point-goal and frontier exploration simultaneously. OmniNav generates short-term waypoints with low latency, while leveraging VLM’s chain-of-thought to reason about long-term goals. These two modules are integrated using a central memory module, based on a KV cache. Through experiments the authors show that the system outperforms state-of-the-art baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The overall idea is very interesting and the paper is well-written. Especially, OmniNav separates embodied navigation into a fast and a slow thinking system which is very interesting. Another strength is that the system has been demonstrated to be deployed on a real robot with low latency."}, "weaknesses": {"value": "Although the architecture in OmniNav is novel, I am not sure if this work is the first unified architecture to support different types of navigation goals, as the authors claim. There have been similar attempts in GOAT[1], GOAT-Bench[2] and InstructNav[3]. Also, many recent works combine frontier exploration and semantic-based frontier exploration[4] within the embodied navigation architecture. I would request the authors to provide more detail about this."}, "questions": {"value": "Other than the weakness that I mentioned above, I have a few other questions for the authors.\n1. What happens if another goal type is added? Do you have to train the system from scratch? \n2. Are the 2D coordinates relative to agent starting position or is it a global coordinate?\n3. To evaluate instruction following (RxR), why did you not include the ndtw metric? I believe ndtw is a better representative metric of instruction following than success rate and spl.\n\nMinor comment - missing citation in Line 377"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qtpJs3AKZh", "forum": "zGtTQTD1zu", "replyto": "zGtTQTD1zu", "signatures": ["ICLR.cc/2026/Conference/Submission12232/Reviewer_FK7W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12232/Reviewer_FK7W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725668293, "cdate": 1761725668293, "tmdate": 1762923176789, "mdate": 1762923176789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work retrains/finetunes a small general purpose VLM (Qwen2.5-VL-3B-Instruct) for language-guided navigation tasks. The model is jointly trained on object-goal, instruct-goal, and point-goal navigation tasks. The final trajectory output is refined through a 5-step trajectory diffusion head. In their approach the authors use the trained VLM in two modes, calling one \"fast thinking\" and one \"slow thinking\". These roughly correspond to the general established concetps of local and global planning, and in this work are mostly distinguished by which images from a memory bank the VLM gets access to and whether or not Chain-of-Thought-Reasoning is used or not. The system is evaluated on a couple of different simulation benchmarks and validated on a real-world robot deployment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The overall approach follows a lot of \"conventional wisdom\" in navigation and planning. What is new (at least to me) in this work is the connections the authors make between these established concepts and the operating modes of VLMs: Indeed CoT reasoning fits well to global planning, and changing the information in the prompt is very much like global planning based on maps and local planning based on what is in view.\n- The real-world robot deployment is a very convincing test to me. I am impressed that this system runs at 5Hz on a conventional, not even that new GPU, and it is great to see that it generalizes to what appears to be the office environment of the authors."}, "weaknesses": {"value": "- The biggest weakness to me in this paper is that there is simply not much to learn. I do not really see any innovation in methodology, this is mostly a system integrating a lot of known components and methodology together. I am all for system papers and I conduct system-level research myself, because it is important to investigate how individual methodology can be integrated together, but system-research really requires much more than just a single ablation study to better understand how to best train a VLM for navigation: E.g. it would be really useful to better understand what kind of training data and training tasks are critical, how much training data is necessary (scaling law), what the range of quality in the training data is, if even larger models would lead to better navigation or if for navigation very large models are not necessary, etc. This paper does not answer any of these questions, it just shows one version of the system and shows that this version works. That is not very insightful.\n- It is quite well hidden and only observable from line 174 that this paper only addresses 2D navigation. That is extremely limiting. I wonder how this fits to the dataset and benchmarks given that HM3D is actually mostly consisting of multi-floor houses that this method is not abel to navigate?\n- One of the claims of the authors is that their system jointly addresses three slightly different ways of how navigation tasks are often framed: Point-goal, object-goal, and instruct-goal. It is not clear to me whether they also evaluate on all of these 3 tasks? The problem here is mostly that for the experiments there are no descriptions of the datasets and benchmarks used, and they are even only named by abbreviation and without reference. What are these benchmarks? What task are they evaluating? With what logic are the baselines selected?\n\n\nThere are further minor inaccuracies and aspects in which the writing and presentation can be improved:\n- Until line 323 it was not clear to me that the fast and slow method actually use the same VLM that is jointly trained. It would help the reader if this is alredy made clear in the introduction of the method, and also in Figure 1.\n- It is not clear how the slow module invokes the fast module once the object is in sight. The caption of Figure 1 claims \"if the target exists in memory, the fast thinking module is invoked\". This contradicts the figure itself, where in the slow thinking moduel there is an option for the target in sight. Also, both modules directly output waypoint coordinates in the figure and there is never a path where the slow module gives input into the fast module.\n- line 196 \"grasp spatial location\". I think this is an overclaim. I do not think it is necessary to understand spatial location in order to produce goals, it is probably sufficient to understand just directions."}, "questions": {"value": "- Figure 3: What is the unit W? \n- Line 294: From my experience, DPVO is producing usually quite bad estimates. There are far more robust monocular SLAM methods (e.g. ORBSLAM, Mast3r-SLAM). Why did the authors choose DPVO and can they quantify the quality of the data that is described in this paragraph?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i4CcyufUYm", "forum": "zGtTQTD1zu", "replyto": "zGtTQTD1zu", "signatures": ["ICLR.cc/2026/Conference/Submission12232/Reviewer_PyvL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12232/Reviewer_PyvL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945301343, "cdate": 1761945301343, "tmdate": 1762923176330, "mdate": 1762923176330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniNav, a unified framework for embodied navigation that simultaneously supports instruction-guided, object-goal, point-goal, and frontier-based exploration tasks within a single architecture. OmniNav integrates a fast system for low-latency waypoint generation and a slow system for long-horizon planning and subgoal decomposition, coordinated through a central memory module. The model leverages a flow-matching policy for continuous trajectory prediction, and is trained on a diverse mix of navigation and general-purpose vision-language data to improve generalization. Extensive experiments demonstrate state-of-the-art performance across multiple benchmarks (R2R-CE, RxR-CE, HM3D-OVON), and real-world robot deployment validates its practicality and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The model jointly support multiple navigation paradigms—instruction, object-goal, point-goal, and frontier-based exploration—within a single model, reducing task-specific customization and promoting cross-task generalization."}, "weaknesses": {"value": "1. The Dual-System claim is unconvincing and ambiguous.  It is unclear whether the fast and slow systems are two distinct models or a single VLM backbone used in two different modes. If they share the same VLM, the \"dual-system\" description is more of a procedural distinction (e.g., different prompting or output heads). If both systems are built upon a large VLM like Qwen2.5-VL (3B parameters), neither can be considered truly \"fast\" or low-latency in a conventional systems sense. The \"fast\" system's speed seems to be achieved primarily through a shorter context window. And the slow system's contribution is primarily for the Object-Goal navigation (OVON) task with frontier-based exploration. The authors also point out that the fast system alone can actually handle all tasks.  This does not constitute a genuine dual-system design as claimed, and the framing seems to be an overstatement.\n\n 2. The role of coordinates as inputs and outputs is ambiguous across tasks. \nFor object goal, the input coordinates likely represent the locations of frontiers or objects,  the diffusion policy appears to serve only as a low-level controller that generates an executable trajectory towards a pre-selected coordinate.  Given that the system already constructs and maintains an explicit 3D occupancy map for frontier detection, adding the diffusion policy is marginal as using a classical motion planner can also achieve it. Also, the inclusion of a point-goal navigation task within this framework appears redundant. \n\n The authors also fail to provide critical details regarding the planning horizon and physical meaning of the output with 5 spatial-temporal waypoints. The two-stage training paradigm also raises the question: is the flow-matching policy in Stage 2 merely functioning as a \"de-discretization\" module, converting short-term action chunks from Stage 1 into a smoothed but equally short-term trajectory?\n\n 3. The 'Observations' reported in Table 1 are incorrect. The whole system functions by maintaining a 3D occupancy map and identifying frontiers is technically infeasible without depth and odometry information. I’m not sure whether the authors realize that this fundamental reliance on geometric information makes this work a different category from the vision-only baselines."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aN2ofrRkej", "forum": "zGtTQTD1zu", "replyto": "zGtTQTD1zu", "signatures": ["ICLR.cc/2026/Conference/Submission12232/Reviewer_8HoJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12232/Reviewer_8HoJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976486514, "cdate": 1761976486514, "tmdate": 1762923174764, "mdate": 1762923174764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}