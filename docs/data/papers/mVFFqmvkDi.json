{"id": "mVFFqmvkDi", "number": 898, "cdate": 1756822465511, "mdate": 1759898236422, "content": {"title": "GP-STPCA: Generalized Power Method for Sparse Tensor Principal Component Analysis", "abstract": "Sparse tensor principal component analysis (STPCA) seeks interpretable low-dimensional representations of high-order data by enforcing sparsity across tensor modes.\nHowever, the resulting optimization is highly nonconvex and computationally demanding, particularly in high-dimensional and unbalanced settings.\nWe introduce GP-STPCA, a unified framework that reformulates STPCA into structured sparse PCA subproblems solvable via the generalized power method.\nOur approach accommodates both $\\ell_{0}$- and $\\ell_{1}$-penalties, in single-unit and block formulations, enabling efficient extraction of multiple sparse components. \nWe provide theoretical guarantees by proving equivalence with the original sparse objective and analyzing convergence. \nAlgorithmically, GP-STPCA further leverages efficient pattern-finding and post-processing to shrink the search space in column-dominant settings.\nExtensive experiments on synthetic recovery tasks, ImageNet reconstruction, and brain connectome analysis demonstrate that GP-STPCA consistently outperforms the SOTA sparseGeoHOPCA in terms of accuracy, sparsity control, interpretability, and computational efficiency.", "tldr": "", "keywords": ["Tensor Optimization; Sparse Tensor Principal Component Analysis; Generalized Power Method"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/58edcfff8a3f49c3c8f4eac24e3aafe088d2f775.pdf", "supplementary_material": "/attachment/f112fc9a392d35878c79d21578a720649828f29f.zip"}, "replies": [{"content": {"summary": {"value": "A method for Tucker-based sparse tensor PCA is proposed. The authors consider a unified setting for both l0- and l1-constraints penalties and single-unit or block formulations. Experiments are conducted on synthetic data, several images, and brain connectome data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. A new method for Tucker-based sparse PCA is proposed.\n\n2. There is some theoretical analysis on it."}, "weaknesses": {"value": "1. The core modeling insights are insufficient. It seems that the authors straightforwardly extend (1)-based sparse PCA methods and (3)-based Tucker sparse PCA formulation. Formulation (4) considers unfolding which is a standard operation in Tucker-based methods. Thus, the modeling novelty is limited. \n\n2. Due to the high technical similarity of the processing of $\\ell_0$, $\\ell_1$ both in element-wise and block-wise sparsity, the so-called unified modeling is of little essential significance. \n\n3. The theory analysis under the Assumptions 1-3 is simple, and of little theoretical significance. \n\n4. The experiments are insufficient. On synthetic data, the experiments only use rank-1 tensors, which is a very special and unrealistic case. The dataset sizes are also limited. Additionally, the authors only compare against GP-STPCA."}, "questions": {"value": "See the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PnFESlkcWm", "forum": "mVFFqmvkDi", "replyto": "mVFFqmvkDi", "signatures": ["ICLR.cc/2026/Conference/Submission898/Reviewer_p57b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission898/Reviewer_p57b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761382145927, "cdate": 1761382145927, "tmdate": 1762915638038, "mdate": 1762915638038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends sparse Principal Component Analysis (PCA) from the matrix setting to higher-order tensor data under the Tucker decomposition framework. The authors formulate the Sparse Tucker PCA (STPCA) problem as a multi-mode sparse low-rank approximation and propose an alternating optimization algorithm based on the generalized power method (GPower)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper extends sparse Principal Component Analysis (PCA) to a multi-mode Tucker structure, aiming to develop a theoretically grounded and computationally tractable framework for sparse tensor principal component analysis (STPCA)."}, "weaknesses": {"value": "1. However, similar ideas have already been explored in prior works such as Sparse Higher-Order PCA [1] and sparseGeoHOPCA. Therefore, the novelty of the proposed framework requires clearer justification and differentiation from these existing approaches.\n\n2. The provided convergence guarantee only applies to the simplified subproblems, not to the full alternating multi-block optimization of the entire STPCA.\n\n3. The motivation and distinction from prior tensor sparse PCA works are not clearly articulated. Similar ideas already exist, such as Sparse Higher-Order PCA (Sparse HOPCA) by Allen (2012) and sparseGeoHOPCA, which also impose sparsity on Tucker/CP decompositions. The paper needs to explicitly discuss these connections and clarify what is genuinely new beyond previous formulations.\n\n4. The experiments are underdeveloped. It compares against only one baseline method, and the real-data evaluation consists of just six color images for quantitative comparison. As it stands, the empirical section is insufficient to validate the proposed method.\n\n5. Writing and Presentation Issues:\n\na. The introduction lacks a clear problem statement and motivation. For example, the sentence “However, introducing sparsity makes TPCA a non-convex and generally NP-hard problem” is abrupt and undefined—what does “TPCA” specifically refer to, and where is it cited?\n\nb. Tucker decomposition, a central concept, is never introduced in the Introduction, which makes it hard for readers to understand the context.\n\nc. Figure 1 (e.g., the black dots in the figure) is confusing—they could be misinterpreted as indicating sparse noise rather than sparsity in the factors.\n\nd. Using “In this paper, ...” to start the Introduction is very uncommon in academic writing.\n\ne. Equations such as (1) and (3) are presented without adequate intuition.\n\n6. The related work section relies heavily on older literature. The paper fails to discuss recent developments in sparse tensor decomposition, low-rank recovery, or structured regularization over the last five years.\n\n[1] Sparse higher-order principal components analysis,Genevera I. Allen"}, "questions": {"value": "See the weaknesses 3-6."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QxRhPEIBs2", "forum": "mVFFqmvkDi", "replyto": "mVFFqmvkDi", "signatures": ["ICLR.cc/2026/Conference/Submission898/Reviewer_brfT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission898/Reviewer_brfT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765100246, "cdate": 1761765100246, "tmdate": 1762915637896, "mdate": 1762915637896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GP-STPCA, a generalized power–based framework for efficient and interpretable sparse tensor PCA. It achieves superior accuracy and efficiency over prior methods through principled reformulation and theoretical convergence guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strength\n1. Provides solid theoretical grounding by establishing equivalence to the original sparse objective and proving step-size convergence under mild convexity assumptions.\n2. Achieves notable computational efficiency by reducing the search space via mode-wise convex reformulation, yielding up to 50–100× speedups over baselines.\n3. Produces well-localized and physically meaningful sparse factors across tensor modes, enhancing interpretability."}, "weaknesses": {"value": "Weakness\n1. Experimental evaluation is limited — the image reconstruction results are based on only a few examples and do not convincingly demonstrate general performance.\n2. The algorithm may be sensitive to initialization, risking convergence to local optima.\n3. Several sparsity-related hyperparameters (γₙ, μₙ, kₙ) require manual tuning with little guidance.\n4. Theoretical guarantees depend on strong convexity assumptions that may not hold in real-world non-convex problems."}, "questions": {"value": "Questions\n1. Larger-scale image reconstruction experiments are needed — evaluating only a few images is insufficient to assess overall model capability.\n2. Can the framework be extended to handle dynamic or variable-order tensors, such as video or time-evolving data?\n3. Could GP-STPCA be integrated with nonlinear tensor models or deep architectures (e.g., autoencoders, deep factorization networks)?\n4. How robust is the method to noise and corruption? Would robust penalties (e.g., Huber loss, correntropy) improve stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concers."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CO9ooLbcgD", "forum": "mVFFqmvkDi", "replyto": "mVFFqmvkDi", "signatures": ["ICLR.cc/2026/Conference/Submission898/Reviewer_EtrH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission898/Reviewer_EtrH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792249417, "cdate": 1761792249417, "tmdate": 1762915637787, "mdate": 1762915637787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GP-STPCA, a Tucker-based framework that reduces sparse tensor PCA (STPCA) to a sequence of sparse matrix PCA subproblems that are allegedly solvable efficiently with the generalized power method.The authors handle both ℓ0 and ℓ1penalties, in single-unit and block formulations, add a “pattern-finding + post-processing” procedure, and prove a stepsize convergence bound for a generalized gradient scheme under strong convexity assumptions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Reduction to right-sparse matrix PCA is sensible for column-dominant unfoldings; writing the mode-wise objective in maximization form and using power-like updates is well motivated.\n2.The post-processing step (polar/SVD refinement on the identified support) is a practical fix to value distortion caused by ℓ1shrinkage."}, "weaknesses": {"value": "1.Questionable novelty relative to established sparse PCA with generalized power. The main technical engine (power-type thresholding on a reduced problem) is lifted directly from Journee et al. (2010). The paper reframes STPCA as a stack of such subproblems but gives little truly new algorithmic substance beyond bookkeeping, deflation, and a routine polar normalization. The “equivalence” and “unified view” read more like repackaging than a substantive advance.\n2.Missing baselines include modern sparse-Tucker/ALS variants, convex relaxations, and other iterative hard-thresholding approaches on tensor modes. The chosen baseline (sparseGeoHOPCA) is a single family; the study lacks breadth.\n3.For the experimental part, it is suggested that the authors add more data sets and tensor-based algorithms . The authors did not carry out more relevant experiments well to support the research motivation of this paper."}, "questions": {"value": "1. The number of experimental samples in the main text of the paper is relatively small. For natural high-order representation structures like tensors, it would be more convincing to use video datasets. Why didn't the author incorporate the above-mentioned high-dimensional data structures?\n\n2. Regarding the approach proposed by the author, is it possible to conduct a theoretical analysis of the error and reach such an error bound?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "As mentioned above."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ueper4reDz", "forum": "mVFFqmvkDi", "replyto": "mVFFqmvkDi", "signatures": ["ICLR.cc/2026/Conference/Submission898/Reviewer_LpQ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission898/Reviewer_LpQ7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185389780, "cdate": 1762185389780, "tmdate": 1762915637674, "mdate": 1762915637674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}