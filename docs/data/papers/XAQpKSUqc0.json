{"id": "XAQpKSUqc0", "number": 7766, "cdate": 1758035349300, "mdate": 1759897833813, "content": {"title": "AttriLens-Mol: Attribute Guided Reinforcement Learning for  Molecular Property Prediction with Large Language Models", "abstract": "Large Language Models (LLMs) have shown promise in assisting molecular property prediction tasks but often rely on human-crafted prompts and chain-of-thought templates. While recent advanced large reasoning models like DeepSeek-R1 employ reinforcement learning for an extended ''thinking'' process, their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol, an attribute-guided reinforcement learning framework for molecular property prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1) a format reward encouraging attribute-based structured output, (2) a count reward to avoid enumerating irrelevant attributes, and (3) a rationality reward using advanced LLMs and RDKit to verify the relatedness of the generated attributes. This approach implicitly elicits the model's inherent knowledge of relevant molecular attributes during reasoning, enables making predictions for the molecular property more effectively. Experiments on both in-distribution and out-of-distribution datasets show that, training both 7B-size R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our proposed AttriLens-Mol method significantly boosts the performance, getting comparable or better results than supervised fine-tuning models (Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o, DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the target property, when used as features for an interpretable decision tree model, yield superior performance compared to attributes generated by prompting LLMs. This shows that AttriLens-Mol effectively elicits more relevant and predictive molecular attributes, leading to enhanced interpretability and performance for property prediction.", "tldr": "We propose AttriLens-Mol, an attribute-guided reasoning framework that enhances molecular property prediction capability of LLMs, achieving superior or compariable performance against advanced LLMs and task-specific models.", "keywords": ["Molecular Property Prediction", "Large Language Models", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f001207a7df60af615541c7a347d4911bf12d494.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Summary:\nThis paper introduces AttriLens-Mol, an attribute-guided reinforcement learning (RL) framework that enhances molecular property prediction in Large Language Models. It uses three specific rewards to guide LLM’s reasoning incluyding a format reward, a count reward, and a rationality reward. The model significantly boosts the performance of 7B models, allowing them to achieve results comparable or superior to much larger models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Pros:\n- The authors has introduced a new model called AttriLens-Mol, to enhance molecular property prediction with LLMs\n- The model has achieved better performance compared with 7B models baselines, and achieve comparable results to much large models"}, "weaknesses": {"value": "Cons:\n- The rationality reward is not self-contained. It relies on judgment from advanced LLMs like GPT-4O and DeepSeek-R1. This way may creates a dependency, meaning the performance is tied to the accuracy and knowledge of external models. \n- More benchmark datasets are encouraged to added and tested. Currently, the model is only tested on six datasets in MoleculeNet. Why other benchmark datasets is not included, e.g., HIV, and Tox21. \n- Compared to standard supervised fine-tuning, the reinforcement learning approach is computationally expensive. For each training sample, the model must generate a group of 8 candidate responses. Each of these 8 responses must then be evaluated by the four separate reward functions"}, "questions": {"value": "Shown in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QQqTmIvhAs", "forum": "XAQpKSUqc0", "replyto": "XAQpKSUqc0", "signatures": ["ICLR.cc/2026/Conference/Submission7766/Reviewer_anam"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7766/Reviewer_anam"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7766/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673324616, "cdate": 1761673324616, "tmdate": 1762919807544, "mdate": 1762919807544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AttriLens-Mol, an attribute-guided reinforcement learning framework designed to enhance molecular property prediction using LLMs. AttriLens-Mol introduces three complementary reward functions—format, count, and rationality—to systematically align LLM reasoning with molecular attributes in a chemically interpretable manner. Extensive experiments on both in-distribution and out-of-distribution MoleculeNet benchmarks (BBBP, BACE, ClinTox, etc.) demonstrate that the proposed approach consistently outperforms supervised and prompting-based baselines. Additionally, decision-tree analyses reveal that the attributes generated by the model can serve as interpretable features for property classification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents an innovative idea by using attribute-level rewards to align LLM reasoning with molecular properties, which is both novel and meaningful. This is supported by a solid methodology, with the design of three coherent and clearly explained rewards. Furthermore, the comprehensive evaluation compares multiple models (Qwen, LLaMA) and algorithms (GRPO, DAPO), demonstrating strong and consistent gains. Interpretability is reinforced through decision-tree analysis based on extracted attributes, effectively supporting the claim of explainable reasoning. Overall, the paper is well-written, clear, structured, and easy to follow."}, "weaknesses": {"value": "- The reliance on closed-source LLMs (GPT-4o and DeepSeek-R1) for generating advantageous attribute ranges raises reproducibility concerns; clarifying the derivation process and assessing whether open-source models yield comparable results would improve the paper. \n\n- The limited experimental scale—approximately 4,000 samples—constrains the empirical scope; including a brief discussion or small-scale analysis of performance on larger datasets would strengthen the credibility and generalization claims."}, "questions": {"value": "1. Why do the authors need to extract relevant attribute information from GPT-4o or DeepSeek-R1? Is there a fixed standard for defining these attribute ranges, rather than relying on LLM-generated values? Could this approach introduce concerns regarding reproducibility?\n\n2. Why did the authors choose to use approximately 4,000 training samples from the BBBP, BACE, and ClinTox datasets? Would increasing or decreasing the amount of training data have a significant impact on the results?\n\n3. Are these advantageous ranges defined separately for each molecular property, or are they shared globally across all tasks? If a single unified set of ranges is used, could this lead to potential attribute bias or information leakage? Additionally, are the model’s results sensitive to small perturbations in these ranges?\n\n4. Since the reward is computed using discrete matching (as shown in Equation 4), have the authors considered the potential issue of reward sparsity that this formulation might introduce?\n\n5. Providing clearer descriptions of the reinforcement learning hyperparameters and settings — such as the KL coefficient, clipping parameter, optimizer and learning rate schedule, reference policy, context length, decoding strategy, seed control, and safeguards against degenerate advantage normalization — would greatly improve the reproducibility and robustness of the work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "spgI65WhW3", "forum": "XAQpKSUqc0", "replyto": "XAQpKSUqc0", "signatures": ["ICLR.cc/2026/Conference/Submission7766/Reviewer_BsNe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7766/Reviewer_BsNe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7766/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809675803, "cdate": 1761809675803, "tmdate": 1762919807236, "mdate": 1762919807236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reinforcement learning (RL) framework, AttriLens-Mol, to fine-tune large language models (LLMs) for molecular property prediction using \"attribute-guided\" rewards. The core contribution is the \"rationality reward,\" which uses a stronger \"guidance LLM\" (like GPT-4O) to define \"advantageous ranges\" and then combines RDKit computations to \"factcheck\" the reasoning steps generated by the 7B model. The paper claims this method \"elicits inherent knowledge\" and achieves strong performance on in-distribution (ID) and out-of-distribution (OOD) tasks. However, the methodology is closer to knowledge distillation (from GPT-4O to the 7B model) rather than the \"discovery\" or \"elicitation\" of new knowledge. Furthermore, the framework is critically dependent on RDKit's fixed descriptors, limiting its generality and application in genuine scientific discovery."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The empirical results of AttriLens-Mol on OOD  regression tasks (Table 2) are indeed impressive. The performance on OOD tasks (ESOL/FreeSolv) is significantly better than the SFT baselines, which is a very strong empirical argument.\n\nThe decision tree analysis in Table 6 is a welcome addition, as it attempts (and succeeds) to quantitatively prove that the attributes (features) generated by the LLM are predictive, not just \"plausible-sounding\" text."}, "weaknesses": {"value": "1. The core \"rationality reward\" is entirely dependent on an external \"guidance LLM\" (like GPT-4O) to define \"advantageous ranges.\" This raises a methodological concern: the 7B model is not \"eliciting inherent knowledge\" as claimed, but rather performing high-efficiency knowledge distillation. The most critical \"chemical common sense\" is injected externally, anchoring the model's performance to the quality of the guidance LLM.\n\n2. The \"rationality reward\" mechanism is hard-coded to RDKit's 53 descriptors, severely restricting its applicability. The framework will fail on tasks requiring features beyond RDKit's scope . Consequently, the paper's title \"...for Molecular Property Prediction\" is an overclaim, as it is limited to \" RDKit-descriptor-predictable\" properties and not genuine scientific discovery.\n\n3. The \"count reward\" (rewarding [3, 10] attributes) is a heuristic, ad-hoc design lacking principled argumentation. This one-size-fits-all constraint is problematic: it could penalize simple tasks (requiring <3 attributes) or hinder complex ones (requiring >10). The ablation study may only show that the model learned to \"fill the quota,\" not \"autonomously select the most relevant attributes\" for the task."}, "questions": {"value": "1. Can you provide an experiment quantifying the\nimpact of the \"guidance LLM\" (e.g., GPT-4O) quality on the final performance? For example, how much\nwould performance drop if a \"weaker\" guidance LLM (like LLaMA3-8B) is used to generate the \"\nadvantageous ranges\"? This is critical for disentangling whether the model is \"learning to reason\" or \"\ndistilling knowledge.\"\n\n2. What is your view on the framework's scalability beyond the fixed RDKit\ndescriptors? If a task's key attributes are unknown or non-RDKit-computable (e.g., predicting quantum-\nchemical properties), how would the framework adapt? Or do you acknowledge that the framework is not\napplicable to such tasks?\n\n3. The design of the count reward seems counter-intuitive as it limits the model's free exploration. Did you experiment with a \"softer\" constraint, such as a sparsity penalty (i.e., a small negative reward for each additional attribute), to replace the hard 3, 10 range? Could this allow the model to autonomously learn the optimal number of attributes, rather than relying on the manually set\nrange?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nzuqyvULaU", "forum": "XAQpKSUqc0", "replyto": "XAQpKSUqc0", "signatures": ["ICLR.cc/2026/Conference/Submission7766/Reviewer_i7Ns"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7766/Reviewer_i7Ns"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7766/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930900083, "cdate": 1761930900083, "tmdate": 1762919806749, "mdate": 1762919806749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reward design for reinforcement learning on molecular property prediction tasks using LLMs. Specifically, it defines two types of rewards: (1) task-agnostic rewards, which include a format reward and a correctness reward, and (2) attribute-oriented rewards consisting of a count reward that enforces the number of attributes to fall within a predefined range (e.g., 3–10), and a rationality reward that encourages selecting relevant molecular attributes and predicting them accurately. Using the proposed rewards, the model is fine-tuned separately with GPRO and DAPO. Experimental results show that this reward design improves predictive performance on both in-distribution and out-of-distribution tasks, while providing interpretable rationales."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The reward function is well-designed to plant fine-grained reasoning capabilities and interpretability.\n* The ablation study demonstrates the effectiveness of each reward component."}, "weaknesses": {"value": "* The performance of the fine-tuned model heavily depends on the completeness and quality of the predicted attributes. In other words, if important but non-standard attributes are not included in the attribute list or the predicted values are incorrect, the LLM may fail to provide correct and complete answers. I suggest analyzing the number of correct attributes or the distribution of the selected (promoted) attributes for each task. \n* Although the proposed reward design improves predictive performance, it still lags behind LLMs such as the GPT series and the DeepSeek series on some tasks. Moreover, there is no discussion or experimental comparison with the state-of-the-art model TxGemma [1], which is trained for molecular property tasks.\n* The instructions used for LLMs are not precisely described, limiting the reproducibility of this work. In addition, using broader datasets such as ToxCast or HIV would further strengthen the contribution of the proposed reward.\n\n[1] Wang et al., \"TxGemma: Efficient and Agentic LLMs for Therapeutics\", ArXiv, 2025."}, "questions": {"value": "* For each task, could you provide the distributions of promoted and inhibited attributes during reasoning? How accurate are the predicted attributes?\n* Are the individual reward components simply summed without weighting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qtJVRobdKP", "forum": "XAQpKSUqc0", "replyto": "XAQpKSUqc0", "signatures": ["ICLR.cc/2026/Conference/Submission7766/Reviewer_p6WF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7766/Reviewer_p6WF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7766/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981247631, "cdate": 1761981247631, "tmdate": 1762919804850, "mdate": 1762919804850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}