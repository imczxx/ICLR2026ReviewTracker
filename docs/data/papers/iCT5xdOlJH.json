{"id": "iCT5xdOlJH", "number": 11844, "cdate": 1758204218504, "mdate": 1759897551234, "content": {"title": "IO-Adam: Rethinking Memory-Efficient Adaptive Optimizers from Gradient Computation", "abstract": "Adaptive Moment Estimation (Adam) is one of the most popular stochastic optimizers for deep neural network training and has become the default optimizer in many scenarios, especially on language tasks. With the first and second moment estimation, Adam provides adaptive learning rates for each parameter, significantly outperforming Stochastic Gradient Descent (SGD). However, as the deep neural networks become larger, the estimation of the first and second moments takes up substantial memory, motivating methods to reduce the memory usage for adaptive optimizers. In this paper, we propose to rethink the first and second moment estimation from a gradient computation perspective. The gradient of the weight matrix is the multiplication of the input and the gradient of the output. Instead of trying to find a low-rank approximation for the first and second moment estimation as in previous works, we propose to track the input and the output gradient for efficient moment estimation. We provide analyses on the connection and difference between our proposed method, the widely used Adam optimizer, and previous memory-efficient optimizers proposed to reduce the memory usage. We conduct experiments to verify the effectiveness of our method, where our method reduces the memory usage by up to $30$% while preserving similar performance or even improving the performance of Adam.", "tldr": "", "keywords": ["Optimizer; Low rank adaptation"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f84cdc32500b8412ca26dbfc6e2f10d356b0324d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a memory-efficient variant of the popular Adam optimizer that exploits the structure of gradients computed through backpropagation. The proposed method uses a second-moment estimate that is lower bounded by the second-moment estimate of Adam. The authors show a regret bound for the proposed optimizer, patterned off the analysis of Adam. Empirical results on fine-tuning, pretraining, and vision tasks demonstrate a 30-40% reduction in memory usage while maintaining comparing performance to Adam. A generalization of the main method is also proposed and explored."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. To my knowledge, the proposed idea is novel and simple to understand, being well-motivated in the context of backpropagation.\n2. The memory reduction is significant, even compared to similar work on memory-efficient Adam variants.\n3. The generalization via Holder's inequality is quite interesting and promising. I found the intuition that $p<2$ should work better to be very insightful."}, "weaknesses": {"value": "1. The theoretical contribution is extremely limited, essentially consisting of only a straightforward extension of the analyses in [1, 2]. A regret bound is common in online learning or reinforcement learning contexts but not necessarily standard for modern optimizer papers. It would be helpful to provide e.g. objective value or gradient norm bounds in order to strengthen the theoretical value of this work.\n2. Given the primarily empirical nature of the contributions, the experiments should be held to the same standards as other empirical papers on optimizers, e.g. [3, 4, 5]. In particular, no evaluation is provided for larger models (7B+ parameters), making it difficult to conclude that the proposed method is scalable.\n3. It is hidden in the appendix that the proposed method takes ~7% more time per step on average than AdamW, and it is merely mentioned that this implementation could possibly be improved. This may or may not have a significant practical impact, but I would have like to see more discussion in the main text.\n\n[1] Diederik P. Kingma, Jimmy Ba. Adam: A Method for Stochastic Optimization, ICLR 2015.\n[2] Sashank J. Reddi, Satyen Kale, Sanjiv Kumar. On the Convergence of Adam and Beyond, ICLR 2018.\n[3] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, Yuandong Tian. GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection, ICML 2024.\n[4] Qijun Luo, Hengxu Yu, Xiao Li. BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models, NeurIPS 2024.\n[5] Tianjin Huang, Ziquan Zhu, Gaojie Jin, Lu Liu, Zhangyang Wang, Shiwei Liu. SPAM: Spike-aware Adam with Momentum Reset for Stable LLM Training, ICLR 2025."}, "questions": {"value": "1. In Line 772, it is mentioned that the analysis requires an additional assumption that was pointed out by Reddi et al. Could you clarify how this assumption is satisfied for the proposed method?\n2. See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OuKvZT2KOI", "forum": "iCT5xdOlJH", "replyto": "iCT5xdOlJH", "signatures": ["ICLR.cc/2026/Conference/Submission11844/Reviewer_3s2b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11844/Reviewer_3s2b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761443589416, "cdate": 1761443589416, "tmdate": 1762922862648, "mdate": 1762922862648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes IO-Adam, a “memory-efficient” variant of Adam that separately tracks the input activations $X$ and output gradients $\\nabla_Y \\mathcal{L}$ for each linear layer instead of storing full-size moment estimates. They re-express the weight gradient $\\nabla_W \\mathcal{L}=(\\nabla_Y \\mathcal{L})X^T$ and construct the second-moment estimate from the outer product of exponential moving averages of $X^2$ and $(\\nabla_Y \\mathcal{L})^2$, reducing memory costs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper introduces an explicit dual-buffer implementation that separately stores input and output gradients, which can slightly reduce memory usage in small-batch settings.\n\n2. The presentation of this paper is good."}, "weaknesses": {"value": "1. The method fails to save memory when applied to attention layers, since both $\\nabla_Y \\mathcal{L}$ and $X$ are high-dimensional matrices rather than vectors. In this case, storing the input and output gradient buffers actually requires more memory than storing the weight gradients themselves. Moreover, in most large language models, attention layers constitute the majority of parameters and computation, making this limitation particularly critical. In addition, they works well only if the batch size is small.\n\n2. If the method can only be applied to linear layers, it is unclear how the attention layers are handled in the LLaMA and GPT-2 experiments or how any memory savings are achieved in those cases.\n\n3. The buffer for the first momentum stores the input and output gradients from the most recent $c$ batches, while the buffer for the second momentum maintains a “ring buffer” of size $b$. Consequently, the first momentum and the second momentum are computed over different batches, resulting in a mismatch between the two moment estimates.\n\n4. The second momentum estimation can be highly biased, potentially far larger than the ground truth. There is no theoretical guarantee or mechanism to control this bias.\n\n5. The modified bias correction term $(1-\\beta_2^t)^2$ is introduced only because the authors multiply two EMAs. There is no theoretical analysis or empirical evidence supporting that this modification leads to unbiased or stable estimates.\n\n6. The experiment results are not convincing. Although IO-Adam exhibits a large second-moment bias, it still outperforms AdamW in all reported cases. The authors attribute this improvement to larger learning rates, which is an unconvincing and insufficient explanation."}, "questions": {"value": "1. Please define the variables $m$ and $n$ in Table 1.\n\n2. From Eq.2, the statement of “the rank of the weight gradient is equal to the batch size $bs$” is not right. The rank should be less and equal to batch size.\n\n3. No experimental details are given for the first-moment buffer. Please clarify."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pEHWgR8JP4", "forum": "iCT5xdOlJH", "replyto": "iCT5xdOlJH", "signatures": ["ICLR.cc/2026/Conference/Submission11844/Reviewer_Qe6v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11844/Reviewer_Qe6v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806754410, "cdate": 1761806754410, "tmdate": 1762922862196, "mdate": 1762922862196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a memory-efficient variant of the Adam optimizer, called IO-Adam. Specifically, for the second-moment estimation in Adam, this paper tracks the input vector and the output gradient for each layer. Since these are smaller than the full weight matrix, this approach significantly reduces memory cost. Experiments show that the proposed method is more memory-efficient than Adam, achieving around 30% reduction in memory usage. The paper also demonstrates that this tracking scheme estimates the second moment as an upper bound of Adam’s original second-moment estimate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper studies an important challenge—the high memory cost of training large models using second-order moment estimates—and effectively reduces memory usage compared to the original Adam.\n2) The motivation is clear, and the proposed method is reasonable. By separately tracking the input vector and output gradient, it efficiently estimates the momentum of weight gradients.\n3) Theoretical analysis and experimental results are clearly presented. The proposed method successfully reduces memory usage and achieves comparable or better performance than related baselines."}, "weaknesses": {"value": "1) According to the paper, memory storage depends on the batch size, meaning different batch sizes may lead to very different memory usage (larger batches may cause higher memory consumption compared to baselines). It is important to conduct a study evaluating performance and memory usage versus batch size.\n2) I have concerns that in standard batch training, the input distribution may change over time due to sampling bias. Since this method stores input vectors, for earlier layers of the network, such estimation may lead to large deviations as the input changes drastically. Could this cause unstable training in the early stages? Or is a larger batch size required to stabilize it?\n3) Another potential weakness is training time. This method may take longer per iteration compared to related works. It would be better to include total training convergence time and comparisons with baselines. If training time is significantly slower, the method’s practical usefulness may be limited.\n4) Minor parts:  \n4.1) The paper mainly focuses on training affine blocks; it would be useful to expand the scope to 2D CNNs.  \n4.2) The paper notes that the optimal learning rate of IO-Adam may be larger than that of Adam. It would be valuable to study the optimal learning rate in broader experiments and, if possible, suggest a scaling guideline (e.g., a factor like 1.5x) for practical application."}, "questions": {"value": "1) Could the authors conduct a study to evaluate performance and memory usage versus batch size?\n2) Given that input distributions change during training, especially in earlier layers, could this cause instability? Would a larger batch size mitigate it?\n3) What is the training time compared to baselines?\n4) How is IO-Adam implemented for CNNs, and what is the performance impact?\n5) Could the authors clarify how to determine or scale the learning rate?\n\nPlease see the [Weakness] section for more details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7zaLuyeDsw", "forum": "iCT5xdOlJH", "replyto": "iCT5xdOlJH", "signatures": ["ICLR.cc/2026/Conference/Submission11844/Reviewer_ohrE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11844/Reviewer_ohrE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879480045, "cdate": 1761879480045, "tmdate": 1762922861760, "mdate": 1762922861760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents IO-Adam, a memory-efficient variant of Adam that redefines how second moments are stored and updated. Instead of maintaining a full per-parameter second-moment matrix, IO-Adam tracks moving averages of squared input activations and squared output gradients, reconstructing an approximate second moment from their outer product. This change significantly reduces optimizer memory use while keeping similar convergence guarantees. The authors also introduce a buffer mechanism to stabilize estimates and a Hölder-based generalization to control the tightness of the approximation. Experiments on NLP and vision tasks show that IO-Adam matches or slightly improves AdamW’s performance while cutting optimizer memory by about 30–40 percent."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Novel and well-grounded idea.** The approach builds on the natural structure of gradient computation, offering a fresh and simple way to reduce memory without complex matrix factorization.\n* **Solid theory.** The paper shows that IO-Adam’s second-moment estimate upper bounds Adam’s and proves a matching regret bound, giving the method theoretical credibility.\n* **Comprehensive experiments.** Evaluations across GLUE, ViT, and large-scale LLaMA pretraining show consistent results with significant memory savings.\n* **Flexibility and extensions.** The buffer mechanism and the Hölder variant make the framework adaptable and potentially useful in broader settings.\n* **Clear practical value.** The optimizer delivers meaningful savings in large-scale training while preserving accuracy."}, "weaknesses": {"value": "* **Limited applicability.** The method only directly applies to linear layers. It is unclear how IO-Adam handles other parameter types such as convolutions, embeddings, or normalization layers. This limits general usability.\n* **Unclear memory accounting.** The paper’s claims of 30–40 percent savings lack a complete breakdown including the cost of buffers, first moments, and parameters that might fall back to Adam.\n* **Runtime overhead.** Tracking inputs and output gradients increases implementation complexity and can slow training. The paper does not quantify this overhead clearly.\n* **Hyperparameter sensitivity.** The buffer size and Hölder exponent are new and sensitive settings. Their tuning rules are not well explained.\n* **Limited statistical reporting.** Some performance gains appear small, with no standard deviation or multi-seed results to confirm significance."}, "questions": {"value": "1. How does IO-Adam handle non-linear or non-matrix parameters such as embeddings or normalization weights?\n2. Could the authors provide a full accounting of memory, including buffers, first moments, and fallback parameters?\n3. How much runtime overhead does IO-Adam introduce compared with AdamW in large-scale training?\n4. Is there a practical heuristic for choosing buffer size or Hölder exponent across tasks?\n5. Could this approach be extended to convolutional layers or combined with other low-rank or quantized optimizers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7nsnBgfS9q", "forum": "iCT5xdOlJH", "replyto": "iCT5xdOlJH", "signatures": ["ICLR.cc/2026/Conference/Submission11844/Reviewer_VV1t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11844/Reviewer_VV1t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762584905505, "cdate": 1762584905505, "tmdate": 1762922861016, "mdate": 1762922861016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}