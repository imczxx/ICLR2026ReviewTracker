{"id": "J4GYMiE3JT", "number": 20053, "cdate": 1758301922472, "mdate": 1759897004106, "content": {"title": "Structural Inference: Interpreting Small Language Models with Susceptibilities", "abstract": "We develop a linear response framework for interpretability that treats a neural network as a Bayesian statistical mechanical system. A small perturbation of the data distribution, for example shifting the Pile toward GitHub or legal text, induces a first-order change in the posterior expectation of an observable localized on a chosen component of the network. The resulting susceptibility can be estimated efficiently with local SGLD samples and factorizes into signed, per-token contributions that serve as attribution scores. We combine these susceptibilities into a response matrix whose low-rank structure separates functional modules such as multigram and induction heads in a 3M-parameter transformer.", "tldr": "Introduces susceptibilities to study the internal structure of language models", "keywords": ["Interpretability", "Statistical Physics", "Singular Learning Theory"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/753b9fca51c7832df63de1b9d0125f45c2de7f25.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces susceptibilities, a new interpretability framework grounded in Bayesian statistical mechanics that measures how specific neural network components respond to controlled changes in the data distribution. By estimating these susceptibilities locally using SGLD, the authors define a notion of expression vs. suppression for model components and construct response matrices that reveal internal structure. Applied to a 3M-parameter transformer trained on the Pile, the method automatically separates known functional modules such as the induction circuit and identifies linguistic patterns like word segmentation and bracket matching."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work introduces a principled and novel framework for understanding the internal computations of deep neural networks, drawing inspiration from Bayesian statistical mechanics and singular learning theory.\n2. The proposed methodology offers an unsupervised pipeline for discovering functional circuits within neural networks.\n3. It not only provides empirical results demonstrating the effectiveness of the framework, but also includes sanity checks to ensure its practical value in understanding transformer-based language models.\n4. The approach establishes strong connections to physics, which may encourage researchers in the field to contribute to the study of the interpretability of neural networks."}, "weaknesses": {"value": "1. Although the theoretical framing is elegant, its practical usefulness remains uncertain. The empirical evaluation focuses only on a small toy model with two attention layers, which is not representative of typical language models.\n   - Training on only a subset of the Pile dataset may be too simplistic, potentially explaining why the first principal component accounts for  95–99% of the representational variance.\n2. The interpretation of principal components remains largely manual and speculative, which may lead to inconsistent or subjective interpretations of the empirical findings.\n3. The proposed susceptibility framework captures correlations in the local loss landscape, in contrast to many mechanistic interpretability approaches that emphasize causal relationships.\n4. Finally, the paper requires expertise in Bayesian statistics, which may limit accessibility and reduce its potential audience."}, "questions": {"value": "1. The empirical results in Section 4.2 show that the first principal component (PC1), interpreted as “word segmentation,” accounts for an overwhelming portion of the variance (95.3% in Layer 0 and 99.1% in Layer 1). Could the authors clarify how the proposed method can reveal more complex circuits when the learned representations appear to be dominated by such a simple feature?\n2. Since susceptibility is fundamentally correlational, did you conduct any causal validation, such as steering or ablating the susceptibility-identified heads, to assess their influence on the model’s outputs?\n3. Appendix C.5 presents a cost model that scales linearly with the number of components. While this may be feasible when analyzing only attention heads, it seems impractical once MLP neurons are included. Do you plan to extend the framework to investigate MLP neurons as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xoyGJot82m", "forum": "J4GYMiE3JT", "replyto": "J4GYMiE3JT", "signatures": ["ICLR.cc/2026/Conference/Submission20053/Reviewer_Wjna"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20053/Reviewer_Wjna"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761512415065, "cdate": 1761512415065, "tmdate": 1762932949411, "mdate": 1762932949411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a new interpretability framework grounded in statistical physics and Bayesian learning theory. The key idea is to view a neural network as a Bayesian statistical mechanical system, where the model’s parameters and interactions can be explored through small perturbations of the data distribution. Small shifts in the input data (like moving from natural text to programming code) produce first-order linear responses in specific components of the model. These responses (called susceptibilities) reveal how strongly and in what direction each component reacts to data changes, providing a principled way to quantify its sensitivity and functional role within the network."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The framework is theoretically rigorous, grounded in statistical physics and Bayesian learning theory, while also being empirically validated through concrete experiments.\n\n* The proposed approach is novel:  Susceptibility analysis connects the functional behavior of model components to shifts in the training distribution and shows that heads with similar response patterns cluster into interpretable groups."}, "weaknesses": {"value": "* The analysis (Sec 4) focuses on a very simple set of patterns (Word Start, Word Part, Word End, Induction Pattern, Right Delimiter). In addition, the framework is demonstrated only on a small toy language model (3M, 2 attention layers, without MLP). Although the authors note that they do not anticipate major obstacles in scaling the method to larger models, applying it to a larger model could have strengthened the work by demonstrating the ability to capture more complex behaviors.\n\n* This is only a suggestion, but I think the presentation could be made more accessible to readers who are less familiar with the theoretical background."}, "questions": {"value": "* as I mentioned in the weaknesses section, I think the paper would be stronger if it demonstrated how the proposed method could be applied to analyze more complex patterns or behaviors (such as bias or knowledge acquisition)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oEDHO9EWJU", "forum": "J4GYMiE3JT", "replyto": "J4GYMiE3JT", "signatures": ["ICLR.cc/2026/Conference/Submission20053/Reviewer_B7pi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20053/Reviewer_B7pi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781671488, "cdate": 1761781671488, "tmdate": 1762932948768, "mdate": 1762932948768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Top level comment"}, "comment": {"value": "Several reviewers asked about scaling to larger models. This is partly addressed in Appendix C.5 but we include here some more detailed remarks. There are two main obstacles to scaling the results in the paper: (i) the computational cost of SGLD sampling and evaluating losses to compute susceptibilities, and (ii) the complexity of larger models.\n\nRegarding (i) there are two main computational costs in our method: (ia) SGLD sampling and (ib) evaluating forward passes to compute losses across our set of tokens at a given SGLD sample. \n\n(ia) It has already been established that SGLD sampling for local learning coefficient (LLC) estimation can be done at scale, see E. Urdshals et al “Compressibility measures complexity: Minimum description length meets singular learning theory” https://arxiv.org/pdf/2510.12077) which studies Pythia models up to 6.9B (see Figure 3 in loc. cit.).\n\n(ib) Computing susceptibilities is more costly than LLC estimation due to the following factors: the number of model components (each of which has its own posterior that needs separate SGLD sampling) and the number of contexts (each of which requires a forward pass at a given SGLD sample in order to compute losses, to put into the formula for the estimated covariance). Apart from the SGLD sampling this scaling behaviour is similar to e.g. circuit discovery via ablations, which also requires evaluating forward passes for a set of model components that are being ablated.\n\nAs models scale the memory requirements mean that these tasks need to be parallelised across multiple GPUs, but this is done in the standard way and is well-supported. Overall there are no theoretical or practical obstacles to scaling to 10B and the costs are not prohibitive (estimates are provided for Pythia 1.4B in Appendix C.5). \n\nRegarding (ii) we agree the difficulties here are more serious. The small transformers studied in the present paper are simple, and so the patterns in the data and corresponding structures in the model are remarkably visible in the susceptibility matrix. We do not expect PCA on its own to be sufficient to discover interesting structure in larger models, which “understand” more complex patterns. In these settings we also make use of non-linear dimension reduction techniques like UMAP, and more sophisticated data analysis to study the internal structure in models. The present paper should be viewed as validation that simple data analysis, in simple models, reveals simple patterns; we agree that it remains an open question (to be addressed in future work) whether more complex data analysis reveals complex structures in larger models.\n\nFor intuition, our method should be thought of as somewhat comparable to EK-FAC for data attribution: while it is computationally nontrivial to estimate Hessian inverses in large models and the results are not straightforward to interpret, this is routinely done (see e.g. R. Grosse et al “Studying large language model generalization with influence functions”) because the information gained is believed to be worth the cost."}}, "id": "vFyYFTUaQZ", "forum": "J4GYMiE3JT", "replyto": "J4GYMiE3JT", "signatures": ["ICLR.cc/2026/Conference/Submission20053/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20053/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20053/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763668947480, "cdate": 1763668947480, "tmdate": 1763668947480, "mdate": 1763668947480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors provide a framework for approximating the effect of a small change in the training data on model components, called susceptibilities. They show that this is equivalent to the covariance between a component’s loss (in this case, attention heads) and the change in total loss under the perturbed data distribution. They then estimate, using locally sampled posteriors with SGLD, susceptibilities for loss on each pair of tokens in the vocabulary, for all attention heads in a 3M-parameter transformer, trained on subsets of Pile. The top principal components of this combined covariance matrix are sensitive to certain classes of interpretable token patterns, such as the induction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The assumptions are clearly stated, and the theory seems well grounded. \n2. Applying susceptibilities to attention heads yields interpretable patterns, previously found in small attention-only transformers via mechanistic interpretability."}, "weaknesses": {"value": "1. Although novel, this attribution method seems very hard to scale to larger models. \n2. The heads that express or suppress the reported patterns in Figure 2 are not yet mechanistically explained. The correlation with Direct Logit Attribution (Figure 32) shows no sign that they are actually responsible for the behaviour. \n3. Relaxing sampling to the local posterior makes sense for small changes, but it is unclear if this holds when 10% of the data is replaced. It would be great if this could be clarified."}, "questions": {"value": "1. Could the authors explain why $\\delta h = 0.1$ is justified? And additionally, compute $\\chi$ for range of values of $\\delta h$ to check how different the susceptibilities are?\n2.  How to interpret the functional roles of these heads with respect to susceptibilities, given that the DLA plot doesn't show any correlation? One could conduct causal ablations on a few samples as a sanity check, but it is unclear if it would be correlated, given that DLA shows no signs of life. Does this method actually find a causal, induction circuit?\n\nIt would be great if the authors could address these two points in the paper (or point me to it, if it already exists). \n\nSome writing suggestions:\n1. A notation table (and maybe an algorithm table) would be extremely helpful as the paper is quite mathematically dense. \n2. Add some more description about the key findings. Eg: suppression and expression in the introduction"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "J0la5pvali", "forum": "J4GYMiE3JT", "replyto": "J4GYMiE3JT", "signatures": ["ICLR.cc/2026/Conference/Submission20053/Reviewer_KJxa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20053/Reviewer_KJxa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003587994, "cdate": 1762003587994, "tmdate": 1762932947788, "mdate": 1762932947788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a novel interpretability framework for transformer networks through susceptibilities. The authors investigate how much do variations in the data distribution influence specific components (i.e. attention heads) of the network. The authors highlight two interesting patterns of behavior identified by a standardized notion of susceptibility, resulting from a change in the parameters, namely expression (loss decreases, probability of next token increases) and suppression (loss decreases, probability of next token decreases). Through experiments on a two-layer attention-only Transformer trained on a subset of the Pile, these patterns are related to existing findings from mechanistic interpretability, rediscovering the induction head, as well as identifying how simple linguistic capabilities are conducted within the network. \n\nOverall, the paper presents a strong mathematical foundation and a novel approach for uncovering mechanistic patterns within transformer networks. The paper is well written, albeit the math is quite dense. My main concerns with the paper are the method scaling to larger models, which the authors proactively address in Appendix C.5, but I believe remains a limitation of their work. Finally, I am slightly concerned how the method should be applied in cases where there is no clear next-token point where susceptibility should be estimated (open-ended generation tasks), as the studied patterns are relatively simple to identify from data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strong mathematical foundation\n- Provides an empirically validated novel method for discovering mechanistic patterns within transformer networks"}, "weaknesses": {"value": "- Computational costs when scaling (mentioned as a weakness)\n- Applicability beyond simple tasks"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kYdQ4J9W1m", "forum": "J4GYMiE3JT", "replyto": "J4GYMiE3JT", "signatures": ["ICLR.cc/2026/Conference/Submission20053/Reviewer_Cmff"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20053/Reviewer_Cmff"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762169622936, "cdate": 1762169622936, "tmdate": 1762932946624, "mdate": 1762932946624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}