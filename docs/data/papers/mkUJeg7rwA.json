{"id": "mkUJeg7rwA", "number": 23475, "cdate": 1758344360891, "mdate": 1759896812707, "content": {"title": "Intra-Prompt Parallel Decoding for Common-Context Question Answering", "abstract": "In common-context question answering (CCQA) tasks, multiple questions share a common context to base their answers from. However, Large Language Models (LLMs) typically generate each answer using an independent prompt. While existing batching and caching techniques help improve parallelism and reduce repeated computations, the separation of questions across prompts limits the achievable speedup, as modern GPUs are underutilized due to a memory bottleneck during attention. We present Intra-Prompt Parallel Decoding (IPPD), a novel inference method that answers multiple common-context questions in parallel within a single prompt. IPPD directly addresses the bottleneck by efficiently sharing both memory and computation during the attention process, as the next token for every question is decoded in a single inference step. IPPD uses virtual position IDs and attention mask manipulation to generate the same output as standard prompting without requiring fine-tuning or any changes to the LLM architecture. Since all parallelism occurs within a prompt, IPPD is fully compatible with batched inference, even when each prompt features a different context. Our experiments show that IPPD delivers up to 7X the effective throughput as standard decoding without quality degradation, and outperforms state-of-art inference acceleration methods on real-world datasets.", "tldr": "Intra-Prompt Parallel Decoding (IPPD) lets LLMs answer multiple questions from the same context simultaneously within a single prompt, significantly boosting throughput.", "keywords": ["Parallel Decoding", "Efficient Inference", "Question Answering", "Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37e95b4e38a9a43e8bb0ced809779445c924b797.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on accelerating common-context question answering, where a shared context (such as a document) is followed by multiple independent questions. The paper introduces Intra-Prompt Parallel Decoding (IPPD).\nIPPD combines multiple questions sharing a common context into a single, structured prompt. By setting position_ids and attention_mask, IPPD decodes all questions in parallel.\nThe authors modify existing benchmarks, such as NarrativeQA and SQuAD 2.0, to fit the above structure format, thereby accelerating their evaluation. The authors say that IPPD achieves up to a 7x throughput improvement over standard batched inference and outperforms Prefix-Caching+PagedAttention."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The underlying logic is sound: Users may ask an LLM multiple questions in a single API call.\n2. The experimental results are robust: the experiments show that IPPD outperforms the baselines listed in the paper."}, "weaknesses": {"value": "1. Limited Practical Applicability due to Input Formatting Assumptions: The paper does not specify how this structured input is created from natural user queries. For instance, when a user submits a prompt such as \"Given this report, please summarize the key findings, list the involved parties, and tell me the final conclusion,\" there is currently no proposed mechanism to parse it into the three distinct questions required by IPPD. Without a defined parsing or pre-processing step, the method is confined to offline batch processing of already-structured datasets and is not directly usable in interactive or conversational systems where user input is unstructured. Although the authors claim to only focus on offline scenarios in the introduction, the scenarios are quite limited.\n2. Insufficient Experimental Comparison and Novelty Concerns: Cascade Inference is a highly relevant baseline that also targets the shared-prefix, multi-question scenario. The paper discusses Cascade Inference in Appendix A.2.3, acknowledging that it has a similar objective of reducing memory access. However, the paper lacks a direct experimental comparison, and its introduction section provides only a vague description of cascade inference. This makes it difficult to assess the true novelty and relative advantages of the proposed IPPD approach."}, "questions": {"value": "1. Suggest adding a subsection to the methodology or discussion that addresses the pre-processing of unstructured user queries."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cLwhSL4uYD", "forum": "mkUJeg7rwA", "replyto": "mkUJeg7rwA", "signatures": ["ICLR.cc/2026/Conference/Submission23475/Reviewer_PotS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23475/Reviewer_PotS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760583094054, "cdate": 1760583094054, "tmdate": 1762942676104, "mdate": 1762942676104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the intra-prompt parallel decoding (IPPD) for common-context question answering (CCQA). This method generates multiple answers in parallel by constructing a compound prompt that includes multiple prompts with shared context, by assigning the correct attention mask."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The evaluation includes multiple QA datasets (NarrativeQA, SQuAD 2.0, RACE and LongHealth)."}, "weaknesses": {"value": "- The proposed method is well-known in the literature."}, "questions": {"value": "The proposed IPPD method is common practice in the literature.\nPlease refer to the following papers that have already proposed similar methods:\n1. SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification\n2. DeFT: Decoding with Flash Tree-Attention for Efficient Tree-structured LLM Inference\n3. FastTree: Optimizing Attention Kernel and Runtime for Tree-Structured LLM Inference\n4. FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding\n\nThe following are some more details about these related works:\nThe author discuss the differences between the proposed IPPD method and prefix caching in section 2, which is appreciated.\nThe author claim the major novelty of IPPD compared to prefix caching is that it can generate multiple answers of multiple questions in parallel, where prefix caching need to recompute the attention scores and only reuse the KV cache.\nHowever, many more other works have studied the parallel decoding of requests with shared context, the CCQA task is just one of the many applications of this idea (a two depth tree with one shared root).\nBesides, these works also explore more advanced techniques to further optimize the attention computation, which can serve as a stonger baseline to compare with in this work.\n\n(Minor) I also find the inference experiment setting confusing. The author turn off the asynchronous decoding feature of vLLM since the baseline methods do not support it.\nHowever, this feature is indeed esential to speed up the decoding when there are multiple requests with shared context and I can't see the reason why the baseline methods cannot support it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YhXQBbQFnB", "forum": "mkUJeg7rwA", "replyto": "mkUJeg7rwA", "signatures": ["ICLR.cc/2026/Conference/Submission23475/Reviewer_1iVT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23475/Reviewer_1iVT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603442302, "cdate": 1761603442302, "tmdate": 1762942675895, "mdate": 1762942675895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Intra Prompt Parallel Decoding (IPPD) proposes packed matrix multiplication computation to speed up jagged prefill and decode requests."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "I think IPPD is quite efficient when the questions and decoding parts are brief, because we do not need to implement a jagged tensor, which often results in lower utilization of TensorCores (matrix multiplication accelerators).\n\nThe method is very intuitive and simple."}, "weaknesses": {"value": "I think this method is only effective when the question (later part prompt) and the answer are extremely short (<128).\nHowever, such a scenario is extremely rare in the agentic AI era. Therefore the effective-ness of this method is pretty limited."}, "questions": {"value": "### Questions\n- What if the decoding length is long? e.g., reasoning models\n- What if prefill is always large? e.g., tool calling\n\n### Formattings\nCan you update the figures to make them more intuitive about your method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CfQHCmbLoR", "forum": "mkUJeg7rwA", "replyto": "mkUJeg7rwA", "signatures": ["ICLR.cc/2026/Conference/Submission23475/Reviewer_uN7x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23475/Reviewer_uN7x"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937542256, "cdate": 1761937542256, "tmdate": 1762942675702, "mdate": 1762942675702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}