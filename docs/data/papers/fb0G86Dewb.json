{"id": "fb0G86Dewb", "number": 21230, "cdate": 1758315151581, "mdate": 1759896933143, "content": {"title": "RewardEval: Advancing Reward Model Evaluation", "abstract": "Reward models are used throughout the post-training of language models to capture nuanced signals from preference data and provide a training target for optimization across instruction following, reasoning, safety, and more domains. The community has begun establishing best practices for evaluating reward models, from the development of benchmarks that test capabilities in specific skill areas to others that test agreement with human preferences. At the same time, progress in evaluation has not been mirrored by the effectiveness of reward models in downstream tasks -- simpler direct alignment algorithms are reported to work better in many cases. This paper introduces RewardEval, a new multi-skill reward modeling benchmark designed to bring new, challenging data for accuracy-based reward model evaluation -- models score about 20 points on average lower on RewardEval compared to RewardBench, a widely-used existing reward model evaluation-- while being highly correlated with downstream performance. Compared to most other benchmarks, RewardEval sources new human prompts instead of existing prompts from downstream evaluations, facilitating more rigorous evaluation practices. In this paper, we describe our benchmark construction process and report how existing models perform on it, while quantifying and providing new insights on how performance on the benchmark correlates with downstream use of the models in both inference-time scaling algorithms, like best-of-N sampling, and RLHF training algorithms like proximal policy optimization.", "tldr": "", "keywords": ["reward models", "benchmark", "evaluation", "post-training", "reinforcement learning from human feedback"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac1b8b5067e2d03c9cb74df986df14a18ff6249a.pdf", "supplementary_material": "/attachment/e7120afdde90f0166a25763986c48090ecbcb20a.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces RewardEval, a benchmark and methodology for evaluating reward models. It provides a standardized, interpretable, and scalable way to assess how well reward models capture human preferences across diverse tasks. Compared with RewardBench, RewardEval focuses on unseen, in-the-wild human prompts sourced from WildChat and applies decontamination to avoid overlap with common downstream evaluations. It also includes analyses of reward-model-guided best-of-n sampling and reinforcement learning from human feedback (RLHF). In practice, RewardEval’s scores show strong correlation with best-of-n downstream performance and reveal an important RLHF insight: for PPO, the alignment and distributional match between the policy and the reward model are critical, and high RewardEval scores alone do not guarantee PPO improvements when the reward model is off-policy or out-of-distribution. Overall, RewardEval serves as a more challenging and downstream-relevant successor to RewardBench, with particular emphasis on instruction following, math, and factuality, areas where many leading reward models continue to struggle."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Presents a well-designed and comprehensive benchmark that goes beyond pairwise preference accuracy to capture more realistic aspects of reward model performance.\n2. Uses unseen, in-the-wild human prompts and diverse domains, improving robustness and reducing contamination compared to prior benchmarks like RewardBench.\n3. Provides insightful analyses on best-of-n sampling and RLHF, highlighting practical implications of distribution and policy mismatch in reward-guided optimization.\n4. Demonstrates strong empirical validation, with RewardEval scores correlating closely with downstream performance.\n5. Offers clear presentation, transparent methodology, and open-source resources that enhance reproducibility and long-term research impact."}, "weaknesses": {"value": "1. The paper feels somewhat incremental compared to RewardBench, as it builds upon a similar benchmarking foundation. Although it includes additional analyses on reward-model-guided training and inference, these studies are not comprehensive enough to establish deeper or more general conclusions.\n2. The analysis of RLHF training dynamics is limited to experiments using the TULU 3 8B model, which restricts the generality of the reported insights across architectures and scales.\n3. Additional evaluation dimensions such as reward model robustness and reward hacking resistance would be highly valuable to the community. In practical RLHF setups, reward models often become ineffective after short on-policy training, as the policy quickly learns to exploit their weaknesses. This limitation is also reflected in the paper’s own findings (Section 5.2), where all evaluated reward models, regardless of their RewardEval scores, lead to similar final policy performance after RL training. Addressing this issue directly would make the benchmark far more impactful and less incremental compared to RewardBench."}, "questions": {"value": "1. How do you plan to extend RewardEval to better capture reward model robustness and resistance to reward hacking? Given that on-policy RL training often leads to rapid overoptimization and degradation of reward signal quality, have you considered incorporating adversarial or on-policy evaluation settings into the benchmark?\n2. In Section 5.2, you observe that all reward models, regardless of their RewardEval performance, produce similar final outcomes after PPO training. Could you elaborate on whether this suggests that current reward model quality is not the primary bottleneck in RLHF, or that the RL optimization dynamics overpower the reward signal?\n3. The analysis of RLHF training dynamics is based solely on the TULU 3 8B model. Do you expect similar trends for other model families? and do you have preliminary evidence to support that expectation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pNyr7c4dyZ", "forum": "fb0G86Dewb", "replyto": "fb0G86Dewb", "signatures": ["ICLR.cc/2026/Conference/Submission21230/Reviewer_XmJn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21230/Reviewer_XmJn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761154490051, "cdate": 1761154490051, "tmdate": 1762941639345, "mdate": 1762941639345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces REWARDEVAL, a large-scale benchmark for evaluating reward models (RMs) used in RLHF and inference-time selection (e.g., best-of-N sampling). The benchmark spans six domains—three familiar ones (Focus, Math, Safety) and three new ones (Factuality, Precise Instruction Following, and Ties). It is constructed from unseen, high-quality human prompts, with four candidate completions per prompt, enabling more granular accuracy measurement and margin-based calibration testing. The authors train 120 Bradley-Terry reward models and show that REWARDEVAL correlates strongly with downstream PPO and BoN performance, while also revealing that RM–policy lineage alignment is crucial for stable RLHF outcomes. Compared to prior datasets such as RewardBench and PPE, REWARDEVAL claims to provide cleaner, harder, and more diagnostic evaluations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Ambitious, comprehensive benchmark spanning six diverse domains.\n\n- Strong empirical study with over a hundred RMs and multiple baselines.\n\n- Identification of practical phenomena such as the importance of model lineage in RLHF.\n\n- Systematic comparison to RewardBench, PPE, and other RM datasets clarifies positioning.\n\n- High reproducibility through public release and clear experimental pipeline."}, "weaknesses": {"value": "- The Ties metric lacks invariance to scaling/temperature and may over-penalize calibrated models.\n\n- Domain averaging ignores differing sample sizes, reducing statistical interpretability.\n\n- Heavy reliance on LLM-as-judge for factuality/safety labels introduces label bias and potential leakage.\n\n- Correlation analyses are based on a single policy distribution, limiting generality.\n\n- “Stronger correlation with BoN” may partially stem from shared data lineage rather than intrinsic benchmark quality.\n\n- No formal error analysis or confidence intervals are reported.\n\nThese weaknesses do not invalidate the idea but suggest that the benchmark’s mathematical rigor and external validity remain limited."}, "questions": {"value": "1. How robust are the REWARDEVAL correlations when evaluated on policies outside the Tulu family (e.g., Mistral or Llama3 or any other family)?\n\n2. Can the authors provide a scale-invariant version of the Ties metric (e.g., based on ranking or normalized variance)?\n\n3. Were the factuality and safety labels cross-checked with independent human annotators to mitigate LM-as-judge bias?\n\n4. Could domain weighting or bootstrapped confidence intervals be added to report uncertainty in the overall score?\n\n5. How do results change if the number of completions (`N`) increases beyond four?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WNQdB58fAE", "forum": "fb0G86Dewb", "replyto": "fb0G86Dewb", "signatures": ["ICLR.cc/2026/Conference/Submission21230/Reviewer_nSPY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21230/Reviewer_nSPY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797838409, "cdate": 1761797838409, "tmdate": 1762941637071, "mdate": 1762941637071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces REWARDEVAL, a new multi-skill benchmark aimed at evaluating reward models, a key contribution of REWARDEVAL is its use of newly collected, unseen human-authored prompts, rather than reusing prompts from downstream evaluation datasets. This deliberate decontamination strategy ensures the benchmark provides a clean, unbiased evaluation of reward model generalization and prevents data leakage from overlapping with RLHF or inference datasets. It also demonstrates strong correlation with downstream performance, including in best-of-N sampling and PPO-based RLHF training, highlighting its value as a predictive diagnostic tool for real-world effectiveness. Beyond benchmarking, the paper offers actionable insights for improving reward model training, for example, finding that training for more than one epoch can enhance performance in certain regimes, counter to common assumptions in preference model fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a well-designed benchmark that significantly improves upon RewardBench by introducing unseen, human-written prompts to ensure data decontamination and incorporating new task categories for broader coverage. It conducts comprehensive experiments analyzing correlations with downstream tasks, yielding several insightful findings: (1) combining multiple data sources enhances average performance, (2) the choice of base model influences reward model effectiveness, and (3) training reward models for multiple epochs does not inherently degrade downstream performance, challenging common assumptions. The structure is clear and experiments are well-thought, and the insights are easy to understand backed by comprehensive experiments."}, "weaknesses": {"value": "\"For RLHF, the reward model should be based on a model of the same lineage as the policy\nmodel or else downstream performance can degrade significantly, so simply taking the\nhighest scoring reward model on a benchmark will not ensure a good post RLHF model.\" this seemed to be a very strong statement, I don't see experiments conducted across various model types, a study on different architectures might be beneficial or make this statement less affirmative might be a better consideration?"}, "questions": {"value": "Do we have results to backup this claim : \"the reward model should be based on a model of the same lineage as the policy model or else downstream performance can degrade significantly\"? Sorry I didn't find it easily in paper? And I assume it means the policy model needs to be the same as the reward model? I don't see a table indicates that correct me if I am wrong."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0jBFvOo6Hp", "forum": "fb0G86Dewb", "replyto": "fb0G86Dewb", "signatures": ["ICLR.cc/2026/Conference/Submission21230/Reviewer_PtnZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21230/Reviewer_PtnZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807043164, "cdate": 1761807043164, "tmdate": 1762941636747, "mdate": 1762941636747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Introduces a tougher reward model benchmark with six domains, a best-of-4 selection format that lowers the random baseline to 25%, and mostly unseen human prompts to reduce contamination.\n- Shows strong correlation with best-of-N sampling and highlights that transfer to PPO depends on on-policy or lineage-matched reward models.\n- Reports that top models score notably lower than on prior benchmarks, indicating increased difficulty and headroom.\n- Provides practical training insights, including benefits from more than one epoch and lineage matching for RLHF."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Principled evaluation design with a lower random baseline that better matches downstream selection.\n- Comprehensive domains, including calibration via ties, and strong empirical validation against best-of-N.\n- Scaled experiments across many trained and existing RMs yield practical insights.\n- Clear practitioner guidance on training and deployment."}, "weaknesses": {"value": "- Candidate pool may bias difficulty and favor models similar to generators.\n- Mixed metrics across domains, with the ties metric blending correctness and calibration, can reduce comparability.\n- Limited policy diversity and small subsets in places may restrict generality and statistical power.\n- Heavy reliance on frontier models for filtering could introduce systematic biases."}, "questions": {"value": "- How stable are rankings if the best-of-4 candidate set is regenerated with a different generator pool or temperatures?\n- Can lineage matching be quantified more continuously to predict PPO transfer beyond a binary on-policy label?\n- What is the computational cost tradeoff of best-of-4 compared to pairwise setups for broad adoption?\n- How robust is the ties subset to subtle quality differences and score distribution shifts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rBhldC6i2u", "forum": "fb0G86Dewb", "replyto": "fb0G86Dewb", "signatures": ["ICLR.cc/2026/Conference/Submission21230/Reviewer_rc4h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21230/Reviewer_rc4h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896830070, "cdate": 1761896830070, "tmdate": 1762941636433, "mdate": 1762941636433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}