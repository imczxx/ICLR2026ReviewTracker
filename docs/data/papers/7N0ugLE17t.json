{"id": "7N0ugLE17t", "number": 13387, "cdate": 1758217296316, "mdate": 1759897440952, "content": {"title": "Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning", "abstract": "Existing reinforcement learning (RL) methods struggle with complex dynamical systems that demand interactions at high frequencies or irregular time intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by replacing discrete-time Bellman recursion with differentiable value functions defined as viscosity solutions of the Hamilton–Jacobi–Bellman (HJB) equation. While CTRL has shown promise, its applications have been largely limited to the single-agent domain. This limitation stems from two key challenges: (i) conventional methods for solving HJB equations suffer from the curse of dimensionality (CoD), making them intractable in high-dimensional systems; and (ii) even with learning-based approaches to alleviate the CoD, accurately approximating centralized value functions in multi-agent settings remains difficult, which in turn destabilizes policy training. In this paper, we propose a CT-MARL framework that uses physics-informed neural networks (PINNs) to approximate HJB-based value functions at scale. To ensure the value is consistent with its differential structure, we align value learning with value-gradient learning by introducing a Value Gradient Iteration (VGI) module that iteratively refines value gradients along trajectories. This improves gradient accuracy, in turn yielding more precise value approximations and stronger policy learning. We evaluate our method using continuous‑time variants of standard benchmarks, including multi‑agent particle environment (MPE) and multi‑agent MuJoCo. Our results demonstrate that our approach consistently outperforms existing continuous‑time RL baselines and scales to complex cooperative multi-agent dynamics.", "tldr": "This paper leverages physics-informed neural networks combined with value gradient iteration to deal with continuous-time multi-agent reinforcement learning problems.", "keywords": ["Continuous-time", "multi-agent reinforcement learning", "physics-informed neural networks"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aaa45785a4bc90af0b2719b0a75e70c5398bdc6b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a continuous-time multi-agent reinforcement learning (CT-MARL) framework called VIP, which leverages PINNs to approximate value functions as solutions to HJB. \nTo address inaccuracies in value gradients that can destabilize policy learning in multi-agent settings, the authors introduce a Value Gradient Iteration (VGI) module that iteratively refines gradients along trajectories, improving value approximation and overall performance.\nOverall, the paper extends continuous-time RL to multi-agent cooperative scenarios, demonstrating good performance over baselines while mitigating the curse of dimensionality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The integration of PINNs with a VGI module is a fresh approach to CT-MARL, addressing key challenges like the curse of dimensionality and gradient inaccuracies in high-dimensional multi-agent systems.\n\n- Provides solid mathematical foundations, including lemmas and theorems with proofs.\n\n- Demonstrates genuine advantage of CT training under varying $\\Delta t$ and nice stress test against a competitive DT agent."}, "weaknesses": {"value": "- VGI targets rely on accurate dynamics and reward models, if they have a little difference, the estimation may differs a lot and fails.\n\n- VGI convergence relies on bounded Jacobians and small $\\Delta t$; it’s not fully clear how sensitive VIP is when these conditions are loosened\n\n- Discrete-time MARL baselines are excluded from the main comparisons."}, "questions": {"value": "- How sensitive is VGI to bias/variance in dynamics and rewards? Have you tried perturbing these models or training with deliberately misspecified dynamics to see VIP’s robustness envelope?\n\n- How does VIP behave with very small/large variable $\\Delta t$? Any adaptive-step control in the algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "zOptAK5usf", "forum": "7N0ugLE17t", "replyto": "7N0ugLE17t", "signatures": ["ICLR.cc/2026/Conference/Submission13387/Reviewer_b9mm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13387/Reviewer_b9mm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761036078729, "cdate": 1761036078729, "tmdate": 1762924026913, "mdate": 1762924026913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a **Continuous-Time Multi-Agent Reinforcement Learning (CT-MARL)** framework, termed **VIP (Value Iteration via PINN)**, which formulates MARL in the continuous-time domain.\nThe authors argue that conventional discrete-time RL (DTRL) methods struggle when applied to systems requiring high-frequency decision-making or irregular time intervals. To address this, VIP replaces discrete Bellman recursion with differentiable value functions that satisfy the **Hamilton–Jacobi–Bellman (HJB)** equations.\n\nKey contributions include:\n\n1. Leveraging **Physics-Informed Neural Networks (PINNs)** to approximate differential value functions governed by HJB PDEs.\n2. Introducing a **Value Gradient Iteration (VGI)** module to refine gradient estimation and stabilize value learning.\n3. Evaluating VIP on continuous-time versions of MPE and multi-agent MuJoCo environments, demonstrating improved sample efficiency and performance over existing continuous-time baselines (e.g., ODE-based RL, HJBPPO, DPI, and IPI)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Timely and ambitious topic:**\n  The paper tackles the underexplored area of continuous-time MARL, aiming to bridge PDE-based control theory and deep RL.\n\n* **Interesting methodological combination:**\n  Integrating PINNs into an actor–critic framework and adding a value-gradient refinement mechanism (VGI) reflects a creative blend of ideas from model-based RL and physics-informed learning.\n\n* **Comprehensive derivations:**\n  Theoretical background and mathematical formulations (e.g., derivations of HJB, VGI, convergence proofs) are detailed and rigorous.\n\n* **Empirical effort:**\n  Experiments on both MPE and multi-agent MuJoCo show that VIP achieves improved training stability and faster convergence than existing continuous-time baselines.\n\n* **Potential significance:**\n  The exploration of continuous-time MARL could inspire future research directions that better align RL with real-world physical systems and continuous control."}, "weaknesses": {"value": "Despite its conceptual ambition, the paper faces several **foundational and empirical limitations**, which prevent it from reaching ICLR acceptance standards.\n\n### (1) Motivation and conceptual clarity\n\nThe motivation for formulating MARL in continuous time is **not well-justified**.\nWhile the authors mention irregular time intervals as a motivation, the distinction between *continuous-time Markov chains (CTMC)*, *continuous control with continuous action spaces*, and *discrete-time approximations* is **not clearly articulated**.\nIt remains unclear **why a CTMC-based formulation is fundamentally necessary**, and what benefits it provides over well-established discrete-time methods with sufficiently small time steps. The experimental tasks are sufficient using the discrete-time methods enough.\nThis conceptual gap makes the overall significance of the work questionable.\n\n### (2) Insufficient discussion on relation to model-based MARL\n\nThe proposed VIP framework involves explicit learning of **a dynamics model** ( f_\\psi(x, u) ) and a **reward model** ( r_\\phi(x, u) ), followed by value and policy optimization — characteristics more aligned with **model-based RL** rather than model-free methods.\nHowever, the paper **does not compare or discuss** how VIP relates to representative **model-based multi-agent RL** or **model predictive control (MPC)** approaches (e.g., MA-MBRL, MA-MPC, or recent differential game solvers).\nThis omission weakens the positioning of the work.\n\n### (3) Limited baseline comparisons\n\nExperimental baselines are limited to other continuous-time algorithms (e.g., HJBPPO, DPI, IPI).\nHowever, for a fair evaluation in MARL, it is crucial to include **widely recognized baselines** such as **SMPE**[1], **Revisiting Off-policy MARL**[2], and **RACE**[3], which remain standard for multi-agent benchmarks like MPE and MuJoCo.\nWithout such comparisons, it is difficult to assess whether improvements stem from the continuous-time formulation or from differences in model complexity.\n\n[1]SMPE: Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration. ICML 2025\n\n[2]Revisiting Cooperative Off-Policy Multi-Agent Reinforcement Learning. ICML 2025\n\n[3]RACE: Improve Multi-Agent Reinforcement Learning with Representation Asymmetry and Collaborative Evolution. ICML 2023\n\n\n\n### (4) Lack of intuitive explanation and algorithmic clarity\n\nThe paper is mathematically dense but lacks intuitive explanation.\nFor instance, the **motivation and role of VGI** are underexplained in the main text, and there is little visualization or step-by-step discussion of the **training procedure**, **computational cost**, or **failure cases**.\nSimilarly, the algorithm section lacks a high-level intuition connecting the PDE-based reasoning with practical RL optimization.\nReaders unfamiliar with HJB theory or PINNs may find it hard to grasp the conceptual novelty.\n\n### (5) Writing and presentation quality\n\nAlthough mathematically complete, the paper suffers from **dense notation, unclear transitions, and insufficient narrative coherence**.\nSections often read like technical reports rather than structured scientific arguments.\nThe connection between the theoretical motivation and empirical evidence needs stronger integration.\n\nWhile the paper presents an interesting integration of **PINNs** and **continuous-time MARL**, it lacks clear motivation, comprehensive baseline coverage, and intuitive exposition.\nThe conceptual distinction between continuous-time and discrete-time MARL remains ambiguous, and the empirical validation does not convincingly demonstrate broad significance.\nSubstantial revisions — including stronger justification, clearer writing, and broader experimental evaluation — are necessary before this work could be considered for publication at ICLR."}, "questions": {"value": "To make the paper stronger and clearer, the following points should be addressed:\n\n1. **Motivation of continuous-time formulation:**\n   Why is the continuous-time formulation superior to a fine-grained discrete approximation?\n   Can you clarify the distinction between CTMC and continuous-action control tasks?\n\n2. **Connection to model-based MARL:**\n   Since VIP involves explicit modeling of ( f_\\psi ) and ( r_\\phi ), how does it differ from model-based methods such as MA-MBRL or MA-MPC?\n   A comparative discussion (or experiment) is needed.\n\n3. **Experimental coverage:**\n   Can you include or at least discuss results against mainstream MARL baselines (e.g., SMPE, Revisiting Off-policy MARL, MAPPO)?\n   This would better contextualize the practical relevance of your approach.\n\n4. **Algorithmic intuition:**\n   Could you provide an intuitive explanation or figure illustrating how the VGI module refines the gradient flow and why it is necessary for training stability?\n\n5. **Scalability and computation:**\n   What is the computational overhead of solving the PINN-based HJB equations in multi-agent systems?\n   Can this method scale to more than 10 agents or to real-time control?\n\n6. **Clarify novelty:**\n   Many parts of the method (PINN + HJB + actor-critic) exist in prior continuous-time single-agent works.\n   Please clearly highlight what is *new* in your multi-agent extension, both conceptually and technically."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "c8chGByUEC", "forum": "7N0ugLE17t", "replyto": "7N0ugLE17t", "signatures": ["ICLR.cc/2026/Conference/Submission13387/Reviewer_qN3L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13387/Reviewer_qN3L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761208266783, "cdate": 1761208266783, "tmdate": 1762924026656, "mdate": 1762924026656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Continuous-Time Mutli-Agent RL algorithm based on Physics-Informed Neural Networks (PINN). The algorithm, called Value Iteration via PINN (VIP), employs Value Gradient Iteration to estimate the gradient of the value function represented as HJB PDEs. A model of the environment and a reward model are trained alongside policy and value function, and are used for estimating the gradients for value function and expected advantage with respect to the critic and policy parameters respectively. Value gradient iteration, a method for refining value function gradient estimates, is introduced and used for computing an additional target for the value function. The algorithm is evaluated empirically and shown to be competitive with a diverse set of recently introduced methods. Finally, an ablation assesses the effect of individual design choices."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper has a great buildup with excellent motivation including a succinct discussion of related work. Problem formulation and other preliminaries are presented cleary. The central theoretical claims all come with a proof. The value gradient iteration (VGI) method introduced in this paper may be of broad interest for CT-RL in general. Utilizing the differential nature of PINNs in order to train them on more refined targets that include the estimated gradient is a great application of this kind of model. A diverse set of environments is used for evaluation and the experimental results look promising. The effectiveness of VGI is shown empirically on a toy example.\n\nOverall, the paper was very pleasant to read and the contribution to the ICLR community is likely very signficant."}, "weaknesses": {"value": "No discrete-time RL baseline was included in the experiments. Y-axis scaling in some of the plots make them difficult to read."}, "questions": {"value": "- The results on the walker environment show a significantly higher cumulative reward for VIP when compared to the baselines. Moreover, the cumulative reward drops significantly with further training. Can you explain this?\n- For the cooperative navigation task many of the compared algorithms show a very large variance which implies that for some seeds the performance is much better. Can you elaborate on that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "74KoiNTpbV", "forum": "7N0ugLE17t", "replyto": "7N0ugLE17t", "signatures": ["ICLR.cc/2026/Conference/Submission13387/Reviewer_jF5y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13387/Reviewer_jF5y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933698836, "cdate": 1761933698836, "tmdate": 1762924026240, "mdate": 1762924026240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends prior work in continuous-time reinforcement learning (CTRL) from single-agent settings to cooperative multi-agent reinforcement learning (MARL). The authors focus on overcoming the challenges of large state dimensionality and accurate value approximation in continuous domains.\nTheir proposed framework combines Physics-Informed Neural Networks (PINNs) with a new Value Gradient Iteration (VGI) module.\n\n- PINNs are used to approximate the viscosity solution of the Hamilton–Jacobi–Bellman (HJB) equation in continuous time, addressing the curse of dimensionality without explicit discretization.\n\n- The VGI module refines value gradients through a recursive, single-step Bellman-like update, which improves stability and accuracy in policy learning.\n\n\nThe paper presents both theoretical analysis (via a contraction-based convergence argument) and empirical validation on continuous-time adaptations of the Multi-Agent Particle Environment (MPE) and Multi-Agent MuJoCo benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Scalability to high-dimensional systems: The integration of PINNs with VGI addresses the curse of dimensionality in continuous-time settings, enabling effective learning in systems with large state spaces. To my knowledge, VGI module is a novel contribution \n- Benchmarking contribution: The authors extend two widely-used discrete MARL benchmarks—Multi-Agent Particle Environment (MPE) and Multi-Agent MuJoCo—to continuous-time settings."}, "weaknesses": {"value": "I have previously reviewed this paper for another conference, and while the authors addressed some feedback in this version, several substantive issues remain unaddressed, which I list here:\n\n- **Scope is limited to Cooperative MARL & lacks comparison with relevant baselines.** Although framed as a CT-MARL approach, the method applies only to cooperative continuous-time MARL scenarios. It does not handle competitive or mixed settings, where agents have conflicting objectives, which would require solving Hamilton–Jacobi–Isaacs (HJI) equations rather than HJB equations. This limits the generality of the contribution—especially because many real-world multi-agent systems involve competition or partial cooperation. Moreover, in the cooperative case, the learning problem effectively reduces to solving a single centralized control problem with a large joint state space. This raises the question: Is the proposed method genuinely multi-agent in nature, or simply a high-dimensional continuous control solver using a centralized critic? Thus, in my understanding, the high dimensionality of the state could also be an issue in CT-RL. This raises the need for a more thorough comparison with single-agent CT-RL methods.\n\n- **Incremental Theoretical Contribution.** The convergence result (Theorem 3.4) relies on a standard contraction mapping argument under bounded Jacobians and time-invariant dynamics. While mathematically sound, it does not introduce new theoretical insight specific to MARL or continuous-time value iteration. The proof does not incorporate any properties unique to cooperative or decentralized MARL (such as non-stationarity, joint-policy coupling, or inter-agent coordination). Thus, it serves more as a consistency check, rather than a result illuminating how the number of agents or their interactions affect stability or learning dynamics.\n\n- **Motivation and Framing**. The motivation is overall clear—discrete-time MARL algorithms degrade under variable time steps—but the experimental evidence for this claim remains limited. Figure 1 provides a useful visual demonstration, but it’s unclear whether the performance drop of discrete methods (e.g., MADDPG) reflects fundamental limitations of discrete-time updates or simply under-tuned baselines; see next comment. This concern was raised by multiple NeurIPS reviewers and remains only partially addressed.\n\n- **Methodological Comment**. It would strengthen the empirical evaluation if the proposed method were also tested on standard discrete-time benchmarks where the baseline algorithms (e.g., MAPPO, MADDPG) are known to perform well and reproducibly, ideally reproducing reported results from the original papers. This would help disentangle whether the observed advantages stem from the continuous-time formulation itself or from known optimization and reproducibility issues in discrete MARL. Since the proposed approach is theoretically a continuous-time generalization of Bellman updates, it should in principle also perform competitively in discrete settings when the time step is small.\n\n- **Limited Experiments.** In summary, the empirical comparison still omits several key baselines and comparative analyses: no competitive or stochastic control baselines, no detailed compute or resource comparison (training time, memory usage), discrete-time baselines (MADDPG, MAPPO, MATD3) are still missing or not well-tuned for variable time steps.\n\nOverall, the authors should better articulate what is new in the multi-agent context and/or appropriately compare the proposed method.\n\n\n## Minor Comments\n\n\n- In 3.4.2, they still mention the use of terminal-condition losses, while they don’t explicitly exist in equation (17) anymore, which makes sense if the paper now addresses the infinite horizon setup that makes it not ideal to compute such a loss. This should be clarified—either by explicitly removing the term from the text or explaining how those losses were absorbed into the loss formulation.\n- The introduction should clearly state that the method applies only to deterministic multi-agent systems.\n- a brief note distinguishing the smooth vs. viscosity case would help prevent confusion."}, "questions": {"value": "1. Can you provide empirical or analytical evidence that the method provides benefits specifically because of its multi-agent nature (e.g., coordination effects), rather than just being a strong centralized continuous control solver?\n2. Computational Cost: Could you report the relative compute cost (training time, memory, convergence rate) of VIP compared to DPI, IPI, or ODE baselines?\n3. Stochastic Dynamics: Would your framework still hold if small stochastic noise were introduced in the dynamics (i.e., moving toward stochastic differential equations)?\nIf not, how sensitive is the PINN–VGI coupling to such noise?\n4. Value Gradient Iteration Stability: The VGI recursion resembles a single-step gradient rollout. Does it ever introduce instability or oscillation during training? Are there hyperparameters controlling its contraction rate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5tA1jq3DWp", "forum": "7N0ugLE17t", "replyto": "7N0ugLE17t", "signatures": ["ICLR.cc/2026/Conference/Submission13387/Reviewer_5Xhd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13387/Reviewer_5Xhd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762122711977, "cdate": 1762122711977, "tmdate": 1762924025792, "mdate": 1762924025792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}