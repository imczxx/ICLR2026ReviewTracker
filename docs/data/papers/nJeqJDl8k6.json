{"id": "nJeqJDl8k6", "number": 16847, "cdate": 1758269354289, "mdate": 1759897215953, "content": {"title": "SimulRAG: Simulator-based RAG for Grounding LLMs in Long-form Scientific QA", "abstract": "Large language models (LLMs) show promise in\nsolving scientific problems. They can help\ngenerate long-form answers for scientific questions, which are crucial for\ncomprehensive understanding of complex phenomena that require detailed\nexplanations spanning multiple interconnected concepts and evidence.\nHowever, LLMs often suffer from hallucination, especially in\nthe challenging task of long-form scientific question answering.\nRetrieval-Augmented Generation (RAG) approaches can ground LLMs by\nincorporating external knowledge sources to improve trustworthiness.\nIn this context, scientific simulators, which play a vital role in\nvalidating hypotheses, offer a particularly promising retrieval source \nto mitigate hallucination and enhance answer factuality.\nHowever, existing RAG approaches cannot be directly applied for\nscientific simulation-based retrieval due to two\nfundamental challenges: how to retrieve from scientific\nsimulators, and how to efficiently verify and update long-form answers.\nTo overcome these challenges, we propose the \nsimulator-based RAG framework (SimulRAG)\nand provide a long-form scientific QA benchmark covering climate science and\nepidemiology with ground truth verified by both simulations and\nhuman annotators. In this framework, we propose a generalized simulator retrieval interface\nto transform between textual and numerical modalities. We further design \na claim-level generation method that utilizes uncertainty estimation scores\nand simulator boundary assessment (UE+SBA) to efficiently verify and update claims.\nExtensive experiments demonstrate SimulRAG outperforms traditional\nRAG baselines by 30.4\\% in informativeness and\n16.3\\% in factuality. UE+SBA further improves efficiency\nand quality for claim-level generation.", "tldr": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Model", "Scientific Simulator", "Scientific QA"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6bf9d895be83dc825d866df57d9279e507b4f8fd.pdf", "supplementary_material": "/attachment/64fca2ed70b40711761172e481e02cb6150c1241.zip"}, "replies": [{"content": {"summary": {"value": "This paper shows a new RAG framework that lets language models query scientific simulators instead of static text. It turns natural questions into simulator inputs, retrieves numerical results, and uses them to fact-check and refine long answers at the claim level. A benchmark in (1) climate and (2) epidemiology shows clear gains (30% more informative and 16% more factual responses) than standard RAG systems."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The proposed simulator retrieval interface and claim-level uncertainty framework (UE+SBA) together is original combination of ideas that enables grounding in dynamic, quantitative environments. also the paper presentation is easy to follow."}, "weaknesses": {"value": "Experiments are limited to two domains (climate and epidemiology). so that leaves questions about generalization as other domains are not tested."}, "questions": {"value": "1. Can the framework generalize across different simulators without re-engineering the retrieval interface, or is it domain-specific?\n\n2. What is the computational overhead of simulator calls compared to standard RAG retrieval, and how does SBA help reduce it quantitatively? Some clarity on this would help readers understand the difference and make balanced understanding"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5YMY1JUz2n", "forum": "nJeqJDl8k6", "replyto": "nJeqJDl8k6", "signatures": ["ICLR.cc/2026/Conference/Submission16847/Reviewer_3vKB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16847/Reviewer_3vKB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962763446, "cdate": 1761962763446, "tmdate": 1762926868815, "mdate": 1762926868815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work improves the drawbacks of Retrieval-Augmented Generation (RAG) by proposing simulation processes for Retrieval and a claim-level generator. Their approach, called SimulRAG, is applied to climate and epidemiology domains. The main challenge that SimulRAG attempts to solve is the issue of failing to construct a correct long answer in the existing RAG-based approach for domain-specific question-answering. In the first process, given an open-ended question, SimulRAG will get simulation outputs as multiple possible contexts. Next, the second process will use a generator to generate a set of multiple atomic claims that form the final answer. The main contribution of the simulation retrieval interface is to leverage a parameter extraction process to be an input for context generation, given a pre-defined set of templates called the simulator output format. The claim-level generation requires a rigorous process of claim evaluation. The authors design the generation process with the following components. First, a metric for claim-level uncertainty estimation leveraged graph construction between answer and claim sets to see which claim has a higher level of confident. The simulator boundary assessment leveraged GPT-4o to see whether a claim parameter is suitable for a simulator. SimulRAG performed multiple rounds of uncertainty estimation (UE) and simulator boundary assessment (SBA) until getting the final answer. The number of rounds is determined by the percentage of claims that need to be verified, called the verification budget. They construct two domain-specific datasets of question-answer with claims, including 200 entities with human-in-the-loop for data annotation. Their reported accuracy shows that the proposed strategy for claim generation outperformed other traditional algorithms on both GPT-4o and Claude 3.5 (Table 1) while it requires only 45% of verification budget to get comparable accuracy with the All RAG configuration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written.\n- The paper’s proposed research domains are sound and clear. LLMs often struggle with solving long-form questions in specific domains, such as climate and epidemiology.\n- The claim-generation process was constructed properly and has potential for applications in other domains."}, "weaknesses": {"value": "- In Line 245, the authors mentioned that the simulator boundary assessment process used GPT-4 as the judge. This introduces a risk of bias, as a high percentage of cases in the simulator were evaluated as suitable for the claim’s parameters and conditions.\n- Lack of details on human-in-the-loop for evaluation data construction. For example, how can authors ensure that humans’ decisions are correct? Is there a protocol that a question/claim will be verified by more than one expert to avoid the bias of a single decision?\n- In Table 3, in some configurations, the improvement of UE+SBA appears to be marginal compared to the Uncertainty strategy, with an improvement of over 1%. While it is still valid, authors are recommended to make a case study on when uncertainty claim generators perform better claims than UE+SBA.\n- The simulation for the simulator retrieval interface generated output based on predefined templates (L184), which poses the risk for generating answers with a restricted set of templates."}, "questions": {"value": "- I would like to learn more about the human-in-the-loop process for creating evaluation data. For example, for each question, how many experts are assigned to it?\n- This work uses close models for every process. I would like to see how this framework performs when built on open LLMs such as Qwen, Llama models.\n- Can you specify how many predefined templates you use for the simulator retrieval interface?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AvxtOhN2ac", "forum": "nJeqJDl8k6", "replyto": "nJeqJDl8k6", "signatures": ["ICLR.cc/2026/Conference/Submission16847/Reviewer_JBsg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16847/Reviewer_JBsg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974125183, "cdate": 1761974125183, "tmdate": 1762926868470, "mdate": 1762926868470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Thiswork introduces SimulRAG, a Retrieval-Augmented Generation framework that uses scientific simulators to ground LLMs for long-form scientific question answering. The key contributions include: (1) a generalized simulator retrieval interface for transforming between textual and numerical modalities, (2) a claim-level generation method with uncertainty estimation and simulator boundary assessment (UE+SBA) for efficient verification, (3) benchmark datasets for climate science and epidemiology with simulator-verified ground truth, and (4) experimental validation showing 30.4% improvement in informativeness and 16.3% in factuality over traditional RAG baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "THis work uses simulators as retrieval sources which is innovative and addresses a real gap in scientific QA systems. Simulators provide dynamic, quantitative evidence that static text corpora cannot offer. The simulator retrieval interface is well-designed. It can handle the challenging transformation between textual questions and numerical simulator inputs/outputs without requiring fine-tuning. This work tested multiple baselines and the UE+SBA method demonstrates that selective claim verification can achieve near-optimal performance while reducing computational costs by around 50%."}, "weaknesses": {"value": "First, the statistical significance concern: the claim decomposition process could be better illustrated with examples in the main text. With only 200 questions per domain, the statistical power of the evaluation may be limited. Ane there is no comparison with fine-tuned models on the benchmark datasets. Are there systematic patterns in which types of claims UE+SBA misidentifies? And there is no confidence intervals or significance tests are provided for the main results"}, "questions": {"value": "When does the simulator retrieval interface extract incorrect parameters?\nThe average 3.6-5.3 claims per answer is relatively small, how does performance scale with longer, more complex answers?\nWhat percentage of errors come from parameter extraction vs. claim verification vs. answer generation?\nsome minor issues: The sensitivity to these hyperparameters is not analyzed and training/computational costs are not reported\nAlgorithm 1,  the Merge function is underspecified, how does the merge work? (can provide more details)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h2VnsAJRW0", "forum": "nJeqJDl8k6", "replyto": "nJeqJDl8k6", "signatures": ["ICLR.cc/2026/Conference/Submission16847/Reviewer_iLbF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16847/Reviewer_iLbF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983360420, "cdate": 1761983360420, "tmdate": 1762926868104, "mdate": 1762926868104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper try to use RAG-style approach to conduct long form QA on scientific domain, focusing on queries that could include some scientific projection or simulation numbers. To integrate numerical scientific simulators into RAG pipeline, they propose SimulRAG that grounds LLMs in scientific simulators, rather than static textual knowledge bases, to improve trustworthiness of the results. To handle the complexity of long-form answers, the framework decomposes answers into fine-grained, atomic claims, allowing for more precise, targeted verification and updating via their proposed UE+SBA method."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Integrating simulation tools is important contribution to generalize RAG to more applications\n2. Valuable new resource on climate modeling and epidemiology domains"}, "weaknesses": {"value": "1. *Mismatch Between Broad Claims and Limited Evaluation Scope*: My first significant concern is the potential overstatement of the framework's applicability. The paper positions itself as a solution for \"long-form scientific QA\" (line 85), a very broad problem domain. This domain includes numerous existing long-form QA benchmarks based on textual knowledge, such as PeerQA [1] or datasets included in the RAG-QA Arena [2].\n\nHowever, the paper's empirical evaluation is constrained exclusively to a new benchmark constructed by the authors themselves. This benchmark, covering climate science and epidemiology, is explicitly designed to be answerable using the specific simulators the SimulRAG framework integrates. This evaluation strategy feels somewhat circular, as the framework's core novelty (integrating simulators) is tested only on tasks guaranteed to require them. It remains unclear how SimulRAG would perform on the wider class of scientific QA problems where a relevant simulator does not exist, and grounding must be performed against static, text-based knowledge bases\n\n2. The comparison between the proposed method and existing RAG methods should clearly identify the extra cost needed. For example, the Figure 4's results. SimulRAG needs to sample multiple different answer sets (line 193) in order to do the followup UE-SBA, while existing RAG only sample once. Given nowadays LLM are known to get better performance of test-time scaling, this sampling cost difference could mean a lot. Without clearly compare, the effectiveness of SimulRAG is doubtful.\n\n\n\n[1] Baumgärtner, Tim, Ted Briscoe, and Iryna Gurevych. \"PeerQA: A Scientific Question Answering Dataset from Peer Reviews.\" arXiv preprint arXiv:2502.13668 (2025).\n[2] Han, Rujun, et al. \"RAG-QA arena: Evaluating domain robustness for long-form retrieval augmented question answering.\" arXiv preprint arXiv:2407.13998 (2024)."}, "questions": {"value": "Can you properly define Scientific Simulator in the overall framework section?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BEO4JUNbxG", "forum": "nJeqJDl8k6", "replyto": "nJeqJDl8k6", "signatures": ["ICLR.cc/2026/Conference/Submission16847/Reviewer_icBx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16847/Reviewer_icBx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997144829, "cdate": 1761997144829, "tmdate": 1762926867727, "mdate": 1762926867727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}