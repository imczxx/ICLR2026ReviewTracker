{"id": "2A3Q2EtGTF", "number": 15677, "cdate": 1758253778248, "mdate": 1759897289603, "content": {"title": "Dynamic Texture Modeling of 3D Clothed Gaussian Avatars from a Single Video", "abstract": "Recent advances in neural rendering, particularly 3D Gaussian Splatting (3DGS), have enabled animatable 3D human avatars from single videos with efficient rendering and high fidelity. However, current methods struggle with dynamic appearances, especially in loose garments (e.g., skirts), causing unrealistic cloth motion and needle artifacts. This paper introduces a novel approach to dynamic appearance modeling for 3DGS-based avatars, focusing on loose clothing. We identify two key challenges: (1) limited Gaussian deformation under pre-defined template articulation, and (2) a mismatch between body-template assumptions and the geometry of loose apparel. To address these issues, we propose a motion-aware autoregressive structural deformation framework for Gaussians. We structure Gaussians into an approximate graph and recursively predict structure-preserving updates, yielding realistic, template-free cloth dynamics. Our framework enables view-consistent and robust appearance modeling under the single-view constraint, producing accurate foreground silhouettes and precise alignment of Gaussian points with clothed shapes. To demonstrate the effectiveness of our method, we introduce an in-the-wild dataset featuring subjects performing dynamic movements in loose clothing, and extensive experiments validate that our approach significantly outperforms existing 3DGS-based methods in modeling dynamic appearances from single videos.", "tldr": "We propose a 3D Gaussian Splatting method that realistically models loose clothing and dynamic appearances from a single video, overcoming the limitations of template-based approaches.", "keywords": ["3D Computer Vision", "Neural Rendering", "3D Avatar Modeling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e71203b065150953bad1314ae54c980f6cdbfdca.pdf", "supplementary_material": "/attachment/ed78286fdc7d20880fae0e121c6a401beab607f8.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a novel, two-stage framework for creating animatable 3D Gaussian Splatting (3DGS) avatars from a single monocular video, with a specific focus on realistically modeling the \"secondary motion\" of loose-fitting garments. The authors identify two key failings of prior work: (1) poor initialization of Gaussians due to a mismatch between \"naked body\" templates and clothed geometry, and (2) an inability to model complex cloth dynamics due to deformation models that lack temporal context.\n\nTo solve this, this paper proposes:\n1)Personalized Gaussian Initialization (PGI): A pre-processing stage that first trains a 4D NeRF to create a canonical, clothed representation of the subject, from which the initial 3D Gaussians are extracted. This ensures the Gaussians' starting positions match the actual clothed shape.\n\n2)Secondary Motion-Aware Deformation (SMAD): A novel deformation model that represents the canonical Gaussians as a graph. A GNN then autoregressively predicts the position and velocity of the graph nodes, inspired by a second-order mass-spring-damper system. By explicitly encoding a buffer of past velocities, this model captures the temporal context crucial for realistic cloth motion.\n\nThis paper validates the method against several recent 3DGS-avatar baselines on three datasets, including the new \"LoCo-Human\" in-the-wild dataset they introduce. The experiments show state-of-the-art results, with quantitative Table 1 and qualitative Fig. 3, 4  data demonstrating a clear superiority in modeling loose clothing and avoiding common artifacts like skirt-splitting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear Problem Definition: The paper does an excellent job of identifying, diagnosing, and illustrating a significant weakness in current monocular avatar creation: the failure to model secondary motion for loose clothing. The analysis of initialization mismatch and temporal-unaware deformation is insightful and directly motivates the proposed solution.\n\nNovel and Sound Methodology: The PGI stage is a clever solution to the initialization problem. Using a deformable NeRF to build a subject-specific, clothed canonical space is a much more robust approach than trying to fit a generic, naked template (e.g., SMPL) to loosely-clothed subjects.\n\nThe SMAD module is the paper's core strength. Moving from a simple pose-conditioned deformation to a physics-inspired, autoregressive GNN is a significant and logical step. The \"Velocity Encoding\" (VE), which incorporates a buffer of past motions, is a direct and effective way to model the history-dependent nature of cloth dynamics.\n\nExtensive and Convincing Experiments: The experimental validation is a major strength. The method is compared against multiple strong, recent 3DGS-based methods. Evaluation spans three distinct challenges: novel view synthesis (ZJU-MoCap), novel pose synthesis (4D-Dress), and in-the-wild generalization (LoCo-Human). The method achieves state-of-the-art quantitative results across the board (Table 1). The qualitative results (Fig. 1, 3, 4) are particularly compelling, clearly showing the elimination of artifacts (like skirts splitting or \"needle\" artifacts) that plague other methods.\n\nDataset Contribution: The introduction of the LoCo-Human dataset, featuring in-the-wild videos of subjects in loose clothing, is a valuable contribution to the community, which lacks such data."}, "weaknesses": {"value": "Insufficient Ablation Experiments: There is no ablation experiments and no discussion about the selection of GNN-based autoregressive deformer with other models.\n\nMissing Experiment Details: In the ablation study of Physics & Graph Design, there is no detailed explanation about the ablation content. For example, physics in A0 and Contact-aware cross-edges in A3 are ambiguous.\n\nDescriptive Ambiguity:  In the experiment part, this paper describes that the LoCo-Human features five Loose-Clothed Humans performing 5 dynamic and 1 static motions per subject. However, in the data statistics of the Appendix, this paper states that the dataset comprises 5 unique subjects, each recorded across 5 sequences.\n\nCost of PGI Stage: The PGI stage relies on training a full 4D NeRF for each subject before training the main SMAD model. Table F shows that this stage takes 12.5 hours, nearly 3x longer than the 4.5-hour SMAD training. While the fast inference (26 fps)  is excellent, the total training time (17 hours) is substantial. This high \"personalization\" cost should be more clearly discussed as a trade-off.\n\nFigure Quality Issue: Some texts in the figure are not correctly ordered, such as the text “Motion” in Fig. 1 (b). There is a redundant line on the right of Figure L.\n\nOthers: In the More Results part of the Appendix, the citation of the figure is not correct."}, "questions": {"value": "Why does this paper choose GNN-based? Is there any model selection ablation experiment?\n\nWhat do the physics in the ablation study represent? Are they the two physics-based losses in Equations 11 and 12?\n\nAnd what do the hierarchical body–cloth graph and contact-aware cross-edges represent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aORzFypZvC", "forum": "2A3Q2EtGTF", "replyto": "2A3Q2EtGTF", "signatures": ["ICLR.cc/2026/Conference/Submission15677/Reviewer_ZfFP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15677/Reviewer_ZfFP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803833560, "cdate": 1761803833560, "tmdate": 1762925929468, "mdate": 1762925929468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for modeling dynamic appearances of avatars from single monocular videos using 3DGS, with an emphasis on loose-fitting garments and secondary motion (e.g., skirt flutter). Instead of relying on pre-defined template-based initialization and skeletal skinning-based animation, this paper obtains dense Gaussian initialization through personalized 4D NeRFs, constructs a velocity-encoded Gaussian graph and finally learns a secondary motion-aware deformation  module that autoregressively predicts second-order dynamics. The authors also collected a new LoCo-Human dataset, containing in-the-wild videos with dynamic cloth motion. Experiments show strong improvements over state-of-the-art 3DGS-based avatar methods (e.g., 3DGS-Avatar, ExAvatar) on multiple datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The velocity-encoded Gaussian graph is a plausible design: it introduces physical intuition (mass–spring–damper) while maintaining differentiability for network learning. Such a design addresses a key gap in existing 3DGS-based avatars that only model the per-frame pose-to-deformation mapping and neglect the second-order dynamics.\n\n* The LoCo-Human dataset addresses the lack of dynamic loose clothing under monocular capture in existing benchmarks. The dataset ethics statement is detailed, with informed consent and consideration for potential misuse.\n\n* The paper is generally well-written and easy to follow. Figures are clear, with helpful comparisons and ablations."}, "weaknesses": {"value": "* Constructing a node graph for modeling loose garments is not a new thing. Similar ideas have been well explored in previous methods using mesh-based representations like \"Real-time Deep Dynamic Characters\" (Habermann et al 2021). More discussions on the relationship between this paper and existing methods are necessary. \n\n* Although the paper draws analogies to second-order mass–spring systems, the GNN-based updates are learned implicitly, and no physical consistency (e.g., mass, stiffness calibration) is enforced. The “physics-inspired” term might overstate the grounding; clarification on whether parameters (k_ij, γ_i) are learned or derived would help."}, "questions": {"value": "Missing citation: \n\nLi et al. Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling. CVPR 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nNYEp0UqPu", "forum": "2A3Q2EtGTF", "replyto": "2A3Q2EtGTF", "signatures": ["ICLR.cc/2026/Conference/Submission15677/Reviewer_Gn6q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15677/Reviewer_Gn6q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808238135, "cdate": 1761808238135, "tmdate": 1762925928939, "mdate": 1762925928939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "* This paper introduces a method for creating a person-specific animatable avatar from a single video.\n* Method combines a commonly used approach of attaching gaussian splats to a human LBS model, and intorduces a secondary motion modeling element on top. Namely, the gaussian location and its moments are modeled as a dynamic system, parameterized by an autoregressive graph neural network conditioned on per-frame latent codes and representations of history/neighbors states and velocity. \n* Evaluation is conducted on a set of standard benchmarks (ZJU-Mocap, 4D-Dress), as well as a newly introduced dataset, comprised of videos more suitable for evaluation on subjects with loose garments (LoCo-Human)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "*Clarity/quality:*\n- Paper is relatively well-written and is easy to follow.\n- Method seems easy to implement.\n\n*Originality / significance:*\n- Existing approaches for avatar modeling indeed lack realistic 2ndary\nmotion modeling, and modeling it with a hybrid physically inspired approach\n(predicting physical properties with a NN) sounds like a technically sound\napproach.\n\n*Evaluation:*\n- Quantitative and qualitative comparison indicates that the method\nperforms favorably compared to the chosen baselines.\n- On the examples shown in the supp video clothing deformations indeed\nlook convincing."}, "weaknesses": {"value": "*Method Limitations:*\n- Method is person-specific, which means that a new model is trained per input video, and if the information (e.g. about the back of the body) is missing, there is no way to recover it from a prior.\n- Similarly, one can be sceptical that complex physics of clothing\neformations can be learned from a single video without relying on any data-driven prior or a large dataset.\n- (Arguable) This means that the method is unlikely to generate truly realistic motions for poses outside of training distribution, and is likely\noverfitting to training sequences.\n\n\n*Novelty / Significance:*\n- The overall GS+LBS pipeline is not novel, not fully clear if the proposed dynamics formulation combined with GNN has\nbeen done before.\n\n*Experimental Evaluation:*\n- It is unclear why the methods are different across different bencharks.\n- Not fully related to the papers itself, but the quality of ZJU-Mocap\ndataset is extremely poor to the point that results on that dataset\nare not informative.\n- (Minor) For ablation study, it would be useful to understand how the A0 performs compared to baselines (is it already better?). Also, was it conducted on a single subject? If so, it is unclear if the results\nare very reliable.\n- (Minor) It is actually unclear if the method is in any way specific to a single video setup (arguably, it is not - see method limitations), and there exists a variety of datasets of much higher quality (ActorsHQ, Goliath) which could inform whether the formulation provides\nextra benefits in less noisy scenarios (and enable using more\nbaselines)."}, "questions": {"value": "1. Autoregressive formulation is known to be prone to error accumulation issues. Have authors considered evaluating their method on longer sequences\nto confirm no \"explosions\" happens due to this?\n\n2. Why different methods across different benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sY5RB9HUse", "forum": "2A3Q2EtGTF", "replyto": "2A3Q2EtGTF", "signatures": ["ICLR.cc/2026/Conference/Submission15677/Reviewer_1tPA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15677/Reviewer_1tPA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843798958, "cdate": 1761843798958, "tmdate": 1762925928450, "mdate": 1762925928450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenging problem of creating animatable 3D human avatars from a single monocular video, with a specific focus on realistically modeling the secondary motion of loose-fitting clothing (e.g., skirts, coats). The authors identify two primary failings of current 3D Gaussian Splatting based methods: cloth shape-agnostic initialization and temporal context-unaware deformation. To solve this, the paper proposes a two-stage framework. The first is Personalized Gaussian Initialization (PGI) that avoids naked-body templates by first training a deformable 4D NeRF on the input video. A set of canonical 3D Gaussians is then extracted from this person-specific clothed shape. The second is Secondary Motion-Aware Deformation (SMAD) where the canonical Gaussians are structured into a graph. A GNN-based deformer then learns to animate these Gaussians.\n\nThe authors also introduce a new in-the-wild dataset, LoCo-Human, featuring subjects in loose clothing captured with smartphones. Experiments on LoCo-Human and other benchmarks (4D-Dress, ZJU-MoCap) show that the proposed method outperforms recent 3DGS-avatar baselines, particularly in the quality and realism of cloth animation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors identify and illustrating (with Fig. 1) a key failure mode of modern animatable avatar methods, i.e., modeling varying clothing dynamics with time. This is in geenral a notoriously difficult and important problem, and the authors' analysis of why current methods fail (template mismatch and lack of temporal context ) is convincing. I would suggest adding some other relevant works [1,2,3,4] for trying to tackle the goal of modeling loose clothing with/without a template using point clouds and [5,6] that similarly use implicit representations for creating an initial representation to model clothing. [7] PhysGaussian is one of the works using physics to model dynamic Gaussians.\n2. The proposed two-stage solution directly addresses the identified problems. The PGI stage creates a high-fidelity canonical representation of the clothed individual, which is a much better starting point for deformation. Note that this is explored in various other works as well [5,6]. The SMAD module's design is the paper's main strength. Moving from a per-frame, pose-conditioned model to an autoregressive, physics-inspired one is an important shift. Using velocity encoding to give the GNN a temporal state allows it to learn complex dynamics (like inertia and damping), which is something static models cannot do.\n3. The experiments are thorough and are done on standard benchmark (ZJU-MoCap) as well as on more challenging and relevant datasets (4D-Dress, LoCo-Human) that contain subjects wearing loose clothing. The quantitative results (Table 1)  show improvement over all baselines across all three datasets. The qualitative results (Fig. 3, 4) visibly demonstrate the method's ability to avoid common artifacts like skirt-splitting and needle artifacts seen in competing work.\n4. The ablation studies (Table 2, Fig. 6) effectively isolate the contributions of the key components, showing that Velocity Encoding and the graph-based SMAD deformer are essential to the performance gains.\n5. The proposed LoCo-Human dataset is a valuable contribution. As it is captured in-the-wild with standard smartphones, it lowers the barrier to entry and will likely spur further research in this area."}, "weaknesses": {"value": "1. The paper explains that N canonical Gaussians are down-sampled to M graph nodes (M << N) and that the GNN deforms these M nodes. However, it never explains how the deformation of these M nodes is propagated back to the full N Gaussians for the final rendering. Is each Gaussian rigidly attached to the nearest graph node? Is there an interpolation scheme (e.g., barycentric, LBS-like)? This is a critical, missing link in the pipeline.\n2. The title is \"Dynamic Texture Modeling...\". However, the paper's novelty and focus are on dynamic geometry and motion. The authors model the deformation of 3D Gaussians (position, covariance) autoregressively. While color is predicted by a decoder (Eq. 8), there is no discussion of modeling dynamic texture (e.g., time-varying BRDFs, wrinkle maps, or view-dependent shading effects). The dynamic appearance is a result of the dynamic geometry, but the title suggests the texture itself is being modeled dynamically, which does not appear to be the case."}, "questions": {"value": "1. The authors describe down-sampling N Gaussians to M graph nodes for the GNN. How are the deformations computed on these M nodes transferred back to the full set of N Gaussians for rendering?\n2. Could the authors clarify the \"Dynamic Texture\" aspect of the title?\n3. Section 4.3 and Table 2 state the best velocity window is T_v = 15, but Appendix D.1 says T_v = 11 yielded the highest performance. Could the authors clarify the difference in interpretation of these in selecting the final model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yTalrFMhWg", "forum": "2A3Q2EtGTF", "replyto": "2A3Q2EtGTF", "signatures": ["ICLR.cc/2026/Conference/Submission15677/Reviewer_Wxo4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15677/Reviewer_Wxo4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966072751, "cdate": 1761966072751, "tmdate": 1762925927973, "mdate": 1762925927973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenging problem of creating animatable 3D human avatars from a single monocular video, with a specific focus on realistically modeling the secondary motion of loose-fitting clothing (e.g., skirts, coats). The authors identify two primary failings of current 3D Gaussian Splatting based methods: cloth shape-agnostic initialization and temporal context-unaware deformation. To solve this, the paper proposes a two-stage framework. The first is Personalized Gaussian Initialization (PGI) that avoids naked-body templates by first training a deformable 4D NeRF on the input video. A set of canonical 3D Gaussians is then extracted from this person-specific clothed shape. The second is Secondary Motion-Aware Deformation (SMAD) where the canonical Gaussians are structured into a graph. A GNN-based deformer then learns to animate these Gaussians.\n\nThe authors also introduce a new in-the-wild dataset, LoCo-Human, featuring subjects in loose clothing captured with smartphones. Experiments on LoCo-Human and other benchmarks (4D-Dress, ZJU-MoCap) show that the proposed method outperforms recent 3DGS-avatar baselines, particularly in the quality and realism of cloth animation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors identify and illustrating (with Fig. 1) a key failure mode of modern animatable avatar methods, i.e., modeling varying clothing dynamics with time. This is in geenral a notoriously difficult and important problem, and the authors' analysis of why current methods fail (template mismatch and lack of temporal context ) is convincing. I would suggest adding some other relevant works [1,2,3,4] for trying to tackle the goal of modeling loose clothing with/without a template using point clouds and [5,6] that similarly use implicit representations for creating an initial representation to model clothing. [7] PhysGaussian is one of the works using physics to model dynamic Gaussians.\n2. The proposed two-stage solution directly addresses the identified problems. The PGI stage creates a high-fidelity canonical representation of the clothed individual, which is a much better starting point for deformation. Note that this is explored in various other works as well [5,6]. The SMAD module's design is the paper's main strength. Moving from a per-frame, pose-conditioned model to an autoregressive, physics-inspired one is an important shift. Using velocity encoding to give the GNN a temporal state allows it to learn complex dynamics (like inertia and damping), which is something static models cannot do.\n3. The experiments are thorough and are done on standard benchmark (ZJU-MoCap) as well as on more challenging and relevant datasets (4D-Dress, LoCo-Human) that contain subjects wearing loose clothing. The quantitative results (Table 1)  show improvement over all baselines across all three datasets. The qualitative results (Fig. 3, 4) visibly demonstrate the method's ability to avoid common artifacts like skirt-splitting and needle artifacts seen in competing work.\n4. The ablation studies (Table 2, Fig. 6) effectively isolate the contributions of the key components, showing that Velocity Encoding and the graph-based SMAD deformer are essential to the performance gains.\n5. The proposed LoCo-Human dataset is a valuable contribution. As it is captured in-the-wild with standard smartphones, it lowers the barrier to entry and will likely spur further research in this area.\n\n[1] POP: The Power of Points for Modeling Humans in Clothing, Ma et al.\n\n[2] SkiRT: Neural Point-based Shape Modeling of Humans in Challenging Clothing, Ma et al.<br>\n[3] Dynamic Point Fields: Towards Efficient and Scalable Dynamic Surface Representations, Prokudin et al.\n\n[4] PocoLoco: A Point Cloud Diffusion Model of Human Shape in Loose Clothing, Seth et al.\n\n[5] DELTA: Learning Disentangled Avatars with Hybrid 3D Representations, Feng et al.\n\n[6] Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling, Li et al.\n\n[7] PhysGaussian: PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics, Xie et al."}, "weaknesses": {"value": "1. The paper explains that N canonical Gaussians are down-sampled to M graph nodes (M << N) and that the GNN deforms these M nodes. However, it never explains how the deformation of these M nodes is propagated back to the full N Gaussians for the final rendering. Is each Gaussian rigidly attached to the nearest graph node? Is there an interpolation scheme (e.g., barycentric, LBS-like)? This is a critical, missing link in the pipeline.\n2. The title is \"Dynamic Texture Modeling...\". However, the paper's novelty and focus are on dynamic geometry and motion. The authors model the deformation of 3D Gaussians (position, covariance) autoregressively. While color is predicted by a decoder (Eq. 8), there is no discussion of modeling dynamic texture (e.g., time-varying BRDFs, wrinkle maps, or view-dependent shading effects). The dynamic appearance is a result of the dynamic geometry, but the title suggests the texture itself is being modeled dynamically, which does not appear to be the case."}, "questions": {"value": "1. The authors describe down-sampling N Gaussians to M graph nodes for the GNN. How are the deformations computed on these M nodes transferred back to the full set of N Gaussians for rendering?\n2. Could the authors clarify the \"Dynamic Texture\" aspect of the title?\n3. Section 4.3 and Table 2 state the best velocity window is T_v = 15, but Appendix D.1 says T_v = 11 yielded the highest performance. Could the authors clarify the difference in interpretation of these in selecting the final model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yTalrFMhWg", "forum": "2A3Q2EtGTF", "replyto": "2A3Q2EtGTF", "signatures": ["ICLR.cc/2026/Conference/Submission15677/Reviewer_Wxo4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15677/Reviewer_Wxo4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966072751, "cdate": 1761966072751, "tmdate": 1763042488483, "mdate": 1763042488483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}