{"id": "xCelVyUVO2", "number": 19841, "cdate": 1758299879412, "mdate": 1759897016426, "content": {"title": "The Sample Complexity of Online Reinforcement Learning: A Multi-model Perspective", "abstract": "We study the sample complexity of online reinforcement learning in the general setting of nonlinear dynamical systems with continuous state and action spaces. Our analysis accommodates a large class of dynamical systems ranging from a finite set of nonlinear candidate models to models with bounded and Lipschitz continuous dynamics, to systems that are parametrized by a compact and real-valued set of parameters. In the most general setting, our algorithm achieves a policy regret of $\\mathcal{O}(N \\epsilon^2 + \\mathrm{ln}(m(\\epsilon))/\\epsilon^2)$, where $N$ is the time horizon, $\\epsilon$ is a user-specified discretization width, and $m(\\epsilon)$ measures the complexity of the function class under consideration via its packing number. In the special case where the dynamics are parametrized by a compact and real-valued set of parameters (such as neural networks, transformers, etc.), we prove a policy regret of $\\mathcal{O}(\\sqrt{N p})$, where $p$ denotes the number of parameters, recovering earlier sample-complexity results that were derived for linear time-invariant dynamical systems. While this article focuses on characterizing sample complexity, the proposed algorithms are likely to be useful in practice, due to their simplicity, their ability to incorporate prior knowledge, and their benign transient behaviors.", "tldr": "", "keywords": ["control theory", "reinforcement learning theory", "dynamical systems", "learning theory"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ffe80d511bbde7511263743045215ae0b8ef9aa3.pdf", "supplementary_material": "/attachment/6c19035707f118f4d87e1d0c517a2e2dfaa6daff.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a theoretical study of the problem of online reinforcement learning in general (non-linear) continuous systems from the perspective of sample complexity. For context, this version of RL is the non-episodic setting which arose from generalising multi-armed bandits: one competes via regret with the best policy according to the ergodic/infinite horizon undiscounted cost functional (see Thm. 2.1-3). \n\nThe algorithmic solution here is a model-based exponential weights method, which applies to both finite and compact sets of models, which is appreciable. In terms of analysis, classical covering arguments on concentration inequalities of least-squares estimation are replaced by packing arguments for model classes in an expected regret analysis. The results consist in three bounds, in classical manner: the first is a gap-dependent bound, the second is scale dependent bound which depends on $\\epsilon>0$ and the packing number of the model class at scale $\\epsilon$, and finally a worst-case bound. \n\nIn the interest of full disclosure, I had the pleasure of reviewing this article before at ICML and EWRL, which allowed the authors to respond directly to some of my comments and questions and address the weaknesses I identified."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The theorems are clear and well-stated, the proofs are clear, and I haven’t found any issues with them in a superficial inspection. Technical claims are made and proven in a sound manner. The discussion of related works has grown and grown over my several encounters with the paper, and its depth and clarity really deserve commendation now!\n\nI think the contributions of the separation principle approach are deserving of dissemination, and while it is still difficult to situate the assumptions of different lines of work in online RL relative to each other,  the new set of assumptions should be of interest to others in the field."}, "weaknesses": {"value": "As said above, my main criticisms of the paper have already been addressed by the authors in previous revisions. \n\nOne minor weakness which remains is that the writing has some strange repetitions that, I think, could be avoided. For instance, there are 3 persistent excitation conditions that vary slightly (and similarly for the Bellman super-solution assumption). Combining all three into a single assumption, e.g. by letting the constant take one of three values depending on the setting, would probably simplify the exposition of the appendices."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4fANQOuSAn", "forum": "xCelVyUVO2", "replyto": "xCelVyUVO2", "signatures": ["ICLR.cc/2026/Conference/Submission19841/Reviewer_Zps8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19841/Reviewer_Zps8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660840605, "cdate": 1761660840605, "tmdate": 1762932014594, "mdate": 1762932014594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies online reinforcement learning for non-episodic, continuous-state, continuous-action dynamical systems when the true dynamics are only known to lie in a rich model class. It proposes a family of posterior-sampling / Hedge-style algorithms that (i) keep a running, normalized one-step prediction loss for each candidate model, (ii) periodically sample a model according to a softmax over these losses, (iii) apply the corresponding certainty-equivalent controller, and (iv) inject excitation to guarantee persistence of excitation. The analysis is given for three settings: (S1) a finite set of nonlinear candidate models, where the frequentist policy regret scales as $O((\\ln N + \\ln m)/\\Delta)$; (S2) a bounded (possibly infinite) class of Lipschitz dynamical systems, controlled via an $\\varepsilon$-packing and yielding regret $O(N\\varepsilon^{2} + (\\ln N + \\ln m(\\varepsilon))/\\varepsilon^{2})$; and (S3) a compact $p$-dimensional parametric family (e.g. neural networks), where the regret becomes $O(\\sqrt{Np})$, recovering LQR-type rates as a special case. Conceptually, the work cleanly separates model identification from certainty-equivalent control, shows that simple excitation suffices to obtain nonasymptotic, frequentist guarantees, and unifies several strands of RL, online learning, and adaptive/multi-model control within a single sample-complexity framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper unifies three increasingly general control/learning regimes with one posterior-sampling–plus–Hedge template. It starts from a finite candidate set $\\{f_1,\\dots,f_m\\}$, where the frequentist policy regret is of order $O((\\ln N + \\ln m)/\\Delta)$, so the dependence on $m$ is logarithmic as in online learning. It then lifts this to an infinite/bounded function class by constructing an $\\varepsilon$-packing and obtains regret of the form $N\\varepsilon^2 + (\\ln N + \\ln m(\\varepsilon))/\\varepsilon^2$, which is the standard approximation vs. estimation tradeoff. Finally, for a compact $p$-dimensional parametric family it shows regret $(c_{r1}\\ln N + c_{r2}p)\\sqrt{N}$, recovering $O(\\sqrt{Np})$ for linear/LQR while still covering nonlinear dynamics. This gives a nonasymptotic, frequentist guarantee that is stronger than Bayesian-average PSRL bounds in related work. \n2. The algorithm is elegant: each round draws a model using a Hedge/posterior update, runs the corresponding certainty-equivalent policy, and injects excitation to ensure persistence-of-excitation and fast posterior concentration. This realizes a practical separation between model identification and control, avoids heavy OFU-style planning in continuous spaces, and naturally incorporates prior knowledge through the candidate set or parameter prior. \n3. The algorithm comes with solid theory. The paper gives three clear regimes of guarantees. For a finite set of models, the policy regret is $O((\\ln N+\\ln m)/\\Delta)$. For a Lipschitz class controlled by an $\\varepsilon$-packing, it becomes $O(N\\varepsilon^2+\\ln m(\\varepsilon)/\\varepsilon^2)$. For a $p$-dimensional parametric family, it is $O(\\sqrt{Np})$, matching LQR/adaptive-control rates. The bounds can be compared and tuned via $\\varepsilon$."}, "weaknesses": {"value": "1. The analysis is essentially realizability-based: in all three settings (S1 with a finite set of candidates, S2 with an $\\varepsilon$-packed class, and S3 with a parametrized family) the true dynamics $f$ is assumed to lie in the modeling class $F$. In S1, Theorem 2.1 yields a policy-regret bound of order $O((\\ln N + \\ln m)/\\Delta)$, but this relies on a separation margin $\\Delta>0$ between the candidate models so that suboptimal ones can be eliminated; when the models are nearly indistinguishable the paper itself points out that one has to revert to an $O(\\sqrt{N \\ln m})$-type rate. For S2 and S3, the proposed procedures (Alg. 2 and Alg. 3) inject Gaussian excitation $n_{u,k} \\sim \\mathcal N(0,\\sigma_u^2 I)$ every $M$ steps to enforce the persistence-of-excitation requirement (Assumption 6) along the closed-loop trajectories. This condition is stated to hold uniformly over all misspecified models in the class, which makes the theory clean but may be nontrivial to verify or enforce on an actual control system.\n2. For the general setting (S2), the paper first reduces the infinite dynamics class to a finite one by taking an $\\varepsilon$-packing and then applies the same multi-model scheme as in S1. This leads to the regret bound $O(N\\varepsilon^2 + \\ln m(\\varepsilon)/\\varepsilon^2)$ in Theorem 2.2. However, using this result in practice implicitly requires (i) access to or construction of such an $\\varepsilon$-packing of the dynamics class, and (ii) the ability to run a controller for every element in the resulting finite cover. The paper also acknowledges that for Lipschitz-bounded dynamics the packing number $m(\\varepsilon)$ can grow very quickly in high dimension, so this part should be read more as an information-theoretic learnability guarantee than as a directly deployable method for high-dimensional continuous systems."}, "questions": {"value": "My concerns are already detailed in the cons section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2bifCXHfCr", "forum": "xCelVyUVO2", "replyto": "xCelVyUVO2", "signatures": ["ICLR.cc/2026/Conference/Submission19841/Reviewer_XnW8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19841/Reviewer_XnW8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960906627, "cdate": 1761960906627, "tmdate": 1762932013406, "mdate": 1762932013406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the sample complexity of online RL in non-episodic settings with continuous state and action spaces. The authors develop a unified algorithmic framework that combines posterior sampling, Hedge-style model weighting, and certainty-equivalent control under persistent excitation. They provide theoretical policy-regret guarantees across three regimes:\n- finite model classes $O((\\ln N + \\ln m)/\\Delta)$, \n- general function classes via $\\varepsilon$-packing ($O(N\\varepsilon^2 + \\ln m(\\varepsilon)/\\varepsilon^2)$),\n- parameterized model families $O(\\sqrt{Np})$. \n\nThe analysis explicitly incorporates model identifiability ($\\Delta$), information complexity (packing number), and structural dimension ($p$), and demonstrates that the bounds are tight up to logarithmic factors. The work also relaxes the classical PE condition by connecting it to controllability Gramians and sub-Gaussian stability assumptions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The discussion of related work is exceptionally clear, and the citations appear comprehensive.\n2. The theoretical analysis is rigorous.\n3. The paper is well organized, and the narrative progresses with a coherent, reader-friendly logic."}, "weaknesses": {"value": "While I do not see any glaring flaws, the following points prevent a stronger recommendation:\n1. Under the stated assumptions, the theoretical guarantees are not particularly surprising. Despite the authors’ thorough comparison with prior work, the contribution seems incremental relative to the papers referenced around line 67 of the manuscript.\n2. The problem setting is rather restricted, and its practical value is uncertain. The paper provides only simple numerical examples in the appendix, leaving real-world applicability unclear. In many practical scenarios, estimating $\\mu_\\theta$ is nontrivial, and the cardinality $|F|$ may grow exponentially with the size of the state space, which is not encouraging."}, "questions": {"value": "1. In the weakly identifiable limit ($\\Delta \\to 0$), can one obtain a smoother adaptive transition between the $(\\ln N + \\ln m)/\\Delta$ and $\\sqrt{N\\ln m}$ regimes, possibly through hierarchical discretization or adaptive model aggregation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZTIubskmD7", "forum": "xCelVyUVO2", "replyto": "xCelVyUVO2", "signatures": ["ICLR.cc/2026/Conference/Submission19841/Reviewer_jN5Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19841/Reviewer_jN5Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967205986, "cdate": 1761967205986, "tmdate": 1762932012601, "mdate": 1762932012601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses theoretical guarantees for non-episodic reinforcement learning in Euclidean state and action spaces with general nonlinear dynamics.\nThe true dynamics are the sum of a deterministic function and sub-Gaussian noise.\nThe learner is given a class $F$ of dynamics models and with each dynamics model $f^i \\in F$, a corresponding deterministic policy $\\mu^i$, that may or may not be optimal.\nRealizability is assumed: the true dynamics $f$ belong to $F$.\nThree types of model-class are studied:\n\n- S1: A finite set of Lipschitz functions.\n- S2: A bounded set in a normed function space.\n- S3: A parameterized family with a compact Euclidean parameter space.\n\nThese are general enough to subsume lots of prior work.\nThe performance criterion is policy regret against the $\\mu$ associated with the true $f$.\n\nAt a high level, the algorithms look like Hedge over the space of candidate models (i.e. possibly a continuum),\nwhere the \"loss sequence\" for each candidate model is given by a sum of normalized one-step squared prediction errors.\nHowever, one cannot use the standard Hedge analysis for this problem, because the prediction errors depend on the visited state sequence, which depends on the policies that have been deployed by the learner since the start.\n\nTo deal with the latter issue, the algorithm imposes its own episodes and holds the policy constant within each episode.\nThe algorithm adds white Gaussian noise to the policy's deterministic actions.\nTHe main challenge is to control the state magnitude while suboptimal policies are in use, and to ensure that the noise and episode length are exciting/exploratory enough to correctly evaluate the predictive accuracy of each candidate.\n\nThe central analysis is for the finite case S1.\nThe assumptions include:\n\n- a technical Assumption 1 about a particular variant of cost-to-go function.\n- typical Lipschitz/smoothness on the policies and stage costs.\n- the cost-to-go has a quadratic lower bound.\n- a \"gap\" lower bound on the (normalized) difference in predictions between the true $f$ and all other candidates $f_i \\in F$.\n  This bound appears to depend on both the properties of the class $F$ (no two candidates are too similar)\n  and the ability of the additive noise in the action space to sufficiently excite those differences.\n\nThe authors prove sublinear policy regret for each of the three settings, although in S2 the regret exponent gets very close to $1$ for high-dimensional states and/or actions.\nIn particular, the results from S3 recover some regret bounds from the literature for linear dynamical systems, i.e. much more restrictive cases.\n\nThe analysis shows that the algorithm identifies the correct candidate model in finite time almost surely.\nThis, in turn, is used for the regret bound.\n\nThe extension to S2 is not computationally tractable, but gives statistical results.\nIt is assumed we can identify the function $\\bar f$ that minimizes the \"Hedge loss sequence\" over the entire $F$.\nThen, we construct finite cover of $F$ around $\\bar f$; select from the cover using the Hedge-like rule; and synthesize the corresponding policy.\n\nThe extension to S3 assumes that we can somehow sample from the Hedge-like probability measure over the parameter space directly.\nThe authors claim a motivation from neural networks, where this is (to my knowledge) wishful thinking.\nHowever, they note that it is tractable for feature-space models of the form $f(x,u;\\theta) = \\phi(x, u)^\\top \\theta$ which are widely studied in RL theory.\nThe synthesis of the corresponding policy is still computationally difficult except in special cases."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper studies non-episodic RL for a class of nonlinear dynamical systems that is more general than lots of related work.\nThis problem is clearly on the frontier of RL theory.\nThe proposed techniques use a nice mixture of online learning theory, Bayesian methods, and nonlinear control theory.\nThe paper should be of interest to researchers with both RL and control backgrounds.\n\nThe main assumptions (besides ignoring computational cost) are Assumption 1, which is related to Bellman optimality and dissipativity, and Assumption 3, which is related to exploration/persistent excitation. To be honest, it is hard for me to confidently say whether or not these assumptions are restrictive. It seems that we have somehow ruled out hard exploration problems, since we are able to get the regret bound with an exploration strategy that is naive compared to those required even for tabular MDPs in the worst case. However, since the setting is single-trajectory, it is clear that *some* kind of assumptions to limit the negative impact of disturbances and control the difficulty of exploration are necessary.\n\nThe framework mostly ignores computational issues and focuses on statistical guarantees, but this is standard in learning theory. The settings S2 and S3 (except for the special linear-in-features case) are possibly too general to admit efficient algorithms. It will be interesting to see if any follow-up work can instantiate those algorithms for other special cases.\n\nThe paper supports, along with other recent work, the overall idea that \"certainty equivalence\" is a good approach for RL. This is a positive result that simplifies our analysis of RL problems.\n\nI did not have time to check the proofs, but the overall proof structure is logical, and the techniques used seem appropriate.\n\nOverall, I think this is a strong contribution to the RL and learning-based control research communities."}, "weaknesses": {"value": "In the Theorem 1 statement, it is a bit confusing to see the equation (1) suddenly called \"$\\mathcal{H}_2$ gain\", maybe it is equivalent to the classic $\\mathcal{H}_2$ gain for linear dynamics and $l(\\cdot,\\cdot)$ quadratic, but this version is still unfamiliar and RL audiences definitely won't know it. In general, the paper seems to assume a level of familiarity with classic control theory that the ICLR audience may not possess; it would improve the paper to do a bit more hand-holding.\n\nRecovering near-optimal regret bounds for linear systems is very nice, but there is a big gap of generality between the proposed work and linear systems.\nIt would be interesting to know if the framework is also capable of recovering regret bounds for more general frameworks like bilinear classes, Bellman eluder dimension, etc.\n\nDeferring detailed related work discussion to the appendix is unusual. I suggest to move more of the most closely related work on RL theory for continuous state+action spaces with nonlinear dynamics to the main body.\n\nI suggest to reallocate space in the main body -- the S2 case already has its algorithm pushed to the appendix, so one must flip back and forth while reading about the packing strategy in Section 3.2 -- I suggest to shorten the discussion of S2 in the main body and use that space to give a bit more detail/intuition on S1 and the related work in the main body.\n\nThe authors discuss the limitations candidly throughout the paper, so it was surprising to see the conclusion without any thoughts on how they might be improved in future work."}, "questions": {"value": "In Equation (1), must $\\gamma^i$ be finite for all $i$?\n\nThe discussion of Assumption 1 could use more intuition. The authors discuss technical details on how the $-\\gamma$ and $-d_u L_u \\sigma_u^2$ terms help adapt the Bellman-like condition to deal with the infinite-horizon average cost objective and the extra price of excitation. However, I am still left unsure about: **what kind of systems/policies have we ruled out by making this assumption**?\n\nTheorem 3.2 is very similar to Theorem 2.1, but not exactly the same. Why can't we have a proof sketch of Theorem 2.1 in this section instead?\n\nOne notable related line of work is the Decision-Estimation Coefficient [1, for example]. It also provides a highly general RL theory framework, uses the model-based RL paradigm, and focuses exclusively on the statistical aspect (not computational). Although (to my knowledge) that work is more abstract and further from practical, and it is an episodic setting, but it seems too closely related to skip. How does this work compare?\n\n[1] Dylan J. Foster, Noah Golowich, Yanjun Han. Tight Guarantees for Interactive Decision Making with the Decision-Estimation Coefficient. COLT 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4NraiHolqT", "forum": "xCelVyUVO2", "replyto": "xCelVyUVO2", "signatures": ["ICLR.cc/2026/Conference/Submission19841/Reviewer_QBs5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19841/Reviewer_QBs5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978444414, "cdate": 1761978444414, "tmdate": 1762932012246, "mdate": 1762932012246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}