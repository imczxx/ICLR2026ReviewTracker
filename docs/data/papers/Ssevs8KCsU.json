{"id": "Ssevs8KCsU", "number": 14936, "cdate": 1758245806333, "mdate": 1763715072834, "content": {"title": "Deterministic Bounds and Random Estimates of Metric Tensors on Neuromanifolds", "abstract": "The high dimensional parameter space of modern deep neural networks ‚Äî the neuromanifold ‚Äî is endowed with a unique metric tensor defined by the Fisher information, estimating which is crucial for both theory and practical methods in deep learning. To analyze this tensor for classification networks, we return to a low dimensional space of probability distributions ‚Äî the core space ‚Äî and carefully analyze the spectrum of its Riemannian metric. We extend our discoveries there into deterministic bounds of the metric tensor on the neuromanifold. We introduce an unbiased random estimate of the metric tensor and its bounds based on Hutchinson‚Äôs trace estimator. It can be evaluated efficiently through a single backward pass, with a standard deviation bounded by the true value up to scaling.", "tldr": "We discover an efficient way to estimate the Fisher information in deep classifiers with bounded variance.", "keywords": ["Fisher information", "informaton geometry", "Hutchinson's trick", "deep learning theory"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0eecfa579e66ad32bb6b543d8cefc2bf2ab42eff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents a new approach along with theoretical results for estimating the Fisher Information Matrix (FIM) in deep neural networks. The deterministic upper and lower bounds were examined via the geometric properties in low-dimensional probability simplex spaces, followed by extending to high-dimensional parameter spaces using pullback metrics. This estimator can be computed with just a single backward pass, providing significant computational benefits over existing Monte Carlo and empirical FIM methods for large networks like DistilBERT."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Theoretical results are sufficient and rigorous.\n\n2. The ease of implementation in standard deep learning frameworks, which is evident by the empirical validation with better MAE while keeping computational costs similar. The framework's independence from architecture and its ability to apply to both diagonal and low-rank approximations offer flexible trade-offs between accuracy and computational cost."}, "weaknesses": {"value": "1. Empirical validation relies solely on DistilBERT and the results from two text classification datasets. How computational overhead scales with network depth and width, memory needs for large models?"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LKcWFRnY23", "forum": "Ssevs8KCsU", "replyto": "Ssevs8KCsU", "signatures": ["ICLR.cc/2026/Conference/Submission14936/Reviewer_bqwM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14936/Reviewer_bqwM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768937762, "cdate": 1761768937762, "tmdate": 1762925274057, "mdate": 1762925274057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes deterministic bounds and Hutchinson-based stochastic estimators for the Fisher Information Matrix (FIM) on neural manifolds. The method provides unbiased, bounded-variance FIM estimates requiring only one backward pass, making it scalable to large networks. Experiments on DistilBERT show feasible and stable approximations compared with empirical and Monte Carlo estimators."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Provides a unified and theoretically grounded framework for deterministic and stochastic FIM estimation with clear variance guarantees.  \n- The Hutchinson estimator is efficient and easy to integrate into deep learning pipelines, enabling scalable information-geometric analysis."}, "weaknesses": {"value": "- The paper does not validate the estimator on analytically known probability distributions, especially in high-dimensional settings. This makes it difficult to verify the claimed accuracy of the proposed bounds and stochastic estimates.\n- The practical impact of improved FIM accuracy on downstream tasks (e.g., optimization, generalization) is not demonstrated. In some real applications [a], approximation may even be preferable to precision, since neural networks themselves are inherently approximate. The authors should strengthen the discussion and empirical evidence on how their estimator benefits real learning tasks.\n\n[a] Deep CNNs Meet Global Covariance Pooling: Better Representation and Generalization"}, "questions": {"value": "see wk"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PatpJvipfo", "forum": "Ssevs8KCsU", "replyto": "Ssevs8KCsU", "signatures": ["ICLR.cc/2026/Conference/Submission14936/Reviewer_rVQk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14936/Reviewer_rVQk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959082631, "cdate": 1761959082631, "tmdate": 1762925273496, "mdate": 1762925273496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes deterministic bounds and Hutchinson-based stochastic estimators for the Fisher Information Matrix (FIM) on neural manifolds. The method provides unbiased, bounded-variance FIM estimates requiring only one backward pass, making it scalable to large networks. Experiments on DistilBERT show feasible and stable approximations compared with empirical and Monte Carlo estimators."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Provides a unified and theoretically grounded framework for deterministic and stochastic FIM estimation with clear variance guarantees.  \n- The Hutchinson estimator is efficient and easy to integrate into deep learning pipelines, enabling scalable information-geometric analysis."}, "weaknesses": {"value": "- The paper does not validate the estimator on analytically known probability distributions, especially in high-dimensional settings. This makes it difficult to verify the claimed accuracy of the proposed bounds and stochastic estimates.\n- The practical impact of improved FIM accuracy on downstream tasks (e.g., optimization, generalization) is not demonstrated. In some real applications [a], approximation may even be preferable to precision, since neural networks themselves are inherently approximate. The authors should strengthen the discussion and empirical evidence on how their estimator benefits real learning tasks.\n\n[a] Deep CNNs Meet Global Covariance Pooling: Better Representation and Generalization"}, "questions": {"value": "see wk"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PatpJvipfo", "forum": "Ssevs8KCsU", "replyto": "Ssevs8KCsU", "signatures": ["ICLR.cc/2026/Conference/Submission14936/Reviewer_rVQk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14936/Reviewer_rVQk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959082631, "cdate": 1761959082631, "tmdate": 1763689820857, "mdate": 1763689820857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an unbiased estimator for the Fisher metric in the parameter space of a neural network, in the setting of multiclass classification, where the underlying geometry is determined by the simplex. The main estimator, along with two additional approximations, is straightforward to compute and leverages automatic differentiation. The authors provide a theoretical analysis to analyze the accuracy and some properties of the proposed estimators. The theoretical claims are further supported by empirical demonstrations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem of approximating the Fisher metric in neural network settings is timely and very important. The proposed estimators are simple and can be easily computed in practice.\n- The technical part of the paper appears to be sound and the theoretical claims correct, but I have not followed every proof in detail. The experiments support the theoretical analysis.\n- The paper is generally well written and accessible."}, "weaknesses": {"value": "- Since the paper proposes an estimator for the Fisher metric, I would expect additional empirical demonstrations regarding the estimator‚Äôs accuracy and efficiency. I acknowledge that the theoretical results constitute the primary contribution, but I believe that further benchmark experiments would help demonstrate the properties of the estimator and also consider comparisons to related methods. The current experimental section provides some insight, but additional comparisons would strengthen the paper.\n- As the authors mention in Section 5, Hutchinson-type estimators have been used in related contexts. While the extension to neural networks is appreciated and well motivated, as a non-expert in Fisher information analysis, I cannot argue which of the theoretical contributions are novel and which may already be known."}, "questions": {"value": "Q1. It would be helpful to include a comparison with a Monte Carlo approach in terms of both computational complexity and accuracy.\n\nQ2. Is it correct that the LB estimator typically performs better when the predicted distribution approaches one of the corners of the simplex (see Lemma 3)?\n\nQ3. I suggest including the true Fisher information in the experimental comparison, at least in settings where it is feasible.\n\nQ4. A simple synthetic example where the full Fisher information matrix can be computed and visualized would help demonstrate the behavior of the estimator."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UtncScMyUK", "forum": "Ssevs8KCsU", "replyto": "Ssevs8KCsU", "signatures": ["ICLR.cc/2026/Conference/Submission14936/Reviewer_u7Fy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14936/Reviewer_u7Fy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762557938586, "cdate": 1762557938586, "tmdate": 1762925273029, "mdate": 1762925273029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes the Fisher Information Matrix (FIM) for neural classifiers through deterministic bounds based on spectral analysis and proposes a novel Hutchinson-based estimator. The theoretical development is rigorous with complete proofs in the appendix, though experimental validation remains limited."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Solid mathematical foundation: All major theorems have complete proofs in the appendix. The spectral analysis of the simplex FIM (Theorem 1) and envelope characterizations (Lemma 2) are rigorously established.\n\nNovel computational approach: The Hutchinson estimator with ‚Äúdetach-and-mix‚Äù construction provides unbiased FIM estimates with provably bounded diagonal CV ‚â§ ‚àö2, addressing variance issues in Monte Carlo methods.\n\nComprehensive theoretical analysis: The paper provides both deterministic bounds and stochastic estimates with detailed variance analysis for Gaussian and Rademacher distributions."}, "weaknesses": {"value": "Experiments: Only DistilBERT tested on AG News and SST-2. Missing: (i) comparison with K-FAC, diagonal empirical Fisher (Adam), exact Gauss-Newton; (ii) optimization performance metrics; (iii) other architectures (CNNs, ResNets). Just I guess a general expansion in this area would be nice, though you note each formulation may require a from-scratch derivation for each. Please comment on this\n\nUnclear practical advantage: No demonstration of how the estimator improves optimization or learning dynamics compared to existing methods.\n\nTerminology issue: ‚ÄúSingular semi-Riemannian metric‚Äù is incorrect I believe - the FIM is degenerate PSD, not indefinite."}, "questions": {"value": "Technical Concerns\n\nComputational complexity underspecified: Power iteration convergence for computing Œª_C, v_C could be slow near uniform distributions (small spectral gap). The O(MC|ùíü_x|) cost needs quantification.\nImplementation gaps: No guidance on choosing probe count for target accuracy, mini-batch handling, or numerical stability considerations.\n\nMinor Issues\n\nNotation overload (multiple uses of FÃÇ for both empirical and MC Fisher)\nSome bounds are loose (e.g., diagonal upper bound always has error ‚â• 1/C). Correct me if im wrong/misinterpret\n\nQuestions for Authors\n\nWhat is the wall-clock time comparison with K-FAC for equivalent accuracy?\nCan you demonstrate improved optimization performance in a practical setting?\nHow does mini-batching affect the variance bounds?\n\nVerdict\n\nThe paper makes solid theoretical contributions with rigorous mathematical development. However, it lacks the experimental validation needed to demonstrate practical value. The theory is great and extensive, but the work feels a little missing in terms of comprehensive experiments showing real optimization improvements. But I am also open to understanding if this mainly geared to a paper of more theoretical build up value (or step in that direction) than a fully practical ready usage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ay9oxTdine", "forum": "Ssevs8KCsU", "replyto": "Ssevs8KCsU", "signatures": ["ICLR.cc/2026/Conference/Submission14936/Reviewer_YTgp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14936/Reviewer_YTgp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762792532482, "cdate": 1762792532482, "tmdate": 1762925272521, "mdate": 1762925272521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}