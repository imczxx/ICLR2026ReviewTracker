{"id": "8o7iV9l27T", "number": 2894, "cdate": 1757298422909, "mdate": 1759898120749, "content": {"title": "Vocabulary Fixation Reveals Visual Attention Sink for Hallucination Mitigation in LVLMs", "abstract": "Large Vision-Language Models (LVLMs) show remarkable multimodal progress, but their reliability is undermined by hallucinations, the tendency to generate text that contradicts visual input. Recent work has established a strong link between hallucination and the model's attention to visual tokens. However, the current understanding of the Visual Attention Sink (VAS) phenomenon---where LVLMs persistently assign high attention to uninformative background tokens---remains superficial, leaving both its underlying mechanisms and its connection to the hallucination phenomenon unexplored. In this work, we present the first in-depth analysis of VAS. Using logit lens, we uncover a key property we term **Vocabulary Fixation**: VAS tokens consistently map to a small, fixed set of semantically vacuous words across all layers. Based on this observation, we propose **Vocabulary Fixation-Based Identification (VFI)** to reliably localize visual sink tokens in LVLMs. Furthermore, we establish a strong correlation between VAS and hallucination, and introduce the *Non-Sink Visual Attention Ratio (NVAR)*, a novel metric to precisely identify attention heads critical for mitigating hallucination. Building on this foundation, we propose **Sink-Aware Visual Attention Enhancement (SAVAE)**, a training-free method that adaptively strengthens the attention of these targeted heads to salient visual content during inference. Extensive experiments across multiple LVLMs and benchmarks demonstrate that SAVAE significantly outperforms existing decoding strategies in mitigating hallucination, while introducing no additional computational overhead.", "tldr": "We discover that visual attention sink tokens in LVLMs exhibit a predictable \"Vocabulary Fixation\" behavior, enabling us to propose SAVAE, a training-free method that significantly reduces hallucination at no extra computational cost.", "keywords": ["Large Vision-Language Models", "Visual Attention Sink", "Hallucination"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d0e9716a96a3b102035ce74c037ca80ed4397e0.pdf", "supplementary_material": "/attachment/3c20c01bdd1abd6ab54d6671dace0ffae7a05412.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on the phenomenon of visual attention sinks (VAS) in large multimodal language models (LVLMs)—the phenomenon in which the model overattentions certain semantically nonsensical background visual tokens during generation, leading to hallucinations. The authors systematically reveal the underlying mechanism of VAS: vocabulary fixation, where these sink tokens are stably mapped to a small set of meaningless fixed words (e.g., <s>, kwiet) across all network layers. Based on this, the authors propose:"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core insight of this paper is the concept of \"lexical solidification.\" Through logit lens analysis, the authors for the first time link the semantic inertness of VAS tokens to the dynamics of the model's internal representations, explaining why these tokens often appear in background regions and providing a theoretical basis for recognition."}, "weaknesses": {"value": "1.The baseline LVLMs used in this paper (LLaVA-1.5, MiniGPT-4, Shikra) are relatively outdated. It is recommended that the authors evaluate the generalization and effectiveness of AAI on more recent baseline LVLMs, such as QwenVL2.5/3, InternVL, more dataset:MME,ScienceQA,GQA....\n\n2.The authors claim that the fundamental properties of VAS remain unexplored. However, to the best of my knowledge, numerous studies have already investigated VAS and its relationship with hallucination, such as [Devils, CVPR 2025], [EAH, EMNLP 2025], [Farsight, CVPR 2025], [TAME, ICLR 2025], [FastV, ECCV 2024], [Clearsight, CVPR 2025], and [SEE WHAT YOU ARE TOLD, ICLR 2025]. It is necessary for the authors to clarify the limitations of these prior works and explicitly distinguish their proposed method from them.\n\n3.The logit lens technique was applied in [Devils, CVPR] to analyze hallucination-related tokens, and it is evident that this paper follows the same approach (as shown in Figure 1(b)). Why is this not acknowledged or cited in Section 3.1?\n\n4.The methodology section requires improved clarity. For example, the variable l in Equation (3) is not defined. The statement “VAS tokens are captured by an internal mechanism that confines them to a semantically inert subspace throughout the LVLM's processing layers” is also confusing and should be clarified.\n\n5.VAS tokens also appear in correct regions in Figure 2—what is the Vocabulary Fixation behavior of these correct VAS tokens? In addition, a comparison of Figure 2(b) before and after AAI intervention should be provided.\n\n6.The paper identifies the Vocabulary Fixation phenomenon by analyzing the decoding trajectories of selected VAS tokens; however, several issues are overlooked. First, since VAS tokens are defined based on attention scores, the correlation between Vocabulary Fixation scores and attention scores remains unclear. Second, the Vocabulary Fixation behavior of non-VAS tokens is not analyzed. Third, existing VAS token identification methods such as TAME, EAH, and SEE WHAT YOU ARE TOLD appear more intuitive and reliable. Finally, the authors determine the threshold hyperparameter for VAS token selection based on the U-shaped distribution of Vocabulary Fixation scores, but this approach lacks rigorous experimental validation and ablation analysis.\n\n7.The design of SAVAE lacks consideration of different attention layers, while prior works such as FastV and EAH have demonstrated that the attention distribution over image tokens varies significantly across layers.\n\n8.The proposed method shows a significant reduction in the CHAIRs score in Table 3, which warrants further attention and discussion. In addition, the experimental results on CHAIR and POPE do not report the recall scores."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EDF663cdwX", "forum": "8o7iV9l27T", "replyto": "8o7iV9l27T", "signatures": ["ICLR.cc/2026/Conference/Submission2894/Reviewer_PYSW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2894/Reviewer_PYSW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760835538874, "cdate": 1760835538874, "tmdate": 1762916433058, "mdate": 1762916433058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the LVLM hallucination problem and proposes a new perspective to mitigate it. The authors deeply investigate the Visual Attention Sink (VAS) phenomenon, where the model places high attention on uninformative background tokens. In this process, they discover a core phenomenon called Vocabulary Fixation (VF). This is a mechanism where specific visual tokens are repeatedly mapped to the same non-semantic words across all layers, acting as one of the root causes of hallucination.\nBased on this insight, the authors propose three components:\n1. VFI (Vocabulary Fixation-Based Identification): Utilizes the Vocabulary Fixation phenomenon to quantitatively detect visual sink tokens.\n2. NVAR (Non-Sink Visual Attention Ratio): Quantitatively identifies attention heads that focus on meaningful visual information.\n3. SAVAE (Sink-Aware Visual Attention Enhancement): A training-free method that mitigates hallucination by enhancing the attention of key heads (selected by NVAR) during inference.\nThe main contributions of the paper are as follows: First, the discovery of a novel mechanism, Vocabulary Fixation, that explains LVLM hallucination. Second, the establishment of quantitative criteria (VFI and NVAR) to identify hallucination-related tokens and heads. Third, the improvement of the model's visual coherence and reliability without additional training or computational overhead through SAVAE."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong Analysis and Motivation: The analysis related to identifying the cause of hallucination, such as the Vocabulary Fixation phenomenon, is diverse and sound.\n2. Clear Narrative Flow: The storyline from motivation to methodology to experiments is natural and well-structured.\n3. Effective Visualization: The use of appropriate and varied visualization methods makes it easy to understand the motivation and the effect of the proposed method."}, "weaknesses": {"value": "1. Experimental Aspect: The experiments demonstrating hallucination mitigation are insufficient.\n- The motivation experiment (the VAS phenomenon itself) is only shown for the LLaVA model. (Figure 6 appears to show the effect of SAVAE and the validity of the VASR metric rather than the VAS phenomenon itself).\n- Although the method was applied to various models/benchmarks to demonstrate hallucination mitigation, the substantial improvements are prominent mainly in the LLaVA model family and the CHAIR benchmark.\n- There is no analysis of the impact on general performance, such as performance on other downstream tasks (e.g., VQA, Retrieval).\n2. Theoretical Aspect: The discovery of the Visual Attention Sink itself is not entirely novel. While calculating it as an evaluation metric is new, the phenomenon itself has been observed in prior work."}, "questions": {"value": "1. Could the authors provide evidence (e.g., a result table or graph) showing the VAS phenomenon (Figures 2, 3) is confirmed in models other than LLaVA?\n2. Have the authors analyzed the impact of their method on the downstream performance of general vision-language tasks (e.g., VQA, Retrieval, etc.)?\n3. If the VAS phenomenon (i.e., \"sink visual tokens\" always being decoded to the same vocabulary at the same spatial location/background patch across different images or prompts) is due to a structural bias within the model itself, is there evidence to show that correcting this bias directly reduces the VAS phenomenon, not just the hallucination mitigation performance? Specifically, are there experiments that confirm a direct correlation with the reduction of the VAS phenomenon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "N/A"}}, "id": "LdM7ZWDO6G", "forum": "8o7iV9l27T", "replyto": "8o7iV9l27T", "signatures": ["ICLR.cc/2026/Conference/Submission2894/Reviewer_mRf6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2894/Reviewer_mRf6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761544939480, "cdate": 1761544939480, "tmdate": 1762916432931, "mdate": 1762916432931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates hallucination in large vision–language models (LVLMs) by analysing how attention is distributed over visual tokens. Through a logit‑lens analysis of LLaVA‑1.5, MiniGPT‑4 and Shikra, the authors identify a visual attention sink (VAS) phenomenon: certain image tokens draw disproportionately high attention yet consistently decode to a small, semantically meaningless vocabulary across all layers. This “vocabulary fixation” insight leads to a Vocabulary Fixation‑Based Identification (VFI) method that detects sink tokens by counting how often a token’s decoded trajectory hits a set of frequent, vacuous words. Building on this, the authors propose the Non‑Sink Visual Attention Ratio (NVAR) to quantify how much an attention head focuses on non‑sink tokens. Heads with high NVAR are presumed more grounded and are selected for reinforcement. The final contribution is Sink‑Aware Visual Attention Enhancement (SAVAE), a training‑free inference‑time method that strengthens the pre‑softmax attention of the top‑NVAR heads by adding a scaled bonus from the mean head attention. Experiments on CHAIR, POPE/POPE‑Chat and AMBER show that SAVAE significantly reduces hallucination metrics and often improves F1/accuracy compared with greedy decoding and prior mitigation methods like PAI, Devils and VISTA. SAVAE also generalizes across models (7B and 13B) and out‑of‑domain benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. Identifies Vocabulary Fixation as the underlying cause of visual attention sinks\n\nS2. SAVAE is training-free, incurs zero additional computational overhead, and is straightforward to deploy\n\nS3. The paper is generally well‑structured."}, "weaknesses": {"value": "W1. VFI requires constructing a model‑specific set of “vacuous” tokens ˆS and selecting a threshold tau based on a U‑shaped distribution. Although the authors argue that the separation is clear, the method still involves manual filtering of semantically meaningful tokens and tuning tau on validation data, which may not generalize across languages or domains.\n\nW2. The analysis focuses on a few LVLMs, all built on similar base architectures (LLaVA, MiniGPT‑4, Shikra). It is unclear how vocabulary fixation and NVAR behave on recent models such as GPT‑4o or Gemini, or on language‑only models. The tasks are primarily captioning and object‑query benchmarks; the effect on dialogue, visual reasoning or instruction‑following tasks is not studied.\n\nW3. Computing NVAR involves distinguishing real vs. hallucinated object tokens using MS‑COCO annotations. This may not be feasible in practice or for arbitrary inputs, and the paper does not discuss how SAVAE would be applied without labelled data.\n\nW4. While SAVAE often improves F1, there are small decreases in accuracy in some settings (e.g., MiniGPT‑4 results show a slight drop in POPE‑Acc). It would be useful to analyze qualitative outputs to see whether the method biases captions toward shorter or more generic descriptions.\n\nW5. The reinforcement mechanism boosts attention on selected heads by adding an averaged layer signal. It remains unclear whether this negatively affects capabilities unrelated to hallucination, such as reasoning, multi‑step dialogue or safety.\n\nW6. Missing important prior work [1]. The observation that VAS tokens fixate on a small set of meaningless words across layers is similar to the observation in [1].\n\n[1] Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models, ACL 2025"}, "questions": {"value": "Q1. How robust is the VFI method if the set of vacuous tokens ˆS is defined automatically (e.g., via a clustering criterion) rather than manual selection? Would a mis‑specified ˆS degrade NVAR estimation or head selection?\n\nQ2. Have you tested SAVAE on visual question answering or multimodal dialogue datasets beyond CHAIR/POPE? Do the head selections generalize when the task prompt differs significantly from a captioning prompt?\n\nQ3. SAVAE’s head ranking uses NVAR computed over real‑object tokens identified via ground‑truth labels. How would you apply your method when such labels are unavailable? Could you approximate NVAR using unsupervised cues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PZpuTJ1MN1", "forum": "8o7iV9l27T", "replyto": "8o7iV9l27T", "signatures": ["ICLR.cc/2026/Conference/Submission2894/Reviewer_UK6u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2894/Reviewer_UK6u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714738561, "cdate": 1761714738561, "tmdate": 1762916432670, "mdate": 1762916432670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical hallucination problem in Large Vision-Language Models (LVLMs) by focusing on the Visual Attention Sink (VAS) phenomenon and bringing up training-free solution. The authors’ core contributions include: (1) Uncovering Vocabulary Fixation, showing VAS tokens consistently maps to a small set of semantically vacuous words across all layers; (2) Proposing Vocabulary Fixation-Based Identification to localize VAS tokens; (3) Introducing standard metric (the Non-Sink Visual Attention Ratio, NVAR) to select hallucination-critical attention heads; (4) Developing SAVAE, a training-free method that enhances these heads’ attention to salient visual content during inference. The paper proves the vocabulary fixation phenomenon by designed experiments, and validates SAVAE on multiple models and benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The discovery of the Vocabulary Fixation phenomenon is insightful and sheds light on possible mechanisms underlying hallucinations in LVLMs.\n\n2. The proposed SAVAE method is a effective, training-free approach that helps mitigate hallucinations in LVLMs.\n\n3. The paper presents a reasonably broad empirical evaluation across several LVLMs and benchmarks, which supports the claimed generality of SAVAE.\n\n4. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. The definition of *Vocabulary Fixation* and its detection via VFI are only supported by descriptive statistics and qualitative visualizations. There is no quantitative evaluation (e.g., precision/recall vs. ground-truth or baseline) proving that VFI correctly identifies sink tokens. This is the base for the entire paper so it's necessary to provide more quantitative evidence.\n\n2. The fixed vocabulary ( \\hat{S} ) and threshold τ are manually chosen and filtered, yet robustness to these design choices is not analyzed. Reproducibility and generality are unclear. \n\n3. The paper does not test whether SAVAE’s improvements actually stem from targeting Vocabulary Fixation. Ablation experiments (e.g., applying SAVAE to non-sink tokens vs sink tokens) are missing."}, "questions": {"value": "1. Can you provide quantitative evaluation of VFI (precision/recall/F1) against manually annotated or proxy ground-truth sink tokens?\n\n2. How sensitive are your results to the choice and size of the fixed vocabulary ( \\hat{S} ) and the threshold τ?\n\n3. Have you tested SAVAE with random or non-sink vocabularies to confirm that its gains specifically result from mitigating fixation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xux91ZWpzo", "forum": "8o7iV9l27T", "replyto": "8o7iV9l27T", "signatures": ["ICLR.cc/2026/Conference/Submission2894/Reviewer_LG9y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2894/Reviewer_LG9y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835964000, "cdate": 1761835964000, "tmdate": 1762916432435, "mdate": 1762916432435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}