{"id": "Rg8PBd9Ow2", "number": 13412, "cdate": 1758217495890, "mdate": 1759897439559, "content": {"title": "Leveraging Explanation to Improve Generalization of Meta Reinforcement Learning", "abstract": "A common and effective human strategy to improve a poor outcome is to first identify prior experiences most relevant to the outcome and then focus on learning from those experiences. This paper investigates whether this human strategy can improve generalization of meta-reinforcement learning (MRL). MRL learns a meta-prior from a set of training tasks such that the meta-prior can adapt to new tasks in a distribution. However, the meta-prior usually has imbalanced generalization, i.e., it adapts well to some tasks but adapts poorly to others. We propose a two-stage approach to improve generalization. The first stage identifies \"critical\" training tasks that are most relevant to achieve good performance on the poorly adapted tasks. The second stage improves generalization by encouraging the meta-prior to pay more attention to the critical tasks. We use conditional mutual information to mathematically formalize the notion of \"paying more attention\". We formulate a bilevel optimization problem to maximize the conditional mutual information by augmenting the critical tasks and propose an algorithm to solve the bilevel optimization problem. We theoretically guarantee that (1) the algorithm converges at the rate of $O(1/\\sqrt{K})$ and (2) the generalization improves after the task augmentation. We use two real-world experiments, two MuJoCo experiments, and a Meta-World experiment to validate the algorithm.", "tldr": "", "keywords": ["meta-reinforcement learning", "generalization", "theory"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6726f779ebeba677ac1ad239c744a6aa906234bf.pdf", "supplementary_material": "/attachment/b980a4a1fa13914e2a0164e74639e1623080fbeb.zip"}, "replies": [{"content": {"summary": {"value": "This paper primarily aims to improve overall generalization performance by identifying critical tasks and further strengthening them through data augmentation. \n\nRegarding the identification of critical tasks, the authors define the algorithm as finding the optimal weights of critical task pairs. I checked the proofs in the appendix, and it seems to be correct. \n\nAs for further improving generalization performance by augmenting critical tasks, the authors propose optimizing the previously predefined distribution P(λ) to maximally store information. This is also formulated as a bi-level optimization problem, for which the authors provide a tighter generalization bound. \n\nExperiments are conducted both in MuJoCo simulations and on real robotic systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. I appreciate the authors can state the empirical observation on performance degradation, which I find is significant to make the paper complete.\n2. I also appreciate the authors could hold an ablation study to compare with the policy obtained through (2) to further validate the role of task augmentation.\n3. The experiments are conducted even on real-robot, which is very impressive for me."}, "weaknesses": {"value": "1. After reviewing the theoretical derivation, I feel it might be problematic. \n\nIn Appendix E DERIVATION OF THE CONDITIONAL MUTUAL INFORMATION: line 819, where is the distribution P(\\overline{\\mathcal{T}}). Line 827, why the integral concerning P(\\overline{\\mathcal{T}}) is removed.\n\nIn Appendix F line 859, why extending the distribution of P(\\overline{\\mathcal{T}}) can be transformed to P(\\lambda_i). In line 870, I do think the fact that P(\\overline{\\mathcal{T}_{i=1:N}} | \\mathcal{T}_{i=1:N})  = P(\\lambda_i) is wrong. The left-hand side is a joint distribution over N augmented tasks (given the original tasks), while the right-hand side is merely P(\\lambda_i), which is not only ambiguous in terms of the index I, but also represents only a single-variable distribution, not a joint distribution over N variables. Furthermore, even if we only consider one-task condition, I do think P(\\overline{\\mathcal{T_i}} | \\mathcal{T_i}) not equals to P(\\lambda_i). Mathematically, it should be \\int P(\\overline{\\mathcal{T_i}} |  \\mathcal{T_i}, \\lambda_i) P(\\lambda_i) d\\lambda_i\n\nI feel that these issues have seriously compromised the theoretical rigor of the paper, so I did not proceed to check the subsequent theoretical content further.\n\n2. The authors propose to augment the state by convex combination, but I do think in many scenarios, the convex combination of state would not be acceptable. Also, I think this is a strong assumption for applying the algorithm. Therefore, the authors at least should make this clarified.\n\n3. In meta RL, while most online methods emerged around 2021-2022, several offline methods also aim to improve generalization ability, including those also based on information theory [1]. I believe discussing these works properly would be beneficial for providing a more complete literature review.\n\n4. Could you please give me an intuition about why only minor degradation on very few tasks happens when focusing more on critical tasks? Maybe this is a combinatorial phenomena stems from  the number of critical tasks and the optimized distribution P(\\lambda) ?\n\n5. Solving the proposed algorithm needs to optimize two bi-level problems iteratively. It seems that there may exist some instability. Do the authors use some tricks to stabilize the training process?\n\n[1] Towards an information theoretic framework of context-based offline meta-reinforcement learning"}, "questions": {"value": "See weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y0EU8nT4oe", "forum": "Rg8PBd9Ow2", "replyto": "Rg8PBd9Ow2", "signatures": ["ICLR.cc/2026/Conference/Submission13412/Reviewer_kbyg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13412/Reviewer_kbyg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551222758, "cdate": 1761551222758, "tmdate": 1762924043387, "mdate": 1762924043387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of imbalanced generalization in Meta-Reinforcement Learning (MRL), where the meta-policy θ exhibits performance disparities when adapting to new tasks. To tackle this issue, the paper proposes a post-hoc improvement method. First, it identifies \"critical tasks\" that are most beneficial for improving performance on \"poorly-performing tasks\" through a bilevel optimization problem. Subsequently, the paper formulates another bilevel optimization framework to learn the optimal data augmentation for these critical tasks. The upper-level objective maximizes the Conditional Mutual Information (CMI) to ensure that the augmentation provides maximum additional information. The lower-level objective updates the meta-policy distribution based on the current augmentation strategy. Through this process, the algorithm iteratively optimizes the data augmentation strategy (specifically, the sampling distribution for the mixup coefficient λ). The authors theoretically prove the convergence and generalization improvement of their algorithm and validate its superior performance through experiments on MuJoCo, Meta-World, and real-world tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The research direction of this paper holds significant industrial value and practical relevance. In industrial applications, there is a strong emphasis on synchronous and balanced convergence across various tasks, with a particular focus on the performance on long-tail or difficult tasks. Identifying critical training tasks is indeed key to enhancing performance on downstream poor-performing tasks from my point of view.\n\nThe method of identifying critical tasks by finding an optimal weight vector w is clever and intuitive, with a well-formulated optimization objective that aligns with cognitive reasoning.\n\nThe introduction of the CMI concept to construct the optimization objective for learning the sampling distribution of the mixup coefficient λ is reasonable and promising. Furthermore, the use of an augmented dataset {\\hat{T}_{cri}} to estimate the posterior P(θ|{T_{cri}}) via expectation (which can be viewed as an application of the total probability formula) is an approximation method.\n\nThe mathematical derivations in the paper are robust, with a solid formulation of the problem and a well-established convergence proof."}, "weaknesses": {"value": "As mentioned in the paper, this data augmentation method requires online MDP tuples from interaction with the environment, which seems to be unavailable in offline RL settings for now.\n\nThe overall method appears complex. It first requires finding the weights w and then proceeds to optimize the mixup sampling distribution parameters for λ, leading to high computational complexity.\n\nThe approach of using poorly-performing new tasks to supplement the training data (via data augmentation) might be viewed as \"hacking\" the test set. It does not seem to enhance the real generalization capability of MRL from a more universal perspective."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W5wPBWHcnL", "forum": "Rg8PBd9Ow2", "replyto": "Rg8PBd9Ow2", "signatures": ["ICLR.cc/2026/Conference/Submission13412/Reviewer_HExQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13412/Reviewer_HExQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805908680, "cdate": 1761805908680, "tmdate": 1762924043015, "mdate": 1762924043015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses generalization in meta-reinforcement learning (meta-RL) by introducing a novel two-stage approach that leverages example-based explanation and information-theoretic task augmentation.Stage 1 identifies critical training tasks most relevant to poorly adapted tasks through a bilevel optimization that assigns task-specific importance weights. Stage 2 improves generalization by maximizing conditional mutual information (CMI) between the meta-policy and augmented critical tasks. The authors use a learnable task mixup augmentation strategy to achieve this, rather than fixed data-mixing rules. Theoretical analysis guarantees O(1/\\sqrt{K}) convergence and improved generalization bounds. Experiments on real-world (drone, stock trading) and simulation benchmarks (MuJoCo, Meta-World) show consistent gains over MAML and recent meta-RL improvement baselines (task weighting, meta-augmentation, meta-regularization)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. New conceptual link between explainability and meta-RL: the example-based explanation mechanism to identify “critical” tasks is creative and intuitively appealing.\n2. Information-theoretic formulation: Using conditional mutual information to formalize “attention” toward critical tasks provides a principled grounding and unifies ideas from explanation, data augmentation, and meta-learning.\n3. Theoretical guarantees: The paper presents convergence and generalization proofs, which, while incremental, give some rigor to the framework.\n4. Empirical validation: Comprehensive experiments across four environments show clear improvements over baselines, with meaningful ablations (number of critical tasks, learned vs. fixed mixup distribution)."}, "weaknesses": {"value": "1. Meta-RL setup realism:\nThe method inherits the common limitation of meta-RL—assuming access to an oracle task distribution P(T) from which both training and test tasks are sampled. In real applications, such a distribution rarely exists, and constructing it requires a near-perfect world model. Therefore, while the algorithm is technically solid, its practical significance is limited.\n\n2. Limited novelty in theory: 1) Theorem 1 (convergence) essentially restates standard SGD-style results for bilevel optimization [1]; 2)Theorem 3 (generalization bound) reduces to the observation that increasing the number of training tasks improves generalization, which is unsurprising and  closely parallels Theorem 1 in (Yao et al., 2021) [2].\n\n3. Conceptual gap between optimization levels: In Eq. (2), the upper-level objective optimizes the weights \\omega to maximize returns on poorly adapted tasks, whereas the final meta-RL evaluation concerns adapted tasks. The link between these two objectives could be clarified, as there may exist a performance gap.\n\t\n4. Scalability concerns: The approach requires solving multiple nested bilevel problems (for both task weighting and CMI maximization). When N^{tr} is large, this may cause optimization difficulties or high computational cost.\n\t\n5. Mutual information interpretation: While MI provides a neat information-theoretic lens, it is not fully clear how optimizing MI translates into concrete improvements beyond encouraging task diversity. The relation between the learned MI term and mixup augmentation could be elaborated further.\n\n\n\n[1] Jun Shu et al., Meta-Weight-Net, NeurIPS 2019.\n\n[2] Improving Generalization in Meta-learning via Task Augmentation, ICML 2021."}, "questions": {"value": "1. In Eq. (2), when N^{tr} becomes large, does the bilevel optimization become unstable or computationally infeasible? How is this mitigated in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xn1ahB1ynI", "forum": "Rg8PBd9Ow2", "replyto": "Rg8PBd9Ow2", "signatures": ["ICLR.cc/2026/Conference/Submission13412/Reviewer_4uvj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13412/Reviewer_4uvj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908850681, "cdate": 1761908850681, "tmdate": 1762924042768, "mdate": 1762924042768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method mining, reweighting and augmenting training tasks in meta-RL in order to improve on difficult validation tasks.  Poor-performance tasks are first mined from a large evaluation pool.  Rather than re-incorporate these into the training set directly, this method then weights original training tasks based on their ability to improve performance on the mined set.  Finally, the MRL is fine-tuned jointly with augmentation mixing parameters $\\lambda$ on augmented versions of the critical tasks.\n\nThe augmentation here is a mixup-like linear combination of states drawn from the existing task setup, so that the new augmented states can fill out the convex hull of the original state space.  The state mixing coefficients are optimized to increase mutual information of the distribution over the meta-learned inits theta_0 between augmented and non-augmented tasks, modeled as Gaussians --- that is, it optimizes the mixing distributions to try to get resultant thetas (post-RL-inner-loop) that are different from what what they would have been training on just the original tasks, making the augmentation actually provide different datapoints for the outer MRL step.\n\nTheoretical convergence results indicate soundness of the approach.  The method is evaluated on multiple MRL benchmarks and settings, showing substantial improvement in poorly performing tasks with little regression in those that already perform well."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The approach of optimizing augmentation parameters to explicitly result in different theta points (not just different mixing states) is quite interesting, and evaluations compared to predefined mixing demonstrate its effectiveness over a reasonable uniform baseline.  The task mining step is also formulated well and makes a lot of sense."}, "weaknesses": {"value": "While there are some good results compared to appropriate baselines and good ideas (especially the mutual information for augmentations), I had a lot of trouble reading and understanding the paper and method.  (Note, I'm not an expert in RL, but do have familiarity with the subject).\n\nMany of the objective formulations are presented at a high mathematical level while relegating crucial explanations to the appendices.  This is especially true of the core equations 4, 5 and 6, along with Alg 1, which offer definitions of objectives but aren't very clear on the actual steps performed by the algorithm in updating them.  There is also a lot to keep track of here, and simpler explanations and/or diagrams would help.  As I think the ideas are good and have promising results, I'm confident the explanations can be simplified more.\n\nAppendix M is another case in point:  These are all good experiments and explanations illustrating the behavior of the method and its results quite well, and also provide concrete examples of the tasks and how the MRL state interpolations fit together.  Putting more of this in the main text would help ground the explanations and highlight key results like performance improvements on the poor tasks used in the mining step.  (Though I recognize this may be more my opinion, pushing these aside in favor of the convergence results in the main text is exactly backwards --- while the convergence results are good to see and summarize, I think much of Appendix M offers more explanatory value of the core ideas in the method)."}, "questions": {"value": "* It might also be interesting contrast the selection of critical tasks to coreset selection\n\n* Alg. 1:  This doesn't say clearly when (or whether or how much) to run the MRL learning/tuning to update $\\theta$ and how many times this should be run to estimate the P(\\theta) distribs, all of this action seems to be subsumed in the last sentence \"Estimate P*.\"\n\n* l.145:  \"We include the algorithm to solve the problem (2) in Appendix C.\" --- Eq 2 looks like it would requires rerunning the the entire meta-learning loops to solve the inner argmax_theta:  this argmax is exactly a weighted form of eq (1).  however the algorithm in Appendix C doesn't seem to do that, what is actually done here is more of an alternating coordinate descent.  What is actually done should be explained more up-front in the main text.\n\n* line 326 says P(theta|{T^cri}) (non-augmented; no bars) is a marginalization of P(theta|{Tbar^cri}) (augmented with bars) over all sampled mixture coeffs lambda.   How is this possible,\ngiven that the non-augmented tasks are the source of the states forming the convex hull of hte augmented states?  Shouldn't they be evaluated at lambda=0 or 1 to get the T^cri samples?  Or does this distribution mean something else?\n\n* It looks likely that critical task weights will tend to be lower for well-represented tasks and higher for underrepresented (though this is not the only factor).  In a limiting case, if there are duplicate tasks in the meta-training set, the weights can be spread among them.  So if tasks that are near-dups are the ones important for a poor performing task, the weights may be lower and they could be missed in the mining selection.  Does this happen or is it not an issue in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C21rNIoRhS", "forum": "Rg8PBd9Ow2", "replyto": "Rg8PBd9Ow2", "signatures": ["ICLR.cc/2026/Conference/Submission13412/Reviewer_u1Lj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13412/Reviewer_u1Lj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762199965677, "cdate": 1762199965677, "tmdate": 1762924042425, "mdate": 1762924042425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}