{"id": "lbLAgGF8OO", "number": 16271, "cdate": 1758262556773, "mdate": 1759897250912, "content": {"title": "An Improved Model-free Decision-estimation Coefficient with Applications in Adversarial MDPs", "abstract": "We study decision making with structured observation (DMSO). The complexity\nfor DMSO has been characterized by a series of work [ FKQR21 , CMB22 , FGH23 ].\nStill, there is a gap between known regret upper and lower bounds: current upper\nbounds incur a model estimation error that scales with the size of the model class.\nThe work of [FGQ+23 ] made an initial attempt to reduce the estimation error to\nonly scale with the size of the value function set, resulting in the complexity called\noptimistic decision-estimation coefficient (optimistic DEC). Yet, their approach\nrelies on the optimism principle to drive exploration, which deviates from the\ngeneral idea of DEC that drives exploration only through information gain.\n\nIn this work, we introduce an improved model-free DEC, called Dig-DEC, that\nremoves the optimism mechanism in [FGQ+23 ], making it more aligned with\nexisting model-based DEC. Dig-DEC is always upper bounded by optimistic DEC,\nand could be significantly smaller in special cases. Importantly, the removal of\noptimism allows it to seamlessly handle adversarial environments, while it was\nunclear how to achieve it within the optimistic DEC framework. By applying\nDig-DEC to hybrid MDPs where the transition is stochastic but the reward is\nadversarial, we provide the first model-free regret bounds in hybrid MDPs with\nbandit feedback in multiple settings: bilinear classes, Bellman-complete MDPs\nwith bounded Bellman-eluder dimension or coverability, resolving the main open\nproblem left by [LWZ25].\n\nWe also improve online function-estimation procedure used in model-free learning:\nFor average estimation error minimization, we improve the estimator to achieve\nbetter concentration. This improves the  $T^{\\frac{3}{4}}$ and  $T^{\\frac{5}{6}}$ regret of [FGQ+23 ] to $T^{\\frac{2}{3}}$and\n $T^{\\frac{7}{9}}$ in the cases with on-policy and off-policy exploration. For squared estimation\nerror minimization in Bellman-complete MDPs, we redesign the two-timescale\nprocedure in [ AZ22 , FGQ+23], achieving $\\sqrt{T}$ regret that improves over the  $T^{\\frac{2}{3}}$\nregret by [ FGQ+23 ]. This is the first time the performance of a DEC-based\napproach for Bellman-complete MDPs matches that of optimism-based approaches\n[JLM21, XFB+23].", "tldr": "", "keywords": ["decision making", "reinforcement learning", "online learning"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed4d375f9f07d59621a7d8ea09b03cee4eddd293.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The author proposed the information-theoretical notion of complexity and a model-free algorithm for the general decision-making problem with structured observations (DMSO). The proposed Dig-DEC mechanism works seamlessly in both stochastic and adversarial environments (with stochastic transitions and adversarial rewards). It achieves the first model-free regret guarantees with bandit feedback in many general settings, resolving the existing open problem.  Additionally, the proposed approach improves the regret bound for Bellman-Complete MDPs to match classical optimism-based approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- General framework that covers many existing RL settings and improves upon the existing optimistic DEC bounds.\n- Guarantees for model-free algorithms in adversarial settings.\n- Interesting online learning technique for the posterior update that seems to be of independent interest."}, "weaknesses": {"value": "- Restrictive linear reward with known feature assumption for an adversarial hybrid setting;"}, "questions": {"value": "- What are typical values of $\\log|\\Phi|$ in the examples described in Section 5 (for example, in the case of worst-case finite MDPs)?\n- How is DigDEC connected to a classical DEC in terms of regret lower bounds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OIugFxafBW", "forum": "lbLAgGF8OO", "replyto": "lbLAgGF8OO", "signatures": ["ICLR.cc/2026/Conference/Submission16271/Reviewer_Z8zP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16271/Reviewer_Z8zP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761415002220, "cdate": 1761415002220, "tmdate": 1762926419830, "mdate": 1762926419830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper positions itself as a bridge between two major approaches : model-based methods (which learn a full world model) and optimistic, model-free methods (which guess the best outcome). Its main contribution is a new, improved framework called Dig-DEC that removes the \"optimism\" mechanism. Instead of blindly chasing high rewards, Dig-DEC drives exploration  by seeking out information that helps it distinguish between different high-level \"theories\" about how the environment works. This shift leads to better performance guarantees in standard settings—improving regret bounds—under certain assumptions.\nIt aims at replacing a simple heuristic (optimism) with a  possibly more fundamental principle (targeted information gain) to solve harder problems more efficiently."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- the paper presents impressive theoretical developments (though I was far from being able to check all the details of the 30 pages of the appendix),\n- the paper introduces Dig-DEC, a new complexity measure for decision-making. By removing the \"optimism\" principle and replacing it with pure \"information gain,\" it provides an interesting exploration driver. \n- The theoretical proof that Dig-DEC is always smaller than or equal to the prior optimistic DEC is a strong, clean result. This gives improved regret bounds but more fundamentally could give new perspectives of what can be done structured MDPs."}, "weaknesses": {"value": "Implementation challenge\nThe first major challenge lies in the computational feasibility of solving the core minimax optimization (point 2 in the algorithm) at each round. This problem seems exceptionally difficult: the learner must optimize over distributions of policies against an adversary optimizing over distributions of models, with an objective function that involves nested expectations and KL divergences over trajectories. For any non-trivial state space, the policy and model classes are likely to be enormous, making an exact solution intractable. The objective is also non-convex and non-concave in general (?). While the paper provides a theoretical blueprint, it offers no practical implementation or approximation scheme. Bridging this gap would require major algorithmic innovations,\n\n\nAssumptions\n The  core assumptions of the paper present a significant gap between the theory and the reality of most RL problems. The most restrictive assumption is possibly the requirement for a pre-defined, finite partition of the model space into infosets where all models within an infoset share a unique optimal policy and value function (Assumptions 1 & 3). In practice, such a discrete and perfectly aligned partition seems unavailable; the \"optimal policy\" may not be uniquely defined or may change during learning. Furthermore, Assumption 4 (linear rewards with known features) for the hybrid setting is a strong structural limitation, as it assumes the learner has perfect knowledge of the reward representation, which is often the very thing that needs to be learned in adversarial settings.\n\nPresentation\nA major weakness is the presentation. The paper is very difficult to read. Many notions are supposed known to the reader and there is no attempt to aim at a bit of self contained presentation.  For instance, the assumptions 5,6 are difficult to grasp. This makes the contribution of the paper less valuable because more difficult to gauge.\nThe presentation of the assumptions could be done differently by first describing informally the context and main restrictions and then making them mathematically precise."}, "questions": {"value": "- What would be the complexity of the minimal AIR optimisation problem? Can you give approximate solutions at a reasonable complexity? (This is one of the most crucial point that the paper does not discuss...)\n\n- Related to that last point, Im a bit lost with the \"model free learning\" terminology.  I guess for a large part of the community, model free means that you do not have access to the model ( and hence to means) but only to observations.\nWhen solving the optimisation problem, you are not model free? \nSo,  given that you claim that optimism principles should be dropped, a lot more details should be given on how a practical estimation scheme can be leveraged  for your proposal?  The regret bounds are MDPs bound? not RL bounds?\nIn conclusion,  some precisions should be given on what you call (along with other papers) model free learning and more importantly what are the contours of your results...\n\n- In Lemma 12, what does: \"In the stochastic setting, Assumption 1 together with -completeness\" mean?\n\n- It would have been helpful to have a very simple toy example to underline that the assumptions are useful..."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DwD7o0FgcX", "forum": "lbLAgGF8OO", "replyto": "lbLAgGF8OO", "signatures": ["ICLR.cc/2026/Conference/Submission16271/Reviewer_B9o3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16271/Reviewer_B9o3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489578471, "cdate": 1761489578471, "tmdate": 1762926419249, "mdate": 1762926419249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new, model-free complexity measure Dig-DEC (Dual Information Gain Decision-Estimation Coefficient), which removes the optimism-based exploration mechanism (allows adversal setting ) and is upper bounded by optimistic DEC. Then, this work adopts this framework to the stochastic and hybrid adversarial MDP setting and achieves a series of SOTA results with different MDP structures (bilinear classes, Bellman-complete, etc.)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "* This work achieves a series of improved results under the Dig-DEC framework."}, "weaknesses": {"value": "* Since this work has provided a series of new SOTA results, it would be very helpful to add some discussion and intution after each results or assumption (for example, Assumption 1)."}, "questions": {"value": "Q1: Can this work provide experiments (even simulation experiments) to show the relationship between Dig-DEC and optimistic DEC."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mv0vBMS0QT", "forum": "lbLAgGF8OO", "replyto": "lbLAgGF8OO", "signatures": ["ICLR.cc/2026/Conference/Submission16271/Reviewer_L37f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16271/Reviewer_L37f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001317161, "cdate": 1762001317161, "tmdate": 1762926418649, "mdate": 1762926418649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a new decision-estimation coefficient (DEC) notion, which enables conducting exploration via information gain instead of the optimism principle in a model-free manner. The first benefit of the new DEC notion is the improvement of previous results in cases of bilinear classes or Bellman-complete MDPs with bounded Bellman eluder dimension. The second benefit is that this new DEC notion leads to the first model-free algorithm for MDPs with stochastic transitions and adversarial loss functions in the bandit feedback setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. **Novelty**: The proposed new notion seems interesting and fundamental, enabling the exploration solely based on information gain.\n2. **Results**: The model-free algorithm leads to a series of new results in stochastic MDPs, with matched or even improved results. This work also resolves the open problem of previous work for solving MDPs with stochastic transitions and adversarial loss functions in the bandit feedback setting.\n3. **Presentation**: This paper is well-written."}, "weaknesses": {"value": "If any, I would feel that some parts could be clearer and more thoroughly explained. For instance, from Table 1, I notice that the regret bound of off-policy exploration might be inferior to that of on-policy exploration. Could the authors explain why this happens? \n\nAlso, most previous works for tabular and linear MDPs with adversarial loss functions use occupancy measure (OM)-based or policy optimization (PO)-based methods, both of which are model-based and require to learn the transitions explicitly to construct the loss estimator. Could the authors intuitively explain how to construct the “loss estimator” without explicit learning of transitions?"}, "questions": {"value": "Please see my questions in the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pUGh5kv5lq", "forum": "lbLAgGF8OO", "replyto": "lbLAgGF8OO", "signatures": ["ICLR.cc/2026/Conference/Submission16271/Reviewer_NZXe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16271/Reviewer_NZXe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014835296, "cdate": 1762014835296, "tmdate": 1762926418175, "mdate": 1762926418175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}