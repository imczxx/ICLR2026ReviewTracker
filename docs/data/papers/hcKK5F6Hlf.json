{"id": "hcKK5F6Hlf", "number": 1339, "cdate": 1756872820665, "mdate": 1763706675489, "content": {"title": "Breaking the Correlation Plateau: On the Optimization and Capacity Limits of Attention-Based Regressors", "abstract": "Attention-based regression models are often trained by jointly optimizing Mean Squared Error (MSE) loss and Pearson correlation coefficient (PCC) loss, emphasizing the magnitude of errors and the order or shape of targets, respectively. A common but poorly understood phenomenon during training is the *PCC plateau*: PCC stops improving early in training, even as MSE continues to decrease. We provide the first rigorous theoretical analysis of this behavior, revealing fundamental limitations in both optimization dynamics and model capacity. First, in regard to the flattened PCC curve, we uncover a critical conflict where lowering MSE (magnitude matching) can *paradoxically* suppress the PCC gradient (shape matching). This issue is exacerbated by the softmax attention mechanism, particularly when the data to be aggregated is highly homogeneous. Second, we identify a limitation in the model capacity: we derived a PCC improvement limit for *any* convex aggregator (including the softmax attention), showing that the convex hull of the inputs strictly bounds the achievable PCC gain. We demonstrate that data homogeneity intensifies both limitations. Motivated by these insights, we propose the Extrapolative Correlation Attention (ECA), which incorporates novel, theoretically-motivated mechanisms to improve the PCC optimization and extrapolate beyond the convex hull. Across diverse benchmarks, including challenging homogeneous data setting, ECA consistently breaks the PCC plateau, achieving significant improvements in correlation without compromising MSE performance.", "tldr": "We theoretically study why Pearson correlation plateaus in attention-based regression and introduce ECA framework to sharpen and extrapolate attention, consistently boosting correlation without compromising MSE, even on homogeneous datasets.", "keywords": ["Attention Mechanisms", "Joint Optimization", "Pearson Correlation", "Data Homogeneity"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/17843cf1de06f5931b3b7f2d8f856eec499c3a48.pdf", "supplementary_material": "/attachment/a88af10219950a63158e9f3ca6bb9dadb0a10ea0.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the \"PCC plateau\" phenomenon in attention-based regression models trained with a joint MSE and PCC loss, where PCC stops improving early even as MSE decreases. The authors provide a theoretical analysis identifying two main causes: 1) an optimization conflict where minimizing MSE paradoxically suppresses the PCC gradient, exacerbated by softmax attention, especially with homogeneous input elements within a sample; and 2) a model capacity limit where convex aggregators like softmax attention restrict PCC improvements based on the convex hull radius of the input embeddings. To overcome these limitations, the paper proposes Extrapolative Correlation Attention, incorporating a dispersion-normalized PCC loss, dispersion-aware temperature softmax, and scaled residual aggregation to stabilize PCC optimization, adapt to homogeneity, and allow extrapolation beyond the convex hull. Experiments across various benchmarks demonstrate that ECA successfully breaks the PCC plateau and improves correlation without degrading MSE performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a thorough theoretical investigation into the PCC plateau phenomenon from two complementary perspectives: optimization dynamics and model capacity. This analysis successfully identifies key bottlenecks inherent in standard softmax attention for correlation learning, specifically the gradient attenuation conflict with MSE and the limitations imposed by convex aggregation.\n- The proposed Extrapolative Correlation Attention (ECA) method directly addresses the theoretically identified bottlenecks. Each component of ECA is well-motivated by the preceding analysis, offering targeted solutions to the optimization and capacity limitations."}, "weaknesses": {"value": "- While the theoretical analysis provides valuable insights, certain aspects could benefit from further development or more rigorous treatment. Specific points regarding the theoretical sufficiency will be elaborated upon in the questions section."}, "questions": {"value": "- While Corollary 2.1 provides a bound on the PCC gradient magnitude, it's not immediately clear if this bound alone sufficiently explains the relative slowdown of PCC compared to MSE during optimization. Could the authors provide a more direct comparison, perhaps by analyzing the relative magnitudes or scaling of the PCC gradient versus the MSE gradient components ?\n- For a more comprehensive understanding of the optimization dynamics, would analyzing the system using gradient flow provide further insights or more robust theoretical guarantees regarding the plateau phenomenon?\n- The central claim is that addressing the identified gradient attenuation and capacity bottlenecks improves PCC performance. Could the authors elaborate further on the core intuition for why mitigating these specific issues leads to better correlation learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EWUxJXOVEE", "forum": "hcKK5F6Hlf", "replyto": "hcKK5F6Hlf", "signatures": ["ICLR.cc/2026/Conference/Submission1339/Reviewer_W6BH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1339/Reviewer_W6BH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620265057, "cdate": 1761620265057, "tmdate": 1762915743093, "mdate": 1762915743093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the *PCC plateau* phenomenon where for attention-based regression models, PCC stops improving early in training even as MSE continues to decrease. One of the contribution of the paper lies in the theoretical characterization of the following conflict: lowering MSE  can paradoxically suppress the PCC gradient, particular when the data to be aggregated is highly homogeneous. Moreover, the authors derive a PCC improvement limit for any convex aggregator (including the softmax attention). Finally, to solve the aforementioned issues, the authors propose several tricks to rescale the PCC loss, adjust attention temperature. Empirical results prove their proposed methods work well in practice."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Novel topics and clear interpretation:** I like the topic studied in this paper since learning PCC is an important problem, and there lacks theoretical understanding of why the PCC plateau happens. Moreover, the decomposition in Proposition 2.1 offers a clean interpretation of how the matching of mean, std, and correlation affect the MSE.\n\n**Clear theoretical analysis** In Section 2, the authors provide several propositions and theorems to argue why the PCC plateau happens by analyzing the gradients. Though the analysis has some weakness (see weakness section for details), these theoretical results provide a clean interpretations of how the homogeneity of the data, batch size, cross-sample deviation affect the overall gradients. Considering this work is the first to address this issue, I appreciate their contributions.\n\n**Strong empirical validation:** In the experiment section, the authors evaluate their proposed methods across four different datasets, and demonstrate sufficient improvement of the increase of PCC when their method has been applied. Moreover, the authors conduct ablation studies to identify the contributions of each component of their proposed algorithm."}, "weaknesses": {"value": "**Missing comparison of gradient between MSE and PCC:** In Section 2.3, the author present gradient of PCC w.r.t. attention logits (see Theorem 2.1), then provide a bound for this gradient in Corollary 2.1, and finally discuss the implications of the bounds. It is clear how each term affect the gradient for PCC. However, it lacks the discussions of the bounds for MSE as well. Since this paper argues *PCC tends to flat earlier while MSE continues to decrease*, it is important to discuss the gradient of MSE, and compare them to support this claim.\n\n**Weak theoretical contribution:** The theoretical contributions of this work is mainly point-wise comparison (section 2.3 and 2.4). For example, in theorem 2.2, the authors state the bounds for $\\rho-\\rho_0$ for fixed $\\tilde R, \\sigma_0, w$. However, it lacks the understanding of how the quantities of interest evolve throughout the training, and what is the effect of them cumulatively on the PCC and MSE. \n\n**Limited discussions on the trade-offs:** This paper introduces a novel objective in equation 8 to improve the learning of PCC. However, adding regularization might affect the performance of MSE, training speed, or stability. However, none of this trade-offs has been discussed in the paper."}, "questions": {"value": "**Question 1** Can you please explain how can we see the conclusion *PCC flattens at early epoch while MSE continues to decrease* in Figure 2? I feel the curves for training mse and training pcc seem to overlap.\n\n**Question 2** In theorem 2.2, because PCC is less than 1, if the RHS of the inequality os larger than 2, it provides meaningless bound. Therefore, the bound is only meaningful when the RHS is substantially less than 2. However, it is unclear whether this quantity satisfies this conditions.\n\n**Question 3** In the experimental sections, are the reposted MSEs training loss or test loss? In Table 1 and Table 2, it seems adding this additional regularization, the MSE also decreases. This is a little surprising to me. The reason is that when you add the additional regularization to improve PCC, essentially you're introducing the bias for the MSE loss, and resulting training loss should not be lower than the one without the regularization. Can you please provide some intuitions why this happens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fhqt34K5qQ", "forum": "hcKK5F6Hlf", "replyto": "hcKK5F6Hlf", "signatures": ["ICLR.cc/2026/Conference/Submission1339/Reviewer_pBmc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1339/Reviewer_pBmc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764520727, "cdate": 1761764520727, "tmdate": 1762915742392, "mdate": 1762915742392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a pervasive but underexplored failure mode in attention-based regression models trained with a joint Mean Squared Error (MSE) and Pearson Correlation Coefficient (PCC) objective: the “PCC plateau,” where PCC stops improving early in training even though MSE continues to fall. The authors trace this plateau to two coupled bottlenecks. First, an optimization bottleneck: minimizing MSE increases the variance of the model’s predictions, but the PCC gradient scales inversely with that variance, so the PCC term effectively “goes silent” as training progresses. This effect is exacerbated in settings where each sample’s elements are internally homogeneous, because the PCC gradient flowing through attention weights becomes extremely weak. Second, a capacity bottleneck: standard softmax attention is a convex aggregator, so its output is restricted to the convex hull of the input embeddings. The authors prove that in low-dispersion regimes this imposes a hard upper bound on how much PCC can improve beyond naive mean-pooling.\n\nGuided by this analysis, the paper introduces Extrapolative Correlation Attention (ECA), a drop-in alternative to softmax attention built to overcome both bottlenecks. ECA has three components: (i) Scaled Residual Aggregation (SRA), which explicitly extrapolates beyond the convex hull to increase representational contrast; (ii) Dispersion-Aware Temperature Softmax (DATS), which sharpens attention adaptively when a sample is internally homogeneous; and (iii) Dispersion-Normalized PCC Loss (DNPL), which rescales the PCC objective to keep its gradient active even late in training. Across synthetic, tabular, biomedical, and multimodal benchmarks, ECA prevents the PCC plateau: it continues improving correlation throughout training and delivers higher final PCC while matching or improving standard error metrics (MSE/MAE)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, the paper is well written. Here are my understanding of the strength part:\n\n1. The paper identifies and analyzes an interesting “PCC plateau” phenomenon, an observed but under-theorized failure mode in attention-based regression models trained with joint MSE+PCC loss. In my perspective, because the phenomenon emerges in multiple architectures and datasets, the problem is likely to generalize and will be of interest to both theory-leaning and applied ML audiences. \n\n2. On the theory side, the paper decomposes why PCC plateaus: an optimization bottleneck, where minimizing MSE inflates prediction variance and in turn suppresses the effective PCC gradient (especially under homogeneous samples), and a capacity bottleneck, where softmax attention is inherently limited because it forms convex combinations and thus cannot escape a tight convex hull when all elements in a sample are very similar. Crucially, the proposed architectural solution — Extrapolative Correlation Attention (ECA) — is directly motivated by those two factors. This feels principled rather than heuristic. \n\n3. The empirical section is broad, well thought out, and aligned with the claims: The authors test on synthetic data with controlled homogeneity to stress-test exactly the regimes where theory predicts the plateau — and show that ECA keeps PCC improving instead of flattening. They evaluate on standard tabular regression benchmarks, showing consistent PCC gains and equal or better MSE/MAE after swapping in ECA, suggesting the method is not domain-specific. They move to challenging applied domains (spatial transcriptomics, multimodal sentiment) where correlation is a core metric and within-sample homogeneity is very strong. They also include ablations of ECA’s components, which helps establish that the gains are actually due to the proposed mechanisms, not just tuning or extra capacity."}, "weaknesses": {"value": "While the paper provides a principled and well-motivated analysis, several aspects could be further clarified or extended:\n\n1. Architectural depth and generality. Most theoretical and synthetic experiments use a one-layer transformer. It remains unclear whether the proposed ECA mechanism yields similar improvements in deeper architectures, where subsequent layers (especially MLPs) could partially overcome the convex-hull limitation. Without results on multi-layer settings, the contribution may appear as a targeted fix for shallow regressors rather than a generally applicable improvement to attention.\n\n2. Effect of normalization layers. The current analysis does not account for the common use of pre- or post-LayerNorm in standard transformer blocks. Layer normalization can re-center and rescale token representations, which might already mitigate homogeneity and alleviate part of the optimization bottleneck. A more systematic investigation of how normalization interacts with the proposed mechanisms would strengthen the theoretical claims.\n\n3. Baseline coverage and fairness of comparison. The baselines focus mainly on vanilla softmax attention with a joint MSE + PCC objective. Missing comparisons include: (1) attention variants that allow negative or signed weights [1, 2], which can also move beyond convex combinations; and (2) feature-wise normalization or whitening methods that implicitly address variance inflation.\nIncluding these would clarify whether ECA’s gains stem from its theoretical design rather than simply greater expressiveness or modified normalization.\n\nReferences\n- [1] Zhang et al., Negative-Aware Attention Framework for Image-Text Matching, CVPR 2022.\n- [2] Lv et al., More Expressive Attention with Negative Weights, arXiv:2411.07176, 2024."}, "questions": {"value": "In general, I like the idea of going beyond the convex hull of softmax attention. However, this limitation may not apply to the linear attention, or other variation of the attention that permits negative attention scores as discussed before. I believe further investigations on how ECA interacts with these non-convex attention would be beneficial."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DEMH0IrL2E", "forum": "hcKK5F6Hlf", "replyto": "hcKK5F6Hlf", "signatures": ["ICLR.cc/2026/Conference/Submission1339/Reviewer_4Xnu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1339/Reviewer_4Xnu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790803079, "cdate": 1761790803079, "tmdate": 1762915742279, "mdate": 1762915742279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the training behavior of attention-based regression models jointly optimized with mean squared error (MSE) and Pearson correlation coefficient (PCC) losses. The authors claim that such models commonly exhibit a “PCC plateau,” where correlation stops improving early in training while MSE continues to decrease. They analyze this through a theoretical decomposition linking MSE and PCC, derive a gradient bound suggesting that PCC gradients shrink with low within-sample dispersion and large prediction variance, and prove a convex-aggregation capacity limit. They propose Extrapolative Correlation Attention (ECA), combining (i) Scaled Residual Aggregation, (ii) Dispersion-Aware Temperature Softmax, and (iii) a Dispersion-Normalized PCC loss. Experiments on synthetic, tabular, biomedical, and multimodal sentiment datasets show small but consistent PCC improvements."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The writing is well structured and easy to follow. The theoretical derivations are transparent and carefully presented.\n2. The work targets an interesting question: how attention-based regressors behave when trained with both magnitude and correlation-based losses. This is a topic that deserves rigorous study and of broad interest to the community.\n3. The authors validate their method across a diverse and well-chosen set of benchmarks, which strongly supports the generality and effectiveness of their approach."}, "weaknesses": {"value": "1. The theoretical analysis is based on a simplified model: a single attention aggregation layer followed by a linear head. However, the models used in practice are significantly more complex. The paper does not discuss how other architectural elements might interact with or alleviate the identified problems.\n\n2. The experiments only validate the downstream effects rather than the mechanisms proposed by the theory. The paper would be more convincing with additional empirical evidence that isolate the role of each claim.\n\n3. PCC plateau is described as a widely relevant failure mode, but the authors do not provide any citations or external evidence of this claim. The only evidence they introduce in the beginning of the paper is figure 2 based on their own experiments. If this problem is well documented in the literature, the authors should provide citations. If external citations are truly unavailable, the authors must provide overwhelming evidence of the problem's existence themselves, or potentially reframe their contribution to be the identification and formalization of the problem itself.\n\n4. The condition is theorem 2.2 can make the result meaningless. The tightness of the bound is also not discussed (what if the denominator goes to zero?), and it does not explain or provide any intuition about the training dynamics, it is just about the best achievable PCC after training."}, "questions": {"value": "1. Your argument that data homogeneity suppresses the PCC gradient and causes PCC plateau seems to conflate two different effects: homogeneity makes the pooled representation almost insensitive to attention weights, which would weaken gradients for any regression loss (including MSE), but you present it as if this is unique to PCC. At the same time, you also argue that PCC suffers an additional optimization-specific attenuation which would not affect MSE the same way. Can you clarify which of these you believe is actually responsible for the gradient bottleneck?\n\n2. The claim about the gradient bottleneck assumes that the prediction deviation increases early in training, but that quantity could also in principle decrease. Do you still observe a plateau if the initialization has a high output deviation?\n\n3. Why is the role of the batch size S not explored further? Did you try running the experiments with different batch sizes and see if it has an impact on the PCC plateau?\n\n4. Could you comment on the role of the direction of $w$ in theorem 2.2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YPvwTrQo7B", "forum": "hcKK5F6Hlf", "replyto": "hcKK5F6Hlf", "signatures": ["ICLR.cc/2026/Conference/Submission1339/Reviewer_y2UY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1339/Reviewer_y2UY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963093131, "cdate": 1761963093131, "tmdate": 1762915742142, "mdate": 1762915742142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}