{"id": "Ke5lWBOS3o", "number": 10922, "cdate": 1758184734514, "mdate": 1763541103972, "content": {"title": "DT-Pro: Proactive Decision Transformers with Implicit Latent Space Planning", "abstract": "Decision Transformers (DTs) address decision making problems through sequence modeling and have achieved surprisingly strong results. However, DTs still struggle in long-horizon tasks due to their poor planning ability. Existing works have demonstrated that subgoal prediction helps to guide DTs' decision making in complex and long-horizon tasks. However, explicit planning via subgoal prediction suffers from suboptimality, inefficiency and instability. In this paper, we present DT-Pro, a variant of DT that enhances its planning ability by integrating a natural implicit planning step into sequence modeling. Compared with explicit planning via subgoal prediction, the implicit planning works by inferring a latent plan from a structured plan space. Through this way, DT-Pro enables high-quality adaptive plan generation and efficient stepwise replanning with only a marginal increase in the computational cost.  Extensive experimental results show that DT-Pro achieves strong performance on a variety of widely used control and navigation benchmarks.", "tldr": "In this paper, we present DT-Pro, a variant of DT that enhances its planning ability by integrating a natural implicit planning step into sequence modeling.", "keywords": ["Reinforcement Learning", "Sequential Decision Making"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51a87d89fb6abfc0c385e3788d8adfc2f699ac71.pdf", "supplementary_material": "/attachment/ee9ed6cdc7827de7ab22a5987a636102fe32e2e8.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes DT-Pro, a Decision-Transformer variant that inserts an implicit latent-plan step. \nPipeline: \n(i) mine \"critical\" subgoals per trajectory using a decayed-RTG heuristic (Alg. 1), \n(ii) encode the subgoal sequence into a compact latent with a contrastive regularizer, and \n(iii) condition a DT-style action model on that latent (encoder frozen in stage-2) with step-wise replanning at test time. \nThe paper reports a higher average normalized return than DT/ADT/WT/CQL/IQL on D4RL Gym-MuJoCo and Maze2d, shows ablations (subgoal strategy, contrastive term, #subgoals), a sparse-reward variant using a DT-predicted RTG signal, and provides decoded-plan visualizations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Simple, targeted idea: Implicit latent plan sidesteps brittle explicit waypoints, integrates cleanly with DT.\n- Empirical signal on long horizons: Consistent gains on D4RL Gym-MuJoCo and Maze2d, subgoal-mining ablation is convincing.\n- Interpretability: Decoded subgoal visualizations suggest some learned temporal structure.\n- Scope & ablations: Vary #subgoals and (claimed) contrastive regularization, explore a sparse-reward setting.\n- Potential impact: If baselines are re-established under a common protocol with stronger statistics, DT-Pro could be a go-to DT variant for longer-horizon offline RL."}, "weaknesses": {"value": "- Objective inconsistency (paper vs. appendix vs. code): Main text describes CE/log-likelihood (plus contrastive) for subgoal decoding/policy, whereas algorithms use squared-error. The supplementary code appears to optimize MSE at both stages, with no contrastive term. This must be reconciled and reflected in the results.\n- Baseline provenance/fairness: Many baseline scores are imported (only a subset rerun), risking potential protocol drift (normalization/eval differences). Stronger conclusions require rerunning baselines under a unified pipeline.\n- Evaluation protocol clarity: Missing or underspecified train/val/test splits, checkpoint selection, exact D4RL normalization, and number of eval rollouts.\n- \"Optional plan decoding\" ambiguity: Abstract hints at optional decoding/search, but evaluation appears not to use any test-time plan decoding, clarify and, if applicable, report results with/without it.\n- Minor polish: Fix ADT average (73.7 vs. text 74.9), make captions self-contained (units, seeds), and add raw returns in the appendix.\n- Reproducibility: Supplementary code indicates default training uses MSE objectives and omits contrastive loss. Evaluation relies on encoder outputs, with no plan-sequence decoding at test time."}, "questions": {"value": "1. Losses: Are subgoal decoding and policy trained with CE/log-likelihood (main text) or L2 (Appendix Algs 3–4)? What does the released code actually optimize? If both were tried, please report a comparison.\n2. Baseline protocol: For Table 1, which baselines were re-run under your pipeline and which were imported? How did you ensure identical target returns, normalization, number of eval episodes, and scoring for imported numbers?\n3. Evaluation splits: What are your train/val/test and checkpoint-selection rules in offline replay? Please state D4RL normalization and eval rollout counts precisely.\n4. Capacity/budgets: Sensitivity to plan dimension, N subgoals (partially reported), and any plan-search budget. A small grid on Hopper-MR and Maze2d-medium would help.\n5. Sparse-reward: Using a pretrained DT to densify RTG - did you check for train/test contamination? How sensitive are results to the critic’s quality? Please add stds for all methods in Table 2.\n6. Test-time decoding/search: The abstract implies an optional decoding/search, sections.4.3/4.4 suggests it’s not needed. Did you try multiple latent-plan samples or a beam over subgoals at inference? If yes, report it. If not, clarify."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t3GmBKhFQ9", "forum": "Ke5lWBOS3o", "replyto": "Ke5lWBOS3o", "signatures": ["ICLR.cc/2026/Conference/Submission10922/Reviewer_32bG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10922/Reviewer_32bG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760630809864, "cdate": 1760630809864, "tmdate": 1762922122809, "mdate": 1762922122809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DT-Pro, a Decision-Transformer variant that inserts an implicit latent-plan step. \nPipeline: \n(i) mine \"critical\" subgoals per trajectory using a decayed-RTG heuristic (Alg. 1), \n(ii) encode the subgoal sequence into a compact latent with a contrastive regularizer, and \n(iii) condition a DT-style action model on that latent (encoder frozen in stage-2) with step-wise replanning at test time. \nThe paper reports a higher average normalized return than DT/ADT/WT/CQL/IQL on D4RL Gym-MuJoCo and Maze2d, shows ablations (subgoal strategy, contrastive term, #subgoals), a sparse-reward variant using a DT-predicted RTG signal, and provides decoded-plan visualizations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Simple, targeted idea: Implicit latent plan sidesteps brittle explicit waypoints, integrates cleanly with DT.\n- Empirical signal on long horizons: Consistent gains on D4RL Gym-MuJoCo and Maze2d, subgoal-mining ablation is convincing.\n- Interpretability: Decoded subgoal visualizations suggest some learned temporal structure.\n- Scope & ablations: Vary #subgoals and (claimed) contrastive regularization, explore a sparse-reward setting.\n- Potential impact: If baselines are re-established under a common protocol with stronger statistics, DT-Pro could be a go-to DT variant for longer-horizon offline RL."}, "weaknesses": {"value": "- Objective inconsistency (paper vs. appendix vs. code): Main text describes CE/log-likelihood (plus contrastive) for subgoal decoding/policy, whereas algorithms use squared-error. The supplementary code appears to optimize MSE at both stages, with no contrastive term. This must be reconciled and reflected in the results.\n- Baseline provenance/fairness: Many baseline scores are imported (only a subset rerun), risking potential protocol drift (normalization/eval differences). Stronger conclusions require rerunning baselines under a unified pipeline.\n- Evaluation protocol clarity: Missing or underspecified train/val/test splits, checkpoint selection, exact D4RL normalization, and number of eval rollouts.\n- \"Optional plan decoding\" ambiguity: Abstract hints at optional decoding/search, but evaluation appears not to use any test-time plan decoding, clarify and, if applicable, report results with/without it.\n- Minor polish: Fix ADT average (73.7 vs. text 74.9), make captions self-contained (units, seeds), and add raw returns in the appendix.\n- Reproducibility: Supplementary code indicates default training uses MSE objectives and omits contrastive loss. Evaluation relies on encoder outputs, with no plan-sequence decoding at test time."}, "questions": {"value": "1. Losses: Are subgoal decoding and policy trained with CE/log-likelihood (main text) or L2 (Appendix Algs 3–4)? What does the released code actually optimize? If both were tried, please report a comparison.\n2. Baseline protocol: For Table 1, which baselines were re-run under your pipeline and which were imported? How did you ensure identical target returns, normalization, number of eval episodes, and scoring for imported numbers?\n3. Evaluation splits: What are your train/val/test and checkpoint-selection rules in offline replay? Please state D4RL normalization and eval rollout counts precisely.\n4. Capacity/budgets: Sensitivity to plan dimension, N subgoals (partially reported), and any plan-search budget. A small grid on Hopper-MR and Maze2d-medium would help.\n5. Sparse-reward: Using a pretrained DT to densify RTG - did you check for train/test contamination? How sensitive are results to the critic’s quality? Please add stds for all methods in Table 2.\n6. Test-time decoding/search: The abstract implies an optional decoding/search, sections.4.3/4.4 suggests it’s not needed. Did you try multiple latent-plan samples or a beam over subgoals at inference? If yes, report it. If not, clarify."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t3GmBKhFQ9", "forum": "Ke5lWBOS3o", "replyto": "Ke5lWBOS3o", "signatures": ["ICLR.cc/2026/Conference/Submission10922/Reviewer_32bG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10922/Reviewer_32bG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760630809864, "cdate": 1760630809864, "tmdate": 1763645124365, "mdate": 1763645124365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new decision transformer (DT) architecture variant, showing state-of-the-art performance on range of offline RL control benchmarks. In difference to the standard DT architecture, the authors learn and use a sub-goal representation as an additional DT input to guide the auto-regressive next action prediction. The sub-goal representation is learned prior to the DT. The sub-goal training data is obtained from the traces in the training data, by splitting for each time step along the trace the remainder of the trace into n fragments, which are equally spaced in terms of the reward-the-go; using the staring point of each fragment as a sub-goal for the considered time step. An auto-encoder is trained to predict for each of those augmented training samples the associated sub-goals. The encoder part is then used to obtain the additional DT input. An experimental study demonstrates superior performance on the Mujoco and Maze2d benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a small but impactful optimization to the DT architecture. It is conceptually relatively simple, seems to introduce only a marginal overhead in training (although this should be evaluated more thoroughly), and leads to state-of-the-art performance in the considered control benchmarks. The text is overall well-written, clearly structured, and easy to follow. Code and benchmarks are available, which should suffice to reproduce the results."}, "weaknesses": {"value": "The authors however oversell their contributions. In particular, there are certain claims in the abstract and the introductions which are not in line with the proposed method or backed up by the experiments. Specifically, there are the following points:\n- \"Enhancing planning ability\": The proposed method improve the performance of DT, but there is no clear evidence that it would improve its \"planning ability\". First, it is actually not clear what \"planning ability\" should be in this context precisely. None of the components of the architecture performs any explicit planning step, e.g., search over multiple alternatives. Secondly, the benchmarks focus entirely on control benchmarks (and Maze2d), none of which require any strong \"planning ability\". To show this claim, the authors need to consider other benchmarks, like puzzles where strategic decisions are essential.\n- \"RTG-based plan search algorithm\": The method for finding the sub-goals is simple. The remaining trace at the step for which the sub-goals should be computed is simply split according to equally distributed intervals of the reward-to-go. Simplicity is not a bad thing, but clearly there is no \"planning\" or even real \"search\" involved. Don't oversell this method.\n- Improves \"Optimality of future plans\": It is not clear at all what this is supposed to mean. What are future plans? What is \"optimality\", and what does it mean to improve optimality? Bottom line is that the proposed method improves the DT performance in some (not even all benchmarks) by a moderate and sometime a considerable portion. And that is about it.\n- Improves \"interpretability and utility of the plans\": Again, how exactly should the proposed method achieve this? Without compelling explanation, I would argue that this claim is plain wrong.\n\nI also find the wording \"plan representation\" misleading. What is presented in sections 4.1 and 4.2 is a model to predict for state reward pairs a set of sub-goals (in terms of state landmarks at which a certain reward-to-goal fraction is reached) to may guide the DT in its next action predictions. At no point does it learn to predict a \"plan\".\n\nSome small clarity issues: The explantation for Algorithm 1 needs to be extended to cover the corner cases. It is not clear what is being done for those time steps where less than n (the number of to be chosen sub-goals) steps are remaining. In Section 4.2, it should be explained how the similarity between the traces is computed. In Section 4.4, it is not clear how a DT can be pre-trained to predict imaginary reward signals, i.e., how the reward function is reshaped to tackle the problem with the sparse reward signals."}, "questions": {"value": "1. How do you handle corner cases in algorithm 1? Can the same t' be selected for multple \\lambda_i?\n2. Could you provide some more justifications for the claimed contributions (cf. review)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n6oDXAxuEx", "forum": "Ke5lWBOS3o", "replyto": "Ke5lWBOS3o", "signatures": ["ICLR.cc/2026/Conference/Submission10922/Reviewer_mue2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10922/Reviewer_mue2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761132744514, "cdate": 1761132744514, "tmdate": 1762922121674, "mdate": 1762922121674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an enhanced variant of Decision Transformer (DT) designed to handle long-horizon tasks, where traditional DTs often struggle due to limited planning capability. The method introduces a two-stage training procedure involving three modules: a Plan Search module that identifies critical subgoals based on decaying returns-to-go, a Plan Coding module learns a compact latent space to represent the plans, and an Action Prediction module that executes these subgoals through generated actions. The approach demonstrates improved performance across selected benchmarks compared to standard DTs and related baselines."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-written and easy to follow, with clear motivation and logical structure.\n\n2. The proposed framework improves performance even under sparse or limited data conditions, showing robustness beyond ideal settings.\n\n3. The ablation studies are well-designed, supporting the validity of the method’s components."}, "weaknesses": {"value": "1. Increased training complexity and computational cost.\n\n    - The method requires two training stages and three modules, compared to the single-stage DT baseline.\n\n    - Although the authors report only 8–12% additional training time per added module, the first-stage cost is not clearly accounted for. The claim that it \"runs entirely offline before training\" is unclear, since all components are trained offline and should still contribute to total compute time.\n\n    - Clarifying the total wall-clock cost or presenting a fair compute comparison with DT would strengthen the paper.\n\n    - The authors mention \"pretraining a DT as a critic\" to provide granular returns-to-go (RTG) signals for Plan Search in sparse environments.\n\n2. Ambiguities in implementation details.\n\n    - The parameter N (number of subgoals) is said to vary by environment, but the rationale or selection criterion is not described.\n\n    - The definition of small-scale datasets used in the first-stage Plan Search module (\"runs entirely offline before training”) is vague, please specify what qualifies as \"small-scale\" and how it was chosen.\n\n3. Questionable experimental coverage for the stated objective.\n\n    - The paper claims to address long-horizon decision-making, yet all tested environments are relatively short-horizon MuJoCo tasks (e.g., UMaze and medium).\n\n    - More suitable benchmarks such as Maze2D-large, AntMaze, or FrankaKitchen (as used in OGBench) would better represent the intended objective.\n\n    - Additionally, recently proposed long-horizon baselines (e.g., TAP [1], diffusion-based planners [3]) are not included, despite being mentioned in the related works section. Omitting these comparisons weakens the evaluation’s credibility.\n\n4. Incremental contribution.\n\n    - While the method improves upon DT, it does so by adding extra components rather than addressing the core limitation of planning horizon in transformer-based RL.\n\n    - The improvement (~18–24% increase in performance with comparable extra training time) is promising but might not constitute a significant conceptual advance beyond existing DT variants.\n\n    - The authors could strengthen their contribution by positioning their approach relative to diffusion-based planning methods [3] or latent-action planners [1,2].\n\n**Minor Suggestions (Not affecting the score)**\n\n1. To facilitate comparison, align the order of tasks in Table 1 with that in Table 2 of the original DT paper, maintaining consistency with prior work.\n\n** References**\n\n[1] Zhang, Tianjun, et al. \"Efficient Planning in a Compact Latent Action Space.\" *The Eleventh International Conference on Learning Representations*.\n\n[2] Park, Seohong, et al. \"OGBench: Benchmarking Offline Goal-Conditioned RL.\" *The Thirteenth International Conference on Learning Representations*.\n\n[3] Janner, Michael, et al. \"Planning with Diffusion for Flexible Behavior Synthesis.\" *International Conference on Machine Learning*. PMLR, 2022."}, "questions": {"value": "1. What qualifies as the “small-scale benchmark dataset” used for the first module? How do you select it in practice?\n\n2. Why was TAP [1] not included as a baseline, given that it targets the same long-horizon problem space and is cited in the related works?\n\n3. How does your method compare conceptually and empirically to diffusion-based planners [3], which also emphasize flexible long-horizon planning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6XrSLcrGaq", "forum": "Ke5lWBOS3o", "replyto": "Ke5lWBOS3o", "signatures": ["ICLR.cc/2026/Conference/Submission10922/Reviewer_NXcP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10922/Reviewer_NXcP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950186561, "cdate": 1761950186561, "tmdate": 1762922121132, "mdate": 1762922121132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}