{"id": "QCMFRe9MM4", "number": 8127, "cdate": 1758066736629, "mdate": 1759897805326, "content": {"title": "Planning with Unified Multimodal Models", "abstract": "With the powerful reasoning capabilities of large language models (LLMs) and vision-language models (VLMs), many recent works have explored using them for decision-making. However, most of these approaches rely solely on language-based reasoning, which limits their ability to reason and make informed decisions. Recently, a promising new direction has emerged with unified multimodal models (UMMs), which support both multimodal inputs and outputs. We believe such models have greater potential for decision-making by enabling reasoning through generated visual content. To this end, we propose Uni-Plan, a planning framework built on UMMs. Within this framework, a single model simultaneously serves as the policy, dynamics model, and value function. In addition, to avoid hallucinations in dynamics predictions, we present a novel approach self-discriminated filtering, where the generative model serves as a self-discriminator to filter out invalid dynamics predictions. Experiments on long-horizon planning tasks show that Uni-Plan substantially improves success rates compared to VLM-based methods, while also showing strong data scalability, requiring no expert demonstrations and achieving better performance under the same training-data size. This work lays a foundation for future research in reasoning and decision-making with UMMs.", "tldr": "We present a way to solve long horizon decision making tasks by planning with Unified Multimodal Models.", "keywords": ["Planing", "Decision Making", "Unified Multimodal Models"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eda059fd57bd908b27b340390563c208a1dee256.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper argues that unified multimodal models (UMMs) can improve long‑horizon planning by treating a single generative model as policy, dynamics, and value, and by “reasoning through generated visual content.” It further proposes self‑discriminated filtering, where the same generative model filters its own dynamics predictions to reduce hallucinations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear articulation of the limitation of language‑only reasoning for decision‑making and a plausible motivation for multimodal reasoning.\n - The idea of integrating policy, dynamics, and value into a single UMM is conceptually coherent and could simplify interface complexity.\n - Self‑discriminated filtering targets a genuine pain point (hallucinated dynamics) and aims to avoid reliance on external validators."}, "weaknesses": {"value": "- Self‑discriminated filtering may be circular: Having the generator judge its own rollouts can lead to correlated errors. Without an independent criterion (calibration, consistency checks across models, or grounded constraints), filtering may either accept coherent hallucinations or over‑reject plausible futures.\n- How is uncertainty addressed in partially observable environments?\n- Relation to prior model‑based RL and multimodal planning: The positioning relative to existing unified architectures (e.g., world models that produce latent visuals, diffusion planners with internal consistency checks) is underdeveloped."}, "questions": {"value": "- How does the framework differ from world models that generate latent visuals and from diffusion‑based planners with internal consistency checks? \n- How does a UMM that reasons via generated visuals compare to a MuZero‑style latent‑space world model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mYrNiXCevY", "forum": "QCMFRe9MM4", "replyto": "QCMFRe9MM4", "signatures": ["ICLR.cc/2026/Conference/Submission8127/Reviewer_rs1q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8127/Reviewer_rs1q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760510866754, "cdate": 1760510866754, "tmdate": 1762920103352, "mdate": 1762920103352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Uni-Plan, a planning framework using unified multimodal models, as opposed to LLMs or VLMs. They propose one model to act as a policy, dynamics model and value function and introduce a self-discriminated filtering mechanism to filter out hallucinations. They evaluate their planning system against open source and closed source VLM-based planning methods, outperforming open-source benchmark and getting close to the closed source GPT-5-Thinking model, whilst their model has a much smaller number of parameters (14B)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The authors choose a comprehensive set of baselines for comparison, with both open-source and closed-source models. I particularly appreciate the results showcasing a similar performance to GPT-5-Thinking-Tool, a closed-source model, with a much more significant training dataset and compute budget. This way, authors provide an open alternative to the research community.\n2) Compared to related work, such as VLP (video language planning), it proposes a single model for policy, dynamics and value function, thus reducing inference time. As expressed in the conclusion, there is, however, further work to be done to improve inference times, especially if this method aims to be applied to real-world planning scenarios.\n3) Proposes a self-discriminator filtering mechanism to address and reduce hallucinations and strengthen the dynamics model quality. I also appreciated the additional object-count consistency check to further validate observation predictions."}, "weaknesses": {"value": "1) Given it's a key component in the self-discriminated filtering mechanism, I would've liked to see a more detail on the inverse dynamics inference method used.\n2) The authors state that the system addresses long-horizon planning tasks, but I could not see any definition of the choice of hyperparameter `H` for time-horizon and in all 3 environments it looks like the planning task is in the order of tens of steps or less. It would be good if authors could elaborate more on what they regard as long-horizon.\n3) Even though I really appreciate the inclusion of the real-world planning example with the dual-arm robot in Section 3.1, I fear only one example in Fig 3 would not suffice to make a claim the planning framework applies reliably to such scenarios. I would've expected a quantitative analysis on the task (FID between imagined state and real execution).\n4) Similarly, I would've liked to see numbers on the benefit of self-discriminating filtering across all environments (not just language task) in Fig 5. I'm not sure if they might be included in the appendix.\n5) In Fig 4, where authors showcase finetuned BAGEL is a strong dynamics model, this is illustrated only as 1 short example (4 steps) - I believe a quantitative evaluation across all environments and multiple runs would make the claim stronger."}, "questions": {"value": "-  *Suggestion*: There should be a small correction in section 3.3 - authors state Uni-Plan consistently achieves higher performance with fewer trajectories, but in Fig 6, on the language table, the performance on Uni-Plan with 500 traj is on par with (if not a bit lower than) the performance of Qwen2.5 with 1K trajectories. In the abstract and the conclusion, the claim is corrected to state \"outperforming VLMs when trained with the **same** amount of data\".\n- *Suggestion*: It would be good to add the number of parameters for BAGEL-VLM in `Table 1` for clarity and completeness in the comparison.\n- *Question*: Could the authors elaborate more on how the hyperparameters A, D, B and H are chosen? I see in the appendix the beams B are 2 for all environments - what is the rationale for choosing that value?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Folww0hZco", "forum": "QCMFRe9MM4", "replyto": "QCMFRe9MM4", "signatures": ["ICLR.cc/2026/Conference/Submission8127/Reviewer_1NeT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8127/Reviewer_1NeT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667220787, "cdate": 1761667220787, "tmdate": 1762920102951, "mdate": 1762920102951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Uni-Plan, a planning framework built on unified multimodal models (UMMs) that jointly handle text and image reasoning for decision-making. Unlike prior language- or vision-language-based planners that rely mainly on textual reasoning, Uni-Plan employs a single UMM as the policy, dynamics model, value function, and self-discriminator. A key innovation is the proposed self-discriminated filtering mechanism, where the model validates its own predicted dynamics by comparing inferred inverse actions with executed ones, effectively reducing hallucinated transitions. Experiments on long-horizon planning benchmarks demonstrate that Uni-Plan achieves about 30% higher success rates than open-source VLM-based methods, matches the performance of the GPT-5-Thinking model, and shows strong data scalability without requiring expert demonstrations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper convincingly demonstrates that a unified multimodal model can serve simultaneously as (i) a policy, (ii) a dynamics model, (iii) a self-discriminator, and (iv) a value function. This is a very interesting idea, and the paper provides solid evidence of its effectiveness.\n2. Under this unified setting, the UMM framework significantly boosts the performance of the base VLM, showing clear advantages in planning and reasoning."}, "weaknesses": {"value": "The experimental settings are relatively simple. All real-world and simulated environments are fully observed and easy to predict. I believe the main challenges in this domain lie in semantic understanding, which is largely powered by the base model. For example, a sufficiently strong base model such as GPT-5-Thinking-tool could outperform even a carefully designed architecture like the one proposed in this paper."}, "questions": {"value": "1. Could you further demonstrate the dynamic model’s ability in more unseen environments, or under more challenging real-world conditions?\n2. Could you provide additional real-robot experiments, such as pick-and-place tasks involving occlusion or unseen objects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "63O8PR95wM", "forum": "QCMFRe9MM4", "replyto": "QCMFRe9MM4", "signatures": ["ICLR.cc/2026/Conference/Submission8127/Reviewer_VZiD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8127/Reviewer_VZiD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854414406, "cdate": 1761854414406, "tmdate": 1762920102594, "mdate": 1762920102594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a high-level planning method that uses foundation models, specifically unified multi-modal models (UMMs). The UMM is used in four distinct roles: 1) as a policy, 2) as a dynamics model, 3) self-discriminator, and 4) value function.\n\nThe proposed method (Uni-Plan) utilizes an image of the environment's initial state and a textual description of the goal to produce a multi-step plan, where each step is executed by a low level controller (the learning of this controller is outside of the paper's scope).\n\nAt each step, the method involves 1) sampling A actions from the policy, 2) generating D next observations, and 3) filtering the D next observations down to 1 observation by tracing which action could have led to the next observations and matching it to the actual action. The authors do this for B beams."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The novel contributions include: 1) using a UMM, 2) fine-tuning with non-expert trajectories, and 3) using a UMM as a self-discriminator. These are all interesting contributions.\n2) The proposed method shows high performance with much fewer trajectories—these results are promising!"}, "weaknesses": {"value": "1) The authors should cite the published version of the papers in their references (e.g., \"Do as I can, not as I say: Grounding language in robotic affordances\" was published at CoRL in 2022; \"Inner monologue: Embodied reasoning through planning with language models\" was published in CoRL in 2022; etc.).\n2) The images of figure 3 are slightly out of focus and there is a black edge at the top of each photo that makes them appear unaligned horizontally.\n3) No true empirical comparison to prior planning approaches with LLMs/VLMs. Experiments only vary the model type (VLM vs. UMM) and model size. Authors claim to have prior planning approaches as baselines, but I would argue that their baselines are VLM-based plan generators not prior approaches. For example, a good baseline for comparison would be SayCan [Ahn et al., 2022].\n4) No statistical rigour."}, "questions": {"value": "1) How many seeds/trials did you run? I see no mention of this and I see no reported standard error or standard deviation.\n2) Feel free to address any weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fuSdatIrGU", "forum": "QCMFRe9MM4", "replyto": "QCMFRe9MM4", "signatures": ["ICLR.cc/2026/Conference/Submission8127/Reviewer_Lo2k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8127/Reviewer_Lo2k"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954687885, "cdate": 1761954687885, "tmdate": 1762920102211, "mdate": 1762920102211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}