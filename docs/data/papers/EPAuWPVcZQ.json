{"id": "EPAuWPVcZQ", "number": 9021, "cdate": 1758107437572, "mdate": 1759897747773, "content": {"title": "A Plug-and-Play Agentic Framework for Text Guided Image Editing", "abstract": "Text-guided image editing with diffusion models struggles to maintain fidelity during complex, multi-aspect edits, where simultaneous changes and preservations are required. While one-shot prompt rewriting offers some improvement, it lacks fine-grained control, often leading to under-editing of desired attributes or over-editing of unrelated regions, or both. To address these gaps, we introduce an agent called `ARTIE` (**A**uditable **R**efinement for **T**ext-Guided **I**mage **E**diting), which is a plug-and-play, inference-time, feedback-based agentic system to enhance pre-trained diffusion models such as Stable Diffusion. At its core, `ARTIE` is organized into three agentic sub-modules: (1) a perception module (`SceneDiff`), which detects over-editing by comparing source and target scene graphs, and under-editing by grounding edit requirements through a dual-verification pipeline comprising an open-set object detector and CLIP; (2) a reasoning/planner module (an LLM-based Prompt Engineer), which takes the diagnostic signals from `SceneDiff` and synthesizes refined positive prompts together with asymmetrically weighted negative prompts; and (3) an action module (the image generator), which executes these refined prompts to produce improved images iteratively. This perception–reasoning–action loop runs in multiple cycles, producing high-quality edited images. Consequently, `ARTIE` also yields an auditable trail of refinement steps where each modification is explained and justified via explicit feedback signals from the perception module. Further, `ARTIE` operates solely through guided prompt engineering, without requiring model retraining or fine-tuning, making it a plug-and-play architecture. Despite being training-free, when applied on top of Stable Diffusion, `ARTIE` consistently improves fidelity and control in multi-aspect editing. Its performance matches or surpasses specialized baselines, thereby setting a new state-of-the-art for explainable, inference-time agentic image editing.", "tldr": "", "keywords": ["text guided image editing", "agentic image editing", "diffusion model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4e363a2eeb74b628cecc1fd7b188cb2d086bb433.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents ARTIE, a plug-and-play agentic framework for text-guided image editing. The system organizes editing into a perception–reasoning–action loop composed of three modules: a visual verifier (SceneDiff) that detects under- or over-editing through OWL-ViT and CLIP similarity, an LLM-based prompt engineer that rewrites text instructions with asymmetric weighting of positive and negative prompts, and a diffusion image generator that iteratively refines results based on a CLIP-derived stopping criterion. Experiments are conducted on MagicBrush and a custom Car125 dataset. While the framework is conceptually interesting, its methodological soundness and empirical support are limited."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The idea of introducing an agentic feedback loop for editing aligns well with current trends in reasoning at inference-time and autonomous self-reflection.\n2. The design is modular and training-free, making it easy to adapt to existing models."}, "weaknesses": {"value": "1. Unclear and questionable use of text-to-image models for editing. The framework employs Stable Diffusion 1.5 and 3.5, which are text-to-image generators not designed for image editing. The paper does not explain how the input image is incorporated, e.g., through noise inversion, latent conditioning, or token concatenation. So, it remains unclear whether ARTIE performs actual editing or simply re-generates new scenes. Without conditioning or cross-attention control, background and object consistency cannot be maintained, and prompt refinement alone cannot guarantee structural fidelity. Furthermore, comparing against text-to-image models rather than established editing models such as InstructPix2Pix, InstructDiffusion, FLUX-Kontext [1], Step1X-Edit [2], or OmniGen2 [3] results in unfair and methodologically weak comparisons.\n2. Limited and inconsistent empirical gains. As shown in Table 1, ARTIE does not consistently outperform baselines, and its improvement is not very significant over SD 3.5 on MagicBrush, which is the more general editing benchmark.\n3. Narrow dataset scope. The custom Car125 dataset focuses almost entirely on vehicle damage restoration, a narrow and synthetic domain that does not demonstrate general-purpose editing capability. The dataset is also unreleased, limiting reproducibility.\n4. Overreliance on CLIP-based metrics. Evaluation relies exclusively on CLIP-based scores, without perceptual (e.g., LPIPS, FID) or human studies, making it difficult to assess real editing quality or visual realism.\n\nReference:\n- [1] \"FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space\", https://arxiv.org/abs/2506.15742\n- [2] \"Step1X-Edit: A Practical Framework for General Image Editing\", https://arxiv.org/abs/2504.17761\n- [3] \"OmniGen2: Exploration to Advanced Multimodal Generation\", https://arxiv.org/abs/2506.18871"}, "questions": {"value": "1. How is the input image integrated into the diffusion process? Does the method use noise inversion, condition image token concatenation, or another conditioning mechanism?\n2. How does the method using T2I model preserve background and object consistency across iterations?\n3. Why were text-to-image models used instead of editing-specialized models such as InstructPix2Pix, FLUX-Kontext [1], Step1X-Edit [2], or OmniGen2 [3]?\n4. Can the authors provide quantitative or qualitative evidence that the proposed prompt refinement improves editing fidelity?\n5. Will the Car125 dataset and code be released to ensure reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dCCjRUrdmR", "forum": "EPAuWPVcZQ", "replyto": "EPAuWPVcZQ", "signatures": ["ICLR.cc/2026/Conference/Submission9021/Reviewer_2Chf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9021/Reviewer_2Chf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626857879, "cdate": 1761626857879, "tmdate": 1762920745204, "mdate": 1762920745204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a plug-ang-play multi-agent pipelines to improve image editing. It basically have a set of modules, including VLMs, LLMs, and object detectors, and more. The mluti-agent pipeline improves the performance on the SD-1.5/SD-3.5 on two benchmarks, Car125 and MagicBrush."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clear and easy to follow.\n2. The plug-and-play multi-agent system improves the performance on two models and two benchmarks."}, "weaknesses": {"value": "- While the paper is technically sound with performance gain, the idea of plug-and-play pipelines for diffusion generation is not a new idea. For instance, [1, 2] proves that using LLM object planning can improve prompting a lot in image generation/editing. [3] then extends this idea with VLM modules, which is close to the paper's agentic setting already. The reviewer believes that these papers worth discussions and even be the baseline in Table 1. It is worth noting that the use of OWL-VIT and LLM engine is the same setup as [2].\n\n- The paper did the experiment on a benchmark called Car125 they created. However, it's a bit unclear why focuusing onthe pre-accident vs post-accident images, especially both descriptions and pre-accident images are all generated as opposed to a real one. Also, the MagicBrush dataset only evaluates high-level, global metrics like CLIP. The reviewer suggests running experiments on fine-grained object-level benchmarks [2] to test counting, negation, spatial understanding, or those benchmarks with real human prompts in GEdit [4].\n\n- The multi-step, multi-agent pipeline would incur a lot of computational overhead. The reviewer believes this should be discussed and mentioned in the main paper at least. Also, it would be good to analyze failure cases of this multi-agent collaboration scenario, perhaps following [5].\n\n\n[1] Lian, Long, et al. \"Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models.\" TLDR.\n\n[2] Wu, Tsung-Han, et al. \"Self-correcting llm-controlled diffusion models.\" CVPR 2024.\n\n[3] Wang, Zhenyu, et al. \"Genartist: Multimodal llm as an agent for unified image generation and editing.\" NeurIPS 2024.\n\n[4] Liu, Shiyu, et al. \"Step1x-edit: A practical framework for general image editing.\" arXiv 2025.\n\n[5] Cemri, Mert, et al. \"Why do multi-agent llm systems fail?.\" arXiv 2025."}, "questions": {"value": "Please read the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jttYBPp49o", "forum": "EPAuWPVcZQ", "replyto": "EPAuWPVcZQ", "signatures": ["ICLR.cc/2026/Conference/Submission9021/Reviewer_BoSM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9021/Reviewer_BoSM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792988857, "cdate": 1761792988857, "tmdate": 1762920744583, "mdate": 1762920744583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ARTIE (Auditable Refinement for Text-Guided Image Editing), an agentic, plug-and-play framework that enhances existing diffusion models for text-based image editing. The key idea is to add an inference-time perception–reasoning–action loop that iteratively refines edits without retraining. For complex image editing, I believe agentic workflows will be very crucial and therefore this is a timely topic."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed perception–reasoning–action loop is well-motivated and effectively integrates verification, planning, and generation in a training-free manner. This is a clear conceptual advance over single-pass diffusion editors.\n\n- The framework can be applied to any pretrained diffusion model, enhancing fidelity without requiring additional training or finetuning. This makes it highly practical for adoption.\n\n- ARTIE’s structured scene graph and aspect-grounding diagnostics provide interpretable feedback, a valuable feature for explainable AI and visual debugging in generative editing."}, "weaknesses": {"value": "Inspire of the strengths, I believe the paper has some weaknesses which the authors are requested to answer:\n\n- The paper applies the agentic workflow on slightly older models (e.g., SD-1.5) and only the SD-3.5 is a slightly newer model. I would request the authors to run the workflow on more recent models (e.g., Flux variants which are tailored for image editing).\n\n- The paper doesn't compare with strong one-shot editing models (e.g., GPT-image, Flux context), given Instruct-Pix-2-Pix is slightly old. Moreover I believe the performance of Pix2Pix is not very far off from the agentic workflow. Therefore the authors need to show the benefit over using the workflow in conjunction with editing models. \n\n- I believe the dataset used for image editing is simple and doesn't fully capture the complexity of real-world edits which need to be very precise. I would urge the authors to take a look at Reddit PSNR dataset or ImgEdit datasets."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uy56NImHiV", "forum": "EPAuWPVcZQ", "replyto": "EPAuWPVcZQ", "signatures": ["ICLR.cc/2026/Conference/Submission9021/Reviewer_qScw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9021/Reviewer_qScw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762188037904, "cdate": 1762188037904, "tmdate": 1762920743939, "mdate": 1762920743939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ARTIE (Audit-trail based Refinement for Text-Guided Image Editing) — a plug-and-play, inference-time agentic wrapper for pretrained diffusion editors (Stable Diffusion). ARTIE runs a perception → reasoning/planning → action loop: (1) SceneDiff (perception) constructs scene graphs (via a VLM) to detect over-edits (preservation-failure triplets) and uses OWL-ViT + CLIP aspect grounding to detect under-edits; (2) an LLM-based Prompt Engineer converts diagnostic signals (failed aspects + uncertainty scores) into refined positive prompts and uncertainty-weighted negative prompts; (3) the image generator (Stable Diffusion) re-runs the edit with the refined prompts and the loop repeats until a reference-free HM-CLIPScore picks the best iteration. The paper is well written and easy to understand. The authors evaluate on a new multi-aspect Car125 dataset (constructed from DamageCarDataset  and the general MagicBrush benchmark, reporting quantitative gains and qualitative improvements over SD-1.5/3.5, InstructPix2Pix, InstructDiffusion and an agentic baseline (GenArtist)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear, modular agentic loop that is plug-and-play. ARTIE requires no retraining of the diffusion backbone and formalizes a perception→reasoning→action loop (SceneDiff + LLM prompt engineer + SD generator) that is easy to attach to existing systems. This design choice is explicitly stated and demonstrated.\n\nDisentangled verification (over-edit vs under-edit) using scene graphs + aspect grounding. The SceneDiff module produces preservation-failure triplets (set difference of source/target scene graphs) for over-editing and OWL-ViT+CLIP-based aspect grounding for missing edits, a structured, auditable diagnostic that directly informs prompt refinement. \n\nWell-chosen evaluation metrics and stopping criterion. The harmonic HM-CLIPScore (harmonic mean of CLIP-T and CLIP-I) is used both as a balanced evaluation metric and as a reference-free stopping criterion that empirically picks better final edits."}, "weaknesses": {"value": "Dependence on external VLM/LLM/verifier stack (error propagation & bias). ARTIE’s SceneDiff relies on LLaMA-4 Maverick for scene graphs, OWL-ViT for localization, and CLIP for grounding; the paper admits errors/bias in these tools propagate and can lead to incorrect corrections. This is a significant dependency (and a practical failure mode) because the agent can only be as good as its verifiers. The limitations section notes this, but more analysis in how the affect of different verifiers affect the approach.\n\nBaseline parity and selection could be stronger/more matched. The authors compare to InstructPix2Pix, InstructDiffusion and GenArtist — but InstructPix2Pix and InstructDiffusion are trained models while ARTIE is an inference wrapper on SD; some baselines are trained specifically on MagicBrush (giving them an advantage). The comparisons in Table 1 are useful but need clearer matched-setup runs (same prompts, same prompt length allowances) and per-category parity.\n\nLatency and compute overhead for interactive use. ARTIE’s loop adds notable runtime: per-iteration costs include tLLM ≈ 2.5 s, tSD ≈ 30 s; total ≈ 36 s per edit (T=5, NFE=50) on an A100 — this makes ARTIE expensive for interactive applications. The appendix reports runtimes but the main text should discuss practical deployment implications and acceptable latency tradeoffs. \n\nHeuristic negative prompts & uncertainty calibration are ad-hoc. The uncertainty-weighted negative prompt mechanism is heuristic. The authors note it can mishandle subtle attributes and plan to learn uncertainty calibration in future work — but current results may hinge on prompt engineering choices that are not fully robust across domains."}, "questions": {"value": "How often does SceneDiff produce incorrect preservation triplets that lead to harmful corrections? Can you quantify false positive/negative rates on a manual subset\n\nHow sensitive are final results to the LLM prompt engineering templates and few-shot examples used for prompt rewriting? (LLM failures can introduce hallucinated constraints.)\n\nCan the authors show results for (a) no negative prompt, (b) heuristic weighting (current) and intuition behind the weighting"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4p6HO0gVWh", "forum": "EPAuWPVcZQ", "replyto": "EPAuWPVcZQ", "signatures": ["ICLR.cc/2026/Conference/Submission9021/Reviewer_wgdD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9021/Reviewer_wgdD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762272097074, "cdate": 1762272097074, "tmdate": 1762920743487, "mdate": 1762920743487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}