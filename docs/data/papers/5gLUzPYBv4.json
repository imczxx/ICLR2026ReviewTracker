{"id": "5gLUzPYBv4", "number": 23884, "cdate": 1758349801769, "mdate": 1759896792542, "content": {"title": "Adversarial Agent Collaboration for C to Rust Translation", "abstract": "Translating C to memory-safe languages, like Rust, prevents critical memory safety vulnerabilities that are prevalent in legacy C software.\nExisting approaches for C to safe Rust translation, including LLM-assisted ones, do not generalize on larger ($> 500$ LoC) C codebases because they depend on complex program analyses that frequently break.\nIn this work, we present ACToR (**A**dversarial **C** **To** **R**ust translator, a simple LLM agent-based approach. \nInspired by GANs, ACToR pits a generator agent against a discriminator agent, which collaborate to iteratively generate a Rust translation. \nOn each iteration, the translator agent synthesizes and refines a Rust translation to pass an existing suite of tests, and then the discriminator agent finds new failing tests.\nWe demonstrate that ACToR translates all of the 63 real-world command-line utilities considered in our benchmarks, which have an average size of 485 lines of code, and it achieves over 90% test pass rate with zero human intervention. To our knowledge, it is the first such system that reliably translates C programs of this scale. Furthermore, ACToR improves translation correctness by up to 18.9% compared to baseline, non-adversarial approaches.", "tldr": "", "keywords": ["Automatic Program Translation", "Code Generation", "Program Synthesis", "LLM Agents"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2611a439a837f67291a6bfe53ef8032d20807e6.pdf", "supplementary_material": "/attachment/085feed41eb74e3c1e1ca0431e1787f7d5be5335.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ACToR (Adversarial C To Rust translator), an adversarial two-agent workflow for translating C programs to safe Rust. A translator LLM agent iteratively produces a Rust version, while a discriminator agent searches for counterexample tests that expose behavioral mismatches. The newly found failing tests are added to the test suite. The method is evaluated on two benchmarks: a micro-benchmark and a macro-benchmark. ACToR improves translation correctness by up to 18.9% compared to the baseline approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and Effective Framework: This work tries to address a well-known weakness in automated code generation, i.e., the difficulty of ensuring semantic equivalence beyond a fixed set of tests\n2. Good ablation studies: the paper isolates the effect of adversarial test generation and a simple fuzzer\n3. Include evaluation with the agentic frameworks."}, "weaknesses": {"value": "1. Correctness is proxied by tests. However, tests may not be able to expose real bugs. No human validation of the translated code or the quality / sufficiency of the test cases.\n2. I am wondering how well this method could generalize to other existing datasets, e.g., CRUST-Bench: A Comprehensive Benchmark for C-to-safeRust Transpilation?\n3. The cost estimation (e.g., tokens / API cost) seems to be neglected for comparison and evaluation of different approaches.\n4. There are many papers that use LLMs for code translation. I am wondering about the technical novelty compared with prior approaches. Also, other papers claim that they do repo-level translation. Why do you claim that your work is the first to work on large-scale programs? Is it really large-scale compared with repo-level's prior work?"}, "questions": {"value": "1. How do you compare your benchmark to \"CRUST-Bench: A Comprehensive Benchmark for C-to-safeRust Transpilation\"?\n2. Is there any guarantee that the final generated Rust programs are really memory-safe? Or the agents could also escape certain checks to bypass? How much confidence in correctness can you get out of the generated Rust programs? Is there any manual validation?\n3. What if you only use the fuzzing script without the agent?\n4. Can you compare your approach with the other paper, \"Exploring and Unleashing the Power of Large Language Models in Automated Code Translation\", and tell me what your technical novelty is? Similarly, for this paper, \"AlphaTrans: A Neuro-Symbolic Compositional Approach for Repository-Level Code Translation and Validation\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZB7g5JfEXS", "forum": "5gLUzPYBv4", "replyto": "5gLUzPYBv4", "signatures": ["ICLR.cc/2026/Conference/Submission23884/Reviewer_NQvr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23884/Reviewer_NQvr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761596403742, "cdate": 1761596403742, "tmdate": 1762942839348, "mdate": 1762942839348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the task of translating C code to safe Rust code with an adversarial setup of LLM agents. By having an agent that performs the translation interact with another that comes up with tests that expose the mismatch between C and Rust code, ACToR steers translation towards general correctness instead of overfitting to a fixed test suite. The authors showed that ACToR is effective on 63 real-world programs of non-trivial size and outperforms naive, coverage-guided, and ablated approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work addresses an important problem using an effective technique that makes intuitive sense and is well-executed. It makes sense to grow the test suite dynamically, and in addition for tests to maximally distinguish code in the two languages. Giving the discriminator agent access to a fuzzing script is a nice touch. As the authors convincingly demonstrate through pass rates relative to each competing tools, these bits give ACToR the edge over baselines and ablations. The paper is well-written, well-structured, and clearly presents this approach and findings. While not ground-breaking and despite a few issues below, I find this work to be a meaningful contribution to the field overall."}, "weaknesses": {"value": "My complaints with the paper are mostly the following:\n\n1. Unsound problem definition. The authors' definition of C-to-Rust translation does not require exclusive use of safe features, yet claims that \"memory safe handling of malicious inputs is guaranteed by the Rust compiler itself.\" Since unsafe code may be involved, this assumption does not hold - it is entirely possible for inputs outside the valid universe U to expose vulnerabilities on the Rust side, which may be the same as or different from those on the C side, and which, at worst, are new vulnerabilities altogether. As such scenarios are not considered in the behavior equivalence check, such a problem definition (and any of its solutions) would defeat the purpose of what motivates translating from C to Rust in the first place (though ACToR apparently does not literally address this problem definition, as it is said to enforce safe Rust as a post-processing step).\n\n2. Unprincipled experimental configuration. The decision to use 10 turns and 3 tests each turn feels handwavy. It seems entirely likely that with more turns and more tests added, more divergences between the C code and Rust code would be exposed. While I understand that the authors are limited by the cost of running the LLMs, I think it is very worthwhile to experimentally study the trade-offs between various (# turns, # tests) configurations and how they impact the degree of translation correctness ACToR can achieve."}, "questions": {"value": "1. Could the authors comment on the issue with the unsound problem definition?\n  \n    Relatedly, what does the authors mean by a post-processing step that enforces safe Rust? Is it a check that looks for unsafe code, or does it transform unsafe code to safe code? Regardless, it is my opinion that a problem definition that requires safe Rust use and a design that enforces safety in transit would make ACToR more principled and impactful. One benefit, for instance, is that keeping Rust code safe throughout the process would prevent overhead associated with backtracking from unsafe code and steer the translation along a optimistic/promising direction. Modifying ACToR in this respect should not be too difficult to implement.\n\n2. Would the authors consider studying the trade-offs between various experimenal configurations and their impact on translation correctness?\n\nMinor: Could the authors briefly comment on how the \"15 manually crafted, diverse seed tests\" are created? I would also be curious to learn how this initial test suite could impact ACToR's performance. Would the authors be open to consider doing an ablation study on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ibhsUTqVBr", "forum": "5gLUzPYBv4", "replyto": "5gLUzPYBv4", "signatures": ["ICLR.cc/2026/Conference/Submission23884/Reviewer_hYZH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23884/Reviewer_hYZH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866025399, "cdate": 1761866025399, "tmdate": 1762942839034, "mdate": 1762942839034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ACToR, an adversarial two‑agent framework for translating C programs to safe Rust. A translator agent proposes a Rust program while a discriminator agent generates tests, including fuzzing‑aided cases, to expose behavioral mismatches, and the process iterates. On 6 micro programs and 57 BSDCoreUtils utilities (median 485 LOC), ACToR reports high test pass rates, outperforming non‑adversarial baselines and showing up to 18.9% relative pass‑rate gains on the macro benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a timely and challenging problem: translating nontrivial C codebases to safe Rust with minimal human intervention.\n- ACToR consistently improves over the naive baseline across three agent-model choices."}, "weaknesses": {"value": "- The core comparisons are against a “naive” single-agent baseline and a “coverage” baseline. There is no head‑to‑head comparison with recent C→Rust systems that combine LLMs with analysis.\n- The evaluation lacks runtime, token, and financial cost information, which is key for judging practicality at scale.\n- Results appear to be single‑run without variance across seeds or model nondeterminism. Stability across runs is not reported. The text notes three programs that aborted before 10 iterations after multiple reruns, hinting at variability. \n- The paper states “to our knowledge, it is the first such system that reliably translates C programs of this scale.” Given the breadth of recent efforts that blend LLMs with program analysis, it would be safer to qualify the claim with the particulars of the benchmark and evaluation protocol, or provide a stronger head‑to‑head against at least one recent approach on a shared subset."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YEyfeoFDGF", "forum": "5gLUzPYBv4", "replyto": "5gLUzPYBv4", "signatures": ["ICLR.cc/2026/Conference/Submission23884/Reviewer_vkya"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23884/Reviewer_vkya"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007142740, "cdate": 1762007142740, "tmdate": 1762942838786, "mdate": 1762942838786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}