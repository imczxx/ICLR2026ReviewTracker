{"id": "QOfHAW7PxE", "number": 17329, "cdate": 1758274765736, "mdate": 1763012182946, "content": {"title": "A Unified Data and Model-Centric Framework for Robust Facial Emotion Recognition", "abstract": "Recent Progress in Deep Learning (DL) has shown that data quality constrains the generalization as much as model design. Facial Emotion Recognition (FER) exemplifies this challenge, as widely used datasets contain mislabeled, duplicated, class imbalanced, and visually affected samples that weaken both accuracy and robustness. In this paper we proposed a data-centric approach to FER, building a systematic pipeline that improves dataset reliability before model training. The pipeline includes (i) Noisy and duplicated samples removal, (ii) landmark-guided facial refinement, and (iii) class-aware re-balanced under-presented emotions in the dataset. Following the data-centric pipeline we proposed a lightweight hybrid CNN-Transformer student model with Emotion Aware Dynamic Distillation (EADD), where knowledge is adaptively distilled from multiple teacher networks depending on their emotion-specific strengths. Despite the multi-teacher knowledge distillation student model is further optimized by adversarial training to enhance its robustness against subtle perturbations in real-world FER. Extensive experiments on FER2013 and KDEF highlights that our approach achieved state-of-the-art robustness, efficiency and trade-offs for real-time FER on Edge devices. The results demonstrate that systematic data refinement is as critical as model innovation. The source code for results reproducibility of the paper is publicly available at (https://github.com/anonymous123810/ICLR2026).", "tldr": "We present a unified data- and model-centric framework for robust facial emotion recognition, combining dataset refinement, hybrid CNN–Transformer architecture, and multi-teacher knowledge distillation with adversarial training.", "keywords": ["Facial Emotion Recognition", "Data-Centric Deep Learning", "Model-Centric Optimization", "Multi-Teacher Knowledge Distillation", "Robust Emotion Recognition"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/acafaea61a8aae1288ffdb941283d8e6e2e86def.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper trains a lightweight hybrid CNN-Transformer model on two datasets: FER2013 and KDEF. During the training process, existing training techniques—multi-teacher distillation and adversarial training—are employed to enhance the robustness of representations.\nThe research problem addressed in this paper is based on early datasets in the face expression recognition, ignoring recent research advances.\nIn terms of methodology, the paper lacks new technical contributions."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is relatively clearly written.\n- The experimental setup effectively illustrates the rationale for teacher selection."}, "weaknesses": {"value": "Rather than developing and exploring new technologies, this paper is more like a research exercise; its technical contributions fall far short of the standards required by ICLR.\n1. The abstract claims the paper focuses primarily on a \"data-centric approach\". However, the data preprocessing techniques proposed are, in fact, common data augmentation methods in deep learning-based facial expression recognition.\n2. Regarding research motivation, the paper identifies issues such as \"mislabeled, duplicated, and class-imbalanced data\" in the FER2013 and KDEF datasets. Yet FER2013 and KDEF were released in 2008 and 2013 respectively, and these data issues are well-recognized consensus in the facial expression recognition field. It is precisely based on this consensus that the community has developed higher-quality datasets like FERPlus, AffectNet, and RAF-DB.\n3. If the authors intend to study the problem of noisy labels, they should include validation experiments on datasets such as AffectNet and RAF-DB, and compare their results with the State-of-the-Art performance on these datasets.\n4. The model's training methods and structural design are merely simple combinations of existing approaches, and there is a lack of justification for why such training strategies or structural designs were adopted."}, "questions": {"value": "I suggest that the authors conduct a systematic review of the current developments in the facial expression recognition field, particularly the FER methods published in top-tier conferences. Meanwhile, they should learn the fundamental methodologies of academic research:\n1. The proposed research problem must be supported by a sufficient literature review to prove that it remains an unresolved issue.\n2. Focus on making substantive, innovative contributions in one specific aspect—such as proposing a new model structure, a new training method, or a new data construction approach. Simply combining existing technologies will only result in a system implementation report rather than an original research paper.\n3. The validation of experimental results should preferably be based on advanced, commonly used datasets in the research field."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7G5u8pPcSI", "forum": "QOfHAW7PxE", "replyto": "QOfHAW7PxE", "signatures": ["ICLR.cc/2026/Conference/Submission17329/Reviewer_Y3qM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17329/Reviewer_Y3qM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761415841834, "cdate": 1761415841834, "tmdate": 1762927254396, "mdate": 1762927254396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "NNuWveSJlm", "forum": "QOfHAW7PxE", "replyto": "QOfHAW7PxE", "signatures": ["ICLR.cc/2026/Conference/Submission17329/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17329/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763012182102, "cdate": 1763012182102, "tmdate": 1763012182102, "mdate": 1763012182102, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified data-centric + model-centric pipeline for Facial Emotion Recognition (FER). On the data side, it claims to (i) remove noisy and duplicate samples, (ii) refine faces via MediaPipe Face Mesh masks, and (iii) perform class-aware augmentation to rebalance labels. On the model side, it introduces a lightweight HyFER (MobileViT-inspired) student and Emotion-Aware Dynamic Distillation (EADD) that adaptively weights three teachers (CrossViT-18, DaViT-B, MobileViT-S) per emotion class, followed by post-distillation adversarial training (FGSM/PGD/DeepFool). Reported results: KDEF accuracy ≈ 99.6% and FER2013 accuracy ≈ 79.4% under 5-fold CV."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses both data quality and student-teacher training, with a structured, modular pipeline (Figure 1, p. 3).\n\n- A lightweight student aimed at edge devices, with some compute stats for teachers (Table 3, p. 8).\n\n- Per-class distillation idea. The emotion-aware weighting over multiple teachers is a reasonable intuition for FER’s class-dependent difficulty (Eq. (5), p. 6).\n\n- Adversarial robustness is at least considered (Table 5, p. 9), which is rare in many FER papers."}, "weaknesses": {"value": "**Limited novelty**: While the performance boost is clear and observable, when it comes to the methodology, it appears to be in engineering and pipeline design; the degree of methodological novelty seems limited.\n\nThe proposed EADD reduces to per-class soft weighting of multiple teachers by their per-class validation accuracy (Eq. 5)—a straightforward ensembling heuristic rather than a new KD principle. Closely related adaptive/multi-teacher KD ideas (attention-, meta-, or policy-weighted) already exist. On the data side, the pipeline relies on manual relabeling, cosine-similarity dedup, and face masking—standard hygiene steps that is also hard to find novel.\n\n**Complexity**:\nThe system stacks three teachers + EADD + an adversarial phase, yet the student lags significantly behind its best teacher model performance in FER 2013. [Table 2,3]\n\n**Narrow Dataset**: The proposed methodology is tested on a single model and evaluated only in two datasets. More rigorous evaluation on WILD-FER datasets like RAF-DB or AffectNet and/or testing with various student/teacher models could help see the proposed method's effect more clearly.\n\n**Limited Details.**: The adversarial phase (FGSM/PGD/DeepFool) lacks specific details, i.e. attack budgets."}, "questions": {"value": "1. Can you report results on the official FER2013 test split (not 5-fold CV), with mean±std over several seeds, so we can compare apples-to-apples with prior work?\n\n2. Could you also conduct ablation within data-cleansing part? It is hard to see which method of the data cleansing contribute to the most. \n\n3. How does your method compare to (a) a learned/adaptive multi-teacher KD baselines (please include few, if possible) and (b) label-noise-aware training (e.g., Cleanlab/FERPlus labels) without EADD?\n\n4. Could you also test your method on more IN-THE-WILD FER data like RAF-DB or Affectnet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6LS6Jj1qo2", "forum": "QOfHAW7PxE", "replyto": "QOfHAW7PxE", "signatures": ["ICLR.cc/2026/Conference/Submission17329/Reviewer_5mWz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17329/Reviewer_5mWz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761499883140, "cdate": 1761499883140, "tmdate": 1762927253978, "mdate": 1762927253978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a “unified” framework for Facial Emotion Recognition (FER) that combines:\n1. A pipeline for data cleaning of the FER2013 and KDEF datasets (duplicate removal, label correction, class-balanced augmentation).\n2. A hybrid CNN-Transformer model (HyFER) to perform emotion classification.\n3. A dual-phase training strategy, including knowledge distillation and post-distillation adversarial training.\n\nExperiments on FER2013 and KDEF show small performance gains compared to previous FER methods."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- **Some empirical effort**: Includes logical ablations for choice of teacher model and adversarial robustness (Tables 2-5).\n- **Focus on data quality**: The motivation that FER datasets contain noise and imbalance is valid and relevant"}, "weaknesses": {"value": "### 1. **Extremely limited novelty**\n- The “data-centric” pipeline consists of standard manual cleaning and simple augmentations (flip/rotate/scale) - techniques already standard in image preprocessing.\n- The model (HyFER) is a MobileViT-inspired CNN+Transformer with no novel modifications beyond that extent. The choice to use a hybrid model over a uniform architecture is not discussed, and the training scheme (multi-teacher KD + adversarial fine-tuning) is entirely standard.  \n  The “Emotion-Aware Dynamic Distillation (EADD)” is just a weighted KD loss using validation accuracy as a softmax weight - this is a minor, mechanical variation.\n- There is no genuinely novel model component or data preprocessing method.\n\n### 2. **Weak empirical validation**\n- Only two small, dated datasets (FER2013, KDEF) are used, both with low resolution and controlled conditions. These benchmarks are long saturated and not suitable for an ICLR level paper. Some models in Table 1 already achieve a highly saturated (97%+) accuracy on KDEF.\n- Improvements over EA-Net in Table 1 are negligible except for a couple cases, calling into question the value of the proposed method, especially given the above weaknesses. \n\n### 3. **Poor writing and presentation**\n- There are numerous grammatical errors throughout the paper (improper capitalization (Line 1, 152), spelling mistakes (Line 331,  351), etc.) \n- Many citations are misplaced or duplicated (e.g., repeated CrossViT and DaViT entries in references, two different citations used for KDEF dataset).  \n- The caption for Figures 1 and 2 are entirely uninformative."}, "questions": {"value": "1. Why were only FER2013 and KDEF used? Have you evaluated on any more modern FER benchmarks (AffectNet, RAF-DB, ExpW)?  \n2. What are the “emotion-specific strengths” for each teacher model? How are they identified?\n3. The data cleaning steps mention \"manual visual inspection\". How reproducible is this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "e10pNvbpyH", "forum": "QOfHAW7PxE", "replyto": "QOfHAW7PxE", "signatures": ["ICLR.cc/2026/Conference/Submission17329/Reviewer_LAF4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17329/Reviewer_LAF4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854677368, "cdate": 1761854677368, "tmdate": 1762927253553, "mdate": 1762927253553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the improve Facial Emotion Recognition (FER) by improving both data quality and model design. For data quality, the authors remove images with wrong labels and generate augmented images for classes with a small number of samples. For model training, the authors use multi-teacher knowledge distillation and adversarial training. The proposed method achieves somewhat good accuracy in the experiments."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "S1: The paper is written in a relatively clear manner and easy to read."}, "weaknesses": {"value": "Computer vision is not my expertise but I have to say that the paper looks like an experiment report from undergraduate students rather than a technical paper targeted for a top conference. My apologies if this sounds offensive. \n\nW1: Novelty is very limited. The image quality enhancement in Section 3.1.1 relies on manual inspection, which does not scale. The data augmentation in Section 3.1.2 uses standard data augmentation methods and simply balances the number of samples from different classes. As stated by the authors in the Introduction, transfer learning and adversarial training have been used in existing works, and thus the training methods of the paper are also not novel.\n\nW2: The main experiment results in Table 2 are weak; the proposed method slightly outperforms existing methods for some metrics but underperforms the baselines for other metrics. Normally, the best method for each metric should be marked in bold but the authors simply mark the proposed method all in bold. The experiment part also focuses entirely on model design and lacks an ablation study for the data quality part of the proposed methods.\n\nW3: The writing of the paper can be significantly improved. There are many typos. Moreover, the related work can be more concentrated on the methods for data quality enhancement in computer vision and the model designs to improve accuracy."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "onpFIFc9H8", "forum": "QOfHAW7PxE", "replyto": "QOfHAW7PxE", "signatures": ["ICLR.cc/2026/Conference/Submission17329/Reviewer_tZVX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17329/Reviewer_tZVX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927672620, "cdate": 1761927672620, "tmdate": 1762927253202, "mdate": 1762927253202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}