{"id": "hg0lpcHdWk", "number": 10069, "cdate": 1758159613163, "mdate": 1759897676256, "content": {"title": "Populate-A-Scene: Affordance-Aware Human Video Generation", "abstract": "Can a video generation model be repurposed as an interactive world simulator? We explore the affordance perception potential of text-to-video models by teaching them to predict human-environment interaction. Given a scene image and a prompt describing human actions, we fine-tune the model to insert a person into the scene, while ensuring coherent behavior, appearance, harmonization, and scene affordance. Unlike prior work, we infer human affordance for video generation (i.e., where to insert a person and how they should behave) from a single scene image, without explicit conditions like bounding boxes or body poses. An in-depth study of cross-attention heatmaps demonstrates that we can uncover the inherent affordance perception of a pre-trained video model without labeled affordance datasets.", "tldr": "We explore the affordance perception potential of text-to-video models by teaching them to predict human-environment interaction.", "keywords": ["Video Generation", "Human-Centric Generation", "Affordance", "Representation Visualization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3859d5d6093b33616d7842b25ca3366cff213a88.pdf", "supplementary_material": "/attachment/f9f26569c797273b951a8f22e9a8d8d2b6e7c252.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Populate-A-Scene, which is a video generation model that can generate human-environment interaction videos from a scene image (without humans) and text prompts. To prepare training data, the authors perform human removal on existing human-scene interaction videos, and then inpaint the missing regions to create synthetic scene-only images. The authors then adapt and fine-tune a pre-trained text-to-video model (Movie Gen). To fuse the input empty scene image into the denoising process, the authors employ a dual-conditioning mechanism that perform channel concatenation and cross-attention to condition the transformer backbone on the scene image, as well as the text prompts. The authors evaluate their method on a curated benchmark for generating human videos in a scene, and show that their method outperforms existing image/video editing and image-to-video approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- One major advantage of this work is that it does not require specifications of human locations and poses to generate human-scene interaction videos from an empty scene image and text prompts. This is enabled by fine-tuning an existing text-to-video model on synthetic data tuples of (text, image, video), obtained by human removal and inpainting.\n- The authors introduce a dual-conditioning mechanism with simple channel concatenation and cross attention to condition the denoising backbone on both scene images and text prompts.\n- The authors demonstrate the implicitly learned affordance understanding of their model by analyzing the cross-attention maps and applying them to a real-world affordance prediction dataset. Results show that their model can locate detailed affordance information."}, "weaknesses": {"value": "- Due to building upon a smaller version of the pre-trained text-to-video model, the generated human-scene interaction videos are of low resolution. The human appearance and anatomy are not very realistic (e.g., in Fig. 1, the man jogging and the horror film examples). In terms of interaction dynamics, the generated humans may lack motion dynamics (e.g., videos in the supplemental) and not respect the text prompts well (e.g., in Fig. 1, the vacuum example). Some of these issues have been discussed in the limitations section.\n- The data preparation process relies on human removal and inpainting, which however may not work well with small objects, as they may be removed together with humans. I did not see generation examples involving small objects, for example, a person picking up a cup. \n- While the proposed approach does not require inputs like human locations and poses, the model may lack fine-grained control over human placements and interactions with specific objects in the scene. It is unclear whether the action prompt is sufficient to provide such control."}, "questions": {"value": "- Can the model generate interactions with small objects in the scene, as mentioned above?\n- Can the model control the interaction generation more precisely with the action prompt? For example, an image with two chairs, can the model generate a person sitting on a specific chair?\n- Only one pre-trained text-to-video model has been tested in this work. Would using other video generation models improve the generation quality?\n- Although the data contain two-person videos, it is unclear whether the model can insert two persons interacting with an empty scene. It seems this can be done one by one, but I imagine there would be error accumulation.\n- The following work might be worth discussion in the related work: \n    - Move-in-2D: 2D-Conditioned Human Motion Generation. Huang et al. CVPR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qhMvwd0WKt", "forum": "hg0lpcHdWk", "replyto": "hg0lpcHdWk", "signatures": ["ICLR.cc/2026/Conference/Submission10069/Reviewer_ZeMC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10069/Reviewer_ZeMC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536473932, "cdate": 1761536473932, "tmdate": 1762921463335, "mdate": 1762921463335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"Populate-A-Scene,\" a novel task and model for generating a video of a human interacting with a static scene image, conditioned on a text prompt. The model's primary contribution is its ability to generate plausible human actions (e.g., \"sitting on a swing,\" \"pedaling a bike\") in the correct semantic locations without any explicit guidance, such as bounding boxes, segmentation masks, or pose sequences.\n\nTo achieve this, the authors fine-tune a pre-trained text-to-video (T2V) diffusion model (MovieGen). The key to their method is a clever, large-scale data-generation pipeline: they take existing videos of human-scene interactions, use segmentation (SAM) and inpainting (SDXL) to remove the human and create a static \"empty scene\" image. The model is then trained on tuples of (empty scene, text prompt, original video) to learn how to \"reverse\" this process, effectively learning to populate the scene.\n\nA significant part of the paper is a detailed analysis of the fine-tuned model. The authors show that the model's internal cross-attention mechanisms learn to identify scene affordances. By visualizing attention maps for action-related words in the prompt, they demonstrate that the model correctly highlights plausible interaction regions on the input image. They validate this claim quantitatively by comparing these attention maps to the ground-truth masks from the PAD affordance dataset, showing a strong spatial correlation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clarity: The paper is well-written. The task, method, and contributions are communicated with great clarity. Figure 1 provides an immediate and impressive overview of the model's capabilities. Figure 2 (data pipeline) and Figure 5 (affordance analysis) are both highly effective at explaining the core technical ideas and a key findingData Pipeline Biases: The clever data generation pipeline is also a potential source of weakness.\n\nInpainting Artifacts: The model is trained to place a human into a region that was, in its training data, inpainted. This could create a subtle bias where the model learns to \"put the human in the blurry/artifacted spot\" rather than \"put the human on the chair.\" The authors' analysis on the PAD dataset (which has no inpainting) is a strong counter-argument, but this remains a potential confounding variable.\n\nData Filtering Biases: The authors acknowledge in Appendix G that their data filtering (requiring a consistent face) eliminates all videos with back-facing humans. This is a significant bias that leads to a predictable failure mode. This fragility seems self-imposed by the data pipeline design.\n\nIncremental Architecture: The architectural modifications (latent concatenation and a cross-modal fusion module in Sec 4.3) are effective but are combinations of existing conditioning techniques. The novelty of this work lies squarely in the problem formulation, the data pipeline, and the analysis, not in a new network architecture. This is a minor weakness.\n\nConfounding Base Model Limitations: The authors are transparent that many failure cases (e.g., static videos, distorted limbs in Appendix G) are inherited from the 4B-parameter MovieGen base model. This makes it difficult to fully isolate the performance of the affordance-conditioning from the rendering capability of the base T2V model. For example, when a generated pose is distorted, is it because the affordance prediction was wrong, or because the affordance prediction was correct but the base model failed to render that pose?"}, "weaknesses": {"value": "Should this really be a video synthesis paper? I looked at the videos in the results - there is barely any human motion in lots of the videos - for example a video is generated with a person standing next to a car and the camera moves? is this really useful or should be claimed as video synthesis? \nPerhaps a more palatable claim would have been to show the model generates images with correct affordances - correct placement of humans etc because thats what it appears to me for most cases. \n\nAlso the claim of scene understanding is slightly exagerated - these models cannot possibly understand the underlying 3d structure of the scene - for example no result can be shown where the person walks through a door or moves a large object etc. \n\nData Pipeline Biases: The clever data generation pipeline is also a potential source of weakness.\n\nInpainting Artifacts: The model is trained to place a human into a region that was, in its training data, inpainted. This could create a subtle bias where the model learns to \"put the human in the blurry/artifacted spot\" rather than \"put the human on the chair.\" The authors' analysis on the PAD dataset (which has no inpainting) is a strong counter-argument, but this remains a potential confounding variable.\n\nData Filtering Biases: The authors acknowledge in Appendix G that their data filtering (requiring a consistent face) eliminates all videos with back-facing humans. This is a significant bias that leads to a predictable failure mode. This fragility seems self-imposed by the data pipeline design.\n\nIncremental Architecture: The architectural modifications (latent concatenation and a cross-modal fusion module in Sec 4.3) are effective but are combinations of existing conditioning techniques. The novelty of this work lies squarely in the problem formulation, the data pipeline, and the analysis, not in a new network architecture. This is a minor weakness.\n\nConfounding Base Model Limitations: The authors are transparent that many failure cases (e.g., static videos, distorted limbs in Appendix G) are inherited from the 4B-parameter MovieGen base model. This makes it difficult to fully isolate the performance of the affordance-conditioning from the rendering capability of the base T2V model. For example, when a generated pose is distorted, is it because the affordance prediction was wrong, or because the affordance prediction was correct but the base model failed to render that pose?"}, "questions": {"value": "Questions\nCould the authors please explain why they think this is significant advacement in terms of video synthesis? It looks like static images are being generated with some camera movement. Also why are the videos generated at such a low resolution? Several open source models such as qwen generate higher resolution video. \n\nProbing the \"Inpainting Scar\" Bias: My main question relates to Weakness #1. The PAD dataset analysis is a good defense against the \"inpainting scar\" hypothesis, but could the authors provide a more direct test? For example, take a training scene where a person was removed from a chair. Then, prompt the model with \"A person sitting on the sofa\" (assuming a sofa is also in the scene). Would the model correctly go to the sofa (proving it follows the prompt and affordance), or would it be \"distracted\" by the inpainted region on the chair?\n\nRationale for Data Filtering: Regarding the \"no back-facing poses\" bias (Appendix G), this seems like a major limitation introduced by the data pipeline. Why was the data filtered using a face detector, which naturally creates this bias, instead of a more robust full-body keypoint detector (like OpenPose, which was used later in the pipeline anyway for filtering)? Is this purely a data-level issue, or did you find the model architecturally struggled to learn such poses?\n\nRobustness to Atypical Prompts: The model excels at common, plausible affordances (sitting, riding, etc.). How robust is it to more specific or atypical prompts that might contradict the \"strongest\" affordance? For example, given a scene with a chair, can the model successfully generate \"a person standing next to the chair\" (and not sit)? Or \"a person jumping on the chair\"? This would test the balance between the learned affordance prior and the guidance from the text prompt.\n\nMulti-Person Generation: The paper mentions training on 29,700 two-person videos, and Figure 7 shows results of adding a second person to interact with an existing one. Can the model populate an empty scene with two interacting people from scratch? For example, given an empty living room, could it handle the prompt \"A person sitting on the sofa, and another person playing guitar next to them\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "omgb2T50Sf", "forum": "hg0lpcHdWk", "replyto": "hg0lpcHdWk", "signatures": ["ICLR.cc/2026/Conference/Submission10069/Reviewer_uqc5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10069/Reviewer_uqc5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877824055, "cdate": 1761877824055, "tmdate": 1762921462997, "mdate": 1762921462997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method to automatically generate a video of a person realistically interacting with a static scene. Given a single image of a scene (e.g., a living room) and a text prompt describing an action (e.g., \"a man is sitting on the sofa\"), the model inserts a person into the scene and generates a short video of them performing that action in a plausible and coherent manner.\nThe core idea is to teach a pre-trained text-to-video (T2V) model to understand affordances—the possible interactions an environment offers (e.g., a chair affords sitting, a bike affords riding). Unlike previous methods, this approach does not require any explicit guidance like masks, bounding boxes, or pre-defined poses to tell the model where to place the person or how they should move. The model learns to infer this information directly from the scene image and the text prompt.\nTo achieve this, the authors fine-tune a Transformer-based T2V model on a specially curated dataset. They create this dataset by taking existing videos of people, using segmentation and inpainting models to automatically remove the person, thus creating pairs of (empty scene, original video, text description)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Revealing Latent Capabilities of T2V Models:\n\nThe paper provides a valuable scientific insight by demonstrating that large, pre-trained text-to-video models implicitly learn about affordances. The analysis of cross-attention maps (Fig. 4) convincingly shows that the model can associate action words (e.g., \"riding,\" \"holding\") with the correct interactable regions in a scene (e.g., a horse, reins), even without being explicitly trained on affordance-labeled data.\n\n- Scalable and Automated Data Curation:\n\nThe method for creating the training dataset is clever and highly automated. By using a pipeline of modern vision models (GroundingDINO, SAM, inpainting models) to remove humans from existing videos, they create a large-scale dataset of (scene, action_video, prompt) tuples. This is a practical and scalable approach that avoids the need for expensive manual annotation."}, "weaknesses": {"value": "- Motivation: \n\nThe paper's motivation is not sufficiently compelling. While it frames the work as creating a \"simulator,\" it lacks any explicit design for modeling physical dynamics. This terminology seems to overstate the model's capabilities, which are more focused on semantic plausibility than physical simulation. Furthermore, the core problem of generating human-scene interactions from an empty scene is already handled well by several state-of-the-art foundation models, which can produce physically plausible results from a simple prompt. This calls into question the necessity of the proposed fine-tuning approach and limits the perceived contribution.\n\n- Comparison for Wan based foundation model: \n\nThe experimental evaluation is incomplete. The paper fails to compare its method against several powerful, contemporary image-to-video (I2V) foundation models, such as Wan 2.2 I2V. Informal tests (tested by myself) using the paper's own image and text prompts on these publicly available models can yield results of comparable or even superior quality. The omission of these key baselines makes it difficult to accurately assess the method's claimed superiority."}, "questions": {"value": "Please explain why your method, after finetuning can have any advantages over than those baseline model such as Wan 2.2 I2V."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "reKCTmVyHe", "forum": "hg0lpcHdWk", "replyto": "hg0lpcHdWk", "signatures": ["ICLR.cc/2026/Conference/Submission10069/Reviewer_U1Kw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10069/Reviewer_U1Kw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899849978, "cdate": 1761899849978, "tmdate": 1762921462726, "mdate": 1762921462726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for affordance-aware human video generation by conditioning a pretrained text-to-video model on scene images. The authors claim that the model implicitly learns affordance from human-scene interaction signals, and analyze cross-attention maps to support this claim."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-The paper tackles an interesting and relevant problem of affordance-aware human-scene video generation.\n\n-The paper provides a straightforward extension to condition a pretrained text-to-video model on scene images.\n\n-Qualitative examples are visually appealing and demonstrate some degree of human-scene interaction."}, "weaknesses": {"value": "-The paper mainly fine-tunes an existing model (e.g., MovieGen) with additional scene conditioning. The architectural modifications (latent concatenation + text-image fusion) appear incremental. \n\n-The affordance aspect, which the paper claims as an essential contribution, seems more like a re-interpretation of what attention maps already provide, rather than a fundamentally new capability.\n\n-The use of cross-attention heatmaps as evidence of affordance perception is weak. These maps do not prove that the model understands object functionality or the feasibility of object interactions. \n\n– The human-removal pipeline heavily depends on segmentation and inpainting models, which may introduce artifacts or incorrect affordance cues.\n\n-Most baselines are general video editing or text-to-video systems that are not designed for human-scene interaction, which makes the comparison less meaningful. \n\n– The dataset is not publicly available, limiting reproducibility and fair comparison."}, "questions": {"value": "-The affordance perception claim seems largely based on attention maps. How do you justify that attention indicates true affordance understanding rather than text-token correlation?\n\n-How well does the approach generalize to unseen object categories or actions not present in the curated dataset? Any failure case analysis?\n\n-The human-removal (inpainting) pipeline appears to erase or distort key affordance-related objects in the scene. For instance, in Fig. 1 (top row, right example), the bicycle seat is already missing in the input scene after human removal, fundamentally altering the bike's action possibilities. How common are such affordance-distorting artifacts in the dataset? Could this lead the model to learn incorrect affordance priors or reduce interaction plausibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o6FUFMMZEW", "forum": "hg0lpcHdWk", "replyto": "hg0lpcHdWk", "signatures": ["ICLR.cc/2026/Conference/Submission10069/Reviewer_J7Sg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10069/Reviewer_J7Sg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070747877, "cdate": 1762070747877, "tmdate": 1762921462112, "mdate": 1762921462112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}