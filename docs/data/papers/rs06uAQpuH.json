{"id": "rs06uAQpuH", "number": 2363, "cdate": 1757064019207, "mdate": 1763221452625, "content": {"title": "Parallelizing Tree Search With Twice Sequential Monte Carlo", "abstract": "Model-based reinforcement learning (RL) methods that leverage search are responsible for many milestone breakthroughs in RL.\nSequential Monte Carlo (SMC) recently emerged as an alternative to the Monte Carlo Tree Search (MCTS) algorithm which drove these breakthroughs. SMC is easier to parallelize and more suitable to GPU acceleration. However, it also suffers from large variance and path degeneracy which prevent it from scaling well with increased search depth, i.e., increased sequential compute. To address these problems, we introduce Twice Sequential Monte Carlo Tree Search (TSMCTS). Across discrete and continuous environments TSMCTS outperforms the SMC baseline as well as a popular modern version of MCTS. Through variance reduction and mitigation of path degeneracy, TSMCTS scales favorably with sequential compute while retaining the properties that make SMC natural to parallelize.", "tldr": "", "keywords": ["search", "planning", "smc", "mcts", "model based", "policy improvement"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ef34c5d15558d2fa88ce3754a891d1dd94a89ca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper focuses on model-based reinforcement learning (MBRL) methods that leverage search algorithms.\nSpecifically, the paper claims that Monte-Carlo Tree Search (MCTS) does not fully utilize the GPU when compared to Sequential Monte Carlo (SMC).\nHowever, the latter suffers from high variance and path degeneracy.\nAs a result, the paper proposes a method that combines the two, called Twice Sequential Monte-Carlo Tree Search (TSMCTS) that aims to improve upon both existing algorithms.\nThe paper demonstrates through both discrete and continuous environments TSMCTS is comparable if not better than existing algorithms in episodic returns, and further demonstrates that TSMCTS maintains smaller variance and less path degeneracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method clearly yields better runtime while maintaining/improving performance when compared to Gumbel MCTS and SMC.\n- The proposed method suffers less from high variance and path degeneracy.\n- The complexity analysis in the appendix A.4 is helpful to understand the asymptotic trade-offs."}, "weaknesses": {"value": "- Method\n\t- On line 268, keeping track of $t$ Q-values can be prohibitively expensive---instead something akin to $Q(\\lambda)$ might just do the work.\n\t- It seems like sequential halving can suffer from high bias if the first iteration filters out good action early---this can easily be the case with stochastic dynamics and low search budget. If this is true, I think the paper should indicate this, otherwise it would be nice to provide a reference, demonstrating that this is not the case.\n- Writing\n\t- On line 116, the notation $s_t$ conflates with the timesteps in the MDP. I suggest disambiguating them with, e.g., superscript, $\\bar{s}_t$, etc.\n\t- Nit: On line 187, I think it should be $\\tau_t = s_0, a_0, \\dots, s_t, a_t, s_{t + 1}$\n\t- Theorem 1 should indicate that the assumptions are true $Q^\\pi$ and transitions $P$, otherwise it can be misleading in the function-approximation case.\n\t- I think the writing can be clearer on line 234. If I understand correctly, $a_0^i \\sim \\pi_\\theta(\\cdot | s_0)$ means that most particles will start from the most-likely action, and consequently the empirical distribution will only have mass on said action. On the other hand, MCTS uses the estimated Q-value to form the empirical distribution.\n- Experiments\n\t- The experiments should describe how the continuous environments are setup---is the action space discretized, or the cross-entropy loss is replaced with a squared loss, etc.?\n\t- How does this approach compare against Levin Tree Search and Luby Tree Search [1]?\n\t- This doesn't affect my score, but how would it perform when the transitions are approximated?\n\nReferences:  \n[1] Orseau, Laurent, et al. \"Single-agent policy tree search with guarantees.\" Advances in Neural Information Processing Systems 31 (2018)."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "b6cx0f5Y1b", "forum": "rs06uAQpuH", "replyto": "rs06uAQpuH", "signatures": ["ICLR.cc/2026/Conference/Submission2363/Reviewer_bDyy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2363/Reviewer_bDyy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663829628, "cdate": 1761663829628, "tmdate": 1762916209045, "mdate": 1762916209045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear reviewers,\n\nThank you for your time, detailed reviews and many valuable comments.\nWe include a dedicated rebuttal for each reviewer, answering their questions and specific concerns, but want to address a few point that were shared among reviewers first.\n\n**Presentation of the evaluation:**\n\nIt seems that our presentation of the evaluation was not sufficiently clearly motivated and presented. We thank the reviewers for pointing that out and include an edit with clarified evaluation (see Section 7). Changes are marked in green to make them easy to identify.\n\nThe gist:\nSPO [1] establishes that SMC can be used for policy improvement, in a similar manner to MCTS in AZ.\nSPO also established an experiment bed, with discrete and continuous action environments, where it demonstrates the strength of SMC for policy improvement.\nThe objective of our work (TSMCTS) is to iterate on the SMC planner used by SPO and improve its facility for policy improvement in RL, through mechanisms that address variance and path degeneracy at the root.\nFor this reason, we use the experimental setup (environments and experiments) established by SPO, in its most recent variation iterated upon by [2].\n\nTo keep the comparison between the planners as direct and as on equal footing as possible, we compare all planners in the same manner, under the same conditions, with exactly the same training loop, DNN architectures, etc., in the same manner as [2] and using their design choices of [2].\nThe only differences between the search-based agents (including the Gumbel MCTS [3]-based agent, which we include for reference, and excluding the model free PPO [4]), is the planner used for policy improvement.\n\nAlthough the contributions of [2] are mostly orthogonal to ours (see Section 6), we agree with reviewer oJ94 that they are interesting to compose with ours. \nWe include additional results (Figure 4 in Appendix C) where we contrast the following SMC-based variants: baseline SMC (Algorithm 1 in [1], i.e. the algorithm we refer to as CAI-SMC where $\\mu=\\pi_\\theta$), SMC + TRT, TSMCTS and TSMCTS + TRT. \nThe TSMCTS variations dominate in all domains but one (HalfCheetah), where SMC + TRT is the dominating variant, followed by TSMCTS + TRT.\nWe note that the twisting parameters of TRT where not re-tuned for TSMCTS, suggesting that it is possible to get even more improvement for the TSMCTS + TRT variant.\n\nWe thank the reviewers again for their comments and we are looking forward to the discussion.\n\nReferences:\n\n[1] Macfarlane et al. \"SPO: Sequential Monte Carlo Policy Optimisation.\" NuerIPS 2024.\n\n[2] de Vries et al. \"Trust-Region Twisted Policy Improvement.\" ICML 2025.\n\n[3] Danihelka et al. \"Policy improvement by planning with Gumbel.\" ICLR 2022.\n\n[4] Schulman et al. \"Proximal policy optimization algorithms.\" 2017."}}, "id": "uLMgiwMqhO", "forum": "rs06uAQpuH", "replyto": "rs06uAQpuH", "signatures": ["ICLR.cc/2026/Conference/Submission2363/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2363/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2363/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763215122451, "cdate": 1763215122451, "tmdate": 1763215122451, "mdate": 1763215122451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TSMCTS, a new Sequential Monte Carlo (SMC) search algorithm for policy improvement. The method is presented as an alternative to MCTS that aims to solve the high variance and path degeneracy issues found in standard SMC for RL methods. The authors build their method incrementally: first, by formalising SMC beyond the Control-as-Inference (CAI) framework; second, by introducing SMCTS, which adds MCTS-inspired value backpropagation to mitigate path degeneracy; and finally, by integrating Sequential Halving at the root to create TSMCTS for better budget allocation. The paper's experiments show TSMCTS outperforming a baseline SMC and GumbelMCTS, claiming it yields lower-variance targets and scales properly with the sequential budget."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper's focus is particularly impactful as it targets two fundamental weaknesses that hinder the scalability and performance of Sequential Monte Carlo (SMC) in Reinforcement Learning:\n\n* Targeting **path degeneracy** by preserving information about multiple actions at the root is a crucial research direction. This prevents the search from prematurely collapsing to a single trajectory, which leads to unstable and impoverished policy targets—a key limitation of standard SMC.\n* Developing systematic **variance reduction** mechanisms is vital for making SMC a scalable planning algorithm. Focusing on techniques that combat the high variance growth with search depth is a key direction, as this is a fundamental bottleneck that prevents SMC from benefiting from deeper lookaheads."}, "weaknesses": {"value": "### 1. Insufficient Baselines\n* **Ambiguous \"SMC Baseline\":** The paper introduces an \"SMC baseline\" that doesn't correspond to a specific, recognized algorithm in the literature. This is confusing and unscientific. The existing methods are distinct: the SMC method from Piché et al. [3], SPO (which uses SMC for policy improvement) [2], and the extension by de Vries et al. with TRT-SMC (SPO + twisted proposals + revived sampling)[4]. The authors should compare against each of these established methods in all experiments. In addition, baselines such as SPO have open-source implementations, so their omission is a significant weakness of this paper. These baselines are a strict requirement, and I would strongly push for rejection without this being fully addressed.\n### 2. Flawed Narrative and Positioning\n* **Insufficient Coverage of Prior Work:** The related work section incorrectly frames the history of SMC in RL. It misses the seminal work by Lazaric et al. [1] that first introduced SMC for RL. The progression should be: Lazaric et al. [1] -> Piché et al. -> SPO (full policy iteration loop, highlighting parallelism) -> de Vries et al. (improvements on SPO via SMC and proposal changes).\n* **Overstated Claims on Parallelism:** The title and framing imply that this paper is the first to identify the parallelism benefits of SMC for RL. This benefit was already robustly established and was a critical contribution of SPO. It is fine to reference this as a benefit of SMC-type methods, but the correct credit must be assigned to SPO where this was demonstrated.\n* **Unclear Motivation for Decoupling from Control as Inference (CAI):** The paper doesn't provide a clear rationale for moving away from the CAI framework, especially since SMC is fundamentally a probabilistic inference method for which CAI is a natural fit.\n### 3. Method Naming Issues\n* **Confusing Introduction of Methods:** The paper introduces three methods (RL-SMC, SMCTS, TSMCTS). It does not make sense to name every iteration of the proposed methods. The paper should propose one method and investigate variations of it to understand the contributions of each change. This risks confusing the literature further, and it is unlikely that future work would use all of these ablations as baselines—just the final, aggregated method. Therefore, only this should be named.\n### 4. SPO vs TRT SMC\n* In multiple areas of the paper [4] is incorrectly referenced, such as in the experimental setup section. De Vries et al [4] is an iteration upon SPO which introduced the full experiment setup, environments and baselines, from which De Vries uses to investigate benefits to alternative SMC approaches. [4] should only be cited when referencing the specific SMC innovations used in their paper and SPO otherwise.\n* SMC used as a policy improvement operator introduced in SPO needs to be made more clear in the related work section, this is a significant paper in the timeline of Sequential Monte Carlo for RL demonstrating not only policy improvement but the benefits of parallelism and competitiveness with MCTS.\n---\n### References\n[1] Lazaric, A., Restelli, M., & Bonarini, A. (2007). Reinforcement learning in continuous action spaces through sequential monte carlo methods. In *Advances in Neural Information Processing Systems 20*.\n\n[2] Macfarlane et al. SPO: Sequential Monte Carlo Policy Optimisation\n\n[3] Piché et al. Probabilistic Planning With Sequential Monte Carlo Methods\n\n[4] de Vries et al. Trust-Region Twisted Policy Improvement"}, "questions": {"value": "- What is the performance of the method from Piche et al (following SPO paper could be referred to as SMC-ENT), SPO and TRT-SMC for all experiments and how does it compare to the final method with all modifications, proposed in this paper"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EhWJntTCjJ", "forum": "rs06uAQpuH", "replyto": "rs06uAQpuH", "signatures": ["ICLR.cc/2026/Conference/Submission2363/Reviewer_oJ94"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2363/Reviewer_oJ94"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908577194, "cdate": 1761908577194, "tmdate": 1762916208718, "mdate": 1762916208718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Twice Sequential Monte Carlo Tree Search (TSMCTS), a planner built on a reformulation of SMC for RL (RL-SMC/SMCTS) that explicitly targets policy improvement at the root rather than trajectory inference. The authors prove that RL-SMC becomes a policy-improvement operator in the infinite-particle limit, then address two classical SMC issues—exploding variance with depth and path degeneracy—by integrating Sequential Halving (SH) at the root and aggregating SMCTS value estimates across SH rounds. Empirically, across discrete and continuous benchmarks, TSMCTS outperforms an SMC baseline and GumbelMCTS, with lower estimator variance, mitigated target degeneracy at the root, and better scaling with sequential compute while retaining SMC’s parallelization benefits."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear conceptual shift. The paper reframes SMC for root-focused policy improvement, pairing importance-weighting/backprop with a value-based “SMCTS” that more closely mirrors MCTS, then layers SH to reduce variance and degeneracy at the root. The pipeline is well-motivated and algorithmically coherent.\n \n- Parallelization narrative. The introduction positions SMC as easier to parallelize (GPU-friendly) than MCTS due to MCTS’s sequential nature and tree-memory overhead—useful context for why this line is promising.\n \n- Addresses core SMC pain points. The paper explicitly diagnoses variance growth with depth and path degeneracy, then justifies why SH (known budget, repeated resets, per-action parallelism) should help at the root."}, "weaknesses": {"value": "- Finite-sample guarantees are thin. The key theory (policy improvement) is stated for infinite particles; practical behavior with finite\nN, finite evaluation accuracy, and shallow search is not quantified with explicit bias/variance or MSE bounds as functions of depth and budget.\n \n- Comparative scope. Beyond GumbelMCTS, stronger or modernized MCTS baselines (e.g., robust PUCT variants, MuZero-style planners under equalized compute/memory) are not deeply explored; reproducibility notes exist, but compute normalization remains largely wall-clock-based, which can be hardware-dependent.\n \n- Root-centric metrics. Degeneracy is principally assessed at the root (active actions). It is less clear how diversity behaves down the tree in deeper horizons, particularly for large or continuous action spaces.\n \n- Trade-off analysis. While SH intuitively reduces variance, the paper does not provide formal rates showing how SH-aggregation plus per-action budget reallocation improve the estimator of the root value or decision quality relative to SMCTS/SMC at fixed compute."}, "questions": {"value": "1. Finite N theory: Can you provide finite-particle error bounds (bias/variance or MSE) for the root value estimator and/or the action selection error under TSMCTS, explicitly showing dependence on depth and per-round budgets, and contrasting with SMCTS/SMC?\n \n2. Robustness to poor priors. Early SH rounds may prune good actions if the policy prior is miscalibrated. Do you employ safeguards (temperature/bonuses or randomized inclusions) to prevent premature elimination? How sensitive is TSMCTS to prior entropy?\n \n3. Beyond the root. Do you track effective sample size/diversity at deeper nodes to confirm that degeneracy is not merely shifted downward? If so, please include these diagnostics; if not, what is your expectation theoretically?\n \n4. SH-induced bias. Since SH repeatedly resets to the root and aggregates across rounds, can you characterize the bias–variance trade-off more formally (e.g., a bound on regret at the root vs. horizon depth), and conditions under which SH yields decision-consistent improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SPMJW45XEF", "forum": "rs06uAQpuH", "replyto": "rs06uAQpuH", "signatures": ["ICLR.cc/2026/Conference/Submission2363/Reviewer_WwXY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2363/Reviewer_WwXY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961363657, "cdate": 1761961363657, "tmdate": 1762916208513, "mdate": 1762916208513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an advanced planning method, SMCTS, which integrates the backpropagation mechanism of MCTS into the classical SMC framework. To further enhance the root policy, the authors introduce TSMCTS, which employs Sequential Halving to focus search resources on a progressively smaller set of actions and runs SMCTS in parallel for each action. TSMCTS reduces variance through Q-value averaging and the SH mechanism while preserving the inherent parallelization advantages of SMC methods."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1.The authors present their method systematically, demonstrating step by step how their algorithm design addresses the limitations of existing approaches.\n2. Combining the backpropagation mechanism of MCTS with the SMC framework is a novel and interesting idea. The inherent parallelization of SMC also shows strong potential for accelerating planning algorithms.\n3. Unlike traditional MCTS, TSMCTS naturally supports both discrete and continuous environments, which is a significant step toward developing a more universal planner."}, "weaknesses": {"value": "1. The paper includes extensive background material and relies heavily on prior works to explain the proposed methods. This makes it difficult to fully understand certain design choices without consulting the references. I suggest presenting a more self-contained description of the algorithms and moving some background discussions to the related work section.\n2. There are several typographical errors and inconsistencies in the notations, suggesting that the notation system could benefit from more careful refinement.\n3. The experimental section lacks detailed setup descriptions and thorough result analysis, particularly for the main experiments. The authors should explain the rationale behind the choice of environments, discuss why performance differs across them, and consider evaluating on more than five environments to strengthen the empirical support."}, "questions": {"value": "1. Why does TSMCTS fail to outperform GumbelMCTS on the Rubik’s Cube task? Is this due to characteristics of the environment, or does TSMCTS generally lose its advantage on more complex tasks?\n2. Please discuss the potential of applying TSMCTS to visual-observation tasks such as Atari, or to problems without an explicit model?\n3. If my understanding of section 2 is correct, a policy improvement operator typically acts on a policy together with its value function. However, in Equation (16), the current policy is paired with the value of the improved policy. Could the authors please explain the theoretical justification for this formulation?\n4. Please discuss how to choose the parameter $\\beta$ in Equation (4)?\n5. In my understanding, GumbelMCTS is designed for discrete tasks originally. How do the authors apply it to continuous tasks?\n6. Other  issues: \n\tIn Equation (4), $\\pi$ and $\\pi_\\theta$ seem to represent the same quantity. Similar inconsistencies appear elsewhere.\n\tIn line 124, the state is equated to a probability, which seems incorrect or unclear.\n\tIn line 128, what does $k$ represent? Likewise, in line 143, the meaning of $Q_i$ is unclear.\n\tin equation 7 and 8, the $A_soft$ is given in different forms.\n\tAlgorithm 1 is never explicitly referenced in the text.\n\tIn algorithm 2, I can't find where ancestor identifier is defined. And if the loop starts from $t=1$, $s_1$ is undefined. The same issue appears in Algorithm 3.\nOverall, I suggest the authors carefully review the entire paper to correct unclear or inconsistent notations and definitions. At the moment this is one of the key points resulting in the selected score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NfcDixz2pl", "forum": "rs06uAQpuH", "replyto": "rs06uAQpuH", "signatures": ["ICLR.cc/2026/Conference/Submission2363/Reviewer_t5vm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2363/Reviewer_t5vm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007792682, "cdate": 1762007792682, "tmdate": 1762916208282, "mdate": 1762916208282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}