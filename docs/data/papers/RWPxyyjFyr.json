{"id": "RWPxyyjFyr", "number": 21265, "cdate": 1758315612897, "mdate": 1759896931780, "content": {"title": "MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion", "abstract": "Large vision–language models (LVLMs) increasingly mediate decisions in shopping, health, and news consumption, where persuasive content is pervasive. An LVLM that is easily persuaded can produce preference-incongruent, unethical, or unsafe outputs. However, their susceptibility remains largely unexplored across diverse topics, strategies, preferences, and modalities. In this paper, we present a unified framework, MMPersuade, for studying multimodal persuasion in LVLMs. It includes a comprehensive multimodal dataset that pairs images and videos with established persuasion principles, covering commercial, subjective and behavioral, and adversarial contexts, and an evaluation framework to measure persuasion effectiveness through third-party agreement scoring and self-estimated token probability. Our study of six leading LVLMs yields three key insights: (i) multimodal inputs are generally more persuasive than text alone, especially in convincing models to accept misinformation; (ii) stated prior preferences decrease susceptibility, yet multimodal information maintains its advantage; and (iii) different strategies vary in effectiveness depending on context, with reciprocity potent in commercial and subjective contexts, and credibility and logic prevailing in adversarial contexts. Our data and framework can support the development of LVLMs that are robust, ethically aligned, and capable of responsibly engaging with persuasive content.", "tldr": "", "keywords": ["Multimodal Persuasion", "LVLM Evaluation", "Fairness", "Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/534e125569ba8f606c0f7994731cef90244968a9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MMPersuade, an evaluation framework designed to assess the susceptibility of large vision-language models (LVLMs) to persuasion in multimodal settings. The framework comprises (1) a multimodal persuasion dataset and (2) dedicated evaluation metrics. The dataset consists of persuasive prompts paired with images and videos, spanning commercial, behavioral, subjective, and adversarial contexts. Using this benchmark, the authors systematically investigate how different factors influence LVLMs as persuadees, focusing on (RQ1) the modality of the input, (RQ2) stated prior user preferences, and (RQ3) the type of applied persuasion strategies. Experimental results suggest that multimodal evidence amplifies persuasive influence (RQ1), explicit prior preferences can mitigate susceptibility (RQ2), and persuasion strategies exhibit varying effectiveness across different persuasion contexts (RQ3)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a comprehensive set of multimodal persuasion scenarios covering diverse contexts and strategy types. As an early benchmark for multimodal persuasion evaluation, it provides a unified framework to compare LVLM behaviors when exposed to persuasive inputs. This resource can support research on both mitigating undesirable persuasion and guiding LVLMs toward appropriate responses under adversarial, multimodal persuasive prompts."}, "weaknesses": {"value": "# 1. Potential bias in model comparison (RQ1)\nThe benchmark partially relies on GPT-generated persuasive content. (The DailyPersuasion dataset used in the commercial and subjective contexts are generated by GPT-4.)  This raises concerns about favorably biasing GPT-based models and disadvantaging others such as Gemini. Consequently, claims about relative resistance or compliance across models may reflect dataset generation artifacts rather than inherent model differences.\n# 2. Lack of Rationale Behind Evaluation Metrics \nDifferent evaluation metrics are highlighted depending on the persuasion context (e.g., Agreement Score for commercial/subjective vs. first-conviction probability for adversarial cases) without a clear explanation of why these choices are appropriate or comparable.\n# 3. Benchmark Positioning and Practical Use Cases \nWhile the benchmark is currently framed as supporting comparisons across persuader-side factors (modality and persuasion strategy) and model comparisons, the dataset seems better aligned with assessing and developing methods that adjust LVLM persuasion resistance or guide model behaviors under persuasive inputs. Positioning the benchmark around these applications, as partially explored in RQ2, would more clearly highlight its practical value and strengthen the contribution. Aligning the experimental design with this use would better demonstrate the benchmark’s utility.\n# 4. Over-implied Generalization to Human Persuasion\nThe paper clearly positions its focus on persuasion in large vision-language models (LVLMs) and does not explicitly claim that its findings generalize to humans. Nonetheless, its framing draws heavily on human-centered scenarios, such as shopping, health advice, and political communication, and employs psychological theories originally developed to explain human persuasion (e.g., Cialdini’s principles, Aristotle’s rhetorical appeals). This rhetorical linkage can implicitly suggest that the observed persuasion mechanisms in LVLMs mirror or inform human psychological processes, even though no empirical or theoretical evidence supports such generalization. The paper would benefit from explicitly clarifying their role as conceptual lenses for structuring persuasion strategies, not as indicators of cognitive similarity between models and humans. A clear statement that the study measures model-level susceptibility within controlled simulations, without assuming correspondence to human persuasion dynamics, would strengthen the methodological transparency and prevent potential overinterpretation of the results.\n# 5. Unrealistic Persuadee Setup\nThe experimental setup models persuasion through scripted, multi-turn conversations between a fixed persuader and an LVLM as the persuadee. The persuadee’s responses are guided by static system prompts, lacking memory, self-awareness, or emotional reasoning. While this setup allows controlled evaluation, it is not an ecologically valid simulation of persuasive interaction, either with humans or with autonomous agents. Consequently, the study measures how models respond to structured instructions rather than how they would behave under authentic persuasion attempts. The paper should acknowledge this gap and justify why the chosen setup meaningfully represents persuasive influence.\n# 6. Limited Exploration of Persuadee Configurations\nThe benchmark uses a single persuadee configuration to assess persuasion effects, with only a small auxiliary test on two preference framings (Appendix D.2). The observed sensitivity to minor framing differences suggests that results could vary substantially across alternative prompts or personality profiles. Relying on one persuadee setup limits the robustness and interpretability of the findings; differences attributed to modality or strategy might instead stem from prompt-specific behaviors. To claim general insights about persuasion in LVLMs, the paper should evaluate multiple persuadee configurations and demonstrate that persuasion effects remain stable across them."}, "questions": {"value": "# 1. Construction of multi-turn persuasive scenarios\nIt is unclear how the multi-turn persuasive conversations are constructed. The paper mentions that the persuader’s messages are sampled from a topic-relevant subset of the dataset, but the exact procedure of generating multiple responses is not explained. It remains ambiguous whether each scenario always includes all strategies exactly once, whether the order of strategies is randomized but fixed per scenario, and whether the same order is consistently used throughout evaluation.\n# 2. Lack of details on token-probability-based evaluation\nThe evaluation method using token probabilities is under-specified. From the example in Figure 21, it appears that in commercial and subjective contexts, function-calling format is used and probabilities are extracted for specific output tokens, yet these details are neither described nor justified in the main paper. It is unclear which text spans serve as input for probability computation, which specific tokens are evaluated, and whether the procedure is consistent across all scenarios.\n# 3. Question on modality comparison (RQ1)\nFor the modality comparison, could the authors clarify how they control for the differences in textual content between the text-only and multimodal conditions? As currently described, it is difficult to attribute performance changes specifically to the presence of multimodal inputs rather than potentially more persuasive language. Have the authors considered alternative designs such as adding multimodal information to the same base text or including non-persuasive images as ablations to more cleanly isolate modality effects?\n# 4. Question on Visual Modality Treatment (RQ1)\nIn the framework, both images and videos are treated as a single multimodal category during evaluation. It would be interesting to know whether the authors have considered analyzing their effects separately. Given that static (image) and dynamic (video) modalities may influence persuasion differently, do the authors view this distinction as unnecessary for their analysis, or have they conducted any preliminary investigation into the separate contributions of each modality?\n# 5. Question on persuasion strategy effects (RQ3)\nCould the authors clarify how the “first conviction round agreement score for each strategy” isolates the effect of a specific persuasion strategy? Based on the description, I understand that each scenario involves multiple turns where exactly one strategy is applied per turn and all strategies appear once in randomized order. If conviction occurs before a given strategy is used, or if the order varies across instances, how can the influence of an individual strategy be reliably attributed?\n# 6. Suggestion on more examples\nThe paper should provide more concrete examples of the data. The dataset appears to vary substantially across persuasion contexts, yet only limited excerpts are shown. Providing these examples in the main paper or appendix would greatly improve clarity and usability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c6RjPhmjR8", "forum": "RWPxyyjFyr", "replyto": "RWPxyyjFyr", "signatures": ["ICLR.cc/2026/Conference/Submission21265/Reviewer_G16g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21265/Reviewer_G16g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761466836725, "cdate": 1761466836725, "tmdate": 1762941659503, "mdate": 1762941659503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MMPERSUADE, a new benchmark and evaluation framework for multimodal persuasion.  It generates image and video-based persuasive content using state-of-the-art generative models and, unlike prior work, focuses on how LVLMs behave as persuadees rather than message generators. The data pipeline /dataste includes multi-turn persuasion settings, three content types (commercial, subjective, adversarial), and six persuasion strategies. To evaluate persuasion effectiveness, the paepr proposes a new metric called PDCG that uses MLLM-as a judge and token probability in a ranking formulation to effectively quantify persuasiveness. \n\nThe main findings are that adding visual data can increase persuasiveness and make stubbornness less effective and through comprehensive analysis it shows what kind of strategies are more effective based on the context and content types."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1- This paper studies an important gap in multimodal persuasion research. Multimodal persuasion and persuasiveness of visual content are emerging topics and are still underexplored.  Specifically, this work introduces the first multimodal persuasiveness benchmark in dialogues and studies LVLM agents as pursuadees. The benchmark studies both image and video generative content and its pipeline and metrics are grounded based on persuasive literature\n\n2- This work proposes a new metric, yet simple.  PDCG effectively measures both the strength of persuasion and its timing. The comprehensive experimental setup includes various sota models and persuasiveness types.  If properly validated, the proposed metric could serve dual purposes: evaluating content persuasiveness and assessing model resistance against unwanted persuasive content \n\n3- Some of the findings are interesting and can potentially be useful in a practical setting. For instance, visual content can help generate better and more persuasive content compared to text-based only, while calling for better and more robust strategies to prevent an increase in persuasion in an unwanted scenario, since including visual content can limit the effectiveness of \"stubbornness.\""}, "weaknesses": {"value": "1- **Some related works are missing**. While I agree that the proposed benchmark is different and novel in the multimodal setting, there exist several multimodal persuasive benchmarks/data. Hence, it's important to acknowledge the existing works and clearly describe the difference of mmpersuasde with existing benchmarks. Some examples are [1-4] are some main examples; however, there are more related works that I encourage authors to acknowledge and discuss in the related work section of the next version. \n\n2- The goal of evaluation for content quality assurance is to ensure the quality of the generated content (images/videos) and their persuasiveness (L161, \"Evaluate the generated outputs ... to ensure persuasiveness, contextual appropriateness, and overall quality.\"). However, the evaluations are *alignment* and *realism*. **Hence, none of the evaluations measure the persuasiveness of the generated content**. Note that current generative models (e.g. T2I) are great/effective at showing the literal content in the input prompt but they may not be powerful tools in generating persuasive, creative or abstract content.  Importantly, the goal of the proposed PDCG is to measure resistance against persuasive content; however, it's not yet. established whether the visual input is persuasive or not, and how persuasive they are. \n\n\n\n[1] MetaCLUE: Towards Comprehensive Visual Metaphors Research, CVPR 2023\n[2] Automatic understanding of image and video advertisements., CPVR 2017\n[3] Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding, EMNLP 2024\n[4] MEASURING AND IMPROVING PERSUASIVENESS OF LARGE LANGUAGE MODELS"}, "questions": {"value": "1- Please clearly describe related work on persuasion and advertisement in computer vision and multimodal settings, and clarify Singh et al ([4]) as it's not text-based only. \n\n2- The evaluations are realism and alignment. How do you make sure that the visual content is targeting the correct persuasion strategy? a quantitative analysis be extremely helpful? Also,  can you please provide some analysis on how the persuasiveness of generated content (e.g., images) impacts the PDCG score? \n\n3- Singh etal ([4] from above) also show that adding an image improves the likes (i.e., persuasiveness). How is [4] different from? While showing similar perspectives/findings is still very valuable, clarifying the difference can better showcase the finding and the contribution. \n\n4- All the visual content is synthetically generated. How does real visual content impact persuasiveness and PDCG? For instance, if using real ads from one of the sources above? [1-4]. If feasible, one approach could be starting from real ads, then verbalizing the images?\n\n5- How do you think this metric can be used in practice? And how should it be interpreted? E.g., for content persuasion or the agent's behavior/resistance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KDA07GP4b3", "forum": "RWPxyyjFyr", "replyto": "RWPxyyjFyr", "signatures": ["ICLR.cc/2026/Conference/Submission21265/Reviewer_17hh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21265/Reviewer_17hh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947887454, "cdate": 1761947887454, "tmdate": 1762941659201, "mdate": 1762941659201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents MMPERSUADE, a benchmark and evaluation framework designed to study how large vision-language models (LVLMs) respond to persuasive communication in text, image, and video formats. It introduces a large-scale dataset comprising over 62,000 images, 4,700 videos, and 450 dialogues that span three domains of persuasion—commercial, subjective or behavioral, and adversarial. The dataset is grounded in established psychological theory, drawing from Cialdini’s six persuasion principles and Aristotle’s rhetorical appeals, and undergoes both model-based and human evaluation to ensure realism and conceptual alignment.\n\nA key contribution of the work is the Persuasion Discounted Cumulative Gain (PDCG) metric, which integrates explicit agreement judgments and implicit token-probability shifts to capture both the strength and timing of persuasive influence. Using this metric, the authors evaluate six LVLMs, including GPT-4 variants, Gemini-2.5, and Llama-4, and find that multimodal inputs consistently enhance persuasive success, even for models with strong prior preferences. The results highlight systematic patterns in how persuasion strategies interact with modality and context, making the study one of the first comprehensive, theory-driven analyses of persuasion susceptibility in multimodal AI systems."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I liked the following things about the paper:\n\n1. The Persuasion Discounted Cumulative Gain (PDCG) Metric -  The paper adapts the concept of discounted cumulative gain (DCG) from information retrieval to model persuasion dynamics. The metric captures both the magnitude and timing of attitude shifts by rewarding early and strong persuasion outcomes. This design provides a unified quantitative measure that integrates explicit agreement scoring and implicit belief estimation through token probabilities. \n\n2. MMPERSUADE comprising over 62,000 images, 4,700 videos, and 450 dialogues spanning commercial, subjective, and adversarial persuasion contexts. \n\n3. Clear writing and easy to read paper"}, "weaknesses": {"value": "1. Motivation and Conceptual Framing: The paper lacks a clear motivation for why persuasion, a fundamentally human and social phenomenon, should be studied within the scope of purely generative LVLMs. Persuasion involves complex constructs such as intention, emotional response, and belief updating—dimensions that generative models do not genuinely possess. Therefore, before introducing a dataset or benchmark, the authors should articulate why modeling persuasion in LVLMs is meaningful. Is the goal to make LVLMs more human-like? To detect manipulative persuasion in multimodal content? Or to test alignment robustness under persuasive stimuli? Without this conceptual framing, the work risks appearing as a technical benchmark detached from theoretical foundations of persuasion research.\n\n\n 2. LVLMs Are Simulating, Not Experiencing, Persuasion: The experimental design implicitly assumes that LVLMs can “be persuaded.” However, as Figure 2 and 6 in the paper itself show, LVLMs are role-playing human persuadees under predefined personas. Thus, the observed effects (agreement, stance change) measure how consistently an LLM can simulate a scripted change of opinion, not whether it was genuinely “persuaded.” This distinction is crucial: the model’s response patterns reflect prompt and persona conditioning, not intrinsic susceptibility. The paper largely discounts the effect of persona prompting, even though prior literature (e.g., Santurkar et al 2023 (https://arxiv.org/abs/2303.17548); Singh et al., 2024) demonstrates that LVLMs’ human-simulation fidelity and value alignment vary widely. Consequently, attributing persuasion effects solely to the LVLM’s internal reasoning—without controlling for prompt design and persona bias—makes the conclusions methodologically fragile.\n\n\n 3. Overreliance on LLM-Generated and LLM-Evaluated Data: Both source datasets—DAILYPERSUASION and FARM—are predominantly LLM-generated or LLM-augmented with minimal human validation. In the proposed pipeline: The prompts are generated by an LLM, The persuader and persuadee roles are both played by LLMs, and The evaluation is again conducted by another LLM acting as judge. This creates a self-referential system where models evaluate and persuade each other with no external anchor in human reality. Such circular validation risks conflating linguistic alignment with cognitive persuasion. Persuasion is not only about textual coherence—it is about affective impact and attitude change—which cannot be assessed meaningfully by another language model. The only human involvement occurs at the end-stage validation, where humans judge whether an LLM-persuadee “accepted” the persuasion attempt. This is arguably the least informative form of human oversight, and it does not offer a ground truth for the persuasion process itself.\n\n 4. Lack of Human Grounding and Ecological Validity: Persuasion is an inherently human construct. Even human experts often fail to predict what arguments persuade others (as shown in Singh et al., 2024). Thus, a persuasion benchmark without human-grounded baselines risks measuring nothing more than intra-model linguistic agreement. The paper cites Singh et al., 2024 but does not employ its validation methodology, which involved:\n - Human judgments of argument persuasiveness (both for self and others),\n-  Field evaluations using real-world digital content (marketing blogs, Reddit posts, tweets), and\n - Empirical proxies of persuasion such as upvotes, likes, or replies.\nBy contrast, MMPersuade operates entirely in synthetic space. Without any calibration to actual human data, its findings cannot be meaningfully generalized to human persuasion behavior. The study needs a human-grounded validation phase—even a small-scale one—to assess whether LVLM-susceptibility patterns align with human judgments.\n\n 5. Real persuasion is interactive and adaptive—persuaders modify their strategies based on the audience’s reactions. In MMPersuade, the persuader’s messages are pre-sampled and static, breaking this essential feedback loop. As a result, the study omits the most critical feature of persuasive dynamics: reciprocal adaptation. This severely limits the ecological validity of the results.\n\n 6. The paper cites prior work such as Kumar et al. (2023) but does not engage with it. They released multimodal persuasion strategies specifically tailored for multimodal content itself."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KtQ0ZVNppg", "forum": "RWPxyyjFyr", "replyto": "RWPxyyjFyr", "signatures": ["ICLR.cc/2026/Conference/Submission21265/Reviewer_Zmkk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21265/Reviewer_Zmkk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762182236174, "cdate": 1762182236174, "tmdate": 1762941658889, "mdate": 1762941658889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}