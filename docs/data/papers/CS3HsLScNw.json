{"id": "CS3HsLScNw", "number": 14552, "cdate": 1758238718410, "mdate": 1759897362972, "content": {"title": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models", "abstract": "Generating high-quality, multi-layer transparent images from text prompts can unlock a new level of creative control, allowing users to edit each layer as effortlessly as editing text outputs from LLMs. However, the development of multi-layer generative models lags behind that of conventional text-to-image models due to the absence of a large, high-quality corpus of multi-layer transparent data. We address this fundamental challenge by: (i) releasing four open, ultra-high-fidelity datasets—\\multilayertrain, \\multilayertrainplus, \\multilayertrainpro, and \\multilayerreal—consisting of 200K, 100K, 20K, and 1K multi-layer transparent images with accurate alpha mattes, respectively. (ii) introducing a training-free synthesis pipeline that generates such data on demand using off-the-shelf diffusion models, and (iii) delivering a strong multi-layer generation model, ART+, which matches the aesthetics of modern text-to-image generation models. The key technical contributions include: LayerFLUX, which excels at generating high-quality single transparent layers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple LayerFLUX outputs into complete images, guided by human-annotated semantic layout. To ensure higher quality, we apply a rigorous filtering stage to remove artifacts and semantic mismatches, followed by human selection. Fine-tuning the state-of-the-art ART model on our synthetic \\multilayertrainpro yields ART+, which outperforms the original ART in 60\\% of head-to-head user study comparisons and even matches the visual quality of images generated by the FLUX.1-[dev] model. Our work establishes a solid dataset foundation for multi-layer transparent image generation, enabling research and applications that require precise, editable, and visually compelling layered imagery.", "tldr": "", "keywords": ["Graphic Design; Multi-Layer Transparent Image; Diffusion Model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a9fadaa2d1b86757f7ab6e488aa158870c84c4d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper releases PrismLayers family datasets for multi-layer transparent image generation and builds LayerFLUX / MultiLayerFLUX, plus an ART+ model fine-tuned on the data. Most pipelines and benchmarks are coupled with FLUX.1-[dev]."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Fills a data gap for layered images with alpha mattes; gives sizes and curation steps.\n- Clear pipeline for generating layers then composing; user studies and metrics are reported."}, "weaknesses": {"value": "- Possible double-blind violation: page-1 public Hugging Face link to the dataset; this can reveal author identity.\n- Styles skew to design/cartoon/3D (e.g., toy, ink, doodle). The photoreal set is only 1K; real-image diversity is limited.\n- Fig. 3 text/typography is hard to read; legends could be clearer.\n- Base-model narrowness: training-free pipeline and benchmarks are largely FLUX-centric; generalization to other backbones is not shown."}, "questions": {"value": "Can you test other base models (e.g., SDXL/DiT/flow variants) to show model-agnostic value beyond FLUX?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nKu6Rya3tT", "forum": "CS3HsLScNw", "replyto": "CS3HsLScNw", "signatures": ["ICLR.cc/2026/Conference/Submission14552/Reviewer_H3ue"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14552/Reviewer_H3ue"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760596661786, "cdate": 1760596661786, "tmdate": 1762924942641, "mdate": 1762924942641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free synthesis pipeline for high-quality multi-layer transparent images: LayerFLUX adopts a \"generate-then-matting\" scheme, using suffix prompts to guide diffusion models in generating single-layer images with uniform backgrounds and leveraging RMBG-2.0 for accurate alpha matte extraction, while MultiLayerFLUX composes these layers by preserving their original aspect ratios and following semantic layouts. The paper curates 4 open high-fidelity datasets—PrismLayers (200K), PrismLayersPlus (100K), PrismLayersPro (20K), and PrismLayersReal (1K)—curated via artifact filtering, TIPS evaluation (a CLIP-fine-tuned transparent image quality model), and human selection (for PrismLayersPro). For evaluation, the paper fine-tunes the ART model on PrismLayersPro to obtain ART+, which outperforms the original ART of head-to-head user study comparisons and approaches the visual quality of FLUX.1-[dev]. Additionally, LayerFLUX outperforms LayerDiffuse on Layer-Bench with 63.1% win rate in layer quality and 61.2% in prompt following. The TIPS model provides a dedicated metric to assess the aesthetic quality of transparent images, validating the effectiveness of the proposed methods and datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- High-quality and comprehensive open-source datasets. The paper constructs four Multi-Layer Transparent Image datasets, with the number of layers ranging up to 50. This makes substantial contributions to the field's advancement.\n- Pipeline for constructing high-quality multilayer transparent datasets. The paper proposes a novel pipeline that leverages FLUX—a powerful full-image generation model—to produce multilayer transparent images.\n- A new preference model for transparent image synthesis. Addressing the incompatibility of existing RGB image quality models with transparent layers, the paper introduces the TIPS (Transparent Image Preference Score) model, which fills a current research gap."}, "weaknesses": {"value": "- Lack of naturalness in fully synthesized datasets. Visualizations indicate the proposed dataset contains numerous cartoon elements that are generally easy to matte out from images. However, the paper does not propose a new approach to acquire transparent images from real-world scenes.\n\n- Failure to address key challenges in multi-layered image generation. For multi-layered image generation, shadows, lighting, transparent objects (e.g., glass), and reflections on water or other surfaces are critical. When adding an object to a scene, the image should change in line with physical laws—a key requirement that the proposed method does not fulfill."}, "questions": {"value": "- When constructing multi-layered image datasets, how does the pipeline define the semantic content of each layer while explicitly modeling physical interactions (e.g., shadow casting, light propagation, and material properties like transparency/reflectivity) across layers?\n-  How does the proposed pipeline ensure visual coherence and physical plausibility in generated multi-layered images?\n- Are the data construction pipeline and benchmark suite (including pre-trained models, evaluation protocols) planned for open-source release?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WOr8SUWshH", "forum": "CS3HsLScNw", "replyto": "CS3HsLScNw", "signatures": ["ICLR.cc/2026/Conference/Submission14552/Reviewer_vNNp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14552/Reviewer_vNNp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832343015, "cdate": 1761832343015, "tmdate": 1762924942159, "mdate": 1762924942159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new series of datasets for multi-layer generation, where the proposed dataset mainly focuses on artistic elements from the qualitative examples present in the paper. The proposed dataset has multiple versions which are differing in terms of data scale and the types of examples included. To generate such data, the authors propose an inference time methodology named LayerFLUX and MultiLayerFLUX, which is integrated on a multi-stage data generation pipeline. The proposed dataset includes samples that consists of many layers compared to existing layered datasets, which is an advantage of the method. Lastly, the authors show the quality of the collected data with improvements reported on ART+ model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors propose a series of datasets that include a significant number of layers and in different scales with changing quality. Unlike the existing datasets, the proposed dataset shows a data scenario that can be generalized better to layered synthesis in imaging scenarios.\n- In addition to layers, the proposed dataset also gives a good source of text rendering data.\n- The paper proposes a multi-stage pipeline to generate a high quality data generation. The proposed pipeline covers the quality issues and the filtering of data in a sensible way."}, "weaknesses": {"value": "- The related work section is limited in the paper and do not cover the majority of layered synthesis work. While it is understandable to not include in the main paper, such works should be acknowledged at least in the supplementary.\n- From the data samples included in the dataset, even in the Real data split of the proposed data, the samples seem artificial and do not serve as real samples.\n- As a primary use case of layered images, objects involving transparency properties have crucial importance. While samples including such objects are shown qualitatively, dataset statistics on such objects would be helpful to assess the usability of the proposed dataset.\n- While showcasing a dataset with considerably higher quality, the contribution made with LayerFLUX and MultiLayerFLUX is questionable. From the text, it seems that the layered generation is only initiated with suffix prompts fed into FLUX. In addition, the paper states that MultiiLayerFLUX is generating multiple single layer images with varying resolutions. After applying the matting stage, one question arises is that how does these layers belong to the same scene (enforced by prompting, or an attention sharing mechanism). Explaining such mechanics is important to be able to understand how can multi-layer synthesis can be facilitated. In the current format the operation \"Compose\" is very ambiguous and can best be interpreted as alpha blending.\n- The organization of the paper can be improved, the current organization is fairly hard to follow. The figures can be better connected with the text with more explanatory captions."}, "questions": {"value": "- How did the authors assess the quality of the data in Table 1 (Alpha quality, aesthetic)?\n- In the multi-layer generation pipeline with MultiLayerFLUX, how does the approach handle overlapping layers, and render them in a coherent scene?\n- In the examples provided for ART and ART+, the difference is not that clear. What is the main point that the readers should pay attention there? To be able to understand the quality difference (if any), we should be provided with generation prompts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M9xagIjA3m", "forum": "CS3HsLScNw", "replyto": "CS3HsLScNw", "signatures": ["ICLR.cc/2026/Conference/Submission14552/Reviewer_7bLz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14552/Reviewer_7bLz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971043314, "cdate": 1761971043314, "tmdate": 1762924940477, "mdate": 1762924940477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PrismLayers, a suite of four open, high-quality datasets (PrismLayers, PrismLayersPlus, PrismLayersPro, and PrismLayersReal) for multi-layer transparent image generation, addressing the lack of large, high-fidelity data in this domain. The authors present a training-free synthesis pipeline using off-the-shelf diffusion models and advanced matting techniques to generate and curate these datasets, which include accurate alpha mattes and diverse styles. They also propose LayerFLUX for single-layer generation and MultiLayerFLUX for compositing, and demonstrate that fine-tuning the ART model on PrismLayersPro yields ART+, a model that outperforms previous methods in user studies and matches the visual quality of leading text-to-image models. The work establishes a foundation for research and applications requiring editable, visually compelling, and precisely layered image geneartions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Releases the first high-fidelity datasets for multi-layer transparent image generation, filling a major gap in the field.\n\n+ The work employs rigorous artifact filtering, aesthetic scoring, and human selection to ensure dataset quality and diversity. The creating pipline and experiences are also valuable. \n\n+ Fine-tuned models (ART+) trained on PrismLayersPro achieve better performance in both quantitative metrics and user studies compared with recent single-layer models."}, "weaknesses": {"value": "- While high-quality, the datasets are primarily synthetic, and may not fully capture the complexity or coherence of real-world multi-layer images.\n\n- The accuracy of text rendering seems one of the eval dimension for this work. But it seems that in both user studies and metrics. How about the text rendering quality/accuracy of the proposed work?"}, "questions": {"value": "Please refer to the detailed questions raised in Weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kGvnZUmu8u", "forum": "CS3HsLScNw", "replyto": "CS3HsLScNw", "signatures": ["ICLR.cc/2026/Conference/Submission14552/Reviewer_uqof"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14552/Reviewer_uqof"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989449073, "cdate": 1761989449073, "tmdate": 1762924940063, "mdate": 1762924940063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}