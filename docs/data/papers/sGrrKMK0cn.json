{"id": "sGrrKMK0cn", "number": 15911, "cdate": 1758256992637, "mdate": 1759897273683, "content": {"title": "GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL", "abstract": "Offline Safe Reinforcement Learning (OSRL) aims to learn a policy that achieves high performance in sequential decision-making while satisfying safety constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost return-to-go values. However, GM-assisted methods face two major challenges in constrained settings: (1) they lack the ability to ``stitch'' optimal transitions from suboptimal trajectories within the dataset, and (2) they struggle to balance reward maximization with constraint satisfaction, particularly when tested with imbalanced human-specified reward-cost conditions. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods.", "tldr": "", "keywords": ["offline safe reinforcement learning", "constrained decision transformer", "improving stitching ability"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/96b28b80dae673efa0cc3e73817bd6db0e93e174.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes GAS, a method designed to balance the reward and cost conditions in offline safe reinforcement learning. GAS achieves a favorable trade-off between safety and reward by combining techniques such as return augmentation, return relabeling, goal generation, and dataset reshaping."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper adopts a reasonable approach to tackling offline safe reinforcement learning by focusing on balancing reward and cost.\n- The use of goal generation to address the reward–cost trade-off is well-motivated.\n- The proposed method demonstrates strong reward performance in tasks with relatively low safety requirements."}, "weaknesses": {"value": "- Methodology\n    - The proposed method combines return augmentation, return relabeling, goal generation, and dataset reshaping, making it overly complex and heavily reliant on manual engineering design, with limited novelty. In fact, similar ideas have been explored in prior work.\n        - Reinformer (ICML 2024 [1]) generates RTG based on state using expectile regression, achieving strong reward stitching capabilities.\n        - CoPDT (NeurIPS 2025 [2]) applies a similar concept in offline safe RL, automatically generating RTG distributions conditioned on state and CTG, and sampling from them using expectile regression. It demonstrates strong cost prioritization and achieves safe performance in Safety Gymnasium Navigation tasks under more stringent thresholds ([10, 20, 40, 80]).\n        \n        Compared to these works, GAS requires extensive dataset relabeling and augmentation, while methods like Reinformer and CoPDT only rely on learning the expectile distribution, making them simpler and more flexible.\n        \n    - The model’s input design—comprising four types of tokens (reward goal, cost goal, RTG, and CTG)—appears unnecessarily complicated. Since the reward goal is already generated, why are RTG and CTG still needed as additional inputs? Although the paper states that reward goal and cost goal are designed to avoid conflicts, what happens when RTG and CTG themselves conflict? How does the model ensure that reward goal and cost goal remain the primary optimization targets, rather than being overshadowed by RTG and CTG? Furthermore, if RTG and CTG do conflict, how is CTG’s priority guaranteed?\n- Experiments\n    - The experiments are mainly conducted on BulletSafetyGym and the Circle tasks from Safety Gymnasium, but they omit more standard and safety-critical environments, such as other Navigation and Velocity tasks from Safety Gymnasium.\n    - The safety requirements in the experimental settings are too lenient.\n        - In BulletSafetyGym, satisfying safety constraints is already relatively easy.\n        - In Safety Gymnasium, most tasks have constraint thresholds above 20 (for a 10% constraint ratio), which are too loose to evaluate the model’s ability under strict safety conditions.\n        - In the “loose constraint” settings, the thresholds remain excessively high, making the evaluation of safety performance insufficient.\n        \n        Overall, the experiments mainly verify that GAS achieves high reward under low safety requirements, but its applicability to high-safety-demand scenarios remains questionable.\n        \n    - The method involves numerous hyperparameters, including expectile level $\\alpha$, reward relabel level $\\delta$, dataset reshape threshold q% and sample probability $\\epsilon$, among others. The authors should conduct a sensitivity analysis to evaluate how these hyperparameters affect performance.\n\n[1] Reinformer: Max-Return Sequence Modeling for offline RL\n\n[2] Adaptable Safe Policy Learning from Multi-task Data with Constraint Prioritized Decision Transformer"}, "questions": {"value": "No other questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xlqMPBaepM", "forum": "sGrrKMK0cn", "replyto": "sGrrKMK0cn", "signatures": ["ICLR.cc/2026/Conference/Submission15911/Reviewer_aKUM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15911/Reviewer_aKUM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760607259090, "cdate": 1760607259090, "tmdate": 1762926130331, "mdate": 1762926130331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper starts from two challenges in offline safe RL: 1) most existing methods lack capability to stitch good sub-trajectories from different sub-optimal trajectories, and 2) they struggle to balance reward and cost. The authors propose Goal-Assisted Stitching (GAS), which includes return augmentation and relabeling, expectile regression, dataset distribution reshaping, to overcome those issues. The authors run experiments on safe RL benchmarks and the results show that GAS show superior performance to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Two motivations of paper, i.e., lack of stitching and failure to balance reward and cost, are clearly presented and important to safe offline RL community.\n- GAS exhibits consistent advantage over most baselines on DSRL in terms of reward improvement and cost constraint satisfaction, in both situations with tight or loose constraint.\n- The experiment is comprehensive. The authors also provide ablations to explain the motivation or show the effectiveness of relabel module."}, "weaknesses": {"value": "- The \"stitching\" ability of the GAS is a little over-claimed. The authors claim the GAS has the capability to stitch the sub-trajectories. However, GAS seems to learn new return-to-go and cost-to-go targets instead of stitching the data. See more in the question section. I understand that the true stitching can be hard but I believe there is a large mismatch between \"stitching\" and GAS's implementation."}, "questions": {"value": "- In fig 2, the stitched trajectory (denoted by red arrow) is confusing. For example, in the stitch of GAS showed by the second arrow (from the 2nd block of $\\tau_j$ to the 1st of $\\tau_k$), since the states in $\\tau_j,\\tau_k$ will be different, how do you make sure the 1st block of $\\tau_k$ can be stitched after the 2nd block of $\\tau_j$? Otherwise it's not even a valid trajectory. \n- In sec.5.2, GAS seeks better $R,C$ based on other sub-trajectory / segmentation as long as their lengths are the same. However, since the starting states $s_t, s_{t'}$ are different, their optimal $R$ and $C$ will also be different. Why do you think the $R,C$ of $s_{t'}$ can be a good reference for $s_t$?\n- The dataset reshaping seems to be also an important component. Do you have any ablation on that? E.g., the performance comparison with v.s. without reshaping."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dxoEoP4VFE", "forum": "sGrrKMK0cn", "replyto": "sGrrKMK0cn", "signatures": ["ICLR.cc/2026/Conference/Submission15911/Reviewer_oGCt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15911/Reviewer_oGCt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879438716, "cdate": 1761879438716, "tmdate": 1762926129238, "mdate": 1762926129238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method called Goal-Assisted Stitching (GAS) to improve how reinforcement learning agents make safe and effective decisions using only offline data. It builds on recent generative model approaches but points out that these methods often fail to combine good parts of different trajectories and cannot properly balance rewards and safety limits. GAS addresses these issues by estimating achievable reward and cost goals directly from the dataset, instead of depending on user-specified targets. It does this through a statistical technique called expectile regression and adds two simple but effective steps: segmenting trajectories into shorter pieces to create more training samples, and reshaping the dataset so that the model sees a more even mix of safe and risky examples. Tested on common safe RL benchmarks, GAS shows more reliable safety under strict constraints and higher returns when the safety limits are relaxed, performing better than previous approaches in both stability and adaptability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work tackles a practical and important problem in Offline Safe Reinforcement Learning (OSRL) with Generative Models (GMs), focusing on two key challenges: balancing reward and cost, and improving the model’s ability to stitch useful transitions from different trajectories.\n- The paper presents solid theoretical support for the proposed GAS algorithm, especially through its use of expectile regression to estimate optimal reward and cost goals without relying on Bellman backups. This design effectively avoids common offline RL problems like value overestimation and out-of-distribution action selection. The clear theoretical foundation not only strengthens the methodological soundness of the approach but also enhances confidence in the reported empirical results, marking it as a notable strength of the work.\n- The paper shows strong empirical rigor through comprehensive experiments on multiple benchmarks and baselines under various constraint settings and ablation studies, convincingly demonstrating the robustness and effectiveness of the proposed GAS method.\n- The paper is clearly written and well organized, moving smoothly from the motivation to the method and then to the experiments, making it easy for readers to follow the main ideas and technical details."}, "weaknesses": {"value": "- The reward and cost goal functions are learned from offline data using expectile regression, which makes them susceptible to the biases of the dataset. If the data are unbalanced or lack high-quality transitions, the estimated goals can become either too optimistic or too conservative, leading the policy in the wrong direction. Because these functions guide the policy’s optimization, even small estimation errors can distort the balance between reward and cost or result in unsafe actions.\n- The use of expectile regression introduces a hyperparameter \\alpha that controls the aggressiveness of the upper-tail estimation. The paper does not analyze the sensitivity of \\alpha or provide adaptive tuning [1].\n- Although the paper includes some information on infrastructure and training time, it does not clearly analyze computational efficiency. The method adds several components—segmented augmentation, dual goal functions, and dataset reshaping—but lacks a direct runtime or scalability comparison with CDT, making it difficult to assess the true efficiency impact [2].\n- While the proposed GAS framework demonstrates strong performance on dense-reward benchmarks such as SafetyGymnasium and Bullet-Safety-Gym, it lacks evaluation in sparse-reward settings [3-5].\n- Although the paper builds on recent progress in generative model–assisted offline safe RL, earlier works [6] have already explored similar ideas such as goal-conditioned augmentation and return relabeling. It would strengthen the paper to more clearly distinguish GAS from these methods, include [6] as a baseline for direct comparison, and incorporate additional recent strong baselines to ensure a fair and comprehensive evaluation [7-8].\n- The reward and cost goal functions are trained separately, without accounting for their inherent correlation in constrained settings. This independent training can produce inconsistent or even infeasible goal pairs, but the paper does not discuss or analyze this potential issue."}, "questions": {"value": "- How sensitive is the performance of GAS to the choice of the expectile level \\alpha?\n- What are the training time and GPU resource requirements of GAS compared with CDT?\n- How would GAS perform on tasks with sparse or delayed rewards?\n- Since the reward and cost goal functions are trained independently, could the authors analyze their mutual consistency and verify that the resulting goal pairs correspond to feasible points on the reward–cost frontier?\n- Could the authors include additional baselines, as mentioned in the weakness section, to further verify the performance of GAS?\n\n## Reference\n\n**[1]** Zhuang, Zifeng, et al. \"Reinformer: Max-return sequence modeling for offline rl.\" arXiv preprint arXiv:2405.08740 (2024).\n\n**[2]** Chemingui, Yassine, et al. \"Constraint-adaptive policy switching for offline safe reinforcement learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 15. 2025.\n\n**[3]** Cen, Zhepeng, et al. \"Learning from sparse offline datasets via conservative density estimation.\" arXiv preprint arXiv:2401.08819 (2024).\n\n**[4]** Rengarajan, Desik, et al. \"Reinforcement learning with sparse rewards using guidance from offline demonstration.\" arXiv preprint arXiv:2202.04628 (2022).\n\n**[5]** Yamagata, Taku, Ahmed Khalil, and Raul Santos-Rodriguez. \"Q-learning decision transformer: Leveraging dynamic programming for conditional sequence modelling in offline rl.\" International Conference on Machine Learning. PMLR, 2023.\n\n**[6]** Wang, Ruhan, and Dongruo Zhou. \"Safe Decision Transformer with Learning-based Constraints.\" 7th Annual Learning for Dynamics\\& Control Conference. PMLR, 2025.\n\n**[7]** Guan, Jiayi, et al. \"Voce: Variational optimization with conservative estimation for offline safe reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 33758-33780.\n\n**[8]** Wei, Honghao, et al. \"Adversarially trained weighted actor-critic for safe offline reinforcement learning.\" Advances in Neural Information Processing Systems 37 (2024): 52806-52835."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZKoaorPCZ6", "forum": "sGrrKMK0cn", "replyto": "sGrrKMK0cn", "signatures": ["ICLR.cc/2026/Conference/Submission15911/Reviewer_RPXE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15911/Reviewer_RPXE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984438567, "cdate": 1761984438567, "tmdate": 1762926128649, "mdate": 1762926128649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method called Goal-Assisted Stitching (GAS) to improve how reinforcement learning agents make safe and effective decisions using only offline data. It builds on recent generative model approaches but points out that these methods often fail to combine good parts of different trajectories and cannot properly balance rewards and safety limits. GAS addresses these issues by estimating achievable reward and cost goals directly from the dataset, instead of depending on user-specified targets. It does this through a statistical technique called expectile regression and adds two simple but effective steps: segmenting trajectories into shorter pieces to create more training samples, and reshaping the dataset so that the model sees a more even mix of safe and risky examples. Tested on common safe RL benchmarks, GAS shows more reliable safety under strict constraints and higher returns when the safety limits are relaxed, performing better than previous approaches in both stability and adaptability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work tackles a practical and important problem in Offline Safe Reinforcement Learning (OSRL) with Generative Models (GMs), focusing on two key challenges: balancing reward and cost, and improving the model’s ability to stitch useful transitions from different trajectories.\n- The paper presents solid theoretical support for the proposed GAS algorithm, especially through its use of expectile regression to estimate optimal reward and cost goals without relying on Bellman backups. This design effectively avoids common offline RL problems like value overestimation and out-of-distribution action selection. The clear theoretical foundation not only strengthens the methodological soundness of the approach but also enhances confidence in the reported empirical results, marking it as a notable strength of the work.\n- The paper shows strong empirical rigor through comprehensive experiments on multiple benchmarks and baselines under various constraint settings and ablation studies, convincingly demonstrating the robustness and effectiveness of the proposed GAS method.\n- The paper is clearly written and well organized, moving smoothly from the motivation to the method and then to the experiments, making it easy for readers to follow the main ideas and technical details."}, "weaknesses": {"value": "- The reward and cost goal functions are learned from offline data using expectile regression, which makes them susceptible to the biases of the dataset. If the data are unbalanced or lack high-quality transitions, the estimated goals can become either too optimistic or too conservative, leading the policy in the wrong direction. Because these functions guide the policy’s optimization, even small estimation errors can distort the balance between reward and cost or result in unsafe actions.\n- The use of expectile regression introduces a hyperparameter $\\alpha$ that controls the aggressiveness of the upper-tail estimation. The paper does not analyze the sensitivity of $\\alpha$ or provide adaptive tuning [1].\n- Although the paper includes some information on infrastructure and training time, it does not clearly analyze computational efficiency. The method adds several components—segmented augmentation, dual goal functions, and dataset reshaping—but lacks a direct runtime or scalability comparison with CDT, making it difficult to assess the true efficiency impact [2].\n- While the proposed GAS framework demonstrates strong performance on dense-reward benchmarks such as SafetyGymnasium and Bullet-Safety-Gym, it lacks evaluation in sparse-reward settings [3-5].\n- Although the paper builds on recent progress in generative model–assisted offline safe RL, earlier works [6] have already explored similar ideas such as goal-conditioned augmentation and return relabeling. It would strengthen the paper to more clearly distinguish GAS from these methods, include [6] as a baseline for direct comparison, and incorporate additional recent strong baselines to ensure a fair and comprehensive evaluation [7-8].\n- The reward and cost goal functions are trained separately, without accounting for their inherent correlation in constrained settings. This independent training can produce inconsistent or even infeasible goal pairs, but the paper does not discuss or analyze this potential issue.\n\n***Disclose the use of LLMs:*** *I used LLM for light writing assistance, including grammar refinement and phrasing suggestions. All substantive assessments, judgments, and technical evaluations in this review are my own. If the authors resolve the concerns and questions I raised, I would be willing to improve my score.*"}, "questions": {"value": "- How sensitive is the performance of GAS to the choice of the expectile level $\\alpha$?\n- What are the training time and GPU resource requirements of GAS compared with CDT?\n- How would GAS perform on tasks with sparse or delayed rewards?\n- Since the reward and cost goal functions are trained independently, could the authors analyze their mutual consistency and verify that the resulting goal pairs correspond to feasible points on the reward–cost frontier?\n- Could the authors include additional baselines, as mentioned in the weakness section, to further verify the performance of GAS?\n\n## Reference\n\n**[1]** Zhuang, Zifeng, et al. \"Reinformer: Max-return sequence modeling for offline rl.\" arXiv preprint arXiv:2405.08740 (2024).\n\n**[2]** Chemingui, Yassine, et al. \"Constraint-adaptive policy switching for offline safe reinforcement learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 15. 2025.\n\n**[3]** Cen, Zhepeng, et al. \"Learning from sparse offline datasets via conservative density estimation.\" arXiv preprint arXiv:2401.08819 (2024).\n\n**[4]** Rengarajan, Desik, et al. \"Reinforcement learning with sparse rewards using guidance from offline demonstration.\" arXiv preprint arXiv:2202.04628 (2022).\n\n**[5]** Yamagata, Taku, Ahmed Khalil, and Raul Santos-Rodriguez. \"Q-learning decision transformer: Leveraging dynamic programming for conditional sequence modelling in offline rl.\" International Conference on Machine Learning. PMLR, 2023.\n\n**[6]** Wang, Ruhan, and Dongruo Zhou. \"Safe Decision Transformer with Learning-based Constraints.\" 7th Annual Learning for Dynamics\\& Control Conference. PMLR, 2025.\n\n**[7]** Guan, Jiayi, et al. \"Voce: Variational optimization with conservative estimation for offline safe reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 33758-33780.\n\n**[8]** Wei, Honghao, et al. \"Adversarially trained weighted actor-critic for safe offline reinforcement learning.\" Advances in Neural Information Processing Systems 37 (2024): 52806-52835."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZKoaorPCZ6", "forum": "sGrrKMK0cn", "replyto": "sGrrKMK0cn", "signatures": ["ICLR.cc/2026/Conference/Submission15911/Reviewer_RPXE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15911/Reviewer_RPXE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984438567, "cdate": 1761984438567, "tmdate": 1763664205059, "mdate": 1763664205059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Goal-Assisted Stitching (GAS) is a generative-model–assisted approach to offline safe reinforcement learning (OSRL) that improves trajectory stitching and the reward–cost balance using Temporal Segmented Return Augmentation (TSRA) plus transition-level relabeling, expectile-trained goal functions, and dataset reshaping; those goals guide a constrained Advantage-Weighted Regression (AWR) policy, yielding safer and higher-reward policies across two benchmarks and multiple constraint levels."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper pinpoints two gaps in GM-assisted offline safe RL: weak stitching and inadequate reward–cost balancing.\n- Using goal functions to guide a constrained Advantage-Weighted Regression policy, aligning actions with feasible reward–cost targets is interesting.\n- GAS adapts to different test-time constraint levels without retraining."}, "weaknesses": {"value": "- In RL, \"stitching\" traditionally refers to combining different trajectories via bootstrapping (the Bellman backup). The paper itself identifies this as a \"crucial ability\" that Generative Model (GM) methods lack. However, the paper's entire premise is to avoid the Bellman backup, which it calls the \"primary source of the OOD problem\". It then calls its own mechanism, supervised goal estimation using expectile regression, \"stitching\". This is a misleading contradiction. The proposed method does not \"stitch\" in the established sense.\n\n- In Fisor, the tight budget is 10 for safety gym and 5 for bullet gym. Based on the cost range table 2, only for CarRun the tight budget of 10% is below 5, while for antrun its 15, 3 times the “tight” budget of fisor.\n\n- The main results table (Table 1) shows that CDT is an unsafe policy in the average tight budget setting on the tasks chosen for the zero-shot ablation (BallRun, DroneRun, DroneCircle, etc.). Comparing GAS against a baseline that already fails is not informative.\n\n- The paper includes comparisons against recent algorithms like CAPS and CCAC in Table 6, but the main results in Table 1 instead feature baselines like CPQ, COptiDICE, and VOCE . Given that CAPS and CCAC are more recent and appear to be stronger competitors than some of those in the main table, they should have been included in the primary comparison."}, "questions": {"value": "- Please check weaknesses.\n\n- Could you train GAS solely on an offline dataset of unsafe trajectories? If it succeeds, this may indicate an ability to stitch together feasible segments from otherwise failed trajectories.\n\n- In the tight constraint setting (Table 1), an interesting observation is that the rewards achieved by GAS are too similar to that of CDT, yet the costs are lower and safely below the constraint, while CDT's costs violate the constraint often.  How can GAS maintain the same reward scale as CDT while successfully reducing costs to a safe level, given that these two objectives are conflicting?\n\n- FISOR is designed to ensure the strict satisfaction of constraints. They focus on identifying the largest feasible region where zero violation is enforced.  How is FISOR able to adapt and show varying performance across the different loose and tight constraint limits presented in Table 1? \n\n- How would GAS perform on additional tasks from SafetyGym or MetaDrive?\n\n- What is the segment length used in TSRA?\n\n- The main results in Table 1 are reported as averages across multiple tight and loose constraint thresholds. Please provide the separate, unaveraged normalized reward and cost returns for GAS and the baseline methods at each individual constraint level."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CBYSpRM45y", "forum": "sGrrKMK0cn", "replyto": "sGrrKMK0cn", "signatures": ["ICLR.cc/2026/Conference/Submission15911/Reviewer_gTZL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15911/Reviewer_gTZL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996565271, "cdate": 1761996565271, "tmdate": 1762926128199, "mdate": 1762926128199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}