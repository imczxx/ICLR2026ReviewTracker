{"id": "ks6Gg8nd0y", "number": 6884, "cdate": 1757999807849, "mdate": 1759897886137, "content": {"title": "Exploring Cross-Modal Flows for Few-Shot  Learning", "abstract": "Aligning features from different modalities is one of the most fundamental challenges for cross-modal tasks. Although pre-trained vision-language models can achieve a general alignment between image and text, they often require parameter-efficient fine-tuning (PEFT) for further adjustment. Today’s PEFT methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively fine-tune a subset of parameters, which can slightly adjust either visual or textual features, and avoid overfitting. In this paper, we are the first to highlight that all existing PEFT methods perform one-step adjustment and are insufficient for complex (or difficult) datasets, where features of different modalities are highly entangled. To this end, we propose the first model-agnostic multi-step adjustment approach by learning a cross-modal velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the correspondence between categories during training, we first utilize a fixed coupling strategy. Then, we propose a noise augmentation strategy to alleviate the data scarcity issue. Finally, we design an early-stopping solver, which terminates the transformation process earlier, improving both efficiency and accuracy. Compared with one-step PEFT methods, FMA has the multi-step rectification ability to achieve more precise and robust alignment. Extensive results have shown that FMA can consistently yield significant performance gains across various benchmarks and backbones, especially on difficult datasets.", "tldr": "we propose the first model-agnostic multi-step adjustment approach in few-shot learning by learning a cross-modal velocity field.", "keywords": ["Representation Learning; Generative Model; Few-shot Learning;"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/413ba12a1db854a17e0c58bb317795991e3d163e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new framework called Flow Matching Alignment (FMA) to improve feature alignment between visual and textual modalities in few-shot learning. The authors observe that current parameter-efficient fine-tuning (PEFT) methods—such as prompt tuning, adapter tuning, and LoRA—perform only one-step feature adjustments, which are insufficient for complex datasets where image–text features are highly entangled. FMA leverages the multi-step rectification ability of flow matching by learning a cross-modal velocity field that iteratively transforms image features toward corresponding text embeddings, achieving finer alignment. To ensure stability and correctness, the method incorporates three key designs: coupling enforcement (to maintain class correspondence), noise augmentation (to mitigate data scarcity), and an early-stopping solver (to prevent over-transformation during inference). Experiments across 11 benchmarks and multiple backbones show that FMA consistently outperforms existing PEFT methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. FMA introduces flow matching to few-shot learning. By formulating traditional PEFT methods as one-step updates, FMA enables more precise iterative alignment between visual and textual features. As argued by athe uthors, FMA better handles entangled multimodal distributions, especially in challenging datasets.\n2. The framework is architecture-independent and can be integrated with various pre-trained vision-language models (e.g., CLIP, CoOp, LoRA) without altering their internal structures."}, "weaknesses": {"value": "1. The multi-step flow matching process requires iterative training and inference, which increases computational cost compared to traditional one-step PEFT methods, potentially limiting scalability for large datasets or real-time applications.\n2. The method relies on carefully chosen parameters such as the number of inference steps, step size, and noise schedule. Suboptimal tuning can lead to degraded performance or instability during alignment. Especially when flow matching originates from generative modeling, and its adaptation to supervised classification tasks lacks rigorous theoretical grounding, in terms of convergence and optimal stopping criteria."}, "questions": {"value": "1. As discussed in the weakness part, is there any theoretical guarantee of convergence and stability of FMA? Given that flow matching originates from generative modeling, what are the theoretical conditions under which FMA ensures convergence to the correct class-aligned distribution in supervised learning settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nfst7QIEoX", "forum": "ks6Gg8nd0y", "replyto": "ks6Gg8nd0y", "signatures": ["ICLR.cc/2026/Conference/Submission6884/Reviewer_girA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6884/Reviewer_girA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761243211031, "cdate": 1761243211031, "tmdate": 1762919131237, "mdate": 1762919131237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reframes PEFT as a one-step adjustment problem that fails on difficult datasets and proposes Flow Matching Alignment (FMA), which learns a velocity field to iteratively transport image features toward their ground-truth text features. It introduces coupling enforcement to preserve class correspondence, noise augmentation to combat data sparsity and manifold collapse, and an early-stopping solver (ESS) that classifies from intermediate states to avoid late-stage drift. FMA is plug-and-play across CLIP and multiple PEFT backbones and shows consistent gains on 11 benchmarks, especially on difficult datasets, with ablations supporting each component."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The diagnosis of one-step PEFT limitations is convincing; the method is simple, modular, and effective across backbones; experiments are comprehensive; and the early-stopping insight is well-supported by empirical phenomena."}, "weaknesses": {"value": "Lack of formal guarantees for coupling assumptions, reliance on validation to set inference steps, missing comparisons with higher-order ODE solvers, coarse difficulty metric, and limited analysis of compute trade-offs and failure modes."}, "questions": {"value": "1. Can ESS be made adaptive without validation tuning (e.g., stop on sufficient logit margin, small velocity norm, or diminishing logit gains)?\n2. How is σ(xt) chosen in practice, and how sensitive are results to the noise magnitude/schedule? Any benefit from uncertainty- or density-adaptive noise?\n3. Did you try higher-order or adaptive ODE solvers (Heun/RK) to reduce truncation error and improve margins at the same step budget?\n4. Is fixed pairing too rigid for multi-modal classes? Would transporting toward multiple positive prototypes or a class subspace help?\n5. Have you considered adding a discriminative loss on intermediate states (e.g., contrastive/margin) to align transport with classification throughout the trajectory?\n6. Can you report detailed overhead (velocity net size, train/infer time, average ESS steps) and scaling with number of classes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "owGXCbJ5Ok", "forum": "ks6Gg8nd0y", "replyto": "ks6Gg8nd0y", "signatures": ["ICLR.cc/2026/Conference/Submission6884/Reviewer_CrjH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6884/Reviewer_CrjH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530208474, "cdate": 1761530208474, "tmdate": 1762919130883, "mdate": 1762919130883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of achieving precise cross-modal alignment in vision-language models (VLMs) for few-shot learning. The authors argue that existing parameter-efficient fine-tuning (PEFT) methods—such as prompt tuning, adapter-based, and LoRA-based approaches—perform only a \"one-step\" adjustment of features, which is insufficient for complex datasets where modalities are highly entangled. To overcome this limitation, the authors propose Flow Matching Alignment (FMA), a model-agnostic framework that leverages flow matching theory to enable multi-step feature transformation. FMA incorporates three key designs: coupling enforcement to preserve class correspondence, noise augmentation to mitigate data scarcity, and an early-stopping solver for efficient and accurate inference. Extensive experiments on 11 benchmarks show that FMA consistently improves performance, especially on challenging datasets, and integrates seamlessly with various backbones and PEFT methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel application of flow matching to cross-modal alignment in few-shot learning, moving beyond generative tasks.\n2. Effective design choices (e.g., early-stopping solver, noise augmentation) that address practical challenges in training and inference."}, "weaknesses": {"value": "1. No analysis of computational overhead or inference latency introduced by multi-step transformation.\n2. Ablation studies do not explore the sensitivity of performance to hyperparameters like inference steps.\n3. The early-stopping strategy uses a fixed step count rather than a sample-adaptive criterion, which may limit optimality."}, "questions": {"value": "1. Could the author provide more detailed information across different datasets in section 4.2 GENERALIZATION ABILITY, where only average performance was given?\n2. Was any exploration done into adaptive early-stopping criteria (e.g., based on feature discriminability) rather than a fixed stepsize?\n3. How does FMA perform in cross-modal retrieval or other downstream tasks beyond classification, given its alignment-focused design?\n4. Could the authors provide more intuition or theoretical insight into why coupling enforcement preserves class-level correspondence in high-dimensional feature spaces?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qD1wcMXWZF", "forum": "ks6Gg8nd0y", "replyto": "ks6Gg8nd0y", "signatures": ["ICLR.cc/2026/Conference/Submission6884/Reviewer_BSTW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6884/Reviewer_BSTW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809870789, "cdate": 1761809870789, "tmdate": 1762919130338, "mdate": 1762919130338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to improve alignment performance between modalities in cross-modal models.\nIt claims that existing methods fail to align well on challenging datasets because they attempt one-step alignment, \nand proposes a multi-step approach to align the embedding vectors of the two modalities.\nSpecifically, it performs flow matching to transform from images to the distribution of text embedding vectors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well-written and well-structured.\n- Alignment of embedding vectors across multi modalities is an important research topic.\n- Flow-matching alignment seems novel. However, its necessity is questionable, and it might be just a combination of new techniques.\n- In experiments, the proposed method outperforms baselines on class classification tasks. However, as written in Weaknesses, it is unclear whether the evaluation is well-designed to confirm the claims."}, "weaknesses": {"value": "- The motivation for multi-step adjustment is unclear. \nFirst, the definition of one-step adjustmentfor poor performance in existing methods is ambiguous.\nFor example, is the claim that PEFT's optimization objective function is inappropriate, or that optimization is insufficient due to difficult learning?\nFigure 2 discusses PEFT's characteristics compared to LP, but the validity of using LP as a baseline for this discussion is unclear. \nIt is unclear how this connects to the statement: “these methods try to adjust their general aligned multi-modal distribution towards the golden distribution by one rectification step.”\n\n- The experimental setups are insufficiently described, resulting in a lack of reproducibility. \nFor example, it states that velocity networks are learned, but I could not find a description of the specific structure of the velocity networks.\nThere is no definition of $\\sigma^2(\\cdot)$. There seems also no report of the number of steps M for the proposed method across each dataset.\nIn addition, there is no evaluation of statistical significance.\n\n- The baseline varies depending on the evaluation. \nWhile Table 1 compares against 8 baselines, Table 2 has one baseline and Table 3 has five.\nSpecifically, the baseline compared in Table 2 is one of the weaker baselines among those appearing in Table 1.\nAlthough there are practical limitations on the number of experiments, comparing against the strongest baseline yields more convincing results.\n\n- The proposed method seems computationally expensive. \nIt requires preparing velocity networks and performing multiple updates during inference (Algorithm 2).\nHow does the computational cost compare to the CLIP-Adapter with two linear layers? How does it compare to PEFT?\nSince the performance improvement over CLIP-LoRA is only 0–2%, the heavy inference cost makes the proposed method less useful.\n\n- Minor issues\n- The space is filled, making it difficult to read.\n The absence of a single line of space before and after figures and tables, such as the caption for Figure 4, violates the template."}, "questions": {"value": "- What is the definition of one-step adjustment? Does it mean that the objective function is set once or that the optimization is only one step? Are Fig. 1(b)-(d) optimal embeddings in some sense?\n\n- In Fig. 2, isn't it a bit simplistic to conclude that PEFT is weak on challenging datasets based on LP?\nCouldn't one also conclude that LP is strong on more challenging datasets?\n\n- What happens if stronger methods are used as baselines in Table 2? Also, did you check the standard deviation of the results and their statistical significance?\n\n- How about the comparison of computational cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kPVQuMvdXg", "forum": "ks6Gg8nd0y", "replyto": "ks6Gg8nd0y", "signatures": ["ICLR.cc/2026/Conference/Submission6884/Reviewer_2Lin"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6884/Reviewer_2Lin"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892197927, "cdate": 1761892197927, "tmdate": 1762919129906, "mdate": 1762919129906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}