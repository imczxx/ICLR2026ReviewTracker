{"id": "dtyTnqaKqn", "number": 5219, "cdate": 1757869114175, "mdate": 1759897987633, "content": {"title": "FocusDiff: Advancing Fine-Grained Text-Image Alignment for Autoregressive Visual Generation through RL", "abstract": "Recent studies extend the autoregression paradigm to text-to-image generation, achieving performance comparable to diffusion models. However, our new PairComp benchmark -- featuring test cases of paired prompts with similar syntax but different fine-grained semantics -- reveals that existing models struggle with fine-grained text-image alignment, thus failing to realize precise control over visual tokens. To address this, we propose FocusDiff, which enhances fine-grained text-image semantic alignment by focusing on subtle differences between similar text-image pairs. We construct a new dataset of paired texts and images with similar overall expressions but distinct local semantics, further introducing an improved GRPO-based algorithm to emphasize such fine-grained semantic differences for desired image generation. Our approach achieves superior performance on existing text-to-image benchmarks and also outperforms most prior prominent methods on PairComp. Anonymous Project: https://anonymous.4open.science/r/FocusDiff_Anonym-1F44.", "tldr": "", "keywords": ["text-to-image generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2141920b94a8bfb366453507f757aec5909da671.pdf", "supplementary_material": "/attachment/1f74100c27ad78c7ca114020fe6c29543855522b.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces FocusDiff, a method improving fine-grained text-to-image alignment. Using the new PairComp benchmark, it shows current models fail to capture subtle semantic differences. FocusDiff employs a paired text-image dataset and a refined GRPO-based algorithm to focus on nuanced variations, achieving better semantic precision and outperforming prior methods on standard and PairComp benchmarks. However, there are some typos in this paper, I hope the authors can fix them."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "[1]. The codes are provided in the supplementary materials, which helps researchers reproduce the results and makes the paper easier to understand.\n\n[2]. This paper introduces PairComp and evaluates paired prompts that differ only in subtle word-level semantics.\n\n[3]. The paper proposes Pair-GRPO, an extension of Group Relative Policy Optimization designed for fine-grained visual generation.\n\n[4]. The experiments are comprehensive and effectively demonstrate the validity of the proposed method.\n\n[5]. The paper have provided the webpage to help me understand."}, "weaknesses": {"value": "[1]. The paper lacks sufficient novelty. The proposed method, GRPO, has already been widely adopted and explored in the field of large language models.\n\n[2]. In 771 line, there is a typo. The phrase “with the pipeline shown in 8” appears to be incomplete or unclear.\n\n[3]. The quality of the generated results is not satisfactory. The images or outputs appear neither aesthetically pleasing nor natural, espically in Figure 5, the results looks fall behind the diffusion model. \n\n[4]. In Figure 3, the shape of the sign and car is changed. The authors should provide an explanation or analysis of this issue."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3POYFw4rC5", "forum": "dtyTnqaKqn", "replyto": "dtyTnqaKqn", "signatures": ["ICLR.cc/2026/Conference/Submission5219/Reviewer_YYr2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5219/Reviewer_YYr2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761314690649, "cdate": 1761314690649, "tmdate": 1762917956078, "mdate": 1762917956078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issue of fine-grained semantic control in autoregressive (AR) text-to-image models. To highlight this limitation, the authors introduce PairComp, a new benchmark consisting of paired prompts with subtle semantic differences. To solve the problem, they propose FocusDiff, a training paradigm that includes a new dataset of similar text-image pairs (FocusDiff-Data) and an enhanced reinforcement learning algorithm (Pair-GRPO) designed to learn from these differences. The method is applied to the Janus-Pro model, and experiments show improved performance on PairComp and other standard benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tClear Narrative: The paper presents a clear and logical narrative, effectively identifying a key limitation in AR models through a benchmark and then proposing a targeted solution comprising a new dataset and a RL training algorithm.\n2.\tComprehensive Experiments: The evaluation is thorough, demonstrating the method's effectiveness not only on the proposed PairComp benchmark but also on established general-purpose benchmarks, which validates the model's overall capabilities post-training.\n3.\tIn-depth Analysis: The work includes extensive analytical experiments. The rigorous evaluation design, such as using stricter metrics like the Full-Pass Rate (FPR) and employing multiple validation models, highlights the authors' workload."}, "weaknesses": {"value": "1.\t[Major] Insufficient Motivation and Positioning: The motivation for the PairComp benchmark feels insufficiently urgent. The challenge of fine-grained control is a known problem. The paper should acknowledge and differentiate its contributions from prior work like Winoground-T2I [1] and EvoGen [2] rather than explain its unique value beyond existing single-prompt benchmarks.\n2.\t[Major] Incremental Algorithmic Contribution: The novelty of Pair-GRPO over the standard GRPO appears limited. It seems that the prompt pairs are completely decoupled as RL inputs, and the advantage is calculated jointly. How does this approach differ from treating the prompt pairs separately as inputs? Does this experiment correspond to w/o Group Expansion? The description of the ablation experiment is too brief and unclear, making it difficult to see the specific ablation settings.\n3.\t[Major] Potential Fidelity-Control Trade-off: While the proposed model significantly improves fine-grained control and compositionality, a potential trade-off in image fidelity is observed in Figure 5. The generated images (e.g., the rabbit and bananas) seem to exhibit an over-smoothing effect, losing texture detail and realism compared to the Janus-Pro-7B baseline, which appears to have better fidelity.\n4.\t[Minor] (1) There are inconsistencies in the capitalization of key terms throughout the manuscript, such as \"PairComp\" versus \"Paircomp.\" (2) Acronyms are not always defined upon their first use. For instance, PPO is mentioned in Section 3.2 but its full name (Proximal Policy Optimization) is not provided.\n\n\nReferences:\n[1] Zhu X, Sun P, Wang C, et al. A contrastive compositional benchmark for text-to-image synthesis: A study with unified text-to-image fidelity metrics. arXiv preprint arXiv:2312.02338, 2023.\n[2] Han E X, Jin L, Liu X, et al. Progressive compositionality in text-to-image generative models. arXiv preprint arXiv:2410.16719, 2024."}, "questions": {"value": "1.\t[For W1] Could the authors elaborate on the unique necessity and contribution of the PairComp benchmark in light of existing compositional evaluation benchmarks such as Winoground-T2I and EvoGen?\n2.\t[For W2] Can you clarify the mechanism of joint advantage calculation in Pair-GRPO and how it differs from a baseline of processing the prompt pairs independently? Additionally, please provide more details on the setup of ablation.\n3.\t[For W3] The results in Figure 5 suggest a potential trade-off where improved semantic control comes at the cost of image fidelity (e.g., over-smoothing). Have the authors analyzed or experimented this phenomenon, and could they comment on this apparent trade-off?\n4.\t[For W4] We kindly ask the authors to review the manuscript for consistency in terminology and to ensure all acronyms are defined upon first use."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l9Fzo98Nbj", "forum": "dtyTnqaKqn", "replyto": "dtyTnqaKqn", "signatures": ["ICLR.cc/2026/Conference/Submission5219/Reviewer_uS6o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5219/Reviewer_uS6o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547503521, "cdate": 1761547503521, "tmdate": 1762917955683, "mdate": 1762917955683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of improving compositionality in autoregressive text-to-image generative models. The authors first introduce a contrastive dataset, where each example consists of two text–image pairs that differ subtly in a specific attribute or semantic aspect. Building on this idea, they also construct a benchmark designed to evaluate models’ sensitivity to fine-grained compositional differences between prompts. To enhance AR models, the paper further extends the GRPO optimization framework by proposing Pair-GRPO, which generalizes GRPO to operate on grouped pairs of prompts, leveraging the proposed contrastive dataset to improve compositional understanding during training."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles a highly important problem, improving compositionality in AR text-to-image generative models, which is crucial for achieving more reliable and interpretable generation.\n- A key contribution is the contrastive pair dataset construction: rather than synthetically generating contrastive pairs (as done in prior work, which often introduces inconsistencies), the authors leverage existing high-quality image editing datasets and apply additional filtering strategies to ensure semantic precision and data quality.\n- The paper presents experimental results, including both quantitative and qualitative evaluations that demonstrate the effectiveness of the proposed approach.\n- For the quantitative evaluation, they have covered multiple benchmarks that assess the compositional performance of T2I models."}, "weaknesses": {"value": "- The writing quality requires significant improvement. The paper often lacks clarity and coherence in some parts. For instance, in the introduction, the arithmetic and geometric evaluation metrics are introduced abruptly without proper explanation, which can confuse readers. In addition, several notation errors and inconsistencies are present (e.g., around lines 136 and 243), which further detract from readability.\n- The evaluation methodology is overly simplistic. The authors rely on a single VLM-based binary question (“Does this image match the prompt?”), which provides a limited assessment of compositional alignment. Prior works have adopted more fine-grained and structured evaluations, such as decomposing prompts into sub-questions to test specific compositional aspects, leading to more reliable measurements.\n- The contrastive pair construction approach, while effective, is not particularly novel. Similar ideas have been explored in earlier works such as [1], which also utilized contrastive data to enhance compositional learning.\n- The paper claims state-of-the-art performance on benchmarks like GenEval, but important baselines such as Flow-GRPO [2] are omitted, despite reporting significantly stronger results.\n- For the T2I-CompBench, it would be beneficial to include all categories (e.g., spatial, numeracy, ...) to provide a more comprehensive comparison.\n\n[1] Han, Evans Xu, et al. \"Progressive compositionality in text-to-image generative models.\" arXiv preprint arXiv:2410.16719 (2024).\n\n[2] Liu, Jie, et al. \"Flow-grpo: Training flow matching models via online rl.\" arXiv preprint arXiv:2505.05470 (2025)."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PZlylwawTO", "forum": "dtyTnqaKqn", "replyto": "dtyTnqaKqn", "signatures": ["ICLR.cc/2026/Conference/Submission5219/Reviewer_8cu7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5219/Reviewer_8cu7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800337649, "cdate": 1761800337649, "tmdate": 1762917955326, "mdate": 1762917955326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing autoregressive text-to-image models fail at fine-grained semantic control (e.g., color, position, counting). This paper introduces a new framework to address this weakness, namely:\n1. PairComp, a benchmark built on minimal pairs of prompts.\n2. A new metric, the geometric mean $s_g$, to penalize models that fail on one half of a pair.\n3. FocusDiff-Data, a 200k-sample dataset of contrastive text-image tuples $(I_1, T_1, I_2, T_2)$)created by applying a VLM to image editing datasets.\n4. Pair-GRPO, an RL algorithm adapted from GRPO (Shao et al., 2024) which expands the advantage-calculation group to include samples from both contrastive prompts $T_1$ and $T_2$ and uses a ground-truth-guided curriculum for stable training.\n5. Janus-FocusDiff, which shows gains over its backbone (Janus-Pro) on PairComp and other T2I benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The PairComp benchmark is a strong contribution. The design is a more direct and rigorous probe than existing single-prompt compositional benchmarks. The justification for the geometric mean ($s_g$) (l. 137) is sound and identifies an important failure mode (unstable generation).\n2. The pipeline for creating FocusDiff-Data (Sec 3.1, Appendix C) is innovative. Using image editing datasets as a source for contrastive pairs is a clever solution to the data-sourcing problem, and the resulting dataset is a valuable contribution.\n3. The adaptation of GRPO, an algorithm from the language reasoning domain, to the problem of autoregressive visual token generation is a creative and non-obvious transfer.\n4. The core ideas of Pair-GRPO are well-motivated. (1) Expanding the group to include contrastive samples (l. 241) is an intuitive way to force the policy to learn differentiation. (2) Likewise using the ground-truth pairs (including negative examples, e.g., $(T_1, \\hat{I}_2)$) as part of a training curriculum (l. 263) is a strong training signal."}, "weaknesses": {"value": "1. Conflating reward model + evaluator. The QA-based reward model for RL is InternVL2.5-26B (l. 820). The primary evaluation model for the PairComp benchmark is also InternVL2.5-26B (l. 124, 216). This is a confound. The RL policy is being optimized to maximize the score of the same model that is used to judge its performance. The gains reported in the main results (Table 1) could represent reward hacking or overfitting to the specific biases of the InternVL2.5-26B evaluator, rather than a true generalizable improvement in alignment.\nThe authors provide results with a held-out evaluator (Qwen2.5-VL-72B) in Appendix Table 6 (l. 919). These should be in the main paper as the primary evidence for the method’s success. \n2. Missing SFT-Only Baseline. The pipeline consists of two stages: (1) SFT on FocusDiff-Data (l. 194, 812), followed by (2) RL with Pair-GRPO (l. 195). The main comparison (e.g., in Table 1) is between the original Janus-Pro-7B (Row 1) and the final Janus-FocusDiff-7B (Row 2), which has had both SFT and RL. Where is the baseline with Janus-Pro-7B + SFT on FocusDiff-Data (but no RL stage)? Without this baseline, it is impossible to disentangle the gains from the SFT stage vs. the novel Pair-GRPO stage. The SFT on this highly-curated contrastive dataset might be responsible for most of the improvement, with the complex RL algorithm providing only marginal benefit. The paper's central claim is about the effectiveness of Pair-GRPO, but the experiments do not provide the necessary evidence to support this claim over the simpler alternative (SFT is sufficient).\n3. Justification for Pointwise vs. Pairwise Reward. The paper adopts a pointwise reward model (l. 821-824), where each (image, text) pair receives an absolute score $S(I, T)$. The GRPO objective (Eq. 1) then normalizes these absolute scores.\nTo my understanding, concurrent research (e.g., Pref-GRPO [2508.20751]) has identified that this pointwise score-maximization paradigm in T2I-RL is unstable and prone to \"illusory advantages\" (where tiny, meaningless score differences are amplified) and reward hacking.\nThese works propose pairwise preference rewards (i.e., \"is $I_a$ better than $I_b$ for $T$?\") as a more robust and stable signal that better aligns with human judgment. The paper cites other RL-for-T2I methods (T2I-R1, l. 876) but misses this work. It should justify why pointwise-reward GRPO is sufficient and not subject to the same instabilities that this other work identifies.\n4. Nit. Naming the paper “focusdiff” is confusing. The method is applied to an autoregressive model (Janus-Pro) and uses Reinforcement Learning (RL). The \"Diff\" suffix is associated with Diffusion models in the current ML literature, which this work is not. This will cause confusion."}, "questions": {"value": "1. Why were the main results (Table 1) reported using the same model for reward and evaluation?\n2. Can you provide the missing baseline: Janus-Pro-7B fine-tuned only on the FocusDiff-Data SFT set? So we can properly attribute the performance gains to the SFT stage vs. the Pair-GRPO stage.\n3. Can you justify your choice of a pointwise reward model for GRPO over a pairwise preference-based model (given the known instability issues of the former that have been addressed by concurrent work)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W62TMkaKmj", "forum": "dtyTnqaKqn", "replyto": "dtyTnqaKqn", "signatures": ["ICLR.cc/2026/Conference/Submission5219/Reviewer_TGyE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5219/Reviewer_TGyE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762193290405, "cdate": 1762193290405, "tmdate": 1762917954934, "mdate": 1762917954934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}