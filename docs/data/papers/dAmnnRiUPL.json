{"id": "dAmnnRiUPL", "number": 10750, "cdate": 1758180968285, "mdate": 1759897631777, "content": {"title": "DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing", "abstract": "A primary challenge in using neural networks to approximate nonlinear dynamical systems governed by partial differential equations (PDEs) is transforming these systems into a suitable format, especially when dealing with non-linearizable dynamics or the need for infinite-dimensional spaces for linearization. This paper introduces DyMixOp, a novel neural operator framework for PDEs that integrates insights from complex dynamical systems to address this challenge. Grounded in inertial manifold theory, DyMixOp transforms infinite-dimensional nonlinear PDE dynamics into a finite-dimensional latent space, establishing a structured foundation that maintains essential nonlinear interactions and enhances physical interpretability. A key innovation is the Local-Global-Mixing (LGM) transformation, inspired by convection dynamics in turbulence. This transformation effectively captures both fine-scale details and nonlinear interactions, while mitigating spectral bias commonly found in existing neural operators. The framework is further strengthened by a dynamics-informed architecture that connects multiple LGM layers to approximate linear and nonlinear dynamics, reflecting the temporal evolution of dynamical systems. Experimental results across diverse PDE benchmarks demonstrate that DyMixOp achieves state-of-the-art performance, significantly reducing prediction errors, particularly in convection-dominated scenarios reaching up to 86.7%, while maintaining computational efficiency and scalability.", "tldr": "DyMixOp is a neural operator that get insights from complex dynamics and uses local-global mixing to efficiently solve multi-scale nonlinear PDEs.", "keywords": ["Complex System", "Neural Operator", "Nonlinear Dynamics", "PDEs Solving", "Inertial Manifold"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b408d96997ca33dccc8c9f20cb85debabcb3fb3c.pdf", "supplementary_material": "/attachment/c8cb7d2b497a42902994b6c054d55bbbd2b07134.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes DyMixOp, a neural operator framework for PDE solving that integrates concepts from complex dynamical systems and inertial manifold theory. The authors aim to provide a “theory-guided” architecture for operator learning by reducing infinite-dimensional nonlinear PDE dynamics to finite-dimensional latent dynamics. The core architectural contribution is the Local-Global-Mixing (LGM) transformation, which combines localized integral operators and global (spectral) operators through element-wise multiplicative interactions. The authors claim this design is inspired by the convection term in fluid dynamics and enables modeling both local and global interactions in a unified manner. DyMixOp stacks multiple LGM layers in a “dynamics-informed” architecture, intended to reflect temporal evolution in dynamical systems. Empirical results on several PDE benchmarks (e.g., 1D KS, 2D Burgers, Darcy, 2D NS, 3D SW) suggest that DyMixOp achieves lower error compared to standard neural operator baselines, while maintaining modest computational cost and scalability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear high-level motivation: The paper identifies a real problem — the gap between heuristic neural operator architectures and the underlying structure of dynamical systems. This is a relevant and timely research direction.\n\n- Systematic architectural framing: The authors attempt to derive their model structure from inertial manifold theory, rather than relying purely on empirical heuristics. This is more principled than many recent operator-learning works.\n\n- Innovative LGM transformation: Combining local convolution-like operators with global spectral components in a multiplicative fashion is an interesting design choice, loosely motivated by physical convection mechanisms.\n\n- Broad experimental coverage: The paper evaluates DyMixOp across different PDE types (parabolic, elliptic, hyperbolic) and dimensions (1D, 2D, 3D). There are also ablation studies and scaling experiments, which are valuable for assessing model robustness.\n\n- Computational considerations: Some effort is made to analyze memory and training time scaling, which is often ignored in PDE learning papers."}, "weaknesses": {"value": "While the paper has clear ambitions, the actual contribution is significantly weaker than its presentation suggests:\n\n- Overstated novelty and vague theoretical link: The claim of “theory-guided” design is not rigorously substantiated. The connection to inertial manifold theory is superficial: the theory motivates a projection but does not dictate the actual LGM architecture. Similar “local + global” designs exist in Fourier Neural Operator, Wavelet neural operator: a neural operator for parametric partial differential equations, and other hybrid models. The multiplicative fusion is not a substantial conceptual leap.\n\n- No rigorous ablation on theoretical claims: The inertial manifold motivation is not tested empirically. There are no experiments isolating the effect of the projection, the impact of manifold dimensionality, or comparisons against standard dimensionality reduction (e.g., PCA or learned linear projections). It’s unclear whether performance gains come from the theoretical foundation or simply from better inductive bias and model capacity.\n\n- Limited methodological novelty: LGM transformation is just a multiplicative combination of a local and a global kernel — conceptually quite close to existing hybrid architectures. The “dynamics-informed architecture” amounts to a residual stack with learnable step sizes, which resembles well-known operator integration schemes.\n\n- Lack of clarity and rigor in analysis: The theoretical part is mostly descriptive, without formal guarantees connecting DyMixOp’s approximation power to the inertial manifold theory. Key details (e.g., effect of ∆t learnable steps, manifold dimensionality) are glossed over.\n\n- Unbalanced experiments: Most benchmarks are relatively standard and well-behaved. There is no evaluation on irregular domains, non-smooth dynamics, or truly stiff PDEs. Performance gains on simpler PDEs are marginal; large reported improvements mainly appear in convection-heavy cases, which may favor the architecture’s bias rather than indicating broad generality.\n\n- Overclaiming in narrative: Statements like “principled foundation” and “direct implementation of mathematical structure” are strong but not justified by the actual level of theoretical rigor or experimental support."}, "questions": {"value": "- Empirical grounding of theory: How exactly does the inertial manifold motivation improve model performance? Please show experiments isolating the projection and manifold dimensionality effects, and compare to naive PCA or linear projections.\n\n- Role of the LGM transformation: What happens if the multiplicative fusion is replaced with additive or concatenation-based fusion? Is the multiplicative interaction critical or just one of many equivalent options?\n\n- Scaling and generalization: How does the method perform on irregular meshes, complex geometries, or PDEs outside the training family? How robust is the architecture to resolution and parameter shifts?\n\n- Theoretical rigor: Can the authors provide formal approximation guarantees or bounds connecting the inertial manifold structure and the expressiveness of LGM layers? Otherwise, the claims of “principled design” are misleading.\n\n- Efficiency trade-offs: Provide explicit runtime and parameter count comparisons for each baseline and DyMixOp. Does the added theoretical structure introduce training overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XeldHTP2PG", "forum": "dAmnnRiUPL", "replyto": "dAmnnRiUPL", "signatures": ["ICLR.cc/2026/Conference/Submission10750/Reviewer_KBnD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10750/Reviewer_KBnD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760853873270, "cdate": 1760853873270, "tmdate": 1762921972524, "mdate": 1762921972524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DyMixOp, a neural operator framework for solving PDEs that integrates insights from complex dynamical systems theory. The core contribution they propose is grounded in inertial manifold theory, which enables the transformation of infinite-dimensional nonlinear PDE dynamics into finite-dimensional latent representations while preserving essential non-linear interactions. The authors propose a Local-Global-Mixing (LGM) transformation inspired by convection dynamics in turbulence, which captures both local fine-scale features and global nonlinear interactions through element-wise multiplication of local and global kernel outputs. Further they parameterize the architecture to resemble a forward in time evolution operator with learnable timestep sizes. The benchmarks carried out show advantage of this architecture over baselines on the test problems considered."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- A core strength of this paper is its grounding in dynamical systems theory. Instead of proposing a new architecture based purely on heuristics, the authors use inertial manifold theory to justify their model's core components with well motivated design choices. Further, their design on the LGM block based on a mix of local and global dynamics and seems to play out empirically as well with the obtained results on convection dominated problems.\n- Shown to demonstrate good performance beating established architectures such as the FNO on the chosen datasets."}, "weaknesses": {"value": "- The paper builds a strong theoretical case for using inertial manifold theory, which involves a global projection onto a low-mode basis. However, the actual implementation of the projection layer ($\\mathcal{P}_m$), the dimension-shifting layer ($\\mathcal{T}$) and the local component of the mixing blocks all use 1x1 convolutions. A 1x1 convolution would be a local, point-wise operator that mixes channels (effectively just a dense transform IIUC). The authors claim that this isn't necessary with larger kernel sizes being used for more performance-intensive applications but this doesn't seem to have been explored and seems contradictory.\n\n- The experiments report prediction errors, but the time horizon is not specified. For dynamical systems, the most critical metric is often long-term stability and error accumulation in autoregressive rollouts. I would expect that with an architecture motivated by the inertial manifold, the learned dynamics should be consistent even through the rollout.\n\n- Further, the claim is that this architecture alleviates the spectral bias inherent to neural operators isn't well evidenced. I would like to see comparisons depicting that this architecture shows lower error in the higher bucket of frequencies as compared to the standard architecture.\n\n- I'm not entirely convinced that these benchmarks are robust enough, and compare against the current advancements in neural operator design. Can you perhaps adopt a few more standard benchmarks that have established results such as the PDEArena benchmarks / TheWell dataset?"}, "questions": {"value": "Please address the weaknesses. Additionally:\n\n- Could you please clarify what is meant by the \"combined parallel-hierarchical structure\" or \"hybrid variant\"? From my understanding, the architecture is purely sequential.\n- Can you clarify on the choice of your hyperparameters? I see that scaling behaviour increases both the width and the layer count. Given that there's a low manifold dimension that we're hoping to uncover, shouldn't we see a saturation in the performance as we increase the width? Have you looked into this?\n- Are all of the benchmarks performed acquired from somewhere or generated? I can see there's citation for 2D Darcy's and 3D SW but not for the others. As mentioned in the weaknesses, can you perhaps do a comparison to established benchmarks such as PDEArena or TheWell?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NH3jVu0L91", "forum": "dAmnnRiUPL", "replyto": "dAmnnRiUPL", "signatures": ["ICLR.cc/2026/Conference/Submission10750/Reviewer_zWnC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10750/Reviewer_zWnC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576292106, "cdate": 1761576292106, "tmdate": 1762921971949, "mdate": 1762921971949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DyMixOp, a neural operator framework guided by inertial manifold theory and complex dynamical systems to learn PDE dynamics efficiently. The method combines local and global transformations through a Local-Global-Mixing (LGM) mechanism, designed to capture both fine-scale and domain-wide interactions."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1) Derivation from inertial manifold theory provides a rare, principled approach to neural operator design.\n\n2) The proposed Local-Global-Mixing transformation effectively combines spectral and convolutional representations."}, "weaknesses": {"value": "1) Architectural implementation details are dense and difficult to follow, and may be difficult to reproduce as the codebase is also not provided.\n\n2) Does not benchmark against some recent operator designs (e.g., Latent Mamba Operator, Transolver, GeoFNO).\n\n3) No systematic exploration of how latent space size affects accuracy or stability.\n\n4) While qualitative results on efficiency are provided, quantitative analysis of FLOPs or runtime scaling is missing.\n\n5) The treatment of irregular geometries or mixed boundary conditions remains somewhat unclear."}, "questions": {"value": "1) How sensitive is the performance of DyMixOp to the chosen latent dimension or projection rank?\n\n2) Can the Local-Global-Mixing transformation be adapted to irregular meshes or graph-based domains?\n\n3) How does the framework handle non-smooth or discontinuous boundaries—is the inertial manifold assumption still valid there?\n\n4) Could DyMixOp be extended to time-dependent or multi-physics systems, and how would the architecture scale in such cases?\n\n5) Have the authors analyzed or visualized the learned spectral kernels to validate the claimed physical interpretability?\n\n6) No codebase and hyperparameter details are provided."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "Dual Submission (https://arxiv.org/pdf/2508.13490). Paper submitted to AAAI 26 as well, simultaneously."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "G0FzdxObtm", "forum": "dAmnnRiUPL", "replyto": "dAmnnRiUPL", "signatures": ["ICLR.cc/2026/Conference/Submission10750/Reviewer_3Beo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10750/Reviewer_3Beo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848500173, "cdate": 1761848500173, "tmdate": 1762921971555, "mdate": 1762921971555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dymixop, a neural operator framework inspired by inertial manifold theory from complex dynamical systems. The key idea is the Local-global-mixing transformation, combining the local and global operations via element-wise multiplication. The framework transforms infinite dim PDE dynamics into finite-dimensional ones while preserving nonlinear interactions. Experiments show the efficacy of the method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed architecture is grounded in inertial manifold theory. \n\n2. The LGM transformation is conceptually elegant and well motivated. \n\n3. Experiments are fairly comprehensive. \n\n4. Results are good both from performance and efficiency angles."}, "weaknesses": {"value": "1. The connection between the inertial manifold theory and actual implementation is loose. Theory assumes specific decompositions and projections but implementation uses learnable convolutions that does not relate to p_m projections in theory. \n\n2. I think the novelty is overstated, given that elementwise multiplication of local and global features isn't new. \n\n3. Many recent baselines are missing - Transolver, UNO etc, FNO is a little dated. \n\n4. A lot of design choices are unclear. \n\n5. Notational inconsistencies exist throughout the paper. \n\n6. There are no resolution interference, OOD generalization, and computational complexity experiments/analysis."}, "questions": {"value": "1. Can it be proved that learnable 1x1 convolutions and P_m actually implement the theoretical projections from the manifold theory? \n\n2. Are there any guarantees coming from theory for the learned representations?\n\n3. How does the method work when inertial manifolds don't exist for a given PDE?\n\n4. The claim about natural decomposition into local and global features needs to be formalized. \n\n5. Can you provide justifications for the hyperparameter choices made?\n\n6. Can the model handle different spatial resolutions at training and test times?\n\n7. The consistency loss term in Eq. 15 seems like an Auto-encoding loss. How is this specific to the given theory?\n\n9. What are the failure cases of the proposed formulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hJQzKyrxfr", "forum": "dAmnnRiUPL", "replyto": "dAmnnRiUPL", "signatures": ["ICLR.cc/2026/Conference/Submission10750/Reviewer_hWWt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10750/Reviewer_hWWt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992458927, "cdate": 1761992458927, "tmdate": 1762921971134, "mdate": 1762921971134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}