{"id": "8vL9nw4o9s", "number": 20784, "cdate": 1758310075121, "mdate": 1759896958796, "content": {"title": "Robust Transfer for Bayesian Optimization with Multi-Task Prior-Fitted Networks", "abstract": "Bayesian optimization (BO) is a sample-efficient optimization technique for black-box optimization, and using transfer learning to leverage historical information from related tasks can greatly improve its performance. Multi-task Gaussian processes are commonly used to transfer knowledge from source tasks to target tasks, but these models often make strong assumptions about the relationships between tasks and thus suffer from negative transfer and degraded predictive performance when these assumptions are violated. In this paper, we present Multi-Task Prior-Data Fitted Networks (MTPFNs), a flexible surrogate model that emulates Bayesian inference over user-specified priors over the relationship between tasks. We also propose a novel data-generation procedure specifically designed for the Bayesian optimization transfer setting which enables MTPFNs to be robust to negative transfer and efficiently leverage relevant information.\nAcross a variety of synthetic and real-world benchmarks including hyperparameter optimization, we demonstrate that MTPFNs successfully transfer knowledge in challenging scenarios where existing multi-task Gaussian processes struggle, outperforming existing robust transfer learning methods for Bayesian optimization.", "tldr": "", "keywords": ["Bayesian optimization", "transfer learning"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a4c6bba2fe52c47365d04fa6403bc3b12aee274.pdf", "supplementary_material": "/attachment/4a52d59e930af8e73f8ee1fc4ef3358a7da6f22e.zip"}, "replies": [{"content": {"summary": {"value": "The article introduces a multi-task prior-data fitted network in order to share information across tasks in multi-task Bayesian optimisation. It is deployed and, unsurprisingly, performs better than the baselines on the tasks that are presented."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Multi-task Bayesian optimisation is hard, largely due to the standard Gaussian process frameworks either failing, or being too data hungry to learn the between-task correlations (and then too slow when sufficient data is presented). I think that this prior-data fitted network approach is potentially valuable and useful. Performance in the selected tasks seems to be good."}, "weaknesses": {"value": "The description of the actual contribution, the MT-PFN, is very loose. We are given a picture (Fig 2) and very little else. This is the key contribution, and I really wanted to understand it.\nThe article talks about explicitly accommodating a probability of a task being irrelevant, but I could not work out how this is encoded in the MT-PFN.\nThere are better methods for testing whether a multi-task model fits multi-task data than slamming it into BO and measuring optimisation performance. Really, this isn't a BO article at all. I would have loved to see some performance metrics for how will the MT-PFN fits data."}, "questions": {"value": "How does MT-PFN actually work? What is the construction, and why does it make sense?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W06Ytvhpoi", "forum": "8vL9nw4o9s", "replyto": "8vL9nw4o9s", "signatures": ["ICLR.cc/2026/Conference/Submission20784/Reviewer_dMBL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20784/Reviewer_dMBL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761305072249, "cdate": 1761305072249, "tmdate": 1762934857474, "mdate": 1762934857474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a multi-task surrogate model (MTPFN) for bayesian optimization by jointly modeling multi-task data through in-context learning. The proposed MTPFN model is a sophisticated integration of several advanced concepts: prior-data fitted networks (PFNs), in-context learning, and hierarchical attention mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tRobustness: Introduces a robust multi-task surrogate model that mitigates negative transfer.\n-\tSignificance: It combines the flexibility of transformer-based models with the robustness needed for real-world multi-task learning.\n-\tPerformance: Demonstrates promising empirical performance across synthetic and real-world benchmarks.\n-\tScalability: The hierarchical attention mechanism allows it to handle more tasks and data points than cubic-scaling GP-based joint models."}, "weaknesses": {"value": "-\tIncremental Technical Novelty: The core ideas (e.g., hierarchical attention, prior-data fitted network) build heavily on existing literature, making the overall contribution incremental.\n-\tUnclear Methodological Clarity: The method section lacks a clear illustration of its methodological details (e.g., the problem formulation, the overall procedure of the MTPFN method, and the exact network architecture). \n-\tComputational Cost of Training: Training the transformer on 50 million synthetic datasets is incredibly computationally intensive.\n-\tDependence on Synthetic Prior: Performance is contingent on the quality of the data generation process. A poorly specified prior could lead to suboptimal performance on real-world tasks."}, "questions": {"value": "-\tPotential Limitation: What are the limitations of the proposed Multi-Task Prior-Data Fitted Network?\n-\tAblation Study: A more explicit ablation study quantifying the individual contribution of the hierarchical attention versus the robust DGP would be valuable.\n-\tHyperparamenter Sensitivity: How sensitive is the model's performance to the specific hyperparameters of the DGP (e.g., the Gamma priors for the lengthscale)?\n-\tSmall Synthetic Dataset: Could the model be effectively trained on a smaller number of synthetic datasets, or are the 50 million samples critical for stable performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QYae9vmOFk", "forum": "8vL9nw4o9s", "replyto": "8vL9nw4o9s", "signatures": ["ICLR.cc/2026/Conference/Submission20784/Reviewer_gndA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20784/Reviewer_gndA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883740592, "cdate": 1761883740592, "tmdate": 1762934819646, "mdate": 1762934819646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of Bayesian optimization in transfer learning setting using Prior Fitted Networks. The authors present a surrogate model called Multi-Task Prior-Data Fitted Networks that emulates Bayesian Inference making it effective to optimize for a new test function by exploiting the relationship between training and test tasks through heirarchical attention mechanism. They also propose a novel data-generation process for training the model that makes it robust against negative transfer. Finally, they also present synthetic and real world experiments to showcase the effectiveness of their method over baselines like MTGP in transfer learning BO domains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an interesting method for transfer learning in Bayesian Optimization setting using Multitask Prior Fitted Networks.\n2. Their model MTPFN is able to adapt test function through incontext samples without requiring retraining which makes it really efficient in practice.\n3. The hierarchical attention mechanism introduced in the paper is very interesting as it reduces the computational complexity allowing it to scale efficiently over larger number of tasks and data points.\n4. The experiment results demonstrates the effectiveness of MTPFNs to robustly model inter-task relationships and avoid negative transfers."}, "weaknesses": {"value": "1. The paper states the experimental observation that MTPFNs are robust against negative transfers but does not provide much insight into why and when does that happen. \n2. Some of the details in the paper appear too condensed. For example, Hierarchical Attention Mechanism discussed in line 240-262 is very dense and it would be really helpful to add sufficient details to make it more comprehensible to readers.\n3. The entire MTPFN model is trained to \"emulate\" the posterior of a specific synthetic Data Generation Process (DGP). If real-world tasks and their relationships do not resemble this mixture prior, the model's performance may degrade significantly. \n4. The paper does not address scalability wrt input dimensionality of $\\mathcal{X}$."}, "questions": {"value": "1. Section 4.1. mentions MTPFNs are robust against negative transfers but it lacks sufficient explanation or intuition to support the claim. Can you please clarify on that? Is this a totally empirical observations and if so why do you think this would generalize to other examples?\n2. The paper demonstrates that a certain value of $p$ improves robustness. But how sensitive is the model's inference-time robustness to this parameter? For instance, if the model is trained with $p=0.2$ but deployed in a scenario with 90% unrelated tasks, will it still perform well, or does this $p$ value need to be \"tuned\"?\n3. In section 4.4, the authors make comparison to fine-tuning approaches and observe that they are brittle. Did they try comparing to more modern PEFT methods, such as LoRA? Since, these methods are designed to adapt to new task without destroying prior knowledge, I suspect they may present a much stronger baseline against MTPFN's in-context approach which would be an interesting comparison to have."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RFeuyyNvJC", "forum": "8vL9nw4o9s", "replyto": "8vL9nw4o9s", "signatures": ["ICLR.cc/2026/Conference/Submission20784/Reviewer_f9Ye"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20784/Reviewer_f9Ye"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762391663099, "cdate": 1762391663099, "tmdate": 1762934765871, "mdate": 1762934765871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces multi-task prior-data fitted networks (MTPFNs) for the problem of Bayesian optimization (BO) from a transfer learning perspective. In particular, the work raises concerns about how the intrinsic corregionalization model (ICM) usually considered for the mixing structure of multi-output GPs may struggle when there is a negative influence between output variables or transfer undesired modelling properties are transferred. From the perspective of the authors, an PFN with a categorical encoding of tasks is more robust than ICM and the usual multi-task GP that practitioners. The paper dedicates a considerable amount of text and some empirical results to prove this hypothesis and to show that MTPFNs are superior for these cases. Finally, the proposed method is tested on a couple of hyperparameter-fitting benchmarks, summarised in Figure 6 (pp. 9) of the manuscript."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- One of the main hypotheses of the paper, which is repeated several times throughout the manuscript, is that multi-task Gaussian processes make strong assumptions about the relationship between tasks, which sometimes degrades performance when such assumptions are violated. Interestingly, I agree with this hypothesis --- and I consider it valid; however we should also mention that there is always a trade-off between \"flexibility\" and \"optimization\". It is obvious that GPs, with their usual linear structure, impose certain strong assumptions that NNs don't. But we must also be aware that such introduction of NNs is not always beneficial, particularly in small data uncertainty-based problems. I do think more or less everyone in the community agrees on that.\n- The work is thorough and has a good scientific spirit in the sense of explaining what the motivations are behind introducing MTPFNs and what they are competing with. In general, it is well written and perfectly clear, no doubts on that.\n- I miss perhaps some broader perspective on the BO's SOTA methods and baselines, like for instance going deeper into that literature and motivating better the problem of multi-task models for BO. Still, I do think the paper did a good job on including critical references and overall on those around PFNs and ICM. The empirical results and the testing on the BO hyperparameter benchmark are sufficient, but not as complete as one might expect for this sort of venue and submission."}, "weaknesses": {"value": "- To me, the central concerns and problems faced in the paper revolve around the \"negative influence/transfer\" of some tasks while performing inference. There are a lot of claims about the impossibility of MTGPs and ICM to address this problem, while the categorical encoding of tasks in the PFN can do that. In this long discussion, I miss a lot of the math, honestly. The ICM linear model is barely mentioned, and its modelling choices are not shown in the form of an equation. With this, even for someone with some experience with MOGPs + ICM, LCM, or convolutions, it is difficult to follow some claims made in the paper. I would be happy to engage in the discussion with the authors, because I really want to understand where this hypothesis comes from and know more about it.\n- Going a bit deeper into the technical details and these limitations of the ICM model, I do think that the claims made from the performance shown in Figure 1 are missing numbers, like predictive posterior on test data etc. Otherwise is difficult to judge visually. It is interesting that authors are somehow indicating that the ICM is not able to reduce the contribution of the (periodic and unrelated) task; let's say that the mixing linear coefficient should be near zero. But I am not entirely convinced that the MTPFN is superior here. The red task has a clear periodic structure that the MTPFN is not using in a horrible way for the performance; it is just showing uncertainty, who knows which of the lengthscales influenced by the green tasks. Additionally, the imbalance in the number of observations between tasks affects somehow, so more empirical tests should be considered for better comprehension.\n- Beating ICM and MTGP final results should at least be shown, and we should also have (as readers/reviewers) some notion of what the computational bill of doing this transition. BO is particularly well known for being applied to\n- heavy problems, but additional evidence on time consumption, parameters, etc, should be included. I am aware of the notions provided in Section 3.2.1, but they are not very general to understand the final procedure.\n- Last but not least, I am somehow concerned that the focus of the paper is BO, but it's empirical validation is limited and somehow the general BO mission does not shine a lot in the manuscript, which gives the work an odd communication thread."}, "questions": {"value": "- Section 3.2 is a bit difficult to understand, in the sense that two techniques are introduced without much context (for someone very familiar with PFNs maybe is easier) and later the hierarchical attention mechanism. From my point of view I can see that they are different versions of the model and they improve it on different limitations. But later for Section 4 is not clear to me what is considered, what is not, how they related to the benchmarks and the data-based problem faced. I would like to hear more on that.\n- This is a very-minor point, but I do think a lot of works and interesting references could also be added to guide the reader through the BO problem. Garnett (2023) as a summary of Bayesian optimization is a bit limited to me. I'm pretty sure that a lot of cool contributions came out before that book ;))"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AEgBOloD1p", "forum": "8vL9nw4o9s", "replyto": "8vL9nw4o9s", "signatures": ["ICLR.cc/2026/Conference/Submission20784/Reviewer_yMBa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20784/Reviewer_yMBa"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762460524614, "cdate": 1762460524614, "tmdate": 1762934744763, "mdate": 1762934744763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}