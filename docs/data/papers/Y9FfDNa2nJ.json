{"id": "Y9FfDNa2nJ", "number": 19830, "cdate": 1758299809123, "mdate": 1759897017120, "content": {"title": "Understanding the Role of Training Data in Test-Time Scaling", "abstract": "Test-time scaling improves the reasoning capabilities of large language models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts (CoTs). This enables models to  tackle more complex problem by breaking them down into additional steps, backtracking, and correcting mistakes. Despite its strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions in the training data under which long CoTs emerge, and when such long CoTs improve the performance, remain unclear. In this paper, we study the performance of test-time scaling for transformers trained on an in-context weight prediction task for linear regression. Our analysis provides a theoretical explanation for several intriguing observations: First,  at any fixed test error, increasing test-time compute allows us to reduce the number of in-context examples (context length) in training prompts. Second, if the skills required to solve a downstream task are not sufficiently present in the training data, increasing test-time compute can harm performance. Finally, we characterize task hardness via the smallest eigenvalue of its feature covariance matrix and show that training on a diverse, relevant, and hard set of tasks results in best performance for test-time scaling. We confirm our findings with experiments on large, nonlinear transformer architectures.", "tldr": "Theoretical framework to study test-time scaling, Chains-of-thought reasoning, and task selection for training in the setting of in-context learning with linear self attention.", "keywords": ["Language models", "Learning theory", "Chains-of-Thought", "Inference compute", "Test error"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/150c82e5e7538e99d0727eedcee73e18a6414ab9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the relationship between test-time scaling and training data in the context of in-context weight prediction for linear regression. The authors develop a theoretical analysis based on linear self-attention (LSA), derive convergence properties, propose a definition of task hardness, and formulate an optimization framework for task selection. Experiments are conducted with both LSA and GPT-2 architectures to validate the theoretical trends."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Provides a systematic theoretical analysis with clear derivations of convergence and error bounds.\n2. Introduces a task hardness measure based on the covariance matrix, which is simple and interpretable.\n3. Offers a formal perspective on task selection, framed as an optimization problem.\n4. Addresses the timely topic of test-time scaling, which has attracted significant attention in the community."}, "weaknesses": {"value": "1. Overly idealized setting: The core analysis relies on linear regression and LSA, which are far from realistic large-scale model training.\n2. Limited GPT-2 validation: Although GPT-2 experiments are included, they remain confined to the same synthetic weight prediction task, limiting external validity.\n3. Simplistic hardness definition: The hardness measure depends only on the smallest eigenvalue, reflecting the limitations of the simplified setting and failing to capture the complexity of real tasks.\n4. Insufficient experimental support: The experiments are small-scale and restricted to synthetic tasks, without evidence from realistic reasoning benchmarks."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a2TN9sdoOw", "forum": "Y9FfDNa2nJ", "replyto": "Y9FfDNa2nJ", "signatures": ["ICLR.cc/2026/Conference/Submission19830/Reviewer_suZT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19830/Reviewer_suZT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900438824, "cdate": 1761900438824, "tmdate": 1762932007270, "mdate": 1762932007270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a theoretical lens for studying how training data influences test-time scaling. They propose a simple linear regression prediction task in which the model repeatedly predicts the weight vector given X,y pairs via chain of thought. They demonstrate that increasing test-time compute reduces the in-context examples requirement during training. They also characterize factors that influence the test-time scaling curve, such as training data hardness and diversity of training data."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This is a well-written paper. I find the intuitions on task hardness, task selection, and diversity to be particularly helpful in understanding the provided theorems.\n* The definitions of task hardness (as a ratio of the sum of variances to minimum eigenvalue) is particularly interesting and to my knowledge quite novel. As the paper states, \"an easy task is one that relies on a few dominant skills... while a hard task draws on many skills, reflected in a long-tailed spectrum\". In the context of LLMs, task difficulty is usually defined as length/complexity of a problem. The proposed definition provides a new axis of task complexity that relates feature conditioning and data geometry.\n* Although the primary focus of the paper is to understand how training data affects test-time scaling curves, the framework naturally explains when overthinking occurs. This interpretability demonstrates the elegance and generality of the theoretical analysis.\n\nOverall, the paper advances our mechanistic understanding of why test-time reasoning improves model performance, and has important consequences for designing the appropriate training data for language models."}, "weaknesses": {"value": "The analysis is mostly confined to the synthetic linear regression task setup. It is not immediately clear how well these definition of task hardness translate to natural language reasoning tasks."}, "questions": {"value": "The theory predicts that the generalization error decays (roughly) as $\\frac{1}{n^{2k}}$ - does this scaling hold even for extremely small $n$? In practice, if the model encounters too few examples during training, there could be an error floor that test-time scaling cannot overcome. Is there a theoretical or empirical lower bound on $n$ below which test-time scaling becomes ineffective or unstable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mqkbJ1PjnX", "forum": "Y9FfDNa2nJ", "replyto": "Y9FfDNa2nJ", "signatures": ["ICLR.cc/2026/Conference/Submission19830/Reviewer_NuKS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19830/Reviewer_NuKS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958538134, "cdate": 1761958538134, "tmdate": 1762932006745, "mdate": 1762932006745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses the topic of test-time scaling and chain-of-thought prompting for transformers, examining when and why allocating extra compute at inference improves performance.\nMotivated by recent systems (e.g., OpenAI’s o1 and DeepSeek R1), the authors analyze transformers trained for in-context weight prediction on linear regression to clarify the training-data conditions under which long chains of thought emerge and help.\nThey show theoretically that, for a fixed test error, increasing test-time compute can substitute for longer training prompt context length.\nThey also find that if the relevant skills are insufficiently represented in the training data, additional test-time compute may harm performance.\nThe work formalizes task hardness via the smallest eigenvalue of the feature covariance matrix and argues that training on diverse, relevant, and hard tasks yields the best gains from test-time scaling.\nThe analysis connects chain-of-thought prompting to multi-step (pseudo)-Newton’s method and derives scaling laws governing the interaction among test-time compute, context length, and task diversity.\nAn optimal task-selection strategy for multi-task training is proposed and validated on linear self-attention models and GPT-2, with experiments extending to large, nonlinear transformer architectures.\nThe authors note limitations: the theory focuses on linear regression and single-layer linear self-attention, and future work should extend to nonlinear data generation and transformers with nonlinear activations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* A coherent framework linking theory and experiments  \nThe paper examines the effectiveness of test-time scaling (longer Chains of Thought) with a back-and-forth between theory (analytical results and scaling laws) and empirical validation (linear self-attention and GPT-2 / larger nonlinear transformers), which strengthens the credibility of its claims.\n\n* Mechanistic identification: CoT as multi-step optimization (pseudo-Newton’s method)  \nBy mapping the inference process to iterative optimization, the paper makes why CoT works more transparent; this unifies prior empirical observations and informs design choices (e.g., preconditioning and stopping criteria).\n\n* A theoretical trade-off between test-time compute and training-time context length  \nUnder a fixed test error, the paper shows that increasing test-time compute can substitute for a longer training context length, providing a principled basis for allocating budget between training and inference.\n\n* A principled metric of task hardness via the smallest eigenvalue of the feature covariance  \nThe work formalizes “hardness” spectrally, enabling computable diagnostics that connect to scaling laws, dataset construction, and benchmark selection.\n\n* Clear conditions for when overthinking can be harmful  \nThe paper specifies that when the necessary skill directions are underrepresented in the training data, increasing test-time compute can degrade performance, offering actionable guidance for safe deployment and monitoring.\n\n* Scaling laws that provide predictability  \nBy deriving scaling relationships among test-time compute, context length, and task diversity (via the features’ covariance spectrum), the paper moves beyond observation to testable predictions that facilitate replication and extension."}, "weaknesses": {"value": "* Narrow applicability of the theory  \nI understand that the authors explicitly state the limitations (linear regression; single-layer linear self-attention) and future extensions (nonlinear data generation; transformers with nonlinear activations) in the Conclusion and Limitations sections.\nHowever, the main claims (CoT ≈ pseudo-Newton’s method, scaling laws, etc.) are not guaranteed to carry over to recent advances in LLMs, such as different model architectures, like LLaMA-base, GPT-OSS-base, and MoE-based models.\nThis is extremely important for the contribution of this paper. It would be better to discuss , as part of the main content, how the proposed method is promising for extension to different model configurations, even if confidence is currently limited.\n\n* Dependence on the task “hardness” metric  \nIn Sec. 3, the paper defines hardness as $\\mbox{tr} (\\Lambda) / \\lambda_{min}(\\Lambda)$ for the task covariance $\\Lambda$, and claims that hardness is based on the smallest eigenvalue of the feature covariance.\nHowever, it seems that relying on a single spectral indicator may be sensitive to preprocessing or representation choices and may not fully capture linguistic complexity or reasoning modes.\n\n* Detectability of the conditions under which overthinking is harmful  \nIn Sec.5, the paper reports observations in which longer CoT hurts.\nWhile the condition is described, a practical method to detect it in advance (a deployable diagnostic or proxy) is not specified.\nIf my understanding is correct, this indicates that there is still no way to predict this in advance.\n\n\n* Generality, scale, and diversity of experiments  \nThe experiments are conducted only on LSA and GPT-2 (12 layers, 8 heads, ~9.5M parameters).\nIt is insightful, but generalization to current large-scale systems and broader task suites (code, long-form reading, knowledge-intensive tasks) remains limited."}, "questions": {"value": "* It remains difficult to separate gains due to the proposed (pseudo-Newton) mechanism from those due to decoding strategies or exploration effects. Is there anything that the authors can discuss on this point?\n\n* Prompt sensitivity / implementation sensitivity\nIf small implementation choices can flip conclusions, reproducibility is weakened.\nIs there anything that the authors can add information on this point?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vZtWAEWxQh", "forum": "Y9FfDNa2nJ", "replyto": "Y9FfDNa2nJ", "signatures": ["ICLR.cc/2026/Conference/Submission19830/Reviewer_8CQr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19830/Reviewer_8CQr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995672895, "cdate": 1761995672895, "tmdate": 1762932006367, "mdate": 1762932006367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies test‑time scaling in a stylized in‑context learning setting. The core technical model is a one‑layer linear self‑attention transformer trained on an in‑context weight prediction task for linear regression with Gaussian features. They find that CoT induces an iterative update that they interpret as a multi‑step pseudo‑Newton method. They define a task hardness measure, derive error bounds that decay with the CoT depth, and argue that more test‑time compute can compensate for shorter training prompts, and that insufficient skill coverage in training can make longer CoTs harmful."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper addresses important questions such as whether more inference compute always helps, whether it can trade off against training context length, and what counts as difficult training data.\n\nProposition 3.2 derives the iterative update which makes the connection between CoT and a preconditioned iterative method precise and inspectable.\n\nThe proposed hardness measure is scale‑invariant, emphasizes tail eigen‑mass, and comes with a narrative mapping eigenvectors to skills and eigenvalues to skill strength, which leads to a clear understanding that harder tasks need longer CoT to reach the same error."}, "weaknesses": {"value": "Experiments initialize with the closed‑form optimum using population statistics, so they basically hard‑code the solution instead of learning it, which invalidates empirical support for the learning‑dynamics claims.\n\nAll experiments are on synthetic linear tasks, there is no comparison to closed‑form ridge/OLS, no ablation on preconditioner estimation error, no evaluation on real reasoning benchmarks, and no demonstration that the proposed task‑selection improves anything beyond plotting selection weights."}, "questions": {"value": "What is the test‑prompt length in Figures 2?\n\nWhat is the sample complexity to estimate a usable covariance from a small validation set, and how robust is the solution to estimation noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pfUNIqpnix", "forum": "Y9FfDNa2nJ", "replyto": "Y9FfDNa2nJ", "signatures": ["ICLR.cc/2026/Conference/Submission19830/Reviewer_cDdC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19830/Reviewer_cDdC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179085543, "cdate": 1762179085543, "tmdate": 1762932005856, "mdate": 1762932005856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}