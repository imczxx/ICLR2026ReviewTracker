{"id": "dOxOX4GF1k", "number": 12929, "cdate": 1758211802266, "mdate": 1759897475934, "content": {"title": "The Robustness-Security Paradox: Channel-Aware Feature Learning for Adversarial Watermark Exploitation", "abstract": "Watermarking is crucial for establishing provenance and detecting AI-generated content. While current approaches prioritize robustness against real-world distortions, we explore how the robustness-security tradeoff manifests in deep learning-based watermarks: robust watermarks necessarily increase the redundancy of detectable watermark patterns embedded in images, creating exploitable information leakage. Leveraging this insight, we introduce an attack framework that extracts watermark pattern leakage through multi-channel feature learning using pre-trained vision models. Unlike previous approaches that require extensive data or detector access, our method achieves both watermark removal (detection evasion) and watermark forgery attacks with just a single watermarked image in a no-box setting. Extensive experiments demonstrate our method outperforms state-of-the-art techniques by 74\\% in detection evasion rate and 47\\% in forgery accuracy, while preserving visual quality.", "tldr": "", "keywords": ["Image Watermarking", "Watermarking Security", "AI-generated content detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c82a7c3c99babac7fad6f011bf5c3aef2694997a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a watermark removal and a watermark forgery attack against post-processing watermarking methods. Their method is based on the key observation that the watermarking signal embedded by these watermarking methods is usually constrained to a few feature channels. By identifying these channels through clustering, they can optimize noise that, when added to the image, either mitigates the watermark signal (for removal) or replicates it (for forgery). They evaluate their method against five post-processing watermarking methods and find overall improvements in success rate and visual quality degradation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-motivated and well-written.\n2. Solid methodology and approach.\n3. Good experimentation section"}, "weaknesses": {"value": "1. Overreaching claims. You only consider post-processing watermarking in your study, as stated in Section 3. However, throughout the paper, you make claims that your results apply to ALL watermarking methods (which would include in-processing watermarks). Examples of general claims for which you only give evidence for post-processing watermarks: line 215, lines 249-251 (the paradox could be limited to post-processing watermarks), line 395 (“representative watermark algorithms”, not true, it would be representative post-processing watermark algorithms”; “universally” overreaching, you did not prove that and only gave evidence for post-processing watermark methods). Also, as per your study against the Tree-Ring watermark, you find that your method actually does not perform well against in-process watermarks, invalidating your universality claim. Your abstract and conclusion should also make it clear that you are only considering post-processing watermarks. As they are, they imply all types of watermarks, which is not the case. These overreaching claims need to be addressed.\n\n2. Misleading results. You claim in the abstract that you improve the state-of-the-art by 74% for watermark removal, which is not what you find in your evaluation (41.8% and 64%). I don't know where the 74% is coming from. Also, the way you computed your improvements over previous work is not fair. Computing your improvement over the best attack for each defense would be a better way of showing how you improve against state-of-the-art in practice. Also, for forgery, computing the overall average and claiming it to be your improvement over state-of-the-art in the abstract is misleading since some of the methods perform quite poorly. You improve over the second-best method by 10% on average, which is still a good improvement. You, however, cannot claim you improve over state-of-the-art by 47% (as you do in the abstract); that is not true. The results reported need to properly reflect actual improvement against state-of-the-art; average improvement against all defenses studied is not a great metric, as it favors including poorly performing attacks like Gaussian noise and Gaussian blur to artificially inflate improvement, which are not state-of-the-art attacks. These results need to be corrected throughout the paper, especially in the abstract and conclusion, as they are misleading regarding the actual improvements.\n3. The epsilon values selected are quite large. While for removal 0.08~20/255 is large, it’s still somewhat manageable (although, usually, for the l-infinity norm, for high resolution images, adversarial attack research has usually used epsilons like 4/255 [1]). However, for forgery, the upper bound on the noise is 0.143 ~ 36/255, which is a lot\n. You also don’t include any images in the main part of the paper. For your ablation study, you further increase epsilon 2, which raises the upper bound on the forgery noise to 0.158 ~ 40/256. This is a significant amount of noise.\n\n4. The False Positive Rate (FPR) used to calibrate the thresholds is not stated; depending on the value, it could affect the success of either attack.\n\n[1] Croce, Francesco, et al. \"Robustbench: a standardized adversarial robustness benchmark.\" arXiv preprint arXiv:2010.09670 (2020)."}, "questions": {"value": "Questions:\n1. What process did you use to embed watermarks for your feasibility study? It would be good to specify.\n2. What is the loss function $\\mathcal{L}$ you use for your attacks? You didn’t specify in Section 4.2, and in Section 5.1, Parameter Settings, you mention the SSIM and L1/L2 norms. Are these the loss functions you use? If so, how does SSIM work over channel features?\n3. What do k and n stand for in the K-Means algorithm?\n4. How was your feature extractor trained, more specifically, on what data?\n5. How did you decide on the epsilon values you selected? Have you tried varying them, in particular, lowering them? \n6. For the forgery attack, you specify that the forged attack is obtained as one of two possible images (either only delta is subtracted or delta is subtracted and delta s is added), which one do you use when measuring the success rate? Are your results the average of both, or are you only picking the best of the two? If it’s the latter, it’s not a fair comparison to other methods that only output one forgery, since you essentially get two tries.\n\n\n\nSuggestions:\n1. AIGC, as an acronym, is not explained.\n2. This is both a question and a suggestion to satiate my curiosity. For the watermark removal attack, did you try adding an extra loss component to minimize the effect of the added noise on the feature channels not selected by your clustering algorithm? It could help preserve image quality and focus the perturbations on the selected feature channels."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "As mentioned in the Weaknesses section, the paper makes overreaching claims about its findings and results. It also presents misleading results that inflate the value of their contribution and improvement over the state-of-the-art. I am not sure if this is a complete breach of ethics, but it does not align with the “Uphold High Standards of Scientific Excellence” clause of the ICLR Code of Ethics. Specifically, “Findings must be reported accurately and honestly. Researchers must not make deliberately false or misleading claims, fabricate or falsify data, or misrepresent results”."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A25G7WNDRP", "forum": "dOxOX4GF1k", "replyto": "dOxOX4GF1k", "signatures": ["ICLR.cc/2026/Conference/Submission12929/Reviewer_QP2W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12929/Reviewer_QP2W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760734167400, "cdate": 1760734167400, "tmdate": 1762923694116, "mdate": 1762923694116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a watermark attack method that achieves both watermark removal and forgery. The proposed method is evaluated against several existing watermark-generation processes and compares with several existing attack methods. Substantial improvement is demonstrated."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The research question is timely and interesting. The concepts of watermark removal and watermark forgery are quite practically relevant. Learned noise is injected to the watermarked images."}, "weaknesses": {"value": "The title (and some parts of the abstract) does not align well with the content of this work.\n\nThe paper is poorly written and organized. In general, it is quite time-consuming to identify and fully understand the contribution of this work and the novelty of the proposed method. \n\nThe attack method is based on the assumption that the underlying watermarks are embedded in the distortion layer between an encoder and a decoder of an image-generation process. It seems to me such applicability may not be necessarily broad. As cited by the authors, post-processing methods are also available, which are not tied to specific image-generation processes, such as diffusion models. It might be helpful to consider the effectiveness / robustness of the proposed method against watermarks embedded by post-processing methods.\n\nDue to the assumption above, the proposed method hypothesizes that the extracted image features may exhibit certain clusters where some clusters correspond to information leakage. It is not clear to me if such an hypothesis could still hold when the underlying watermarking mechanism is changed.\n\nMeanwhile, the proposed idea of identifying information leakage through extracted features was discussed in heuristic approaches in the existing literature, as mentioned by the authors. The novelty of the proposed method, in terms of generalizing this observation to deep learning frameworks with distortion layers, is not necessarily substantial."}, "questions": {"value": "It seems to me that watermark removal and forgery correspond to type I and type II errors in a standard hypothesis testing setting (or false positive/negative, depending on what is defined as 1). Would the proposed method experience a tradeoff between the success rates of removal and forgery?\n\nThe ides of injecting noise to the image is intuitive. However, is it possible or effective to also inject noise at the embedding layer? Would a double-layer noise structure further improve the performance of the proposed method?\n\nDoes the proposed method require the unwatermarked images as the input, in addition to their watermarked counterparts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Iq26E06Cks", "forum": "dOxOX4GF1k", "replyto": "dOxOX4GF1k", "signatures": ["ICLR.cc/2026/Conference/Submission12929/Reviewer_PmLJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12929/Reviewer_PmLJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601436645, "cdate": 1761601436645, "tmdate": 1762923693875, "mdate": 1762923693875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for evading state-of-the-art watermarking approaches and also copying them. The insight is that many of the most robust watermark become more ``detectable'' slash learnable to some degree."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem is interesting and relevant. It is interesting that we can do forgery and removal all in one paradigm. To my knowledge the method is new and novel.\n\n- Empirical results show promise."}, "weaknesses": {"value": "- The attack suite is not comprehensive enough. There are many attacks missing: rotation, regeneration, and others cited in popular benchmarks like WAVES.\n\n- The number of methods evaluated is not enough and does not include true state of the art (older methods). Please include VINE [1] and Gaussian Shading [2], and etc. Note that this list is not exhaustive. I'm not convinced that the same pattern can be generalized to these settings.\n\n- I noticed in the Appendix there was also discussion about the limitation to Tree-Ring. I think that weakness to this class of diffusion-based watermark that use inversion for embedding watermarks is significant in my opinion as this is a defining direction of the field. \n\n- Number of images is not enough (100).\n\n- Algorithm may be slow as it needs to do it per image. (Thought it is parallelizable).\n\n\n[1] Lu, Shilin, et al. \"Robust watermarking using generative priors against image editing: From benchmarking to advances.\" arXiv preprint arXiv:2410.18775 (2024).\n[3] Yang, Zijin, et al. \"Gaussian shading: Provable performance-lossless image watermarking for diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."}, "questions": {"value": "- I'm not sure I understand this intuition given ``training watermarking models with increasingly diverse and severe distortions induces embedding redundancy''. Could you shed some more insight.\n\n- I am still kind of confused by why outlier features/groups with fewer samples are examples of leakages.\n\n- I'm wondering if you can somehow fully parameterize the optimization procedure to find the deltas and instead of doing it per-image create a general mapping that gives it to you automatically."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TmidDXoQjA", "forum": "dOxOX4GF1k", "replyto": "dOxOX4GF1k", "signatures": ["ICLR.cc/2026/Conference/Submission12929/Reviewer_DC2f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12929/Reviewer_DC2f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899801532, "cdate": 1761899801532, "tmdate": 1762923693351, "mdate": 1762923693351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}