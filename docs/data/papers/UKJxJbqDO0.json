{"id": "UKJxJbqDO0", "number": 21008, "cdate": 1758312750634, "mdate": 1759896947320, "content": {"title": "SynthPert: Enhancing Biological Reasoning in LLMs via Synthetic Reasoning Traces for Cellular Perturbation Prediction", "abstract": "Predicting cellular responses to genetic perturbations represents a fundamental challenge in systems biology, critical for advancing therapeutic discovery and virtual cell modeling. While large language models (LLMs) show promise for biological reasoning, their application to perturbation prediction remains underexplored due to challenges in adapting them to structured experimental data. We present SynthPert, a novel method that enhances LLM performance through supervised fine-tuning on synthetic reasoning traces generated by frontier models. Using the PerturbQA benchmark, we demonstrate that our approach not only achieves state-of-the-art performance but surpasses the capabilities of the frontier model that generated the training data. Our results reveal three key insights: (1) Synthetic reasoning traces effectively distill biological knowledge even when partially inaccurate, (2) This approach enables cross-cell-type generalization with 87\\% accuracy on unseen RPE1 cells, and (3) Performance gains persist despite using only 2\\% of quality-filtered training data. This work shows the effectiveness of synthetic reasoning distillation for enhancing domain-specific reasoning in LLMs.", "tldr": "We develop SynthPert, an LLM fine-tuned on synthetic reasoning traces that achieves state-of-the-art performance in predicting cellular responses to genetic perturbations, outperforming even the frontier model that generated its training data.", "keywords": ["cellular perturbation prediction", "genomics applications", "LLMs", "reasoning", "synthetic data", "perturbation prediction"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f533622d841d3d675f3fa98a31b6ed0c0539cc7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SynthPert, a method for improving LLM performance on cellular perturbation prediction by fine-tuning a base model on synthetic chain-of-thought reasoning traces from a more advanced frontier model. Evaluated on the PerturbQA benchmark, the method reportedly outperforms the frontier model that generated its training data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The core idea of \"synthetic reasoning distillation\" is innovative.\n- The result that the student model surpasses its teacher is striking if true."}, "weaknesses": {"value": "- The performance of large language models is not comprehensively evaluated in this study. For example, GPT-5-thinking may improve over all proposed models.\n- There is a potential data leakage issue. Modern language models evolve in time and are likely to have seen PerturbQA data a priori. Therefore, the risk of data leakage is high, and the performance gain may not be meaningful.\n- The novelty of direct 3-way classification is limited.\n- The authors used a public GitHub repository to share the code, which is not recommended and can leak author identity. Furthermore, I share the concerns raised in the GitHub issue regarding the metric calculation and method comparison. A satisfactory response is needed to address the reproducibility concern. This is especially important as the authors directly adopt perturbQA results for other benchmarking methods.\n- It is unclear how the results compare to some other well-established benchmarks, e.g. [1-2].\n\n[1] Viñas Torné, Ramon, et al. \"Systema: a framework for evaluating genetic perturbation response prediction beyond systematic variation.\" Nature Biotechnology (2025): 1-10.\n[2] Ahlmann-Eltze, Constantin, Wolfgang Huber, and Simon Anders. \"Deep-learning-based gene perturbation effect prediction does not yet outperform simple linear baselines.\" Nature Methods (2025): 1-5."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9hBfOl1mTC", "forum": "UKJxJbqDO0", "replyto": "UKJxJbqDO0", "signatures": ["ICLR.cc/2026/Conference/Submission21008/Reviewer_jUeR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21008/Reviewer_jUeR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761426894975, "cdate": 1761426894975, "tmdate": 1762999992620, "mdate": 1762999992620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of predicting perturbation effects using LLMs. The input is a textual description of the cell type, perturbation, and target gene, and the label is the direction of change in expression for the input gene. In addition to the no-context approach, the authors introduce a new fine-tuning strategy that involves training the model using synthetic, biological explanations of observed train labels. They demonstrate that the latter approach outperforms existing methods across four cell lines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces an effective knowledge distillation scheme via SynthPert and shows that training on reasoning traces outperforms training on data alone. This has implications for future tuning of LLMs.\n- The paper rigorously evaluates the model's ability to generalize across biological contexts by performing an experiment where an entire cell line  (RPE1) was held out from training. RPE1 is the only non-cancerous line among the four, and the results suggest the model can transfer biological principles from cancer cell lines to healthy."}, "weaknesses": {"value": "- The authors reference works that show that simpler baselines often outperform complex models, but such baselines are missing from the current study. The appropriate train/cell-specific mean/majority baselines (e.g., as done in STATE [1]) should be included in all experiments to further verify that the results are not due to memorization.\n- The authors do not specify what downstream ML model was trained on the GenePT/scGPT embeddings to produce results in tables 1-2. Did the authors experiment with multiple model families and were the hyperparameters tuned? Addressing the choice of the underlying predictor is necessary to assess the rigor of the baseline comparisons.\n\n[1] https://www.biorxiv.org/content/10.1101/2025.06.26.661135v2\n\nOther\n- In your last example in Appendix B, it seems from the model output that the gene is being downregulated, but you claim the model incorrectly predicts this gene to not be DE."}, "questions": {"value": "The process of generating a large body of synthetic data and then filtering it down by 98% using a judge LLM implies a significant computational cost. Can the authors comment on the resources required for this step and accessibility to the general public?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "X13sy70IWY", "forum": "UKJxJbqDO0", "replyto": "UKJxJbqDO0", "signatures": ["ICLR.cc/2026/Conference/Submission21008/Reviewer_JZRM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21008/Reviewer_JZRM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535364159, "cdate": 1761535364159, "tmdate": 1762999992682, "mdate": 1762999992682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SynthPert, a method for predicting cellular responses to genetic perturbations using large language models (LLMs). The task is: given (cell type, perturbation, target gene), predict whether that gene will be upregulated, downregulated, or not differentially expressed after the perturbation.\nKey ideas:\n1. Instead of just fine-tuning a model on labeled tuples, SynthPert fine-tunes on synthetic chain-of-thought (CoT) biological explanations generated by a stronger \"frontier\" LLM (o4-mini).\n2. These explanations are filtered by a critic LLM and only “excellent”-quality reasoning traces are kept.\n3. A smaller open model (DeepSeek-R1 8B, LoRA-tuned) is fine-tuned on those selected reasoning traces.\n\nThey evaluate on PerturbQA, which reformulates perturbation biology as a natural-language classification task. Compared to baselines including GEARS, scGPT, GenePT, SUMMER (RAG + LLM reasoning), and a plain SFT-on-data baseline, SynthPert:\n1. Achieves higher AUROC on differential expression and direction-of-change across 4 human cell lines.\n2. Solves the full 3-class prediction problem directly (up / down / no change), rather than splitting into two subtasks like SUMMER.\n3. Generalizes to an unseen cell type (RPE1) with ~87% accuracy, suggesting cross-cell-type transfer.\n4. Beats its own teacher model (o4-mini) on that 3-class task (89% vs 52% accuracy in the general setting), despite being smaller and trained on only ~2% of filtered data.\n\nThe authors argue that reasoning structure itself, even if partially factually wrong, is more valuable than raw labels, and that synthetic reasoning distillation can “unlock” latent biological capability in mid-size LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong empirical results\nSynthPert consistently outperforms strong baselines (SUMMER, GEARS, scGPT, GenePT, GAT) under PerturbQA.\nIt works in a challenging 3-class joint setup, not just binary subtasks.\nIt generalizes to an unseen cell line — which is a big deal, since cross-cell transfer is usually where perturbation models break.\n\n2. Surpassing its own teacher\nThis is eye-catching: the student (8B, LoRA-tuned on filtered traces) beats the teacher frontier model on the final task. That’s the kind of result ICLR likes because it suggests an interpretable path toward smaller, specialized domain LLMs without continuously calling an expensive frontier model."}, "weaknesses": {"value": "1. Reliance on proprietary frontier models\nThe synthetic traces come from o4-mini (frontier model) and are filtered by another LLM “critic.” This raises some questions:\nHow reproducible is this pipeline for other labs that don’t have access to those same frontier models?\nWould an open model judge produce equally good traces?\nIs there a risk of just laundering proprietary model knowledge into a smaller open(ish) model?\n\n2. Biological correctness vs “sounding biological”\nThe critic LLM is grading explanations on being “biologically plausible,” but plausibility ≠ correctness. The appendix admits that even high-scoring chains-of-thought still include false mechanistic statements, especially at the pathway level, and that “hallucinated pathway links” show up more in wrong predictions. That means the model can still be confidently wrong — in a way that sounds mechanistic.\n\n3. Distributional skew / class imbalance\nMost genes are “not differentially expressed,” so a naive model could try to game accuracy by spamming that class. The authors argue SynthPert is not just exploiting that, and they do mention class rebalancing experiments. Still:\nUpregulated recall for SynthPert (0.14) is low compared to o4-mini (0.62), even though SynthPert has higher precision.\nFor discovery work, low recall on minority classes can be a practical issue: you miss interesting positive hits."}, "questions": {"value": "1. Distillation Paradox\nThe smaller student model surpasses its larger teacher.\n→ What exactly allows this “distillation paradox”?\n→ Does it arise from regularization (LoRA compression), or from filtering low-quality traces that reduce noise?\n\n2. Synthetic Data Generation\nThe critic model uses a 5-point scale to rate explanations.\n→ What fraction of generated reasoning traces were rejected vs. retained?\n→ How sensitive are final results to the critic’s strictness (e.g., “excellent” vs. “good” threshold)?\nWere hallucinations or factual errors in the remaining “excellent” traces quantified beyond the small 17-example sample in Appendix B?\n\n3. Cross-Cell-Type Generalization\nThe RPE1 holdout experiment is impressive (87% accuracy), but RPE1 is epithelial and differs substantially from training lines.\n→ Has generalization been tested on other withheld lineages (e.g., immune or neuronal cells)?\n→ Could this success stem from overlapping perturbation targets rather than true cross-lineage generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WQfILpjQiB", "forum": "UKJxJbqDO0", "replyto": "UKJxJbqDO0", "signatures": ["ICLR.cc/2026/Conference/Submission21008/Reviewer_wrsK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21008/Reviewer_wrsK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799502978, "cdate": 1761799502978, "tmdate": 1762999992695, "mdate": 1762999992695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SynthPert, an LLM based approach for predicting cellular perturbation effects. The key idea in this paper is to fine-tune an LLM on synthetic chain-of-thought reasoning generated by OpenAI o4-mini. They show that SynthPert achieves state-of-the-art  results on the PerturbQA benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- SynthPert achieves SOTA results on PerturbQA\n- SynthPert shows a practical way to use LLMs for predicting cellular perturbation effects"}, "weaknesses": {"value": "- The overall contribution of this paper is incremental. The proposed method, fine-tuning a language model on synthetic reasoning traces generated by a larger model, follows a standard pattern of LLM distillation and reasoning supervision, without introducing a new architectural insight, training paradigm, or theoretical framework. The approach is interesting from a practical perspective, but conceptually it represents a straightforward application of existing ideas (e.g., chain-of-thought supervision). This paper is more suitable for a workshop paper rather than a full conference. \n-The empirical scope is limited. The evaluation relies solely on the PerturbQA benchmark, which is limited. \n- PerturbQA is a derived benchmark published in 2024 (Wu et al.). Given o4-mini’s pretraining cutoff is likely late 2024 or early 2025, it’s possible that the benchmark’s data distributions, example tuples, or even results tables were present in its pretraining corpus."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dxvPP6fiGM", "forum": "UKJxJbqDO0", "replyto": "UKJxJbqDO0", "signatures": ["ICLR.cc/2026/Conference/Submission21008/Reviewer_xpVd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21008/Reviewer_xpVd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762296436709, "cdate": 1762296436709, "tmdate": 1762999992739, "mdate": 1762999992739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}