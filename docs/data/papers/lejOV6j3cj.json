{"id": "lejOV6j3cj", "number": 7339, "cdate": 1758016947617, "mdate": 1763651221364, "content": {"title": "Embracing Discrete Search: A Reasonable Approach to Causal Structure Learning", "abstract": "We present FLOP (Fast Learning of Order and Parents), a score-based causal discovery algorithm for linear models. It pairs fast parent selection with iterative Cholesky-based score updates, cutting run-times over prior algorithms. This makes it feasible to fully embrace discrete search, enabling iterated local search with principled order initialization to find graphs with scores at or close to the global optimum. The resulting structures are highly accurate across benchmarks, with near-perfect recovery in standard settings. This performance calls for revisiting discrete search over graphs as a reasonable approach to causal discovery.", "tldr": "Discrete graph search is a reasonable structure learning approach enabling accurate causal discovery; the size of the graph space was not the barrier.", "keywords": ["Causal Discovery", "Bayesian Networks", "DAGs", "Structure Learning"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5549095cfb890c56cc26a8f633e2ec0a9f625770.pdf", "supplementary_material": "/attachment/e1d6177721afdda3f8df65f4983c9ec6e08178ad.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces FLOP (Fast Learning of Order and Parents), a novel score-based, discrete search algorithm for causal structure learning for linear additive noise models (ANMs). FLOP builds upon an order-based search framework, similar to the BOSS algorithm, but introduces four key components that lead to significant gains in both speed and accuracy: warm-start parent selection, dynamic Cholesky updates, principled order initialization, iterated local search. Experiments across a wide range of benchmarks (ER, SF, Alarm, etc.) demonstrate that FLOP variants achieve state-of-the-art accuracy (measured by SHD and AID) while being orders of magnitude faster than competing methods like BOSS, GES, and DAGMA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper is well-written and easy to follow.\n\n2. By combining warm-start parent selection with dynamic Cholesky updates, FLOP achieves massive computational speed-up. Furthermore, this speed-up enables more exhaustive search, leading to higher accuracy.\n\n3. The performance of FLOP is validated across many benchmarks."}, "weaknesses": {"value": "1. The authors do not provide theoretical analysis on finite-sample performance of FLOP. I think this is a major weakness of this paper. First, the authors repeatedly emphasize the importance of finite-sample performance, e.g., \"Finite-sample performance is a practical challenge, despite the mature identifiability theory and asymptotic guarantees\", \"one of the core issues of score-based methods in practice are finite-sample induced local optima\". Second, the authors consider the simplest structural causal model where structural functions are linear, exogenous noises are Gaussian, and there is neither latent confounder nor selection bias.\n\n2. The performance of FLOP is only validated on synthetic data. Although the authors consider real-world networks from the bnlearn repository, the data is still synthetic. This is not convincing for a paper whose primary contribution does not lie in theory.\n\n3. The ILS perturbation strategy is defined as $k = \\ln p$ random swaps of two elements. The paper simply states this is \"robust\" without an ablation study or sensitivity analysis."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v1gGi5I8oX", "forum": "lejOV6j3cj", "replyto": "lejOV6j3cj", "signatures": ["ICLR.cc/2026/Conference/Submission7339/Reviewer_UFtS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7339/Reviewer_UFtS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761450389683, "cdate": 1761450389683, "tmdate": 1762919462683, "mdate": 1762919462683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to the Reviewers"}, "comment": {"value": "We thank all reviewers for their thoughtful and constructive feedback. The comments have helped us identify ways to improve presentation and add context.\n\n__Core Contribution and Clarifications__\n\nIt is great to see that all reviewers appreciate the central idea of the paper. For example, Reviewer MbRD writes that our work \"compellingly challenges the conventional notion that discrete search cannot scale outside of small graphs\" and Reviewer cdXL highlights that FLOP \"brings together several smart incremental additions\" to make discrete search competitive. Reviewer ZiZP notes that \"Overall, this work makes an important contribution to the field by reestablishing the relevance and potential of discrete optimization in score-based causal discovery, a domain recently dominated by continuous optimization approaches\".\n\nAt the same time, the reviews have helped us identify areas to clarify:\n\n- __Our main contribution is on search, not proposing a new score (statistical vs search problem).__\n\nIn score-based causal discovery, there are two separate questions:\n\n    1) Is the true graph score-optimal? (statistical)\n    2) Can we find a score-optimal graph? (search)\n\nFLOP tackles question 2) and in practice finds graphs with close-to-optimal BIC scores. This shows that, contrary to common claims, effective BIC score optimization through discrete search is possible. Once this is achieved, how well a BIC-optimal graph recovers the true graph depends solely on the scoring criterion, not the optimization algorithm. We have emphasized this distinction further by additionally reporting the BIC score in the ER and SF graph settings (Figure 5) as well as in the newly added experiments on non-linear, semi-synthetic, and real-world data (Appendix C.7, C.8 and C.9; see also below).\n\n- __Synthetic data vs real-world data.__\n\nIn our initial submission, the simulations focused on synthetic data where the model assumptions are met and the ground-truth graph is known. On these settings, FLOP obtains state-of-the-art graph learning results, corroborating the claim that discrete search for well-scoring graphs is feasible and effective. In the current revision, we further include results on non-linear data (non-linearities generated by MLPs and by sampling Gaussian Processes), semi-synthetic data (causalAssembly), and real-world data (Sachs). Here, as we show, FLOP is still successful in optimizing the Gaussian BIC score, while the quality of the found well-scoring graphs then depends on whether the scoring criterion is suitable to recover the ground-truth graph. This also highlights opportunities for future work: to design suitable scoring criteria that can be evaluated and updated efficiently, enabling effective discrete search for causal discovery beyond the linear Gaussian setting.\n\n\n__Updates to the Paper__\n\nIn the current revision, we added:\n- Appendix C.7, C.8 and C.9 which show additional experimental results on non-linear data, the causalAssembly semi-synthetic data, and the Sachs real data set.\n- Figures 5, 12, and 13 showing the BIC scores of the learned graphs in multiple settings, which we compare to the SHD results.\n- Appendix A, which features a new visualization of FLOP's high-level control flow and delineates how FLOP differs from the BOSS algorithm."}}, "id": "0qZWKUDEVE", "forum": "lejOV6j3cj", "replyto": "lejOV6j3cj", "signatures": ["ICLR.cc/2026/Conference/Submission7339/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7339/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7339/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763651851087, "cdate": 1763651851087, "tmdate": 1763651851087, "mdate": 1763651851087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses the acceleration of score-based causal discovery algorithms. Traditionally, estimating causal DAGs from observational data using score-based criteria (e.g., BIC) has been regarded as computationally intractable due to the NP-hardness of discrete search, raising concerns about scalability in large datasets. In recent years, continuous optimization methods such as NOTEARS and DAGMA have emerged as popular alternatives. However, these approaches suffer from challenges including non-convex optimization, approximate enforcement of acyclicity constraints, and the need for post-hoc thresholding.\n\nThe paper proposes a new score-based causal structure learning algorithm, FLOP (Fast Learning of Order and Parents), which dramatically improves the efficiency of discrete search and achieves better performance than existing continuous optimization approaches. FLOP targets causal DAG estimation under a linear additive noise model (linear ANM). It introduces parent set reuse, where the parent sets of each node are initialized from the results of previous searches, reducing both computational and memory costs. A modified grow-shrink procedure is proposed and theoretically proven to correctly identify the Markov boundary. In addition, dynamic Cholesky updates enable efficient computation of conditional variances in the BIC score by reusing Cholesky decompositions, reducing the cost from $O(k^3)$ to $O(k^2)$. The method also incorporates principled order initialization, which constructs an initial variable order by placing highly correlated nodes adjacent to each other, and Iterated Local Search (ILS) to substantially improve the accuracy of discrete optimization.\n\nExperimental results on synthetic data demonstrate that FLOP runs more than 100 times faster than conventional discrete search algorithms while achieving nearly the same accuracy as exact search methods. Moreover, it outperforms continuous optimization methods, highlighting the renewed importance of discrete optimization in score-based causal discovery."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "While discrete search–based score methods have often been overshadowed by the recent rise of continuous optimization approaches due to their NP-hardness, this paper makes a significant contribution by demonstrating that discrete search can be made dramatically more efficient through theoretically grounded and well-designed algorithmic improvements. The proposed method introduces principled modifications to key components of the discrete search process, resulting in substantial computational gains.\n\nThrough simulation experiments, the authors show that their method runs over 100 times faster than conventional discrete search algorithms, achieving nearly the same accuracy as exact search methods. Moreover, it also outperforms representative continuous optimization methods. Overall, this work makes an important contribution to the field by reestablishing the relevance and potential of discrete optimization in score-based causal discovery, a domain recently dominated by continuous optimization approaches."}, "weaknesses": {"value": "While this paper makes a valuable contribution by demonstrating that score-based causal discovery with discrete search can be made computationally practical through well-engineered algorithmic improvements, its theoretical and methodological novelty is somewhat limited. Many of the proposed components, such as parent set reuse, dynamic Cholesky updates, and local search, are not fundamentally new, but rather effective adaptations of existing techniques.\n\nMoreover, although each of the algorithmic enhancements is well-motivated and complementary, their effectiveness may depend on specific data conditions. For example, the parent set reuse and dynamic Cholesky updates assume relatively stable local structures and well-conditioned covariance matrices; the principled order initialization relies on correlations being informative about causal directions; and the iterated local search may require tuning depending on the ruggedness of the score landscape. A more explicit discussion of these limitations and potential failure cases would further strengthen the paper’s credibility and generality."}, "questions": {"value": "Could the authors discuss situations in which each of the proposed techniques, namely, parent set reuse, dynamic Cholesky updates, principled order initialization, and iterated local search, might fail to perform as expected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aiiNAhVbk3", "forum": "lejOV6j3cj", "replyto": "lejOV6j3cj", "signatures": ["ICLR.cc/2026/Conference/Submission7339/Reviewer_ZiZP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7339/Reviewer_ZiZP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761458199019, "cdate": 1761458199019, "tmdate": 1762919461828, "mdate": 1762919461828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary. The authors propose a modification of the BOSS algorithm (Andrews et al., 2023), a score-based approach for Markov Equivalence Class discovery from data generated according to a linear Gaussian model. The authors propose 4 modifications to improve the efficiency and accuracy of the algorithm. They show that their modifications preserve theoretical guarantees of convergence of the method to a solution in the Markov equivalence class. The empirical results support the claim of improved scalability and accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The main contribution of this paper is an algorithm with really impressive empirical performance, both in terms of accuracy and scalability.  Theoretical guarantees of identifiability of the Markov equivalence class that are common in score-based approaches are maintained, as the authors demonstrate. Finally, the empirical results are convincingly and well presented. Figure 2 clearly shows improvements in the run time. Figure 1,3,4 clearly show the improvements in terms of accuracy due to the ingredients they add to their algorithm. Nice work!"}, "weaknesses": {"value": "1. **Real world  / beyond linear Gaussian experiments.** The proposed method achieves impressive performance on linear Gaussian data, which are covered by the theory of the algorithm. However, the scope of these models is quite limited. It would be relevant to experiment beyond linear Gaussian synthetic data (at least nonlinear ANM + vary the noise distribution — analytic or NN generated nonlinearities, e.g., see [Montagna et al. 2023](https://causally.readthedocs.io/en/latest/generated/causally.scm.causal_mechanism.NeuralNetMechanism.html#causally.scm.causal_mechanism.NeuralNetMechanism)), and also with real-world datasets (that’s where causal discovery is interesting at the end of the day). E.g. you can consider the following datasets that are easy to find: bnlearn networks considered in the paper (please use real data when available), Sachs, Lucas, Dream (relevant for high dimensions), and the Syntren synthetic data generator. \n2. **Hyperparameter tuning**. The authors set $\\lambda_{BIC} = 2$. It is, however, unclear whether this is optimized for the analysed datasets, and how sensitive FLOP is to this hyperparameter. Could the authors discuss that, and present experimental results at different values of $\\lambda_{BIC}$?\n3. **Incremental nature of the contribution**. It is worth noting that the new method consists of smart but incremental additions to existing methodologies — this doesn’t change my opinion on the positive value of the work. In this sense, I am curious about whether the *warm start* tweak of Section 3.1 and the use of ILS in Section 4.2 are novel in causal discovery, or whether the authors have a reference to past use of these ideas. \n\nI am happy to raise my score upon satisfactory execution of the required experiments!"}, "questions": {"value": "What are all the hyperparameters of the method? In the weaknesses I ask clarifications about $\\lambda_{BIC}$, but are there others? If yes, can the authors clarify how the hyperparameters are set, and empirically analyse the sensitivity of FLOP to changes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "baGHGowG1l", "forum": "lejOV6j3cj", "replyto": "lejOV6j3cj", "signatures": ["ICLR.cc/2026/Conference/Submission7339/Reviewer_cdXL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7339/Reviewer_cdXL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491906120, "cdate": 1761491906120, "tmdate": 1762919460600, "mdate": 1762919460600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FLOP (Fast Learning of Order and Parents), a highly efficient score-based algorithm for causal structure learning in linear models. FLOP significantly accelerates the reinsertion-based local search over topological orders by \"warm-starting\" the grow-shrink parent selection and using $O(k^2)$ Cholesky rank-one updates for score computation, avoiding $O(k^3)$ re-computation . This speed advantage is leveraged to run an Iterated Local Search (ILS) with a principled order initialization, enabling the algorithm to escape local optima and achieve accurate graph recovery with dramatically lower runtime. The work compellingly challenges the conventional notion that discrete search cannot scale outside of small graphs."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "This paper builds upon a prior discrete search algorithm BOSS, by proposing 4 distinct modifications: 1) reusing parent sets for the grow-shrink procedure to improve runtime, 2) dynamically updating the choleksy decomposition to improve runtime, 3) initializing the first order to mitigate against known failure modes of order-based methods, and 4) running iterated local search to escape local minima. \n\nModifications 1), 2), and 3) seem entirely original to my knowledge (for 4) see the authors response to my first question in the Questions section). The quality of the work is high, as proofs provided make sense, all notation is clear, standard definitions and concepts in causal discovery are consistently used, and extensive coverage of the relevant literature is explored. The paper is generally clear, with a few caveats that I discuss in the Weaknesses and Questions section. Most importantly, the empirical results concerning the reduction in runtime are quite significant - in particular, Figure 2 shows how the addition of each modification leads to substantial decreases in the runtime needed for a fixed number of nodes, enabling scaling to graphs with over 500 variables. This is extremely significant given the fact that most discovery algorithms in the literature fail to scale past 30-50 nodes. In general, FLOP builds upon a growing trend of papers [1,2, 3] exploiting local structure previously ignored by methods aiming only for consistency. Additionally, this paper puts forward ideas that could be very helpful for other researchers in the field, even beyond score-based subfield.\n\nReferences:\n\n[1] Hybrid Top-Down Global Causal Discovery with Local Search for Linear and Nonlinear Additive Noise Models., Hiremath et al., NeurIPS 2024\n\n[2] LoSAM: Local Search in Additive Noise Models with Mixed Mechanisms and General Noise for Global Causal Discovery., Hiremath et al. UAI 2025\n\n[3] Strong and Weak Identifiability of Optimization-based Causal Discovery in Non-linear Additive Noise Models, Li et al., ICML 2025."}, "weaknesses": {"value": "1. The empirical analysis is conducted largely on simulated linear settings, with only a few real-world dataset considered. Given that the main contribution of FLOP is strictly better empirical performance (no theoretical guarantees outlining provable improvements are given), a more substantial empirical study of the performance of FLOP in nonlinear ANM and real-world data would help contextualize the contribution of FLOP. This is true even if results are not as promising as in the linear setting, as it might highlight where modifications in future could be made to build on top of FLOP.\n\n 2. Although the authors differentiate FLOP from BOSS at some specific subparts of the algorithm, (for ex. at the end of Section 2 they highlight the BOSS reinsertion strategy to the reader to build upon), it is difficult for a reader who is not already familiar with the BOSS paper to parse. For example, at the beginning it’s not clear whether Algorithm 1 constitutes the entirety of the BOSS algorithm, or just a subpart (it is, which is discussed later). Would be nice for the full algorithm environments of BOSS and FLOP and corresponding walk-throughs to be present (or if necessary signaled to the reader early on and placed in the Appendix) to facilitate easy comparison for the reader. Or perhaps even adding a figure describing the general pipeline of these specific score-based methods could be helpful, for better clarity."}, "questions": {"value": "1. How does this approach compare to heuristic adaptions of GES such as XGES [1], where heuristics similar to ILS are used to escape local minima? Theoretical and experimental comparison would both be useful here.\n\n2. How does the proposed approach here compare empirically against recent score-based methods such as GRaSP [2], or against FCM methods designed for linear ANMs, such as DirectLiNGAM [3]? In general, it would be useful for the authors to undertake a more comprehensive empirical comparison to other recent methods, as many of the paper’s contributions relate to the promise of improved empirical performance, but without any formal guarantees of improvement.\n\n3. To what extent can the heuristic modifications made in FLOP be translated to the setting of discrete search in nonlinear ANMs? Some modifications seem like they might likely generalize such as the modified grow-shrink procedure, but other improvements such as dynamic Cholesky updates seem only possible due to the linearity assumptions.\n\n4. Is it possible to recalculate the ordering metric to use some more common notion of topological divergence, as seen unnormalized in [4] or normalized in [5,6]? This would provide additional insight into the performance of FLOP.\n\n5. Please give more details on this: “The standard hardness constructions rely on data-generating processes that cannot be represented by a DAG over the observed variables (Chickering, 1996; Chickering et al.,2004).” Not sure if I understand correctly what it means for a DGP to not be representable as a DAG over the observed variables (do you mean they require unobserved variables?).\n\n6. In Figure 2, can the authors clarify what the difference between FLOP naive and FLOP (reusing parents) is? My understanding is that the algorithm generated by modifying grow-shrink is exactly FLOP? If not, what exactly is the base FLOP algorithm? \n\nReferences:\n\n[1] Extremely Greedy Equivalence Search, Nazaret et al., UAI 2025.\n\n[2] Greedy Relaxations of the Sparsest Permutation Algorithm, Lam et al., UAI 2022.\n\n[3]DirectLiNGAM: A Direct Method for Learning a Linear Non-Gaussian Structural Equation Model, Shimizu et al., JMLR 2011.\n\n[4]Causal Discovery with Score Matching on Additive Models with Arbitrary Noise, Montagna et al., CleaR 2023.\n\n[5] Hybrid Top-Down Global Causal Discovery with Local Search for Linear and Nonlinear Additive Noise Models., Hiremath et al., NeurIPS 2024\n\n[6] LoSAM: Local Search in Additive Noise Models with Mixed Mechanisms and General Noise for Global Causal Discovery., Hiremath et al. UAI 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WmKZh85fdo", "forum": "lejOV6j3cj", "replyto": "lejOV6j3cj", "signatures": ["ICLR.cc/2026/Conference/Submission7339/Reviewer_MbRD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7339/Reviewer_MbRD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668312741, "cdate": 1761668312741, "tmdate": 1762919459264, "mdate": 1762919459264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}