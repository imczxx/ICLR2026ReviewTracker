{"id": "ytowjGMISZ", "number": 4030, "cdate": 1757586936706, "mdate": 1763098236473, "content": {"title": "A Reason-then-Describe Instruction Interpreter for Controllable Video Generation", "abstract": "Diffusion Transformers have significantly improved video fidelity and temporal coherence; however, practical controllability remains limited. \nConcise, ambiguous, and compositionally complex user inputs contrast with the detailed prompts used in training, yielding an intent–output mismatch. We propose ReaDe, a universal, model-agnostic interpreter that converts raw instructions into precise, actionable specifications for downstream video generators. ReaDe follows a reason-then-describe paradigm: it first analyzes the user request to identify core requirements and resolve ambiguities, then produces detailed guidance that enables faithful, controllable generation. We train ReaDe via a two-stage optimization: (i) reasoning-augmented supervision imparts analytic parsing with stepwise traces and dense captions; (ii) a multi-dimensional reward assigner enables stable, feedback-driven refinement for natural-style captions. Experiments across single- and multi-condition scenarios show consistent gains in instruction fidelity, caption accuracy, and downstream video quality, with strong generalization to reasoning-intensive and unseen inputs. ReaDe offers a practical route to aligning controllable video generation with accurately interpreted user intent.", "tldr": "We propose ReaDe, a universal, model-agnostic interpreter that converts raw instructions into precise, actionable specifications for downstream video generators.", "keywords": ["multimodal content understanding", "instruction interpretion", "controllable video generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/42b7467c51a9efd514617c742225d79d66ca0b39.pdf", "supplementary_material": "/attachment/7f70886ba0e7da3b31a4883e83ca18f11463001c.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ReaDe, the first universal video instruction interpreter for controllable video generation. It also introduces a multi-dimensional reward assigner to enhance training and control. Experimental results are promising and demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes ReaDe, the first universal video instruction interpreter for controllable video generation.\n\nThe presentation is clear, coherent, and well-structured.\n\nThe work introduces new data construction and reward design strategies.\n\nThe experimental comparisons are comprehensive and sufficient."}, "weaknesses": {"value": "The paper does not include a reward curve, which is important for illustrating training dynamics and stability.\n\nIt remains unclear whether there are any reward conflicts or reward hacking phenomena during optimization.\n\nFine-tuning an open-sourced model (e.g., Wan) on the refined caption dataset generated by the proposed method could provide a more convincing analysis. Currently, the refined captions are only used during inference, which is not sufficient to fully validate the method’s effectiveness."}, "questions": {"value": "My main concern is that visual control signals such as camera parameters, poses, or reference images cannot always be effectively interpreted as textual prompts. While this approach may work in simpler cases, it is unlikely to generalize well to more complex scenarios. Therefore, the underlying assumption of this line of work—that all forms of visual control can be represented or interpreted textually—does not fully hold."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z9qACz1Dv0", "forum": "ytowjGMISZ", "replyto": "ytowjGMISZ", "signatures": ["ICLR.cc/2026/Conference/Submission4030/Reviewer_MTFY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4030/Reviewer_MTFY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977267609, "cdate": 1761977267609, "tmdate": 1762917142222, "mdate": 1762917142222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "H6Ybc3H91P", "forum": "ytowjGMISZ", "replyto": "ytowjGMISZ", "signatures": ["ICLR.cc/2026/Conference/Submission4030/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4030/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763098235414, "cdate": 1763098235414, "tmdate": 1763098235414, "mdate": 1763098235414, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose a model-agnostic interpreter that converts raw instructions into precise, actionable specifications for downstream video generators."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written.\n\n2. The author uses reinforcement learning to improve the accuracy of the prompts given by the interpreter, and experiments show that better results were achieved."}, "weaknesses": {"value": "1. From an insight perspective, it has been validated by many previous works that accurate prompts lead to better generation results. Therefore, designing a more accurate VLM has not brought additional benefits to this task.\n\n2. The data used is sourced from GPT-4o, so I believe this ability is a distillation of 4o on specific tasks, which isn't particularly interesting.\n\n3. Improving the video captioning ability of VLMs has been the focus of much research before. Moreover, compared to previous methods, using better base models, such as Qwen2.5-omni, will inherently improve model performance. We can also use larger models to achieve improvements.\n\n4.The author is also using CogVideoX-5B, which is somewhat outdated now.\n\n5. I believe the author’s reward should come from feedback on the video output results, rather than the accuracy of the text. Because accurate prompts don't necessarily mean good output video results."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iCMAT6ITud", "forum": "ytowjGMISZ", "replyto": "ytowjGMISZ", "signatures": ["ICLR.cc/2026/Conference/Submission4030/Reviewer_uR5J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4030/Reviewer_uR5J"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003668342, "cdate": 1762003668342, "tmdate": 1762917142014, "mdate": 1762917142014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReaDe, a model-agnostic instruction interpreter designed to improve controllability in text-to-video generation. ReaDe follows a reason-then-describe paradigm, using a multimodal LLM to parse user inputs—such as text, images, depth maps, and camera trajectories—into structured, detailed prompts. The method is trained in two stages: supervised fine-tuning with chain-of-thought reasoning, followed by reinforcement learning with multi-dimensional rewards. Experiments show that ReaDe improves instruction faithfulness, caption accuracy, and video quality across single- and multi-condition settings. It also generalizes well to unseen and reasoning-intensive instructions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method is well-motivated: it leverages MLLMs to handle multimodal inputs and uses RL to enhance generalization, resulting in more faithful and controllable video generation.\n- Comprehensive experiments demonstrate ReaDe’s effectiveness across multiple metrics and condition types, outperforming competitive baselines like Any2Caption."}, "weaknesses": {"value": "- The claim of being *model-agnostic* is insufficiently supported. Quantitative experiments only use CogVideoX-2B; no results are shown for other video generators, limiting the generalizability of this claim.\n- The ablation study lacks rigorous comparisons to justify the statement that $R_{\\text{user}}$ and $R_{\\text{detail}}$ contribute the most. Table 6 does not include controlled experiments that isolate the effect of individual reward components, making the conclusion less convincing."}, "questions": {"value": "- The claim of model-agnosticism raises a conceptual question: if a video model is trained on short captions, could a dense structured prompt from ReaDe hurt its performance? Can the authors provide insight or experiments addressing this?\n- The *video-quality feedback* mechanism mentioned in Figure 2 is not described in detail. Could the authors elaborate on how this feedback is implemented and integrated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "81WRsFbnfe", "forum": "ytowjGMISZ", "replyto": "ytowjGMISZ", "signatures": ["ICLR.cc/2026/Conference/Submission4030/Reviewer_TJwC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4030/Reviewer_TJwC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006340629, "cdate": 1762006340629, "tmdate": 1762917141772, "mdate": 1762917141772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}