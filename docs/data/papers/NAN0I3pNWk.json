{"id": "NAN0I3pNWk", "number": 24549, "cdate": 1758357880210, "mdate": 1759896760907, "content": {"title": "DXFeat: Depth-Aware Features for Robust Image Matching", "abstract": "This study introduces DXFeat, a novel architecture that integrates depth infor-mation as an auxiliary branch for keypoint detection, leveraging depth cues to enhance localization accuracy, which improves localization accuracy with an average 3.1% gain while preserving inference efficiency. DXFeat refines feature extraction during training while maintaining computational efficiency. The model incorporates a modified reliability loss and learnable weighting mechanisms, balancing accuracy and robustness. By optimizing network channels while preserving high-resolution inputs, DXFeat supports both sparse and semi-dense matching, making it well-suited for visual  localization and augmented reality. A depth-assisted refinement module further enhances feature representation using coarse local descriptors. Notably, the depth auxiliary branch is only needed during training, ensuring streamlined deployment. Comprehensive evaluations on MegaDepth, ScanNet, and HPatches confirm that the combination of loss-level optimization and depth-auxiliary refinement yields consistent AUC improvements, establishing DXFeat as a strong and efficient framework for real-world image matching tasks.", "tldr": "", "keywords": ["Image Matching", "Keypoint Detection", "Sparse Matching", "Semi-Dense Matching", "Depth-Auxiliary"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9ac58e3e15febc3c7e6517e969e850cc23cb408.pdf", "supplementary_material": "/attachment/eff27a7bb90ca924c58c08a3608c8cbc6670e9f2.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces DXFeat, a depth-aware feature extraction architecture designed for robust and efficient image matching. The core innovation is the integration of a depth auxiliary branch (used only in training) to enhance keypoint detection and local descriptors, paired with a modified reliability loss and layerwise learnable weight fusion. The method builds on the XFeat backbone but introduces a Relative Depth Consistency (RDC) loss and lightweight, learnable fusion of depth cues. DXFeat is evaluated across MegaDepth, ScanNet, and HPatches datasets, showing improved matching accuracy and runtime competitive with state-of-the-art lightweight methods. Both ablation studies and results tables suggest consistent performance boosts, supporting the case for its practical merits in resource-constrained settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The motivation for leveraging depth as an auxiliary training signal is clearly articulated, addressing practical problems such as keypoint loss from viewpoint changes and camera motion. DXFeat’s focus on real-world constraints (computational cost, mobile deployment, and varying scene conditions) is justified."}, "weaknesses": {"value": "1. While the paper positions itself as improving XFeat by adding a depth auxiliary branch, it gives very limited discussion of other works that fuse depth or multimodal cues for detection/description. Additionally, the depth auxiliary branch proposed in this paper has not been validated in studies beyond XFeat. This weakens the argument for novelty, as very similar paradigms exist, albeit not always for exactly the same setting. Explicit experimental and conceptual differentiation is required.\n2. The method in Table 2.3 should add the reference.\n3. The manuscript does not acknowledge nor measure potential downsides, such as the costs of auxiliary depth supervision (dataset requirements, annotation effort) or failure cases when deploying on data where depth is unavailable at training."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CQt05dSPKx", "forum": "NAN0I3pNWk", "replyto": "NAN0I3pNWk", "signatures": ["ICLR.cc/2026/Conference/Submission24549/Reviewer_puJS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24549/Reviewer_puJS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761131461585, "cdate": 1761131461585, "tmdate": 1762943120364, "mdate": 1762943120364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DXFeat, a depth-aware local feature extraction framework designed to improve keypoint detection and descriptor robustness in image matching tasks. Building on the XFeat architecture, the authors incorporate three main innovations: a depth auxiliary branch used only during training, a Relative Depth Consistency (RDC) loss that enforces geometric stability, and a Huber-based reliability loss to achieve smoother optimization. These additions are intended to make the model more resilient to depth and viewpoint variations while keeping inference efficiency on par with lightweight baselines. The method is evaluated on MegaDepth, ScanNet, and HPatches, where it achieves around 3% improvement over XFeat and performs competitively with other efficient local feature methods such as SiLK and ALIKE. Overall, the paper targets the important trade-off between robustness and efficiency in feature matching and provides solid empirical evidence of improvement, though the gains are modest."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The auxiliary depth branch used only during training is an elegant way to leverage geometric cues without inflating inference-time complexity.\n\n2. Evaluation on three benchmarks (MegaDepth, ScanNet, HPatches) demonstrates robustness across indoor and outdoor settings.\n\n3. DXFeat achieves consistent accuracy improvements while retaining inference speed comparable to XFeat.\n\n4. The paper is generally well written, with clear motivation and consistent methodology organization."}, "weaknesses": {"value": "1. The paper assumes availability of depth maps during training, but this assumption is restrictive for many real-world settings. The authors should discuss how DXFeat could be applied with synthetic or estimated depth (e.g., using foundation models like DepthAnything v2). An empirical comparison using such pseudo-depth could validate broader applicability.\n\n2. The ablation on the auxiliary depth branch is shallow. There is no investigation into how branch structure, complexity, or depth quality affects final performance. Moreover, hyperparameter choices (e.g., λ_RDC = 0.5) are not justified or explored through sensitivity analysis.\n\n3. The paper introduces multiple new loss terms (RDC and Huber-based reliability loss), but there is no visualization or trajectory analysis to explain how these affect convergence or stability. Such insights could strengthen understanding of why the model performs better.\n\n4. The claim that depth supervision improves robustness is not empirically validated. It would be important to test DXFeat under perturbations or corruptions (e.g., Gaussian noise, blur, viewpoint jitter) to confirm its resilience.\n\n5. The work is largely empirical. The authors provide no theoretical reasoning or qualitative evidence explaining why integrating depth improves descriptor generalization or how different modules interact. Visualizations (e.g., weight distributions, reliability maps, or feature embeddings) would add interpretability.\n\n6.  Table 1 lacks critical computational comparisons, i.e., parameter counts, FLOPs, and inference latency. These metrics are essential for supporting claims about efficiency and lightweight deployment.\n\n7. The ablation study (Tables 4–5) reports performance deltas but provides no deeper explanation of how the proposed components complement each other. More rigorous or incremental ablation analysis would clarify interdependencies between RDC, reliability loss, and layer-wise weighting.\n\n8. Results are summarized without meaningful interpretation. For instance, the method fails to outperform baselines in HPatches illumination MHA but outperforms in viewpoint MHA, yet no reasoning is offered. Analytical discussion of such discrepancies would make the contribution more convincing.\n\n9. Presentation issues. (1) Minor typos (e.g., missing spaces) and notation inconsistencies (e.g., variable “i” in Eq. 5) need correction.\n(2) Figures (e.g., Fig. 4–7) lack sufficient explanation, particularly regarding variance and normalization."}, "questions": {"value": "1. How would DXFeat perform if trained using synthetic or predicted depth maps from a foundation model (e.g., DepthAnything v2)?\n\n2. How sensitive is the model to the λ_RDC hyperparameter and the structure of the depth branch?\n\n3. Why was Huber loss preferred over other robust losses? Could the authors visualize its impact on training stability?\n\n4. Can the authors report computational cost (FLOPs, parameters, latency) for fair comparison with other lightweight matchers?\n\n5. How does DXFeat perform under input noise, viewpoint perturbation, or blur?\n\n6. Why does DXFeat underperform in illumination-variant HPatches scenes, and how might this limitation be mitigated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x8uZvvoSwg", "forum": "NAN0I3pNWk", "replyto": "NAN0I3pNWk", "signatures": ["ICLR.cc/2026/Conference/Submission24549/Reviewer_6Mbb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24549/Reviewer_6Mbb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761145833915, "cdate": 1761145833915, "tmdate": 1762943119966, "mdate": 1762943119966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper adds a depth-assisted regularization loss to XFeat, which helps in relative pose and homography estimation. The idea is quite straightforward, but more experiments would be needed to really prove its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea is straightforward and easy to follow."}, "weaknesses": {"value": "A simple but truly effective idea can be ICLR-worthy. But right now the experiments don’t hit that bar.\n\nMissing experiments:\n\n(1) DeDoDe v2 is mentioned in related work, but there’s no comparison. Please add it.\n\n(2) Check what DeDoDe v2 and XFeat typically evaluate on, and include those (e.g., Image Matching Challenge, visual localization benchmarks).\n\nOther questionls:\n\n(1) You claim the second contribution improves generalization — how do you show that?\n\n(2) What happens if you add the same depth-assisted regularization to the detection branch? Please try it or explain why not."}, "questions": {"value": "Please address my concerns in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "B0JcqGRL8F", "forum": "NAN0I3pNWk", "replyto": "NAN0I3pNWk", "signatures": ["ICLR.cc/2026/Conference/Submission24549/Reviewer_QZLU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24549/Reviewer_QZLU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489739288, "cdate": 1761489739288, "tmdate": 1762943119754, "mdate": 1762943119754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DXFeat, a depth-aware extension of the lightweight feature framework XFeat, whose main goal is to improve the robustness of local feature detection and description under strong viewpoint / depth / focal changes by introducing an auxiliary relative depth estimation task during training. To make this depth-aware auxiliary task actually help the main feature task and to keep training stable after adding it, the authors further introduce (1) a multi-level feature fusion with learnable layer weights, and (2) a Huber-style loss for reliability supervision. With these additions, DXFeat keeps almost the same inference-time cost as XFeat, but achieves consistently better matching / relative pose results on benchmarks such as MegaDepth, ScanNet, and HPatches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. DXFeat achieves higher accuracy than the almost identical XFeat architecture on indoor/outdoor relative pose estimation, while keeping the inference speed unchanged.\n2. Compared to XFeat, DXFeat keeps the inference-time network simple and does not introduce extra runtime complexity.\n3. To make the extra relative-depth auxiliary task actually help local feature learning, the authors add two stabilizing modules — (i) multi-level fusion with learnable weights and (ii) a Huber-style loss for reliability supervision.\n4. The paper provides detailed ablation studies showing that each of these components contributes to the final performance."}, "weaknesses": {"value": "1. **Limited gains in sparse matching.** Although DXFeat *consistently* outperforms XFeat in the **semi-dense matching setup**, the improvements become much smaller in the **sparse matching** regime, and on some metrics of MegaDepth and HPatches DXFeat is even slightly below XFeat. \n2. **No downstream visual localization results.** The paper does not report results on downstream tasks such as visual localization, which are the most direct consumers of viewpoint-robust local features. The lack of such an evaluation is a notable omission and makes it harder to judge real-world impact.\n3. **Related work on 3D-/depth-informed matching is under-discussed.** The paper would be stronger if it explicitly positioned DXFeat against recent “match-in-3D” methods such as [1] DUSt3R, [2, 3] (Speedy) MASt3R, and also [4] rectified-features approaches. These works also leverage 3D structure (often via point maps) to improve correspondence quality, so discussing how DXFeat differs would help. Likewise, although LiftFeat is included in experiments, it is highly related and deserves a short discussion in Related Work rather than only appearing in tables.\n\n[1] DUSt3R: Geometric 3D Vision Made Easy\n\n[2] Grounding Image Matching in 3D with MASt3R\n\n[3] Speedy MASt3R\n\n[4] Single-Image Depth Prediction Makes Feature Matching Easier"}, "questions": {"value": "1. DXFeat improves over XFeat much more in semi-dense than in sparse matching. What do you think causes this?\n2. For the learnable layer-wise fusion, do you use just one global scalar per layer (shared for all images)? If so, what are the final learned weights/ratios?\n3. If you keep the depth branch for analysis, what depth quality do you get on ScanNet and HPatches? Could poor/irrelevant depth on HPatches explain why DXFeat is slightly worse than XFeat on some HPatches metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZlNUB62a1m", "forum": "NAN0I3pNWk", "replyto": "NAN0I3pNWk", "signatures": ["ICLR.cc/2026/Conference/Submission24549/Reviewer_SVxp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24549/Reviewer_SVxp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966248176, "cdate": 1761966248176, "tmdate": 1762943119451, "mdate": 1762943119451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}