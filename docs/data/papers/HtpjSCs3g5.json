{"id": "HtpjSCs3g5", "number": 5711, "cdate": 1757928386369, "mdate": 1763692385421, "content": {"title": "PixelCraft: A Multi-Agent system for High-Fidelity Visual Reasoning on Structured Images", "abstract": "Structured images (e.g., charts and geometric diagrams) remain challenging for multimodal large language models (MLLMs), as perceptual slips can cascade into erroneous conclusions. Intermediate visual cues can steer reasoning; however, existing cue-based methods are constrained with low-fidelity image processing and linear, rigid reasoning patterns, limiting their effectiveness on complex structured-image tasks. In this paper, we propose PixelCraft, a novel multi-agent system for high-fidelity image processing and flexible visual reasoning on structured images. The system comprises a dispatcher, a planner, a reasoner, critics, and a set of visual tool agents. To achieve high-fidelity processing, we construct a high-quality corpus and fine-tune an MLLM into a grounding model, whose pixel-level localizations are integrated with traditional computer vision (CV) algorithms in tool agents. \nBuilding on this foundation, PixelCraft facilitates flexible visual reasoning through a dynamic three-stage workflow of tool selection, agent discussion, and self-criticism. \nMoreover, unlike prior linear reasoning patterns that simply append historical images, PixelCraft maintains an image memory to allow the planner to adaptively revisit earlier visual steps, explore alternative reasoning branches, and dynamically adjust the reasoning trajectory during discussion. Extensive experiments on challenging chart and geometry benchmarks demonstrate that PixelCraft significantly improves visual reasoning performance for advanced MLLMs, setting a new standard for structured image reasoning.", "tldr": "We propose PixelCraft, a novel multi-agent system that unifies high-fidelity tool agents with a flexible reasoning workflow for structured images such as charts.", "keywords": ["chart understanding", "multi-agent system", "visual reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f05614b035585dd39423d47ac2f30a6bf7758220.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents PixelCraft, a novel multi-agent system designed to address significant failings in MLLM-based visual reasoning for structured images like charts and geometric diagrams. The authors identify two primary weaknesses in existing methods: low-fidelity image processing, which leads to perceptual errors, and rigid, linear reasoning patterns (like visual CoT) that cannot correct mistakes. PixelCraft's core contribution is a dual-pronged solution. First, it achieves high-fidelity processing by synergizing a compact MLLM, fine-tuned on a new synthetic corpus for precise pixel-level grounding, with classical CV algorithms that act as tool agents. Second, it enables\nflexible, non-linear reasoning through a dynamic workflow involving a planner, reasoner, and critics. A key innovation is the \"image memory,\" which allows the planner to adaptively revisit, recall, and branch from earlier visual steps, facilitating backtracking and exploration of alternative reasoning paths. The authors demonstrate through extensive experiments on challenging chart and geometry benchmarks that their system significantly improves reasoning accuracy for advanced MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated and significant problem. The paper addresses the challenging problem of visual reasoning on structured images, a well-known weak point for current MLLMs. This task is difficult because it requires precise perception of details (like axis values or geometric points) and multi-step logical deduction, where small perceptual errors can cascade into completely wrong answers. The paper clearly identifies these failings as a critical and practical area for improvement.\n\n2. Comprehensive experimental setup. The authors validate their system on a strong suite of recent and difficult benchmarks, including CharXiv, ChartQAPro, and the auxiliary-line subset of Geometry3K. Testing against these diverse datasets demonstrates the system's robustness and generalizability. Furthermore, the comparison is not limited to simple baselines; it includes advanced agentic methods like Debate and Reconcile, which makes the consistent and significant performance gains of PixelCraft more convincing.\n\n3. High-quality presentation and readability. The paper is very easy to read and well-organized, making the complex multi-agent architecture and reasoning process accessible to the reader. The figures are a standout component for presentation; Figure 1 provides an excellent, intuitive comparison of why standard CoT and Visual CoT fail while PixelCraft succeeds, and Figure 2 clearly illustrates the entire agentic workflow, from selection to discussion and correction."}, "weaknesses": {"value": "1. Significant efficiency and latency issues. The multi-agent workflow, which involves a dispatcher, planner, reasoner, and multiple critics, inherently requires numerous sequential LLM inference calls for a single query. This leads to a substantial increase in latency and computational cost compared to simpler CoT methods, as confirmed by the paper's own analysis (e.g., 16.45s vs. 3.75s on CharXiv), which may limit its practical applicability.\n\n2. Limited methodological novelty in the agentic framework. While the application to structured images is effective, the core agentic workflow (combining a planner, tool-use, and a reasoner) is a widely adapted paradigm in recent LLM research. The novelty of the agent framework itself is somewhat incremental, with the main contributions lying in the specialized toolset and its application rather than a fundamentally new agentic reasoning process.\n\n3. Constrained generalizability and flexibility due to a fixed toolset. The system's success is heavily reliant on its manually-curated set of visual tools, which were specifically designed for charts and geometric diagrams. This presents a generalization bottleneck; the system would likely fail in scenarios with novel image types or tasks requiring tools not in its predefined set. This lack of flexibility to adapt or generate new tools on the fly is a key limitation, which the authors also acknowledge."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Pjol7tflOl", "forum": "HtpjSCs3g5", "replyto": "HtpjSCs3g5", "signatures": ["ICLR.cc/2026/Conference/Submission5711/Reviewer_hJXi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5711/Reviewer_hJXi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761607766926, "cdate": 1761607766926, "tmdate": 1762918210660, "mdate": 1762918210660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We thank all reviewers for their thoughtful feedback and careful evaluation of our work. We appreciate that you found visual reasoning on structured images to be an important and timely problem (k4VN, hJXi), that our multi-agent system with high-fidelity tools is both novel and technically sound (yBc8,tt9L), that our experiments are comprehensive and convincing across diverse benchmarks and strong agentic baselines (k4VN, yBc8, hJXi), and that the paper is well written and clearly organized (yBc8,tt9L,hJXi). Taking into account all comments, we have revised the draft and provided point-by-point responses to each reviewer, while also addressing common concerns in the general response below.\n\nSeveral reviewers asked for cost analysis. In the following, we first summarize the main changes since the original submission and then analyze our costs regarding latency and API calls.\n\n### **Revisions**\n\n- Computational cost analysis: We additionally compare the computational cost of PixelCraft with Visual Tool Baselines (Refocus and Visual CoT) and Multi-Agent Baselines (Debate and Reconcile) in **Appendix E**, showing that **PixelCraft is not only effective but also computationally efficient relative to baseline agentic frameworks**. @k4VN, @yBc8, @tt9L, @hJXi\n- Evaluation on a real-world document benchmark: We additionally evaluate PixelCraft on InfographicsVQA (see **Appendix F**) without introducing any extra tools, demonstrating strong generalization to noisy, real-world document/poster images. @yBc8, @k4VN, @hJXi\n\n### **Computational cost analysis**\n\nThere is a general tradeoff that agentic frameworks achieve better performance by incurring higher computational costs with their iterative planning and tool execution compared to the basic CoT. To demonstrate PixelCraft's relative efficiency compared with other agentic frameworks, we conducted a cost analysis comparing it with both Visual Tool Baselines (Refocus, Visual CoT) and Multi-Agent Baselines (Debate, Reconcile).\n\nWe report the average inference latency (seconds per query) and the average number of API calls below. Please note that \"Visual CoT\" here refers to a linear agent baseline equipped with visual tools but simply includes all the historical images for reasoning.\n\n|Method|Latency (s)|API Calls|Accuracy on CharXiv|\n|-|-|-|-|\n|Refocus|14.31|2.0|60.7|\n| Debate |27.65| 7.0 | 62.4 |\n| Reconcile | 25.90| 6.0 | 63.5|\n| Visual CoT | 19.31 | 4.4 | 65.0 |\n| PixelCraft (Ours) | 16.45| 5.7 | **68.1** |\n\n### **Analysis of Efficiency and Performance:**\n\n- **Scalability vs. Multi-Agent Systems:** As shown in the table, PixelCraft demonstrates superior efficiency compared to existing multi-agent frameworks, reducing latency by approximately **36% to 40%** relative to Reconcile and Debate. This efficiency stems from PixelCraft's **flexible reasoning paradigm**. Unlike Debate or Reconcile, which typically enforce a fixed workflow (e.g., fixed debate rounds) regardless of query complexity, our system employs an adaptive workflow. For simpler queries, the Planner can terminate the process early with fewer steps, whereas complex queries trigger deeper reasoning. This adaptability avoids the computational overhead inherent in rigid multi-agent interactions.\n- **Comparison with Linear Agent (Visual CoT):** Crucially, PixelCraft achieves lower latency than the Visual CoT baseline (16.45s compared to 19.31s). This result validates the computational advantage of our **Image Memory** design. Unlike the linear Visual CoT approach, which indiscriminately inputs the entire history of generated images into the context window and thereby increases token count and processing time, our planner selectively recalls only the necessary visual clues. This mechanism ensures a more compact context and faster inference without sacrificing reasoning depth.\n- **Trade-off Analysis vs. Refocus:** While PixelCraft incurs a slight increase in latency compared to Refocus, it yields a substantial performance gain. Specifically, on CharXiv using GPT-4.1-mini, PixelCraft achieves an **accuracy improvement of 7.4%** over Refocus.\n- **Analyze on API calls**: In PixelCraft, we reduce both context length and the planner’s decision burden by introducing a dispatcher, image memory, critics, and an isolated visual reasoner instead of embedding reasoning logic directly inside the planner. This design yields more API calls overall but much shorter contexts per call, which in turn lowers latency. As shown in the table, PixelCraft issues a moderate number of API calls yet attains superior latency efficiency compared to baseline systems.\n\nIn summary, PixelCraft optimizes the balance between cost and performance, delivering substantial accuracy improvements over baselines while maintaining a computational footprint significantly smaller than baseline multi-agent systems. We have incorporated this detailed cost analysis into the revised Appendix E."}}, "id": "pd20ycWsS1", "forum": "HtpjSCs3g5", "replyto": "HtpjSCs3g5", "signatures": ["ICLR.cc/2026/Conference/Submission5711/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5711/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5711/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763691984030, "cdate": 1763691984030, "tmdate": 1763691984030, "mdate": 1763691984030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PixelCraft, a multi-agent system to perform complex reasoning over charts and geometric figures. The multi-agent system features a tool selector, a tool planner, visual critic, plan critic and multiple tool agents. Experiments on chart reasoning benchmarks like ChartXiv and ChartQAPro, and geometry reasoning benchmarks shows substantial improvement over other framework or vaniila visual CoT."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "+ The paper proposes a novel multi-agent system to improve visual reasoning performance on structured image.\n+ Achieves high-fidelity image process with a fine-tuned grounding model\n+ The paper is well written and easy to follow."}, "weaknesses": {"value": "+ My major concern is the cost of the proposed multi-agent system compared with vanilla visual CoT. As shown in Appendix D.4, the response time of the multi-agent system is approximately three or four times longer than baseline methods. Therefore, it would be helpful to reveal the cost (or the number of generated tokens) of PixelCraft in comparision with baseline method. \n\n+ Moreover, it is not clear if the compared baseline methods also use a critic to iteratively refine and improve the final result. If not, I would suggest equipping the baseline methods with a critic to do test-time scaling, and draw a comparison when the respond time/cost is similar."}, "questions": {"value": "See the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PkELlxh2Jk", "forum": "HtpjSCs3g5", "replyto": "HtpjSCs3g5", "signatures": ["ICLR.cc/2026/Conference/Submission5711/Reviewer_tt9L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5711/Reviewer_tt9L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934431014, "cdate": 1761934431014, "tmdate": 1762918210342, "mdate": 1762918210342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new framework designed to improve how multimodal models reason about structured visual content such as charts, diagrams, and geometry problems. It introduces a multi-agent architecture where specialized agents, like a planner, reasoner, and visual tool users, collaborate to analyze images, exchange feedback, and iteratively refine their reasoning. A key feature is a pixel-level grounding module, which allows the system to interact precisely with image regions through actions like zooming, masking, and highlighting. The framework also incorporates an image memory and planning mechanism that enables non-linear reasoning and self-correction. Experiments show that PixelCraft achieves strong performance across visual reasoning benchmarks, outperforming previous multi-agent and single-model approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: This paper integrates multi-agent collaboration with fine-grained visual grounding for structured image reasoning. While the idea of multi-agent reasoning is not entirely new, combining it with pixel-level visual tools and planner-managed image memory adds a modest but meaningful improvement in coordination and interpretability.\n\n2. Quality: The work is technically sound and experimentally thorough. The system design is coherent, and the experiments cover diverse benchmarks with reasonable comparisons and ablations.\n\n3. Clarity: The paper is generally well organized, with clear descriptions of each agent’s role and effective figures that illustrate the workflow.\n\n4. Significance: The contributions are incremental but relevant. The framework addresses practical challenges in visual reasoning and could be adapted to other structured domains, offering a small yet useful step forward for multimodal systems."}, "weaknesses": {"value": "1. Unclear Cost and Scalability\n\nThe multi-agent architecture (planner, critics, tool agents, etc.) implies high computational and communication overhead. The paper doesn’t report inference latency, average agent calls, or cost per sample — critical factors for assessing practical deployability. Since they use a strong vision-language backbone (Qwen2.5-VL), much of the pipeline’s gain might come at the cost of efficiency. Report computation and cost metrics, average number of reasoning turns, total API calls, and latency per image — to give a fair sense of scalability.\n\n2. Overreliance on Synthetic or Narrow Benchmarks\n\nMost experiments use structured-image datasets (e.g., ChartQA, Geometry3K), which are quite specialized. This limits generalization to open-domain or real-world structured reasoning tasks like document or diagram understanding. It’s not shown whether PixelCraft scales to noisy or natural images where structure is less explicit. I suggest adding a small evaluation on broader multimodal datasets (e.g., DocVQA, InfographicsVQA) or including a discussion on how the system might handle unstructured visuals."}, "questions": {"value": "I am confused about the Figure. 4. Why is the usage frequency of Legend Masking and Adding Auxiliary very low, but the performance gain is giant? Most of the time, these tools will not be called."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HKyKjoW6PE", "forum": "HtpjSCs3g5", "replyto": "HtpjSCs3g5", "signatures": ["ICLR.cc/2026/Conference/Submission5711/Reviewer_yBc8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5711/Reviewer_yBc8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961856979, "cdate": 1761961856979, "tmdate": 1762918209958, "mdate": 1762918209958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims at solving visual reasoning on structured images, eg. Charts and geometric diagrams. The authors propose PixelCraft, a multi-agent system comprising of three main componets: query-aware agent selection, agent discussion, and iterative self-correction with a planner managed image memory. To achieve accurate visual grounding and image editing, this work also finetunes Qwen2.5-VL-3B into a visual grounding model based on a synthesized training set, and uses it as a reliable Tool Agent. Experiments on multiple recent datasets demonstrated the superior performance of PixelCraft against competitors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Visual reasoning on structured images is important and has a wide range of applications. This paper proposes an effective multi-agent system for performing visual reasoning on structured images, achieving strong performance across multiple datasets.\n- This paper identifies visual grounding as a key obstacle in performing visual reasoning tasks on structured images, and proposes an effective approach to constructing a high-precision visual grounding model based on synthetic training data and MLLM fine-tuning.\n- This paper provides detailed experimental results that sufficiently demonstrate the effectiveness of each proposed component."}, "weaknesses": {"value": "- It appears that in the current implementation and evaluation, PixelCraft requires pre-specified tool sets for different task types, such as chart reasoning and geometric reasoning, which limits the assessment of PixelCraft’s generalization capability.\n- The paper does not report the computational cost of PixelCraft, particularly in comparison with direct answering and other test-time scaling methods such as Chain-of-Thought (CoT) and Multi-Agent Systems. Comparing both accuracy and computational cost would enable a more comprehensive evaluation of the proposed method’s advantages."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GxEvrxJ6LS", "forum": "HtpjSCs3g5", "replyto": "HtpjSCs3g5", "signatures": ["ICLR.cc/2026/Conference/Submission5711/Reviewer_k4VN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5711/Reviewer_k4VN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762102672721, "cdate": 1762102672721, "tmdate": 1762918209700, "mdate": 1762918209700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}