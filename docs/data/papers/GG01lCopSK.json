{"id": "GG01lCopSK", "number": 5867, "cdate": 1757942453591, "mdate": 1763714932598, "content": {"title": "Conditionally Whitened Generative Models for Probabilistic Time Series Forecasting", "abstract": "Probabilistic forecasting of multivariate time series is challenging due to non-stationarity, inter-variable dependencies, and distribution shifts. While recent diffusion and flow matching models have shown promise, they often ignore informative priors such as conditional means and covariances. In this work, we propose Conditionally Whitened Generative Models (CW-Gen), a framework that incorporates prior information through conditional whitening. Theoretically, we establish sufficient conditions under which replacing the traditional terminal distribution of diffusion models, namely the standard multivariate normal, with a multivariate normal distribution parameterized by estimators of the conditional mean and covariance improves sample quality. Guided by this analysis, we design a novel Joint Mean-Covariance Estimator (JMCE) that simultaneously learns the conditional mean and sliding-window covariance. Building on JMCE, we introduce Conditionally Whitened Diffusion Models (CW-Diff) and extend them to Conditionally Whitened Flow Matching (CW-Flow). Experiments on five real-world datasets with six state-of-the-art generative models demonstrate that CW-Gen consistently enhances predictive performance, capturing non-stationary dynamics and inter-variable correlations more effectively than prior-free approaches. Empirical results further demonstrate that CW-Gen can effectively mitigate the effects of distribution shift.", "tldr": "A novel class of generative models for probabilistic time series forecasting.", "keywords": ["Diffusion Model", "Probabilistic Time Series Forecasting", "Conditional Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff574360efda9f0fd064deacb029726d8af31930.pdf", "supplementary_material": "/attachment/e3d46e6d6b0462114c770a8e689779bef017d2b0.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose to use a separate model to predict sliding-window means and covariances and subsequently use those to whiten to prediction targets of generative forecasting models.\nTheir method improves forecasting results for a broad selection of generative forecasting models on several datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Consistent improvements in experimental results\n1. Clear graphical illustration of method"}, "weaknesses": {"value": "1. As far as I understand from the algorithms in the appendix, conditional whitening with a JMCE is a wrapper or pre/postprocessing around a generative forecasting model. While this can be interwoven with the diffusion/flow matching dynamics in Section 4, this does not seem essential. If I am correct, I think it would be an advantage to highlight the simplicity of the final method."}, "questions": {"value": "1. Do you train the JMCE model jointly with the generative model?\n1. Line 313: How can this be done more efficiently exactly?\n1. Are you modifying the generative models themselves? Or do you train the models on the conditionally whitened data as they are?\n1. Have you ablated the different components of your JMCE loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aZuIPsrVj8", "forum": "GG01lCopSK", "replyto": "GG01lCopSK", "signatures": ["ICLR.cc/2026/Conference/Submission5867/Reviewer_wUr1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5867/Reviewer_wUr1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760994720103, "cdate": 1760994720103, "tmdate": 1762918312907, "mdate": 1762918312907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces conditionally whitened generative models that incorporate information in the noising process. The authors tailor diffusion and flow matching processes by including conditional information obtained via a mean and covariance estimator (termed JMCE). Their approach is theoretically motivated and empirically demonstrated to improve generative performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The methodology is theoretically justified, and the authors show under which conditions the distance between the prior and conditional distribution is minimized.\n- A simple mean and covariance estimator is introduced and well-motivated to parametrize the generative process.\n- The proposed conditional whitening leads to empirical performance improvements.\n- The framework includes diffusion and flow matching."}, "weaknesses": {"value": "- The model requires a two-stage process now. First fit JMCE, then train the generative model.\n- The derived diffusion process requires the inversion of the covariance matrix, resulting in a higher runtime complexity compared to standard diffusion models. A runtime comparison would aid the comparison.\n- The limitations of the method should be discussed more thoroughly. Furthermore, I recommend separating the related work section from the introduction.\n\nMinors:\n\n- L260: Sentence incomplete"}, "questions": {"value": "- Can the model be trained in an end-to-end fashion or does it require a two-stage process?\n- Did you try diagonal covariance parametrizations to reduce the runtime complexity?\n- Can you elaborate more on the performance of the JMCE model itself? How does it compare in forecasting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZvL7jyJNsd", "forum": "GG01lCopSK", "replyto": "GG01lCopSK", "signatures": ["ICLR.cc/2026/Conference/Submission5867/Reviewer_pWon"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5867/Reviewer_pWon"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656184239, "cdate": 1761656184239, "tmdate": 1762918312557, "mdate": 1762918312557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CW-Gen, which whitens data using learned conditional means and covariances from a Joint Mean-Covariance Estimator before training generative models. The idea to align the model priors with data statistics is sound and yields consistent empirical gains across datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Tackle a challenging, important and underrepresented aspect for time-series modelling: multivariate time-series.\n- Clear motivation: Proposes a method that allows for a new prior, that is closer to the data-distribution.\n- Well written and theoretically founded.\n- Extensive evaluation, which show that the proposed prior actually improves state-of-the art models."}, "weaknesses": {"value": "- Unclear connection of DKL reduction in Theorem 1 to actual practical guarantees: Can you (a) clarify the tightness of the bound; (b) give intuition / toy examples showing when the condition is achievable (or not); (c) explicitly discuss regimes where the condition can fail\n- Unclear estimator quality: Covariance targets are noisy; stability and regularization not analyzed.\n- The paper notes that CW requires eigen-decomposition per sample / per time step but then glosses over practical limitations. More precise complexity analysis and discussion of remedies (approximate eigen/svd, diagonal + low-rank approximation, block-diagonalization, randomized SVD or factor models) could benefit the paper.\n- The paper argues joint mean+full covariance is important. But it’s not fully convincing which component drives gains. There are some ablations (backbone/wEigen) but I don’t see a simple controlled ablation that compares: (a) subtract mean only, (b) subtract mean and scale by diagonal variance (NsDiff style), (c) full covariance JMCE. Can you add a direct ablation and show where full covariance helps."}, "questions": {"value": "- Can you be very explicit on how your proposed method compare to other (univariate) flow-based and diffusion-based methods that introduced flexible priors, e.g., [1,2]\n- Can you add a short experiment showing numeric values of both sides of Equation (3) on training/validation examples so readers can see how far the estimators are from satisfying the bound in practice.\n- Choice of sliding window length (95 for most datasets, 15 for ILI) needs justification. Why 95? How sensitive are results to this hyperparameter?\n\n\n1. Modeling temporal data as continuous functions with stochastic process diffusion, ICLR 2023\n2. Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting, ICLR 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JYKhvjX7IZ", "forum": "GG01lCopSK", "replyto": "GG01lCopSK", "signatures": ["ICLR.cc/2026/Conference/Submission5867/Reviewer_skbS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5867/Reviewer_skbS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749784284, "cdate": 1761749784284, "tmdate": 1762918312228, "mdate": 1762918312228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CW-Gen, including CW-Diff and CW-Flow. It proposes a novel JMCE to simultaneously estimate the conditional mean and sliding-window covariance of future time series, which guides the whitening process. The authors provide theoretical guarantees showing conditions under which their approach improves the generative model's sample quality by reducing the KL divergence between the conditional distribution and the model's terminal distribution. Experimental evaluations demonstrate improvements in probabilistic forecasting accuracy, capturing non-stationary dynamics and inter-variable correlations better than baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper establishes conditions that justify why replacing the traditional terminal Gaussian distribution with one parameterized by estimated conditional mean and covariance improves sample quality.\n2. The JMCE simultaneously learns accurate conditional means and sliding-window covariances with eigenvalue control to ensure stability and robustness—this nuanced approach effectively addresses non-stationarity and heteroscedasticity.\n3. Experiments show the outperformed performance over baselines.\n4. The algorithms and theoretical proofs are clearly detailed, and code for experiment reproduction is available."}, "weaknesses": {"value": "1. The approach involves computationally expensive operations, particularly eigen-decomposition for whitening covariance matrices, which scales cubically with dimensionality. For very high-dimensional datasets, CW-Gen can become quite slow, limiting real-time deployment scenarios.\n2. The framework's reliance on complex joint estimators and whitening transformations may impose a higher barrier to adoption compared to simpler baseline models.\n3. The paper ablates hyperparameters but does not isolate the individual contributions of conditional mean estimation versus covariance estimation. Which component drives most improvements?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4pqEpzgYOC", "forum": "GG01lCopSK", "replyto": "GG01lCopSK", "signatures": ["ICLR.cc/2026/Conference/Submission5867/Reviewer_Nwt6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5867/Reviewer_Nwt6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974616487, "cdate": 1761974616487, "tmdate": 1762918311902, "mdate": 1762918311902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}