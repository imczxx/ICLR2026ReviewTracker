{"id": "W2NINfoVtN", "number": 5352, "cdate": 1757903612649, "mdate": 1759897980097, "content": {"title": "VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By Value Sign Flip", "abstract": "We introduce Value Sign Flip (VSF), a simple and efficient method for incorporating negative prompt guidance in few-step (1-8 steps) diffusion and flow-matching image generation models. Unlike existing approaches such as classifier-free guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by flipping the sign of attention values from negative prompts. Our method requires only a small computational overhead and integrates effectively with MMDiT-style architectures such as Stable Diffusion 3.5 Turbo and Flux Schnell, as well as cross-attention-based models like Wan. We validate VSF on challenging datasets with complex prompt pairs and demonstrate superior performance in both static image and video generation tasks. Experimental results on our proposed dataset NegGenBench show that VSF significantly improves negative prompt adherence (reaching 0.420 negative score for quality settings and 0.545 for strong settings) compared to prior methods in few-step models, which scored 0.320-0.380 negative score, and even CFG in non-few-step models (scored 0.300 negative score), while maintaining competitive image quality and positive prompt adherence. Our method is also a suppressed generate-then-edit pipeline, while also having a much faster runtime. Code, ComfyUI node, and dataset will be released. Videos generated are in the Supplementary Material.", "tldr": "", "keywords": ["Image Generation", "Diffusion Models", "Negative Guidance"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9c55d41f92d2632edccbc08770d7a537be9c19f9.pdf", "supplementary_material": "/attachment/c3902a3f55dba879a320144a84027771cd29a4c3.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Value Sign Flip (VSF), a lightweight, training-free negative-guidance method for few-step diffusion/flow models that flips the sign of negative-prompt value vectors inside attention. For cross-attention models VSF concatenates positive/negative keys/values and applies a \\alpha scaling to the negative values; for MMDiT-style models (e.g., SD 3.5 Turbo, FLUX Schnell) it further duplicates the negative tokens and masks attention paths so only image -> negative interactions are affected, optionally adding a bias to stabilize quality. VSF adapts token-, layer- and step-wise, aiming to avoid oversaturation and “mixing” failures seen when forcing CFG or using fixed-strength attention methods like NASA and NAG. On a new negation benchmark (NegGenBench) built from challenging positive/negative prompt pairs, VSF improves negative adherence while keeping quality and positive adherence competitive, and runs in ~3 s with few steps; human and MLLM-based evaluations broadly agree with these trends."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed VSF is simple and plug-and-play. And it does not include many runtime overhead.\n- It performs better than baseline methods on the collected benchmarks. It achieves better negative-prompt compliance at similar quality vs. NAG/NASA and even CFG (multi-step) in reported settings. The visualization results look visually good.\n- Clear ablations/trade-offs (α, β, masking/duplication, Whole-Embedding Flip)"}, "weaknesses": {"value": "- Evaluation relies heavily on MLLM judges (LLaMA/Qwen variants) for pos/neg/quality scoring; such metrics can be biased or insensitive to artifacts, and the dataset/hyperparameters are author-curated. \n- Some quality trade-offs remain at higher negative strength; masking/bias choices introduce extra knobs and implementation complexity in MMDiT stacks. \n\nWriting:\n-  Figure 3, 5, 6 is too small, could be scaled up for better organization."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ODYxATiGFf", "forum": "W2NINfoVtN", "replyto": "W2NINfoVtN", "signatures": ["ICLR.cc/2026/Conference/Submission5352/Reviewer_2X36"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5352/Reviewer_2X36"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948815227, "cdate": 1761948815227, "tmdate": 1762918019929, "mdate": 1762918019929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Since previous negative prompts approach generally employ score matching, they are not compatible with few-step generation models.\n- To address the problem, they simply flip the value in the attention layer, which results in effectively removing the undesired contents in the final images."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is simple but effective\n- The proposed method can be applied to a few-step model."}, "weaknesses": {"value": "- The proposed method is not novel. Manipulating attention has been employed for image editing with diffusion models and the flow-matching model. (e.g., [Attend-and-exit], [self-guidance], [BoxDiff])\n- In Figure 5, VFS not only eliminates the undesired contents but also changes the other components. Specifically, in the starry night examples, the city has gone. Also, in Figure 5(right), the car is still reflected in the image with the proposed method.\n- Figures should be well illustrated. ( font size of figure 3,5 is too small, and figure 6 is too small)\n\n[Attend-and-exit]: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models\n[self-guidance]: Diffusion Self-Guidance for Controllable Image Generation\n[BoxDiff]: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion"}, "questions": {"value": "- Compared to the previous works using attention manipulation, does the proposed method contain a specific technique or contribution for being compatible with few-step models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kwBjOVkJ1g", "forum": "W2NINfoVtN", "replyto": "W2NINfoVtN", "signatures": ["ICLR.cc/2026/Conference/Submission5352/Reviewer_qk8d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5352/Reviewer_qk8d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976725330, "cdate": 1761976725330, "tmdate": 1762918019674, "mdate": 1762918019674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Value Sign Flip (VSF): a negative guidance.\n\nNegative guidance is useful in generative pipelines: progressively adjusting the results by removing something from the image.\n\nProblem statement:\n* CFG does not work in few-step configurations.\n* Negative Steer Away Attention (NASA) is currently limited to cross-attention models.\n* Normalized Attention Guidance (NAG) primarily targets quality control rather than avoiding negative prompts.\n* NASA and NAG subtract negative attention.\n* They do not generalize to various timesteps, layers, or image regions.\n\nMethod (VSF)\n* (Cross-attention-based models) Flipping the sign of negative prompt values within the attention calculation.\n* (DiT models) Duplicating negative prompts, one remains unflipped, another is flipped.\n\nAdvantages\n* VSF removes some concepts from generated images: wheels from bicycle, hands from clock, etc.\n    * maintaining image quality and adherence to the positive prompts\n* VSF works in few-step (1~8 steps) diffusion and flow matching models.\n* VSF generalizes to SD 3.5 turbo, FLUX schunell, and Wan.\n* VSF is computationally cheap.\n\nNew dataset: NegGenBench\n* Positive-negative prompt pairs from ChatGPT o3.\n* Negative prompts are core components from positive prompt; it is intentionally challenging.\n\nEvaluation: fine-tune a VLM (Qwen) for measuring faithfulness to negative prompts"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\n1. The proposed method is new.\n\nQuality:\n1. The related work section covers relevant literature: CFG, Negative Guidance, and Few-step generators\n2. The competitors are aggressively chosen, even Nano Banana.\n3. Discussion is thorough\n    1. trade-off between positive and negative prompts\n    2. trade-off between quality and negative prompts\n    3. attention maps\n    4. ablation study\n\nClarity:\n1. The explanations are kind to the readers, step-by-step from NASA to the proposed method.\n\nSignificance:\n1. The method is simple and effective.\n    1. simple: concatenate the values and keys of the positive and negative prompts, then flip the sign of the negative prompt values\n    2. effective: Table 2"}, "weaknesses": {"value": "minor\n1. Please properly use \\citet and \\citep\n2. Fonts are too small in the figures.\n3. Is “unbrulla” a typo? or is there a message?"}, "questions": {"value": "1. Why does NASA have points with negative score below 50 only in Figure 6?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FXEE8j82Jb", "forum": "W2NINfoVtN", "replyto": "W2NINfoVtN", "signatures": ["ICLR.cc/2026/Conference/Submission5352/Reviewer_emZq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5352/Reviewer_emZq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762083960826, "cdate": 1762083960826, "tmdate": 1762918019426, "mdate": 1762918019426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a simple approach for negative guidance of the text-to-image (T2I) models. Current approaches flip the sign of the attention output for this, but this ends up on applying the same scale of the negative guidance across different areas of the image, and all different layers of the model. Instead, they use the attention map to calculate a per-token wieght for this negative guidance. They first develop this for approaches that use cross-attention mechanism (like latent diffuoin architecture), and then propose a new mechanism to adapt this to multimodal diffusion transformer (MMDiT)-based models like SD3, and others."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea of this paper is simple, but I like how they have distilled knowledge from the literature, and based on that—as well as their solid understanding of the attention mechanism—they have proposed this simple idea."}, "weaknesses": {"value": "1) There are some grammatical errors and confusing parts in the paper that need to be addressed:\n- Should $x_{t-1}$ be $x_{t+1}$ in Eq. (1)?\n- line 166: *\"The method NASA applies the guidance in intermediate states instead of the predicted noise or velocity.\"* — this statement is somewhat ambiguous.\n- line 188: \"*However, it also limits the model’s ability to follow negative prompt guidance if the constraint is set to be too tight ...*\" — this sentence could be improved for clarity and readability.\n\n2) The concepts used to illustrate the issue with generation quality in Figure 2, under the presence of negative and positive prompts, are not optimal. Since winter and snow are strongly related concepts, they may be entangled in the diffusion model’s learned distribution. Using one as a positive and the other as a negative prompt may not clearly demonstrate the intended issue with current models, as the observed effect could stem from this conceptual dependence rather than the model’s capability to interpret negative guidance.\n\n3) This paper mentions that \"*rendering prompts containing negations ineffectively or made the negative prompt appears even more (e.g., a prompt like “a scientist who is not wearing glasses” will often generate a scientist with glasses—sometimes even more frequently than a simple prompt like “a scientist”).*\".  \nThere are some fairness-oriented approaches, such as ITI-Gen [1] and FairQueue [2], that discuss related issues. They point out that this cannot be addressed using **hard prompts**, but can be mitigated through prompt learning. While I understand that your setup is different, discussing the similarities and differences between your approach and these works could strengthen the paper.\n\n4. I believe the task of **negative guidance** can be viewed as a special case of **image editing**, where the prompt explicitly describes the removal of a concept while no input image is provided. From this perspective, using Qwen-Image as an external baseline (Table 1) is an interesting choice. Given that it can edit images while keeping other regions intact, it could serve as an informative upper bound for the negative guidance task. The performance gap could also highlight directions for future research. However, the details of how Qwen-Image is used, including the experimental setup, should be included in the main paper (at least at a high level).\n\n$ $\n\n*References:*\n\n[1] ITI-GEN: Inclusive Text-to-Image Generation, ICCV'23\n\n[2] FairQueue: Rethinking Prompt Learning for Fair Text-to-Image Generation, NeurIPS'24"}, "questions": {"value": "Please check the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZaAlKluDb8", "forum": "W2NINfoVtN", "replyto": "W2NINfoVtN", "signatures": ["ICLR.cc/2026/Conference/Submission5352/Reviewer_DjfS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5352/Reviewer_DjfS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762343182221, "cdate": 1762343182221, "tmdate": 1762918019195, "mdate": 1762918019195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}