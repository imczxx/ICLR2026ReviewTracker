{"id": "9X8u6tXXAF", "number": 17099, "cdate": 1758272094433, "mdate": 1759897197920, "content": {"title": "Relation-Aware Graph Foundation Model for Few-shot Learning", "abstract": "In recent years, large language models (LLMs) have demonstrated remarkable ability to generalize across diverse natural language processing (NLP) tasks. Inspired by this success, graph foundation models (GFMs) have emerged as a promising direction in graph learning, aiming to achieve cross-dataset generalization through large-scale pre-training. However, unlike language models that rely on explicit token representations, graphs lack a well-defined unit for generalization, making it challenging to design effective pre-training strategies. In this work, we propose REEF, a novel GFM framework that leverages relation tokens as the fundamental units. Analogous to token vocabularies in LLMs, we construct a vocabulary of relation tokens to encode relational information within graphs. To accommodate diverse relations, we introduce two hypernetworks that adaptively generate the parameters of aggregators and classifiers in graph neural networks based on relation tokens. In addition, we design another hypernetwork to construct dataset-specific projectors and incorporate a dataset-level feature bias into the initial node representations, enhancing flexibility across different datasets with the same relation. Furthermore, we adopt graph data augmentation and a mixed-dataset pre-training strategy, allowing REEF to capture relational diversity more effectively. Extensive experiments show that REEF consistently outperforms existing methods on both pre-training and transfer learning tasks, and demonstrates strong generalization in few-shot transfer scenarios, underscoring its potential as a powerful foundation model for graph-based applications.", "tldr": "", "keywords": ["Graph foundation models", "Graph neural networks"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/849e274bb9f203003b5946b47519c0346f37297d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In the paper, the author proposed REEF, a relation-aware GFM model for few-shot graph learning. Specifically, in REEF, each different relation across different datasets is treated as an independent token and combined together for joint training. The feature of each relation is initialized using LLM. Node features from each dataset are generated by SVD with augmentation from the dataset description. The model architecture of REEF is based on RGCN with a joint update for both node and relation representation. Finally, the model is pretrained on relation prediction tasks. The author evaluated the pretrained model on multiple baselines under supervised and few-shot settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The writing is clear and easy to follow.\n- The few-shot performance of the proposed method is good."}, "weaknesses": {"value": "- The biggest concern in my mind is the claim of relation-awareness. The author spends much space on introducing the vocabulary of relation tokens. However, it is still unknown to me why the relation is more transferable than other components in the graph. Is it simply because there is a more common relation type across different datasets? However, given all the datasets authors are considering, only the citation network shares a relation to some extent. \n- With relation as the basic vocabulary of GFM, the REEF still uses LLM to generate the original embedding for each relation. What's the main difference between REEF's way and previous methods like OFA, ZeroG? \n- The REEF initializes the node feature using SVD, but adds the LLM-generated dataset description as augmentation. I am wondering what the rationale behind it is? Why not initialize all features using LLMs? \n- The REEF requires a training independent classifier for each different relations, I think it is not generalizable enough.\n- There lack ablation studies to investigate the effectiveness of each model component in REEF, especially some experiments to show the effectiveness of the relation vocabulary."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZslythqdBn", "forum": "9X8u6tXXAF", "replyto": "9X8u6tXXAF", "signatures": ["ICLR.cc/2026/Conference/Submission17099/Reviewer_4jEJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17099/Reviewer_4jEJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620758883, "cdate": 1761620758883, "tmdate": 1762927101709, "mdate": 1762927101709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a novel graph foundation model for few-shot learning. Inspired by LLM, this work propose to use a language model to encode the edge/relations in different datasets with a unified relation vocabulary and use hypernetworks to generate network weights for learning and aggregating messages. The method shows improvements on various setting. It also shows reasonable ability to scale up w.r.t. the size and diversity of the input data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of using relation as the central vocabulary is novel, it provides a different lens into understanding the graph universal modeling problem. \n\n- The hypernetwork design is also interesting. Compared to changing edge feature, this explicitlies modify the message-passing/aggregation process, which can make knowledge transfer easier, given that the hypernetwork is well-trained."}, "weaknesses": {"value": "The key weakness I observe is that the innovation of the paper is not clear. This appears to me a combination of multiple existing techniques. Specifically:\n\n- The overall universal graph learning framework is not new. From using LLM to generate relation feature, using SVD to unify input feature, to pre-training over multiple graph dataset. These have already been widely explored by many related works mentioned by the author. This new framework does not provide guideline or principle in designing a graph foundation model.\n\n- The hypernetwork, while new to the graph foundation model, is an extensively explore topic, and it has been directly applied to GNN research.\n\n- The network design itself is also not novel (RGCN).\n\nThe paper also has overclaiming issue:\n\n- While the concept of foundation model can be different to different people, a foundation model should at least possess some level of training-free generalizability. The paper indicates in line 281, that this work still always needs downstream finetuning, which breaks the whole point of having a foundation model.\n\n- The transfer learning only happens on the few-shot scenario, no zero-shot ability, which is an ability that many previous work possess, and the author also mentioned these work. (many works in table 3 can do zero-shot) This shows that the work actually is less generalizable.\n\n- The pre-training scaling improvement is not significant, the figure exaggerate the improvememnt a bit. Especially considering fluctuation in pubmed and citeseer, I am skeptical about whether this model can scale."}, "questions": {"value": "- What happend to cases where the node feature size is smaller then the pre-defined dimension? This is very realistic, as some dataset can be nominal dataset, with only few features.\n\n- How do you consturct the target graph relations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GyeViSwWfH", "forum": "9X8u6tXXAF", "replyto": "9X8u6tXXAF", "signatures": ["ICLR.cc/2026/Conference/Submission17099/Reviewer_nJAg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17099/Reviewer_nJAg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761693734625, "cdate": 1761693734625, "tmdate": 1762927101425, "mdate": 1762927101425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **REEF**, a graph foundation model that leverages **relation tokens** as the core unit of generalization. The framework employs **three hypernetworks** to adaptively generate:  \n1. Relation-specific aggregators  \n2. Relation-specific classifiers  \n3. Dataset-specific projectors, along with **dataset-level feature biases**.\n\nREEF aims to enhance transferability and generalization across multiple domains by focusing on the dynamic generation of these components tailored to specific relations and datasets. Empirical results demonstrate that the model performs well across various tasks and settings, showcasing strong transferability and generalization abilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The architecture elegantly handles both **relation-level** and **dataset-level** variations. The separation of concerns—**relation-specific parameters** versus **dataset-specific adaptations**—is both conceptually sound and practically effective. This decoupling allows for more flexible and adaptable models.\n\n-  The paper presents comprehensive experiments across **four domains** and **ten datasets**, with various evaluation settings including **pre-training performance**, **transfer learning**, **few-shot scenarios**, and **LM-based node features**. This robust experimental setup strengthens the paper’s empirical foundation, showcasing the model's flexibility and broad applicability."}, "weaknesses": {"value": "- While the paper presents the method as a **relation-token-based Graph Foundation Model (GFM)**, it closely resembles **heterogeneous GNNs** (e.g., RGCN). The main innovation seems to be the use of **hypernetworks** to generate relation-specific aggregators and classifiers. The paper does not clearly explain why **token-like** relation representations bring benefits similar to those of tokens in **Large Language Models (LLMs)**, beyond serving as inputs to hypernetworks. Further clarification of this aspect would help distinguish REEF from existing GNN-based methods.\n\n- The gains observed in the experiments are mostly within **similar domains** and tasks. It remains unclear how well the approach generalizes to **domains and tasks** significantly different from those used in pre-training. The paper lacks compelling evidence regarding **robust out-of-domain generalization**. More experiments or discussions on this front would strengthen the paper's claims.\n\n-  As a foundation model, **strong generalization**, especially in **zero-shot settings**, is expected. The paper primarily focuses on **few-shot results** but does not include **zero-shot performance** on unseen datasets or tasks. Reporting performance in zero-shot settings would make the **foundation model** claim more convincing and show the model's ability to generalize without task-specific fine-tuning."}, "questions": {"value": "- In line 73, the paper argues that **node-** or **dataset-level units** are too heterogeneous to capture universal knowledge and concludes that relations are therefore more coherent and transferable. However, relations themselves may vary significantly across domains (e.g., citation networks like **arXiv** vs. social networks like **Reddit**). The current argument primarily relies on same-domain examples and doesn't fully justify the general conclusion that relations are a universally superior fundamental unit for pre-training. Could the authors provide broader evidence or a more nuanced characterization of why relations should be considered a more universal unit across diverse domains?\n\n-  How are relations not present in the pre-training vocabulary handled at inference time? Is there a **zero-shot mechanism** in place to **compose or infer parameters** for genuinely unseen relations from textual descriptions alone? Clarifying how the model handles this scenario would provide insight into its robustness and real-world applicability.\n\n- What is the primary task in the main experiments—**link prediction** or **node classification**? The paper mentions both tasks, and it would be helpful to have a clear delineation of the main evaluation tasks and protocols. This clarity will help the reader better understand the context of the experimental results.\n\n- Why was **SVD** chosen for **feature alignment** (Equation 2) instead of using **learnable projections**? Have **alternative alignment strategies**, such as **linear** or **non-linear learned projectors**, been compared? What are the trade-offs in using SVD over these other methods?\n\n- The related work section discusses both **LLM-based** and **GNN-based GFMs**. However, have you compared REEF against recent **LLM-based baselines** like **GOFA** [1] or **TEA-GLM** [2]? Including such comparisons (or explaining why they are incompatible) would help contextualize REEF's contribution and further highlight its novelty.\n\n### References:\n1. **GOFA**: A Generative One-for-All Model for Joint Graph Language Modeling.  \n2. **TEA-GLM**: LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yFvegT5odL", "forum": "9X8u6tXXAF", "replyto": "9X8u6tXXAF", "signatures": ["ICLR.cc/2026/Conference/Submission17099/Reviewer_LsVj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17099/Reviewer_LsVj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716003042, "cdate": 1761716003042, "tmdate": 1762927101026, "mdate": 1762927101026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents REEF, a relation-aware graph foundation model that introduces relation tokens as the fundamental units for pretraining. Inspired by the success of LLMs, REEF uses hypernetworks conditioned on relation and dataset representations to parameterize aggregators, classifiers, and projectors, aiming to achieve strong generalization and transferability in few-shot learning. Extensive experiments show that REEF outperforms existing methods across multiple domains and settings."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "S1: Proposes relation tokens as foundational units for GFMs, bridging a conceptual gap between NLP token-based models and graph data.\n\nS2: Employs three hypernetworks to handle relation-specific and dataset-specific variations, enhancing flexibility and generalization.\n\nS3: Comprehensive experiments show significant improvements in both pretraining and few-shot transfer settings, with clear comparisons to strong baselines.\n\nS4: Offers a scaling law analysis demonstrating how pretraining data scale affects performance.\n\nS5: Shows robustness in cross-domain and text-attributed graph scenarios, outperforming TAG-specific baselines."}, "weaknesses": {"value": "The work lacks a formal justification or theoretical insight into why relation tokens should serve as the universal unit of generalization over nodes or datasets.\n\nWhile REEF achieves strong results, there's little discussion on interpretability or insight into what specific relation tokens learn or capture in the pretrained model.\n\nThe paper would benefit from citing and comparing to several relevant works to strengthen its position within the graph foundation and relation modeling literature. \n\nWhile REEF hinges on the idea of encoding relation tokens via sentence-level language models (Sentence-BERT), it does not rigorously justify why textual descriptions are sufficient or optimal for capturing complex structural semantics of graph relations. Many real-world relations (e.g., co-purchase patterns or knowledge graph triples) have ambiguous or domain-specific meanings not easily distilled into natural language. A deeper discussion or ablation using other encoding mechanisms (e.g., learned embeddings, ontology-based encoders) is warranted.\n\nThe mixed-dataset pretraining strategy, while shown to work well overall, does not explore negative transfer risks in heterogeneous domains. The paper observes (e.g., Figure 3) that adding PubMed to Citeseer degrades transfer to Cora, yet does not quantify or analyze domain conflicts. A more nuanced treatment of domain interference—possibly via inter-dataset similarity metrics or dataset ablation studies—would add rigor.\n\n\nREEF’s design includes multiple hypernetworks and dynamic parameter generation, yet offers no interpretability or visualization of what the model learns via relation tokens. For example: Are some relations more transferable than others? Do the relation embeddings form meaningful clusters? Can relation tokens be reused or generalized across tasks?"}, "questions": {"value": "I think there is a conceptual and methodological connection between this work (REEF) and graph prompting, even though the authors don’t explicitly frame it in those terms. e.g. Prompt Analogy via Relation Tokens, Parameter Modulation via Prompts, etc. I wonder what do the authors think of it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gm3YTdCDAp", "forum": "9X8u6tXXAF", "replyto": "9X8u6tXXAF", "signatures": ["ICLR.cc/2026/Conference/Submission17099/Reviewer_mjCs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17099/Reviewer_mjCs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968860925, "cdate": 1761968860925, "tmdate": 1762927100335, "mdate": 1762927100335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}