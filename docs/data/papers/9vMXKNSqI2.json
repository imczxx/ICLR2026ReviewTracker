{"id": "9vMXKNSqI2", "number": 7147, "cdate": 1758009668508, "mdate": 1759897870239, "content": {"title": "Towards Principled Task Grouping for Multi-Task Learning", "abstract": "Multi-task learning (MTL) aims to leverage shared information among tasks to improve learning efficiency and accuracy. However, MTL often struggles to effectively manage positive and negative transfer between tasks, which can hinder performance improvements. Task grouping addresses this challenge by organizing tasks into meaningful clusters, maximizing beneficial transfer while minimizing detrimental interactions. \nThis paper introduces a principled approach to task grouping in MTL, advancing beyond existing methods by addressing key theoretical and practical limitations. Unlike prior studies, our method offers a theoretically grounded approach that does not depend on restrictive assumptions for constructing transfer gains. We also present a flexible mathematical programming formulation that accommodates a wide range of resource constraints, thereby enhancing its versatility.\nExperimental results across diverse domains, including computer vision datasets, combinatorial optimization benchmarks, and time series tasks, demonstrate the superiority of our method over extensive baselines, thereby validating its effectiveness and general applicability in MTL without sacrificing efficiency.", "tldr": "This paper presents a theoretically sound task grouping method that optimizes knowledge transfer between related tasks and significantly outperforms existing baselines across multiple domains.", "keywords": ["Multi-task Learning", "Task Grouping"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/669027aa95ba49bc6e8f909426550dd395362629.pdf", "supplementary_material": "/attachment/621c108477255e2a06580f2a1211b28a82840764.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies task grouping by organizing tasks into coherent clusters. It provides theoretical guarantees for task affinity measures and formulates task grouping as a mathematical programming problem."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a theoretical analysis of task grouping and proposes methods to construct task groups based on transfer gain and affinity."}, "weaknesses": {"value": "1. The paper’s idea of forming task groups using transfer gain or task affinity is not new. Prior work [2] has already explored transfer-gain or affinity based relationships between tasks and groups. Compared to the main baseline [1], the proposed method does not appear highly novel at a high level, since it still learns separate networks for clusters of tasks based on task affinity.\n\n[1] Efficiently Identifying Task Groupings for Multi-Task Learning (NeurIPS 2021)\n\n[2] Selective Task Group Updates for Multi-Task Optimization (ICLR 2025)\n\n2. The scalability of this work is quite limited. The experiments are conducted mainly with ResNet-18. The proposed approach and experimental settings do not seem practical for modern architectures that achieve strong generalization across diverse tasks with very large parameter counts."}, "questions": {"value": "Given that your results are based on ResNet-18, how does the method scale to large scale models such as ViTs, VLMs, and LLMs where strong zero-shot baselines may leave limited headroom for group-based improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CQ6oMoAONC", "forum": "9vMXKNSqI2", "replyto": "9vMXKNSqI2", "signatures": ["ICLR.cc/2026/Conference/Submission7147/Reviewer_7BHe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7147/Reviewer_7BHe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928070922, "cdate": 1761928070922, "tmdate": 1762919310596, "mdate": 1762919310596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of grouping tasks together for multi-task learning (MTL) to maximize the reduction in loss due to positive transfers between tasks. The paper makes three main contributions. First, it introduces a variant of Task Affinity Grouping (TAG)  (Fifty et al., 2021) to measure transfer gains between pairs of tasks. Second, it proves theoretically that averaging pairwise gains within a group with this formulation is approximation of the gains from the group. Third, it formulates the grouping problem with constraints as a Mixed Integer Program. The paper evaluates the transfer predictions and grouping approaches on widely used benchmark datasets (Taskonomy, CelebA, COP, ETTm1), showing improvement over TAG and an earlier approach, HOA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Task grouping for MTL is an important problem.\n* The proposed approach seems to outperform TAG (and some other baselines) in terms of finding groups with positive transfers.\n* Showing that the average of pairwise transfers is an approximation of the group transfer is an important result.\n* Considering various constraints on grouping is beneficial, and the experiments are interesting."}, "weaknesses": {"value": "* The significance of the Mixed Integer Program is not entirely clear. Prior work on task grouping introduced various approaches designed for this problem (e.g., MTG-Net (Song et al., 2022) uses a branch-and-bound-like algorithm to search for the optimal groupings when the number of tasks is low, and it selects near-optimal groupings via a beam-search approach with polynomial complexity when the number of tasks is high). Compared to prior work, the MIP formulation and the application of a generic solver seems like a step backwards. The consideration of complex constraints (and the experimental results on them) is interesting, but the computational approach seems rather straightforward.\n* While TAG, HOA, PCGrad, etc. are important approaches from the literature, they are not necessarily the SOTA in task grouping. MTG-Net and Grad-TAG (Song et al., 2022; Li et al., 2024) are more recent and claim to attain better results. While the paper includes results for some of these baselines (for some of the datasets) in the appendix, the results presented in the main text focus on older methods. Given that some of the newer methods, such as MTG-Net, have very competitive performance, their results should be included in the main text. Also, it would be good to evaluate MTG-Net on all datasets since it can be applied to pretty much any MTL problem.\n* While the approximation result is nice, the bound does grow proportionally with the number of tasks. So the bound is not that useful in the sense that the prediction error is additive as expected.\n* More importantly, the result applies to an affinity metric (called transfer gain), which is not the same as the actual gain from MTL. From a practical perspective, the real metric is how much lower is the loss of an MTL model trained using a group of tasks compared to the loss of an STL model. While the affinity metric is of course closely related to this real objective metric, it is not obvious how useful the result of Proposition 1 is for this objective metric.\n* It would be good clarify in the paper that the MTL baseline in Figure 1 means training all tasks together in a single group."}, "questions": {"value": "Is there a particular reason for not applying MTG-Net to all the datasets? Seems like a low-hanging fruit."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e3uMnkD18h", "forum": "9vMXKNSqI2", "replyto": "9vMXKNSqI2", "signatures": ["ICLR.cc/2026/Conference/Submission7147/Reviewer_diQN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7147/Reviewer_diQN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141243878, "cdate": 1762141243878, "tmdate": 1762919309640, "mdate": 1762919309640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of task grouping in Multi-Task Learning (MTL), introducing a principled methodology for grouping tasks to optimize positive transfer while mitigating negative interactions. The authors develop a robust, assumption-free metric for transfer gain among tasks, improving upon previous methods such as TAG by eliminating restrictive convexity assumptions. Furthermore, they propose a generic, mathematically rigorous Mixed Integer Programming framework for grouping tasks, capable of incorporating diverse resource and structure constraints. Extensive empirical evaluations on computer vision, combinatorial optimization, and time series benchmarks demonstrate significant improvements over existing task grouping and MTL baseline models. Key contributions include computationally efficient optimization strategies, exhaustive ablation studies, and visual analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The task grouping problem is recast as a flexible mathematical programming formulation (Section 4.2), which smoothly incorporates rich real-world constraints. The proposed formulation is a practical contribution for future work and industrial deployment of MTL systems.\n\n2.The experiments in Section 5.3 and Figure 2 highlight the adaptability and reliability of the mathematical grouping solver under budget and size constraints, further validating the utility of the proposed optimization formulation."}, "weaknesses": {"value": "1. Comparisons to Surrogate and LinearSurrogate baselines are uneven—sometimes included, sometimes omitted. It is not fully clear whether all relevant task grouping strategies (particularly those from recent literature) were run on all benchmarks.\n2. Although the derivations culminating in Proposition 1 and its bound (Section 4.1 and Appendix A) are clear, theoretical assumptions (bounded and Lipschitz losses) may be unrealistic in practical deep learning settings, limiting the bound's applicability.\n3. Key figures (Figure 1, Figure 2) lack statistical variability measures (e.g., confidence intervals), potentially misrepresenting model robustness. Especially for the ETTm1 and COP domains, reporting means without dispersion may give a misleading sense of robustness."}, "questions": {"value": "1.Could the authors elaborate on the impact of loss normalization, batch size, and stochasticity on the empirical quality of cumulative transfer gain $\\mathcal{S}_{i \\rightarrow j}$, especially when task outputs are heterogeneous?\n\n2.Regarding Equation (8) and related constraints, could you provide a concrete worked example mapping the mathematical constraints to a specific real-world scenario, such as grouping tasks on a multi-GPU node with both memory and compute limitations?\n\n3.What is the precise mechanism to avoid degenerate groupings (e.g., all tasks in one group, singleton groups) when the grouping constraints are relaxed? Have you observed pathological solutions in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xzTvsGCCmW", "forum": "9vMXKNSqI2", "replyto": "9vMXKNSqI2", "signatures": ["ICLR.cc/2026/Conference/Submission7147/Reviewer_3JKb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7147/Reviewer_3JKb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762428916448, "cdate": 1762428916448, "tmdate": 1762919308552, "mdate": 1762919308552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents principled task grouping for multi-tasking learning problems. The proposed method formulates task grouping as a flexible mathematical programming problem that can incorporate practical resource constraints, thus avoiding strong assumptions used in prior work. Experimental results in vision, combinatorial optimization, and time-series tasks demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The experimental results demonstrate the effectiveness of the proposed method.\n2. The ideas behind the proposed approach are simple yet effective.\n3. The paper is well-written and easy to understand."}, "weaknesses": {"value": "1. Compared baselines do not contain more releases after 2023.\n2. There should be more discussions on the definition of $\\mathcal{S}$. Its formulation is quite similar to classic methods based on distance functions or the covariance matrix. How is $\\mathcal{S}$ different from these related approaches?\n3. The performance gain in some tasks is not that evident. Thus, a profound analysis of these obtained results should be conducted in the manuscript."}, "questions": {"value": "1. How does the proposed method perform when compared with approaches published after 2023?\n2. There should be more discussions on the definition of $\\mathcal{S}$. Its formulation is quite similar to classic methods based on distance functions or the covariance matrix. How is $\\mathcal{S}$ different from these related approaches?\n3. When compared with other baselines, the performance gain of the proposed approach is not that evident. Are there any reasons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "psuswAF2J6", "forum": "9vMXKNSqI2", "replyto": "9vMXKNSqI2", "signatures": ["ICLR.cc/2026/Conference/Submission7147/Reviewer_whPc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7147/Reviewer_whPc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762866337660, "cdate": 1762866337660, "tmdate": 1762919308094, "mdate": 1762919308094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}