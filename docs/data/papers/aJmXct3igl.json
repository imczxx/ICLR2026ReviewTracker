{"id": "aJmXct3igl", "number": 14097, "cdate": 1758228473255, "mdate": 1759897389969, "content": {"title": "Type-Compliant Adaptation Cascades", "abstract": "Reliably composing Large Language Models (LLMs) for complex, multi-step workflows remains a significant challenge. The dominant paradigm---optimizing discrete prompts in a pipeline---is notoriously brittle and struggles to enforce the formal compliance required for structured tasks. We introduce Type-Compliant Adaptation Cascades (TACs), a framework that recasts workflow adaptation as learning typed probabilistic programs. TACs treat the entire workflow, which is composed of parameter-efficiently adapted LLMs and deterministic logic, as an unnormalized joint distribution. This enables principled, gradient-based training even with latent intermediate structures. We provide theoretical justification for our tractable optimization objective, proving that the optimization bias vanishes as the model learns type compliance. Empirically, TACs significantly outperform state-of-the-art prompt-optimization baselines. Gains are particularly pronounced on structured tasks, improving FinQA from $12.0\\%$ to $24.7\\%$ for a Qwen 3 8B model, MGSM-SymPy from $57.1\\%$ to $75.9\\%$ for a Gemma 2 27B model, MGSM from $1.6\\%$ to $27.3\\%$, and MuSR from $36.5\\%$ to $62.6\\%$ for a Gemma 7B model. TACs offer a robust and theoretically grounded paradigm for developing reliable, task-compliant LLM systems.", "tldr": "We introduce Type-Compliant Adaptation Cascades (TACs), treating an entire typed workflow as a single probablistic program parametrized by lightweight PEFT modules, allowing end-to-end training with latent variables.", "keywords": ["language model adaptation", "probabilistic programming", "reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3ce09f421b5d43d895a27fc9d4e24bc07320759c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Type-Compliant Adaptation Cascades, a framework that adapts multi-step LLM workflows by modeling them as typed probabilistic programs. A training algorithm, TACSTaR, enables gradient-based optimization under type constraints. The approach achieves strong improvements on structured reasoning and QA tasks, outperforming prompt-optimization and untyped baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Conceptual novelty: The paper reframes multi-step LLM workflows as typed latent-variable models, bridging probabilistic programming with LLM adaptation.\n2. Principled optimization: The proposed TACSTaR algorithm generalizes STaR under type constraints and offers theoretical guarantees for unbiased learning as compliance improves.\n3. Strong empirical validation: Experiments demonstrate consistent and large gains across reasoning, structured generation, and classification tasks."}, "weaknesses": {"value": "1. Incomplete ablation of theoretical components: The contributions of type compliance, unnormalized optimization, and amortized inference are intertwined. A controlled study isolating each factor’s impact (e.g., removing Theorem-justified bias correction) would strengthen causal attribution.\n2. Accessibility and implementation clarity: Although the system is well-explained, the probabilistic program abstraction may be difficult for non-probabilistic ML practitioners to reproduce. Public release of code or example workflows would greatly improve reproducibility."}, "questions": {"value": "1. The proposed framework relies heavily on the assumption that type compliance can be accurately estimated during training. How sensitive is the method to imperfect or noisy type parsers in practice?\n2. Figure 1 is difficult to interpret and could be organized more clearly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rNTnWI1T60", "forum": "aJmXct3igl", "replyto": "aJmXct3igl", "signatures": ["ICLR.cc/2026/Conference/Submission14097/Reviewer_Hdqp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14097/Reviewer_Hdqp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760527662332, "cdate": 1760527662332, "tmdate": 1762924573601, "mdate": 1762924573601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes integrating a probabilistic program with an LLM to constrain generation by a typed workflow and improve structured reasoning.\n\nBy casting the workflow as a directed acyclic hypergraph, it decomposes the task into node-level subprograms (LLM adaptors or deterministic steps). From the program perspective, this means breaking global fine-tuning/prompting into local, trainable adaptors that support differentiation and probabilistic query.\n\nAs a probabilistic program, it defines an unnormalized joint as the product of local factors; they drop the global partition gradient and rely on the facts that when type-compliance mass is 1 the optimum matches normalized MLE, and when compliance is high the gradient bias is provably small.\nThey use importance sampling (IS) in two distinct ways to make this practical: (i) a local, inference-time IS+renormalization classifier at finite-label nodes, and (ii) a global, training-time self-normalized IS (SNIS) that reweights amortized (x,y)-conditioned proposals by the model’s unnormalized joint, which turns rationalization into a principled E-step.\n\n\nOverall, this makes type compliance efficient without global rejection sampling, and it improves reasoning accuracy on structure-heavy tasks because training concentrates on validator-consistent traces and on gold-conditioned latent proposals supplied by amortized inference."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) Reframes typed LLM pipelines as trainable probabilistic programs, enabling posterior tricks such as node-level IS + renorm at inference and full-trace SNIS with amortized proposals at training. \n\n(2) Decomposes generation into typed subprograms with deterministic validators, using a bias-controlled objective (drop $\\nabla_\\theta \\log Z$) for tractable learning and SNIS to correct proposal mismatch.\n \n(3) Supports both finite-label outputs (benefiting at training and inference) and free-form outputs (benefiting primarily at training)."}, "weaknesses": {"value": "(1) Reasoning remains local: Amortization helps training only. No global posterior search at test time.\n\n(2) Training–inference gap persists. Proposal–posterior mismatch for SNIS is unquantified during training; inference lacks sampling-quality  diagnostics.\n\n(3) Design sensitivity to node granularity. No principled guidance on where/when to introduce nodes (too few ⇒ weak structure; too many ⇒ compounding failure/variance)\n\n(4) Diversity unmeasured; risk of mode seeking. With type-compliance → 1, outputs may become valid-but-repetitive;\n\n(5) Writing: The narrative thread breaks around §2.2, where definitions, algorithms, and theory are interleaved without signposting, so the exposition reads like an info dump. Abstraction levels are mixed (types/parse–canon/δ-edges alongside amortized TAC) before the base typed program is internalized. The ordering also conflates roles (forward program vs. inference (local IS) vs. training (SNIS + amortized)), making it hard to see why each component appears and how it is used."}, "questions": {"value": "(1) Add diagnostics for SNIS proposal-posterior gaps during training and inference IS quality. \n\n(2) What's the sweat spot of node granularity? Intuitively, TAC with few long nodes behave like global sequence FT/sampling, and many short nodes may degenerate with greedy steps.\n\n(3) Add diversity/collapse checks under rising type-compliance.\n\n(4) Confirm validators aren’t doing the heavy lifting. no label leakage and doesn't solve the structure task per se."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jdAiFLI321", "forum": "aJmXct3igl", "replyto": "aJmXct3igl", "signatures": ["ICLR.cc/2026/Conference/Submission14097/Reviewer_nsoG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14097/Reviewer_nsoG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761030397653, "cdate": 1761030397653, "tmdate": 1762924573062, "mdate": 1762924573062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Type-Compliant Adaptation Cascades (TACs), a framework for training multi-step LLM workflows as probabilistic programs rather than relying on discrete prompt tuning. Each step in the workflow is treated as a typed probabilistic transformation, allowing gradient-based optimization while enforcing type correctness. The method, TACSTaR, shows strong improvements over DSPy-style prompt optimization and the original STaR algorithm on reasoning-heavy benchmarks such as MGSM, FinQA, and MuSR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I am not an expert in this field. Here are my non-expert comments:\n- The idea of treating LLM workflows as typed probabilistic programs is well-motivated.\n- The paper is clearly written and provides both theoretical justification and practical implementation details. \n- Empirical results seems good, showing consistent and often large gains over prompt-optimization baselines."}, "weaknesses": {"value": "I am not an expert in this field. Here are my non-expert comments:\n- I believe that computational cost and scalability are not discussed in details.\nSince i am not an expert, I don t know if baselines are missing or not."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "iWhfnyM5XE", "forum": "aJmXct3igl", "replyto": "aJmXct3igl", "signatures": ["ICLR.cc/2026/Conference/Submission14097/Reviewer_Vw9m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14097/Reviewer_Vw9m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919256509, "cdate": 1761919256509, "tmdate": 1762924572619, "mdate": 1762924572619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of composing LLM calls in a relatable and efficient way. The authors of the paper introduce Type-Compliant Adaptation Cascades (TACs), which is a framework that recasts the adaptation of the workflow as a typed probabilistic program. A key part is that this makes it possible to use gradient-based training. The paper also introduces a training algorithm TACSTAR, and shows certain convergence criteria. The approach is evaluated using different LLMs (Gemma and Qwen) and compared against state-of-the-art prompt optimization frameworks, such as DSPy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents an interesting and novel approach based on the Self-Taught Reasoner (STaR) algorithm for adaptation, using the MC-EM approach underneath.\n\n- The key idea is that the TAC approach can make use of gradient-based learning, compared to other existing approaches, such as DSPy.\n\n- The experiments and the evaluation questions in section 4 are reasonable and well articulated"}, "weaknesses": {"value": "- The paper emphasizes the typed aspect of the approach, but it is unclear what this means more formally from a programming language perspective. Does it support only primitive types (int, bool, floats, etc.), or compound types (tuples, records, etc), or recursive types (e.g., algebraic data types), or function types (for higher order functions), or a combination of these? That is, the typing aspect of the work is not that clearly articulated.\n\n- The evaluation seems to only show the resulting evaluation results, not the computational costs for achieving these results. For example, if DSPy's prompt optimization systems are allowed to run for a longer time, the results will most likely become better. The same is probably true for TAC. That is, as part of the evaluation and experimental results, it is also important to show how much computing time was needed for the different experiments, and how this compares with the accuracy results."}, "questions": {"value": "- Please clarify how the approach connects to probabilistic programming, besides the fact that the problem is defined as a graph? That is, is the connection to PPLs that it is a Bayesian problem overall? It is not clear in what way the user interacts, and actually constructs a PPL program, in the sense of standard sample and observe statement, or if everything is automated under the hood.\n\n- What kind of programs can be constructed, and what are the limitations? Can, e.g., universal PPLs be encoded, where branching and/or recursion is dependent on previous samples, or is the overall program in the end a fixed graph?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ylAyelQzP7", "forum": "aJmXct3igl", "replyto": "aJmXct3igl", "signatures": ["ICLR.cc/2026/Conference/Submission14097/Reviewer_yicQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14097/Reviewer_yicQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998286339, "cdate": 1761998286339, "tmdate": 1762924572123, "mdate": 1762924572123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}