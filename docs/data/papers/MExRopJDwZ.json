{"id": "MExRopJDwZ", "number": 6136, "cdate": 1757954077010, "mdate": 1759897933673, "content": {"title": "Cross-Domain Offline Policy Adaptation via Selective Transition Correction", "abstract": "It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse dynamics model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics. Since limited data may result in inaccurate model training, we further employ a forward dynamics model to retain corrected samples that better match the target dynamics than the original transitions. Consequently, we propose the Selective Transition Correction (STC) algorithm which enables reliable usage of source domain data for policy adaptation. Experiments on various environments with dynamics shifts demonstrate that STC achieves superior performance against existing baselines.", "tldr": "A simple method that corrects source domain transitions to fulfill alignment with the target domain dataset.", "keywords": ["reinforcement learning", "cross-domain", "off-dynamics"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/19059219becc322410bc457e92962959e02075a2.pdf", "supplementary_material": "/attachment/c815a3258a64e303cba6437245d639b1d64081b3.zip"}, "replies": [{"content": {"summary": {"value": "This paper propose modifying the source domain data into the target domain data for better cross-domain learning and sample efficiency in cross-domain offline RL. The method learns an inverse policy model, a reward model and a forward dynamics model to achieve this modification and unified them into an algorithm called Selective Transition Correction (STC). Theoretical analysis and empirical evaluations are conducted to validate the framework's validity and effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-structured, with clear introduction of the methodology and experimental setup.\n\n- The paper proposes modifying source domain transitions to align with the target domain, as opposed to filtering data. In principle, this method is expected to improve the efficiency of data usage."}, "weaknesses": {"value": "- The core idea is intuitive and reasonable, yet the proposed algorithm appears to be a forced assembly of distinct modules, lacking rigorous validation. In particular, the integration of the forward dynamics model undermines the paper’s logical coherence, while the designs of the inverse policy model and reward model also become largely meaningless.\n\n- There is a large disconnect between the theory part and the proposed algorithm. While the THEORETICAL ANALYSIS aims to verify the validity of the inverse policy model and reward model, the authors subsequently state that \"Since Dtar contains limited data, the learned inverse policy model may be unreliable in OOD regions.\" Yet they still need to incorporate a separate forward dynamics model. Ultimately, the analysis loses its value in guiding algorithm design and appears to be a proof created solely for the purpose of writing a proof.\n\n- In my view, the three assumptions lack realism or have weak relevance to the experimental design. Take Assumption 1 as an example: it mandates that the dynamics difference between the source and target domains must be small. Yet the experiments provide no indication of how large this discrepancy actually is, making it impossible to verify the proposed bounds. For Assumption 2, considering the scarcity of target domain data, there is no explanation for how the assumption could hold—whether in a rigorous or even just an intuitive sense.\n\n- The experiments use 5 seeds for repetition, but as illustrated in Figure 3, the standard deviations during learning are remarkably large. Such high variability, combined with the small number of runs, reduces the persuasiveness of the results. Moreover, the parameters are selected via a cherry-picking approach from a parameter search, and the authors even switch to different values post-search. This makes it difficult to believe the algorithm can generalize to other tasks when using the same parameters.\n\n- The method requires learning three separate models, which makes it excessively heavy in terms of computational complexity. Additionally, the design logic behind these models is not sufficiently clear. Most notably, no experiments are conducted to justify the rationale for designing these specific models or to demonstrate the advantages of such a design. See Questions for more details."}, "questions": {"value": "- Given the limited target domain data, how can we validate that the inverse policy model and reward model have been well learned?\n\n- After the selection process, how many modified data samples remain?\n\n- How to measure the difference between the two domains? What impact does this discrepancy have on selective correction and the final results? And does this align with the theoretical analysis?\n\n- The inverse policy model always outputs a modified action, even when $s_{src}$ and $s'_{src}$ do not belong to the target domain. How should this case be handled? And does the theoretical analysis account for it?\n\n- The reward model uses a first-order Taylor expansion around the original action. Why not predict the reward directly, similar to how the action is predicted?\n\n- Line 191 states: \"If the inverse policy model is sufficiently accurate, the corrected transition is expected to better align with the underlying dynamics of the target domain compared to the original transition.\" However, Line 274 mentions: \"Note that we still include source transitions with large discrepancies for training since we believe there are still some underlying shared behaviors embedded in those data that can be beneficial for policy learning.\" Do these two statements conflict?\n\n- Please refer to the \"Weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fXgZhj23d2", "forum": "MExRopJDwZ", "replyto": "MExRopJDwZ", "signatures": ["ICLR.cc/2026/Conference/Submission6136/Reviewer_Zx6W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6136/Reviewer_Zx6W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749922216, "cdate": 1761749922216, "tmdate": 1762918493616, "mdate": 1762918493616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "STC is a novel method for cross-domain offline RL, which modifies the source domain data into the target domain data."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* STC can explicitly make the source data align with the target dynamics.\n* STC achieves superior performance against existing baselines."}, "weaknesses": {"value": "*  The method trains an inverse policy and a reward model, which may bring more computational burden."}, "questions": {"value": "* In Equation (4), could you clarify how to implement and obtain the $\\hat{r}\\_{\\text{src}}$? \n  Specifically, how do you compute the term \n  $\\nabla\\_a r(s\\_{\\text{src}}, a)^\\top \\big|\\_{a = a\\_{\\text{src}}} (\\hat{a}\\_{\\text{src}} - a\\_{\\text{src}})$?\n\n* The target domain dataset contains only 5,000 transitions. \n  I am concerned about whether the inverse policy model, reward model, and forward dynamics model \n  can be sufficiently trained under such limited data. \n  Have you tried using different numbers of transitions? \n  Is there a large performance difference?\n\n* Could the authors include comparisons and a discussion with recent cross-domain offline RL studies (e.g., PSEC, DmC)? Incorporating these baselines and analysing the differences would strengthen the paper and better position the proposed method within the current literature.\n\nReference:\n\nLiu, T., Li, J., Zheng, Y., Niu, H., Lan, Y., Xu, X., Zhan, X. Skill expansion and composition in parameter space. In International Conference on Learning Representations, 2025. \n\nVan, L. L. P., Nguyen, M. H., Kieu, D., Le, H., Tran, H. T., & Gupta, S. DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain Reinforcement Learning. arXiv preprint arXiv:2507.20499."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UNZtFEXPWH", "forum": "MExRopJDwZ", "replyto": "MExRopJDwZ", "signatures": ["ICLR.cc/2026/Conference/Submission6136/Reviewer_hVvb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6136/Reviewer_hVvb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751022128, "cdate": 1761751022128, "tmdate": 1762918493281, "mdate": 1762918493281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the challenge of adapting offline RL policies across domains with mismatched dynamics by introducing Selective Transition Correction (STC), a method that corrects source domain transitions by leveraging an inverse policy model and a forward dynamics model trained on the target domain, specifically aligning actions and estimated rewards to the target environment. A selection mechanism filters corrections based on consistency with the target dynamics. Experiments demonstrate that STC typically outperforms several baselines across various MuJoCo tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper theoretically analyzes the dynamics and value discrepancy induced by transition corrections with explicit assumptions and supporting proofs.\n\nThe paper conducts extensive experiments on various task domains and compares with sufficient baselines."}, "weaknesses": {"value": "The success of STC depends heavily on the quality of the inverse policy and forward dynamics models. Insufficient or low-diversity target data will likely lead to suboptimal or even detrimental corrections.\n\nThe Taylor expansion in section 4.1 assumes local smoothness and is clipped for stability, but the limitations of this approximation—especially in highly nonlinear reward landscapes—have not yet been explored empirically or theoretically. Such approximations may produce inaccurate rewards for modified transitions, potentially introducing bias or overconfidence in out-of-distribution (OOD) settings.\n\nSome common ideas have been investigated in other cross-domain RL settings, such as the correction module in CAT, and sequence consistency in [2]\n\nFor clarity, some notation is inconsistent (see e.g., mixing $\\widetilde{\\mathcal{M}}{\\text{src}}$, $\\widehat{\\mathcal{M}}{\\text{src}}$, and $\\mathcal{M}_{\\text{tar}}$ across proofs and main sections) that may confuse readers.\n\n[1] Cross-domain adaptive transfer reinforcement learning based on state-action correspondence\n\n[2] Cross Domain Policy Transfer with Effect Cycle-Consistency"}, "questions": {"value": "Could the approach extend to online learning settings or non-dynamics (observation) mismatch scenarios? \n\nCould the authors provide more insight into the computational overhead introduced by training/using three separate models (inverse, forward, reward) for transition correction and selection? How is the computation cost compared to other baselines? Would this scale to larger or more complex domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cQaG4rJZ2s", "forum": "MExRopJDwZ", "replyto": "MExRopJDwZ", "signatures": ["ICLR.cc/2026/Conference/Submission6136/Reviewer_fRdm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6136/Reviewer_fRdm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762182172025, "cdate": 1762182172025, "tmdate": 1762918492928, "mdate": 1762918492928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}