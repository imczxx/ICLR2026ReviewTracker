{"id": "aH7eyx64pC", "number": 15262, "cdate": 1758249465989, "mdate": 1759897317394, "content": {"title": "OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning", "abstract": "Recent advancements in multimodal slow-thinking systems have demonstrated remarkable performance across various visual reasoning tasks. However, their capabilities in text-rich image reasoning tasks remain understudied due to the absence of a dedicated and systematic benchmark. To address this gap, we propose OCR-Reasoning, a novel benchmark designed to systematically assess Multimodal Large Language Models on text-rich image reasoning tasks. Specifically, OCR-Reasoning comprises 1,069 human-annotated examples spanning 6 core reasoning abilities and 18 practical reasoning tasks in text-rich visual scenarios. Unlike existing text-rich image understanding benchmarks that only provide a final answer, this benchmark additionally provides a detailed step-by-step reasoning process. This dual annotation enables the evaluation of both the models' final answers and their reasoning processes, thereby offering a holistic assessment of text-rich reasoning capabilities. By leveraging this benchmark, we conducted a comprehensive evaluation of the latest MLLMs. Our results demonstrate that even the most advanced MLLMs exhibit substantial difficulties in text-rich image reasoning tasks, with none achieving an accuracy above 50\\% on our benchmark, indicating that the challenges of text-rich image reasoning are an urgent issue to be addressed. The dataset and evaluation scripts will be made publicly available.", "tldr": "", "keywords": ["multimodal slow-thinking systems;text-rich image understadning;reasoning model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ceb7c119df6dd6f21cf47f7b54430e76b5c37ba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reveals severe capability gaps in current advanced multimodal slow-thinking systems when performing text-rich image reasoning tasks, attributing this limitation fundamentally to the absence of a systematic and specialized evaluation benchmark. To address this challenge, this paper introduces the OCR-Reasoning benchmark, characterized by its large-scale manual annotations, coverage of multi-dimensional reasoning tasks, and core feature of \"dual annotation\" providing both answers and detailed step-by-step reasoning processes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper accurately identifies a crucial gap in multimodal reasoning research by proposing the first benchmark specifically designed for systematically evaluating \"text-rich image reasoning\" capabilities\n﻿\nThe constructed OCR-Reasoning benchmark surpasses traditional models that focus solely on answer correctness. By incorporating annotations of step-by-step reasoning processes, it achieves dual evaluation of both model reasoning paths and final answers, providing a more comprehensive and profound perspective for diagnosing model capability shortcomings."}, "weaknesses": {"value": "1. The study primarily focuses on diagnostic evaluation but does not validate the effectiveness of its proposed \"thinking with images\" reasoning approach. A critical question remains unanswered: can explicitly training models to generate such visual reasoning chains on the proposed benchmark lead to significant gains in reasoning performance? The paper would be significantly strengthened by including fine-tuning experiments that demonstrate whether and how leveraging this benchmark for training, as opposed to merely for evaluation, improves model capabilities on text-rich reasoning tasks.\n\n2. The paper would benefit from a more thorough comparative analysis between the proposed OCR-Reasoning dataset and existing text-rich benchmarks (e.g., TextVQA, DocVQA, ChartQA). While the novel \"dual annotation\" is highlighted, a quantitative and qualitative comparison is needed to clearly delineate its advantages. This should explicitly detail the dataset's superiorities in terms of data quality (e.g., the depth and consistency of reasoning chain annotations), data characteristics (e.g., the diversity and complexity of reasoning types beyond simple QA), and coverage (e.g., the inclusion of scenarios that require multi-hop reasoning).\n\n3. Although the benchmark's quality is commendable, its scale (approximately 1,069 examples) may limit its comprehensiveness and statistical power. The covered visual domains might not fully represent the vast spectrum of real-world text-rich images. To enhance the robustness and generalizability of the findings, the authors could consider expanding the dataset to include more diverse sources, such as web screenshots (for UI/UX reasoning), financial documents (for complex table and report understanding), academic papers, or product manuals. This would ensure that the benchmark tests reasoning capabilities across a broader range of practical contexts.\n\n4. The benchmark is constructed manually, which ensures high quality but is not scalable for generating large-scale training data. A discussion on potential methodologies for scalable training data construction is a crucial missing piece. The authors could propose and discuss semi-automatic or synthetic data generation techniques that could produce large volumes of text-rich image reasoning data. For instance, exploring how advanced models (like GPT-4V) could assist in drafting reasoning chains for human review, or how to create synthetic tasks that inherently require visual-textual reasoning, would greatly enhance the practical utility and impact of this work beyond evaluation."}, "questions": {"value": "see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mbf00RcoUt", "forum": "aH7eyx64pC", "replyto": "aH7eyx64pC", "signatures": ["ICLR.cc/2026/Conference/Submission15262/Reviewer_KL6Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15262/Reviewer_KL6Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761469000498, "cdate": 1761469000498, "tmdate": 1762925563177, "mdate": 1762925563177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the OCR-Reasoning benchmark, designed to evaluate the reasoning capabilities of Multimodal Large Language Models (MLLMs) in complex text-rich image scenarios. Unlike existing benchmarks that focus on text extraction, OCR-Reasoning includes both final answers and step-by-step reasoning processes for 1,069 annotated examples across six core reasoning categories. The benchmark reveals significant limitations in current MLLMs, with top models achieving only around 50% accuracy in reasoning tasks, highlighting struggles in integrating visual, textual, and logical information. The study emphasizes the importance of Chain-of-Thought prompting and provides valuable insights into common failure modes such as calculation, spatial comprehension, and logical errors. This benchmark aims to inspire future advancements in multimodal reasoning systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper demonstrates strong originality by introducing a new benchmark, OCR-Reasoning, that evaluates multimodal large language models on text-rich image reasoning—a domain previously underserved by existing datasets focused mainly on text extraction. Its dual annotation of final answers and reasoning steps represents a creative and meaningful extension of prior benchmarks.\n\nIn terms of quality, the study employs a rigorous and transparent methodology, including systematic dataset curation, expert annotation, and comprehensive evaluation across a broad spectrum of state-of-the-art models. The experimental design is sound, with clear baselines, detailed performance breakdowns, and thoughtful error analysis.\n\nThe paper exhibits strong clarity, with a well-structured presentation, clear motivation, and informative figures that effectively illustrate dataset design and results. The writing is precise and technically competent, enabling easy understanding of both the problem and its importance.\n\nRegarding significance, the work makes a timely and impactful contribution to the multimodal reasoning community. By exposing the current limitations of MLLMs in integrating textual and visual reasoning, it establishes an essential benchmark that will likely guide future research on improving model reasoning and evaluation frameworks."}, "weaknesses": {"value": "The dataset is relatively small (1,069 samples), limiting generalization and coverage of diverse real-world scenarios. The reliance on LLM-as-Judge introduces potential bias; incorporating human or cross-model validation would improve reliability. The paper lacks deeper diagnostic analysis explaining why models fail, and provides limited quantitative comparison with prior benchmarks. Finally, details on dataset release and reproducibility are insufficient, which may hinder adoption."}, "questions": {"value": "1. Dataset Scale and Coverage:\nCould the authors clarify whether there are plans to expand OCR-Reasoning beyond 1,069 samples? A larger and more diverse dataset (e.g., multilingual, domain-specific, or handwritten documents) would improve the benchmark’s representativeness.\n2. Evaluation Bias in LLM-as-Judge:\nHow do the authors mitigate potential bias when using LLMs to evaluate reasoning quality, especially if the judging model shares architecture or training data with the tested models? Would cross-model or partial human evaluation be feasible for validation?\n3. Reasoning-Type Analysis:\nThe paper shows category-wise results but lacks deeper diagnostics. Could the authors provide finer-grained error analyses or ablations—for example, separating failures due to OCR errors, visual reasoning, or logical inference?\n4. Comparison with Existing Benchmarks:\nIt would be helpful to include a more direct experimental comparison or transfer evaluation with existing datasets (e.g., DocVQA, ChartQA, OCRBench). How does OCR-Reasoning specifically challenge models beyond these benchmarks?\n5. Reinforcement Learning Methods:\nThe paper notes that RL-based methods perform poorly. Could the authors elaborate on how a better reward design for text-rich reasoning might look, or what specific factors caused these RL models to fail?\n6. Dataset Accessibility and Reproducibility:\nPlease clarify the intended release details—license, format, annotation schema, and evaluation scripts. Ensuring full reproducibility will significantly strengthen the paper’s long-term impact.\n7. Future Directions:\nThe authors mention possible improvements in reward design and dataset expansion. Could they outline a concrete roadmap for how OCR-Reasoning might evolve into a standardized benchmark suite for multimodal reasoning?\nThese clarifications and extensions could meaningfully strengthen the paper’s rigor, reproducibility, and long-term value to the community."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q0E89LXagb", "forum": "aH7eyx64pC", "replyto": "aH7eyx64pC", "signatures": ["ICLR.cc/2026/Conference/Submission15262/Reviewer_eZdh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15262/Reviewer_eZdh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761520182768, "cdate": 1761520182768, "tmdate": 1762925562356, "mdate": 1762925562356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OCR-Reasoning, a benchmark for evaluating MLLMs’ text-rich image reasoning, with 1,069 human-annotated examples (6 core reasoning abilities, 18 tasks) and annotations of both reasoning processes and answers (unlike existing benchmarks only annotating answers). Evaluations show top closed-source MLLM  doesn’t exceed 50% on it, while open-source ones perform worse."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Filling text-rich image reasoning evaluation gaps: Existing text-rich image benchmarks focus on text extraction but lack systematic reasoning assessment. OCR-Reasoning addresses this, measuring MLLMs’ reasoning in practical scenarios.\n2. Sample design forcing reasoning: Few answers in its samples are directly extractable from OCR results; models must actively reason, avoiding reliance on text extraction to truly reflect their reasoning levels.\n3. Comprehensive annotations for in-depth evaluation: Unlike benchmarks that only annotate final answers, OCR-Reasoning  labels both reasoning processes and answers, enabling holistic analysis of models’ problem-solving abilities"}, "weaknesses": {"value": "1. Limited dataset scale: Most of the data collection and annotation processes rely on manual work, and the high associated costs result in the dataset scale being only comparable to previous methods, failing to achieve larger-scale expansion"}, "questions": {"value": "1. OCR-Reasoning annotates both reasoning processes and final answers, while existing text-rich image benchmarks (e.g., DocVQA, OCRBench) mostly only annotate final answers and their samples’ answers are often directly extractable from OCR results. What are the specific core differences between OCR-Reasoning and these benchmarks in terms of sample design and annotation logic? What key role does this difference play in evaluating the true reasoning capabilities of MLLMs?\n2. The paper states that existing reinforcement learning (RL) methods perform poorly on OCR-Reasoning, due to mismatched reward functions and a disconnect between training data and the benchmark’s scenarios. Does the study propose preliminary improvement directions (e.g., specific reward function design ideas, training data selection criteria)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nBZvZX72aK", "forum": "aH7eyx64pC", "replyto": "aH7eyx64pC", "signatures": ["ICLR.cc/2026/Conference/Submission15262/Reviewer_YFNZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15262/Reviewer_YFNZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567057560, "cdate": 1761567057560, "tmdate": 1762925561737, "mdate": 1762925561737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses the lack of systematic benchmarks for evaluating Multimodal Large Language Models (MLLMs) in text-rich image reasoning. It proposes OCR-Reasoning, a benchmark with 1,069 human-annotated examples covering 6 core reasoning abilities and 18 practical tasks, featuring both final answers and step-by-step reasoning processes. Extensive evaluations of LLMs, MLLMs, and document-oriented MLLMs show that no model achieves over 50% accuracy, with image input outperforming OCR text alone and CoT prompting benefiting most models. The core contributions include the first reasoning process-annotated benchmark for text-rich images, systematic model evaluation, and identification of key improvement directions."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. OCR-Reasoning is the first benchmark to systematically assess reasoning processes in text-rich image scenarios, addressing a long-overlooked need.\n2. The comprehensive evaluation includes multiple model categories and zero-shot settings, ensuring generalizable results.\n3. Detailed error analysis and qualitative case studies deepen understanding of model limitations beyond accuracy metrics."}, "weaknesses": {"value": "1. While the handwritten data in OCR-Reasoning provides valuable transcribed college-level STEM problems, it would be beneficial to consider incorporating more everyday real-world handwritten scenarios to further enhance the benchmark's coverage of diverse text-rich reasoning tasks commonly encountered in practice.\n\n2. The paper presents an interesting observation that CoT prompting may have backfired on VL-Rethinker-7B, potentially due to conflicting built-in reflection mechanisms. It would strengthen this finding if the authors could provide additional ablation studies or experiments to further validate this hypothesis and better understand the underlying mechanisms.\n\n3. The human validation for the LLM-as-Judge method demonstrates careful evaluation on DouBao-1.5-Vision-Pro. To further establish the robustness of this evaluation approach, it would be valuable to extend the validation across additional models and reasoning categories, which could help address potential concerns about judge bias and generalizability of the assessment methodology."}, "questions": {"value": "The zero-shot evaluation effectively demonstrates out-of-the-box model capabilities. Have the authors explored or considered exploring few-shot prompting or fine-tuning scenarios on OCR-Reasoning? Could such experiments provide insights into whether models achieve substantial improvements with modest amounts of task-specific guidance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h4mO3sFF9d", "forum": "aH7eyx64pC", "replyto": "aH7eyx64pC", "signatures": ["ICLR.cc/2026/Conference/Submission15262/Reviewer_cM84"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15262/Reviewer_cM84"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149651137, "cdate": 1762149651137, "tmdate": 1762925559382, "mdate": 1762925559382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}