{"id": "26Msp6pV5i", "number": 19532, "cdate": 1758297009800, "mdate": 1759897034283, "content": {"title": "Inferring the Invisible: Neuro-Symbolic Rule Discovery for Missing Value Imputation", "abstract": "One of the central challenges in artificial intelligence is reasoning under partial observability, where key values are missing but essential for understanding and modeling the system. This paper presents a neuro-symbolic framework for latent rule discovery and missing value imputation. In contrast to traditional latent variable models, our approach treats missing grounded values as latent predicates to be inferred through logical reasoning. By interleaving neural representation learning with symbolic rule induction, the model iteratively discovers—both conjunctive and disjunctive rules—that explain observed patterns and recover missing entries. Our framework seamlessly handles heterogeneous data, reasoning over both discrete and continuous features by learning soft predicates from continuous values. Crucially, the inferred values not only fill in gaps in the data but also serve as supporting evidence for further rule induction and inference—creating a feedback loop in which imputation and rule mining reinforce one another. Using coordinate gradient descent, the system learns these rules end-to-end, enabling interpretable reasoning over incomplete data. Experiments on both synthetic and real-world datasets demonstrate that our method effectively imputes missing values while uncovering meaningful, human-interpretable rules that govern system dynamics.", "tldr": "", "keywords": ["Neuro-symbolic Learning", "Rule Discovery", "Interpretable Reasoning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39fb4fe235a97f38a3f28f2be7685aa7cd7c51cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a neuro-symbolic framework for missing value imputation that integrates neural feature learning with symbolic rule discovery. The key contributions are A hybrid imputation architecture that couples a neural network (for pattern recognition and latent representation) with a symbolic reasoning engine (for interpretable rule-based inference of missing values), and a a rule discovery mechanism that extracts symbolic if-then rules from latent representations to explain imputation decisions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The fusion of symbolic rule discovery with neural representation learning is both conceptually elegant and practically meaningful. It directly addresses a persistent problem in missing data research: neural models can impute accurately but are opaque, whereas symbolic models are interpretable but rigid.\n2. The ability to generate human-readable rules adds practical value for analysts, especially in regulated domains (e.g., healthcare, finance).\n3. The iterative mechanism that alternates between neural imputation and symbolic rule adjustment is well-motivated. It allows symbolic reasoning to guide neural learning and vice versa, enhancing both accuracy and transparency."}, "weaknesses": {"value": "1. While the motivation is sound, the paper lacks theoretical guarantees for convergence or consistency of rule discovery. There’s no formal justification that the extracted rules remain faithful to the true data-generating process, especially under high missingness rates.\n2. The symbolic component introduces combinatorial complexity, especially when discovering multi-variable rules. The approach may not scale well to high-dimensional datasets with thousands of features.\n3. Missing data in real-world structured domains might behave differently, so generalizability remains unclear."}, "questions": {"value": "1. How does the symbolic rule discovery scale with feature dimensionality and rule length? Could pruning or differentiable symbolic learning help?\n2. Have you conducted any human evaluation to assess whether the discovered rules are semantically meaningful and useful to domain experts?\n3. How does the framework explicitly handle MNAR data when missingness depends on the missing values themselves?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OjmV90h94r", "forum": "26Msp6pV5i", "replyto": "26Msp6pV5i", "signatures": ["ICLR.cc/2026/Conference/Submission19532/Reviewer_PgSy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19532/Reviewer_PgSy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812174507, "cdate": 1761812174507, "tmdate": 1762931421582, "mdate": 1762931421582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel neuro-symbolic framework (NS-FCN) for jointly performing missing-value imputation and interpretable rule discovery in heterogeneous datasets. The proposed model treats missing entries as latent predicates and combines neural embedding learning with differentiable logical reasoning in a closed-loop fashion—where imputation and rule induction reinforce one another. Using coordinate gradient descent, sequential covering, and soft logical operators, NS-FCN discovers both conjunctive and disjunctive rules and imputes missing data. Experiments on synthetic and real-world datasets show promising imputation accuracy and human-interpretable rules."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework’s closed-loop design elegantly unifies imputation and rule induction, a combination that is conceptually strong and rarely explored.\n\n2. The method produces explicit logical rules that are easy to interpret and verify. This property is valuable for domains where transparency is critical.\n\n3. The paper is clearly written, with intuitive figuresillustrating the reasoning and training process. The problem is generally well motivated and related-work coverage are comprehensive."}, "weaknesses": {"value": "Despite its novelty, several issues limit the methodological soundness and clarity.\n\n* The paper claims that the coordinate gradient descent “never increases” the loss, but no proof, or convergence bound is presented. It is unclear whether this property holds under stochastic mini-batching or asynchronous updates when rules interact via shared predicates.\n* The freezing of “perfect rules” during training may lead to premature convergence or sub-optimal local minima, especially when data contain noise or imbalanced predicates. This could hinder exploration of alternative rule structures.\n* Equation (2) relies on a non-differentiable argmax to match rule components to predicate embeddings. The paper does not explain whether gradients are approximated or ignored. If predicate embeddings are frozen, this matching process may prevent end-to-end adaptation and yield inconsistent updates.\n* The soft-min and log-sum-exp approximations can distort logical semantics. Soft-min may amplify small numerical differences, resulting in unstable gradient propagation.\n* Although the experiments are diverse, comparisons are limited to interpretable baselines. It would be informative to benchmark against state-of-the-art deep imputation methods, e.g., MICE, GAIN, MissForest) to better contextualize performance gains. Sensitivity to missingness ratios and noise could also be analyzed more rigorously."}, "questions": {"value": "1. Have you conducted a sensitivity analysis for the temperature parameters used in the soft-min and log-sum-exp approximations? How do these values impact the model's performance and stability across different datasets?\n\n\n2. The paper mentions that negative predicates are treated as independent predicates. ​ Have you explored alternative approaches to model negative predicates more effectively, such as incorporating negation directly into the rule learning process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Rth7tXi4Uj", "forum": "26Msp6pV5i", "replyto": "26Msp6pV5i", "signatures": ["ICLR.cc/2026/Conference/Submission19532/Reviewer_vtkH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19532/Reviewer_vtkH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886967527, "cdate": 1761886967527, "tmdate": 1762931421139, "mdate": 1762931421139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors aim to combine neural embedding–based ILP with rule-based missing value imputation. However, the paper doesn’t clearly explain why their methodology should outperform the baselines they compare against. The only reason - apart from being the first to combine these three techniques - seems to be a more native and advanced way of handling continuous variables. But the baselines this method is compared against are not even introduced in the related works or other sections. It’s also not very well written and hard to follow."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a methodology that leverages neural embedding–based inductive logic programming to enhance rule-based methods for missing value imputation, which handles both binary and continuous variables. The method employs a coordinate gradient descent algorithm specifically tailored to this type of problem. Its novelty lies in being the first to combine ILP and rule-based approaches for improved missing value imputation. The integration of soft predicate learning enables native handling of continuous features, avoiding discretization and improving flexibility. The paper is well written and detailed. The empirical results on both synthetic and real datasets are\nstrong, demonstrating both interpretability and accuracy."}, "weaknesses": {"value": "The paper does not clearly justify why the proposed method outperforms the baselines, particularly in real-world experiments. The other methods in Tables 4 and 5 are not clearly introduced or discussed in the main body of the paper. In particular, the baselines listed in Table 7 (with references) should be properly cited and briefly discussed in the Related Work section. Moreover, the primary reason for NS-FCN’s superior performance—aside from being the first approach to integrate ILP and rule-based imputation—appears to be its native support for continuous variables (lines 412–414). While the ablation studies are thorough, they could more clearly isolate the contribution of individual system components (e.g., the impact of the fine-tuning step versus differentiable forward chaining)."}, "questions": {"value": "1. Please introduce and justify the baseline methods used in the comparison (in Table 4, 5 and 6).\n2. Are there other missing-value imputation methods that natively handle continuous features (rather than relying on hard thresholds)? This seems to be a key limitation of the baselines you compare against."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6Vw1Ji473H", "forum": "26Msp6pV5i", "replyto": "26Msp6pV5i", "signatures": ["ICLR.cc/2026/Conference/Submission19532/Reviewer_GyS2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19532/Reviewer_GyS2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919022655, "cdate": 1761919022655, "tmdate": 1762931420531, "mdate": 1762931420531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new Neuro-Symbolic framework called NS-FCN (Neuro-Symbolic Forward Chaining Network), designed to integrate missing value imputation with interpretable logical rule discovery. \nWhile existing approaches for missing data either rely purely on statistical imputation or require fully observed data for rule induction, \nNS-FCN treats missing values as latent predicates and learns to infer them within a differentiable NeSy framework. \nThe model establishes a closed-loop system where the imputed values feed back into rule discovery, promising to improve both imputation accuracy and interpretability over time. \nThe combination of symbolic logic is done via a differentiable forward-chaining mechanism that uses soft logical operators to handle both continuous and discrete features. \nEmpirical evaluations on synthetic and real-world datasets show that NS-FCN achieves high accuracy in imputing missing values while simultaneously finding human-interpretable rules that describe the data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "One strength of the paper, which is the main contribution by the authors, is proposing a closed feedback loop between imputation and rule discovery,  that allows the model to iteratively improve its reasoning and inference capabilities. \nAs a merit, the framework handles heterogeneous data types, both discrete and continuous, by transforming continuous features into soft predicates, which is valuable in cases where both real and categorical values are considered. \nThe model’s optimization method, based on coordinate gradient descent, seems to be a useful and scalable recipe for learning in this setting. \n\nThe experimental analysis also reveals that the framework generates logical rules that remain meaningful and consistent with domain knowledge. The empirical studies are good and cover a wide setting, showing that the model performs competitively against black-box neural architectures while maintaining explainability."}, "weaknesses": {"value": "While the experiments involve also different black-box neural competitors, the range of baselines could be expanded to include more recent generative models, such as those based on diffusion processes. Due to my limited experience on this specific topic, I do not understand if the baselines are the effective SotA or one could have repurposed auto-encoders or diffusions.\n\n\nOne thing is not mentioned and discussed is the relation to reasoning shortcuts, where learning may result in incorrect predicates and rules, as in [1,2,3]. \nSince NS-FCN operates with learning latent discrete values, an analysis of whether it avoids such behavior would have strengthened its claims about interpretability and reliability. \nIn fact, the validation of interpretability remains largely qualitative; the paper reports interpretable rules, but lacks a systematic  assessment of their quality. \n\nAdditionally, the dependence of the model’s performance on temperature hyperparameters in the soft logical operators is not clear and not enough explored.\n\n------\n\n[1] Learning with Logical Constraints but without Shortcut Satisfaction, Li et al., 2023 \\\n[2] Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts, Marconato et al. 2023 \\\n[3] Shortcuts and identifiability in concept-based models from a neuro-symbolic lens, Bortolotti et al., 2025"}, "questions": {"value": "I don't have further questions, I hope to see some discussion around the weaknesses I spotted."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sMpKzEGf0j", "forum": "26Msp6pV5i", "replyto": "26Msp6pV5i", "signatures": ["ICLR.cc/2026/Conference/Submission19532/Reviewer_KLE1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19532/Reviewer_KLE1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762177965632, "cdate": 1762177965632, "tmdate": 1762931420174, "mdate": 1762931420174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}