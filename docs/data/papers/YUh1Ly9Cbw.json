{"id": "YUh1Ly9Cbw", "number": 4392, "cdate": 1757671411599, "mdate": 1763707119555, "content": {"title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion", "abstract": "Masked diffusion models have shown promising performance in generating high-quality samples in a wide range of domains,\nbut accelerating their sampling process remains relatively underexplored. To investigate efficient samplers for masked diffusion, this paper theoretically analyzes the MaskGIT sampler for image modeling, revealing its implicit temperature sampling mechanism. Through this analysis, we introduce the \"moment sampler,\" an asymptotically equivalent but more tractable and interpretable alternative to MaskGIT, which employs a \"choose-then-sample'' approach by selecting unmasking positions before sampling tokens. In addition, we improve the efficiency of choose-then-sample algorithms through two key innovations: a partial caching technique for transformers that approximates longer sampling trajectories without proportional computational cost, and a hybrid approach formalizing the exploration-exploitation trade-off in adaptive unmasking. Experiments in image and text domains demonstrate our theory as well as the efficiency of our proposed methods,\nadvancing both theoretical understanding and practical implementation of masked diffusion samplers.", "tldr": "We theoretically investigate the MaskGIT sampler and propose several methods for efficient masked diffusion sampling.", "keywords": ["MaskGIT", "masked diffusion", "efficient sampling", "generative models", "theory"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d500c1c5ef9de3eda9910bbe1a9a962bb97ff6d6.pdf", "supplementary_material": "/attachment/805afa6d679f6cddcfe6f4c3c47e05edaa22ac96.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides a deep analysis of the MaskGIT sampler's behavior in image and text generation, revealing its implicit temperature sampling mechanism and explaining its degraded performance with increased sampling steps. Building on this theoretical analysis, the authors introduce the \"moment sampler,\" an asymptotically equivalent but more tractable and interpretable alternative that employs a \"choose-then-sample\" (CTS) strategy. To further enhance the efficiency of CTS algorithms, two key techniques are proposed:\n1）Partial Caching Approximation: For Transformer-based models, this technique approximates sampling trajectories with more effective steps without proportionally increasing computational cost.\n2）Hybrid Approach for Exploration-Exploitation Balancing: This formalizes the exploration-exploitation trade-off in adaptive unmasking for masked diffusion sampling, developing a hybrid method that combines the strengths of both.\nExperiments on the ImageNet image dataset and OpenWebText text dataset validate the theoretical findings and demonstrate the efficiency and effectiveness of the proposed methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Theoretical Analysis. The paper provides the theoretical analysis of the MaskGIT sampler, uncovering its implicit temperature sampling mechanism and explaining its performance degradation with increased sampling steps. This understanding is crucial for further advancements in masked diffusion models.\n2. The introduction of the moment sampler offers an asymptotically equivalent yet more interpretable and tractable alternative. It adopts \"choose-then-sample\" strategy and proposes partial caching approximation and the hybrid approach for exploration-exploitation balancing. Partial caching promises to improve sampling efficiency without significantly increasing computational cost, while the hybrid method effectively balances exploration and exploitation in sampling strategies."}, "weaknesses": {"value": "1. It is a critical problem to determine the tokens to be sampled in the \"select-and-sample\" paradigm. The analyses in this part are insufficient. It confuses me how Algorithm 2 is implemented and how to determine the k tokens to be sampled. More details and maybe code are encouraged to verify the actual modification of this method over Maskgit.\n2. This paper employs the unconditional MAGE as the baseline, lacking necessary verification on diverse models, e.g., the class-conditional Maskgit, and text-conditional model.\n3. Even if on MAGE, the performance gain is marginal compared to the baseline as shown in Figure 3 and 4. Besides, a quantitative performance table are encouraged for better visualization."}, "questions": {"value": "While the partial caching method aims for efficiency, the paper notes that its performance improvement is not significant on certain GPUs (e.g., H100 GPU), and computational overhead can even become noticeable (Section D.2.1). This may suggests that the practical benefits of this method might be highly dependent on hardware and model architecture, and its general applicability requires further investigation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CoaeS70CSf", "forum": "YUh1Ly9Cbw", "replyto": "YUh1Ly9Cbw", "signatures": ["ICLR.cc/2026/Conference/Submission4392/Reviewer_Xexd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4392/Reviewer_Xexd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643573544, "cdate": 1761643573544, "tmdate": 1762917334541, "mdate": 1762917334541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response to Reviewers"}, "comment": {"value": "Dear Reviewers,\n\nWe are grateful to all the reviewers for valuable feedback and appreciating our theoretical contributions. While we answer individual questions in separate replies to each reviewer, let us summarize our revision of the paper here:\n\n- **Section 3: Error bound for Eq. (2) and its derivation** (to Reviewers jaeH, 57uk): We have added a quantitative bound on the concentration inequality and its derivation after Eq. (2).\n- **Section 3: Omitting $\\beta$ and add explanation of $\\gamma$** (to Reviewer Xexd): Regarding the notation, we omitted the non-necessary introduction of $\\beta=1+1/\\alpha$ and added more explanation of $\\gamma$ at the end of page 4.\n- **Section 5: Interpretation of image-generation results** (to Reviewers DU35, Xexd): As briefly described at the beginning of Section 5, we do not try to outperform MaskGIT by Moment in the image experiments in Section 5.1. Rather, we try to show that our Moment sampler can approximate MaskGIT well in image experiments. To mitigate the diversity collapse of MaskGIT in language experiments (Figure 5(Right), Figures H-K), deriving the Moment sampler has been essential to further lead to U-Moment and Hybrid in the language experiments in Section 5.2. Since our initial description in this regard could be confusing, we have added the \"relative difference\" between MaskGIT and Moment in Table A, and also added more explanations in the captions of Figure 3-5 to guide understanding the results.\n- **Section A: Added PyTorch implementation of MaskGIT and Moment** (to Reviewer Xexd): To clarify the implementation of Moment sampler and its difference from MaskGIT, we added their sample PyTorch implementations in Figures A, B and highlighted the differences. We have also slightly modified Algorithm 1 (swapped Steps 1 and 2) to better align two implementations.\n- **Section E.1 & E.2: FID results of MAGE and conditional MaskGIT** (to Reviewer Xexd): We have added detailed numbers of MAGE in Section E.1, and we conduct additional class-conditional ImageNet experiments with MaskGIT-PyTorch and added the results in Section E.2 to re-confirm the approximation ability of the Moment sampler.\n- **Section E.3: Language experiments with larger/different models** (to Reviewer jaeH): We have added language experiments with two additional models: SDTT-large (863M parameters) and Di4C (modified architecture: mixture modeling). In both cases we confirm that our Hybrid sampler improves efficiency while keeping/improving quality-diversity tradeoff.\n- **Section E.4: Asymmetry of Hybrid sampler** (to Reviewer jaeH): Since the Hybrid sampler is asymmetric with respect to the order of two methods in selecting indices, we have swapped the order of two methods (U-Moment and Halton) as an ablation study.\n- **Section E.5: Generated samples** (to Reviewers jaeH, DU35): We added generated samples by MAGE (MaskGIT and Moment samplers) and SDTT (MaskGIT and Hybrid samplers) for qualitative comparison.\n\nWe again thank the constructive feedback from the reviewers, and we believe this revision greatly helps clarifying our contributions. Currently, we highlight the changes in blue and use alphabetic labels for tables and figures added to avoid changing existing numbering. We will rename them in the camera-ready version."}}, "id": "bZ6E7vGktv", "forum": "YUh1Ly9Cbw", "replyto": "YUh1Ly9Cbw", "signatures": ["ICLR.cc/2026/Conference/Submission4392/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4392/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4392/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763707260850, "cdate": 1763707260850, "tmdate": 1763708126314, "mdate": 1763708126314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why MaskGIT’s parallel decoding sometimes produces inconsistent image quality. It reformulates MaskGIT sampling as a moment-based process, revealing that it implicitly performs temperature-scaled categorical sampling. Through this lens, the authors propose two improvements—partial caching and balancing—to stabilize token updates across steps. Partial caching reduces noise by reusing confident predictions, while balancing controls the trade-off between stability and diversity. Theoretical analysis and experiments show that combining both leads to higher-quality samples with fewer steps. These findings generalize beyond Moment to other parallel diffusion-style samplers like MaskGIT, showing the mechanism’s universality."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- There are too many notations, which makes the paper hard to follow, but if one manages to track them carefully, the explanations appear to be quite correct and well-founded.\n\n- Naturally, in the MaskGIT sampler, increasing the strength of the added Gumbel noise would lead to sampling from a more smoothed distribution, and it was nice to see this explicitly stated at the beginning of Section 3.\n\n- The difference from the original MaskGIT sampler is clearly demonstrated through Theorem 2."}, "weaknesses": {"value": "- The paper defines too many unnecessary notations, which makes it difficult to read. For example, why did they even define $\\beta=1+\\frac{1}{\\alpha}$? It seems that $\\gamma$ is an important parameter, but it is only defined in the appendix through the algorithm, not in the main text.\n\n- Honestly, it’s unclear whether the performance gap with MaskGIT is actually significant. In Figure 3, the temperature seems to be a more critical factor that influences performance. Would it be possible to show ImageNet samples generated by MaskGIT and the Moment sampler under the same random seed and number of steps?"}, "questions": {"value": "- In Figure 3, it seems that the optimal temperature varies across sampling steps. Can this phenomenon be explained mathematically?\n\n- Are the methods introduced in Sections 4.1 and 4.2 applicable to the original MaskGIT sampler?\n\n- Is the performance gain significant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "diI5siLd94", "forum": "YUh1Ly9Cbw", "replyto": "YUh1Ly9Cbw", "signatures": ["ICLR.cc/2026/Conference/Submission4392/Reviewer_DU35"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4392/Reviewer_DU35"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821807924, "cdate": 1761821807924, "tmdate": 1762917334152, "mdate": 1762917334152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel sampling approach for Masked Diffusion models, termed Moment Sampler.\nThe authors provide a theoretical analysis of the MaskGIT sampler, which is commonly used in Masked Diffusion sampling, and introduce the Moment Sampler as an approximation to it.\nThey prove that under certain conditions, the proposed method well approximates the MaskGIT sampler.\nFurthermore, they theoretically show that the loss of the generated samples obtained by the proposed algorithm is bounded in terms of exploration and exploitation, and to achieve this, they also propose an adaptive order selection method.\nExperimental results demonstrate that the proposed method can effectively approximate the MaskGIT sampler across various domains, including image and language generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides a strong theoretical analysis of the MaskGIT sampler.\nThis contributes to a deeper understanding of the mathematical foundation underlying how MaskGIT generates images.\n\n- The authors propose the Moment Sampler, which effectively approximates the MaskGIT sampler while enabling the use of transformer caching, thereby reducing the computational complexity of the sampling process.\n\n- The proposed approach is domain-agnostic and applicable to both image and text generation within the masked diffusion framework."}, "weaknesses": {"value": "- The main limitation of the proposed method is that it does not surpass the performance of the MaskGIT sampler.\nIt is designed as an approximation method, and achieving a good approximation requires increasing the number of sampling steps (i.e., decreasing the demasking ratio).\n\n- In Proposition 3, the paper claims that one-by-one sampling is unbiased; however, this theoretical advantage may not hold for generating large images or long sentences.\nIn practice, some degree of biased sampling may be unavoidable to maintain efficiency."}, "questions": {"value": "- Error bound for the approximation in Eq. (2): \nIn Eq. (2), you state that the approximation holds when $N−k$ is large. Could you provide an error-bound analysis for this approximation? Specifically, for which ranges of $N−k$ does the approximation remain accurate, and within what quantitative error tolerance?\n\n- In Appendix D, it is mentioned that partial caching requires more computation, but the reason for this is unclear.\nWhat specific advantages does partial caching provide compared to the original MaskGIT sampler?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "poreV0XdLq", "forum": "YUh1Ly9Cbw", "replyto": "YUh1Ly9Cbw", "signatures": ["ICLR.cc/2026/Conference/Submission4392/Reviewer_57uK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4392/Reviewer_57uK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923109915, "cdate": 1761923109915, "tmdate": 1762917333804, "mdate": 1762917333804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper fouces on improving the sampling efficiency of masked diffusion models. The paper first provides a theoretical analysis of the MaskGIT sampler and show that it implictly incorportates a temperature-based sampling mechanism. Building on this insight, they propose the moment sampler, an asymtotically equivalent yet more interpretable choose-then-sampler approach. To improve efficiency, the paper introduces a partial cahcing technique and a hybrid strategy that formalize the exploration-exploitation trade-off during sampling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper provides a clear theoretical explanation of what conditoinal distribution the MaskGIT sampler draws samples from, effectively conneting it to the Gumbel-top-k tric. The analysis showing that MaskGIT sampling implicitly performs biased temperature-based sampling is insightful and could be valuable for future studies on masked diffusion model sampling.\n\n* Building upon this analysis, the introduction of the moment sampler is reasonable. The proposed choose-then-sample algorithm is simple yet novel, enabling the integration of efficiency-enhancing technqiues discussed in the paper."}, "weaknesses": {"value": "* While the theoretical analysis is convincing, the practical advantage of the proposed method as a more efficient sampler is not fully demonstrated.\n  * The paper suggests that, from the experiments in language modeling, temperature sampling reduces generation diversity, but I want to know that the proposed approach mitigate this effect. Further empirical evidence and discussion would clarify this point.\n  * Also, this language experiments appear to be conducted using only a single small pretrained model. Since the proposed method is described as a post-hoc sampling improvement, it would be valuable to verify whether the same effects and efficiency gains are observed across different models, including larger and more recent architectures.\n\n* It could be beneficial to provide the derivation of the approximation in Eq. (2) in more detail. Since this approximation forms the foundation of subsequent formluation, a step-by-step explanation and, if possible, an error analysis would strengthen the rigor of the paper.\n\n* The proposed method for handling the exploration-exploitation trade-off appears somewhat ad hoc. It is unclear whether simply selecting the first-m and -n elements from two orderings adequately captures the trade-off mechanism. Also, the proposed hybrid method seems asymmetric: swapping indices $i$ and $j$ may yield different $k$ even when the corresponding $m$ and $n$ are the same (e.g., $k=(4,3,2,6,5,1)$ in the paper's example). It would be helpful to discuss whether this asymmetry could cause any issues or biases."}, "questions": {"value": "Please provide discussions or clarifications on the points raised in the Weaknesses section.\n\nI think the theoretical analysis of the paper to be strong and insightful, but I would like to see more experimental evidence demonstrating the practical strengths of the proposed method. In particular, additional experiments across different models or settings that clearly highlight the efficiency and effectiveness of the proposed approach would significantly strenghten the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qZAU3WW7MB", "forum": "YUh1Ly9Cbw", "replyto": "YUh1Ly9Cbw", "signatures": ["ICLR.cc/2026/Conference/Submission4392/Reviewer_jaeH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4392/Reviewer_jaeH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984716763, "cdate": 1761984716763, "tmdate": 1762917333552, "mdate": 1762917333552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}