{"id": "YUh1Ly9Cbw", "number": 4392, "cdate": 1757671411599, "mdate": 1759898035007, "content": {"title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion", "abstract": "Masked diffusion models have shown promising performance in generating high-quality samples in a wide range of domains,\nbut accelerating their sampling process remains relatively underexplored. To investigate efficient samplers for masked diffusion, this paper theoretically analyzes the MaskGIT sampler for image modeling, revealing its implicit temperature sampling mechanism. Through this analysis, we introduce the \"moment sampler,\" an asymptotically equivalent but more tractable and interpretable alternative to MaskGIT, which employs a \"choose-then-sample'' approach by selecting unmasking positions before sampling tokens. In addition, we improve the efficiency of choose-then-sample algorithms through two key innovations: a partial caching technique for transformers that approximates longer sampling trajectories without proportional computational cost, and a hybrid approach formalizing the exploration-exploitation trade-off in adaptive unmasking. Experiments in image and text domains demonstrate our theory as well as the efficiency of our proposed methods,\nadvancing both theoretical understanding and practical implementation of masked diffusion samplers.", "tldr": "We theoretically investigate the MaskGIT sampler and propose several methods for efficient masked diffusion sampling.", "keywords": ["MaskGIT", "masked diffusion", "efficient sampling", "generative models", "theory"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/66eeac774deaf7ac5fdfa0c6c0c43eb5fa637b89.pdf", "supplementary_material": "/attachment/805afa6d679f6cddcfe6f4c3c47e05edaa22ac96.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides a deep analysis of the MaskGIT sampler's behavior in image and text generation, revealing its implicit temperature sampling mechanism and explaining its degraded performance with increased sampling steps. Building on this theoretical analysis, the authors introduce the \"moment sampler,\" an asymptotically equivalent but more tractable and interpretable alternative that employs a \"choose-then-sample\" (CTS) strategy. To further enhance the efficiency of CTS algorithms, two key techniques are proposed:\n1）Partial Caching Approximation: For Transformer-based models, this technique approximates sampling trajectories with more effective steps without proportionally increasing computational cost.\n2）Hybrid Approach for Exploration-Exploitation Balancing: This formalizes the exploration-exploitation trade-off in adaptive unmasking for masked diffusion sampling, developing a hybrid method that combines the strengths of both.\nExperiments on the ImageNet image dataset and OpenWebText text dataset validate the theoretical findings and demonstrate the efficiency and effectiveness of the proposed methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Theoretical Analysis. The paper provides the theoretical analysis of the MaskGIT sampler, uncovering its implicit temperature sampling mechanism and explaining its performance degradation with increased sampling steps. This understanding is crucial for further advancements in masked diffusion models.\n2. The introduction of the moment sampler offers an asymptotically equivalent yet more interpretable and tractable alternative. It adopts \"choose-then-sample\" strategy and proposes partial caching approximation and the hybrid approach for exploration-exploitation balancing. Partial caching promises to improve sampling efficiency without significantly increasing computational cost, while the hybrid method effectively balances exploration and exploitation in sampling strategies."}, "weaknesses": {"value": "1. It is a critical problem to determine the tokens to be sampled in the \"select-and-sample\" paradigm. The analyses in this part are insufficient. It confuses me how Algorithm 2 is implemented and how to determine the k tokens to be sampled. More details and maybe code are encouraged to verify the actual modification of this method over Maskgit.\n2. This paper employs the unconditional MAGE as the baseline, lacking necessary verification on diverse models, e.g., the class-conditional Maskgit, and text-conditional model.\n3. Even if on MAGE, the performance gain is marginal compared to the baseline as shown in Figure 3 and 4. Besides, a quantitative performance table are encouraged for better visualization."}, "questions": {"value": "While the partial caching method aims for efficiency, the paper notes that its performance improvement is not significant on certain GPUs (e.g., H100 GPU), and computational overhead can even become noticeable (Section D.2.1). This may suggests that the practical benefits of this method might be highly dependent on hardware and model architecture, and its general applicability requires further investigation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CoaeS70CSf", "forum": "YUh1Ly9Cbw", "replyto": "YUh1Ly9Cbw", "signatures": ["ICLR.cc/2026/Conference/Submission4392/Reviewer_Xexd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4392/Reviewer_Xexd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643573544, "cdate": 1761643573544, "tmdate": 1762917334541, "mdate": 1762917334541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why MaskGIT’s parallel decoding sometimes produces inconsistent image quality. It reformulates MaskGIT sampling as a moment-based process, revealing that it implicitly performs temperature-scaled categorical sampling. Through this lens, the authors propose two improvements—partial caching and balancing—to stabilize token updates across steps. Partial caching reduces noise by reusing confident predictions, while balancing controls the trade-off between stability and diversity. Theoretical analysis and experiments show that combining both leads to higher-quality samples with fewer steps. These findings generalize beyond Moment to other parallel diffusion-style samplers like MaskGIT, showing the mechanism’s universality."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- There are too many notations, which makes the paper hard to follow, but if one manages to track them carefully, the explanations appear to be quite correct and well-founded.\n\n- Naturally, in the MaskGIT sampler, increasing the strength of the added Gumbel noise would lead to sampling from a more smoothed distribution, and it was nice to see this explicitly stated at the beginning of Section 3.\n\n- The difference from the original MaskGIT sampler is clearly demonstrated through Theorem 2."}, "weaknesses": {"value": "- The paper defines too many unnecessary notations, which makes it difficult to read. For example, why did they even define $\\beta=1+\\frac{1}{\\alpha}$? It seems that $\\gamma$ is an important parameter, but it is only defined in the appendix through the algorithm, not in the main text.\n\n- Honestly, it’s unclear whether the performance gap with MaskGIT is actually significant. In Figure 3, the temperature seems to be a more critical factor that influences performance. Would it be possible to show ImageNet samples generated by MaskGIT and the Moment sampler under the same random seed and number of steps?"}, "questions": {"value": "- In Figure 3, it seems that the optimal temperature varies across sampling steps. Can this phenomenon be explained mathematically?\n\n- Are the methods introduced in Sections 4.1 and 4.2 applicable to the original MaskGIT sampler?\n\n- Is the performance gain significant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "diI5siLd94", "forum": "YUh1Ly9Cbw", "replyto": "YUh1Ly9Cbw", "signatures": ["ICLR.cc/2026/Conference/Submission4392/Reviewer_DU35"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4392/Reviewer_DU35"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821807924, "cdate": 1761821807924, "tmdate": 1762917334152, "mdate": 1762917334152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel sampling approach for Masked Diffusion models, termed Moment Sampler.\nThe authors provide a theoretical analysis of the MaskGIT sampler, which is commonly used in Masked Diffusion sampling, and introduce the Moment Sampler as an approximation to it.\nThey prove that under certain conditions, the proposed method well approximates the MaskGIT sampler.\nFurthermore, they theoretically show that the loss of the generated samples obtained by the proposed algorithm is bounded in terms of exploration and exploitation, and to achieve this, they also propose an adaptive order selection method.\nExperimental results demonstrate that the proposed method can effectively approximate the MaskGIT sampler across various domains, including image and language generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides a strong theoretical analysis of the MaskGIT sampler.\nThis contributes to a deeper understanding of the mathematical foundation underlying how MaskGIT generates images.\n\n- The authors propose the Moment Sampler, which effectively approximates the MaskGIT sampler while enabling the use of transformer caching, thereby reducing the computational complexity of the sampling process.\n\n- The proposed approach is domain-agnostic and applicable to both image and text generation within the masked diffusion framework."}, "weaknesses": {"value": "- The main limitation of the proposed method is that it does not surpass the performance of the MaskGIT sampler.\nIt is designed as an approximation method, and achieving a good approximation requires increasing the number of sampling steps (i.e., decreasing the demasking ratio).\n\n- In Proposition 3, the paper claims that one-by-one sampling is unbiased; however, this theoretical advantage may not hold for generating large images or long sentences.\nIn practice, some degree of biased sampling may be unavoidable to maintain efficiency."}, "questions": {"value": "- Error bound for the approximation in Eq. (2): \nIn Eq. (2), you state that the approximation holds when $N−k$ is large. Could you provide an error-bound analysis for this approximation? Specifically, for which ranges of $N−k$ does the approximation remain accurate, and within what quantitative error tolerance?\n\n- In Appendix D, it is mentioned that partial caching requires more computation, but the reason for this is unclear.\nWhat specific advantages does partial caching provide compared to the original MaskGIT sampler?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "poreV0XdLq", "forum": "YUh1Ly9Cbw", "replyto": "YUh1Ly9Cbw", "signatures": ["ICLR.cc/2026/Conference/Submission4392/Reviewer_57uK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4392/Reviewer_57uK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923109915, "cdate": 1761923109915, "tmdate": 1762917333804, "mdate": 1762917333804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper fouces on improving the sampling efficiency of masked diffusion models. The paper first provides a theoretical analysis of the MaskGIT sampler and show that it implictly incorportates a temperature-based sampling mechanism. Building on this insight, they propose the moment sampler, an asymtotically equivalent yet more interpretable choose-then-sampler approach. To improve efficiency, the paper introduces a partial cahcing technique and a hybrid strategy that formalize the exploration-exploitation trade-off during sampling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper provides a clear theoretical explanation of what conditoinal distribution the MaskGIT sampler draws samples from, effectively conneting it to the Gumbel-top-k tric. The analysis showing that MaskGIT sampling implicitly performs biased temperature-based sampling is insightful and could be valuable for future studies on masked diffusion model sampling.\n\n* Building upon this analysis, the introduction of the moment sampler is reasonable. The proposed choose-then-sample algorithm is simple yet novel, enabling the integration of efficiency-enhancing technqiues discussed in the paper."}, "weaknesses": {"value": "* While the theoretical analysis is convincing, the practical advantage of the proposed method as a more efficient sampler is not fully demonstrated.\n  * The paper suggests that, from the experiments in language modeling, temperature sampling reduces generation diversity, but I want to know that the proposed approach mitigate this effect. Further empirical evidence and discussion would clarify this point.\n  * Also, this language experiments appear to be conducted using only a single small pretrained model. Since the proposed method is described as a post-hoc sampling improvement, it would be valuable to verify whether the same effects and efficiency gains are observed across different models, including larger and more recent architectures.\n\n* It could be beneficial to provide the derivation of the approximation in Eq. (2) in more detail. Since this approximation forms the foundation of subsequent formluation, a step-by-step explanation and, if possible, an error analysis would strengthen the rigor of the paper.\n\n* The proposed method for handling the exploration-exploitation trade-off appears somewhat ad hoc. It is unclear whether simply selecting the first-m and -n elements from two orderings adequately captures the trade-off mechanism. Also, the proposed hybrid method seems asymmetric: swapping indices $i$ and $j$ may yield different $k$ even when the corresponding $m$ and $n$ are the same (e.g., $k=(4,3,2,6,5,1)$ in the paper's example). It would be helpful to discuss whether this asymmetry could cause any issues or biases."}, "questions": {"value": "Please provide discussions or clarifications on the points raised in the Weaknesses section.\n\nI think the theoretical analysis of the paper to be strong and insightful, but I would like to see more experimental evidence demonstrating the practical strengths of the proposed method. In particular, additional experiments across different models or settings that clearly highlight the efficiency and effectiveness of the proposed approach would significantly strenghten the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qZAU3WW7MB", "forum": "YUh1Ly9Cbw", "replyto": "YUh1Ly9Cbw", "signatures": ["ICLR.cc/2026/Conference/Submission4392/Reviewer_jaeH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4392/Reviewer_jaeH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984716763, "cdate": 1761984716763, "tmdate": 1762917333552, "mdate": 1762917333552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}