{"id": "rYbYbgeaEv", "number": 24387, "cdate": 1758356345326, "mdate": 1759896768738, "content": {"title": "Efficient Patch Search in Whole Slide Images via Morphological Momentum Prototype Learning", "abstract": "Digital histopathology images play a crucial role in cancer diagnosis, therapeutic response prediction, and identification of clinically relevant morphological features. However, processing Whole Slide Images (WSI) with gigapixel resolution introduces significant challenges in computer vision, exceeding the memory capacity of standard vision encoders. To address this, recent methods employ a multi-stage pipeline: dissecting the image into small patches, extracting patch-level features, and aggregating these features using global pooling through Multi-Instance Learning (MIL) to form a final slide-level representation. Despite achieving clinical-grade performance, this approach becomes increasingly complex with higher magnification due to the quadratic increase in patch numbers and the generation of numerous irrelevant or redundant patches. This complexity burdens the global pooling network, resulting in long inference times and excessive computational resources, while redundant patches introduce noise during the MIL process, limiting the model's ability to utilize high-magnification features fully. To overcome these challenges, we propose MMPL, an efficient method that redefines WSI diagnosis as a searching process of relevant patch-level representations with a learned set of global prototypes. MMPL trains a fixed set of prototypes to retrieve the most informative patches, computing the diagnostic score using only the retrieved patches. Evaluated on multiple public WSI classification benchmarks, MMPL achieves state-of-the-art performance in accuracy across various pathology tasks, including tumor subtyping, grading, and metastasis detection.", "tldr": "", "keywords": ["Whole Slide Image"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/66a3e864c9237b38fa2eeb3418a196d045778789.pdf", "supplementary_material": "/attachment/87f9347a1e8c3d6422728116e4c90b53836d1099.pdf"}, "replies": [{"content": {"summary": {"value": "This work presents an efficient method for WSI analysis, called Momentum Morphological Prototype Learning (MMPL). MMPL trains a fixed set of prototypes to retrieve the most informative patches, and then computes the diagnostic score using only the retrieved patches, which can largely reduce the computational cost. The authors conducted experiments on three public datasets to evaluate the proposed method, along with a comparison with existing methods. Ablation study is also carried out to verify the contribution of the key components of the proposed method. Experimental results show that MMPL achieves the best performance on the CAMELYON16 and TCGA datasets for metastasis detection and tumor subtyping, but has inferior performance on the PANDA dataset for tumor grading."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The manuscript presents a new paradigm for WSI analysis. Different from previous works that utilized all patches of WSI for analysis, this work proposed MMPL that trains several prototypes to retrieve the most informative patches for WSI representation. In this manner, the computational cost is significantly reduced. For prototyping learning, the authors adopted an optimal transport (OT) formulation with uniform marginal constraints to achieve balanced representation of distinct morphological patterns while preventing prototype collapse and ensuring diverse information distribution. In summary, the proposed method is of some novelty."}, "weaknesses": {"value": "1)\tThe proposed method involves multiple hyperparameters, such as the number of prototypes, k value for top-k sampling, and K value for the feature queue. Although the authors conducted experiments to investigate the effects of prototype number and k value on the classification performance on CAMELYON16, it is still unclear whether the proposed method is sensitive to those hyperparameters across different datasets.\n2)\tThe computational cost of retrieval should be considered during training. Given the queue size set to 100,000, there is nontrivial computation per training iteration. Please elaborate on it.\n3)\tThe proposed method did not achieve state-of-the-art performance on the PANDA dataset."}, "questions": {"value": "Besides the weakness mentioned above, there are some concerns as follows:\n1)\tPlease specify the number of WSIs of the PANDA and TCGA datasets in Section 4.1.\n2)\tPlease briefly introduce the workflow of inference after the introduction of Section 3.4, which could give readers a better understanding of the test stage.\n3)\tThe authors mentioned that their method MMPL used only 1.18% of the patch embeddings. It is better to provide the computation process in the supplementary file.\n4)\tFor the visualization of the prototypes in Figure 3, I am surprised to see that the white background (non-tissue regions, i.e., prototype 14) is accounted for in this work. Normally, we exclude the white background in the data pre-processing stage. Please elaborate on it.\n5)\tThe proposed method did not achieve state-of-the-art performance on the PANDA dataset, and there is no analysis of the reason. Besides, I am not clear about PANDA (R) and PANDA (K). For the prostate cancer grading tasks, did the authors compute the kappa score between the model predictions and the ground truth labels? Accuracy is also a commonly used evaluation metric."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UZZPRFYzJw", "forum": "rYbYbgeaEv", "replyto": "rYbYbgeaEv", "signatures": ["ICLR.cc/2026/Conference/Submission24387/Reviewer_tnwR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24387/Reviewer_tnwR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761130988538, "cdate": 1761130988538, "tmdate": 1762943066398, "mdate": 1762943066398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework named Momentum Morphological Prototype Learning (MMPL) for weakly supervised classification of Whole Slide Images (WSIs). The method addresses the computational inefficiency in multiple instance learning caused by an excessive number of instances by reformulating the problem as an efficient patch search and retrieval task. MMPL employs an optimal transport (OT) formulation to learn a set of prototypes, a design intended to prevent prototype collapse and ensure diversity in pattern representation. Additionally, the authors adopt a feature queue combined with exponential moving average (EMA) updates to mitigate cluster drift induced by rapid encoder parameter changes. Based on the similarity between prototypes and image patches, the method selects a subset from the WSI's patch collection and performs classification using only this subset. The authors demonstrate the effectiveness of their approach on multiple public histopathology datasets, and ablation studies confirm the contribution of each component within the proposed framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It raises a valuable question: how can we address the explosive growth in the number of patches during WSI tiling due to increased resolution, along with the consequent issues of computational efficiency and noise?\n2. The proposed framework introduces an optimal transport (OT) formulation to learn prototypes in a self-supervised manner, and incorporates a retrieval mechanism to reduce computational cost during classification; furthermore, the entire framework is end-to-end trainable, demonstrating a certain degree of novelty.\n3. The effectiveness of MMPL is validated on multiple datasets, with comparisons to various state-of-the-art methods, and ablation studies are conducted to verify the contribution of each component."}, "weaknesses": {"value": "1. The authors' central claim—that their framework achieves high efficiency—has not been substantiated by sufficient experimental evidence or rigorous analysis.\n2. The experimental comparisons exhibit anomalous results, which the authors fail to explain; these discrepancies may indicate incorrect implementation of the method or an unfair experimental setup.\n3. Key techniques such as optimal transport for preventing prototype collapse, and the use of a feature queue combined with exponential moving average (EMA) to mitigate feature drift, have been previously proposed in influential works, yet the authors do not acknowledge or discuss these prior contributions."}, "questions": {"value": "1. Could the authors provide a computational complexity analysis and performance metrics (e.g., inference latency, throughput) for the proposed framework when processing a single whole slide image (WSI) to completion of classification, and compare these metrics with those of other existing methods?\n2. In Table 1, the Kappa score of IBMIL appears unusually low. Is this due to an implementation error? If not, could the authors provide an analysis explaining the underlying reasons?\n3. Is it possible that end-to-end training, while effectively fitting the training task, might lead to overfitting and thus perform poorly on out-of-domain test sets or in real-world scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qeGr2O5Sfk", "forum": "rYbYbgeaEv", "replyto": "rYbYbgeaEv", "signatures": ["ICLR.cc/2026/Conference/Submission24387/Reviewer_KdfC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24387/Reviewer_KdfC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762244302633, "cdate": 1762244302633, "tmdate": 1762943066155, "mdate": 1762943066155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Momentum Morphological Prototype Learning (MMPL), a prototype-driven framework for efficient Whole Slide Image (WSI) classification. MMPL learns a fixed set of global morphological prototypes and uses an optimal transport assignment with the Sinkhorn-Knopp algorithm over a momentum feature queue to avoid prototype collapse and keep prototypes diverse, and retrieve only the most informative top-k patch features per slide. This retrieval reduces the number of patch vectors the aggregator must process and enables end-to-end training with lower memory use. The authors evaluate MMPL on standard WSI benchmarks (CAMELYON16, PANDA, TCGA), report state-of-the-art accuracy/AUC/Kappa in several settings, and present ablations showing benefits of the feature queue, OT-based prototypes, and dynamic top-k retrieval."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1 Reformulating WSI diagnosis as a multi-vector retrieval problem driven by learned prototypes and solving prototype collapse with an OT constraint is a fresh combination of ideas that hasn’t been widely applied in this context. Combining a momentum queue with Exponential Moving Average (EMA) for stable prototype learning is a reasonable adaptation and integration of ideas derived from self-supervised learning. \n\n2 Experiments across three widely-used WSI datasets (CAMELYON16, PANDA, TCGA) with multiple baselines (ABMIL, DSMIL, ZoomMIL, IBMIL, PANTHER, VIB) show consistent improvements. The ablation (queue/prototypes/OT vs K-means/uniform top-k) supports the mechanistic claims. \n\n3 The method is explained step-by-step with helpful figures showing architecture and a visualization of prototype patch retrieval. Loss terms and the joint training objective are clearly defined. \n\n4 WSI analysis is a high-impact application area; achieving similar or better performance while dramatically reducing processed patches addresses a real practical constraint (compute/memory) and can enable wider deployment and more frequent end-to-end training."}, "weaknesses": {"value": "1 Efficiency claims need quantitative runtime/memory ablation. The paper states that only 1.18% of patches are used and claims reduced inference time, but provides no systematic runtime / GPU memory / FLOPs measurements vs baselines.\n\n2 Sensitivity to key hyperparameters (M, queue size, top-k policy, τ, λ). The method depends on prototype count M, the queue size K, and the top-k allocation vector Ij. Only limited ablation is shown.\n\n3 Theoretical justification or failure modes. While OT with uniform marginals mitigates collapse, the paper lacks a short formal discussion of when prototype assignments might still be suboptimal (e.g., extremely class-imbalanced slides) and how the method handles rare morphologies.\n\n4 The MMPL architecture diagram is not aesthetically pleasing, except for the defect of misaligned box lines at first glance. Some symbols do not appear in the main text and are not explained in the captions"}, "questions": {"value": "1 Runtime & memory: Please report GPU memory usage and per-slide inference latency for MMPL and for at least tow strong baseline on the same hardware. \n\n\n2 Prototypes and M selection: How sensitive is performance to M? Despite OT uniform marginals, is there a risk that some prototypes remain unused? Provide the distribution of prototype assignments (counts per prototype) and show whether OT enforcement truly yields balanced semantic coverage. \n\n3 Provide clearer details and results comparing the two-step frozen-backbone training to full end-to-end training. Because the paper claims end-to-end is possible, thus conducting more ablation studies would make the argument more convincing. If possible, Could you provide a short expert evaluation or annotation to show prototypes align with meaningful histology concepts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vqjBzhMQLO", "forum": "rYbYbgeaEv", "replyto": "rYbYbgeaEv", "signatures": ["ICLR.cc/2026/Conference/Submission24387/Reviewer_6sEU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24387/Reviewer_6sEU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762258903064, "cdate": 1762258903064, "tmdate": 1762943065895, "mdate": 1762943065895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces MMPL, a prototype-based method that searches whole-slide images for only the most informative patches to make a slide-level decision. MMPL works by learning a small set of global “morphological” prototypes with a momentum encoder and queue, assigning patch embeddings to prototypes via Sinkhorn-Knopp optimal transport, and then retrieving the top-k relevant patches per prototype to compute the prediction using just those patches. The proposed novelty is the combination of prototype learning with a memory queue for stable training and a dynamic allocation of retrieved patches per prototype. Experiments span CAMELYON16, PANDA (K/R), and TCGA tasks, comparing against MIL baselines such as ABMIL, DSMIL, CLAM, TransMIL, etc., and also evaluate different encoders including ResNet-50 and pathology foundation models (UNI, CONCH, and GigaPath)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Lots of ablations: top-k selection, the effect of the queue and prototype components, uniform versus dynamic per-prototype selection, prototype assignment via Sinkhorn versus K-means, and backbone choices. \n- Presentation of the paper is good and math is sound. Method extends SK-OT for pathology and is overall sound (working both unsupervised and supervised)."}, "weaknesses": {"value": "- Very limited experiments and tasks. Only three datasets are evaluated (C16, PANDA, TCGA-Lung) and are only classification (no survival tasks). While experimental design on ablating performance of MMPL is sound, most of the tasks that MPPL are evaluated on are a bit simple. It would be interesting to evaluate on a greater range of tasks.\n- Unclear if number of prototypes in MMPL are fixed versus adaptive, e.g. if the number of clusters can change. Why is the number of prototypes fixed to 15? Interpretability of MMPL clusters could also show deeper insights (comparison with PANTHER in learning quality of prototypes).\n- Hard to understand where performance of supervised versus unsupervised MMPL is evaluated.\n- Technically, this work addresses the problem of learning better prototypes for weakly-supervised classification tasks in pathology. The work generally extends the idea of SK-OT to the pathology domain, and though mostly applied, has good empirical experimentation of the technical components. For better impact, I would like to see more interpretability and evaluation on more diverse tasks as there are many MIL architectures that succeed on the evaluated tasks."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iECLRNExKm", "forum": "rYbYbgeaEv", "replyto": "rYbYbgeaEv", "signatures": ["ICLR.cc/2026/Conference/Submission24387/Reviewer_68kJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24387/Reviewer_68kJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762583801303, "cdate": 1762583801303, "tmdate": 1762943064925, "mdate": 1762943064925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}