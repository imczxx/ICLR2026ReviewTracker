{"id": "riOevy2RwZ", "number": 25292, "cdate": 1758366251756, "mdate": 1759896726476, "content": {"title": "Towards Text-Mask Consistency in Medical Image Segmentation", "abstract": "Vision-language models for medical image segmentation often produce masks that conflict with the accompanying text, especially under multi-site/multi-lesion descriptions. We trace this failure to two factors: (i) highly templated and repetitive clinical language causes one-to-one hard contrastive learning to yield numerous false negatives, weakening cross-modal alignment; and (ii) predominantly vision-driven, one-way cross-attention lacks a language-dominant, spatially aware pathway, hindering effective injection of textual semantics into the spatial visual domain. To this end, we propose Consistency-enhanced Two-stage Segmentation (C2Seg). In the pretraining stage, Cluster-aware Contrastive Learning uses a frozen strong baseline to construct an intra-batch text similarity matrix as soft labels, thereby alleviating false negative conflicts and producing more discriminative visual representations. In the fusion stage, we introduce a Bidirectional Complementary Attention Module, where each modality dominates attention along its own path, fostering deep interaction and structural consistency between visual and textual representations. In order to enhance the expressive power of multimodal features, we further adopt KAN-based Attention Gating. Without updating the language encoder, our approach significantly improves text--mask consistency and segmentation accuracy on two public medical imaging datasets. Code is provided in the supplementary material.", "tldr": "", "keywords": ["Medical image segmentation", "Vision language models", "Multimodal learning", "Kolmogorov–Arnold Networks"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e05da9928ef8ca6dc9c0d857b79e738dd17148dc.pdf", "supplementary_material": "/attachment/086874f5351d8b08c7774ba8b5507c5ac84f2171.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes C2Seg, a vision-language framework for medical image segmentation that aims to achieve better alignment between clinical text descriptions and visual features. It introduces a Cluster-aware Contrastive Learning (CaCL) method that uses soft labels derived from text–text similarity in a frozen language space to address issues of repetitive and templated clinical phrasing, which can create false negatives in traditional contrastive setups. To fuse the two modalities, the paper presents a Bidirectional Complementary Attention Module (BCAM), which performs dual cross-attention between image and text tokens while preserving spatial structure. Finally, a KAN-based nonlinear gating mechanism (K-Gate) is added to selectively suppress or enhance modality-specific noise before fusion. Experiments on multiple medical segmentation datasets show moderate improvements in Dice and mIoU scores, suggesting that the proposed modules enhance cross-modal alignment and segmentation quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of the paper lies in its idea of using soft labels for contrastive learning through the proposed Cluster-aware Contrastive Learning (CaCL) module. Instead of relying on traditional hard negatives, which can misclassify semantically similar clinical descriptions as distinct, the method introduces a more flexible alignment strategy by estimating text–text similarity in a frozen language space and converting it into soft supervision. This is a smart and well-motivated way to handle the repetitive and templated nature of clinical text, and it effectively reduces false negatives during image–text alignment. The paper is also well-presented, with clear and visually appealing illustrations that help convey the framework and intuition behind each component. In addition, the authors conduct extensive quantitative experiments and compare their method against a wide range of state-of-the-art models, showing consistent, if moderate, performance improvements, which supports the effectiveness of their overall approach."}, "weaknesses": {"value": "Although Figure 1 looks visually good, I think it is a bit overcomplicated for an idea figure. It includes too many technical details, which makes it harder to quickly understand the main concept. A high-level illustration focusing only on the problem and the core idea would be clearer.\n\nThe main motivation sentence, “clinical descriptions are highly templated and semantically repetitive”, feels vague. I understand the general point, but it does not clearly explain why this is an actual problem for learning. If the issue is about false negatives in contrastive learning due to repetitive phrasing, it should be stated more explicitly.\n\nFigure 2 is quite dense, and the overall pipeline is not very easy to follow. Some modules, like BCAM, appear in the diagram without a proper explanation. A short caption or visual cue describing what BCAM actually does and how it connects to other parts would help a lot.\n\nThe bidirectional fusion idea does not feel very new. Similar techniques have already been explored in earlier works such as [1] and [2]. In fact, the spatial relationship preserving approach here, where visual features are flattened into patch tokens, fused with text through cross-attention, and then reshaped back to the 2D grid, is essentially the same process described in [2]. The authors should clarify what is actually new in BCAM beyond this, for example, whether the “complementary role” assignment or the gating introduces any measurable improvement. A more detailed comparison with these existing techniques would help solidify the claimed novelty and make the contribution clearer.\n\nNever clearly explain what defines a cluster or how those soft labels are derived or updated during training. It seems the text–text similarity matrix is computed once from a frozen encoder, which makes the “cluster-aware” term a bit misleading; here is no actual dynamic clustering or adaptive grouping during training.\n\nThe motivation for using KAN is not convincing. The paper does not explain why a spline-based gating function is needed or what specific limitation of existing nonlinearities like ReLU, GELU, or MLP-based gates it solves. As it stands, KAN feels like it was just added as-is without a strong reason. The improvement shown in the ablation study is also quite small, so it is hard to tell if the gain comes from KAN itself or simply from adding another nonlinear component. A clearer justification and a fair comparison with simpler gates would make this part more credible.\n\nSince the entire claim revolves around better text–mask alignment, the paper should ideally visualize how text tokens influence spatial regions.\n\nNo formalized loss functions are described, nor is it explained how different objectives are balanced during training.\n\n[1] Cho, Yubin, Hyunwoo Yu, and Suk-Ju Kang. \"Cross-aware early fusion with stage-divided vision and language transformer encoders for referring image segmentation.\" IEEE Transactions on Multimedia 26 (2023): 5823-5833.\n\n[2] Sultan, Rafi Ibn, et al. \"BiPVL-Seg: Bidirectional Progressive Vision-Language Fusion with Global-Local Alignment for Medical Image Segmentation.\" arXiv preprint arXiv:2503.23534 (2025)."}, "questions": {"value": "Please address the points raised in the Limitations section and provide clarifications or responses to the concerns I posed there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DHQYLDsQRp", "forum": "riOevy2RwZ", "replyto": "riOevy2RwZ", "signatures": ["ICLR.cc/2026/Conference/Submission25292/Reviewer_RBwK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25292/Reviewer_RBwK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761509822150, "cdate": 1761509822150, "tmdate": 1762943389028, "mdate": 1762943389028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes C2Seg, a two-stage vision–language framework for medical image segmentation, aiming to improve alignment between textual descriptions and segmentation masks. The first stage introduces Cluster-aware Contrastive Learning (CaCL), which converts intra-batch text similarities into soft labels for contrastive supervision. The second stage adds a Bidirectional Complementary Attention Module (BCAM) that integrates vision- and language-dominant paths, followed by a KAN-based Attention Gating (K-Gate) for nonlinear feature selection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written, logically structured, and easy to follow.\n2. The topic of improving text–mask alignment in multimodal segmentation is relevant to medical AI.\n3. The authors conduct detailed ablation studies and provide reproducibility information."}, "weaknesses": {"value": "1. The methodological novelty is limited, and the contribution appears primarily engineering-oriented. CaCL functions as a soft-label variant of conventional contrastive learning, closely aligned with supervised or semantic-aware contrastive paradigms. BCAM represents only a modest architectural modification to existing bidirectional cross-attention designs, while K-Gate simply replaces standard MLPs with KAN units without clear theoretical motivation or demonstrated substantive benefit. Overall, the paper assembles several established components—soft contrastive learning, dual-path attention, and nonlinear gating—without offering a cohesive conceptual advance or deeper theoretical insight.\n2. The central problem of “text–mask consistency” is not clearly defined. The paper lacks a formal metric or quantitative measure beyond Dice and mIoU, making it unclear whether the improvement truly reflects better semantic alignment or merely better segmentation accuracy.\n3. The experimental scope is narrow, relying solely on two COVID-related datasets with templated or synthetic text, and both tasks are limited to binary lesion segmentation. This setup is not representative of real clinical scenarios and does not evaluate the method under more challenging multi-class or multi-organ settings. As a result, the proposed approach remains highly restricted in scope, and the experiments do not demonstrate generalization to other organs, modalities, or real free-text clinical reports.\n4. The reported performance improvements are modest (≈1–2% Dice) compared with the additional complexity introduced by multi-stage training and multiple attention and gating modules. The computational cost–benefit trade-off is not justified."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YSMiYrJOXk", "forum": "riOevy2RwZ", "replyto": "riOevy2RwZ", "signatures": ["ICLR.cc/2026/Conference/Submission25292/Reviewer_RDjv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25292/Reviewer_RDjv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791540909, "cdate": 1761791540909, "tmdate": 1762943388718, "mdate": 1762943388718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles text-mask inconsistency in medical image segmentation through C2Seg, featuring Cluster-aware Contrastive Learning (CaCL) and Bidirectional Complementary Attention Module (BCAM). The motivation is clear and results show improvements, but I have concerns about novelty and experimental scope."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem is well-motivated—templated clinical text causing false negatives in contrastive learning is genuine, nicely illustrated in Figure 1.\n\n- Soft-label construction using text-text similarity is intuitive with clear mathematical formulation in Equations 1-3.\n\n- BCAM's spatial structure preservation addresses M3Att's limitation where fully connected projection loses local details."}, "weaknesses": {"value": "The \"frozen strong baseline\" for computing text-text similarity lacks justification—medical text has domain-specific semantics that general CLIP might not capture well [1], yet there's no sensitivity analysis to different encoders like BioBERT or PubMedBERT. The row-mean debiasing M'ij = max{Mij - μi, 0} also needs theoretical grounding since hard thresholding might remove valid semantic structure [2], and the gradient analysis in Equation 3 is purely conceptual without quantitative validation of how much false negative attenuation actually occurs. The language-dominant path in Equation 6 sums over N tokens without normalization which should cause unstable magnitudes across varying text lengths, and BCAM's spatial preservation claim lacks attention visualizations to verify spatially-coherent patterns compared to M3Att.\n\nNovelty is limited since soft-label contrastive learning already exists in Prototypical Contrastive Learning [3], ProtoNCE [4], and medical-specific MedKLIP [5]. BCAM is essentially incremental modification of M3Att with changed spatial handling, and KAN integration is straightforward MLP substitution without domain-specific adaptation—Table 3 even shows pure KAN encoder performs worse than hybrid, contradicting claims about superior nonlinear modeling. The overall framework combines existing techniques without fundamentally new mechanisms for text-mask consistency.\n\nEvaluation is severely limited to two COVID chest imaging datasets, preventing generalization claims to diverse anatomies like brain MRI, abdominal CT, or pathology [6]. Missing comparisons with recent medical VLMs like LLaVA-Med, BiomedCLIP, SAM-Med, and MedSAM that directly target medical segmentation. No analysis of text quality robustness when reports have typos, abbreviations, or incomplete descriptions [7], and no computational cost reporting despite CaCL adding O(B²C) complexity and BCAM doubling attention paths—training time, memory, inference speed are all absent. Hyperparameter choices (ρ=0.8, τ=0.07, batch size 256) appear arbitrary without sensitivity analysis, and the \"text reuse\" problem (7000 samples sharing 300 descriptions) is mentioned but never correlated with performance gains.\n\n\n### Referenced Works\n\n[1] Müller et al., \"Joint Learning of Localized Representations from Medical Images and Reports\", Medical Image Analysis, 2022\n\n[2] Khosla et al., \"Supervised Contrastive Learning\", NeurIPS, 2020\n\n[3] Li et al., \"Prototypical Contrastive Learning of Unsupervised Representations\", CVPR, 2021\n\n[4] Dwibedi et al., \"With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations\", ICCV, 2021\n\n[5] Wu et al., \"Medical Knowledge-enhanced Large Language Model\", Nature Medicine, 2024\n\n[6] Ma et al., \"Segment Anything in Medical Images\", MICCAI, 2024\n\n[7] Bozkurt et al., \"Reporting Standards for Clinical Decision Support\", Nature Digital Medicine, 2023"}, "questions": {"value": "Can you provide empirical evidence for CaCL's effectiveness by measuring actual false negative rates, gradient magnitudes, or negative pair distances before/after, and show sensitivity analysis for frozen encoder choice across domain-specific alternatives? The current analysis is purely conceptual without quantitative validation.\n\nWhy does Table 3 show pure KAN encoder underperforming hybrid design, and what's the rationale for hyperparameters (ρ=0.8, τ=0.07, batch size 256) without sensitivity analysis? How does text length variation affect the unnormalized summation in Equation 6?\n\nWhere are attention visualizations verifying BCAM's spatial preservation, failure case analyses for multi-site/multi-lesion scenarios, and computational cost comparisons (training time, memory, FLOPs) for practical deployment? How does performance degrade with noisy or incomplete text descriptions common in real clinical settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TAtXSKdh3E", "forum": "riOevy2RwZ", "replyto": "riOevy2RwZ", "signatures": ["ICLR.cc/2026/Conference/Submission25292/Reviewer_bmue"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25292/Reviewer_bmue"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815807992, "cdate": 1761815807992, "tmdate": 1762943388477, "mdate": 1762943388477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a text-mask consistency enhanced two stage segmentation framework (with CaCL, BCAM, and K-Gate). The experiments have been done on two COVID datasets and showed clear improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The methodology is sound and the paper is well-structured.\nThe paper’s motivation is clearly articulated by identifying specific causes of text–mask inconsistency, which justifies the two-stage approach. \nThe results show clear improvements in Dice and mIoU, and the ablation studies are thorough, which lends credence to the methodological claims."}, "weaknesses": {"value": "1) The proposed C2Seg framework (with CaCL, BCAM, and K-Gate) appears to be a thoughtful integration.  While each component has roots in existing techniques (e.g. contrastive learning and dual attention are established concepts, and KANs have been explored in vision backbones), which require more detailed clarification on the technical contribution. \n2) The experiments use two public medical imaging datasets (QaTa-COV19 and MosMedData+), both are in the COVID-19 chest imaging domain, so the representativeness is somewhat narrow.\n3) One minor critique is that the evaluation emphasizes standard segmentation metrics without a direct quantitative measure of “text–mask consistency”, but qualitatively the improvements imply better alignment."}, "questions": {"value": "please address the comments in weakness session. clarify the technical novelty, add more datasets, and maybe more evaluation metrics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oSEjS8jWP5", "forum": "riOevy2RwZ", "replyto": "riOevy2RwZ", "signatures": ["ICLR.cc/2026/Conference/Submission25292/Reviewer_WFDv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25292/Reviewer_WFDv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009067651, "cdate": 1762009067651, "tmdate": 1762943388169, "mdate": 1762943388169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}