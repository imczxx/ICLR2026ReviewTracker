{"id": "03jzVlLxEe", "number": 22991, "cdate": 1758337883115, "mdate": 1759896837180, "content": {"title": "NERVE: Noise-Variability-Robust EEG Foundation Model with Electrode-Brain Interactions", "abstract": "Electroencephalography (EEG) is an indispensable modality for measuring and recording brain electrical activity, with broad applications in brain–computer interfaces (BCI) and healthcare. While early EEG models predominantly adopted supervised learning methods due to the scarcity of large-scale datasets and the heterogeneity across tasks and datasets, the recent success of large foundation models has driven increasing efforts to build EEG foundation models. However, most existing studies focus on handling signals with varying formats while overlooking inherent characteristics of EEG signals during acquisition, including low signal-to-noise ratios (SNR), high variability across samples, and spatial dependencies arising from electrode placement within the acquisition system. To address these challenges, we propose NERVE, a novel noise-variability-robust EEG foundation model with electrode-brain interactions. Specifically, pre-training of NERVE begins with learning a noise-robust neural tokenizer that encodes EEG patches into discrete neural tokens. The tokenizer is trained through denoising temporal–spectral prediction to reconstruct temporal and frequency information of the original signal from noise-augmented inputs. NERVE is further pretrained to predict the neural codes of masked EEG patches, integrated with a variability-robust objective that promotes uniform EEG representations. To incorporate spatial structure in EEG, we propose an electrode-position-aware transformer as the backbone for both the tokenizer and the foundation model. It enables the model to capture spatial dependencies among electrodes and brain regions via attention mechanisms. NERVE demonstrates competitive performance across diverse BCI tasks and improved robustness to noise and variability compared to existing EEG foundation models.", "tldr": "", "keywords": ["Foundation model", "Electroencephalography", "EEG", "Self-supervised learning", "Pre-training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2af8f2986c76341d381f0b7aced096521dd9722f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose **NERVE**, a noise- and variability-robust EEG foundation model designed to address key challenges in EEG analysis, including low signal-to-noise ratios (SNR), high inter-sample variability, and spatial dependencies arising from electrode placement in acquisition systems. The proposed framework consists of three core components. First, a **noise-robust neural tokenizer** encodes EEG patches into discrete neural tokens. Second, a **variability-robust pretraining strategy** enforces alignment and uniformity in the representation space to improve robustness against distributional shifts. Third, an **electrode-position–aware (EPA) transformer** serves as the backbone for both the tokenizer and the foundation model, explicitly modeling the spatial structure of EEG channels."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The topic of EEG foundation models (EFMs) is highly significant and relevant to the advancement of brain–computer interface (BCI) technologies.\n\n2.The overall logical flow of the introduction and methodology sections is clear and easy to follow.\n\n3.The work appears to be technically complete and systematically presented."}, "weaknesses": {"value": "1.**Section 2.1** introduces the EPA transformer, which is intended to capture spatial dependencies among electrodes and brain regions. However, the logical flow of this section is not tightly connected to the stated goal of modeling such spatial dependencies.\n\n2.**Section 2.2** does not directly address the low SNR problem in EEG signals. Merely citing the inspiration behind the method does not sufficiently justify its validity or effectiveness.\n\n3.In **Section 3.3**, the attribution of the model’s strong performance lacks clarity. While Table 3 demonstrates overall improvement, there is insufficient evidence to link the observed gains to the specific factors claimed by the authors."}, "questions": {"value": "1.Time embeddings and channel embeddings are defined as *d*-dimensional learnable vectors. How do the authors ensure that the learned parameters indeed correspond to temporal and channel-specific information, respectively?\n\n2.The discussion of electrode-to-brain and brain-to-electrode interactions is overly brief. Could the authors provide a more formal justification or theoretical explanation for these interactions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5Z3MN1JjSC", "forum": "03jzVlLxEe", "replyto": "03jzVlLxEe", "signatures": ["ICLR.cc/2026/Conference/Submission22991/Reviewer_1XLF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22991/Reviewer_1XLF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761210355224, "cdate": 1761210355224, "tmdate": 1762942467768, "mdate": 1762942467768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes NERVE, a novel EEG foundation model designed to address key acquisition-related challenges of EEG signals: low signal-to-noise ratio, high inter- and intra-subject variability, and spatial dependencies among electrodes. By introducing a noise-robust neural tokenizer, a variability-robust pretraining objective, and an electrode-position-aware transformer architecture, NERVE demonstrates competitive performance across multiple BCI tasks and improved robustness compared to existing foundation models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-organized, with each component clearly explained and logically presented. NERVE demonstrates strong empirical performance, achieving state-of-the-art or competitive results on the majority of downstream tasks compared to several recent EEG foundation models. The motivation is clear and well-grounded, and the method is thoughtfully designed to directly address the three key challenges highlighted in the paper: low signal-to-noise ratio, high intra- and inter-subject variability, and spatial dependencies among electrodes."}, "weaknesses": {"value": "1. The criss-cross attention mechanism depicted in Figure 1(b) appears to be inaccurately represented.\n\n2. While the core contribution of this work lies in proposing a noise-robust foundation model, the evaluation of noise robustness is insufficient. The authors have conducted comparative anti-noise experiments only on a limited number of datasets (Fig. 4, Fig. 7). Moreover, the experimental results indicate that the differences between NERVE and the baseline models on EMG, EOG, and environmental noise are marginal. The Gaussian condition, under which NERVE demonstrates its best noise robustness, is less common in practical scenarios, as most EEG signals undergo preprocessing—thereby diminishing the practical impact of NERVE’s contribution. Additionally, the authors should provide visual examples under varying noise injection conditions to better illustrate model behavior.\n\n3. The authors present extensive experiments to demonstrate the stability of NERVE’s performance in intra-subject settings. However, in my own opinion, this emphasis appears misaligned with the intended role of a foundation model for EEG. Such models are expected to prioritize generalization across diverse scenarios, tasks, and multicenter environments, rather than focusing on individual-specific adaptations. Robustness in both intra-subject and inter-subject contexts has already been extensively studied in dedicated literature."}, "questions": {"value": "1. How did the authors verify that the Neural Tokenizer had been adequately trained for noise robustness prior to the masked reconstruction pre-training?\n\n2.  Could the authors provide an ablation study on the Neural Tokenizer? Since the noise robustness of NERVE appears to stem primarily from the pre-trained Neural Tokenizer, would the anti-noise performance deteriorate significantly if this component were removed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wolwjwfHoQ", "forum": "03jzVlLxEe", "replyto": "03jzVlLxEe", "signatures": ["ICLR.cc/2026/Conference/Submission22991/Reviewer_c7uQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22991/Reviewer_c7uQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761287330840, "cdate": 1761287330840, "tmdate": 1762942467369, "mdate": 1762942467369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper highlights the importance of robustness to noise and intra-subject variability in EEG foundation models. To address these challenges, the authors designed specialized modules—such as the EAP and the noise-robust tokenizer—as well as tailored learning objectives, including masked codebook reconstruction with KoLeo regularization, to enhance model robustness. Their robustness analysis reveals that existing EEG foundation models often produce unstable representations for the same class and struggle to disentangle subject-specific from class-specific information. In contrast, the proposed approach demonstrates improved stability and resilience to variability. Overall, the paper raises important awareness of the diverse sources of noise, variability, and artifacts that EEG foundation models must effectively account for."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors rigorously quantified noise and variability (both inter- and intra-subject) and evaluated the proposed models with corresponding analyses. This provides evidence of NERVE’s robustness to noise and individual differences. Such analysis is novel in the literature of EEG foundation models. \n2. The systematic comparison of various attention mechanisms in Section F.3 provides valuable insights for future research on the architectural design of transformer blocks in EEG modeling.\n3. The proposal to adapt the denoising autoencoder framework for EEG foundation models is a novel and valuable contribution. It highlights an important issue in current practice—the inadequacy of using mean squared error (MSE) loss for training on noisy data—and could raise broader awareness within the community regarding the need for more robust training objectives.\n4. The curated pre-training corpus, comprising 27 public datasets encompassing EEG recordings from a wide range of task paradigms, represents a substantial advancement in data diversity and scale. It is notably more comprehensive than the corpora employed in previous studies, providing a stronger foundation for pre-training and evaluation."}, "weaknesses": {"value": "1. In Section 3.2, comparisons with previous models are conducted using three random seeds. Although this number aligns with some prior studies, increasing the number of random repetitions to five or more would strengthen the statistical reliability of the results and more convincingly demonstrate the superiority of NERVE.\n2. Line 407 of Section 3.3 attributes the superior performance on the High-Gamma dataset to the EPA attention mechanism; however, this claim is not substantiated by appropriate ablation studies. Moreover, several foundation models that do not incorporate EPA attention (e.g., LaBraM, CBraMod, NeuroLM) also exhibit comparatively strong performance on this dataset, further questioning the causal link between EPA attention and the observed results.\n3. The effectiveness of KoLeo in enhancing robustness to variability is not consistently assessed across all downstream datasets. Moreover, as shown in Figure 8, its impact appears to vary substantially between datasets. A more detailed analysis explaining this phenomenon—such as identifying which characteristics of the datasets (e.g., task type, signal-to-noise ratio, temporal complexity, or inter-subject variability) contribute to these differences—would provide deeper insight into when and why KoLeo is most beneficial.\n4. The position router introduced in Line 208 of Section 2.1 is claimed to model cortical regions; however, no explicit physical prior is incorporated into its design. Since both the router $P$ and weight matrices $W^Q$, $W^K$ are both learnable linear operators, their product during back-propagation is equivalent to a single transformation matrix. Consequently, the Q-K product defined in equation (4) can be reformulated to $(HE)(HE)^T$ where $E \\in \\mathbb{R}^{C \\times R}$. Given that $R = 6$ in this paper -- typically smaller than the number of EEG channels $C$ -- the router usually smaller than number of EEG channels $C$, this router essentially performs a dimension reduction operation analogous to the Linformer (Wang et al, 2020), compressing spatial dimension to reduce computational cost. Therefore, apart from converting the parallel spatial-temporal attention blocks into sequential order and lowering attention complexity, the proposed EPA Transformer is not fundamentally novel mechanism compared to the Criss-cross attention method proposed in CBraMod. As such, the claimed functionality of this module as a cortical-region modeler appears unsubstantiated. \n5. Artifacts in scalp EEG—particularly biological artifacts—are inherently non-stationary stochastic processes that contaminate the signal across spatial, temporal, and spectral domains (see Figure 2 in Agounad, 2025). In the pre-training stage, the use of Gaussian noise as data augmentation represents an overly simplified assumption of the complex noise and artifact characteristics typically encountered in EEG recordings. Moreover, in the robustness-to-noise analysis, additive Gaussian noise is injected into specific frequency bands. This approach effectively introduces continuous sinusoidal oscillations in the time domain, which does not accurately reflect the characteristics of EMG and EOG artifacts that the study aims to simulate. Consequently, the conclusions drawn from this robustness analysis may be limited in their validity and generalisability to real-world EEG noise conditions.\n\nWang, S., Li, B. Z., Khabsa, M., Fang, H. & Ma, H. Linformer: Self-Attention with Linear Complexity. arXiv preprint arXiv:2006.04768 (2020).\n\nAgounad, S., Tarahi, O., Moufassih, M. et al. Advanced Signal Processing and Machine/Deep Learning Approaches on a Preprocessing Block for EEG Artifact Removal: A Comprehensive Review. Circuits Syst Signal Process 44, 3112–3160 (2025). https://doi.org/10.1007/s00034-024-02936-3"}, "questions": {"value": "1. The selected fine-tuning datasets differ substantially in trial duration and temporal dynamics. For instance, the DEAP dataset’s video-watching task involves long, continuous time courses, whereas the High Gamma dataset’s movement task exhibits rapidly evolving dynamics, often within sub-second timescales. How does NERVE accommodate these diverse temporal characteristics using a unified patch length of 1 second (200 samples)?\n2. The phase information in higher-frequency EEG bands is typically highly sensitive to temporal jitters and tends to become unreliable for frequencies above approximately 6 Hz. Could the authors elaborate on why capturing phase information is claimed to be important across all frequency bands, as stated in Lines 258–263 of Section 2.2? Furthermore, the temporal reconstruction loss and frequency reconstruction loss defined in Equation (9) are strongly correlated, since the frequency-domain representations are derived from the temporal-domain signals via the Discrete Fourier Transform (DFT). Please clarify the rationale for including both loss terms—specifically, what distinct information or complementary benefits each contributes to model optimisation.\n\nSome imprecise parts observed in the paper: \n1. The cited material (Miah et al., 2021) in Line 96 does not appear to focus on modeling spatial structure. It would be helpful if the authors could clarify how this reference supports the stated argument regarding the importance of incorporating spatial information. Additionally, Line 81 of Section 1 mentions that “quantized embeddings do not necessarily ensure robustness to noise” as a limitation of LaBraM; however, this specific limitation is not discussed in the cited source (Hu et al., 2023). Please provide further justification or an alternative reference that substantiates this claim.\n2. Line 80 mentions “low SNR makes reconstruction challenging”, a better expression would be making reconstruction “inappropriate” as the model learns to reconstruct noises as well as semantic information. \n3. The dot operation in Equations (4) and (5) could mislead the readers, as it is often preserved for dot product instead of matrix multiplications. \n4. The loss term in Equation (10) seems to be probability logits rather than neural code reconstruction loss."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wApgpd5kOO", "forum": "03jzVlLxEe", "replyto": "03jzVlLxEe", "signatures": ["ICLR.cc/2026/Conference/Submission22991/Reviewer_ejCD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22991/Reviewer_ejCD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751596512, "cdate": 1761751596512, "tmdate": 1762942467098, "mdate": 1762942467098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NERVE, a noise- and variability-robust EEG foundation model that explicitly addresses three acquisition-related challenges: low signal-to-noise ratio, high inter- and intra-subject variability, and spatial dependencies among electrodes. NERVE introduces a noise-robust neural tokenizer trained via denoising temporal–spectral prediction, a variability-robust pre-training objective using KoLeo regularization, and an electrode-position-aware (EPA) transformer to capture spatial structure. Evaluated on multiple downstream BCI tasks, NERVE demonstrates competitive performance and improved robustness to noise and variability compared to existing EEG foundation models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- NERVE explicitly tackles three real-world acquisition-related issues—low SNR, high inter/intra-subject variability, and spatial electrode dependencies—often overlooked by prior EEG foundation models.  \n- The combination of a noise-robust neural tokenizer, KoLeo-based variability regularization, and the electrode-position-aware (EPA) transformer provides a coherent framework for robust representation learning.  \n- Experiments across diverse BCI tasks and noise conditions demonstrate consistent performance gains and improved robustness over existing baselines."}, "weaknesses": {"value": "- While NERVE introduces several modifications, its core pre-training framework closely resembles that of LaBraM—relying on masked EEG modeling with a vector-quantized neural tokenizer. The primary differences (e.g., Gaussian noise augmentation and the electrode-position-aware attention) are incremental rather than fundamentally novel, weakening the paper’s claim to significant architectural innovation.\n\n- The motivation for noise robustness hinges on training the neural tokenizer with synthetic Gaussian noise. However, real-world EEG noise arises from diverse physiological (e.g., EMG, EOG) and environmental sources with complex, non-stationary characteristics. Relying solely on Gaussian noise may not adequately capture this complexity, raising concerns about the practical relevance and generalizability of the proposed noise-robustness strategy.\n\n- The paper demonstrates that NERVE outperforms baselines even on clean data, yet it does not convincingly explain why the noise-augmented tokenizer leads to better representations in the absence of synthetic noise. This suggests that performance gains may stem from factors other than noise robustness (e.g., regularization effects), but the contribution of the noise-robust tokenizer to representation quality remains ambiguous.\n\n- A critical experiment is absent: a direct comparison between NERVE trained with a standard (non-noise-robust) neural tokenizer and the proposed noise-robust variant. Without this control, it is difficult to isolate and validate the actual benefit of the denoising temporal–spectral prediction objective, undermining the evidence for its necessity."}, "questions": {"value": "- **Why is Gaussian noise specifically chosen in the Noise-robust Neural Tokenizer Training?**  \n  The paper justifies the use of Gaussian noise by noting its statistical independence and resistance to rule-based filtering. However, it does not provide a thorough comparison with other noise types (e.g., EMG, EOG, or real-world artifacts) or explain why Gaussian noise is a suitable proxy for the complex, structured noise commonly present in real EEG recordings.\n\n- **How does training with Gaussian noise improve robustness to other types of noise (e.g., environmental, EMG, or EOG)?**  \n  The results suggest that NERVE trained with Gaussian-augmented inputs shows improved robustness across diverse noise conditions. Yet, the mechanism behind this cross-noise generalization remains unclear. Since Gaussian noise differs significantly in spectral and physiological characteristics from structured artifacts like eye blinks or muscle activity, it is uncertain why robustness to such diverse noise sources would emerge from Gaussian-only augmentation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dxvAusftiR", "forum": "03jzVlLxEe", "replyto": "03jzVlLxEe", "signatures": ["ICLR.cc/2026/Conference/Submission22991/Reviewer_S8KX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22991/Reviewer_S8KX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823967066, "cdate": 1761823967066, "tmdate": 1762942466820, "mdate": 1762942466820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}