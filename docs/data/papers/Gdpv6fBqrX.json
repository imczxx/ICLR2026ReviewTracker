{"id": "Gdpv6fBqrX", "number": 6108, "cdate": 1757952921013, "mdate": 1763123283628, "content": {"title": "PROF: An LLM-based Reward Code Preference Optimization Framework for Offline Imitation Learning", "abstract": "Offline imitation learning (offline IL) enables training effective policies without requiring explicit reward annotations. Recent approaches attempt to estimate rewards for unlabeled datasets using a small set of expert demonstrations. However, these methods often assume that the similarity between a trajectory and an expert demonstration is positively correlated with the reward, which oversimplifies the underlying reward structure. We propose PROF, a novel framework that leverages large language models (LLMs) to generate and improve executable reward function codes from natural language descriptions and a single expert trajectory. We propose Reward Preference Ranking (RPR), a novel reward quality assessment and ranking strategy without requiring environment interactions or RL training. RPR calculates the dominance scores of the reward functions, where higher scores indicate better alignment with expert preferences. By alternating between RPR and text-based gradient optimization, PROF fully automates the selection and refinement of optimal reward functions for downstream policy learning. Empirical results on D4RL demonstrate that PROF surpasses or matches recent strong baselines across numerous datasets and domains in D4RL, highlighting the effectiveness of our approach.", "tldr": "A novel framework that leverages LLMs to generate and improve executable reward function codes.", "keywords": ["offline imitation learning", "reward design", "large language model"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/13cb9b2cf55b00eb6aaa6675d36abdb5c4d9ab30.pdf", "supplementary_material": "/attachment/8f78f354eb8bbfa8e847839e12d5d1813468f856.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, a framework, namely PROF, is proposed by leveraging LLMs to generate and improve executable reward function codes from natural language descriptions and a single expert trajectory. In which, Reward Preference Ranking (RPR) is introduced for the reward quality assessment and ranking, then text-based gradient optimization is utilized to automate the selection and refinement of optimal reward functions for downstream policy learning. Experimental results conducted on D4RL demonstrated the effectiveness of the presented approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. By leveraging LLMs to generate and improve executable reward function codes is promising, especially for the offline RL setting. \n2. Experimental results show that PROF achieves similar or better performance against some baselines on the D4RL benchmark.\n3. The integration of LLM, preference ranking, and textual optimization for addressing the reward design issue is novel and effective."}, "weaknesses": {"value": "1. While experimental results on some cases demonstrate the effectiveness of the presented framework, theoretical guarantees are lacking for the performance improvements. \n2. In the paper, only one expert trajectory is utilized, which is in contrast to reality, where optimal behaviors may be diverse and not necessarily proximal to a limited set of demonstrations (as also claimed in the paper). Experiments with multiple expert demonstrations are suggested. \n3. With the help of LLM, some prior knowledge is incorporated for the presented framework, but for some baselines, such knowledge is not available, the performance comparison in the experiments in some sense is unfair. \n4. The most recent baseline for the comparison is publicly available in 2023 (seabo), more recent baselines or existing methods but can also leverage LLM to provide some prior knowledge should be compared and discussed."}, "questions": {"value": "Please refer to the weakness points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UKFz0emSdL", "forum": "Gdpv6fBqrX", "replyto": "Gdpv6fBqrX", "signatures": ["ICLR.cc/2026/Conference/Submission6108/Reviewer_y2AM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6108/Reviewer_y2AM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724214770, "cdate": 1761724214770, "tmdate": 1762918469156, "mdate": 1762918469156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "322Xyq0Azu", "forum": "Gdpv6fBqrX", "replyto": "Gdpv6fBqrX", "signatures": ["ICLR.cc/2026/Conference/Submission6108/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6108/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763123282611, "cdate": 1763123282611, "tmdate": 1763123282611, "mdate": 1763123282611, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a method for offline imitation learning in scenarios where both expert and non-expert trajectories are available. The core idea is to generate a set of reward functions by prompting a large language model (LLM) with a description of a Gym environment. The method then selects the best reward function via preference ranking and performs a text-gradient update to further refine it. The authors evaluate their approach on several baselines, including the D4RL dataset."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The authors evaluated the algorithm on multiple environments as well as using multiple LLMs."}, "weaknesses": {"value": "1. The authors compare their method only against non–state-of-the-art algorithms [1,2] in this field. When considering more recent work in this area [1,2], the proposed method would likely underperform. Therefore, the claim of outperforming all baselines appears to be based on a selective choice of comparisons, which gives the impression of cherry-picking baselines to favor their approach.\n\n2. The authors mention that a single iteration is sufficient for full training. In that case, how does this approach compare to Eureka [3]? The two methods seem conceptually similar, except that the proposed method uses textgrad instead of LLM-based reflection.\n\n[1,2] https://arxiv.org/abs/2402.13037, https://arxiv.org/pdf/2507.12815\n[3]    https://arxiv.org/pdf/2310.12931"}, "questions": {"value": "1. Could you provide additional benchmark evaluations against actual SOTA methods? \n2. Could you comment on how your paper is different from eureka [3]?\n3. What are the potential limitations of the method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vctumNc2UG", "forum": "Gdpv6fBqrX", "replyto": "Gdpv6fBqrX", "signatures": ["ICLR.cc/2026/Conference/Submission6108/Reviewer_HDdH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6108/Reviewer_HDdH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778910980, "cdate": 1761778910980, "tmdate": 1762918468269, "mdate": 1762918468269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework, PROF, that uses LLMs to automatically generate and optimize reward function code for imitation learning. Instead of relying on prelabeled rewards or environment interactions, it produces candidate reward functions from textual prompts and refines them through an iterative \"Reward Preference Ranking\" process, which evaluates reward quality using expert trajectories and unlabeled data. The framework then applies TextGrad, a textual gradient optimization method, to iteratively improve the reward code. Experiments on D4RL show that PROF consistently matches or outperforms the baselines across multiple domains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper automates reward function generation and optimization using LLMs without requiring environment interaction.\n- It achieves strong empirical performance, surpassing or matching baselines across D4RL tasks."}, "weaknesses": {"value": "Offline algorithms make sense when interactions with the environment is costly. Here, to prompt the LLM for the reward functions, the environment information is fed into LLM. In the examples in the paper, this seems to be just the observation and action space (according to Appendix C.2), which is fine. But I am not sure if this amount of information would be sufficient in more realistic and complex environments. It may be needed to input the dynamics as well (e.g., step function in the gym convention). But that would mean there is already a simulator. If there is a simulator, online algorithms are not bad at all. I think the paper should demonstrate performance on a real robot environment to make their argument convincing.\n\nWhile this is my most major concern, below are more minor comments:\n- In line 103, should it be \"IL\" instead of \"RL\"? Because BC is not an offline RL algorithm.\n- \"Reward Design via LLMs\" subsection may want to talk about method that use LLMs or VLMs to generate/learn rewards (though VLMs may be less relevant here). Some examples are: RoboCLIP, RLAIF, RL-VLM-F, Video2Reward.\n- Equation 4 is confusing in that it made me think the paper assumes access to the reward values of the expert demonstrations. This is not the case, but I find the presentation is confusing in general. It makes it difficult for the reader that the paper first defines everything without explaining why and then the next subsection tells the method/intuition.\n- Injecting Gaussian noise to the demonstrations (especially the actions) reminds me of the D-REX paper by Brown et al. It should be cited.\n- Equation 5 computes a standard deviation over a set of vectors. While the meaning is clear, this is not mathematically well-defined. I recommend that the paper should stick to formal math notation when introducing something mathematically.\n- Similarly, Equation 7 defines noise-injected observations and actions as \"vector plus distribution,\" but mathematically, this is a distribution, and not a sample from that distribution. \n- The paper should include a comparison against Eureka (Ma et al.) and its variants. That is an important baseline for this work. It can be argued that it is online RL, but it does not have to implemented online. It can be applied on an offline RL setting as well."}, "questions": {"value": "See the questions and the comments in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9LUmwDljGs", "forum": "Gdpv6fBqrX", "replyto": "Gdpv6fBqrX", "signatures": ["ICLR.cc/2026/Conference/Submission6108/Reviewer_zKh7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6108/Reviewer_zKh7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979565065, "cdate": 1761979565065, "tmdate": 1762918467539, "mdate": 1762918467539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **PROF**, an LLM-based framework for *offline imitation learning* that learns reward functions from one expert trajectory and unlabeled data. PROF first uses an LLM to generate executable reward code, then applies a *Reward Preference Ranking* (RPR) score to rank candidates without environment interaction, and finally refines them with TextGrad-style code optimization. Experiments on D4RL MuJoCo, AntMaze, and Adroit show consistent gains, especially on AntMaze and Adroit, with comparisons to IQL, SEABO, and OTR. The method achieves competitive or superior results under fair evaluation with five seeds and standard deviations reported."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Originality:** Introduces a formal reward-ranking signal (RPR) that does not require environment interaction; conceptually novel for offline IL.\n* **Empirical quality:** Evaluated on three diverse D4RL domains with multiple seeds and std devs reported. Competitive or superior to recent baselines (SEABO, OTR) under standard normalized-score protocols.\n* **Transparency:** Provides clear algorithmic structure (generation → ranking → optimization) and example prompts; includes iteration ablations and LLM comparisons.\n* **Significance:** Meaningful practical impact for offline imitation learning; demonstrates reward generalization across domains and partial LLM portability.\n* **Fairness:** Baselines include recent state-of-the-art (SEABO 2024, OTR 2023), evaluated in the same framework."}, "weaknesses": {"value": "1. **Missing component ablations:** No sensitivity study for key RPR hyperparameters $(\\delta, \\alpha_o, \\alpha_a, H)$ or the noise-perturbation mechanism. This limits understanding of what drives performance.\n2. **Reward hacking risk:** At iteration $T=3$, the LLM fabricates variables, leading to spurious but high-ranked rewards. Indicates misalignment between ranking score and real performance.\n3. **Baseline parity unclear:** Baseline results (SEABO, OTR) seem taken from prior work rather than fully re-run under identical conditions. Cross-paper reuse may bias comparisons.\n4. **Limited generality:** PROF is mostly tested with IQL; only one TD3+BC example is shown. Generality across learners (e.g., CQL, AWAC) remains untested.\n5. **Scaling heuristics:** Task-specific reward scaling (Table 7) may reduce claims of plug-and-play reward generalization.\n6. **Narrow LLM ablation:** Cross-LLM results only on HalfCheetah; unclear whether findings hold for more complex tasks.\n7. **Related work positioning:** Needs clearer distinction from recent online LLM-based reward design works (e.g., CARD) to sharpen the novelty claim."}, "questions": {"value": "1. How sensitive is RPR to $(\\delta, \\alpha_o, \\alpha_a, H)$? Please provide ablations.\n2. How can the method detect or mitigate reward hacking during later iterations (e.g., fabricated variables)?\n3. Were SEABO and OTR re-run in your environment with identical seeds and budgets?\n4. Does PROF generalize to other offline RL algorithms (CQL, AWAC)?\n5. Can cross-LLM tests be replicated on AntMaze and Adroit to confirm portability?\n6. Were the reward-scaling heuristics tuned per task? What is the performance change without them?\n7. Will full code, prompts, and seeds be released?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tKBET2rUsm", "forum": "Gdpv6fBqrX", "replyto": "Gdpv6fBqrX", "signatures": ["ICLR.cc/2026/Conference/Submission6108/Reviewer_6aP8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6108/Reviewer_6aP8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066326196, "cdate": 1762066326196, "tmdate": 1762918466604, "mdate": 1762918466604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}