{"id": "XoRApddgFG", "number": 7643, "cdate": 1758030283535, "mdate": 1759897841599, "content": {"title": "UNIVERSAL REPRESENTATION OF GENERALIZED CONVEX FUNCTIONS AND THEIR GRADIENTS", "abstract": "Solutions to a wide range of optimization problems take the form of generalized convex functions (GCFs) or their gradients. These functions are ubiquitous in practice and their structure can be leveraged to collapse certain nested bilevel objectives (i.e. adversarial training) into single-level problems amenable to standard first order optimization methods. In spite of recent advances in parameterizing convex functions and their gradients, the more general GCFs and their gradients have received little attention. We first motivate their study by highlighting their applications, then provide a new differentiable layer with a convex parameter space and show it and its gradient are universal approximators for GCFs and their gradients respectively. Finally, we put the theory to practice by demonstrating its effectiveness in learning optimal pricing mechanisms when selling multiple goods. The methods developed here are implemented in a publicly available supplementary package that also contains the code for reproducing the experiments.", "tldr": "We show how generalized convex functions can be parameterized with universal approximation guarantees, yielding differentiable layers that simplify bilevel optimization and improve applications in adversarial training and optimal transport.", "keywords": ["Generalized Convexity", "Universal Approximation", "Differentiable Layers", "Bilevel Optimization", "Optimal Transport", "Adversarial Training"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3b069c79549e9353a0defae76320f5048992c586.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a new differentiable parameterization for generalized convex functions (GCFs) and their gradients, extending prior work on convex and input-convex neural networks. It proves universal approximation results for both functions and gradients under mild semiconvexity conditions and establishes a convex parameter space that enables stable first-order optimization. The authors connect their construction to neural network architectures, interpreting finitely convex functions as shallow networks with max aggregation. \nThey validate the approach on a mechanism design problem, showing it recovers known optimal auction outcomes. Overall, the work offers a unified theoretical framework linking convex analysis and learnable, structured function classes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main strength lies in its theoretical originality—generalizing convex-function parameterizations to the broader and practically relevant class of GCFs. The paper provides rigorous proofs and smooth differentiable variants, filling a gap in the literature on universal approximators for generalized convex functions and their gradients. \nConceptually, it bridges convex analysis with neural network design, suggesting new structured architectures that preserve generalized convexity."}, "weaknesses": {"value": "**Novelty**: This seems to be a direct extension of the work Balazs et al 2015 for convex functions. Here, the scalar product is replaced by the function $phi(x,y)$, but the results do not present any particular challenge to prove. Besides, the experimental validation choses phi to be a scalar product, so the motivation for general $phi$ in practical problems is weak.   \n\n**Narrow empirical validation** Experiments focus solely on mechanism design and fail to demonstrate the method’s potential on broader or more recognizable machine learning tasks. The paper does not showcase improvements in adversarial training, optimal transport, or robust optimization—domains explicitly cited as motivations. As a result, the contribution may seem more mathematical than impactful for mainstream ML practice. The computational scalability and practical guidance for applying the method in high-dimensional settings remain underexplored. To strengthen the work, additional experiments on modern ML problems would be essential to illustrate the method’s real-world relevance and power.\n\n\n**Clarity**: I found parts of the paper are unclear and confusing. In particular, in the section 4.2 on mechanism design and the experimental details in section 7 related to it. Specifically, I did not understand how the expected revenu of the seller is defined, why is the problem a bilevel problem, what is learned and how. A full description of the objectives to optimize, the algorithms should be provided. Although the code is provided, I could not understand it without knowing beforehand these elements. \nAlso, some notions are introduced but without proper reference to the literature: in particular DRIC seems to be a standard concept in economics, but no reference is provided for it. I found section 4.2 on mechanism design a bit obscure. For instance, I don't understand how the seller maximizes their revenue by optimizing the price. What is the objective function that they optimize\n\n\n\n**Soundness**: The proof of proposition 5.1 assumes implicitly that X,Y are compact, this was stated nowhere in the main paper."}, "questions": {"value": "- In theorem 3.2, shouldn't it be f = (f^{XY})_X instead of f = (f^{YX})_X.\n- The paper focuses on approximation of the gradient  of finite approximations to these generalized convex functions, but often, it is the parameters of the approximation that we'd like to learn using gradient methods, which is different from approximating the gradient of a generalized convex function wrt to its input. What can be said about learning the parameters  of these approximations using gradient methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kevSZeuwIP", "forum": "XoRApddgFG", "replyto": "XoRApddgFG", "signatures": ["ICLR.cc/2026/Conference/Submission7643/Reviewer_PtpT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7643/Reviewer_PtpT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687656378, "cdate": 1761687656378, "tmdate": 1762919717283, "mdate": 1762919717283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to establish a unified theory for Generalized Convex Functions (GCFs) and their gradients in the context of parameterization and universal approximation. It proposes a differentiable finite representation — finitely Y-convex functions — as an alternative to neural network architectures. The authors claim that:\n- They provide universal approximation theorems for GCFs and their gradients;\n- They prove gradient convergence under semiconvexity assumptions;\n- They demonstrate an application to multi-item mechanism design."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- It tackles an important theoretical gap by attempting to unify convex and generalized convex structures within a learnable, differentiable framew.\n- The link between generalized convexity and bilevel optimization (e.g., mechanism design, optimal transport) is conceptually appealing.\n- The mathematical framework, if rigorously developed, could provide a new lens for understanding structure-preserving function approximation."}, "weaknesses": {"value": "- In the introduction, the paper argues that GCFs can transform bilevel problems into single-level optimization problems, yet all subsequent examples (e.g., mechanism design and optimal transport) already presuppose the existence of GCF representations via the Φ-transform.\nThis creates a circular argument:\n  “We study GCFs because they simplify bilevel optimization; we know they simplify it because we assume the problem already admits a GCF form.”\n\n- The notion of generalized convexity was formalized long ago in Singer (1997) and Rubinov (2013). The paper fails to clearly articulate whether its novelty lies in the parameterization of the function space or in an extension of existing approximation theorems. This ambiguity undermines the conceptual contribution.\n\n- Theorem 5.1 (GCF UAP) merely asserts that “Proposition 5.1 + 5.2 ⇒ density,” yet Proposition 5.1’s proof relies on Φ being globally Lipschitz.\nSince Φ is only assumed to be locally Lipschitz, there is no guarantee of a global constant on compact domains.\n→ Therefore, the universal approximation claim is not rigorously established.\n\n- If GCFs truly simplify bilevel optimization, the paper should demonstrate clear advantages under nonlinear or non-Euclidean Φ functions (e.g., adversarial or transport-type cost functions).\nHowever, all experiments use the trivial linear Φ(x, y) = ⟨x, y⟩, which reduces to the standard convex setting.\nHence, the results fail to support the “generalized” claim.\n\n- The manuscript, in my ability, is difficult to read and overly terse. Many claims (e.g., “we show,” “we extend”) are stated without actual derivations or rigorous proofs, which makes the paper opaque to readers outside the narrow mathematical optimization community."}, "questions": {"value": "see the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lbjDGrlUo9", "forum": "XoRApddgFG", "replyto": "XoRApddgFG", "signatures": ["ICLR.cc/2026/Conference/Submission7643/Reviewer_MUph"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7643/Reviewer_MUph"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735033031, "cdate": 1761735033031, "tmdate": 1762919716945, "mdate": 1762919716945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies generalized convex functions (GCFs) and their gradients. The authors present the applications about the general GCFs, and then provide a new differentiable layer with a convex parameter space where it and its gradient are universal approximators for GCFs and their gradients\nrespectively. Finally, they also conduct experiments to demonstrate its effectiveness in learning optimal pricing mechanisms when selling multiple goods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The overall framework of this paper is clear. The concept of generalized convexity could be an interesting topic for further exploration."}, "weaknesses": {"value": "The theoretical contribution of this paper is limited. The work appears to summarize several properties related to generalized convexity but does not provide any particularly insightful perspectives. In addition, some important lemmas and theorems, such as Theorem 5.3, lack sufficient explanation. I believe that including several necessary remarks would be more helpful for readers."}, "questions": {"value": "My main concern about this paper lies in its theoretical contribution. The current version appears to lack technical novelty. Specifically, Sections 3.2 and 5 seem to be a collection of properties related to generalized convexity. Could the authors provide a more detailed summary of the technical novelty and contributions?\n\nRegarding the experimental section, the authors only conduct experiments on a few illustrative cases. I am wondering whether there are any experiments on real-world benchmarks, such as CIFAR or similar datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "56yqQ3OYVf", "forum": "XoRApddgFG", "replyto": "XoRApddgFG", "signatures": ["ICLR.cc/2026/Conference/Submission7643/Reviewer_qwjF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7643/Reviewer_qwjF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794072457, "cdate": 1761794072457, "tmdate": 1762919716253, "mdate": 1762919716253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}