{"id": "x66u6TEDUw", "number": 9040, "cdate": 1758108303292, "mdate": 1763761842930, "content": {"title": "Improving Long-Range Interactions in Graph Neural Simulators via Hamiltonian Dynamics", "abstract": "Learning to simulate complex physical systems from data has emerged as a promising way to overcome the limitations of traditional numerical solvers, which often require prohibitive computational costs for high-fidelity solutions. Recent Graph Neural Simulators (GNSs) accelerate simulations by learning dynamics on graph-structured data, yet often struggle to capture long-range interactions and suffer from error accumulation under autoregressive rollouts. To address these challenges, we propose Information-preserving Graph Neural Simulators (IGNS), a graph-based neural simulator built on the principles of Hamiltonian dynamics. This structure guarantees preservation of information across the graph, while extending to port-Hamiltonian systems allows the model to capture a broader class of dynamics, including non-conservative effects. IGNS further incorporates a warmup phase to initialize global context, geometric encoding to handle irregular meshes, and a multi-step training objective to reduce rollout error. To evaluate these properties systematically, we introduce new benchmarks that target long-range dependencies and challenging external forcing scenarios. Across all tasks, IGNS consistently outperforms state-of-the-art GNSs, achieving higher accuracy and stability under challenging and complex dynamical systems.", "tldr": "We propose a novel Graph Neural Simulator that preserves information during propagation, enabling it to model complex physical dynamical systems with long-range dependencies.", "keywords": ["Graph Neural Simulators", "Long-range interactions", "Learning Simulators", "AI4Science"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3624fe620eca132c4e3f8866c21296a6e056fd48.pdf", "supplementary_material": "/attachment/a7223a9d9ec5d1683ea36ff13f58d2ea3344f244.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Information-preserving Graph Neural Simulators (IGNS), a graph-based neural simulator that improves modeling of complex physical systems. IGNS enforces Hamiltonian dynamics to preserve long-range interactions and extends to non-conservative systems. It includes warmup initialization, geometric encoding, and multi-step training to enhance stability. Evaluated on new benchmarks with long-range dependencies and external forces, IGNS outperforms state-of-the-art methods in accuracy and robustness for dynamic systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed Information-preserving Graph Neural Simulator introduces a principled integration of port-Hamiltonian dynamics into graph-based simulators, marking a significant step beyond existing message-passing and oscillatory GNN frameworks.\n- Theoretical analyses are thorough and provide a clear justification for the model’s ability to capture complex and long-range physical interactions. \n- Experimental evaluation is comprehensive, spanning six datasets and consistently demonstrating the superior accuracy and stability of IGNS compared to strong baselines.\n- The paper is clearly written and well structured: the motivation for each component (port-Hamiltonian core, warmup phase, geometric encoding, and multi-step loss) is clearly articulated, and the accompanying figures effectively convey both the methodology and the empirical findings."}, "weaknesses": {"value": "- Although the theoretical analysis establishes information preservation and universality, it remains largely qualitative in linking these properties to the observed empirical improvements. A more quantitative or ablation-based verification (e.g., measuring gradient norms or energy conservation over rollouts) would provide stronger evidence for the theoretical claims.\n- The training/testing computational overhead of the port-Hamiltonian formulation and the warmup phase is not explicitly analyzed; reporting runtime or memory costs relative to standard GNSs would clarify the practical trade-offs. \n- Although the benchmarks are diverse, most tasks are synthetic or controlled simulations. It would strengthen the paper’s significance to include or discuss applications in more realistic or large-scale physical systems.\n- The geometric encoding used to map edges to features follows the same formulation as previous works (e.g., MGN) and therefore cannot be considered a novel contribution.\n- Several related approaches are not cited or compared [1–4], which limits the contextual positioning of this work within recent advances in graph-based physical simulation. \n- Introducing \"warmup phase\" in GNNs is not novel. Eagle [2] employs a warmup-like phase in its encoder, using multiple message-passing blocks to aggregate local and global context before rollout, which parallels the proposed initialization strategy.\n- The separation of state variables into coordinates and momenta, as well as the coordinate–momentum supervision in Eq. (10), are established techniques already used in [2–3].\n\n[1] EvoMesh: Adaptive Physical Simulation with Hierarchical Graph Evolutions. ICML 2025\n\n[2] Eagle: Large-Scale Learning of Turbulent Fluid Dynamics with Mesh Transformers. ICLR 2023\n\n[3] Efficient Learning of Mesh-Based Physical Simulation with BSMS-GNN. ICML 2023\n\n[4] Physics meets Topology: Physics-informed topological neural networks for learning rigid body dynamics"}, "questions": {"value": "1. In L201-203 and in Appendix D, the paper states that $\\gamma_\\theta(t)$ and $\\tau_\\theta(t)$ are time-varying coefficient vectors produced by MLPs with parameters θ. Could the authors clarify what exactly is used as the input t to these MLPs? Is t normalized to a fixed range (e.g., [0, 1]) or directly represented as the raw timestep index? Additionally, if the model is trained on trajectories with 400 steps, can the learned time-dependent MLPs generalize to longer rollouts (e.g., 1000 steps) without retraining, or does the model rely on an absolute temporal scale?\n2. L231-233 states: \"Thanks to the energy conserving core of IGNS, this globally informed latent state is preserved throughout the rollout, rather than being dissipated.\" Could the authors clarify why this happens? Can you provide qualitative or quantitative analysis showing how the latent state is preserved over time？\n3. Regarding the multi-step loss: how many time steps were included in the loss computation? Are the reported results based on single-step MSE or on the rollout of the entire sequence? Please include these experiment details.\n4. Why does WaveBall not require a warmup phase?\n5.  In the supplementary code (`igns.py`), L651: `x = self.one_step(x, edge_index, edge_weight, batch, t=i)` passes `i` as `t`. Here, `i` corresponds to the layer index rather than the time step. Can the authors explain why this is done?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HcyQAmUy8g", "forum": "x66u6TEDUw", "replyto": "x66u6TEDUw", "signatures": ["ICLR.cc/2026/Conference/Submission9040/Reviewer_S8R7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9040/Reviewer_S8R7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760847700775, "cdate": 1760847700775, "tmdate": 1762920758315, "mdate": 1762920758315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a graph neural simulator built on a port-Hamiltonian system. Unlike the existing relevant literature, the core idea of the proposed method is to introduce the symplectic integrator while proposing a port Hamiltonian involving non-conservative energy terms in order to closely align the proposed emulator with the ODE dynamics. The warmup iteration is also proposed in an attempt to enhance the long-range message propagation. The paper also conducts theoretical analysis about the universality and the sensitivity of the model. The proposed framework is evaluated on a range experiment including a couple of new scenarios designed to assess the long-range propagation capability under external forcing.\n\n\nOverall, I think the paper is not ready for the publication in the current form, because 1) the main arguments about the theoretical analysis and proposed architecture are over-claimed and loose, and 2) the experiments miss some relevant baselines and essential ablation study. In particular, Theorem 2 is almost completely identical to a theoretical result in an existing literature. A detailed clarification is necessary to highlight the difference. The detail of the warmup iteration is also missing, which is another factor that causes the difficulty to assess the significance and soundness of the contribution of the paper. The details are given in \"Weaknesses\" and \"Questions\" columns."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The universality result (although I do not fully understand the proof yet) is novel. \n- Proposal of new tasks, specifically designed to test long-range propagation and oscillatory dynamics under external forcing."}, "weaknesses": {"value": "**Over-claimed assertions:**\n- A sentence starting at line 290 regarding Theorem 1 is over-claiming. Being with/without compact supports makes a huge difference in the significance.\n- The multi-step objective is a pretty common loss function for training auto-regressive models. The authors need to cite relevant papers or argue that this is a pretty common approach. The use of symplectic integrator in this context is also not novel. \n- Theorem 2 is almost identical to a sensitivity result in [1], and it is unclear if this theorem is different enough from the result of [1] to claim it as novel and/or original.\n\n**Misdirected experiments:**\n- The core idea of the paper is closely related to the idea in [1], but apparently the baselines in the main experiments do not include the model, which makes the experiments setting look unfair and the result unfairly unconvincing.\n- The ablation version of damping and forcing/residual terms is missing. This ablated model is essentially equivalent to the idea in [1] adopted to symplectic integrator with well-adopted multi-step loss, which I believe is valuable to be compared.\n- The other fundamental difference is use of symplectic integrator and warmup iteration, but the ablation experiment also misses these aspects.\n- The paper addresses the long-range propagation problem, but the experiment misses the impact of increasing the resolution of the space, which controls the difficulty of the long-range message propagation. \n\n**Inaccurate description on the assertion and proof of Theorem 1:**\n- Line 287: $\\dot{x}_{0}$ should not be included.\n- Theorem 1 should need a compact support on which $F$ is approximated by $\\Psi_{\\theta}$.\n- Line 855: The second Hamiltonian equation. This should be introduced formally.\n- Line 1028 misses the definition of $\\bar{p}$ and the relation between $p$ and $q$ is unclear, so the derivation of (24) is non-trivial.\n\n**Minor:**\n- Typo: the map $\\Phi$ is duplicated in line 987\n- Diameter is important metric in this context, but it is missing from Table 3\n\n\n[1] Heilig, et.al., \"Port Hamiltonian Architectural Bias for Long-Range Propagation in Deep Graph Networks.\", ICLR 2025."}, "questions": {"value": "**Regarding the warmup iteration:**\n- How exactly does the warmup iteration work? I cannot find its detail including the update formula for each iteration.\n- Is the port-Hamiltonian used in the warmup phase shared with the time-evolving forward model? \n\n**Proof of Theorem 1:**\n- Why is it fine to set $D$ to be $0$ without the loss of generality? It is not obvious at first glance.\n- What does B represent at line 989. Is it a parameter involved in $r(t)$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "N/A"}}, "id": "reD1qXiKA4", "forum": "x66u6TEDUw", "replyto": "x66u6TEDUw", "signatures": ["ICLR.cc/2026/Conference/Submission9040/Reviewer_eSDm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9040/Reviewer_eSDm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540817790, "cdate": 1761540817790, "tmdate": 1762920757721, "mdate": 1762920757721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Information-preserving graph neural simulators (IGNS). It is a novel approach aimed to improve long-range propagation of information and to reduce error accumulation in task related with physical modelling. By using port-Hamiltonian dynamics, IGNS preserves information across graphs and captures conservative and non-conservative dynamics. Authors use some key innovations, for example: a warmup phase for initializing global context, geometric encoding for irregular meshes, and a multi-step training objective for stable long-horizon predictions.\n\nAdditionally, the authors provide strong theoretical guarantees for IGNS's universality and gradient preservation. Authors provide comprehensive experiments on six benchmarks, including new tasks. IGNS consistently outperforms many baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea is novel and interesting. \n2. The paper has solid theoretical foundations (universality, non-vanishing gradients) and robust experimental design against many strong baselines across different physical systems.\n3. The paper is generally well-written, logically structured, with clear problem statements and architecture descriptions. Good use of figures, tables, and appendices.\n4. It offers a substantial advancement in GNSs by solving critical long-standing limitations."}, "weaknesses": {"value": "1. A direct ablation for authors' static geometric encoding feels like missed. The authors make an argument that model's specific geometric encoding helps avoid \"overfitting,\" which is important for generalizing well. However, authors don't really show us an experiment where they directly compare their own model with and without developed static encoding. Instead, they rely an indirect comparison to other architectures.\n2. No inference time analysis. It it is hard to understand, beyond just training time, how fast do the models predict one or multiple steps during inference? This is crucial for \"accelerate simulations\". \n3. No scale analysis. How does the computational cost scale with increasing graph size (nodes, edges) or rollout horizon?"}, "questions": {"value": "1. See W2 and W3. It is interesting to see results of inference time and scale analysis.\n\n2. In the message-passing update you use only q_{i} and q_{j} (see eq. 9). Does this exclude the momentum variables p_{i} and p_{j} from exchange between neighbors, or is q there meant to denote the full node state [q,p]? Please clarify.\n\n3. Did you conduct an ablation study to directly compare static geometric encoding of IGNS against a dynamic geometric encoding strategy (e.g., updating edge features per time step) within the IGNS framework (not changing the model)? It is needed to empirically validate its claimed benefit in preventing overfitting. If so, where are these results presented? If not, please conduct this ablation study."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qG7Y8ly2gj", "forum": "x66u6TEDUw", "replyto": "x66u6TEDUw", "signatures": ["ICLR.cc/2026/Conference/Submission9040/Reviewer_KM4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9040/Reviewer_KM4o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757552991, "cdate": 1761757552991, "tmdate": 1762920757307, "mdate": 1762920757307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Information-preserving Graph Neural Simulators (IGNS) for learning physical dynamics of  both conservative and non-conservative port-Hamiltonian systems. Key features of the work are: 1) Hamiltonian formalism to eliminate part of the model systematic drifts; 2) a warmup phase for global context initialization; 3) a multi-step training loss for long-horizon stability.\nThe authors also present new benchmarks (Plate Deformation, Sphere Cloth, Wave Balls) to test long-range dependencies.\nIGNS achieves good results across all datasets, showing higher or comparable accuracy and stability than MeshGraphNets and GraphCONs together with some other methods."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* novelty in introducing a port-Hamiltonian formalism to graph simulators\n* theoretical proofs of universality and non-vanishing gradients\n* data efficiency, importance of warm-up steps and length of confident prediction horizon were investigated\n* code attached"}, "weaknesses": {"value": "* the advantages of Hamiltonian dynamics simulation were not properly studied (like the energy conservation for the conservative systems)\n* the generalizability is under question\n* the file dataset.zip in supplimentary link is corrupt\n* the paper lacks qualitative discussion of the distinction between two versions of the algorithm is not clear (IGNS, IGNS_ti (time-independent))"}, "questions": {"value": "* What are the computational expensies of your model training and inference compared to competitors and numerical simulator?\n* What are the limits of your model generalization? (out of distribution, new geometries, etc.)\n* Why for Kuramoto-Sivashinsky equation the results for IGNS_ti are so much better than for IGNS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hPPz1S8Gb7", "forum": "x66u6TEDUw", "replyto": "x66u6TEDUw", "signatures": ["ICLR.cc/2026/Conference/Submission9040/Reviewer_Rdux"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9040/Reviewer_Rdux"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958451935, "cdate": 1761958451935, "tmdate": 1762920756684, "mdate": 1762920756684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Answer to all reviewers"}, "comment": {"value": "We thank the reviewers for acknowledging the clear presentation, the comprehensive empirical study, and the provided theoretical justification of our work.\n\nWe see three common concerns among the reviews, namely method clarity, ablations on core components, and complexity analysis. To address these and individual concerns, we revised the submission, with changes marked in Blue. In particular, we want to highlight:\n\n1. **Method clarity**. Our goal is to improve long-range dependencies in Graph Neural Simulators. To this end, we use a port-Hamiltonian system as the dynamics core. Our Theorem 1 shows that integrating this system can approximate the target dynamics. This enables **PDE matching**, where we can **supervise the full trajectory with a fixed-window** $T$ **using multi-step loss at both training and testing**. This process mirrors classical PDE solvers, where information propagates in space as time advances. Empirically, we show that we can obtain high accuracy on complex tasks like Wave Balls with long horizons (up to $T=200$), where autoregressive methods such as MGN fail severely (see Tab. 12).\n    \n    We recall that prior work typically uses short $T \\leq 20$ in both training and testing [1,2] or trains on short windows and then rolls out autoregressively [3], or applies multi-step supervision only in a reduced latent space [4] rather than on the original graph.\n    \n2. **Ablations and analysis**. We added an energy conservation analysis (App. I). In Fig. 7 (in the new revision), the conservative core (no forcing/damping) keeps normalized energy oscillating or nearly constant with sufficiently small $\\Delta t$. Meanwhile, the full model exchanges energy as expected. In addition, Fig. 6 investigates the contribution of individual terms, showing validation curves for IGNS/IGNS_ti without forcing and/or damping, as well as without the warmup phase or geometric encoding. This ablation explains why the full model IGNS with all components obtains the highest accuracy.\n3. **Complexity and runtime.** IGNS keeps the graph convolution backbone’s linear cost w.r.t. node and edge count, meaning one step is $\\mathcal{O}(∣V∣+∣E∣)$; Assuming $L$ warmup steps and rollouts of length $T$, the total cost is, the total cost is $\\mathcal{O}((L+T)(∣V∣+∣E∣))$. For comparison, MGN needs $S$ message-passing updates per step (often $S=15$), resulting in a higher cost of $\\mathcal{O}(ST(∣V∣+∣E∣))$. We added an inference runtime table in Tab. 8, which consistently shows that IGNS is faster than MGN.\n4. **Two new task variants.** The additional ablations were done on two new task variants, namely, *SphereCloth-direct* (where we only connect the ball to four corners) to probe long-range propagation and the contribution of individual components, and *Wave Balls-200* (trained with curriculum scheduling as in [1]) to test long-horizon supervision capability and run-time analysis.\n\nAgain, we thank the reviewers for their efforts and provide detailed answers to their individual reviews to clarify and address their individual concerns. We hope our clarifications and revisions will address your concerns and prompt a reconsideration of your evaluation and score.\n\n[1] Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks, ICLR 2023\n\n[2] RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools. CoRL 2023\n\n[3] Eagle: Large-Scale Learning of Turbulent Fluid Dynamics with Mesh Transformers. ICLR 2023\n\n[4] Predicting Physics in Mesh-reduced Space with Temporal Attention. ICLR 2022"}}, "id": "RcehOkW8Ia", "forum": "x66u6TEDUw", "replyto": "x66u6TEDUw", "signatures": ["ICLR.cc/2026/Conference/Submission9040/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9040/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission9040/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763762270637, "cdate": 1763762270637, "tmdate": 1763768521972, "mdate": 1763768521972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}