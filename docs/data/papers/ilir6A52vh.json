{"id": "ilir6A52vh", "number": 21926, "cdate": 1758323634260, "mdate": 1759896895805, "content": {"title": "Planning at Inference: MCTS Test-Time Scaling for Long Video Generation", "abstract": "Generating long videos with consistent content and visual quality remains a ma-\njor challenge, as existing one-shot and chunked methods often suffer from se-\nmantic drift and compounding artifacts. We explore Test-Time Scaling (TTS)\nas a framework for long video generation, formulating the task as a sequential\ndecision-making problem. Our approach uses Monte Carlo Tree Search (MCTS)\nto evaluate multiple continuations with look-ahead rollouts and backpropagated\nrewards, and we introduce a Multi-Tree MCTS variant that improves exploration\nin continuous generation spaces. The method is modular and can be applied to ex-\nisting backbones without retraining. Experiments on Cosmos-Predict2 and other\nmodels show consistent improvements in object permanence, temporal coherence,\nand text-video alignment over Best-of-N, Greedy, and Beam search. Furthermore,\nour method produces high-quality videos exceeding 20 seconds, surpassing the\noutput of leading models like Sora and Kling by 18% and 47% respectively, all\nwhile maintaining comparable visual fidelity. Although the results are limited\nby the quality of current generators and verifiers, our study highlights both the\npromise of search-based TTS and the limitations of today’s video generation and\nevaluation models.", "tldr": "We leverage Monte Carlo Tree Search–based test-time scaling to select better continuations, enabling the generation of coherent long videos.", "keywords": ["Video Generation", "Long Video Generation", "Test Time Scaling", "Monte Carlo Tree Search"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f1fa4e58f6fc3e1b843f6d1596141449216660b9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper frames long video generation as sequential decision making and proposes test time search with Monte Carlo Tree Search to plan over chunked continuations, guided by a process reward model for local chunk quality and an outcome reward model that aggregates scores over the full sequence. The method is model agnostic, sits on top of existing backbones without retraining, and introduces a multi tree variant to widen exploration in continuous spaces. Across several generators, the approach improves temporal consistency and object permanence relative to autoregressive decoding, Best of N, greedy, and beam search, and reports longer, competitive quality videos when compared qualitatively and with automated metrics to recent long video systems. The paper provides algorithmic details, ablations on compute budget, and comparisons of single tree versus multi tree search, while also acknowledging dependencies on the underlying generator and verifier quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear formulation of long video generation as planning with Monte Carlo Tree Search, including a walk through of selection, expansion, rollout, and backpropagation plus an explicit UCB objective.\n\n2. Multi tree search broadens exploration under a fixed branching factor and empirically outperforms single tree for the same budget.\n\n3. Practical recipe that is plug in and does not require retraining, which increases utility for current systems constrained by backbone quality."}, "weaknesses": {"value": "1. Heavy reliance on automated reward signals for both search guidance and evaluation, with outcome reward defined as a simple sum over chunks, risks overfitting to verifier idiosyncrasies rather than human preference on long horizon coherence. A controlled human study is missing.\n\n2. The exploration constant, branching factor, rollout policy, and beam initialization depth can strongly affect MCTS behavior. Sensitivity analysis is not comprehensive."}, "questions": {"value": "1. How sensitive are results to the weighting of VideoScore, CLIP alignment, and the LAION perceptual model in the process reward, and to the definition of the outcome reward as a sum rather than a learned temporal model\n\n2. Under a fixed wall clock and identical hardware, how does the method compare to beam and greedy tuned for the same final runtime, including beam initialization time and rollout parallelism"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xmyn5DbZo3", "forum": "ilir6A52vh", "replyto": "ilir6A52vh", "signatures": ["ICLR.cc/2026/Conference/Submission21926/Reviewer_6AqY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21926/Reviewer_6AqY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800825984, "cdate": 1761800825984, "tmdate": 1762941983950, "mdate": 1762941983950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes using MCTS for planning-based long video generation, which expands an important direction in the TTT field. Through this approach, the paper even achieves long video generation results that surpass closed-source SOTA models, demonstrating the potential of TTT in long video generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The work has a certain degree of novelty and community value. The paper is the first to apply MCTS-based TTT to long video generation, showcasing the value of classical methods in the video domain.\n- The experimental results are impressive. The proposed method enables Cosmos-Predict2 to surpass or tie with closed-source SOTA models (Sora/Kling), which demonstrates the strong potential of TTT."}, "weaknesses": {"value": "- Tab. 5 should include a comparison of the computational cost.\n- Regarding the long-video baselines, the paper would be more sound if a more comprehensive set could be included [1,2]\n\n[1] FIFO-Diffusion: Generating Infinite Videos from Text without Training\n\n[2] Skyreels-v2: Infinite-length film generative model\n\n- The paper lacks discussion and comparison with several recently accepted works on long-video generation.\n\n[1] Zhao et al., Riflex: A Free Lunch for Length Extrapolation in Video Diffusion Transformers (ICML 2025).\n\n[2] Tan et al., FreePCA: Integrating Consistency Information Across Long-Short Frames in Training-Free Long Video Generation via Principal Component Analysis (CVPR 2025).\n\n[3] Lu et al., FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention (NeurIPS 2024).\n\n[4] Cai et al., DitCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation (CVPR 2025)."}, "questions": {"value": "See the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "agahc7Vbmo", "forum": "ilir6A52vh", "replyto": "ilir6A52vh", "signatures": ["ICLR.cc/2026/Conference/Submission21926/Reviewer_UF3o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21926/Reviewer_UF3o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977464734, "cdate": 1761977464734, "tmdate": 1762941983632, "mdate": 1762941983632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author introduces a Multi-Tree MCTS variant that improves exploration in continuous generation spaces."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written.\n\n2. The author introduces a Multi-Tree MCTS variant that improves exploration in continuous generation spaces. It is interesting."}, "weaknesses": {"value": "1. I would like to know the time it takes to generate a 1-minute video with and without using your MCTS, and provide a quantitative comparison of the results.\n\n2. The biggest issue with video generation is the excessive time consumption. This MCTS could make generating a long video take 24 hours, potentially requiring 20 times more time.\n\n3. It is difficult to implement. The biggest challenge of this model is the accurate training of the Process Reward Model and Outcome Reward Model. As we know, video quality is hard to evaluate (the error rate of evaluation is high). Any slight error in the evaluation of these two models could lead to a massive search error.\n\n4. MCTS does not have good robustness for the Process Reward Model and Outcome Reward Model.\n\n5. I believe the author should focus on reinforcing the video model with reinforcement learning instead of using TTS, as it is a more efficient and practical solution."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GeXYcew9ZC", "forum": "ilir6A52vh", "replyto": "ilir6A52vh", "signatures": ["ICLR.cc/2026/Conference/Submission21926/Reviewer_kWNU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21926/Reviewer_kWNU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010464903, "cdate": 1762010464903, "tmdate": 1762941983368, "mdate": 1762941983368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}