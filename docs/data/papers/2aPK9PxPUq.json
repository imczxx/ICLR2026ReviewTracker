{"id": "2aPK9PxPUq", "number": 2907, "cdate": 1757299646461, "mdate": 1763683884780, "content": {"title": "Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer", "abstract": "In this work, we present Color3D, a highly adaptable framework for colorizing both static and dynamic 3D scenes from monochromatic inputs, delivering visually diverse and chromatically vibrant reconstructions with flexible user-guided control. In contrast to existing methods that focus solely on static scenarios and enforce multi-view consistency by averaging color variations which inevitably sacrifice both chromatic richness and controllability, our approach is able to preserve color diversity and steerability while ensuring cross-view and cross-time consistency. In particular, the core insight of our method is to colorize only a single key view and then fine-tune a personalized colorizer to propagate its color to novel views and time steps. Through personalization, the colorizer learns a scene-specific deterministic color mapping underlying the reference view, enabling it to consistently project corresponding colors to the content in novel views and video frames via its inherent inductive bias. Once trained, the personalized colorizer can be applied to infer consistent chrominance for all other images, enabling direct reconstruction of colorful 3D scenes with a dedicated Lab color space Gaussian splatting representation. The proposed framework ingeniously recasts complicated 3D colorization as a more tractable single image paradigm, allowing seamless integration of arbitrary image colorization models with enhanced flexibility and controllability. Extensive experiments across diverse static and dynamic 3D colorization benchmarks substantiate that our method can deliver more consistent and chromatically rich renderings with precise user control. The code will be publicly available.", "tldr": "We propose a unified and versatile 3D colorization framework, termed Color3D, which enables user-guided colorization of both static and dynamic scenes.", "keywords": ["3D Gaussian Splatting", "3D Editing", "3D Colorization", "3D Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a141444dbe9477113083db3ab2ea038e8d2304a6.pdf", "supplementary_material": "/attachment/ded594fc74d8b7ba5ddf69ace7c73542a90fa34b.zip"}, "replies": [{"content": {"summary": {"value": "Color3D proposes a method for colorizing monochromatic images in 3D representations for static and dynamic scenes. Naive implementations, like colorizing 2D multi-view images and reconstructing the 3D scene lead to severe cross-view inconsistencies. Recent methods that distil the color information to a 3D representation sacrifice controllability and often have desaturated colors in the final output.\n\nKey contributions of the proposed methodology are as follows:\n\n-   The key idea is to colorize (automatic/reference-based/prompt-based)  a single \"key\" view which is the most informative one and then fine-tune a scene-specific colorization network on that view.\n-   This scene-specific colorizer learns a deterministic color mapping for the scene and is then applied to all other views/frames, enforcing cross-view and cross-time color  consistency .\n-   Finally, the colorized views (with known luminance and predicted chrominance) are fused into a Gaussian Splatting representation in Lab color space.\n\nExperiments on standard benchmarks (LLFF, Mip-NeRF 360, DyNeRF) and \"in-the-wild\" legacy videos show that Color3D produces vivid and consistent colorizations."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **[S1] Technical Novelty:** The per-scene colorization for a single view is a novel idea which achieves consistent colorization across views. The robust technical pipeline achieves this consistency through key design choices. Specifically, utilizing a pre-trained 2D colorization encoder (DDColor) preserves the generalization capability of the model. The use of the Lab color space leads to more stable results. Finally, warm-up training of the 3D representation on the Luma ($\\text{L}$) channel first ensures the model establishes a strong geometric structure before introducing complex color information.\n\n- **[S2] Practical Applications:** The authors show results on \"in-the-wild\" multi-view images and historical video (Fig.6), producing vivid and plausible colors while maintaining consistency.\n\n- **[S3] Controllability**: The proposed method allows users to control the colorization using text descriptions or reference images. Earlier methods did not offer this type of user control.\n\n- **[S4]** Thorough Experimentation for Key-View selection: The authors performed detailed experiments for \"Key-View Selection\" module. Results in Fig. 7 demonstrate that this module is critical for better colorization."}, "weaknesses": {"value": "- **[W1] Limited Comparison to Recent Methods:** The authors mention \"ChromaDistill\" in Related Work, but do not perform a quantitative comparison. For complete experimentation, it is also necessary to quantitatively compare the results against a video colorization baseline.\n\n- **[W2] Cross-View Consistency**: ChromaDistill utilized a  long-term and short-term view consistency method for measuring the geometric consistency. However, the authors do not show this metric. The manuscript will benefit from this metric.\n\n- **[W3]** As the method utilizes training for each scene, it has an extra training overhead.\n\n\n**Typos:**\n\n-   L74: It should be \"aim\" instead of \"aims\".\n-   L857: It should be \"suffers\" instead of \"suffer\".\n-   L347: It should be \"entire\" instead of \"entir\"."}, "questions": {"value": "-   **[Q1]** Have the authors tried fine-tuning on _more_ than one view? Using two or three colorized views might improve coverage for large scenes. Is there a reason the approach is limited to one view?\n\n-   **[Q2]** How does the proposed method handle motion for dynamic scenes? In case the object that was not in the key view appears in the scene, how is its colour determined? Does the generative augmentation simulate such cases? This is not clear in the current manuscript."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "I2fvjk8qc7", "forum": "2aPK9PxPUq", "replyto": "2aPK9PxPUq", "signatures": ["ICLR.cc/2026/Conference/Submission2907/Reviewer_TQsU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2907/Reviewer_TQsU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822848153, "cdate": 1761822848153, "tmdate": 1762916438528, "mdate": 1762916438528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Brief Summary of Paper Revisions"}, "comment": {"value": "We sincerely thank all the reviewers for their valuable suggestions. We have updated the manuscript with the following revisions based on the reviewers' suggestions. A brief summary is as follows:\n\n1. **More experimental results:**\n\n    - Add long-term and short-term temporal consistency (TC) metric across all relevant tables.\n    - Add more comparisons on the 140-scene DL3DV-140 and the dynamic HyperNeRF datasets (Tab. 1 and Tab. 2).\n    - Add video colorization model ColorMNet as a reference baseline (Tab. 1 and Tab. 2).\n    - Add comparisons with ChromaDistill (Tab. 5).\n    - Add visualizations under challenging lighting conditions (Fig. 18).\n\n2. **More discussion and analysis:**\n\n    - Provide analysis of the generalizability of the personalized colorizer (Line 452).\n    - Provide diagnostic analysis of color consistency in personalized colorizer (Line 463).\n    - Provide rigorous theoretical analysis of color consistency in personalized colorizer (Line 882).\n    - Introduce an all-in-one fine-tuning scheme for large-scale datasets (Line 999).\n    - Introduce a feed-forward 3D colorization paradigm (Line 1019).\n    - Provide analysis of the number of key views (Line 1107).\n    - Provide analysis of the robustness of key view selection (Line 1130).\n    - Provide analysis of generative augmentation for dynamic scenes (Line 1159).\n    - Provide analysis of colorization for semantically similar objects (Line 1184).\n    - Provide analysis of optimization efficiency (Line 1220).\n\nWe hope the revised paper can effectively address reviewers' concerns. Please let us know if there are any further problems or suggestions."}}, "id": "zIIA3HyQRM", "forum": "2aPK9PxPUq", "replyto": "2aPK9PxPUq", "signatures": ["ICLR.cc/2026/Conference/Submission2907/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2907/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2907/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763684819646, "cdate": 1763684819646, "tmdate": 1763684819646, "mdate": 1763684819646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. This paper focuses on the task of controllable and consistent 3D scene editing. It aims to address the limitations of previous methods that often lack precise controllability and multi-view color consistency.\n\n2. The authors propose Color3D, a two-stage framework. In the first stage, a text-to-image diffusion model is used to modify the reference view’s color according to user input. In the second stage, Score Distillation Sampling (SDS) is applied to enforce 3D consistency. Unlike prior work, the method performs Gaussian Splatting optimization in the LAB color space, which helps maintain color consistency across different viewpoints.\n\n3. Experiments on Color-Edit-3D dataset show that Color3D achieves state-of-the-art results. The ablation study highlights the importance of the Color Consistency Regularization (CCR) module, which provides a clear performance gain when included."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is technically sound and well-motivated.\n\n2. The experiments are thorough and provide strong empirical validation."}, "weaknesses": {"value": "1. The computational cost is unclear — how long does the optimization take during the second stage?\n\n2. The main novelty seems to lie in optimizing in the LAB color space instead of RGB. While this is a reasonable choice for editing tasks, the contribution may be somewhat limited in scope.\n\n3. It would be interesting to see how the approach performs under challenging lighting conditions (e.g., scenes with lamps or strong reflections)."}, "questions": {"value": "Please address the issues listed in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2cH37clt6w", "forum": "2aPK9PxPUq", "replyto": "2aPK9PxPUq", "signatures": ["ICLR.cc/2026/Conference/Submission2907/Reviewer_Rsix"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2907/Reviewer_Rsix"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919444309, "cdate": 1761919444309, "tmdate": 1762916438283, "mdate": 1762916438283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Color3D, a framework for colorizing both static and dynamic 3D scenes (represented by 3D/4D Gaussian Splatting) from grayscale inputs. Its core idea is to avoid the multi-view inconsistency of applying 2D colorizers independently by instead personalizing a single colorization model per scene. This is done by selecting and colorizing one key view, then fine-tuning a pre-trained colorizer (using adapters) on augmented versions of this single view to learn a scene-specific, deterministic color mapping. This personalized colorizer is then used to colorize all other views/frames consistently. A dedicated Lab color space Gaussian representation with a warm-up strategy is used to improve reconstruction fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow. The proposed method can be adapted to various colorization models, making it highly practical and valuable for real-world applications, especially in AR/VR scenarios.\n\n2. The experimental results on LLFF and Mip-NeRF 360 and D-NeRF colorization setting have shown the Color3D's good performance, not only on statistic 3d scenes but also on dynamic 4d scenes. Also, the method demonstrably outperforms existing alternatives across multiple  metrics (FID, CLIP Score, Matching Error), showing superior consistency, color vividness, and alignment with user intent."}, "weaknesses": {"value": "1. It seem like the requirement to fine-tune a personalized colorizer for every new scene adds a non-trivial computational cost (∼8 minutes per scene) compared to a generic, one-time-trained model, limiting its scalability for large-scale applications.\n2. The entire color propagation relies on a single key view. If this view is unrepresentative or lacks critical scene elements, the colorizer's generalization may be hampered, potentially leading to incomplete or less vibrant colorization in occluded regions."}, "questions": {"value": "The paper shows that the personalized colorizer is robust to viewpoint changes from the key view. How does it handle novel views containing objects or textures that are semantically similar but visually different (e.g., another type of chair)? Would it incorrectly transfer learned colors or struggle to color them plausibly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LQL4JGuCaB", "forum": "2aPK9PxPUq", "replyto": "2aPK9PxPUq", "signatures": ["ICLR.cc/2026/Conference/Submission2907/Reviewer_Rt9r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2907/Reviewer_Rt9r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972251085, "cdate": 1761972251085, "tmdate": 1762916438093, "mdate": 1762916438093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Color3D, a unified framework for colorizing both static and dynamic 3D scenes reconstructed from monochrome inputs. Rather than colorizing multiple views independently, which causes cross-view inconsistency, the method colorizes one key view using any off-the-shelf 2D colorization model, then fine-tunes a per-scene personalized colorizer to propagate the learned color mapping to all other views or time steps.E xperiments on LLFF, Mip-NeRF 360, and DyNeRF datasets demonstrate consistent and controllable colorization across viewpoints and time, with improvements in FID, CLIP score, and Matching Error compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**High controllability and flexibility.**\nThe system supports multiple control modalities: reference-based colorization, language-conditioned colorization, and automatic default color prediction.\n\n**Computational practicality.**\nDespite using fine-tuning, the reported per-scene personalization time (~8 minutes) is relatively efficient compared to retraining full colorization networks. The framework does not require full 3D model retraining, and adapters make it lightweight enough for scene-level deployment."}, "weaknesses": {"value": "**Limited generalization beyond scene-specific fine-tuning.**\nThe reliance on per-scene personalized colorizer tuning implies that a new model must be trained for each scene. This restricts scalability for large datasets or interactive applications. The approach is elegant but computationally heavy when many scenes must be processed.\n\n**Inductive bias assumption not rigorously examined.**\nThe claim that a single-view-trained colorizer generalizes to novel viewpoints via inductive bias remains empirical. The paper could benefit from deeper theoretical or diagnostic analysis to substantiate why the learned mapping remains consistent under large view changes.\n\n**Limited evaluation.**\nThe datasets being evaluated on are LLFF (static), Mip-NeRF-360 (static), and DyNeRF(dynamic). Although promising results are shown, the scale of evaluation is still limited. Also, for dynamic scenes, there are more dimensions for evaluation like temporal color-consistency, which is missing in the current experiments."}, "questions": {"value": "1. How stable is the personalized colorizer’s output under large camera baselines (e.g., >60° change)? If still good, what are the fundamental factors that may contribute to this effect?\n\n2. How sensitive is performance to errors in key-view selection?\n\n3. How could this appoarch be generalized to feed-forward style 3D generation models? This means per-scene optimizations are removed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ju3tRjBijV", "forum": "2aPK9PxPUq", "replyto": "2aPK9PxPUq", "signatures": ["ICLR.cc/2026/Conference/Submission2907/Reviewer_uPuV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2907/Reviewer_uPuV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997472529, "cdate": 1761997472529, "tmdate": 1762916437939, "mdate": 1762916437939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}