{"id": "xszAt23a17", "number": 4402, "cdate": 1757674096660, "mdate": 1759898034435, "content": {"title": "CHOrD: Synthesizing Spatially Coherent, House-Scale, Organized, and Diverse 3D Indoor Scenes via Image-Based Layout Guidance", "abstract": "We introduce CHOrD, a generative framework for synthesizing spatially coherent, house-scale, hierarchically organized, and diverse 3D indoor scenes. At the core of CHOrD is a two-stage generation paradigm: given a floor plan, CHOrD first synthesizes an intermediate, image-based 2D layout representation, which is subsequently transformed into a graph-based scene structure. In contrast to existing tabular-based or LLM-based generative models, the enhanced spatial capabilities of CHOrD substantially reduce long-standing artifacts frequently observed in prior work—such as physically implausible collisions, out-of-bound objects, inconsistent orientations, and incomplete layouts missing essential object placements. Furthermore, unlike existing methods, CHOrD can be conditioned on complex, irregular room shapes and is robust in synthesizing house-wide layouts that adhere to both geometric and semantic floor plan structures. We also introduce a novel layout dataset with expanded coverage of object categories and room configurations, as well as significantly improved data quality. CHOrD achieves state-of-the-art performance on both the 3D-FRONT dataset and our proposed dataset, excelling in spatial coherence, quality, and diversity, without relying on collision detection, iterative re-generation for self-correction, or predefined rules.", "tldr": "A generative framework that employs an intermediate image-based layout representation to synthesize spatially coherent, house-scale 3D indoor scenes with state-of-the-art quality and diversity, along with a new high-quality dataset.", "keywords": ["Indoor Scene Synthesis", "Digital Twins", "Procedural Generation", "Generative Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4097ceff236920aed9b666d9e0cdb2f9de25700b.pdf", "supplementary_material": "/attachment/cc615529dfc83f65b67eb7daa82f9a9a3bf5c1fa.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduce CHOrD, a two-stage generative framework for indoor house-level scene synthesis and digital twin creation. The main novel contribution of this work stems from the fact that instead of directly predicting a 3D scene graph or object list, CHOrD first generates a 2D image-based furnished scene layout conditioned on a given floor plan image using a diffusion-based image-to-image model. This output is then parsed by a fine-tuned YOLOv8 object detector and segmenter to extract a hierarchical scene graph. 3D meshes are retrieved and rendered according the scene graph to produce photorealistic, simulation-ready environments.\n\nThe core insight of the authors is that an intermediate 2D layout produced by image encoders and decoders with strong spatial priors enhances spatial reasoning, allowing CHOrD to effectively avoid common artifacts such as object collisions, misalignment, and out-of-bound placements—without the need for costly post-processing of collision checks or iterative self-correction. In addition, CHOrD can support multi-level autoregressive layout generation, enabling fine-grained spatial composition, e.g., objects on tables, and multi-modal conditioning, e.g., text-guided and open-plan floor planning.\n\nThe authors also present a new CHOrD dataset containing 9,706 clean, fully furnished scenes covering 26 furniture super-categories, including kitchens, bathrooms, and balconies - areas underrepresented in prior datasets. The quantitative results provided in the paper on both 3D-FRONT and CHOrD datasets show state-of-the-art performance compared to baselines, while qualitative comparisons confirm CHOrD’s spatial coherence and robustness to irregular room shapes."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's core innovation is the introduction of an image-based intermediate layout representation as a key insight, which significantly enhances spatial reasoning and coherence. This approach to scene graph generation effectively reduces spatial artifacts, while being robust to implementation in various other pipelines, enabling streamlined adoption to the industry. \n\nThe paper’s central claim—the use of a generative model to produce an image-based intermediary representation leveraging strong spatial priors—is clearly articulated and well supported. The intuition behind this design choice is effectively explained and further substantiated through both experimental analysis and qualitative comparisons to baseline methods. The authors provide quantitative results across multiple datasets and evaluation metrics (FID, KID, POR, PIoU), demonstrating consistent superiority over baselines. \n\nThe paper presents a clear motivation and problem definition, addressing the challenge of spatial incoherence in 3D indoor scene synthesis. Its core insight—introducing a 2D image-based layout as an intermediate representation— enhances spatial reasoning and coherence. CHOrD is fully data-driven, avoiding handcrafted rules, collision detection, or iterative regeneration. Moreover, CHOrD supports hierarchical and fine-grained layout generation, enabling realistic multi-level spatial relationships. \n\nThe authors also contribute a high-quality dataset that expands room and object coverage beyond 3D-FRONT, and provide a comprehensive evaluation demonstrating consistent state-of-the-art performance across multiple metrics and datasets. \n\nFinally, the model shows robustness to out-of-distribution spatial artifacts, supported by both theoretical justification and empirical validation."}, "weaknesses": {"value": "The paper would benefit from comparisons to a broader range of benchmarks. CHOrD is effectively compared only to InstructScene and DiffuScene, with the comparison to PhyScene being quite limited. Although other baselines did not release training code, it would still strengthen the paper to include reported metrics from those works, even with appropriate caveats, to better contextualize CHOrD’s performance within the broader literature.\n\nThe paper lacks ablation studies. For instance, the necessity and impact of training the diffusion model are not analyzed or discussed, nor is the contribution of the segmentation component examined in detail.\n\nThe discussion of CHOrD’s limitations is somewhat superficial. The section primarily lists unaddressed directions—such as stylistic control or segmentation accuracy—but does not critically analyze inherent limitations of the proposed method itself, nor does it present or reflect on any observed failure cases.\n\nMinor issues:\n\nSome phrases are repeated unnecessarily — for instance, the example “such as placing objects on a coffee table” appears three times throughout the paper.\n\nFigure 4: The highlighted squares do not accurately correspond to the visible regions, and it would be helpful to indicate the camera orientations. Additionally, the ordering of the images on the right side seems arbitrary and would benefit from a clearer logical structure.\n\nSection 3.2 (Fine-grained Layout): The paragraphs might flow better if presented in reverse order.\n\nTables 1 and 2: The method names should be consistent across both tables—currently, one lists reference names while the other uses method names. Moreover, Table 2 is missing a “Method” column title, which should be added for clarity.\n\nSection 3.1: the sentence \"In 2D images, implausible spatial artifacts are instantly\nvisible and flagged as OOD samples, enabling the model to generate coherent, realistic layouts.\" requires further explanation or citation."}, "questions": {"value": "Have you evaluated how much each component (e.g., diffusion model, YOLO segmentation) contributes to performance? How was the amount of training determined?\n\nWere any stability or convergence issues observed during diffusion training?\n\nHave you evaluated the performance of CHOrD across datasets (e.g., trained on 3D-FRONT and tested on CHOrD or vice-verse) to validate robustness of training?\n\nWhat types of room layouts or object arrangements provide a challenge for CHOrD?\n\nHow does CHOrD handle non-orthogonal geometries such as circular rooms? Some examples on this would be good.\nAdditionally, is CHOrD able to position furniture such that it is not aligned with any of the walls?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YDoGk1OnAq", "forum": "xszAt23a17", "replyto": "xszAt23a17", "signatures": ["ICLR.cc/2026/Conference/Submission4402/Reviewer_gk1H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4402/Reviewer_gk1H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553427952, "cdate": 1761553427952, "tmdate": 1762917344778, "mdate": 1762917344778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CHOrD, a generative framework for synthesizing spatially coherent, house-scale, and hierarchically organized 3D indoor scenes. The core innovation is a two-stage process that first generates an intermediate 2D image-based layout representation from a floor plan, which is then converted into a scene graph. This approach leverages the spatial reasoning capabilities of image-based models to mitigate common artifacts like collisions and incomplete layouts observed in prior tabular or LLM-based methods. The authors also introduce a new, high-quality dataset, the CHOrD dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Unlike many prior methods restricted to simplistic rectangular rooms or single-room layouts, CHOrD can handle house-scale layouts with complex, irregular room shapes and floor plan structures.\n\n- The paper introduces a large, clean dataset (9,706 scenes, $\\approx 1.4\\times$ larger than 3D-FRONT). This dataset offers expanded coverage (including fully furnished kitchens, bathrooms, and balconies) and is artifact-free.\n\n- CHOrD achieves superior quantitative results across all key metrics (FID, KID, POR, PIoU) on both the 3D-FRONT and the proposed CHOrD datasets, demonstrating its ability to generate high-quality, diverse, and coherent layouts."}, "weaknesses": {"value": "- The pipeline's second stage relies on fine-tuned YOLOv8 to detect objects and extract the structured scene graph from the generated 2D image. How to ensure the object orientation within the scene graph, since the orientation of object is very important for a reasonable scene structure. Can you quantify the failure rate of YOLOv8 detection and describe the specific training strategy used to ensure the YOLO model accurately maps the colored, top-down 2D layout image into precise 3D bounding boxes and orientations?\n\n- The baselines (DiffuScene, InstructScene, PhyScene) are limited to synthesizing individual rooms, while CHOrD can synthesize house-scale layouts. However, the quantitative evaluation in Table 2 includes single-room results (Bedroom, Living Room) and one \"Entire House\" column. The comparison is unfair; there are some work related on whole house layout generation, such as HouseGAN and HouseGAN++, and its follow-up works. I think some comparisons with this kind of work are more essential.\n\n- How are the inter-room dependencies (e.g., door placements, connectivity) implicitly encoded and maintained throughout the conditional diffusion process for the overall floor plan image? And if the input room boundaries are rotated by random angles, what about the robustness of the proposed model?\n\n- The Empty Room Rate for the CHOrD dataset is 0.2902 (Table 8). Since the dataset is described as \"artifact-free and ready to use\" and containing \"fully furnished kitchens, bathrooms, and balconies\", could you clarify what constitutes an \"empty room\" in the CHOrD dataset? Does this mean some rooms, like small utility rooms or hallways, are intentionally unfurnished in the ground truth, or is this still considered an unavoidable artifact?\n\n- Given that the CHOrD dataset is much cleaner than 3D-FRONT (Table 8 shows baseline PIoU of 0.2547 for 3D-FRONT vs. 0.0018 for CHOrD dataset), were the baseline models (DiffuScene, InstructScene) re-trained on the raw, uncleaned 3D-FRONT dataset or on the cleaned subset (4,847 scenes) used by previous work?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7EV3pPVBbD", "forum": "xszAt23a17", "replyto": "xszAt23a17", "signatures": ["ICLR.cc/2026/Conference/Submission4402/Reviewer_v9NZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4402/Reviewer_v9NZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710763323, "cdate": 1761710763323, "tmdate": 1762917344426, "mdate": 1762917344426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a method for synthesising furniture layouts conditioned on floorplans. In contrast to existing approaches that use generative models in a 'symbolic' space of scene descriptions, the proposed method instead uses an image-to-image latent diffusion model to map from a floorplan (containing just walls, doors, windows, etc.) to a furnished layout (using colored boxes to denote furniture). The resulting image is then 'interpreted' by a standard object detection pipeline, in order to convert it back to a more conventional scene representation indicating where furniture instances are to be placed. Results on 3D-FRONT and a custom dataset show better performance (in terms of realism and interpenetrations) than several recent baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed approach to furniture layout generation is novel. The idea of directly generating in plan-view image space, conditioned on a plan-view image of the empty room is straightforward but elegant, and presumably easier for models to learn than other representations (e.g. predicting bounding-box coordinates).\n\nAs an extra contribution beyond the technical approach, the paper introduces a new dataset of layouts, apparently of higher quality than the widely-used 3D-FRONT, and sufficiently large for training generative models from.\n\nEmpirical comparisons against three fairly recent baseline methods (DiffuScene, InstructScene, PhyScene) show improvements over those baselines on both datasets (3D-FRONT and the proposed dataset). This improvement is uniform across metrics including distributional similarity to ground-truth layouts (FID and KID), as well as object penetration rates (POR and PIoU).\n\nThere is an additional experiment showing that the model can also be used to generate 'fine grained' layouts, i.e. arrangements of adornments such as objects placed on tables etc (as opposed to just large furniture items).\n\nThere is a brief but informative analysis of why the models tend not to generate out-of-distribution (intersecting) furniture elements so often compared with prior methods.\n\nThe paper is clear, well-structured, and pleasant to read."}, "weaknesses": {"value": "The paper title, abstract and introduction strongly emphasise \"house-scale\" generation, i.e. jointly modelling several rooms together. However nothing in the method is specific to this setting, and there is no quantitative evaluation of how well this works – in particular how well inter-room dependencies are captured, i.e. whether a truly accurate joint distribution across all the furniture in a house is learnt, or just per-room marginals.\n\nThe method only seems to support floorplan-conditioned generation. While conditioning on floorplans is vital, for a layout generation model to be useful it must also be possible to provide text conditioning or other guidance to ensure the layout meets other requirements for the target domain. This is now standard for methods in this area, including the baselines DiffuScene and InstructScene.\n\nThe proposed dataset only specifies object classes and bounding-boxes. It does not incorporate any information on style, shape, etc. This greatly limits its usefulness in the task of generating plausible layouts, since haphazard choice of furniture styles is a common failure mode and hallmark of automated layout generation methods.\n\nIt is unclear how the proposed dataset of layouts was collected. It is stated they were prepared by \"experts\" but there is a lack of information on who these experts were, how they were instructed, and how the quality of the resulting layouts was verified. This is problematic given the history of somewhat dubious datasets (SUN-CG and 3D-FRONT) in this area that have tended to contain large proportions of low-quality scenes (as the authors themselves note in the case of 3D-FRONT).\n\nThe section on fine-grained generation is rather minimal; in particular it is not clear how large the dataset was nor how it was collected; it is also not clear whether overfitting might have occurred.\n\nUsing an object detector to 'interpret' the diffusion-generated furniture layout plan-view image and convert back to a symbolic bounding-box representation feels somewhat hacky, and borderline strong enough as the main technical contribution for an ICLR paper. Indeed overall the pipeline is very much an engineered system built out of standard well-understood components, albeit combined in a novel and effective way."}, "questions": {"value": "Most relevant issues are discussed under \"Weaknesses\" above. In particular…\n\nPlease provide evidence or argumentation to properly support the \"house-scale\" claim, beyond a small set of visual examples?\n\nPlease provide more details on both the main CHoRD dataset and the smaller dataset used for fine-grained object layout, in particular the protocol that was used to ensure the layouts are of high quality.\n\nWhat does \"2D bounding boxes defined by … 3D coordinates\" (L295) mean? Are the furniture and room elements represented in 2D or 3D?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wQkGRJZWGp", "forum": "xszAt23a17", "replyto": "xszAt23a17", "signatures": ["ICLR.cc/2026/Conference/Submission4402/Reviewer_emkT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4402/Reviewer_emkT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859423617, "cdate": 1761859423617, "tmdate": 1762917343984, "mdate": 1762917343984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a scene synthesis method and an indoor dataset. The method first generates a floor plan using a diffusion model. Based on this floor plan, a detector detects large objects. Based on these objects, a hierarchical scene graph is extracted, which maintains the relationships between objects and rooms. For large objects that can be regarded as a platform, this again can be used for generating small object layouts using a diffusion model, so the method iteratively finishes the indoor synthesis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is very easy to understand and has a very clear pipeline.\nThe paper contributes a large-scale dataset to the community for further research. The dataset has many unique characters that 3D-FRONT does not have."}, "weaknesses": {"value": "1. The method appears to be a straightforward pipeline that chains existing components (diffusion → detection → diffusion) in a loop. The contribution seems incremental, as each module has prior art and the combination does not clearly yield a novel algorithmic insight.\n\n2. The paper says fine-grained object generation is autoregressive in L241-242, but the description (and Fig. 10) looks like one-shot generation of all small items conditioned on a parent anchor. That’s hierarchical/conditional, not autoregressive. If it is truly AR, please provide a formula how you model the problem. Please spell out the factorization and decoding order and show that each item conditions on previously placed siblings.\n\n3. The manuscript groups DiffuScene under “graph-based” methods in L247-249, but DiffuScene doesn’t actually use explicit edges during generation. Meanwhile, the community has some scene-graph-based generation methods, like CommonScenes, GraphDreamer, EchoScene, Planner3D, and MMGDreamer. Please I wonder why the authors neglect them in the baselines and references?\n\n4. What exactly is the hierarchical scene graph in this paper? The paper references it often but never really defines it. Please provide node/edge types, attributes, hierarchy rules, and how constraints are enforced. Without a precise definition, it’s hard to judge the claimed benefits.\n\n5. The experiments lack of evaluation of dinning rooms, where fine and cluttered objects matter. \n\n6. After carefully inspecting the supplenmentary materials, I found that the rooms in the dataset only provide names (`roomName`) in Chinese. For an international venue, please provide English (or bilingual) labels."}, "questions": {"value": "The bounding boxes are generated from a BEV floor plan where all objects are clearly separated. However, how are the clutter situations handled? Typically, a chair is inserted into a table slot; thus, the bounding boxes have overlaps. This would affect the performance of the object detector.\n\nThe paper only researches BEV renderings, as far as I understand. If I am right, the teaser is a bit confusing. If I am wrong and the paper can actually provide 3D rooms, how is the physical simulation handled? For example, how are objects naturally placed on the floor or on the table without any penetrations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jQLVprJ9c4", "forum": "xszAt23a17", "replyto": "xszAt23a17", "signatures": ["ICLR.cc/2026/Conference/Submission4402/Reviewer_qXFA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4402/Reviewer_qXFA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898170465, "cdate": 1761898170465, "tmdate": 1762917343391, "mdate": 1762917343391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}