{"id": "98IxaUQtMY", "number": 2060, "cdate": 1756982949137, "mdate": 1759898171615, "content": {"title": "SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models", "abstract": "Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To tackle this issue, we present **SERE**, a **S**imilarity-based **E**xpert **R**e-routing method for **E**fficient batch decoding in MoE models. SERE dynamically reduces active experts in an input‑aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single‑line code change. Experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to $2.0\\times$ speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment.", "tldr": "We propose SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models.", "keywords": ["MoE Model", "Inference Acceleration", "Batch Decoding", "Expert Re-routing"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0871dbd0af5bf30ea68cb9163d0f50949e5774f2.pdf", "supplementary_material": "/attachment/423d16d29b91394a87349bd07ae21a240e12466c.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes SERE (Similarity-based Expert Re-Routing), a method to accelerate batch decoding in Mixture-of-Experts (MoE) large language models. The key idea is to identify and exploit functional similarity among experts to reduce redundant expert activation during inference. SERE precomputes an expert similarity matrix using a calibration dataset, then dynamically re-routes tokens from low-contribution (“secondary”) experts to their most similar “primary” experts while preserving dissimilar, critical ones. This reduces the number of active experts per batch without retraining or architectural changes.\n\nThe authors implement an efficient custom CUDA kernel compatible with vLLM, allowing seamless deployment with minimal code modification. Extensive experiments on several state-of-the-art MoE models (Qwen, DeepSeek) and reasoning, math, and coding benchmarks show up to 2× speedup in decoding with negligible accuracy loss."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The analysis in this paper is well-reasoned. It begins with a similarity analysis that reveals important insights into the relationships among MoE experts, which then guide subsequent research and design decisions. I like this style.\n\n\nThe kernel-level optimization and re-routing mechanism are well-aligned with system-level efficiency goals. This is an impressive aspect of the study. \n\nExtensive experiments on multiple MoE architectures (Qwen, DeepSeek) and benchmarks (OpenCompass, MATH, HumanEval) show up to 2× decoding speedup with minimal accuracy loss (<3%)."}, "weaknesses": {"value": "The work is mostly empirical and engineering-oriented; it lacks theoretical analysis of why similarity-based re-routing preserves model behavior or formal guarantees on capacity preservation.\n\nNo analytical insight is given into how similarity thresholds interact with model generalization or stability."}, "questions": {"value": "The similarity matrix is computed using a fixed calibration dataset (e.g., FineWeb-Edu).\n\nHow sensitive is SERE to domain shift or unseen data distributions? Would reusing the same similarity matrix across different domains (e.g., code vs. math) still maintain accuracy?\n\nAs I understand it, SERE first constructs a similarity matrix and then applies re-routing for the secondary experts. This is an interesting and promising idea. However, it would be valuable for the authors to discuss whether the dataset used to build the similarity matrix is representative enough and whether its distribution supports generalization. I do not consider this a weakness, but rather an opportunity for clarification and discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zLdUySvo5H", "forum": "98IxaUQtMY", "replyto": "98IxaUQtMY", "signatures": ["ICLR.cc/2026/Conference/Submission2060/Reviewer_2FF4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2060/Reviewer_2FF4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761399378972, "cdate": 1761399378972, "tmdate": 1762916008223, "mdate": 1762916008223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SERE (Similarity-based Expert Re-routing), an inference-time method to accelerate batched decoding in MoE models. The key motivation is that the many experts activated across a batch in the decoding stage create a memory-bandwidth bottleneck. SERE addresses this by observing that router scores are often concentrated on a few \"primary\" experts. It proposes to re-route tokens assigned to lower-ranked \"secondary\" experts to their most functionally similar primary counterparts. This similarity is pre-computed offline using a data-driven approach on a calibration set. The method is designed to preserve \"critical\" experts that are dissimilar to others, preventing catastrophic capability loss. The authors demonstrate SERE's effectiveness on several modern MoE models, showing up to a 2x speedup in decoding with minimal accuracy degradation on reasoning benchmarks. One major contribution is the release of an efficient, plug-and-play CUDA kernel compatible with the vLLM inference server."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors address a relevant problem that limits the efficiency and scalability of MoEs.\n- The idea of the method is simple yet meaningful. The results indicate speedups up to 2x with acceptable accuracy drop.\n- The authors provide an efficient kernel pluggable into vLLM."}, "weaknesses": {"value": "**Reliance on and Sensitivity to Calibration Data:** The method's foundation is a similarity matrix computed on a calibration dataset. This introduces an offline computation step and a data dependency. While the ablations in Tables 4 and 5 show robustness to the choice of a general-domain dataset, the results also hint at a potential domain mismatch problem. For instance, the average similarity scores for the 'Code' domain are consistently lower than for others, suggesting that re-routing decisions might be less effective or even harmful for specialized domains not well-represented in the calibration data. The authors should (1) discuss this limitation and the potential need for domain-specific calibration, and (2) compare their data-driven similarity approach to a static, data-free alternative (e.g., using the cosine similarity of expert FFN weights) as a baseline. This would clarify the benefit of using activation-based similarity.\n\n**Focus on Decoding:** The paper's focus and evaluation are exclusively on the batched decoding stage. The authors do not discuss or evaluate SERE's impact on the initial prefill stage, which is typically compute-bound. It is not clear if the method would provide any speedup in this stage, and it could potentially degrade the quality of the initial KV-cache generation. The authors need to discuss this in the paper."}, "questions": {"value": "- In some MoE models, including Qwen3, there is a top-k weight normalization to adjust the total expert weights to 1. Do you keep this operation?\n- Would this method work for the prefilling stage?\n- It would be helpful to include standard deviation values to Tables 4 and 5 (AVG column)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zMXJP0faQJ", "forum": "98IxaUQtMY", "replyto": "98IxaUQtMY", "signatures": ["ICLR.cc/2026/Conference/Submission2060/Reviewer_FvHj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2060/Reviewer_FvHj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936886061, "cdate": 1761936886061, "tmdate": 1762916008029, "mdate": 1762916008029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores expert re-routing during the inference stage to reduce the number of active experts and thereby improve efficiency. The authors demonstrate a good trade-off between performance and efficiency with their simple yet effective approach."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- This work tackles a critical and interesting challenge in MoE serving by employing a simple and intuitive mitigation method. \n- The authors' implementation, which features custom CUDA kernels, is good for its easy adaptation to vLLM. \n- Backed by several insightful observational experiments, the method achieves surprisingly good results."}, "weaknesses": {"value": "- The method builds on the observation that experts exhibit high similarity scores. Does this phenomenon occur across all MoE models, or is it specifically a byproduct of using upcycling [1] as the initialization strategy?\n\nMinor problem:\n- It would be helpful to provide more details about the specific problem this work aims to address. At first glance, readers might assume the paper focuses on improving load balancing. Including key terms such as “expert eviction” could better clarify the intended scope and contribution.\n\n[1] SPARSE UPCYCLING: TRAINING MIXTURE-OF-EXPERTS FROM DENSE CHECKPOINTS"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rv3HZEzBrM", "forum": "98IxaUQtMY", "replyto": "98IxaUQtMY", "signatures": ["ICLR.cc/2026/Conference/Submission2060/Reviewer_gTSr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2060/Reviewer_gTSr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762216041791, "cdate": 1762216041791, "tmdate": 1762916007745, "mdate": 1762916007745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}