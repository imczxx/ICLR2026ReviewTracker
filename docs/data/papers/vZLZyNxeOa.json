{"id": "vZLZyNxeOa", "number": 10638, "cdate": 1758178246606, "mdate": 1763009305288, "content": {"title": "Temporal Preference Optimization of Large Multimodal Models", "abstract": "Despite recent advancements in video large multimodal models (video-LMMs), accurate temporal grounding remains a key challenge. In this work, we introduce Temporal Preference Optimization (TPO)—a post-training framework that unlocks superior temporal reasoning in video-LMMs without requiring human annotations. TPO enables preference modeling by manipulating video inputs to generate contrastive responses, ensuring that preferred responses are more temporally grounded than dis-preferred ones. Through preference learning, TPO enhances the model’s capability for more comprehensive video understanding with better temporal reasoning. Extensive experiments on LongVideoBench, MLVU, and Video-MME demonstrate that TPO significantly improves temporal grounding across multiple video-LMMs.   Notably, LLaVA-Video-TPO achieves state-of-the-art performance among 7B models on Video-MME, establishing TPO as a scalable and effective solution for advancing temporal understanding in video analysis.", "tldr": "", "keywords": ["Video Understanding", "Vision-Language Model", "Preference Learning", "Post-Training"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b782ce9c318b44c952f15ad9541830af11a97f24.pdf", "supplementary_material": "/attachment/2b50b9c583c3ba1e6ae199b123b50293246f93ca.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Temporal Preference Optimization (TPO), a post-training framework designed to enhance the temporal grounding of video-LMMs without human annotations. TPO works by generating contrastive responses from manipulated video inputs, using preference learning to teach the model to favor answers that are more temporally accurate. Experiments demonstrate that TPO improves temporal reasoning on multiple benchmarks, establishing it as a scalable method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The intuition of the paper is straightforward: use DPO, but for temporal understanding. With a simple yet effective pipeline, they generate both preferred and dis-preferred responses using full and incomplete clips. The results show significant performance improvements on various temporal benchmarks and outperforms other methods such as pure SFT or classic DPO."}, "weaknesses": {"value": "1. Though the authors introduced two types of data for training, namely generating with 1) irrelevant and 2) incomplete information, there is no explanation as to how this was chosen as the criteria for data curation. Is it because you examined some models' failure cases and found out that the failure modes, or is there some other reason?\n2. Do we really need to optimize with both a) and b) data? Even as the authors have claimed, a) simulates *extreme* scenarios. For me, I understand the intuition of b) since models do not do well on retrieving *all* the correct frame(s) for temporal tasks; on the other hand, a) seems too extreme and may not be necessary since there is literally nothing to reason about, and the models may not gain as useful signals as b). Did the authors do any experiments to see if they only train on a) and only train on b), would the performance gains be different?\n3. It would be great if the authors can evaluate on new temporal benchmarks like TempCompass [1], TemporalBench [2], and Vinoground [3].\n4. It would also be good if the authors can demonstrate that using TPO with the data curated does not hurt general benchmark performances.\n\n[1] Liu et al., 2024, TempCompass: Do Video LLMs Really Understand Videos?\n\n[2] Cai et al., 2024, TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models\n\n[3] Zhang et al., 2024, Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "utSNUKpUWv", "forum": "vZLZyNxeOa", "replyto": "vZLZyNxeOa", "signatures": ["ICLR.cc/2026/Conference/Submission10638/Reviewer_mGWY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10638/Reviewer_mGWY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761335896415, "cdate": 1761335896415, "tmdate": 1762921893546, "mdate": 1762921893546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "ZdoS6aBEip", "forum": "vZLZyNxeOa", "replyto": "vZLZyNxeOa", "signatures": ["ICLR.cc/2026/Conference/Submission10638/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10638/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763009304429, "cdate": 1763009304429, "tmdate": 1763009304429, "mdate": 1763009304429, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Temporal Preference Optimization (TPO), a lightweight post-training framework that improves video-LMMs’ temporal grounding and reasoning without human temporal annotations by generating contrastive preference pairs from the same query answered on original (relevant) versus corrupted or incomplete frames, filtering noisy pairs with a small LLM, and then optimizing with Direct Preference Optimization plus a minor auxiliary SFT loss so the model prefers responses aligned with when evidence occurs, not just what appears; across LongVideoBench, MLVU, and Video-MME, TPO consistently outperforms baselines, with LLaVA-Video-TPO achieving state-of-the-art among 7B models, and the recipe is practical (e.g., ~4 hours on 8×A100 with fixed 32 sampled frames). A scalable annotation-free temporal supervision pipeline via input manipulation and LLM post-filtering; a preference-learning objective instantiating DPO (with small SFT) tailored to temporal grounding; and strong empirical gains across three long-video benchmarks and multiple bases"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Simple, efficient training recipe. The method trains in roughly four hours on 8×A100 (80 GB) with fixed 32 sampled frames shared across data generation and training, indicating practical scalability.\n- Targeted objective that preserves general ability. DPO on temporal preference pairs is positioned to enhance temporal reasoning while retaining pretrained knowledge."}, "weaknesses": {"value": "- Limited benefit on short-video settings. The authors note performance is only comparable to SFT baselines on the Video-MME-short subset, suggesting gains concentrate on longer temporal contexts.\n- Fixed-frame sampling could bottleneck long-horizon reasoning. The design uses a constant 32 frames for both generation and training; the implications for very long or high-motion videos are not extensively studied.\n- Dependence on an external LLM for curation. The data pipeline requires GPT-4o-mini for question curation and post-filtering, introducing cost/availability/bias considerations that are not thoroughly quantified."}, "questions": {"value": "## Temporal grounding definition & evidence.\nCould you specify which aspects of “temporal grounding” are under-defined here and what concrete operational tests (e.g., frame-level localization, counterfactual masking) you would need to see to accept that the method truly improves when-aware reasoning rather than generic accuracy?\n\n## LLM post-filter robustness & bias.\nWhich failure modes of the LLM-based filtering (e.g., label leakage, stylistic bias, prompt sensitivity) most concern you, and what ablations or audits (alternative teachers, temperature sweeps, bias probes) would convincingly address them?\n\n## Evaluation reliability & generalization.\nWhere do you believe the current results are insufficiently reliable (e.g., short-video subsets, high-motion segments), and which additional analyses—stratified metrics with uncertainty (bootstrap CIs), new baselines, or zero-shot datasets—would meaningfully change your assessment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jj8rh6pDIp", "forum": "vZLZyNxeOa", "replyto": "vZLZyNxeOa", "signatures": ["ICLR.cc/2026/Conference/Submission10638/Reviewer_p9KZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10638/Reviewer_p9KZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816631525, "cdate": 1761816631525, "tmdate": 1762921893148, "mdate": 1762921893148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Temporal Preference Optimization (TPO), a post-training framework to enhance temporal reasoning in Video-LMMs without requiring manual annotations. TPO generates contrastive supervision by manipulating video inputs to create preferred and dis-preferred responses, which are refined through a lightweight LLM-based post-filtering step. The approach is evaluated on LongVideoBench, MLVU, and Video-MME, showing modest improvements over baseline models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes a scalable, annotation-free approach to improve temporal reasoning in Video-LMMs.\n- Efficiently generates contrastive data via input manipulation, avoiding costly manual annotations.\n- Addresses the critical challenge of temporal grounding in Video-LMMs."}, "weaknesses": {"value": "- The paper lacks detailed evaluations directly targeting temporal reasoning (*e.g.*, adversarial temporal testing or failure case analysis).\n- The experiments focus on general video understanding rather than explicitly validating improvements in temporal reasoning.\n- Limited discussion with related work (*e.g.*, Hound-DPO, VistaDPO) reduces clarity on TPO’s unique contributions.\n- Over-reliance on synthetic contrastive data without evaluation of its quality or generalizability.\n- The impact of the LLM-based post-filtering step is not analyzed in detail.\n- Missing studies to isolate the contribution of individual components (*e.g.*, post-filtering, preferred vs. dis-preferred responses)."}, "questions": {"value": "Please see Weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "u7OeWVAzZw", "forum": "vZLZyNxeOa", "replyto": "vZLZyNxeOa", "signatures": ["ICLR.cc/2026/Conference/Submission10638/Reviewer_2SVZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10638/Reviewer_2SVZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840221240, "cdate": 1761840221240, "tmdate": 1762921892487, "mdate": 1762921892487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Temporal Preference Optimization (TPO), a post-training framework that enhances temporal reasoning and grounding capabilities in large video-language models (video-LMMs) without requiring manual annotations. TPO generates contrastive preference pairs by comparing model responses to original versus temporally corrupted video clips, then refines models through Direct Preference Optimization (DPO). Experiments on LongVideoBench, MLVU, and Video-MME demonstrate consistent performance gains across multiple backbones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clarity of methods.** The contrastive setup between relevant and manipulated frames, combined with LLM-based post-filtering, is simple yet effective.\n2. **Strong results.** TPO demonstrates consistent improvements across diverse benchmarks and models (LongVA-TPO and LLaVA-Video-TPO), outperforming baselines."}, "weaknesses": {"value": "1. **Limited technical contribution.** The idea is simply generating positive/negative captions pairs and optimizing with DPO, which seems to be limited in their novelty or technical contribution.\n2. **Comparison with recent RL-based approaches.** Direct comparisons with recent reinforcement or segmentation-based optimization methods (e.g., Time-R1, Grounded-VideoLLM) are missing."}, "questions": {"value": "Typo in FIgure 3-(a); MLUV → MLVU"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lShOWLaSLA", "forum": "vZLZyNxeOa", "replyto": "vZLZyNxeOa", "signatures": ["ICLR.cc/2026/Conference/Submission10638/Reviewer_aGyH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10638/Reviewer_aGyH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894780611, "cdate": 1761894780611, "tmdate": 1762921892083, "mdate": 1762921892083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}