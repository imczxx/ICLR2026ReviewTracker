{"id": "y1EQ5EH5zF", "number": 18199, "cdate": 1758285074823, "mdate": 1759897120225, "content": {"title": "Less is More: Denoising Knowledge Graphs For Retrieval Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) systems enable large language models (LLMs) instant access to relevant information for the generative process, demonstrating their superior performance in addressing common LLM challenges such as hallucination, factual inaccuracy, and the knowledge cutoff. Graph-based RAG further extends this paradigm by incorporating knowledge graphs (KGs) to leverage rich, structured connections for more precise and inferential responses. A critical challenge, however, is that most Graph-based RAG systems rely on LLMs for automated KG construction, often yielding noisy KGs with redundant entities and unreliable relationships. This noise degrades retrieval and generation performance while also increasing computational cost. Crucially, current research does not comprehensively address the denoising problem for LLM-generated KGs. In this paper, we introduce DEnoised knowledge Graphs for Retrieval Augmented Generation (DEG-RAG), a framework that addresses these challenges through: (1) entity resolution, which eliminates redundant entities, and (2) triple reflection, which removes erroneous relations. Together, these techniques yield more compact, higher-quality KGs that significantly outperform their unprocessed counterparts. Beyond the methods, we conduct a systematic evaluation of entity resolution for LLM-generated KGs, examining different blocking strategies, embedding choices, similarity metrics, and entity merging techniques. To the best of our knowledge, this is the first comprehensive exploration of entity resolution in LLM-generated KGs. Our experiments demonstrate that this straightforward approach not only drastically reduces graph size but also consistently improves question answering performance across diverse popular Graph-based RAG variants.", "tldr": "We propose to denoise LLM-generated knowledge graphs for RAG, which reduces about half the scale of the knowledge graph while improving the performance of RAG.", "keywords": ["Retrieval Augmented Generation", "Knowledge Graph", "Large Language Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4899aa7274e853b5cff559f263376255c983e43b.pdf", "supplementary_material": "/attachment/f8861182e1c83fd2649cee930c4ab2e3e9ce223a.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies the effect of denoising LLM-generated knowledge graphs used in graph-based RAG and provides comprehensive analysis on the importance of entity resolution for graph RAG methods. \nAccording to the paper, the proposed pipeline, DEG-RAG, consists of two core components:\n•\tEntity resolution: blocking → matching (using embeddings / neighbor info / types) → merging or synonym linking to collapse duplicates and near-duplicates; and\n•\tTriple reflection: an LLM “judge” scores triples and low-score triples are removed.\nThe authors evaluate multiple entity resolution design choices (blocking types, embedding families, similarity modes, merge strategies) across four Graph-RAG systems (LightRAG, HippoRAG, LGraphRAG, GGraphRAG) and four datasets from the UltraDomain benchmark (Agriculture, CS, Legal, Mix). Primary empirical claims: removing ≈40% of nodes + relations yields consistent QA improvements (winning-rate >50% in many settings), type-aware blocking tends to work best, classical KG embeddings can rival LLM embeddings, and direct merging often beats synonym-linking only. They also show robustness to aggressive reduction ratios (even ~70% in some datasets)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper researches an important bottleneck in graph-based RAG pipelines—noise in LLM-generated KGs and empirically highlight the importance of entity resolution for graph RAG methods, i.e., without entity resolution, graph-based RAG will degrade into vanilla RAG.\n- It ablates multiple entity resolution design component choices across several RAG systems and domains, and provides useful insights on the impacts of different entity resolution components on graph-based RAG systems, e.g., type-based blocking is more effective than semantic-based or structure-based blocking and classical KG embeddings can outperform LLM embeddings in efficiency and accuracy.\n- The experimental results show that denoising improves QA despite heavy graph reduction and verifies the “less is more’’ effect."}, "weaknesses": {"value": "- This paper does not introduce novel technical contribution but instead to improve the mined subgraph with existing entity resolution techniques. \n- Lack of quantitative measurement of entity resolution quality. The paper emphasizes that the entity resolution step removes duplicates and merges entities of same identities. However: There is no ground-truth or gold dataset used to measure precision, recall, or F1 of merges vs. non-merges. As a result, it’s unclear whether the denoised graph is semantically cleaner or merely smaller. A single case study is provided but a systematic evaluation is missing.\n- The evaluation heavily depends on LLM-based judgment rather than objective correctness, the  “win rate” between responses generated with/without denoising—is inherently subjective and model-dependent. This introduces potential concerns: the same biases or stylistic preferences may favor DEG-RAG outputs without reflecting actual factual correctness or reasoning quality. While this approach is becoming common in RAG evaluations, the absence of human annotation or reference-based metrics (e.g., factual F1, answer string match) makes it unclear whether improvements stem from better reasoning."}, "questions": {"value": "Q1: I did not find a clear explanation about the entity reduction ratio, why this can be controlled, and how it is controlled. I guess this is done by randomly unmerging some of the merged nodes, but then the question is whether the result is sensitive to the selection of the unmerged nodes? \nQ2: Is there any reason for setting the triple reflection threshold to 0.2? According to the scoring guidelines in Appendix C.4, “0.0–0.3: Invalid or highly questionable (factually wrong, illogical, meaningless)”, 0.2 does not look like a persuasive threshold."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VSCafpHyUP", "forum": "y1EQ5EH5zF", "replyto": "y1EQ5EH5zF", "signatures": ["ICLR.cc/2026/Conference/Submission18199/Reviewer_nAoT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18199/Reviewer_nAoT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719207731, "cdate": 1761719207731, "tmdate": 1762927947013, "mdate": 1762927947013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets noise in LLM-constructed knowledge graphs used for Graph-based RAG. It proposes DEG-RAG, a two-step denoising pipeline: entity resolution to merge duplicate entities and triple reflection to filter incorrect relations. The authors run a systematic study of blocking, embeddings, similarity metrics, and merging strategies tailored to LLM-generated KGs. Experiments show the approach can remove about 40% of entities and relations while improving QA performance across several Graph-RAG variants, arguing that graph quality matters more than graph size for effective retrieval and generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear problem focus: it tackles noise in LLM-built knowledge graphs, a practical bottleneck for Graph-RAG.\n\n- Simple, modular pipeline: entity resolution plus triple filtering is easy to plug into existing Graph-RAG systems.\n\n- Consistent gains: denoising improves QA quality across multiple Graph-RAG variants."}, "weaknesses": {"value": "- Limited novelty.\nThe pipeline (entity resolution and triple filtering) is standard in knowledge graph cleaning [1,2]. The approach heavily relies on an LLM acting as the judge, rather than introducing a fundamentally new algorithm.\n\n- Missing denoising baselines.\nThe paper does not compare against strong baselines on knowledge graph refinement, so the advantage over dedicated methods is unclear.\n\n- Evaluation focuses on downstream QA only.\nIt rarely reports denoising metrics such as precision, recall, over-merge rate, or error rate. \n\n- Transferability is uncertain.\nExperiments use LLM-extracted graphs. It is unclear how well the method works on curated graphs like DBpedia or Wikidata, or on noisy industrial data.\n\n- Scalability and maintenance are underexplored.\nBlocking, embedding, similarity checks, merging, and re-indexing can be expensive on very large graphs. The cost is not well detailed.\n\n- Risk of information loss.\nDenoising may remove rare but important facts or collapse distinct entities. \n\n[1] Knowledge graph refinement: A survey of approaches and evaluation methods[J]. Semantic web, 2016, 8(3): 489-508.\n\n[2] Subagdja B, Shanthoshigaa D, Wang Z, et al. Machine learning for refining knowledge graphs: A survey[J]. ACM Computing Surveys, 2024, 56(6): 1-38."}, "questions": {"value": "Transferability is uncertain. Experiments use LLM-extracted graphs. It is unclear how well the method works on curated graphs like DBpedia or Wikidata, or on noisy industrial data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MqQJ17YDpe", "forum": "y1EQ5EH5zF", "replyto": "y1EQ5EH5zF", "signatures": ["ICLR.cc/2026/Conference/Submission18199/Reviewer_14qP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18199/Reviewer_14qP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848873351, "cdate": 1761848873351, "tmdate": 1762927946597, "mdate": 1762927946597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of noise and redundancy in LLM-generated knowledge graphs (KGs) used by graph-based retrieval-augmented generation (RAG) systems. The authors observe that current graph-RAG pipelines often suffer from duplicated entities (aliases, abbreviations, typos) and unreliable triples that degrade retrieval quality and increase computational overhead.\nTo address this, they propose DEG-RAG, a lightweight post-processing pipeline that denoises KGs through (1) entity resolution (ER)—which merges duplicate entities using blocking, embedding-based similarity, and merging/linking—and (2) triple reflection, which filters low-confidence relations based on an LLM-judged reliability score.\nThe cleaned graph is then reused by various existing graph-RAG retrievers (LightRAG, HippoRAG, LGraphRAG, GGraphRAG). Empirical results on the UltraDomain benchmark show consistent improvements in LLM-judged answer quality (≈5–15% win-rate gains) while reducing the KG size by ~40%. The authors also perform ablations on each design choice (blocking strategy, embedding type, merging method, threshold) and show robustness to high pruning ratios and generality across different LLM backbones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear and impactful motivation — The issue of noisy LLM-generated KGs is real and under-explored; the authors address it with a practical, model-agnostic approach.\n\nSimple but effective methodology — The proposed ER + triple-filtering pipeline is lightweight, interpretable, and compatible with existing graph-RAG frameworks.\n\nComprehensive experiments — Multiple graph-RAG baselines, four domains, and detailed ablations (blocking, embeddings, thresholds) make the evaluation convincing.\n\nStrong empirical gains with efficiency — DEG-RAG improves answer quality while reducing graph size, supporting the “less is more” thesis.\n\nInsightful analysis — The observation that graph-RAG degenerates to chunk-RAG without entity resolution is novel and theoretically motivated."}, "weaknesses": {"value": "Limited theoretical grounding — While intuitively reasonable, the paper lacks a formal analysis of why denoising improves reasoning (e.g., retrieval connectivity, semantic precision).\n\nDependence on LLM scoring — Triple reflection uses an LLM-as-judge, which may introduce bias or high variance; the reliability of this signal is not deeply validated.\n\nEvaluation via LLM-as-judge only — No human or retrieval-based metrics are reported; LLM-judged win-rates might not fully capture factual correctness.\n\nDomain generalization — Experiments are limited to four UltraDomain subsets; it is unclear whether the approach scales to open-domain or cross-lingual corpora."}, "questions": {"value": "Complexity and scalability: What is the computational complexity of the ER and triple reflection steps? How does the runtime scale with the number of entities/triples?\n\nSensitivity to parameters: How sensitive are the results to the ER threshold (δ_ER) and triple score threshold (δ_TR)? Are there guidelines for tuning them automatically?\n\nTriple reflection reliability: How consistent are the LLM-as-judge scores across different models (e.g., Qwen vs GPT-4o)? Have you measured inter-model agreement?\n\nGraph structure preservation: Does merging entities or removing triples ever break necessary connectivity (e.g., for multi-hop reasoning)? How is information loss mitigated?\n\nDownstream generalization: Have you tried DEG-RAG on non-UltraDomain datasets or on real-world document corpora (e.g., academic papers, Wikipedia)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JGUOPfJFfW", "forum": "y1EQ5EH5zF", "replyto": "y1EQ5EH5zF", "signatures": ["ICLR.cc/2026/Conference/Submission18199/Reviewer_Nyok"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18199/Reviewer_Nyok"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889776158, "cdate": 1761889776158, "tmdate": 1762927946154, "mdate": 1762927946154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to denoise LLM-generated knowledge graphs for Graph RAG, using entity resolution and a triple-reflection step to remove redundant entities and filter unreliable relations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Tackles a timely, important problem for both industry and academia.\n2. Combines entity resolution with triple reflection to prune redundant nodes and questionable relations.\n3. Empirically robust performance that remains stable even after removing up to ~70% of entities."}, "weaknesses": {"value": "1.\tKey implementation details are omitted, e.g., how canonical entities are chosen and how retrieval operates after merging.\n2.\tTriple reflection relies on LLM-as-judge, which incurs high computational cost and may be impractical at scale.\n3.\tEvaluation depends on LLM-as-judge metrics, which are sensitive to model choice and prompts and may not reliably reflect true method quality."}, "questions": {"value": "1.\tFor entity resolution, how are canonical entities selected and evaluated for representativeness? After merging, both original and merged entities appear in the KG. How does retrieval handle this, and how are hyperparameters (e.g., top-k) chosen?\n2.\tFor triple reflection, since each triple is judged by an LLM, runtime and cost could be prohibitive for large KGs. Can the authors propose or test lightweight alternatives, e.g., learned classifiers, heuristics, or ensemble-based filters, that approximate LLM judgments?\n3.\tFor performance evaluation, given the sensitivity and inconsistency of LLM-as-judge, please include alternative or complementary metrics for KG quality to better validate the denoising effectiveness across methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EjDHN0mFQR", "forum": "y1EQ5EH5zF", "replyto": "y1EQ5EH5zF", "signatures": ["ICLR.cc/2026/Conference/Submission18199/Reviewer_yvcH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18199/Reviewer_yvcH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762096285788, "cdate": 1762096285788, "tmdate": 1762927945391, "mdate": 1762927945391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}