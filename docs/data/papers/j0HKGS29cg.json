{"id": "j0HKGS29cg", "number": 9512, "cdate": 1758125511466, "mdate": 1759897715261, "content": {"title": "Learning to Align and Act: Cross-Modal Gating and Multimodal Reward Shaping for Web Agents", "abstract": "Web-based reinforcement learning agents face two fundamental challenges that limit their effectiveness in real-world applications: cross-modal misalignment between visual screenshots and HTML DOM representations, and severe reward sparsity in multi-step interaction tasks. Existing approaches typically rely on static fusion strategies that fail to adapt to the dynamic importance of different modalities across task phases, while sparse binary rewards provide insufficient guidance for efficient learning in long-horizon scenarios. To address these limitations, we propose a novel framework that integrates cross-modal attention gating with multimodal feedback-driven reward shaping. Our gating mechanism dynamically regulates the contribution of visual and textual modalities based on task context and trajectory history, enabling adaptive coordination throughout the decision-making process. Simultaneously, our reward shaping approach decomposes sparse terminal rewards into dense step-level signals derived from both visual UI state changes and textual content validation, providing informative feedback at each interaction step. Extensive experiments on MiniWoB++, WebShop, and Mind2Web demonstrate that our method achieves significant improvements over strong baselines, with 6-8% gains in task success rate and 44% reduction in sample complexity. Ablation studies reveal that the combination of gating and shaping yields synergistic benefits, with our gating controller learning to make confident, context-aware modality selections (achieving 25% lower attention entropy) while the multimodal reward shaping increases feedback density from 3-6% to 23-30% of interaction steps. These results establish a new paradigm for multimodal reinforcement learning in web environments, demonstrating that adaptive modality coordination and granular feedback alignment are essential for robust and efficient web agent training.", "tldr": "Cross-modal gating with reward shaping achieves 8% higher success and 44% lower sample complexity for real-world web agents.", "keywords": ["Reinforcement Learning", "Web Agents", "Multimodal Learning", "Reward Shaping"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f64078574d4bca8c2d451d18a6a3637c8cb81ecf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a reinforcement learning framework for web agents that integrates two main innovations: cross-modal attention gating and multimodal reward shaping. The gating mechanism adaptively balances visual and textual inputs (DOM structures and rendered screenshots) at each timestep, aiming to mitigate the misalignment problem inherent in web environments. Complementing this, the reward shaping approach decomposes sparse terminal rewards into intermediate signals derived from visual and textual feedback, guiding the agent more effectively during training.\n\nEmpirical results on MiniWoB++, WebShop, and Mind2Web show moderate but consistent improvements in success rates and sample efficiency. The paper claims these gains arise from the synergy between the gating and shaping components, supported by ablation studies and visualization analyses of attention behavior during task execution."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s clarity and implementation quality are strong. Its methodological formulation is concise and intuitive, with a well-illustrated pipeline and clearly defined objectives. The introduction of gating within the web agent context is an original extension of known attention control mechanisms, addressing a meaningful practical issue—DOM–visual misalignment—common in browser-based automation. The integration of modality-specific reward signals with the fusion process also demonstrates a thoughtful, application-oriented design.\n\nExperimentally, the results indicate measurable benefits in convergence speed and task completion, supported by diverse metrics beyond mere task success rate. The narrative is coherent, the visualizations effectively communicate the model’s behavior, and the overall presentation is polished, which enhances readability and interpretability."}, "weaknesses": {"value": "Despite these merits, the conceptual novelty remains somewhat limited. The gating mechanism itself is not new, and its justification over more expressive fusion techniques—such as cross-attention, mixture-of-experts routing, or adapter-based fusion—is not adequately discussed or empirically validated. The reward shaping method also appears heavily dependent on this gating architecture, which reduces generality and raises questions about whether the method would extend to other multimodal RL contexts.\n\nFurthermore, the baselines chosen for comparison do not convincingly represent the current state of the art in web agents. Modern architectures like WebArena or WebAgent-R1, as well as retrieval-augmented or self-correcting approaches, are cited but not included in the experiments. The ablation studies, while informative, focus on component removal rather than alternative design choices, leaving open whether the proposed components are truly optimal. Details on detector and validator implementation are also sparse, limiting reproducibility."}, "questions": {"value": "The authors should clarify the theoretical and empirical justification for choosing gating over more advanced multimodal fusion mechanisms. Comparisons against cross-attention or MoE-based fusion would significantly strengthen the argument. Likewise, it would be useful to analyze whether the proposed reward shaping scheme is potential-based and thus policy-invariant; otherwise, it might bias the learning process or encourage subgoal overfitting.\n\nAdding results on WebArena and including stronger baselines would help contextualize the reported performance gains. Detailed implementation information about the visual change detector and text validator modules should be provided, as these elements critically affect the shaping signal. Finally, exploring whether the gating mechanism generalizes to tri-modal settings or token-level gating would demonstrate broader applicability and robustness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rzSKBNUWnj", "forum": "j0HKGS29cg", "replyto": "j0HKGS29cg", "signatures": ["ICLR.cc/2026/Conference/Submission9512/Reviewer_2kwq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9512/Reviewer_2kwq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927938199, "cdate": 1761927938199, "tmdate": 1762921083025, "mdate": 1762921083025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to solve two core problems in web-based RL agents: cross-modal misalignment and reward sparsity in long-horizon tasks. This paper proposes a framework that combines a cross-modal attention gating mechanism to dynamically weight visual and textual features based on task context , and a multimodal feedback-driven reward shaping scheme that converts sparse terminal rewards into dense, step-level signals by detecting UI changes and validating text . Experiments are conducted on MiniWoB++, WebShop, and Mind2Web, claiming improvements in task success rate and sample efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The two problems addressed by this paper, cross-modal fusion  and reward sparsity, are critical problems in building robust web agents today.\n2. The proposed method evaluated across three benchmarks of varying scale and complexity (MiniWoB++, WebShop, Mind2Web) and provide ablation studies to analyze the two main components."}, "weaknesses": {"value": "1. The visual reward ($R_{vision}$) is defined as detecting \"UI state changes (e.g., pop-ups, button color)\". This is a problematic assumption. An agent can easily trigger UI state changes by \"randomly clicking\" (e.g., clicking ads, opening incorrect dropdown menus). Rewarding state change would encourage this type of non-goal-oriented exploration rather than meaningful task progression. The paper does not clearly state how it distinguishes \"beneficial\" state changes from \"meaningless\" ones.\n2. The textual reward ($R_{text}$) is similarly ill-defined. It is described as \"validates field content or label alignment\" or \"form field correctness, label matching\". The paper provides no detail on how this is implemented. Does this validation require an external oracle? Or does it assume the benchmark environment provides this (which is common in many benchmarks, but this is a strong assumption not discussed)? This ambiguity makes the contribution non-reproducible and difficult to evaluate for its true effectiveness.\n3. The framework's core architecture uses standard visual (e.g., ViT) and textual (e.g., BERT) encoders, which is an extremely common setup in multimodal research. The paper's claimed novelty lies in the \"cross-modal attention gating\". However, this mechanism is a simple gated weighted sum generated by a small controller. This adaptive weighting is a straightforward application of gating units and, as acknowledged in the related work, does not conceptually advance beyond existing adaptive fusion techniques.\n4. Poor writing and presentation quality. The quality of the figures is low, and there is significant redundancy.\n\n## Minor Issues\n1. Appendix Section A.1.1 (\"Evaluation Metrics\") and Section A.1.4 (\"Evaluation Metrics\")  are nearly identical, word-for-word repetitions.\n2. The paper states it uses ResNet-50/ViT-B/16 for vision and BERT-base/HTML-aware Transformer for text. However, the main results tables (e.g., Table 1) do not specify which combination of backbones was used to achieve the reported SOTA results, making reproduction and fair comparison difficult."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ceei0qkJro", "forum": "j0HKGS29cg", "replyto": "j0HKGS29cg", "signatures": ["ICLR.cc/2026/Conference/Submission9512/Reviewer_NCM7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9512/Reviewer_NCM7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988037103, "cdate": 1761988037103, "tmdate": 1762921082809, "mdate": 1762921082809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses two critical challenges in training reinforcement learning agents for web navigation: cross-modal misalignment and reward sparsity. The authors propose a novel framework that integrates a cross-modal attention gating mechanism with a multimodal reward shaping scheme. The gating controller dynamically adjusts the influence of visual (screenshot) and textual (DOM) representations based on the task context, allowing the agent to prioritize the most relevant modality at each step. Complementing this, the reward shaper converts sparse terminal rewards into dense, step-level feedback by detecting intermediate signals of progress, such as UI state changes from the visual stream and content validation from the textual stream. These two components are designed to work synergistically, where the dense rewards provide a strong learning signal for the gating controller, and the adaptive gating ensures the agent's focus aligns with the source of the reward. The authors validate their approach through extensive experiments on three diverse benchmarks—MiniWoB++, WebShop, and Mind2Web, while demonstrating significant improvements over existing methods. The proposed agent achieves a 6-8% higher task success rate and reduces sample complexity by 44%."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "[S1] This paper studies a highly popular topic (web agent) and tries to address important research questions (how to leverage both text and image modalities).\n\n[S2] The proposed method (gating + reward shaping) works better among the baselines (RAG/Toolformer/PPO/SAC) in various aspects."}, "weaknesses": {"value": "[W1] There are not sufficient discussion/comparison to the previous literature in this domain; Humphreys et al., (https://arxiv.org/abs/2202.08137) proposed multimodal computer control agents and achieved human level performance with RL in MiniWoB++. Furuta et al., (https://arxiv.org/abs/2305.11854) and Shaw et al., (https://arxiv.org/abs/2306.00245) proposed to leverage multimodal information with the recent multimodal LLM architectures. Zheng et al., (https://arxiv.org/abs/2401.01614) proposed the web agent based on GPT-4V. WebRL (https://arxiv.org/abs/2411.02337) and  UGround (https://arxiv.org/abs/2410.05243) proposed RL training or multimodal architecture to resolve web navigation. Please consider to compare the performance and discuss the difference and relevance with those works.\n\n[W2] \"Preliminaries\" section is really needed. It is quite unclear which variable/subscripts stands for what. The current paper significantly relies on the reader's knowledge.\n\n[W3] The proposed architecture is quite similar to WebShop (https://arxiv.org/abs/2207.01206); ResNet + BERT. Moreover, there are no discussion or explanation about why the authors did not use LLMs or VLMs as a backbone models, which are already used in recent web agent works (WebRL: https://arxiv.org/abs/2411.02337,  UGround: https://arxiv.org/abs/2410.05243).\n\n[W4] The proposed method is not evaluated in recent and more advanced multimodal web benchmark such as Visual WebArena (https://arxiv.org/abs/2401.13649). I think the results on Visual WebArena are more valuable than including the results in MiniWoB++ for the recent web agent research community.\n\n[W5] There are many unclear points in evaluations and settings. Please see **Questions** section below.\n\n[W6] There are many points to be improved in the formatting and description in the figures. (1) \"Conference Submissions\" in the title. (2) unclear upperscript $o_{tv}^{1}$ in Figure 1, (3) unnecessary parenthesis in Figure 1, (4) \"current stat\" in Figure 1, (5) unclear right arrow from \"Gross-Modal Gating Controller\" to $a_t$, (6) unclear node of $a_t$ with \"reward signal\" in Figure 1, (7) \"sigoiod\" in Figure 2, (8) unnecessary parenthesis in Figure 2, (9) unclear right arrow with \"RL Optimization Loop\" in Figure 3, (10) lack of \"R_t\" in \"Shaped Reward Computation\" node in Figure 3, (11) we cannot read some text in Figure 6 by the overlap with graph legends, etc."}, "questions": {"value": "[Q1] In WebShop (https://arxiv.org/abs/2207.01206), the reported task success rate was at most 28.7%. However, this paper reports 60-75% (in Table 1). I think the base architecture is mostly the same as what WebShop paper proposed. Could you give us detailed explanations?\n\n[Q2] Also, in Mind2Web (https://arxiv.org/abs/2306.06070), the reported task success rate was at most 7.1%. There are huge  gaps compared to this paper. Could you also give us detailed explanations?\n\n[Q3] Could you provide the details of \"RAG-style Agent\" and \"Toolformer-style Agent\"?\n\n[Q4] How many tasks & what kinds of tasks are you using in MiniWoB++? In prior works (https://arxiv.org/abs/2306.00245, https://arxiv.org/abs/2305.11854, https://arxiv.org/abs/2202.08137, https://arxiv.org/abs/2303.17491), there are variety of subsets, and the choices of tasks can lead to different average success rates.\n\n[Q5] Could you explain more about attention-entropy? Where (+ which layer, what kinds of tokens, how) do you measure the entropy of attention? In the current paper, it is hard to parse that information."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ESCloUouRi", "forum": "j0HKGS29cg", "replyto": "j0HKGS29cg", "signatures": ["ICLR.cc/2026/Conference/Submission9512/Reviewer_SoGH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9512/Reviewer_SoGH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136404828, "cdate": 1762136404828, "tmdate": 1762921082413, "mdate": 1762921082413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a web-RL framework that fixes visual–text misalignment and sparse rewards using dynamic cross-modal gating and multimodal reward shaping. The gating module adaptively balances vision and DOM features, while the reward shaper turns sparse terminal rewards into dense visual/textual feedback. Across MiniWoB++, WebShop, and Mind2Web, the method improves success rates by 6–8% and cuts sample complexity by 44%. Ablations show both components are complementary and crucial for robust web agents"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Giving the agent the ability to shift between vision and web (DOM) information is an interesting innovation\n\n- Densifying the reward facilitates the learning problem"}, "weaknesses": {"value": "- The reward shaping procedure depends on manually defined visual/state-change detectors and text validators. This limits generalization to pages with unusual layouts, noisy DOMs, or non-standard UI behaviors.\n- Limited/no evaluation in larger \"in-the-wild\" large scale websites with complicated UIs and more sophisticated layouts / tasks."}, "questions": {"value": "Have the authors evaluated their method on more complicated website datasets?\n\nThe paper formatting is a bit strange, with much whitespace left above and below tables. Could the authors improve formatting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8FmummYpBh", "forum": "j0HKGS29cg", "replyto": "j0HKGS29cg", "signatures": ["ICLR.cc/2026/Conference/Submission9512/Reviewer_7wGf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9512/Reviewer_7wGf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762486120653, "cdate": 1762486120653, "tmdate": 1762921082145, "mdate": 1762921082145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}