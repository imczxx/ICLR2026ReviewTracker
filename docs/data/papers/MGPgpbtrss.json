{"id": "MGPgpbtrss", "number": 19459, "cdate": 1758296394203, "mdate": 1759897037716, "content": {"title": "Unsupervised discovery of symmetries and symmetry-based domains from raw data", "abstract": "Signals are often generated by processes that respect a symmetry group of the domain they live on. Observed signals are obtained from underlying samples by some transformation corresponding to a physical process, resulting in a group action that is often more complicated than the action on the underlying domain. Learning the symmetry group and the underlying symmetry-based domain are two intertwined problems of fundamental importance.\nIn this paper, we develop a method that simultaneously discovers symmetries and symmetry-based domains in a fully unsupervised setting, without assuming that the group action is transitive. Our approach is based on a lifting operation inspired by Group Convolutional Networks, mapping the space of observed features to a domain parametrized by group elements. By utilizing a powerful locality prior, we are able to learn symmetry actions such as translations, permutations and frequency shifts, on datasets with much higher dimensionalities than has been possible before. Since the domain is hidden, we assume the symmetry group acts directly on the space of samples, which in the familiar case of natural images means the underlying pixel translation symmetries to be learned are  for a set of images. As well as discovering the relevant symmetries directly from raw data, our method also offers new approach towards solving linear inverse problems.", "tldr": "We develop a method for unsupervised discovery of symmetries and underlying symmetry-based domains from raw data", "keywords": ["Symmetry Learning", "Representation Learning", "Group Equivariant Neural Networks", "Unsupervised Learning", "Machine Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d89a743f07ff84479810209d237b65e5a5d10735.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an unsupervised framework to jointly discover symmetries and the underlying symmetry-based data domain. It introduces a lifting operation that maps observed samples into a group-parameterized domain where symmetry becomes equivalent to shift-invariance. Then, the method combines invariance, resolution, and infomax losses to ensure nontrivial, independent, and informative representations and parameterizes group generators through learnable Lie algebra bases."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles the ambitious and important problem of unsupervised discovery of symmetry structures directly from raw data. It uses a lifting-based approach that connects group invariance to shift stationarity. The formulation is mathematically grounded and the methodology is described in clear details."}, "weaknesses": {"value": "## Presentation\n\nThe experiment section of this paper is **incomplete**. In Sec 5, there is only one subsection describing the experiment setups. Figure 3 and 4 and Table 1 may have presented some results, are not referenced anywhere in the text. Some other results are provided in the Appendix. However, a paper should be self-contained without the Appendix. It is okay that some complementary results and detailed dataset setups, for example, are deferred to the Appendix. However, in its current state, the paper does not effectively show and explain any experimental results in the main text, which is not acceptable.\n\nIn other sections beyond experiments, the paper also relies heavily on cross-references to the Appendix. Sometimes it explicitly links equations from the appendix, e.g., L231, (20). Sometimes notations from the appendix are directly used in the main text without reference or definition, which is even worse, e.g., L227, $\\phi$. The mathematical framework introduced in the methodology may be correct, but the reliance on back-and-forth cross-references and intertwined notational dependency make it less accessible to readers.\n\n## Method & Contribution\n\nFirst and foremost, it appears to me that the contributions mentioned at the end of Sec 1 do not match the methodology in Sec 4. Either the summarized contribution points are inaccurate, or the methodology section has not provided a clear enough explanation. For example, one of the contributions is summarized as the \"ability to discover the symmetries when they act intransitively\", but intransitivity of group actions is not mentioned anywhere in the methodology. Similarly, another contribution point states that the method discovers irreducible representations, which are again not discussed anywhere else in the main paper. These go beyond just presentational issues, since they confuse the readers about the core contribution of the paper and make it difficult to evaluate the correctness and the significance of the proposed methodology.\n\nThen, from my understanding, the paper aims to discover a hidden space where the symmetry of the data distribution acts simply with a regular representation, i.e., shift invariance (wrt an Abelian group). The paper presents a convoluted way to achieve this, while I believe there are simpler solutions. For example, [1,2,3], among other methods for learning equivariant representations, can all be applied once we specify the group action on the hidden space (i.e., the symmetry-based domain in the context of this paper). Also, compared to the setting in this paper, these existing methods are not limited to (1) Abelian groups, and (2) data domains exactly being the group $\\Omega=G$, but instead more general spaces equipped with a $G$-action. Therefore, a critical question is why the proposed method in this paper is superior to existing works in terms of applicability, efficiency, etc.\n\n## Experiments\n\nAs mentioned above, the current experiment section is incomplete. Thus, it is hard to provide any meaningful evaluation for this part.\n\n## References\n\n[1] Latent Space Symmetry Discovery. ICML 2023.\n\n[2] Structuring representations using group invariants. NeurIPS 2022.\n\n[3] Neural Fourier Transform: A General Approach to Equivariant Representation Learning. arxiv 2023."}, "questions": {"value": "* L317: Missing cross-reference.\n* L342: How is the JS divergence in the invariance loss estimated? It is better to explicitly refer to the relevant section instead of saying \"... is described below\", which is confusing because it is not described *immediately* below.\n* L349: There are $N_1 \\times ... \\times N_P$ different $\\mathbf n$'s, scaling quickly with the resolution and the group dimension. Is the summation over all possible $\\mathbf n$'s efficient?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YNpWGJAoS4", "forum": "MGPgpbtrss", "replyto": "MGPgpbtrss", "signatures": ["ICLR.cc/2026/Conference/Submission19459/Reviewer_SCTN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19459/Reviewer_SCTN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761006535131, "cdate": 1761006535131, "tmdate": 1762931375440, "mdate": 1762931375440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an unsupervised framework to discover symmetry groups and the associated hidden domains directly from raw data. The core idea is a lifting operation that maps inputs to a representation where the unknown symmetry acts as shifts. A lifting network learns commuting generators (restricted to abelian groups) and a resolving filter, while an auxiliary network estimates information theoretic quantities that drive the loss. Experiments on synthetic data show that the method can recover high‑dimensional group actions without paired or sequential samples."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. A novel setting for extracting symmetries without relying on sequence or paired data; independence from specific input structures is compelling.\n\nS2. Compared to prior work, the method can learn higher-dimensional representations of group actions.\n\nS3. The technical presentation is simple and easy to follow."}, "weaknesses": {"value": "W1. Questionable realism of assumptions, especially the linear and invertible observation map M; many real tasks involve nonlinear or lossy transforms that may not permit full recovery.\n\nW2. No experiments on real-world datasets; evaluation is limited to synthetic tasks.\n\nW3. Practical utility is under-validated in the experiments:\n- No comparisons against reasonable baselines.\n- Loss ablations (in Appendix) are mostly qualitative; please provide quantitative ablations to show how much each term helps."}, "questions": {"value": "Q1. Beyond permutation, what concrete real tasks fit your setting? Permuted MNIST does not feel realistic; examples from imaging, time series, or scientific data would help.\n\nQ2. Why must the group be abelian? Where would the approach fail without commutativity, and what parts of the pipeline depend on it?\n\nQ3. The bilevel/min–max training between the lifting and auxiliary networks could introduce instability (similar to GANs). Did you observe instability in practice? If so, what stabilization measures worked?\n\nQ4. How were the loss weights alpha and beta selected?\n\nQ5. For the resolution loss, what motivated its inclusion, and under what properties of the true signal does it help most? Since it promotes coordinate independence, when might it bias the reconstruction?\n\nMinor comments\n- The figure on page 6 appears without a figure number and caption; please add both. The text inside the panels is also hard to read at the current size."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WzLQBLhFn6", "forum": "MGPgpbtrss", "replyto": "MGPgpbtrss", "signatures": ["ICLR.cc/2026/Conference/Submission19459/Reviewer_wKZ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19459/Reviewer_wKZ4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761021540031, "cdate": 1761021540031, "tmdate": 1762931375036, "mdate": 1762931375036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to discover symmetries of high dimensional data in an unsupervised way. Although there are other methods for discovering symmetry in data, this method is said to contribute in four key ways: (1) handling group actions which are not transitive; (2) discovery of symmetry-based domains; (3) scalability to higher dimensions; and (4) more generalized symmetry representations. Two networks are trained, namely a lifting network (leveraging group convolutions) and the so-called auxilliary network. The groups considered herein are matrix groups which are Abelian, and experiments deal with translations and frequency-shifting, among other types of invariance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The notion of addressing scalability issues in symmetry detection is important, since many existing methods have been criticized for scalability problems.\n\n2. This paper builds upon many interesting ideas, including group convolutions."}, "weaknesses": {"value": "1. *Issue of transitivity discussion.* The authors are mistaken either in their understanding of transitive group actions or else in the group actions considered in recent literature. The authors claim that a novelty of their work is that their method works with intransitive group actions. However, a simple example of an intransitive group action is a group action in the plane defined by a rotation about the origin, which has most certainly been taken up by recent literature, much of which is cited by the authors. On the other hand, it is possible that the authors are misusing the term \"transitive group action,\" as evidenced by the following statement (lines 407-413): \"This setting forms a preliminary test to method’s ability for discovering symmetries when the dataset doesn’t involve identical samples those are globally translated versions of each other. In other saying, we aim to see if the model can learn the symmetries when the group acts intransitively.\" It seems that the authors may intend for the first sentence to serve as an implied definition of a transitive group action. However, if this is the case, the first statement does not provide an accurate definition of a transitive group action.\n\n2. *The commutative case is quite restrictive.* In the plane, the set of rotations and translations are not commutative. Additionally, 3-d rotations are not generally commutative. The restriction to the commutative case is quite inhibitive, significantly affecting the extent to which this paper can be considered a meaningful contribution.\n\n3. *The inabilities of other methods are not well established.* The authors claim that their method uniquely handles high-dimensional cases. But this should be demonstrated experimentally: in a high-dimensional case, the authors should show that all other methods (or at least a representative sample of other methods) either do not scale or fail to recover the ground truth symmetries. Experimental comparison could better illustrate the advantages of the proposed approach. Additionally, some existing methods do not require, as line 52 suggests, 100x100 matrices to capture translational symmetry in 10x10 data: see weakness 5 below.\n\n4. *It is not clear what problems this method solves*. Weakness 3 discusses the issue with the claim of scalability to higher dimensions, and weakness 1 speaks to the claim of novelty regarding transitivity. But I also feel that contributions 2 and 4 are not sufficiently motivated: it is not clear what problems are benefited from these contributions. I acknowledge, however, that this perceived limitation could be a result of my own misunderstanding, and I welcome any clarity the authors can provide.\n\n5. *Some work is left out of consideration*. The authors have not considered recent work on symmetry discovery using the Lie derivative, which may be of particular interest due to the focus on translational symmetry. Two recent papers are: (1) \"Symmetry Discovery Beyond Affine Transformations\" (Shaw et al., 2024), and (2) \"A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning\" (Otto et al., 2023).\n\n6. *Experimental descriptions leave much to be desired*. There are several issues with the experimental descriptions. In particular, I do not know under which experiment to interpret Table 1. I think the operators of translation and frequency shifts need to be given explicitly, along with all other types of invariance which appear in Table 1. The meaning of the figures is not clear. Also, it's not entirely clear what the authors mean by a \"translation-invariant dataset,\" given that the problem statement of finding a group of transformations which preserve the data distribution: truly, there are no translation invariant probability distributions. I also think the authors should relate each experiment to their stated contributions (which they do in the first experiment for intransitivity, which is problematic as mentioned in weakness 1). In fine, the experiments section seems as though it were placed in the paper as an afterthought: it lacks clarity, organization, and coherence."}, "questions": {"value": "1. How does your computational complexity compare to other methods?\n\n2. I'm not sure I understand how padding enforces translational invariance. Can you explain this?\n\n3. Other questions are implied from the perceived weaknesses--in particular, weakness 6. Addressing the weaknesses may result in an increased score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VsujpPfIbl", "forum": "MGPgpbtrss", "replyto": "MGPgpbtrss", "signatures": ["ICLR.cc/2026/Conference/Submission19459/Reviewer_16jy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19459/Reviewer_16jy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865445523, "cdate": 1761865445523, "tmdate": 1762931374592, "mdate": 1762931374592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel method for the unsupervised discovery of symmetries and symmetry-based domains from raw data. The key innovation is handling cases where the symmetry group acts intransitively on the dataset - meaning not all samples can be obtained from each other through symmetry operations. The approach uses a lifting operation inspired by Group Convolutional Networks (GCNs) to map observed signals to a representation space where symmetries manifest as simple translations. The method is demonstrated on various tasks including discovering pixel translations in MNIST images, recovering neighborhood structures in shuffled Ising models, and finding frequency-shift operators in time series data, achieving dimensionalities up to 27² × 27² for symmetry representations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "$\\textbf{Originality}$: The handling of intransitive group actions is genuinely novel in unsupervised symmetry discovery. The connection to inverse problems through symmetry learning is creative.\n\n$\\textbf{Technical soundness}$: The theoretical framework connecting lifting operations, group representations, and information theory is well-developed. The use of the locality prior to enable scalability is clever.\n\n$\\textbf{Experimental diversity}$: Testing on translation, frequency-shift, permutation, and combined symmetries demonstrates generality\nPractical impact: Real applications like unshuffling pixels (Figure 1) and recovering Ising model neighborhoods show the method's potential beyond toy problems.\n\nThe authors acknowledge limitations (abelian groups only, Gaussian distribution issues, performance degradation at scale) which is good."}, "weaknesses": {"value": "$\\textbf{Limited scalability analysis}$: While 27×27 is achieved, the drop in performance (Table 1, last row) suggests fundamental limitations. More analysis of how performance degrades with dimension would be valuable.\n\n$\\textbf{Comparison with baselines}$: The paper lacks quantitative comparisons with existing methods. Even if limited to 4×4, showing where your method excels would strengthen claims.\n\n$\\textbf{Gaussian distribution failure}$: This is mentioned but not thoroughly investigated. Since many real-world distributions are approximately Gaussian, this is concerning.\n\n$\\textbf{Hyperparameter sensitivity}$: The loss function has two hyperparameters (α, β) but no ablation study or guidance on setting them. How sensitive are results to these choices?\n\n$\\textbf{Computational cost}$: A detailed discussion of training time, memory requirements, or computational complexity compared to alternatives.\n\n$\\textbf{Non-abelian groups}$: While acknowledged as a limitation, some discussion of the fundamental barriers and potential paths forward would be valuable."}, "questions": {"value": "Performance degradation: In Table 1, the 27×27 MNIST shows significantly lower cosine similarities (0.675-0.722). Is this a fundamental limitation or could it be addressed with more training/different architectures?\n\nHyperparameter selection: How were α and β chosen? What is the sensitivity to these values? Can you provide guidance for practitioners?\n\nGaussian distributions: You mention covariance-based joint entropy approximation as problematic for Gaussians. Have you tried alternative entropy estimators (e.g., k-NN based, kernel-based)?\n\nNon-orthogonal representations: Section 5.1 mentions learning non-orthogonal distortions with a learnable K. How general is this? Could you elaborate?\n\nComputational cost: What are the training times and memory requirements for different problem sizes?\n\nReal inverse problems: Have you tested on any real inverse problems (e.g., image deblurring, compressed sensing)? This would greatly strengthen the contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3tmWVlXwVH", "forum": "MGPgpbtrss", "replyto": "MGPgpbtrss", "signatures": ["ICLR.cc/2026/Conference/Submission19459/Reviewer_vUGc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19459/Reviewer_vUGc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868621769, "cdate": 1761868621769, "tmdate": 1762931374276, "mdate": 1762931374276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}