{"id": "C15sPKE4uR", "number": 16274, "cdate": 1758262567404, "mdate": 1759897250717, "content": {"title": "Deconstructing Self-Bias in LLM-generated translation benchmark", "abstract": "As large language models (LLMs) begin to saturate existing benchmarks, automated benchmark creation using LLMs (LLM-as-a-benchmark) has emerged as a scalable alternative to slow and costly human curation. While these generated test sets have to potential to cheaply rank models, we demonstrate a critical flaw. LLM-generated benchmarks systematically favor the model that created the benchmark: they exhibit self-bias on low resource languages to English translation tasks. We show three key findings on automatic benchmarking of LLMs for translation: First, this bias originates from two sources: the generated test data (LLM-as-a-testset) and the evaluation method (LLM-as-an-evaluator), with their combination amplifying the effect. Second, self-bias in LLM-as-a-benchmark is heavily influenced by the model’s generation capabilities in the source language. For instance, we observe more pronounced bias in into-English translation, where the model’s generation system is developed, than in out-of-English translation tasks. Third, we observe that low diversity in source text is one attribution to self-bias. Our results suggest that improving the diversity of these generated source texts can mitigate some of the observed self-bias.", "tldr": "self-bias in LLM generated benchmark", "keywords": ["Self-bias", "automatic benchmark creation", "evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e265c2f86404140ddd7a21a7c15256b31e07b2f7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the self-bias in LLM generated benchmarks. LLM generated benchmarks favor the LLM that generated the benchmark dataset. The paper finds strong self-bias in low resource language to English translation. This self-bias comes from two sources, LLM's generation capability in the source language and using LLM as an evaluator. The authors then demonstrate that this self-bias can be attributed to LLM's limited generation capability in the source language."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The experimental design is sound. It cleanly separates out the two components of self-bias in LLM-as-a-benchmark: benchmark generation and evaluation.\n2. The authors perform experiments to determine whether LLM self-bias can be explained by the LLM's preference to generate source texts that are easily translatable by the LLM. Their results of source-only vs source+reference generation conclusively show that this hypothesis is true.\n3. The authors also show that even for source-only generation the LLM has low diversity, which leads to the self-bias persisting even in this scenario.\n4. Finally the authors also conduct experiment for out of English direction showing lower self-bias. Thus the experiments are comprehensive."}, "weaknesses": {"value": "1. The self bias measurement is based on ranking. This measure might be sensitive to the number of LLMs under consideration (just three in this paper). A larger set of LLMs (including open source multilinguality focused LLMs) may strengthen the results."}, "questions": {"value": "Why were the four low-resource languages chosen? Why not other languages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o3QF38kuTD", "forum": "C15sPKE4uR", "replyto": "C15sPKE4uR", "signatures": ["ICLR.cc/2026/Conference/Submission16274/Reviewer_j9bj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16274/Reviewer_j9bj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654737415, "cdate": 1761654737415, "tmdate": 1762926423468, "mdate": 1762926423468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission addresses the self-bias issue in the LLM-as-a-benchmark evaluation, where an LLM can overrate itself when using it as the evaluator or towards the benchmarks that it creates. This submission formally defines and decomposes self-bias into two components: bias from testset generation and bias from evaluation. Focusing on low-resource machine translation, this submission conducts experiments with Gemini-2.5-Pro, GPT-4.1, and Claude-Opus-4 across six translation directions. The results show that self-bias exists in both components and is strongest when combined. This submission further analyses the biases under different aspects, presenting a study that covers multiple points toward the self-bias in LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This submission addresses an important issue in LLM evaluation and is generally well written.\n2. It provides a formal definition and decomposition of self-bias into testset and evaluator components, which improves the clarity over prior discussions.\n3. The experiments cover three major LLMs across six translation directions with multiple controlled conditions.\n4. The analysis is interesting and can be insightful for future work."}, "weaknesses": {"value": "1. All experiments are limited to MT, leaving unclear how similar self-bias mechanisms apply to other generative or reasoning tasks.\n2.  Each language direction includes only 200 instances, which limits statistical robustness and the strength of causal claims. Also, there is no significance analysis in the results.\n3. The evaluated systems are all closed-source (Gemini, GPT-4.1, Claude), where open LLMs should be considered."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "APbvPOm9fM", "forum": "C15sPKE4uR", "replyto": "C15sPKE4uR", "signatures": ["ICLR.cc/2026/Conference/Submission16274/Reviewer_cz1b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16274/Reviewer_cz1b"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837470219, "cdate": 1761837470219, "tmdate": 1762926422396, "mdate": 1762926422396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that using LLMs to generate training data (even just inputs) or to act as a judge, biases any evaluation that also uses the same LLM as a participating system. The paper is well written but overall this is such an obvious problem that it does not really require such detailed investigation. There are not really any new insights presented."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The experiments are well designed and executed. \n\nThe paper is well written.\n\nThe analysis of lack of variety of generated sources is interesting."}, "weaknesses": {"value": "The overall point that LLMs are biases when used as both data generator/judge and participant is a very obvious problem. Anything a model produces will lie well within its distribution, and therefore it is likely to cover it well. \n\nThe particular setup to use LLMs to generate _source_ sentences for translation is a bit - there is typically not a lack of monolingual sentences to act as source, the problem is the lack of accurate reference translations. Of course, generating reference translations with an LLM and then treating them as gold standard is even more problematic (and it is the reason why at WMT shared tasks the professional translators are prohibited from using any machine translation tool)."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b2r4SUNQnf", "forum": "C15sPKE4uR", "replyto": "C15sPKE4uR", "signatures": ["ICLR.cc/2026/Conference/Submission16274/Reviewer_c4v9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16274/Reviewer_c4v9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969690200, "cdate": 1761969690200, "tmdate": 1762926421839, "mdate": 1762926421839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies self-bias in LLM-generated translation benchmarks. It first decomposes the concept of LLM-as-a-benchmark into two components: LLM-as-a-testset and LLM-as-an-evaluator. The observed bias arises from the interaction between these two roles and is more pronounced in low-resource-to-English translation tasks. The paper further demonstrates that limited diversity in the source texts contributes to self-bias, while increasing source-text diversity can mitigate it. Despite this limitation, LLM-as-a-benchmark remains a valuable approach for ranking weaker models and for evaluating translations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper conceptualizes LLM-as-a-benchmark as consisting of two distinct components, LLM-as-a-testset and LLM-as-an-evaluator, and provides dedicated analyses benefiting from this decomposition.\n2. LLM-as-a-benchmark remains a valuable approach for ranking weaker models and for evaluating translations."}, "weaknesses": {"value": "The testset generation process in the LLM-as-a-benchmark framework is not sufficiently well designed. A well-constructed benchmark should incorporate both diversity control and quality control [1] [2] [3]. The conclusion that “generating more diverse source texts can mitigate self-bias” arises precisely because the LLM-as-a-benchmark approach adopted in the paper lacks diversity control, which leads to unnecessary self-bias. Similarly, the degeneration issue observed in the generated source texts (see Figure 1) results from the absence of quality control. Consequently, the self-bias observed in a simply generated testset is analyzed within an overly narrow and unrealistic context, which undermines the persuasiveness of the argument. Therefore, it is helpful for testset generation to incorporate appropriate diversity control and quality control, so that analyses of self-bias conducted on such datasets can be sufficiently convincing.\n\nReferences\n\n[1] https://lmsys.org/blog/2024-04-19-arena-hard\n\n[2] Li, Tianle, et al. \"From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline.\" arXiv preprint arXiv:2406.11939 (2024).\n\n[3] Lin, Bill Yuchen, et al. \"WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild.\" The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "1. Since self-bias is sensitive to the diversity and quality of the benchmark, it is helpful that the diversity and quality of the benchmark are presented in detail.\n2. Moreover, studying self-bias across benchmarks with varying levels of diversity and quality provides more informative insights than studying self-bias on a simply generated benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gRqt3Ly0DM", "forum": "C15sPKE4uR", "replyto": "C15sPKE4uR", "signatures": ["ICLR.cc/2026/Conference/Submission16274/Reviewer_BRFz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16274/Reviewer_BRFz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984427245, "cdate": 1761984427245, "tmdate": 1762926421220, "mdate": 1762926421220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}