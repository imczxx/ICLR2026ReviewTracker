{"id": "mUiUV8qvS0", "number": 2411, "cdate": 1757076698279, "mdate": 1759898149838, "content": {"title": "Token-level Inference-Time Alignment for Vision-Language Models", "abstract": "Vision-Language Models (VLMs) have become essential backbones of modern multimodal intelligence, yet their outputs remain prone to hallucination-plausible text misaligned with visual inputs. Existing alignment approaches often rely on expensive fine-tuning with annotated preference data or sequence-level inference strategies  that provide only coarse, delayed feedback. To overcome these limitations, we present TITA (Token-level Inference-Time Alignment), a lightweight framework that freezes the base VLM and instead trains a reward model to approximate its distribution. During inference, implicit preference signals are extracted as log-probability ratios between the reward model and the target VLM, yielding dense autoregressive feedback. This formulation can be viewed as an inference-time variant of Direct Preference Optimization (DPO), providing token-level corrective signals without retraining the backbone. Extensive evaluations on LLaVA-1.5-7B and 13B show consistent gains across 12 benchmarks, with improvements of 8.6% on MMVet and 6.7% on POPE, indicating stronger general understanding and reduced hallucinations. Additional experiments on Qwen2.5-VL-7B and DeepSeek-VL2-27.5B show comparable gains, especially in hallucination reduction and VQA accuracy, while incurring negligible inference overhead. Code is available at: https://anonymous.4open.science/r/TITA-BEC6.", "tldr": "TITA enables fine-grained hallucination suppression through token-level inference-time alignment, achieving stronger grounding and reasoning with minimal overhead and no retraining of the base model.", "keywords": ["Large Multimodal Model", "Preference Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d34868a5f4f30804b77e278eeed9801e648a400.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes TITA, a token-level inference-time alignment framework for large vision–language models (VLMs). Instead of relying on costly fine-tuning or sequence-level reward reranking, TITA learns a small autoregressive reward model to guide decoding at the token level, using log-probability ratios between the reward model and the base VLM as implicit preference signals. The approach provides dense feedback during generation and shows consistent hallucination reduction and general VQA improvement across LLaVA, Qwen2.5-VL, and DeepSeek-VL2 families with minimal inference overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of bringing Direct Preference Optimization into inference-time, at the token level, is both conceptually neat and practical. It bridges the gap between coarse sequence-level feedback and expensive retraining.\n2. The experiments are broad (12 benchmarks, several VLM families) and show clear, consistent gains in hallucination suppression and visual reasoning accuracy with very low additional cost.\n3.The figures and algorithm explanations are intuitive; the comparisons with prior training-time and inference-time alignment frameworks (Fact-RLHF, CSR, SeVa, Critic-V) are fair and informative."}, "weaknesses": {"value": "1. It would be valuable to analyze how the reward model’s scale or quality influences performance — for example, comparing smaller versus larger reward models to verify robustness of token-level alignment.\n2. While the paper shows cross-model adaptability (7B to 27B), it would be insightful to analyze how reward model quality affects alignment. For instance, does using a smaller or noisier reward model degrade token-level signals significantly?"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NeAoXWAzng", "forum": "mUiUV8qvS0", "replyto": "mUiUV8qvS0", "signatures": ["ICLR.cc/2026/Conference/Submission2411/Reviewer_4Pei"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2411/Reviewer_4Pei"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632141770, "cdate": 1761632141770, "tmdate": 1762916225965, "mdate": 1762916225965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TITA, a lightweight inference-time alignment framework designed to efficiently suppress hallucinations in VLMs by providing dense, token-level feedback signals. The core idea of TITA is to combine a frozen base VLM with a trained lightweight reward model, using the log-probability ratio between them to guide the decoding process. It constructs preference data via a self-supervised multi-view fusion approach. Experiments demonstrate that TITA significantly reduces hallucinations (e.g., +6.7% on POPE) and improves VQA performance across models such as LLaVA, Qwen2.5-VL, and DeepSeek-VL2, while introducing negligible inference overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) TITA innovatively transforms sequence-level rewards into token-level signals, addressing the issues of feedback delay and high computational cost in existing methods. By directly guiding the decoding process without the need for sequence re-ranking, it enables timely intervention against hallucinations with extremely low training cost.\n\n(2) TITA is a plug-and-play method that does not modify the parameters of the base model, giving it strong generality and allowing it to be flexibly applied to VLMs of different scales and architectures."}, "weaknesses": {"value": "(1) TITA relies on image augmentation and response fusion to generate the “winning” responses. This mechanism primarily captures the comprehensiveness of visual elements, which may make it difficult to learn deeper semantic or complex reasoning errors that cause hallucinations in VLMs. As a result, the reward model may be limited in capturing more sophisticated preference patterns.\n\n(2) The proposed method is highly sensitive to the scaling factor lambda. As shown in Figure 3 of the paper, the performance peaks at λ  = 0.6 and drops rapidly afterward. This indicates that parameter tuning may be required when applying the method, potentially even across different tasks.\n\n(3) The reward model is trained using a sequence level BT loss to learn overall preferences between a winner (yw) and a loser (yl). However, during inference, it is used to provide token-level guidance for next-token generation. This conversion from sequence-level preference to token-level guidance may theoretically introduce inconsistencies especially in long-sequence generation, where locally optimal token choices may not guarantee the best overall sequence quality."}, "questions": {"value": "Does TITA-guided inference alter the model’s attention distribution? What is the quantitative relationship between this change and the reduction in hallucination rates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w7Qjf2CyZt", "forum": "mUiUV8qvS0", "replyto": "mUiUV8qvS0", "signatures": ["ICLR.cc/2026/Conference/Submission2411/Reviewer_MXoj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2411/Reviewer_MXoj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858597378, "cdate": 1761858597378, "tmdate": 1762916225388, "mdate": 1762916225388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TITA, a test-time alignment framework designed to mitigate hallucinations in Vision-Language Models (VLMs). The method employs a fine-tuned, lightweight reward model to guide the decoding at the token level of the target VLM during inference. Compared with existing approaches, the authors claim that TITA achieves superior effectiveness and efficiency. Experimental results demonstrate that the proposed framework yields significant overall performance improvements over the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear and Well-Structured: The paper is well-organized, with detailed explanations of the preliminary, intuition, and methodology.\n\n- Superiority in Alignment: The experimental results demonstrate that the proposed method achieves the overall best performance on the general VQA and hallucination benchmarks compared to the baselines."}, "weaknesses": {"value": "- The backbones used in the experiments are somewhat outdated, particularly since the main results presented in Table 2 are based on the LLaVA 1.5 series models. While I acknowledge that the authors also provide results using Qwen-2.5-VL and DeepSeek-VL2, a more comprehensive evaluation using such recent and stronger VLMs would strengthen the manuscript.\n\n- As a highly competitive and rapidly evolving research area, VLM alignment should provide evaluation against up-to-date methods and backbones. However, the paper primarily compares its approach with relatively outdated baselines (from 2023–2024) and employs older backbone models. This is difficult for me to fully assess the effectiveness of the proposed method.\n\n- The presentation accuracy could be further improved. For example, TITA is not the best-performing method on MMB when using LLaVA-1.5-7B. SeVa achieves higher performance in this setting.\n\n- The core techniques incorporated in TITA are based on well-established principles from prior works. While the derivation is clear and well-presented, it does not introduce fundamentally new concepts but rather applies existing methods in a different context."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "71Qq64UHya", "forum": "mUiUV8qvS0", "replyto": "mUiUV8qvS0", "signatures": ["ICLR.cc/2026/Conference/Submission2411/Reviewer_j1ML"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2411/Reviewer_j1ML"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979890783, "cdate": 1761979890783, "tmdate": 1762916225190, "mdate": 1762916225190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}