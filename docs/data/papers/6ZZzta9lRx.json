{"id": "6ZZzta9lRx", "number": 17234, "cdate": 1758273740086, "mdate": 1759897189325, "content": {"title": "Context-Enriched Embeddings for Robust 3D Scene Understanding", "abstract": "3D scene understanding is fundamental for embodied AI and robotics, supporting reliable perception for interaction and navigation. Recent approaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning embedding vectors to 2D class-agnostic masks generated via vision-language models (VLMs) and projecting these into 3D. However, these methods often produce fragmented masks and inaccurate semantic assignments due to the direct use of raw masks, limiting their effectiveness in complex environments. To address this, we leverage SemanticSAM with progressive granularity refinement to generate more accurate and numerous object-level masks, mitigating the over-segmentation commonly observed in mask generation models such as vanilla SAM, and improving downstream 3D semantic segmentation. To further enhance semantic context, we employ a context-aware CLIP encoding strategy that integrates multiple contextual views of each mask using empirically determined weighting, providing much richer visual context. We evaluate our approach on multiple 3D scene understanding tasks, including 3D semantic segmentation and object retrieval from language queries, across several benchmark datasets. Experimental results demonstrate significant improvements over existing methods, highlighting the effectiveness of our approach.", "tldr": "", "keywords": ["3D scene understanding", "Foundation models", "VLMs", "Mask generation", "Embeddings"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e48d1d48664b65aa7b19ec814ed929cc60bb55ff.pdf", "supplementary_material": "/attachment/47f405fc702d3a4a6fa8552819e473f44729c58f.zip"}, "replies": [{"content": {"summary": {"value": "Summary of the Paper\nThe paper introduces CORE-3D, a training-free pipeline designed for open-vocabulary 3D scene understanding and object retrieval based on natural language queries. The authors identify two key weaknesses in prior methods: 1) the generation of fragmented and incomplete object masks by standard segmentation models, and 2) the lack of sufficient visual context when assigning semantic labels, leading to inaccurate classifications.\nTo address this, CORE-3D proposes a multi-stage approach. First, it uses SemanticSAM with a progressive granularity strategy to generate high-quality 2D object masks. Second, it develops a context-aware encoding strategy that computes CLIP embeddings from five different contextual views of each mask and aggregates them. Finally, these 2D predictions are lifted to 3D, where they are merged and refined using geometric heuristics to ensure multi-view consistency. This results in a coherent 3D semantic map that can be queried using natural language to perform complex segmentation and retrieval tasks without any task-specific training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Impressive Empirical Performance: The primary strength of this work lies in its impressive empirical results. The proposed method, CORE-3D, achieves state-of-the-art performance across multiple standard benchmarks, including Replica, ScanNet, and SR3D+. It significantly outperforms prior approaches in both 3D semantic segmentation metrics (mIoU, fmIoU) and language-based object retrieval accuracy. The strong quantitative and qualitative results provide compelling evidence of the method's effectiveness in complex 3D environments.\nWell-Motivated Methodological Design: The paper effectively identifies and addresses key limitations of existing methods. The proposed context-aware CLIP embedding strategy, which aggregates features from multiple contextual crops, is a well-motivated and intuitive approach to mitigate semantic ambiguity caused by context-deficient object views. This design choice directly tackles a critical bottleneck in open-vocabulary 3D perception."}, "weaknesses": {"value": "Lack of Ablation Studies and Unclear Contribution Attribution: The most significant weakness of this paper is the complete absence of ablation studies. The authors introduce several new components—a progressive SemanticSAM refinement strategy, a multi-crop context-aware embedding scheme, and 3D geometric refinement heuristics—but provide no experiments to disentangle their individual contributions. Consequently, the true source of the substantial performance gains remains unclear. This omission undermines the paper's methodological rigor and makes it difficult to verify the utility of each proposed component.\nLimited Technical Novelty and Over-reliance on Existing Models: The technical novelty of the work is limited, as the framework is primarily a sophisticated integration of powerful, pre-existing models (SemanticSAM, CLIP, LLMs/VLMs). The performance heavily relies on the superior capabilities of SemanticSAM, which is not a contribution of this paper. The authors list a \"SemanticSAM refinement strategy\" as a main contribution, yet this appears to be more of a specific application of the model's inherent multi-granularity features rather than a novel algorithmic extension. Without a crucial ablation that compares its performance when using a more standard backbone (e.g., the original SAM), it is difficult to assess the true impact of the authors' own contributions versus the significant gains inherited from a superior off-the-shelf component."}, "questions": {"value": "1.Regarding the lack of ablation studies: The performance gains demonstrated are impressive, but the lack of ablation studies makes it difficult to attribute these gains to specific components of your method. Could you provide experiments to quantify the contribution of each key component: (a) the progressive granularity refinement, (b) the context-aware CLIP embedding, and (c) the 3D mask merging/refinement? In particular, a crucial baseline would be to replace SemanticSAM with the original SAM within your pipeline to demonstrate the effectiveness of your proposed strategies independent of the advanced segmentation backbone.\n2.On Hyperparameter Sensitivity: The paper states that the weights for the context-aware embedding and the overlap thresholds (τ_k) for mask generation are \"empirically tuned.\" Could you provide more details on the tuning process? More importantly, how sensitive is the model's performance to these hyperparameters? A sensitivity analysis would significantly strengthen the claims of the method's robustness and aid in its reproducibility.\nAnalysis of Limitations and Failure Cases: The qualitative results are strong, but the paper would benefit from a discussion of the method's limitations. Could you provide some examples of failure cases? For instance, in what types of scenes or for what kinds of objects does the segmentation or retrieval pipeline struggle? An analysis of these failures could provide valuable insights for future work"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HdmLO2cBki", "forum": "6ZZzta9lRx", "replyto": "6ZZzta9lRx", "signatures": ["ICLR.cc/2026/Conference/Submission17234/Reviewer_RZwy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17234/Reviewer_RZwy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761308973840, "cdate": 1761308973840, "tmdate": 1762927192315, "mdate": 1762927192315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method for building feature embedded 3D space. AUthors observe that prior works relies on inaccurate raw masks from SAM model, and object-only CLIP extraction that ignores surrounding context. To overcome, authors suggest multi-view consistent mask uplifting pipeline and context-aware CLIP features aggregation. For context aware CLIP feature, authors suggest to use multi-resolution image cropping from object to surroundings and weighted aggregation for context understanding. The paper also define a open-vocab object retrieval task and present a possible solution to handle such problem. Across experiments, the method shows improved context understanding compared to prior works."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes a new task (object retrieval) and presents one possible solution for the problem.\n\n- Introduces a multi-resolution, context-aware strategy to strengthen context understanding—an aspect underemphasized in prior work."}, "weaknesses": {"value": "**Heavy reliance on manual hyperparameters**\n- Many steps require hand-tuned choices (cropping patches for context-aware CLIP, weighting for aggregation, filtering thresholds). These appear distribution- and scene-dependent, risking poor robustness when scene composition shifts.\n\n**Missing ablations for design choices.**\n- It is difficult to verify the benefit of each component:\n- No direct evidence that multi-granularity crops improve context understanding.\n- Other design choices are not empirically validated, making the claimed advantages hard to trust.\n\n**Presentation & reproducibility (minor but notable).**\n- Figures have tiny text and excessive margins; readability can be improved.\n- The heading starting at line 319 should be bold.\n- Reproducibility details are sparse (e.g., which retrieval model and which prompts for object retrieval). Including these (even in an appendix) would strengthen credibility."}, "questions": {"value": "All questions are embedded within the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sUJwcLxmtR", "forum": "6ZZzta9lRx", "replyto": "6ZZzta9lRx", "signatures": ["ICLR.cc/2026/Conference/Submission17234/Reviewer_Bam8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17234/Reviewer_Bam8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819619173, "cdate": 1761819619173, "tmdate": 1762927191982, "mdate": 1762927191982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an algorithm for building a semantic point embeddings. Given posed images and points, this paper first extract the class-agnostic masks from images. Second, for each view, this method compute CLIP embeddings while considering the different size of bounding boxes that potentially have more context information. Last, the method merges 2D masks into 3D masks and locate the CLIP embeddings on the 3D masks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The overall flow of this paper is readable and understandable. The proposed schemes are reasonable to build up the semantics embeddings on top of pointclouds. \n\nThe proposed pipeline is quite similar to the ConceptFusion, but this paper addresses the problem of SAM and resolve this by SemanticSAM, which makes sense to me. Moreover, different size of crops for extracting CLIP embeddings looks good to me as well. While it can be a naive solution, the addressed problem stated in Line 242 of the manuscript is true.\n\nWhile the paper achieves performance improvement compared to previous studies, I found many issues."}, "weaknesses": {"value": "__W1. Weak comparison__  \nI believe that the authors mostly track the related studies that are originated from ConceptFusion, ConceptGraph, etc. However, in the recent studies, such as Mosaic3D [A] and RegionPLC [B], these studies also tackle the same task, open vocabulary 3D semantic segmentations. Moreover, if the authors refer to the line of this studies, these methods also utilize the Vision Language Models for their data generation pipeline where the generated data are used to train the 3D neural networks. \n\nDespite the high similarities, the authors did not cite the papers. Accordingly, there is no technical comparisons with this submission and [A,B]. Thus, I cannot be sure whether this paper really achieves high-quality predictions in comparison with [A,B] as well. The authors should have checked the related studies a lot.\n\n__W2. Mask merging is not something new__\nI recommend the authors to refer to this paper, Gaussian grouping [C]. This paper aims to obtain the view-consistent masks (which is called `mask merging` in this submission) by leveraging 3D Gaussian Splatting [D]. In my understanding, this submission as well as [C] starts from the similar input data: images, camera parameters (extrinsic / intrinsic). Within the detail, [C] also computes the overlapping ratio between masks and 3D Gaussians as proposed by this submission. \n\nBased on this understanding, I do not think that the proposed mask merging described in Section 3.3 is novel or unique. Moreover, there is no qualitative / quantitative analysis on this module, so I cannot catch its effectiveness as well.\n\n__W3. What is the template to use VLM?__\nIn Section 3.4, the authors use VLM to perform the object retrieval task. Commonly, when the papers use the VLM or LLM, the papers provide the templates to be used for their target task. However, in this submission, I cannot find such a information throughout the manuscript. This is quite a serious problem.\n\n__Related works__  \n[A] Mosaic3D, CVPR 2025  \n[B] RegionPLC, CVPR 2024  \n[C] Gaussian grouping, ECCV 2024\n[D] 3D Gaussian Splatting for Real-Time Radiance Field Rendering, SIGGRAPH 2023"}, "questions": {"value": "Overall, the paper is not ready for the submission. I recommend the authors to check the relevant and the recent studies about pointcloud based OpenVocab segmentation methods. Moreover, if the authors look at the 3D Gaussian based methods, such as LangSplat[H] and OpenGaussian [F], the authors can find the numerous ways of extracting CLIP embeddings from images.\n\nNonetheless, I hope the authors' responses for my opinions and analysis.\n\n__Related works__  \n[F] OpenGaussian, Neurips 2024  \n[H] LangSplat, CVPR 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nJQJrqpy41", "forum": "6ZZzta9lRx", "replyto": "6ZZzta9lRx", "signatures": ["ICLR.cc/2026/Conference/Submission17234/Reviewer_ENYB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17234/Reviewer_ENYB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925951952, "cdate": 1761925951952, "tmdate": 1762927191628, "mdate": 1762927191628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}