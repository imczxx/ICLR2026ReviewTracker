{"id": "3vmDyYC9Tn", "number": 5767, "cdate": 1757933239791, "mdate": 1763712186328, "content": {"title": "Visual Prompt-Agnostic Evolution", "abstract": "Visual Prompt Tuning (VPT) enables effective adaptation of a frozen Vision Transformer (ViT) to downstream tasks by inserting a small number of learnable prompt tokens into the token sequence at each layer. However, we observe that existing VPT variants often suffer from unstable training dynamics, characterized by gradient oscillations. A closer layer-wise analysis reveals that shallow-layer prompts tend to stagnate early, while deeper-layer prompts exhibit high-variance oscillations, leading to a cross-layer mismatch. These issues contribute to slower convergence and degraded final performance. To address these challenges, we propose the Prompt-Agnostic Evolution ($\\mathtt{PAE}$) method, which can strengthen vision prompt tuning by explicitly modeling the dynamics of learnable prompts. From a frequency-domain perspective, we initialize prompts in a task-aware direction by uncovering and propagating frequency shortcut patterns that the backbone inherently exploits for recognition. To ensure coherent evolution across layers, we further employ a shared Koopman operator, which imposes a global linear transformation rather than uncoordinated, layer-specific updates. Finally, inspired by Lyapunov stability theory, we introduce a regularizer that constrains error amplification during evolution. Extensive experiments demonstrate that using $\\mathtt{PAE}$ with VPT variants not only accelerates convergence with an average 1.41$\\times$ speedup but also yields 1–3% gains on 25 datasets with multi downstream tasks. Beyond performance, $\\mathtt{PAE}$ remains prompt-agnostic and lightweight, and it integrates seamlessly with diverse VPT variants without backbone modification or inference-time changes, providing a practical and scalable solution for advancing prompt tuning.", "tldr": "", "keywords": ["computer vision", "visual prompt tuning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3bf093f28d8edc71d8f1fbf35804fffda9a5f7d6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Visual cue adaptation effectively adapts frozen pre-trained models by inserting a small number of learnable cue tokens into each layer of ViT. However, existing VPT variants often suffer from dynamic instability in training, manifested by gradient oscillations, early stagnation of shallow-layer cues, and high-variance oscillations of deep-layer cues, leading to slow convergence and ultimately degraded performance. To address these challenges, this paper proposes Prompt-Agnostic Evolution (PAE), which enhances VPT by explicitly modeling the dynamic evolution of learnable cues."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. PAE is lightweight and independent of prompts, a significant decoupling advantage.\n2. It performs well, especially on classification tasks.\n3. The derivation is robust, with no major issues."}, "weaknesses": {"value": "This method introduces multiple new hyperparameters, increasing the complexity of model tuning. Ablation experiments in the paper show that model performance is sensitive to the choice of these parameters. This means that in real-world applications, tedious search and fine-tuning for different tasks may be required to achieve optimal results, which somewhat limits its plug-and-play usability.\n\nThe experimental validation in the paper focuses primarily on the specific architecture ViT-Base and the task of image classification. While achieving convincing results on the FGVC and VTAB-1k datasets, the effectiveness of this method on a wider range of visual tasks, such as object detection and semantic segmentation, has not yet been verified. This raises questions about its general applicability."}, "questions": {"value": "The paper simplifies the evolution of cues across layers to a global linear transformation (the Koopman operator) in a shared latent space. While this linear assumption effectively promotes inter-layer consistency and stabilizes training, does it limit the model's expressive power, especially when dealing with complex tasks or very deep networks with highly nonlinear inter-layer relationships? Exploring nonlinear dynamical models or introducing layer-wise adaptive evolutionary operators might yield further improvements in model performance at the expense of a small degree of simplicity.\n\nReframing cue tuning as a dynamical system is a novel perspective. In addition to stabilizing training and accelerating convergence, can this framework also be used to improve model interpretability? For example, by analyzing the eigenvalues ​​and eigenvectors of the learned Koopman operator, can we gain insight into how the model adjusts its internal representation to adapt to new tasks? Or can we identify the \"dynamic patterns\" that are crucial for specific tasks, thereby providing theoretical guidance for more effective cue design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o6AU19X4tR", "forum": "3vmDyYC9Tn", "replyto": "3vmDyYC9Tn", "signatures": ["ICLR.cc/2026/Conference/Submission5767/Reviewer_oUNy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5767/Reviewer_oUNy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760453938000, "cdate": 1760453938000, "tmdate": 1762918248037, "mdate": 1762918248037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank you for your constructive and insightful comments. We have carefully revised our paper to address your concerns. All significant changes and new experiments in the revised PDF are **highlighted**.\n\n**Summary of Major Updates:**\n\n1. **Expanded Scope & Architectures:** We added experiments on **ViT-L/16, ViT-H/14** (Large/Huge models), **Swin-B** (Hierarchical), **MAE** (Self-supervised), and **SETR** (Segmentation architecture) to demonstrate scalability and robustness (**Table 2, Fig. 6, and Fig. 7**).\n2. **New Task (Dense Prediction):** We added **Semantic Segmentation** results on **ADE20K** to demonstrate generalization beyond classification (**Table 2**).\n3. **Stronger Baselines:** We added comparisons with **LoRA, SPT-LoRA, and DM-LoRA** in the Appendix (**Tables 8-11**).\n4. **In-depth Analysis:** We included **CKA (Fig. 7)**, **Intra-class variance (Fig. 8)** and **Spectral analysis (Fig. 10)** to visualize the mechanism of our dynamical system.\n5. **Clarifications:** We revised the **Introduction** to discuss projection-based methods and the **Results** to quantify initialization costs.\n\nWe believe these updates significantly strengthen the paper and fully address the concerns regarding generalization, efficiency, and mechanism."}}, "id": "DMCInsEgeK", "forum": "3vmDyYC9Tn", "replyto": "3vmDyYC9Tn", "signatures": ["ICLR.cc/2026/Conference/Submission5767/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5767/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5767/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763712200651, "cdate": 1763712200651, "tmdate": 1763712200651, "mdate": 1763712200651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Visual cue adaptation effectively adapts frozen pre-trained models by inserting a small number of learnable cue tokens into each layer of ViT. However, existing VPT variants often suffer from dynamic instability in training, manifested by gradient oscillations, early stagnation of shallow-layer cues, and high-variance oscillations of deep-layer cues, leading to slow convergence and ultimately degraded performance. To address these challenges, this paper proposes Prompt-Agnostic Evolution (PAE), which enhances VPT by explicitly modeling the dynamic evolution of learnable cues."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. PAE is lightweight and independent of prompts, a significant decoupling advantage.\n2. It performs well, especially on classification tasks.\n3. The derivation is robust, with no major issues."}, "weaknesses": {"value": "This method introduces multiple new hyperparameters, increasing the complexity of model tuning. Ablation experiments in the paper show that model performance is sensitive to the choice of these parameters. This means that in real-world applications, tedious search and fine-tuning for different tasks may be required to achieve optimal results, which somewhat limits its plug-and-play usability.\n\nThe experimental validation in the paper focuses primarily on the specific architecture ViT-Base and the task of image classification. While achieving convincing results on the FGVC and VTAB-1k datasets, the effectiveness of this method on a wider range of visual tasks, such as object detection and semantic segmentation, has not yet been verified. This raises questions about its general applicability."}, "questions": {"value": "The paper simplifies the evolution of cues across layers to a global linear transformation (the Koopman operator) in a shared latent space. While this linear assumption effectively promotes inter-layer consistency and stabilizes training, does it limit the model's expressive power, especially when dealing with complex tasks or very deep networks with highly nonlinear inter-layer relationships? Exploring nonlinear dynamical models or introducing layer-wise adaptive evolutionary operators might yield further improvements in model performance at the expense of a small degree of simplicity.\n\nReframing cue tuning as a dynamical system is a novel perspective. In addition to stabilizing training and accelerating convergence, can this framework also be used to improve model interpretability? For example, by analyzing the eigenvalues ​​and eigenvectors of the learned Koopman operator, can we gain insight into how the model adjusts its internal representation to adapt to new tasks? Or can we identify the \"dynamic patterns\" that are crucial for specific tasks, thereby providing theoretical guidance for more effective cue design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "o6AU19X4tR", "forum": "3vmDyYC9Tn", "replyto": "3vmDyYC9Tn", "signatures": ["ICLR.cc/2026/Conference/Submission5767/Reviewer_oUNy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5767/Reviewer_oUNy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760453938000, "cdate": 1760453938000, "tmdate": 1763730327012, "mdate": 1763730327012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Visual prompt tuning enables parameter-efficient fine-tuning. However, existing VPT variants often suffer from unstable training. To address this challenge, the authors propose the Prompt-Agnostic Evolution (PAE) by explicitly modeling the dynamics of learnable prompts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem of unstable VPT training is well defined. The observations on shallow- and deep-layer prompts are interesting. The clear mismatch for gradient oscillations is something researchers might be interested in. \n\n2. The paper is easy to follow, and the problem-solving is practical.\n\n3. The ablation study is sufficient."}, "weaknesses": {"value": "1. The masking then project idea sounds similar to projection-based (a.k.a instance-aware) prompt tuning [1-2], where these papers use input projection directly to guide prompt training. The authors need to discuss them and clearly separate their differences. \n\n2. The format in conclusion is a little bit weird. Please fix it.\n\n3. The motivation of this paper can be clearer, for example, why the authors want to discover frequency shortcuts. I understand the observations; however, their motivations are unclear to me.\n\n[1] Visual Instance-aware Prompt Tuning\n\n[2] All You Need is One: Capsule Prompt Tuning with a Single Vector"}, "questions": {"value": "My question mainly focuses on the discussions on projection-based prompt tuning methods. Other than that, this paper looks good to me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZsSsY5UnXF", "forum": "3vmDyYC9Tn", "replyto": "3vmDyYC9Tn", "signatures": ["ICLR.cc/2026/Conference/Submission5767/Reviewer_9KV8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5767/Reviewer_9KV8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760831637904, "cdate": 1760831637904, "tmdate": 1762918247693, "mdate": 1762918247693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a core instability issue in Visual Prompt Tuning (VPT), a fine-tuning method for Vision Transformers (ViTs). The authors observe that many VPT variants suffer from unstable training processes, characterized by gradient oscillations and a cross-layer mismatch phenomenon, where prompts in shallower layers stagnate while those in deeper layers oscillate to compensate. This ultimately slows down convergence and hinders optimal performance. The paper identifies two root causes for this problem: task-agnostic prompt initialization and the independent, uncooperative optimization of prompts at each layer. To resolve this, the authors propose Prompt-Agnostic Evolution (PAE), a framework that treats prompt tuning as a dynamical system. PAE consists of two novel components. The first is Modal Pre-Alignment (MPA), which provides a task-aware initialization by identifying the most discriminative frequency shortcuts for a given task and using them to generate initial prompts. The second is the Koopman-Lyapunov Discrete Dynamical System (KLD), which governs the prompt optimization. It uses a shared Koopman operator to enforce a coherent linear evolution for prompts across multiple layers within a shared latent space, and a Lyapunov-style regularizer to ensure this evolution remains stable. In experiments on the FGVC and VTAB-1k benchmarks, applying PAE to various VPT methods consistently improved accuracy by 1-3% and accelerated convergence by an average of 1.48x. As a prompt-agnostic module, PAE can be integrated into existing methods without modifying the model's backbone and has zero overhead at inference time, making it a practical and effective solution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The primary strength of this research is its novel conceptualization of prompt tuning as a dynamical system, providing a principled framework to address the observed training instabilities. By applying the Koopman operator and Lyapunov stability theory, it moves beyond empirical heuristics and introduces an explicit mechanism to coordinate prompt updates across layers, directly tackling the optimization mismatch problem. Another key innovation is the Modal Pre-Alignment (MPA) strategy. This method effectively solves the cold start problem in prompt tuning by using a task-aware initialization based on frequency-domain analysis. By identifying the frequency shortcuts already utilized by the pre-trained backbone, MPA provides initial prompts that are well-aligned with the task objective from the outset, which emerged as the single largest contributor to performance gains. Finally, the PAE framework demonstrates remarkable robustness and practicality. Its prompt-agnostic design allows it to be seamlessly integrated as a plug-and-play module into numerous state-of-the-art VPT variants, consistently improving performance in all cases. This proves that PAE addresses a fundamental weakness in the VPT paradigm. The comprehensive empirical validation, which includes not only accuracy metrics but also insightful loss landscape and Grad-CAM visualizations, provides strong evidence for its effectiveness. The fact that there is zero inference-time overhead further solidifies its value for real-world applications."}, "weaknesses": {"value": "Assumptions of the KLD Framework: The Koopman-Lyapunov Discrete Dynamical System (KLD) assumes that the prompt dynamics can be effectively modeled by a single, global, linear operator. This could create a representational bottleneck for complex tasks where different dynamics in shallow and deep layers might be more beneficial. The framework's success also hinges on the assumption that prompt evolution is approximately linear in the learned latent space, which has not been validated across diverse model architectures and scales. The Lyapunov-style stability constraint, while effective, might be overly restrictive, potentially preventing the model from exploring optimal solutions that require a temporary increase in complexity. Consequently, although performance has been demonstrated, these are results from a single model, and the effectiveness of these simplifications (a single global operator and regularization) may diminish as conditions become more complex. \n\nLimited Experimental Scope: The paper's empirical validation is limited to a single backbone architecture (ViT-Base/16), so it remains unverified whether PAE's effectiveness generalizes to other architectures. It also does not investigate how performance varies with model scale (e.g., larger or smaller ViTs). Lastly, the absence of a direct comparative analysis with other major PEFT (Parameter-Efficient Fine-Tuning) families, such as LoRA, makes it difficult to fully assess the pros and cons of PAE-enhanced VPT within the broader PEFT landscape. An exploration of its orthogonality and complementarity with these methods would be beneficial.\n\nIn conclusion, the study has demonstrated the success of the KLD framework's simplifying assumptions within the specific context of ViT-Base/16. However, it has not been verified whether this success will hold under more complex conditions, such as with more intricate architectures, much larger-scale models, or when combined with other PEFT techniques like LoRA. In other words, a key limitation of this research is that the possibility that the success of this simplification is a coincidence within the limited experimental scope cannot be ruled out."}, "questions": {"value": "I would appreciate your response to the points raised in the \"Weakness\" section.\n\nAdditionally:\n- I would like to know how the convergence speed varies with changes in the hyperparameters alpha and beta.\n- I am interested in the performance differences based on intra-class variance. It would be helpful to see the difference in performance gains between the best- and worst-performing classes or class groups in the dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethic concerns"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YzFwRiRneh", "forum": "3vmDyYC9Tn", "replyto": "3vmDyYC9Tn", "signatures": ["ICLR.cc/2026/Conference/Submission5767/Reviewer_PEzU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5767/Reviewer_PEzU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761280464190, "cdate": 1761280464190, "tmdate": 1762918247420, "mdate": 1762918247420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Prompt-Agnostic Evolution (PAE), a framework designed to stabilize and accelerate training in Visual Prompt Tuning (VPT) for Vision Transformers. The authors identify that existing VPT variants suffer from unstable gradients, including shallow-layer stagnation and deep-layer oscillations. To address this, they propose two key components: 1) Modal Pre-Alignment (MPA): A frequency-domain initialization that aligns prompts with task-relevant frequency “shortcuts.” 2) Koopman-Lyapunov Discrete (KLD) system: A shared dynamical model where prompts evolve across layers under a Koopman operator with Lyapunov-based regularization for stability.\nExperiments on FGVC and VTAB-1k benchmarks show 1–3% accuracy improvements and 1.5× faster convergence in terms of the number of required epochs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel formulation: Reframing prompt tuning as a dynamical system using Koopman theory and Lyapunov stability is novel and mathematically grounded.\n\n2. Comprehensive analysis: The paper clearly diagnoses VPT training instability through layer-wise gradient visualizations and supports it with quantitative results.\n\n3. Strong empirical performance: PAE consistently improves various VPT baselines, showing strong generalization across tasks and benchmarks.\n\n4. Prompt-agnostic applicability: The method is modular, adding no inference-time overhead and requiring no backbone modification.\n\n5. Clear ablations: Ablation studies demonstrate the complementary roles of MPA and KLD, and verify robustness against random initialization and batch selection."}, "weaknesses": {"value": "- Motivation clarity: While the dynamical-system framing is novel and interesting, the necessity of such complexity for solving gradient oscillation may be overstated. Simpler temporal regularization could have been compared. For example, could simpler smoothing (e.g., temporal moving average across layers) achieve comparable stability?\n\n- Dependence on frequency bias: MPA relies on identifying “frequency shortcuts,” which may not exist or be stable in non-natural image domains, limiting transferability.\n\n- Missing comparison on across-layer effects: The paper does not cite prior work such as [ref1], which examines how prompts interact across layers. A more direct comparison and analysis between that study’s findings and the proposed PAE framework would strengthen the discussion.\n\n[ref1] Improving Visual Prompt Tuning for Self-supervised Vision Transformers, ICML 2023\n\n- Questionable real-world efficiency: Although Fig. 1(a) and Table 1 show faster convergence in terms of epochs, the overall efficiency claim may be overstated. When accounting for MPA initialization overhead and the extra hyperparameters (α, β, K, w, r) that expand the tuning space, the total wall-clock time might actually increase. Hence, the practical benefit in large-scale or hyperparameter-sensitive scenarios remains uncertain."}, "questions": {"value": "Please see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gdxzwRyVOW", "forum": "3vmDyYC9Tn", "replyto": "3vmDyYC9Tn", "signatures": ["ICLR.cc/2026/Conference/Submission5767/Reviewer_Z7w9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5767/Reviewer_Z7w9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009824627, "cdate": 1762009824627, "tmdate": 1762918247083, "mdate": 1762918247083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}