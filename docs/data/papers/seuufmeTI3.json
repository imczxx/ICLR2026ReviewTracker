{"id": "seuufmeTI3", "number": 24862, "cdate": 1758361298210, "mdate": 1763694999453, "content": {"title": "On the Convergence of LoRA-Based Federated Learning: A Unified Analysis of Aggregation-Broadcast Operators", "abstract": "Federated Learning (FL) enables collaborative model training across decentralized data sources while preserving data privacy. However, the increasing scale of Machine Learning (ML) models poses significant communication and computation challenges in FL. Low-Rank Adaptation (LoRA) has recently been integrated into FL as a Parameter-Efficient Fine-Tuning (PEFT) strategy, substantially lowering communication costs by transmitting only a small set of trainable parameters. Nevertheless, how to aggregate LoRA-updated local models on the server remains a critical and understudied problem. This paper presents a comprehensive theoretical analysis of LoRA-based FL frameworks. We first classify existing aggregation schemes into two main categories: Sum-Product (SP) and Product-Sum (PS). We then introduce the Aggregation-Broadcast Operator (ABO) as a general class encompassing all aggregation-broadcast methods. Any method in this class ensures local or global convergence as long as the corresponding Weak or Strong Convergence Condition is satisfied. In particular, we prove that the SP and PS aggregation methods satisfy the weak and strong convergence conditions, respectively, but differ in their ability to achieve the optimal convergence rate. Moreover, we conducted extensive experiments on standard open datasets to verify our theoretical findings.\n\nAI Acknowledgment: We acknowledge that AI tools were employed to assist in paper writing and polishing the text to improve readability.", "tldr": "Theoretical convergence analysis for lora enabled distributed fine-tuning.", "keywords": ["federated learning", "low rank adaptation", "convergence analysis", "fine turning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3779ec681b0e02b33047e7ed2fa85b1a171acfc4.pdf", "supplementary_material": "/attachment/241f9f48953e7298839a5ba40c975a8f03c14418.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides the first unified convergence analysis of LoRA-based Federated Learning (FL) systems.\nWhile LoRA significantly reduces communication costs by transmitting low-rank adapters instead of full model updates, how these low-rank matrices should be aggregated and broadcasted across clients remains underexplored. \nThe authors introduce a general theoretical framework based on the Aggregation–Broadcast Operator (ABO), which captures and unifies various existing LoRA aggregation schemes. Two categories are defined:\n(1) SP-type (Sum–Product) aggregation, such as FedAvg and FlexLoRA; (2) PS-type (Product–Sum) aggregation, such as RBLA and FFA-LoRA. The paper provides convergence guarantees for both types: SP operators satisfy weak convergence (local convergence); \nPS operators satisfy strong convergence (global convergence) with an optimal rate of O(1/T). \nExtensive experiments on multiple benchmarks (MNIST, FMNIST, KMNIST, QMNIST) confirm the theoretical findings, showing the distinct trade-offs between rank reduction and local training epochs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The presentation is clear, with clean notation and solid mathematical exposition.\n* The experiments align well with theoretical predictions, reinforcing the soundness of the analysis."}, "weaknesses": {"value": "* The experiments are only on small vision datasets (MNIST variants); results on larger-scale or real-world FL settings (e.g., language models or CIFAR) would better demonstrate generality.\n* Although convergence conditions are clear theoretically, the paper provides limited guidance on how to choose between SP and PS schemes in practice.\n* Similar low-rank decomposition or subspace parameterization techniques (though not explicitly named “LoRA”) have already been explored in federated settings for model compression and aggregation, for example [1][2]. The paper does not sufficiently discuss or compare with these existing studies, which weakens its claim of novelty.\n\n[1] FedLMT: Tackling System Heterogeneity of Federated Learning via Low-Rank Model Training with Theoretical Guarantees. ICML 2024.\n[2] The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning. ICML 2025"}, "questions": {"value": "How does the proposed analysis differ from previous low-rank decomposition approaches in federated learning, such as [1][2] work on communication-efficient low-rank FL?\n\n[1] FedLMT: Tackling System Heterogeneity of Federated Learning via Low-Rank Model Training with Theoretical Guarantees. ICML 2024.\n[2] The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning. ICML 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PEOcXNzEa2", "forum": "seuufmeTI3", "replyto": "seuufmeTI3", "signatures": ["ICLR.cc/2026/Conference/Submission24862/Reviewer_bujH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24862/Reviewer_bujH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566567610, "cdate": 1761566567610, "tmdate": 1762943226006, "mdate": 1762943226006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "elLvemni2H", "forum": "seuufmeTI3", "replyto": "seuufmeTI3", "signatures": ["ICLR.cc/2026/Conference/Submission24862/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24862/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763694998882, "cdate": 1763694998882, "tmdate": 1763694998882, "mdate": 1763694998882, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies FL when clients fine‑tune only LoRA adapters. It formalizes a general Aggregation‑Broadcast Operator (ABO) that maps client LoRA factors $(B_i^{(t+1)}, A_i^{(t+1)})$ to broadcasted factors $(P(\\cdot), Q(\\cdot))$. Within this framework, the authors define *Weak* and *Strong* Convergence Conditions**, prove $O(1/\\sqrt{T})$ convergence rates for local (weak) and global (strong) convergence. \n\nAdditionally, this paper positions “Sum‑Product” (SP, averaging full low‑rank updates then SVD‑factoring) as satisfying only the weak condition due to SVD truncation error, while “Product‑Sum” (PS, averaging $B$ and $A$ separately) satisfies the strong condition, and it empirically compares these two on MNIST‑family datasets under extreme non‑IID (1 class/client) with grid sweeps over LoRA rank and local epochs. \nThis work demonstrates that SP is more sensitive to small ranks, while PS is more sensitive to numerous local epochs, aligning with the theory.\n\n\n\n---\n### LLM usage disclosure (reviewer)\nI used GPT‑5 to help polish and organize this review; I take full responsibility for the content."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The ABO abstraction cleanly covers common LoRA‑FL aggregators (SP/FedAvg‑style on $\\Delta W$, PS with mean of $A$ and $B$, and others by choice of $P,Q$). This helps compare methods on equal footing.\n \n2. The analysis isolates an aggregation‑broadcast mismatch term $R$ (Def. 2) and shows how it controls rates $D/(\\eta T)+M\\eta$ (Theorem 2) and $D/(\\eta T)+(M+N)\\eta$ (Theorem 4). The narrative that SVD truncation in SP inflates $R$ is intuitive and practically applicable.\n \n3. Corollaries suggest that if you can realize $PQ=\\frac1m\\sum_i B_iA_i$, you minimize $R$ under the weak condition; if you can realize $P=\\overline{B}, Q=\\overline{A}$, you minimize $R$ under the strong condition.\n \n4. Heatmaps over rank and local epochs in Fig. 1 provide a compact, readable check of sensitivities predicted by the bounds."}, "weaknesses": {"value": "1. **Assumptions are strong and under‑motivated.**\n\n- **Bounded adapter norms** (Assumption 3: $|B_i^{(t)}|_F\\le C_B), (|A_i^{(t)}|_F\\le C_A)$ are nontrivial to guarantee for SGD without explicit projection/regularization. The paper relies on them but does not propose mechanisms ensuring they hold in practice.\n\n- **All clients active; constant step‑size; single global $\\eta$.** The sensitivity to partial participation and step‑size schedules is not addressed, which limits practical relevance. \n\n2. **Sufficient conditions are almost tautological.** Theorems 1 & 3 restate the weak/strong conditions at the per‑client level, but they don’t provide constructive, verifiable criteria that can be used to check a new aggregator. The interesting part is the corollaries, but those essentially say “choose $P, Q$ to equal the population averages,” which is obvious when dimensions permit. \n\n3. **PS “optimality” assumes equal ranks and compatible shapes.** Corollary 2 states PS attains the strong‑condition optimum “if all clients share the same LoRA rank.” Some of the LoRA-FL literature features heterogeneous rank. This analysis does not formally cover these practical cases, and the experiments do not evaluate them. \n\n4. **Empirical evaluation is too limited**\n\n- Only small‑scale **MLP on MNIST/Fashion/Q/KMNIST**, 10 clients, class‑partitioned non‑IID. No CIFAR/Imagenet, no NLP tasks, no larger models, no heterogeneous ranks, no partial participation. \n\n- Also, reporting highest test accuracy achieved per setting is optimistic, final accuracy or area‑under‑curve is more informative. There are no variances or CIs. \n\n5. **Clarity issues and internal inconsistencies.**\n\n- In Fig. 1, subfigure labels and the text appear swapped.\n\n- In Sec. 5.2, the statement \"when $\\delta=0.01$...\"  must be $\\delta=0.07$ to match the heatmap.\n\n- Notation drifts between Aggregated Broadcasting Operator and Aggregation‑Broadcast Operator.\n\n6. **Limited novelty in rate statements.** \nThe $O(1/\\sqrt{T})$ type results for nonconvex FL with local steps under smoothness/bounded variance assumptions are well‑known. The novelty is the way the ABO mismatch enters the constants. That’s interesting, but incremental without broader empirical validation."}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k0Z1sPDrHK", "forum": "seuufmeTI3", "replyto": "seuufmeTI3", "signatures": ["ICLR.cc/2026/Conference/Submission24862/Reviewer_BnTq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24862/Reviewer_BnTq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895226415, "cdate": 1761895226415, "tmdate": 1762943225801, "mdate": 1762943225801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a unified convergence theory for LoRA-based federated learning through the Aggregation–Broadcast Operator (ABO) framework, which generalizes how clients’ LoRA updates are aggregated and redistributed. It analyzes two main types: Sum–Product (SP) which average the products $B_i A_i$ and then factorize via SVD and Product–Sum (PS) which average the factors separately and multiply $\\bar{B} \\bar{A}$. The paper also introduces weak and strong Convergence Conditions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The ABO formulation is simple and clarifying; it makes precise what “aggregate then factorize” (SP) and “factorize then aggregate” (PS) are doing and offers general conditions under which either will converge."}, "weaknesses": {"value": "-\tThe experiments use small MLPs on MNIST family datasets with 10 clients, whereas the motivation centers on large models where LoRA matters most. There is no evaluation on CIFAR like vision tasks, transformers, or LLM fine-tuning settings used by state-of-the-art LoRA FL baselines (e.g., FLoRA, FlexLoRA).\n\n-\tFig.1 appears to contradict the intended conclusions. For epoch = 1, the PS panel shows accuracy rising from 27.4% at δ=0.01 to 91.4% at δ=0.07 (range ≈ 64 points), while the SP panel rises from 41.4% to 89.1% (range ≈ 47.7). Thus, PS looks more sensitive to rank than SP, contrary to the text’s claim that “SP is more sensitive to the LoRA rank.” Section_5.2 also swaps SP and PS numbers (e.g., attributing 27.4% to SP when it belongs to PS). This weakens the empirical support for the theoretical sensitivity statements.\n\n-\tThe paper reports the best accuracy achieved over training rather than convergence curves, gradient norms, or communication/compute metrics tied to the theory (e.g., how E impacts the constants R,P,Q)\n\n-\tMethods that specifically target heterogeneous ranks and aggregation noise FLoRA (stacking), RBLA (rank aware), FFA LoRA (freeze one factor), and RoLoRA (alternating factors) are not compared experimentally, though they are among the most relevant competitors and are explicitly discussed in the related work.\n\n-\tThe ABO analysis does not explicitly map to stacking (FLoRA) or alternating (RoLoRA) schemes; it is unclear whether those operators satisfy the weak/strong conditions and with what constants.\n\n-\tThe main theorems assume all clients are active every round and homogeneous ranks for the PS optimality statement. Modern FL frequently uses partial participation and rank heterogeneity; some cited methods are designed around that. The analysis does not cover these cases.\n\n-\tThe “optimality” claims are about constants, not rates (both results are $O(1/\\sqrt{T})$. However, the optimal point in Corollary 1 minimizes $∑_i∥Z−BiAi∥$ at $Z=1/m(∑BiAi)$ , without a rank constraint. In practical SP, the server must broadcast rank r factors; the best rank-r solution to that least-squares problem is the truncated SVD of the mean, which the paper labels as “broadcast error” and uses to argue sub-optimality. That comparison is unfair unless the same rank-r constraint is enforced in the definition of “optimal” under Weak Convergence Condition. Clarify the objective being optimized (unconstrained vs rank-constrained)."}, "questions": {"value": "1.\tIn section_5.2, the text attributes values that visually belong to the other panel (e.g., 27.4% appears in the PS panel, not SP). Which panel corresponds to which method, and do the sensitivity conclusions still hold after correcting these swaps?\n\n2.\tCorollary 2 mentions PS achieves the optimal rate when clients share the same rank. How does the analysis change for heterogeneous ranks (the more common case)? Can zero padding or RBLA-style rank-aware averaging be brought under your strong condition, and what constants result?\n3.\tCan FLoRA style stacking or RoLoRA’s alternating updates be expressed as ABOs that satisfy weak/strong conditions? If so, what are the bounds?\n4.\tSP requires SVD; PS requires broadcasting full factor averages. What is the server-side compute and communication per round for SP vs. PS in your implementation, and how does this scale with model size?\n5.\tUnder the Weak convergence condition, if the server must broadcast rank r factors, is the true optimal P, Q (minimizing your bound) the best rank r approximation to $1/m∑_iB_iA_i$  (i.e., truncated SVD)? If so, how does your “broadcast loss” discussion change? Please formalize the rank constrained counterpart of Corollary 1.\n\n6. PS “optimality.” Is “optimal” meant only within the ABO discrepancy metric (minimizing R,P, Q)? Can you reconcile cases where minimizing R may not correlate with downstream accuracy (e.g., PS’s product of means bias vs. task loss)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "14juDDE5Gi", "forum": "seuufmeTI3", "replyto": "seuufmeTI3", "signatures": ["ICLR.cc/2026/Conference/Submission24862/Reviewer_5AtY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24862/Reviewer_5AtY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989856659, "cdate": 1761989856659, "tmdate": 1762943224212, "mdate": 1762943224212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified theoretical framework for analyzing the convergence behavior of Federated LoRA for both two classic aggregation methods, i.e., sum product-aggregation broadcast operator (SP-ABO) and sum product (PS)-ABO. This paper reveals that SP cannot achieve the optimal convergence rate due to broadcast errors, whereas the PS operator satisfies the strong convergence condition and achieves both global convergence and the optimal convergence rate. Extensive experiments evaluate the theoretical findings. However, I have some concerns as follows: 1) The recent work claims that PS has the aggregation error, i.e., FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations. 2) The scenario of heterogeneous ranks among clients should also be considered in the theoretical analysis, but it is missing in this paper; 3) The experimental results are not complete. Although this paper focuses on the convergence analysis of SP and PS methods, state-of-the-art aggregation methods are also required in the experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a unified theoretical framework for analyzing the convergence behavior of Federated LoRA for both two classic aggregation methods, i.e., sum product-aggregation broadcast operator (SP-ABO) and sum product (PS)-ABO. This paper reveals that SP cannot achieve the optimal convergence rate due to broadcast errors, whereas the PS operator satisfies the strong convergence condition and achieves both global convergence and the optimal convergence rate. Extensive experiments evaluate the theoretical findings. Overall, this paper aims to address a significant issue and well-written."}, "weaknesses": {"value": "I have some concerns as follows: 1) The recent work claims that PS has the aggregation error, i.e., FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations. 2) The scenario of heterogeneous ranks among clients should also be considered in the theoretical analysis, but it is missing in this paper; 3) The experimental results are not complete. Although this paper focuses on the convergence analysis of SP and PS methods, state-of-the-art aggregation methods are also required in the experiments."}, "questions": {"value": "My questions are shown as follows: \n1) The recent work claims that PS has the aggregation error, i.e., FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations; \n2) The scenario of heterogeneous ranks among clients should also be considered in the theoretical analysis, but it is missing in this paper; \n3) The experimental results are not complete. Although this paper focuses on the convergence analysis of SP and PS methods, state-of-the-art aggregation methods are also required in the experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "itnD0OlOid", "forum": "seuufmeTI3", "replyto": "seuufmeTI3", "signatures": ["ICLR.cc/2026/Conference/Submission24862/Reviewer_eCQc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24862/Reviewer_eCQc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762074131878, "cdate": 1762074131878, "tmdate": 1762943223742, "mdate": 1762943223742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}