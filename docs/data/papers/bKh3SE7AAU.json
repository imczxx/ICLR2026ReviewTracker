{"id": "bKh3SE7AAU", "number": 8401, "cdate": 1758081961434, "mdate": 1759897787558, "content": {"title": "COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference Optimization", "abstract": "Real-world LLM agents must master strategic tool orchestration and user preference optimization through multi-turn interactions to assist users with complex planning tasks. We introduce COMPASS (Constrained Optimization through Multi-turn Planning and Strategic Solutions), a benchmark that evaluates agents on realistic travel planning scenarios. We cast travel planning as a constrained preference optimization problem, where agents must satisfy hard constraints while simultaneously optimizing soft user preferences. To support this, we build a realistic travel database covering transportation, accommodation, and ticketing for 20 U.S. National Parks and a comprehensive tool ecosystem that mirrors commercial booking platforms. Evaluating state-of-the-art models, we uncover two critical gaps: (i) an acceptable–optimal gap, where agents reliably meet constraints but fail to optimize preferences, and (ii) a plan-coordination gap, where performance collapses on multi-service (flight, hotel) coordination tasks, especially for open-source models. By grounding reasoning and planning in a practical, user-facing domain, COMPASS offers a benchmark that directly measures an agent’s ability to optimize user preferences in realistic tasks, bridging theoretical advances with real-world impact.", "tldr": "COMPASS is a benchmark that tests whether LLM agents can intelligently optimize user preferences in realistic, tool-based travel planning tasks.", "keywords": ["multi-turn tool calling", "preference optimization", "reasoning and planning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1619a2fbbb934f554fc8b7eac7e6846c8222c81.pdf", "supplementary_material": "/attachment/cf62963a03e9776a38c0ed34a9a8748088a59057.zip"}, "replies": [{"content": {"summary": {"value": "This paper propose the Compass benchmark that focus on multi-turn preference optimization with rich and complex environments and tools.\nThey adopt hard constraints and soft preferences and the latter is expected to be optimized through multi-turn interactions.\nThey also design different levels of tasks to evaluate model performance.\nThe tools included are diverse and comprehensive, mirroring the real-world environment.\nThe results show that despite some models can perform well on acceptable rate, they fall short in optimal rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a complex environment that mirrors real-world and messy noises rather than a cleaned and simplified simulation.\n2. It focuses on the optimal solution rather than an acceptable solution that evaluates how models can best serve as an applicable agent.\n3. The multi-turn interactions with the simulated user brings more complications into this problem."}, "weaknesses": {"value": "1. The simulated user query could be deployed to attain more diverse interactions beyond the 241 tasks."}, "questions": {"value": "1. The paper find that agents have higher acceptable rate than the optimal rate. Do you have a hypothesis for why this happens? Is it a failure of insufficient working memory, lack of exploration or sticking to the original solution without searching further?\n2. Can the user simulators be deployed to create more diverse and complex test cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H75uHiphDF", "forum": "bKh3SE7AAU", "replyto": "bKh3SE7AAU", "signatures": ["ICLR.cc/2026/Conference/Submission8401/Reviewer_9Th3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8401/Reviewer_9Th3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870552811, "cdate": 1761870552811, "tmdate": 1762920302178, "mdate": 1762920302178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces COMPASS, a benchmark for evaluating LLM agents’ constrained preference optimization in travel planning, addressing gaps in existing tool-use and planning benchmarks. However, critical limitations in novelty prevent it from meeting ICLR’s standards for publication."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The benchmark design integrates realistic travel data (20 U.S. National Parks), a multi-turn user simulator, and a comprehensive tool ecosystem, which aligns with real-world agent deployment scenarios.\n\n2. The identification of \"acceptable–optimal gap\" and \"plan-coordination gap\" provides insights for future agentic travel planning."}, "weaknesses": {"value": "1. The primary issue is that the core idea of integrating constrained satisfaction with preference optimization is built upon existing research without introducing substantive innovations. For instance, TravelPlanner initially addressed hard constraint satisfaction, while subsequent works such as ChinaTravel [1] have emphasized the synergy between hard and soft constraints, and TripTailor [2] has also explored aspects of personalization. However, this paper entirely overlooks these aspects, thereby significantly undermining its claimed contributions.\n\n2. Although the authors focus on constraint satisfaction problems, the Related Work section predominantly discusses tool-use benchmarks, while largely ignoring established benchmarks in travel planning—a domain that has seen extensive research. This omission represents a notable gap in the literature review.\n\n3. The proposed level classification lacks intuitive justification. Why not use the number of constraints as a basis for level categorization, which would offer a more straightforward and interpretable framework?\n\n4. The concept of an \"acceptable–optimal gap\" raises questions about its practical relevance. Under hard constraints, single-objective preference optimization admits a deterministic solution. While achieving this optimum may be challenging for models, it is debatable whether reaching the true optimum is necessary in real-world scenarios. What, then, is the substantive significance of this gap?\n\n5. The relationship between the \"plan-coordination gap\" and constrained optimization remains unclear and appears conceptually disjointed. The connection between the two proposed gaps and how LLMs address constraint satisfaction problems is neither well-motivated nor clearly articulated.\n\n6. The evaluation methodology for hard and soft constraints is inadequately defined. Using the set of feasible solutions as a metric is problematic: when the number of constraints is small and the database is large, the feasible set can become excessively large—or even impossible to enumerate—rendering evaluation infeasible. While such a measure may be meaningful for preference evaluation, insisting on optimality for all preferences is arguably unreasonable. This touches upon multi-objective optimization (MOO), where Pareto optimality should be considered. Why not employ model-based ranking for preference evaluation instead?\n\n\n7. The experiment completely ignored Agent-based algorithms, such as ReAct and LLM-module. Why?\n\n\n\n[1] ChinaTravel: An Open-Ended Benchmark for Language Agents in Chinese Travel Planning. 2024.\n\n[2]  TripTailor: A Real-World Benchmark for Personalized Travel Planning. 2025."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dkhtuI2LMb", "forum": "bKh3SE7AAU", "replyto": "bKh3SE7AAU", "signatures": ["ICLR.cc/2026/Conference/Submission8401/Reviewer_Raxv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8401/Reviewer_Raxv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969027643, "cdate": 1761969027643, "tmdate": 1762920301756, "mdate": 1762920301756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Proposes COMPASS, a multi-turn benchmark for tool-mediated travel planning and preference optimization.\n\n- Builds an interactive evaluation framework with tools, databases, and a user simulator reflecting dynamic user behaviors.\n\n- Evaluates several LLM agents, revealing gaps in preference optimization and multi-tool coordination performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "I find this paper’s contribution significant in its dataset and evaluation framework.\n\n- **Dataset**\n    - The dataset itself is highly valuable. While most prior works rely on synthetic data, this paper collects a *real-world* dataset through APIs, which makes it much more realistic and potentially very useful for other researchers in this domain.\n- **Systematized Evaluation Framework**\n    - The authors integrate various evaluation aspects from prior travel-planning studies into a systemized framework.\n    - To my knowledge, this is the first systematic attempt to evaluate preference optimization, which is an essential dimension in this domain.\n    - **Answer construction**: The authors consider all possible combinations of factors when constructing reference answers, and then define metrics accordingly. This goes beyond simple “pass/fail” checks for each constraint and represents a thoughtful attempt to capture composite preferences."}, "weaknesses": {"value": "While the dataset and framework contributions are strong, some parts, especially the experimental section, leave room for improvement.\n\n**w1. Dataset description insufficiency**\n\n  * The paper lacks a detailed explanation of how the dataset was collected via APIs.\n  * While collecting *real-world* data is commendable, the authors should provide a short case study illustrating **what aspects of real-world interactions are captured that synthetic datasets fail to model**.\n\n **w2. Missing citations / related work**\n\n  * There exist prior works on **multi-turn travel planning** (e.g., [1]) that address similar interactive planning settings, which is similar to one of their proposed user settings (*progressive constraint revelation*).\n\n **w3. Framework design clarity**\n\n  * *User Simulator Design* (around L250) introduces within-conversation dynamics and persona diversity, which are interesting ideas.\n  * However, it remains unclear **whether these behaviors are grounded in the real-world dataset** or are purely heuristic / author-defined.\n  * The rationale and distribution (e.g., “how often users exhibit such behaviors”) should be explicitly stated.\n\n **w4. Framework validation**\n  * While authors provide validation of framework in Table 2, It would be helpful if the validation were reported separately for each simulation type.\n\n **w5. Limited evaluation depth**\n\n  * The results are not sufficiently analyzed across different **user simulation types**, though model behavior likely varies under each setting.\n  * Although tool-calling is presented as a key feature of the benchmark, the paper does not analyze **how often models succeed or fail at tool calls**, nor how such success/failure impacts final outcomes.\n  * (Minor) The discussion in L411–413 about open-source model tendencies seems weak, since only the Qwen models were evaluated, limiting generalizability.\n\n**w6. Limited novelty/insight in findings**\n\n  * The contribution of collecting a real-world dataset is valuable, but some findings derived from it struggle to present clear takeaways beyond the dataset itself.\n  * For instance:\n\n    * Section 5.1 on *constraint conflicts* somewhat overlaps with the findings of [1]. \n    * Section 5.2 on *conversation efficiency* reproduces patterns already observed in prior reasoning tasks ([2]).\n\n**w7. Presentation and display issues (minor)**\n\n  * Tables and figures are placed far from where they are discussed, disrupting readability (e.g., Figure 2 on page 2).\n---\nReference\n\n[1] Flex-TravelPlanner: A Benchmark for Flexible Planning with Language Agents, Oh et al, 2025\n\n[2] MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback, Wang et al, 2023"}, "questions": {"value": "Already covered in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GJ52mCSPV1", "forum": "bKh3SE7AAU", "replyto": "bKh3SE7AAU", "signatures": ["ICLR.cc/2026/Conference/Submission8401/Reviewer_3qtC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8401/Reviewer_3qtC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976778626, "cdate": 1761976778626, "tmdate": 1762920301374, "mdate": 1762920301374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a multi-turn benchmark for evaluating Large Language Model (LLM) agents on tool-mediated planning and constrained preference optimization within a travel planning environment. The benchmark's evaluation reveals that current state-of-the-art LLMs can satisfy basic constraints but consistently fall short in finding truly optimal, preference-aligned solutions, particularly as task complexity increases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is original in how it formalizes multi-turn travel planning as constrained preference optimization, explicitly separating hard feasibility constraints from soft preference objectives. This framing enables measurable evaluation of both “acceptable” solutions and optimization quality.   \n2. The benchmark design is reasonable. It couples a realistic tool ecosystem (hotels, flights, permits) with a dynamic, persona-driven user simulator and provides ground-truth optima via exhaustive enumeration over feasible combinations. Metrics are interpretable and complementary, and the analysis is thorough across plan-coordination levels, constraint count, search complexity, and conversation efficiency."}, "weaknesses": {"value": "1. External validity is constrained by the LLM-based user simulator. Despite some human audits, the simulator’s coverage of adversarial, ambiguous, or inconsistent user behaviors is limited, and there is no direct comparison with real users.  \n2. The domain scope is narrow, which can even be considered an extension of the TravelPlanner benchmark. Triptailor[1] is also a multi-turn travel planning benchmark, which involves more detailed itineraries and personalized requirements. Treating this work as an interface design and analyzing the challenges of the task on different benchmarks would make a greater contribution to the community. \n3. The analysis of tool usage is crucial in travel planning, as discussed in section 5.3. However, it's unfortunate that only a case study is provided there. Is there a better mechanism for quantitative analysis in this area? \n4. The paper's title emphasizes \"Tool-Mediated Planning and Preference Optimization,\" yet a substantial portion of the content is dedicated to addressing constraints of varying difficulty levels. This creates a certain degree of misalignment between the stated focus and the actual technical emphasis. \n\n[1] Triptailor: A real-world benchmark for personalized travel planning"}, "questions": {"value": "1. Simulator coverage: How diverse are personas and scripts relative to the full task space? Can you provide coverage analyses and persona fidelity checks, including adversarial or underspecified behaviors?  \n2. In an agent that uses multiple rounds of tool calls to ultimately achieve planning, how should the agent consider calling the tools, how should the tools be called, and whether the call is successful? What methods are available for analyzing the capabilities of an LLM in this regard?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YQkC3QSqJ8", "forum": "bKh3SE7AAU", "replyto": "bKh3SE7AAU", "signatures": ["ICLR.cc/2026/Conference/Submission8401/Reviewer_YyYx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8401/Reviewer_YyYx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991441226, "cdate": 1761991441226, "tmdate": 1762920301003, "mdate": 1762920301003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}