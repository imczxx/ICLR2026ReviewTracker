{"id": "aAP5qqgzJh", "number": 18662, "cdate": 1758289830657, "mdate": 1759897089313, "content": {"title": "Propaganda AI: An Analysis of Semantic Divergence in Large Language Models", "abstract": "Large language models (LLMs) can exhibit *concept-conditioned semantic divergence*: common high-level cues (e.g., ideologies, public figures) elicit unusually uniform, stance-like responses that evade token-trigger audits. This behavior falls in a blind spot of current safety evaluations, yet carries major societal stakes, as such concept cues can steer content exposure at scale. We formalize this phenomenon and present **RAVEN** (**R**esponse **A**nomaly **V**igilance), a black-box audit that flags cases where a model is simultaneously highly certain and atypical among peers by coupling *semantic entropy* over paraphrastic samples with *cross-model disagreement*. In a controlled LoRA fine-tuning study, we implant a concept-conditioned stance using a small biased corpus, demonstrating feasibility without rare token triggers. Auditing five LLM families across twelve sensitive topics (360 prompts per model) and clustering via bidirectional entailment, RAVEN surfaces recurrent, model-specific divergences in 9/12 topics. Concept-level audits complement token-level defenses and provide a practical early-warning signal for release evaluation and post-deployment monitoring against propaganda-like influence.", "tldr": "We audit LLMs for concept-triggered response uniformity using RAVEN, which couples semantic entropy with cross-model disagreement; validated via a stance-implant experiment and an evaluation across five models and twelve topics.", "keywords": ["Large Language Models (LLMs)", "LLM Security", "Semantic Divergence", "Semantic Inconsistency", "Black-box Auditing"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5847c63191a24497bbf86cedb4e5278c96f2ad53.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The current work provides a black-box audit method which flags when a model is highly certain as well as atypical amongst its peers, or otherwise exhibiting so called concept-conditioned semantic divergence. They utilize a LoRA based fine-tuning study and audit five LLM families in order to uncover propaganda-like influence."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. the scope and threat model sections are well written and easy to follow\n2. the set up and algorithmic depiction of the RAVEN methodology is well motivated\n3. all five relationship categories make sense and fall nicely within many of the major buckets that most LLM evaluations currently aim to capture.\n4. the experimental methodology is clear in its direction and the motivation for how to answer each of the four proposed research questions through this methodology is evident."}, "weaknesses": {"value": "1. use of GPT-4o-mini for semantic clustering. this is potentially non-reproducible given the closed source nature of this model. why were popular open-weight models not consulted? how different would the experimental results be if, say, a Qwen model was used in place of GPT-4o-mini?"}, "questions": {"value": "N/A see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7ZoQxnFbc1", "forum": "aAP5qqgzJh", "replyto": "aAP5qqgzJh", "signatures": ["ICLR.cc/2026/Conference/Submission18662/Reviewer_rxGs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18662/Reviewer_rxGs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953770115, "cdate": 1761953770115, "tmdate": 1762928362360, "mdate": 1762928362360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RAVEN, a black-box method to detect concept-conditioned semantic divergence in LLMs by combining semantic entropy and cross-model disagreement. Experiments show that RAVEN can reveal stance-like, concept-triggered biases across multiple models and topics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies and formalizes concept-conditioned semantic divergence as a distinct and socially relevant risk in LLMs, extending safety evaluation beyond token-level triggers.\n\n2. The proposed RAVEN method is a simple yet effective black-box auditing approach that requires no access to model internals and thus is applicable to both open and closed LLMs.\n\n3. The study includes both controlled stance-implantation experiments and large-scale audits, providing evidence that concept-level biases can be both induced and naturally present in deployed systems."}, "weaknesses": {"value": "1. The choice of \\alpha=0.4 and the detection thresholds(\\theta_\\epsilon, \\theta_d) are not analyzed for robustness or sensitivity, leaving uncertainty about parameter stability across settings.\n\n2. Semantic clustering fully relies on GPT-4o-mini for bidirectional entailment, which may propagate that model’s own biases into the detection results. No human verification or quantitative validation is provided to confirm clustering reliability.\n\n3. The LoRA-based concept bias experiment uses an even 50/50 split of biased and control samples, yet the impact of bias proportion on implant strength and detectability is not studied."}, "questions": {"value": "1. Have you evaluated how the suspicion score S changes under different settings of \\alpha, \\theta_\\epsilon, and \\theta_d? Some sensitivity or ablation results would clarify the robustness of RAVEN’s detection behavior.\n\n2. Since GPT-4o-mini is used for bidirectional entailment, did you perform any manual or quantitative validation (e.g., human-labeled cluster agreement) to confirm the reliability of the clustering process?\n\n3. In the stance implantation study, the training set uses 50% biased data. Have you examined how different bias ratios (e.g., 25%, 75%) affect both implant effectiveness and RAVEN’s detectability?\n\n4. How would RAVEN extend to multi-turn dialogues or dynamic concept discovery, where semantic divergence may accumulate gradually rather than appear in single-turn prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KjGDXvs1NV", "forum": "aAP5qqgzJh", "replyto": "aAP5qqgzJh", "signatures": ["ICLR.cc/2026/Conference/Submission18662/Reviewer_3gdN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18662/Reviewer_3gdN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979844596, "cdate": 1761979844596, "tmdate": 1762928361892, "mdate": 1762928361892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The method introduced in this paper RAVEN (Response Anomaly Vigilance) has an interesting take on bias that results from training data. It demonstrates that entire concepts can be learned through controlled fine-tuning, and for that reason could be implanted on purpose. The paper shows across several topics for various LLMs how such biased learning can happen."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is important because it addresses a valid worry: How do LLMs shape opinion or portray information in general. Unlike newspapers they are currently still mostly seen as neutral but might not have been designed that way.\nThe paper is innovative in combining semantic entropy (within-model consistency) and cross-model disagreement. It uses bidirectional entailment for clustering responses which is more sophisticated than simple lexical matching. The black-box approach is realistic. The authors rightly point to limitation in their triag of signals."}, "weaknesses": {"value": "The paper could be improved by being more careful with its concepts. \"Propaganda AI\" is not defined. It is also not clear what the differences are between propaganda (intention) and bias (function of the training data) etc. \n\nThere is no justification on why the topics have been selected and why they are considered sensitive. It is not clear if when the majority of the models say the same this is closer to the truth, it is just closer to the corpus. A divergent model could be right. Polling averages are also sometimes wrong when there is systematic nonresonse. \n\nIt might help to consult with social science theorists like Goffman to think about presentation of concepts, with agenda-setting research, and survey methodology and techniques for detecting acquiescence bias, social desirability bias, etc."}, "questions": {"value": "Questions on could ask: When Mistral consistently gives positive assessments of Tesla, is this propaganda or reflective of predominantly positive coverage in training data? What are the criteria for distinguishing intentional manipulation and prescriptive pull? What could be normative benchmarks? On climate change and vaccination subject matter experts might see it as problematic that Mistral rejects arguments for vaccine hesitancy. What is the expert validation? What are the justifications for the suspicion score values? Could you show some examples of text responses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S9Oe6Vn62w", "forum": "aAP5qqgzJh", "replyto": "aAP5qqgzJh", "signatures": ["ICLR.cc/2026/Conference/Submission18662/Reviewer_1bHm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18662/Reviewer_1bHm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982312711, "cdate": 1761982312711, "tmdate": 1762928361442, "mdate": 1762928361442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}