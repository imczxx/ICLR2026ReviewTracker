{"id": "q9Jz28sqgd", "number": 10286, "cdate": 1758166014842, "mdate": 1759897660790, "content": {"title": "Towards General Agentic Intelligence via Environment Scaling", "abstract": "Advanced agentic intelligence is a prerequisite for deploying Large Language\nModels in practical, real-world applications. Diverse real-world APIs demand\nprecise, robust function-calling intelligence, which needs agents to develop\nthese capabilities through interaction in varied environments. The breadth of\nfunction-calling competence is closely tied to the diversity of environments\nin which agents are trained. In this work, we scale up environments as a step\ntowards advancing general agentic intelligence. This gives rise to two central\nchallenges: (i) how to scale environments in a principled manner, and (ii) how\nto effectively train agentic capabilities from experiences derived through inter-\nactions with these environments. To address these, we design a scalable frame-\nwork that automatically constructs heterogeneous environments that are fully\nsimulated, systematically broadening the space of function-calling scenarios.\nWe further adapt a two-phase agent fine-tuning strategy: first endowing agents\nwith fundamental agentic capabilities, then specializing them for domain-\nspecific contexts. Extensive experiments on agentic benchmarks, τ-bench,\nτ2-Bench, and ACEBench, demonstrate that our trained model, AgentScaler,\nsignificantly enhances the models’ function-calling capability.", "tldr": "", "keywords": ["tool use", "general agent"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/312e9c1eb3f87adeb598712744685986d386f8c2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AgentScaler, a framework for advancing general agentic intelligence through environment scaling and teacher-free experience learning. The authors propose an automatic pipeline that constructs diverse tool-use environments by clustering 30k+ APIs into functional domains, materializing them as executable code with mock database schemas, and generating synthetic agent–human trajectories through simulated interactions. Agents are then fine-tuned in two stages: a general foundation phase and a domain-specific specialization phase.\nEmpirical evaluations on τ-Bench, τ²-Bench, and ACEBench show consistent improvements over Qwen3 baselines, with further cross-lingual tests on ACEBench-zh. The work positions itself as a scalable, verifiable alternative to teacher-model distillation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The environment construction pipeline is genuinely novel. It derives “gold” supervision signals directly from environment logic (database state transitions and tool dependency graphs) rather than relying on stronger teacher models or human annotation. Allowing it to scale nicely\n\n- Representing tools as read/write operations over structured domains is an elegant unifying abstraction that allows programmatic  environment generation.\n\n- The three-stage trajectory filtering process (validity → state alignment → exact match) provides a credible mechanism for ensuring quality and consistency in synthetic data."}, "weaknesses": {"value": "- The paper never explicitly states which model generated the simulated trajectories. It appears that the base model under training were used for generation, but this should be made explicit to rule out hidden distillation.\n\n- Limited notion of “generalization”. The only OOD test (ACEBench-zh) evaluates cross-lingual robustness, not transfer to unseen tool domains or schema structures. Because τ-Bench–derived tools appear in both training and evaluation, true domain generalization remains untested.\n\n- While overall metrics improve, per-domain results fluctuate substantially. For example, in ACEBench-zh the “Special” subset consistently drops across model sizes, and in τ²-Bench some domains improve dramatically while others regress. No analysis is offered to explain these discrepancies.\n\n- The pass^k curves show higher pass^1 accuracy but a faster decline as k increases, ultimately converging with the baseline. This indicates better one-shot precision, not improved stability or consistency across runs.\n\n- The paper offers no insight into why certain domains or subsets improve more than others. Missing are per-domain data distribution analyses, qualitative error cases, or ablations on environment size or filtering stages.\n\n- The environment builder reproduces τ-Bench-like schemas (“high consistency with official implementations”), suggesting potential overlap between training and benchmark environments, which weakens the validity of reported gains."}, "questions": {"value": "- Which model was used as the agent during synthetic experience generation — the same Qwen3 base model or a stronger teacher? It would be helpful clarify explicitly.\n\n- Data–evaluation overlap: How do you ensure that APIs, schemas, or tool sequences from τ-Bench or τ²-Bench are not directly included in the training environments? Do you plan to evaluate on unseen tool domains or unseen function schemas to substantiate claims of “general agentic intelligence”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2Ec0uepc0X", "forum": "q9Jz28sqgd", "replyto": "q9Jz28sqgd", "signatures": ["ICLR.cc/2026/Conference/Submission10286/Reviewer_rX7d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10286/Reviewer_rX7d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812359541, "cdate": 1761812359541, "tmdate": 1762921639319, "mdate": 1762921639319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AgentScaler, a pipeline for developing general agentic intelligence through large-scale simulated tool-use environments and two-stage training (general → domain-specialized). It automatically constructs verified tool-use trajectories from 30K+ APIs, clusters them into domains, and trains models (4B–30B) that outperform comparable open LLMs on τ-Bench, τ²-Bench, and ACEBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a novel and automated approach to transform large-scale APIs into executable and verifiable environments, significantly improving reproducibility and coverage for agentic training.\n  - The two-stage training strategy, which separates general capability building from domain specialization, is well-motivated and empirically validated to enhance performance across benchmarks.\n  - The results demonstrate strong effectiveness, with compact models (e.g., 30B) achieving performance close to proprietary systems, showing that the proposed pipeline is efficient and scalable."}, "weaknesses": {"value": "- The methodological novelty of the paper is relatively limited. The two-stage training strategy (general-to-domain specialization) closely resembles existing agent fine-tuning pipelines such as AgentFlan, ToolAce\n  - The paper does not compare its environment construction approach or data generation pipeline with other existing frameworks, nor does it analyze in detail how environment scaling quantitatively affects model performance."}, "questions": {"value": "- Could the authors provide a more detailed quantitative analysis of how environment scaling influences model performance? For example, how does expanding the number or diversity of simulated environments affect the model’s tool-use accuracy, generalization, or stability across benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XSd8bXTi2K", "forum": "q9Jz28sqgd", "replyto": "q9Jz28sqgd", "signatures": ["ICLR.cc/2026/Conference/Submission10286/Reviewer_HUX4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10286/Reviewer_HUX4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903960232, "cdate": 1761903960232, "tmdate": 1762921638279, "mdate": 1762921638279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors argue that agents need to interact with diverse and scale environments to gain real world function calling abilities. Therefore, the authors build a scaled up environment framework to automatically construct simulated heterogeneous environments for agents to interact with. Then they finetuned agents on trajectories collected from their simulated environments and then evaluated on benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors provided a useful tool of scaling up simulated environments for agent to interact with.\n- The authors performed comprehensive experiments by training on models of different sizes and evaluated on multiple benchmarks.\n- The authors compared their models with various baselines."}, "weaknesses": {"value": "- Seed-OSS-36B (which has similar model size with AgentScaler-30B-A3B) achieves higher performance on ACEBench-en (both normal and overall). Yet the authors still claimed that they achieved state of the art performance on all three benchmarks they evaluated on.\n- In Table 2, it appears that after training, the performance of AgentScaler-4B dropped 15.3 on special, AgentScaler-8B dropped 5.1 on normal, AgentScaler-30B-A3B dropped 3.4 on special, yet the authors didn't explain why this might be happening. However, the authors still claimed that their trained model, AgentScaler, significantly enhances the models' function-calling capability.\n- With the abundant existing environments that agents could interact with, the authors didn't make it clear why simulated environments are useful."}, "questions": {"value": "- Why does performance dropped after training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MKT3nsUynx", "forum": "q9Jz28sqgd", "replyto": "q9Jz28sqgd", "signatures": ["ICLR.cc/2026/Conference/Submission10286/Reviewer_t5Y1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10286/Reviewer_t5Y1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968527221, "cdate": 1761968527221, "tmdate": 1762921637619, "mdate": 1762921637619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}