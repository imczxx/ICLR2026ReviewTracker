{"id": "zHuV3Vatov", "number": 9987, "cdate": 1758154642662, "mdate": 1763767205192, "content": {"title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks", "abstract": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but they cannot reliably verify their own logic. Even when they reach correct answers, the underlying reasoning may be flawed, undermining trust in high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a neuro-symbolic method that extracts and verifies formal logical arguments from CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order logic and identifies premises that ground the argument in source context, commonsense knowledge, or prior reasoning steps. The symbolic representation enables automated solvers to verify logical validity while the NL premises allow humans and systems to identify ungrounded or fallacious reasoning steps. Experiments on the ProofWriter, LegalBench, and BioASQ\ndatasets show VeriCoT effectively identifies flawed reasoning, and serves as a strong predictor of final answer correctness. We also leverage VeriCoT’s verification signal for (1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct preference optimization (DPO) using verification-based pairwise rewards, further improving reasoning validity and accuracy.", "tldr": "", "keywords": ["Neuro-symbolic methods", "Large Language Models", "Chain-of-Thought", "Reasoning verification", "Formal logic"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9a76c9e6a61a2c8396cc042d4a6cf3966807812a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces VERICOT, a neuro-symbolic method designed to verify reasoning trace of LLMs. \nThe methodology consists of two main components: 1. Autoformalization – translating each natural-language CoT step into a precise formal logic representation and 2. Verification – using automated constraint solvers to check whether the formal statement is logically consistent and entailed by explicitly identified premises. VERICOT demonstrates effectiveness across diverse tasks (ProofWriter, LegalBench, BioASQ), reliably detecting logical errors. Also, The authors also apply this methodology in two use cases—real-time self-reflection and constructing high-fidelity datasets—to actively improve LLM reasoning quality and task performance."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper deals with relatively unfamiliar concepts such as neuro-symbolic reasoning and first-order logic, yet it is written in a way that is easy for readers to follow. The authors also provide appropriate examples that effectively aid understanding.\n\n2. The overall approach of the paper is well thought out. Decomposing the CoT into atomic units, expressing them in a formal language, and verifying them is a promising direction for improving the interpretability of reasoning.\n\n3. The categorization presented in the paper is clear and logically sound.\n\n4. The two-step problem-solving process—formalization and premise generation—is well designed, and the authors effectively demonstrate the usefulness of their method through its applications in real-time inference and dataset generation.\n\n5. The paper makes appropriate use of the LLM-as-a-judge, relying on it only partially rather than excessively, and successfully leveraging its advantages."}, "weaknesses": {"value": "1. Questions about the metrics presented in Table 1\n\n1-1. I have several questions regarding the metrics used in the experimental results shown in Table 1.\nThe Pass Rate measures whether a CoT trace is verifiable. Did each baseline method (ER or DSB) apply its own verification method, different from VeriCOT’s?\nIf so, I’m not sure a higher Pass Rate necessarily indicates a better verification method. Some verification methods can be very strict and reject even minor leaps in reasoning, while others may be more lenient and mark more traces as “pass.”\nWouldn’t it be better to apply a stricter verification method, along with the self-reflection mechanism shown in Section 3.4, to produce higher-quality CoTs?\n\n1-2. Related to the above, as I understand it, VCAR represents the proportion of test cases that satisfy both Pass Rate and Precision. If Pass Rate is high, then VCAR will naturally appear favorable. However, as mentioned in 1-1, if a high Pass Rate doesn’t necessarily mean a better verification process, doesn’t that also undermine the interpretive value of the VCAR metric?\n\n1-3. In my opinion, among all the metrics for CoT verification, Precision is the most important.\nThis is because we examine LLMs’ CoTs under the expectation that if their reasoning is logical and sound (even when we don’t know the ground truth), their final answer is likely correct.\nTherefore, Precision—the proportion of truly correct CoTs among those verified by a given method—seems the most meaningful and task-relevant metric.\nHowever, according to Table 1, VeriCOT does not achieve higher Precision than the baselines, and in the case of ProofWriter, its Precision is even lower than VeriCOT-NoPrem. The authors should provide a justification for this.\n\n1-4. What is the reason for the differences in Task Accuracy among methods? Were all methods evaluated on the same CoT samples?\n\n2. Since the proposed approach verifies CoTs through multiple rounds of LLM inference, it would be helpful to report the average verification cost per CoT (e.g., number of LLM calls, computation cost, etc.).\n\n3. Premise generation and premise evaluation—two core components of the proposed method—both rely on LLMs. Consequently, performance may vary greatly depending on which LLM is used. However, the experiments were conducted solely with Claude 3.5 Sonnet.\nIt seems essential to analyze how performance changes across LLMs with different capabilities.\nIf results vary by model, the authors should explain why; if they do not, an explanation for the stability would also be valuable.\n\n4. It would be helpful to include qualitative examples of the Commonsense type premises among the three types of NL Premises.\nThe classification in Table 2—Grounded contextual premise, Acceptable commonsense premise, and Necessary commonsense premise—is somewhat difficult to evaluate in terms of reliability.\nWhat exactly distinguishes something “acceptable” from “necessary”?\nA clearer analysis of what counts as a commonsense premise (and what does not) would strengthen the paper.\nSince the proposed method’s Pass Rate likely depends heavily on how well the model generates commonsense premises, this aspect deserves more attention.\n\n5. It seems that this paper assumes the CoT is already divided into individual steps, but an explanation of how the CoT steps were segmented would be necessary.\n\n\nOverall, the paper’s motivation, proposed solution, structure, and writing quality are excellent. However, I believe the experimental results are still insufficient to convincingly demonstrate the practical usefulness of the proposed method."}, "questions": {"value": "The main questions are in the Weaknesses section.\n\nAdditionally, (This is not a drawback, but just a question:)\nCould this methodology also be applied to reasoning traces produced by reasoning-scaled models trained with reinforcement learning (RL), rather than CoTs generated purely through prompting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ByJQGDzDBz", "forum": "zHuV3Vatov", "replyto": "zHuV3Vatov", "signatures": ["ICLR.cc/2026/Conference/Submission9987/Reviewer_37xu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9987/Reviewer_37xu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381282641, "cdate": 1761381282641, "tmdate": 1762921419913, "mdate": 1762921419913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper augments CoT with generating verifiable first-order logic expressions, which can be used by a constraint solver, to verify the chains of thought being generated by LLMs. The paper spells out a careful procedure to do this (Section 2), which consists of generating a formal logical expression from each COT step ($C_i \\rightarrow F_i$), validating the formal expression, then generating premises to support the formal expression ($F_i \\rightarrow P_i$). \n\nThis procedure is especially nice in characterizing the unreliable CoT steps (ex: contradictions, ungrounded reasoning, etc.). \n\nThe authors then demonstrate the benefits of their procedure on a few benchmarks that rely on reasoning (ProofWriter, LegalBench, BioASQ). Augmenting VeriCOT does improve the reliability of the model's CoT steps (as measured by precision, pass rate), but does not seem to affect the end task accuracy much. \n\nThe authors study the effect of fine-tuning LLMs with VeriCOT. While simply applying SFT does not seem to make much of an effect, applying DPO (to favor correct VeriCOT steps vs. incorrect VeriCOT steps) does improve task accuracy, suggesting a better post-training recipe for domains that require multiple logical reasoning steps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper provides an intuitive procedure to validate the reliability of CoT sequences. In particular, looking at the VeriCOT failure cases is a good way of uncovering common \"fallacies\" of model CoTs (e.g., contradictions, ungrounded reasoning). \n\nOverall, the paper makes a nice coherent, and well-scoped contribution to test the reliability of LLM's CoT steps."}, "weaknesses": {"value": "From the brief description of the baseline methods being compared to, it sounds like VeriCOT isn't the first to incorporate FOL (or similar expressions that is conducive to do verification along the way) to validate and improve CoT. It would be helpful to explicitly spell out how VeriCOT differs from these other approaches.\n\nShould there be another baseline where the LLM-judge just verifies each natural language reasoning step (without the FOL counterparts) to isolate the effect of FOL steps? \n\nWhile VeriCOT seems to improve on precision and pass rate, it doesn't seem to affect task accuracy that much. Does this suggest that it's not so much the correctness of the CoT that matters, but instead something else like generation diversity? What are your thoughts on why this is? \n\nVeriCOT relies heavily on a LLM to correctly judge the generated premises as valid or invalid. How reliable are Claude's judgements for this?"}, "questions": {"value": "What is the difference between updating $P_i$ with $P_i = P_{i-1}$ vs. $P_i = P_{i-1} \\cup \\{P_i\\}$?\n\nIn Table 4 - it doesn't seem like SFT-ing with VeriCOT makes much of a difference, unless DPO is applied. Any thoughts as to why?\n\nHow does VCAR combine precision and pass rate? It seems a bit odd to combine the two given that the two measures seem to have very different distributions. \n\nWhy do you think precision drops after self-reflection for ProofWriter?\n\nWhat results do you expect to see when VeriCOT is applied to \"unfaithful\" CoT, in which models justify their answers \"post-hoc\"? (https://arxiv.org/abs/2305.04388)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z0hGuUI6hu", "forum": "zHuV3Vatov", "replyto": "zHuV3Vatov", "signatures": ["ICLR.cc/2026/Conference/Submission9987/Reviewer_MriA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9987/Reviewer_MriA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601418696, "cdate": 1761601418696, "tmdate": 1762921419408, "mdate": 1762921419408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes a system that provides a multi-step process for transforming a Chain of Thought into a formal argument, with the goals of grounding and validating individual steps in the CoT and identifying logical errors in the CoT. This process answers three questions: can the CoT admit of auto-formalization, are the steps in the CoT supported, and what premisses does the CoT rely upon. This approach accomplishes two objectives: inference-time checking of the correctness of a CoT, and providing examples of CoTs that are verified to be correct for use in the fine-tuning of a reasoning model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The core innovation lies in combining LLM-based autoformalization with SMT solver verification using Z3, which provides formal guarantees about logical consistency within the formalized representation. This approach offers more rigor than purely LLM-based verification methods, while remaining more flexible than pure formal methods that require pre-specified logical frameworks.\n\nI see an analogy with the informal process of proof checking of a human-generated proof in informal and formal reasoning. The idea of using this to validate CoTs, and then using the validated CoT through fine tuning in a self-improving process is interesting. This concept\ndemonstrates practical versatility, and suggests a path toward increasingly reliable reasoning systems."}, "weaknesses": {"value": "A fundamental concern is the circular dependency created by using LLMs for both autoformalization and premise generation, which are then employed to verify LLM outputs. This architectural decision introduces potential for systematic biases where the verifier may share failure modes with the system being verified. While the paper acknowledges this limitation, it doesn't adequately explore the implications or propose mitigation strategies. The system essentially asks LLMs to judge their own reasoning through a different lens, which may not catch errors that stem from fundamental misunderstandings encoded in the model family.\n\nThe presence of an \"untranslatable\" error category reveals a significant limitation in the approach's scope. Certain reasoning steps fundamentally cannot be represented in the supported first-order logic fragment, as illustrated by the LegalBench example where probabilistic statements like \"might be jurisdictional implications\" cannot be encoded. The paper reports these untranslatable rates but offers no strategies for handling such cases beyond marking them as failures, which limits the system's applicability to real-world scenarios where uncertainty or non-classical reasoning may be needed.\n\nThe reliance on LLM-as-judge evaluation for premise quality assessment introduces another layer of potential error and bias. While Table 2 reports high acceptance rates between 77% and 93%, these metrics don't guarantee that premises are actually correct or complete. The circular dependency problem resurfaces here, as LLMs evaluate premises generated by LLMs, potentially missing systematic errors or hallucinations."}, "questions": {"value": "What specific technical advances would be required to achieve verification rates sufficient for practical use? Would a human-in-the-loop approach for handling untranslatable cases be viable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IkU4mSJ9iQ", "forum": "zHuV3Vatov", "replyto": "zHuV3Vatov", "signatures": ["ICLR.cc/2026/Conference/Submission9987/Reviewer_D7eV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9987/Reviewer_D7eV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710750219, "cdate": 1761710750219, "tmdate": 1762921418306, "mdate": 1762921418306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VERICOT, a neuro-symbolic framework to validate the logical consistency of Chain-of-Thought (CoT) reasoning. It autoformalizes each CoT step into First-Order Logic (FOL) and uses an SMT solver (Z3) for verification. The authors claim their core novelty is a \"premise generation\" step, where an LLM infers and formalizes implicit commonsense or contextual premises to complete logical gaps. The resulting verification signal is then used for inference-time self-reflection, supervised fine-tuning (SFT), and direct preference optimization (DPO)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper tries to address the problem of verifying the step-by-step logical integrity of CoT, not just the final answer. The mechanism for generating implicit premises to fill logical gaps is multi-step beyond simple auto formalization. Verification, when it passes, is a high-precision signal, filtering incorrect reasoning and improving reliability over the base model. The authors demonstrate the utility for model improvement, using the verification signal for self-reflection and as a high-quality filter for SFT/DPO data. The error analysis (Ungrounded, Contradiction, Untranslatable..) is insightful and provides actionable feedback for the LLM self-reflection loop."}, "weaknesses": {"value": "1. The system \"verifies\" an LLM-generated CoT against LLM-generated premises. This is an internal consistency check, not a sound grounding check. There is lots of literature about LLMs preferring their own outputs.  A human evaluation or ground truth is required to measure the error rate of the autoformalizer and the premise generator to prove the system isn't just \"consistently wrong.\"\n2. The low pass rate on LegalBench-SARA is concerning. Qualitative error analysis is needed to explain this failure.\n3. Writing could be improved for clarity. For example, Score in Tab 2 is not defined anywhere in the main text of the paper. Formatting errors are present, such as \\approx in Line 425."}, "questions": {"value": "To me, the core drawback to be fixed is that the paper must include a direct comparison against other neurosymbolic SMT-based CoT verifiers cited in the related work, namely SAT-LM (Ye et al., 2023), Logic-LM (Pan et al., 2023), Proof of Thought (Ganguly et al., 2024; 2025) and Logic-LM++ (Kirtania et al., 2024). On cursory browsing, current leaderboards for ProofWriter with Proof of Thought claim higher verified performance with GPT-5 than reported in your paper, and the SMT-LIB generation traces seem very similar. Without comparing against these techniques, the novelty and performance of VERICOT are unverified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EjpkTo8ikG", "forum": "zHuV3Vatov", "replyto": "zHuV3Vatov", "signatures": ["ICLR.cc/2026/Conference/Submission9987/Reviewer_C6Nf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9987/Reviewer_C6Nf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957746583, "cdate": 1761957746583, "tmdate": 1762921417843, "mdate": 1762921417843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}