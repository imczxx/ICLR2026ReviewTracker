{"id": "4AEivvf9uv", "number": 2490, "cdate": 1757122890032, "mdate": 1759898145025, "content": {"title": "Transfer between Modalities with MetaQueries", "abstract": "Unified multimodal models aim to integrate understanding (text output) and generation (pixel output), but aligning these different modalities within a single architecture often demands complex training recipes and careful data balancing. We introduce MetaQueries, a set of learnable queries that act as an efficient interface between autoregressive multimodal LLMs (MLLMs) and diffusion models. MetaQueries connects the MLLM's latents to the diffusion decoder, enabling knowledge-augmented image generation by leveraging the MLLM's deep understanding and reasoning capabilities. Our method simplifies training, requiring only paired image-caption data and standard diffusion objectives. Notably, this transfer is effective even when the MLLM backbone remains frozen, thereby preserving its state-of-the-art multimodal understanding capabilities while achieving strong generative performance. Additionally, our method is flexible and can be easily instruction-tuned for advanced applications such as image editing and subject-driven generation.", "tldr": "Introducing MetaQuery, a minimal recipe for building state-of-the-art unified multimodal understanding (text output) and generation (pixel output) models", "keywords": ["Unified Multimodal Understanding and Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ed3644e169be9c9603fd6e341777f869a88d2a7.pdf", "supplementary_material": "/attachment/86cd2343505ed2380c9de15aa50fc2dd0dd95664.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents an interesting approach that freezes the LLM backbone solely for understanding tasks, while delegating generation tasks to image-generative models. This design simplifies data balancing and removes the need for complex training procedures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper introduces MetaQuery, a framework that leverages the capabilities of frozen MLLMs.\n\n2.The proposed method achieves state-of-the-art performance in both multimodal understanding and image generation tasks.\n\n3.The approach also shows potential for extension to other image-related applications, such as image editing, through appropriate fine-tuning."}, "weaknesses": {"value": "1. The concept does not appear entirely novel, as similar ideas have been explored in prior works such as Seed-X, MetaMorph, and Next-GPT. Could the authors clarify the main differences between their approach and these methods? \n\n2. It would strengthen the paper if the authors could include additional experiments on subtasks that more directly assess the benefits of a unified model, such as interleaved image-text generation or other multimodal interaction tasks."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U2jBrd8vmd", "forum": "4AEivvf9uv", "replyto": "4AEivvf9uv", "signatures": ["ICLR.cc/2026/Conference/Submission2490/Reviewer_hJkF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2490/Reviewer_hJkF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620008720, "cdate": 1761620008720, "tmdate": 1762916254787, "mdate": 1762916254787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of creating unified multimodal models that excel at both understanding (text output) and generation (pixel output) without performance degradation. The authors propose **MetaQueries**, a set of learnable tokens that act as an efficient interface between a **frozen** autoregressive multimodal LLM (MLLM) and a diffusion model."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea of MetaQueries as a \"plug-and-play\" interface is a significant strength, promoting modularity in a field dominated by monolithic models.\n2. The paper's most impressive contribution is the *proof* (via Figure 7 and Table 4) that MetaQueries are not just a simple conditioning mechanism but a functional bridge for transferring high-level MLLM capabilities (world knowledge, reasoning) to the generation process. SOTA on the WISE benchmark confirms this.\n3. The \"Tune vs. Freeze MLLM\" and \"MetaQueries vs. Last-Layer Embedding\" ablations are critical, well-executed, and strongly support the paper's central claims."}, "weaknesses": {"value": "1. The paper honestly notes (Section 5.1) that its method lags behind autoregressive (AR) visual-token models like Janus-Pro on prompt-alignment benchmarks (GenEval, DPG). While the authors provide a qualitative defense (better visual quality, Appendix E), this remains a quantitative gap. It would be beneficial to discuss if this is a fundamental limitation of the diffusion-decoder approach or if it could be closed with more data/tuning.\n2. In Section 3.1, the paper states, \"we continue to use causal masking for the entire sequence\". This is slightly ambiguous. A clearer explanation of the exact attention mask between the `[Multimodal Input]` and `[MetaQueries]` would be helpful. For instance, do the queries attend to the input, but the input cannot attend to the queries?\n3. The trainable \"connector\" is a key component, but it's not deeply analyzed. Appendix A.2 explores two designs, but the impact of connector depth/complexity on performance vs. efficiency is not fully explored. How much of the \"alignment\" is handled by the queries versus this 24-layer transformer?"}, "questions": {"value": "1. Following up on Weakness #1: Do you believe the prompt-alignment gap (vs. AR models like Janus-Pro) is a fundamental trade-off for the superior visual quality of diffusion, or could this gap be closed, perhaps by scaling the 25M image-caption pre-training data or further tuning the connector?\n2. Following up on Weakness #2: Could you please clarify the exact attention mechanism? Given a sequence `[Input Tokens, MetaQuery Tokens]`, what is the attention mask? Is it a standard causal mask over the entire concatenated sequence?\n3. In Table 2, the \"Freeze MLLM\" setting achieves a *better* (lower) FID score than \"Tune MLLM\" (e.g., 6.06 vs 6.28 when training DiT). This is counter-intuitive, as one might expect tuning to help. Do you have a hypothesis for why *not* tuning the MLLM leads to slightly better visual quality?\n4. The instruction-tuning data pipeline (Section 4) is very clever. How sensitive is the model's performance to the MLLM-generated instruction? For example, did you find that variations in the system prompt (Appendix B) led to significant differences in the model's final editing/subject-driven capabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "voQXCivppT", "forum": "4AEivvf9uv", "replyto": "4AEivvf9uv", "signatures": ["ICLR.cc/2026/Conference/Submission2490/Reviewer_7rvH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2490/Reviewer_7rvH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659306175, "cdate": 1761659306175, "tmdate": 1762916254587, "mdate": 1762916254587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed to unify multimodal understanding and generation by combining bespoke pretrained models. Compared to existing unified LLMs, this work adopted a trainable connector to act as a bridge between pre-trained MLLMs and the diffusion decoder. Learnable queries as input to fuse vision-language information from the pre-trained MLLMs as input conditions for diffusion decoding. Extensive experimental results demonstrated the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized, and the figures are well-prepared.\n2. This approach achieves state-of-the-art results on the existing multimodal understanding and generation benchmarks.\n3. The reasoning and knowledge-augmented generation looks interesting."}, "weaknesses": {"value": "1. My biggest concern is the size of the connector, which is extremely large when adopting Qwen2.5-VL 3B and 7B as the base MLLMs. The connector size may be larger than the diffusion decoder, which makes the claim that this transfer is effective even when the MLLM backbone remains frozen less convincing.  \n\n2. Given the above point, the paradigm of MetaMorph may be more effective. This raises another concern: what if we take the MLLM's output tokens as direct input for the connector? Would it be more effective, or would there be fewer learnable parameters required to make this transfer? Additional experimental results are required."}, "questions": {"value": "See weaknesses. I will adjust the rating according to the authors' response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UFRthY68O4", "forum": "4AEivvf9uv", "replyto": "4AEivvf9uv", "signatures": ["ICLR.cc/2026/Conference/Submission2490/Reviewer_NM2y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2490/Reviewer_NM2y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734330896, "cdate": 1761734330896, "tmdate": 1762916254293, "mdate": 1762916254293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose MetaQueries, a simple and light module that bridges the frozen LLM and a diffusion model, to boost the multimodal understanding and generation. MetaQueries is a series of learnable tokens that fed into the LLM to learn latent conditions to align to the conditional space of diffusion models. While the authors conduct experiments over multiple benchmarks, I think the contribution mostly comes from the engineering, rather than the technical novelty."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- text is easy to follow\n- method is simple and efficient, by inserting learnable tokens, the training process only needs to fine-tune diffusion model."}, "weaknesses": {"value": "- limited novelty: the main component, MetaQueries, is essentially a form of learnable prompts / queries, similar to prior adapters such as Q-Former\n- lack of theoretical analysis: no deeper analysis on why or when frozen MLLM features can serve as effective generative conditions, nor exploration of failure cases or transfer limitations"}, "questions": {"value": "please refer to the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g6dixDYfDI", "forum": "4AEivvf9uv", "replyto": "4AEivvf9uv", "signatures": ["ICLR.cc/2026/Conference/Submission2490/Reviewer_Z3r9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2490/Reviewer_Z3r9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762063498614, "cdate": 1762063498614, "tmdate": 1762916254060, "mdate": 1762916254060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}