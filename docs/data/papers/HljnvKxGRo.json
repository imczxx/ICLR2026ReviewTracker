{"id": "HljnvKxGRo", "number": 1960, "cdate": 1756971839299, "mdate": 1759898176526, "content": {"title": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers", "abstract": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional capabilities in visual synthesis, yet their deployment remains constrained by substantial computational demands. To alleviate this bottleneck, many sparsity‑based acceleration methods have been proposed. However, their diverse sparsity patterns often require customized kernels for high-performance inference, limiting universality. We propose **FlashOmni**, a unified sparse attention engine compatible with arbitrary DiT architectures. FlashOmni introduces flexible *sparse symbols* to standardize the representation of a wide range of sparsity strategies, such as feature caching and block‑sparse skipping. This unified abstraction enables the execution of diverse sparse computations within a single *attention kernel*.  In addition, FlashOmni designs optimized *sparse GEMMs* for attention blocks, leveraging sparse symbols to eliminate redundant computations and further improve efficiency.  Experiments demonstrate that FlashOmni delivers near‑linear, closely matching the sparsity ratio speedup  (1:1) in attention and GEMM‑*Q*, and achieves 2.5x–3.8x acceleration in GEMM‑*O* (87.5% of the theoretical limit). Applied with a multi‑granularity sparsity strategy, it enables the HunyuanVideo (33K) to achieve about 1.5x end‑to‑end acceleration without degrading visual quality.  FlashOmni is available at https://anonymous.4open.science/r/FlashOmni-B980.", "tldr": "A Unified Sparse Attention Engine for Diffusion Transformers", "keywords": ["sparse attention", "inference acceleration", "efficient kernels"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1dc6ef9c765c95fdf29ca511a2c146613a6b0ab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes FlashOmni, a sparse attention for Diffusion Transformers (DiTs). It introduces sparse symbols, which are a unified abstraction that represents multiple sparsity strategies in a general sparse implementation. FlashOmni also proposes some methods to eliminate redundant computations. Experiments across text-to-image and text-to-video generation tasks demonstrate a high attention acceleration and up to 1.5× end-to-end speedup on large models without loss of visual quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The sparse symbols proposed in the paper are reasonable.\n\nThe paper provides an engineering contribution, including detailed kernel-level designs.\n\nAlthough algorithmically limited, the proposed framework may serve as a useful system integration."}, "weaknesses": {"value": "- **Motivation not well justified.** The paper claims that the current sparse attention design space is fragmented and prevents reuse across different applications. However, this claim is not convincing. Both static and dynamic sparsity methods are application-driven. Different methods are developed to address distinct modeling challenges rather than due to a lack of reusability. In practice, there already exist unified and highly usable APIs (e.g., *SpargeAttn*, *FlexAttn*) that support a customized sparse FlashAttention API, easily supporting both dynamic and static sparse attention implementation.\n\n- **Limited originality of the proposed sparse symbols.** The introduced sparse symbols ($S_c$) and ($S_s$) are essentially a *mask encoding trick* rather than a new method. Representing sparse masks with 8-bit values only compresses storage by at most 8×, which is a naive bit-packing implementation. Compared with already widely used lookup-table (LUT) (such as in VSA or SpargeAttn), this approach provides negligible memory savings and no algorithmic novelty.\n\n- **Implementation-level nature of GEMM-Q and GEMM-O.** The proposed GEMM-Q and GEMM-O kernels are primarily caching tricks. They do not fundamentally improve the theoretical efficiency of sparse attention. Their contribution is mostly engineering-level rather than algorithmic.\n\n- **No complete end-to-end speedup experiments.** There is no complete end-to-end model speedup comparison experiments. Additionally, Figures 6 and 8 use the term *inference performance*, which is not clear. The paper should clarify that these are attention speed."}, "questions": {"value": "**Questions about experimental results.** The reported performance of FlashAttention on A100 GPUs (93 TOPS) appears significantly lower than expected, as prior works and open benchmarks report values in the range of 140–190 TOPS. Similarly, *SpargeAttn*, which is based on *SageAttention*, should achieve well above 250 TOPS even without sparsity on HunyuanVideo. There may be possible benchmarking or configuration issues that need clarification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AMIfAqNP3J", "forum": "HljnvKxGRo", "replyto": "HljnvKxGRo", "signatures": ["ICLR.cc/2026/Conference/Submission1960/Reviewer_85zg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1960/Reviewer_85zg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694479503, "cdate": 1761694479503, "tmdate": 1762915974977, "mdate": 1762915974977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **FlashOmni**, a unified sparse attention engine aiming to overcome the fragmentation of existing sparse acceleration methods for Diffusion Transformers (DiTs) by integrating them into a single framework. The method introduces unified sparse symbols and optimized kernels to support diverse sparsity strategies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Clarity and Visual Appeal**: The paper's figures and main body are well-presented and contribute to clarity in explaining the overall system design."}, "weaknesses": {"value": "- **Incremental Novelty**: The primary weakness is the limited conceptual novelty. The paper's sparsity-finding logic is not new; it explicitly adopts TaylorSeer [1] for its feature caching strategy and aligns its block-skipping approach with SpargeAttention [2]. The contribution is therefore the combination of these two existing methods, which feels incremental.\n\n- **Contribution Feels Limited to Implementation**: The main novelty—the unified kernel and sparse symbols—can be viewed as a clever but small, implementation-level engineering optimization rather than a new scientific algorithm. It is not clear that this systems-level integration, by itself, meets the high bar for ICLR. \n\n- **Incremental Performance Gains and Unfair Comparison**: The reported end-to-end speedups (e.g., \"~ 1.5x end-to-end acceleration\" on Hunyuan) are positive but not transformative. While Table 1 and Table 2 show that FlashOmni achieves better quality metrics (like PSNR/LPIPS) than the individual baselines (SpargeAttn [2], TaylorSeer [1]), the gain from this complex \"unification\" is not clearly isolated. Thus the comparison is unfair. \n\n[1] Liu, J., Zou, C., Lyu, Y., Chen, J., & Zhang, L. From reusing to forecasting: Accelerating diffusion models with taylorseers. In ICCV 2025.\n\n[2] Zhang, J., Xiang, C., Huang, H., Xi, H., Zhu, J., & Chen, J. SpargeAttention: Accurate and Training-free Sparse Attention Accelerating Any Model Inference. In ICML 2025."}, "questions": {"value": "- **Missing Baseline Data in Table 1 2**: In Table 1 2, the \"Full-Attention\" baseline for the \"FLUX.1 [dev]: 50 steps\" configuration is missing all quality metrics. The cells for PSNR, LPIPS, SSIM, CLIP-IQA, and FID are all blank. This is the most critical baseline for that experiment. Without these numbers, it is impossible to assess the quality degradation or preservation of any of the sparse methods, including the author's own. Can the authors please provide these crucial baseline numbers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XZ7mYqtcZr", "forum": "HljnvKxGRo", "replyto": "HljnvKxGRo", "signatures": ["ICLR.cc/2026/Conference/Submission1960/Reviewer_pugu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1960/Reviewer_pugu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714481223, "cdate": 1761714481223, "tmdate": 1762915974792, "mdate": 1762915974792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FlashOmni, a unified sparse attention engine designed to accelerate DiTs. FlashOmni introduces flexible sparse symbols to standardize diverse sparsity strategies—including feature caching and block-sparse skipping—within a single unified framework. The paper also proposes optimized sparse GEMMs (General Matrix Multiplications) for attention computation, effectively reducing redundant operations. Experimental results show that FlashOmni maintains visual quality and achieves ∼1.5× end-to-end acceleration on the Hunyuan model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-structured and clearly presents its motivation, design, and experiments.\n\n- The topic is highly relevant, especially given the growing importance of efficient computation for large-scale video generation models.\n\n- The idea of treating feature caching as a new dimension of sparsity is novel and elegant, providing a natural way to unify sparse attention and caching under one abstraction.\n\n- The authors conduct comprehensive evaluations, covering both performance metrics (speedup and efficiency) and qualitative results (visual quality), which support the practicality of the proposed approach."}, "weaknesses": {"value": "- The paper could be improved by explicitly presenting absolute end-to-end performance metrics (e.g., inference latency, total speedup) alongside visual quality metrics in a single table. This would help readers better understand the efficiency–quality tradeoff.\n\n- (Minor) The writing and exposition could be refined to make the core idea—especially the integration of feature caching into the sparsity framework—easier to follow for readers who are not already familiar with these mechanisms and system kernel implementation."}, "questions": {"value": "- Could the authors provide absolute end-to-end latency or throughput numbers, in addition to relative speedups?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1auYHAF0oX", "forum": "HljnvKxGRo", "replyto": "HljnvKxGRo", "signatures": ["ICLR.cc/2026/Conference/Submission1960/Reviewer_dxME"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1960/Reviewer_dxME"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945067293, "cdate": 1761945067293, "tmdate": 1762915974306, "mdate": 1762915974306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "FlashOmni presents a unified sparse attention engine designed to accelerate DiTs through combining feature caching and block-sparse skipping strategies. The paper introduces flexible 8-bit sparse symbols to standardize representation of diverse sparsity patterns, implements optimized sparse attention kernels and GEMMs, and demonstrates 1.5x end-to-end speedup on HunyuanVideo with 2-5x acceleration in individual components. The framework adopts an \"Update-Dispatch\" paradigm where sparse symbols are refreshed at update steps and used to guide efficient sparse computation across subsequent timesteps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Unified abstraction for multi-granularity sparsity: The sparse symbol design elegantly unifies feature caching and block-sparse skipping within a single framework, addressing fragmentation in existing approaches.\n2. Strong empirical performance: Achieves near-linear speedup and maintains superior visual quality compared to baselines across multiple metrics.\n3. Practical kernel implementation: The general sparse attention kernel that decodes symbols at runtime eliminates the need for task-specific kernel implementations, improving engineering efficiency."}, "weaknesses": {"value": "1. Incomplete generalization analysis: The paper claims FlashOmni is \"applicable to any Diffusion Transformers,\" yet experiments only cover MMDiT architectures (FLUX, HunyuanVideo). \n\n2. May lack comparison with concurrent methods: The baseline selection omits important recent work in DiT acceleration. Where is comparison with methods like DeepCache layer-wise caching variants, or other 2024-2025 methods? The related work section mentions FORA but doesn't include it in all experimental tables.\n\n3. Insufficient analysis of quality-speed trade-offs: While Table 1 shows various configurations, there's no systematic Pareto frontier analysis. How should practitioners choose between configurations? The jump from 28% to 46% sparsity (FLUX experiments) lacks intermediate points. What is the optimal operating point for different use cases?\n\n4. GEMM-O speedup gap inadequately addressed: The 87.5% theoretical speedup achievement for GEMM-O (vs. near-100% for GEMM-Q/attention) is mentioned but not deeply analyzed. The explanation about \"multiple decoding operations along reduction axis\" may be too brief or vague. Specifically: (1) Can this gap be closed with better decoding strategies? (2) What is the computational breakdown, how much overhead comes from decoding vs. irregular memory access? (3) At what sparsity level does this become the bottleneck?\n\n5. Limited architectural diversity in experiments: All tested models share similar architectural properties (transformer-based, similar attention patterns). How does FlashOmni perform on: (1) models with vastly different sequence lengths? (2) architectures with cross-attention vs. self-attention only? (3) different token gathering strategies (the paper uses mean pooling with n=2, but what about other values or methods)?"}, "questions": {"value": "Please address the implicitly embedded questions in the weakness.\n\nWhat is the trade-off curve between symbol size, decoding overhead, and sparsity granularity? Could adaptive bit-width based on sequence length or model size improve efficiency?\n\nHow did you determine the update step frequency N in your experiments? \n\nWhat is the memory overhead of storing sparse symbols, cache bias, and intermediate features compared to baselines?\n\nHow robust is FlashOmni to suboptimal hyperparameter choices? Can you provide heuristics or automated methods for hyperparameter selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LJEd9bLC58", "forum": "HljnvKxGRo", "replyto": "HljnvKxGRo", "signatures": ["ICLR.cc/2026/Conference/Submission1960/Reviewer_eW6A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1960/Reviewer_eW6A"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983619255, "cdate": 1761983619255, "tmdate": 1762915974043, "mdate": 1762915974043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "**Common Revision Plan**\n\n--------------------\nWe would like to thank the reviewers for their constructive comments and will make the following changes. \n\n**[P1: End-to-End inference performance comparison]**\nWe will add End-to-End performance comparison to subsection 4.3 (Efficiency Evaluation)., which will include an end-to-end  inference performance comparison on HunyuanVideo(33K) A100. (shown in detailed response to reviewers-85zg & dxME).\n\n**[P2: Overhead Analysis]**\nWe will add a new Overhead Analysis sub-subsection into Section 4.3 (Efficiency Evaluation). This addition will serve as a supplement to the original performance comparison, providing a more comprehensive view of efficiency. The analysis will cover three key aspects: (1) memory overhead, (2) pre-processing overhead, and (3) kernel performance (shown in detailed response to reviewers-eW6A & dxME & 85zg). \n\n\n**[P3: Ablation Study]**\nWe will add a new Scaling Analysis and Sparsity ablation into Section 4.4(Ablation Study). (shown in detailed response to reviewer-eW6A).\n\n**[P4: Refine the writing and exposition]**\nWe will make a revision for the writing, especially part about integration of feature caching into the sparsity framework. Make the exposition and  the main contribution more clear for readers.  \n\n\n**[P5:Typos, caption of Table and Figures]**\nWe will thoroughly review the confusion-related comments raised by the reviewers, improving any unclear captions accordingly and correcting those typos."}}, "id": "N6Vdntx4Hq", "forum": "HljnvKxGRo", "replyto": "HljnvKxGRo", "signatures": ["ICLR.cc/2026/Conference/Submission1960/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1960/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission1960/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763594402005, "cdate": 1763594402005, "tmdate": 1763594402005, "mdate": 1763594402005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "1. Fix TOPS 2. Compare speed on more GPUs (RTX4090, 5090, H100) 3. Compare with tuning-free methods"}, "comment": {"value": "First, from the perspective of speed experiments, I have two suggestions:\n\n1. **Correction of TOPS in the paper**\n\n   The TOPS reported in the original version of your paper is not using the standard TOPS definition and should be fixed.\n\n2. **Speed comparison on more GPUs**\n\n   For a method that aims to be generally useful, it is important to demonstrate speedups on different GPUs, not only on A100. Moreover, in practice, diffusion models are often deployed on GPUs used for inference, such as RTX 4090 and RTX 5090. Therefore, the paper should also report speed comparisons on **RTX 4090**, **RTX 5090**, **H100**, etc. By the way, for SpargeAttn, you can directly benchmark using the `spas_sage2_attn_meansim_topk_cuda` API.\n\nAdditionally, some diffusion models do not use `headdim=128`, attention speed comparison on `headdim=64` is also needed.\n\n**Lastly,** I also suggest comparing video quality with some tuning-free methods, such as the block-sparse variant of MInference and the tuning-free version of SpargeAttn, etc., for fairness."}}, "id": "ToGa7A48B5", "forum": "HljnvKxGRo", "replyto": "HljnvKxGRo", "signatures": ["ICLR.cc/2026/Conference/Submission1960/Reviewer_85zg"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1960/Reviewer_85zg"], "number": 17, "invitations": ["ICLR.cc/2026/Conference/Submission1960/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763611071790, "cdate": 1763611071790, "tmdate": 1763611471030, "mdate": 1763611471030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}