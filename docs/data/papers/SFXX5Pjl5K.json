{"id": "SFXX5Pjl5K", "number": 4655, "cdate": 1757736163372, "mdate": 1759898021424, "content": {"title": "EntropyLong: Effective Long-Context Training via Predictive Uncertainty", "abstract": "Training long-context language models to capture long-range dependencies requires specialized data construction. Current approaches, such as generic text concatenation or heuristic-based variants, frequently fail to guarantee genuine long-range dependencies.\n    We propose \\textbf{EntropyLong}, a novel data construction method that leverages predictive uncertainty to verify dependency quality. Our approach identifies high-entropy positions in documents, retrieves semantically relevant contexts from large corpora, and verifies their utility by assessing whether they reduce prediction entropy. This \\textit{model-in-the-loop verification} ensures each dependency represents measurable information gain rather than spurious correlation. We construct training samples with long-range dependencies by combining original documents with these verified contextual supplements.\n    Using FineWeb-Edu and Cosmopedia, we generate a dataset of 128K-length sequences with verified dependencies. Models trained on this data demonstrate significant improvements on RULER benchmarks, particularly in tasks requiring distant information. Following instruction fine-tuning, our models also achieve substantial gains on LongBench-v2, demonstrating enhanced long-context understanding. Extensive ablation studies further validate the necessity and effectiveness of entropy-based verification for long-context training.", "tldr": "", "keywords": ["Longcontext"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4d9a7ad99c5251504f86ae8ad5d7787fa0fbd974.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the lack of genuine long-range dependencies in long-context training data. It proposes EntropyLong, a novel data pipeline that uses a \"model-in-the-loop\" to verify data quality. The method identifies high-uncertainty (high-entropy) token positions in a document, retrieves relevant context, and verifies that this context reduces the model's prediction entropy at that position. Only verified context is used to construct final training samples. Training a Llama-3-8B model on this data shows significant performance gains on RULER and LongBench-v2 benchmarks over strong baselines, with ablations confirming the necessity of the entropy verification step."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors proposed a novel idea to create true long-context dependencies in long-context pre-training data. Their method using entropy to select tokens and relevant contexts shows effective results to answer the premise that a good dependency is one that measurably reduces model uncertainty.\n2. This paper demonstrates clear and consistent performance improvements over two baselines (Quest and NExtLong) on RULER and LongBenchv2 benchmarks.\n3. The analysis in section 6 is solid and convincing, providing detailed hyper-parameter selection and validating the paper's core hypothesis."}, "weaknesses": {"value": "1. The proposed method needs a series of ablation studies to find effective thresholds. If we want to apply to a new model, we need to carefully find high entropy tokens and select dependent contexts. I'd like to see if this dataset can be generalized to other models besides the one (Llama3-8B) used to construct the dataset. \n2. The baselines and the proposed method focus on finding extra documents for each short chunk to create long dependencies. We may need additional baselines by creating long dependencies based on current chunks like CIP[1] or UTK[2]. \n3. In section 3.2 item 2, the authors claim the optimal context C is the one that maximizes the information gain. However, in table 5, we saw the performance doesn't improve if we pick highest information gain contexts. The authors believe the problem is because of the data quantity. Then can we ensure the total tokens are the same with higher threshold to validate this claim?\n\n[1] CIP: LongSkywork: A Training Recipe for Efficiently Extending Context Length in Large Language Models \n[2] UTK: Untie the Knots: An Efficient Data Augmentation Strategy for Long-Context Pre-Training in Language Models"}, "questions": {"value": "1. For each document, you will find prepended contexts by changing two thresholds including high-entropy tokens and effective contexts to reduce entropy. How did you maintain the size of total length to be 128K in your case? If the contexts + original document > 128K, will you just separate as different samples like <context1><doc> and <context2><doc>? If so, does this mean we may train more on the tokens in original document <doc> than other documents?\n2. The proposed threshold will affect the quality and quantity of the data. I think this work can be stronger if we analyze the trade-off between $\\alpha$ and $\\epsilon$ to have similar total token budget but one is high $\\alpha$ and low $\\epsilon$ and the other is low $\\alpha$ and high $\\epsilon$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HcCh3lPn2b", "forum": "SFXX5Pjl5K", "replyto": "SFXX5Pjl5K", "signatures": ["ICLR.cc/2026/Conference/Submission4655/Reviewer_L2EC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4655/Reviewer_L2EC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761264998325, "cdate": 1761264998325, "tmdate": 1762917493055, "mdate": 1762917493055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce a methodology for improving the long-context performance of language models by modifying training data based on high-entropy regions. The approach begins by identifying positions in a training document where the model shows high predictive uncertainty, measured through token-level entropy. For each such high-entropy region, semantically similar documents are retrieved using embedding similarity. Among the retrieved candidates, only those that, when prepended to the original document, lead to a measurable reduction in entropy for that region are kept. These verified documents are then added to the beginning of the training document, producing an extended version that contains additional context proven to lower uncertainty.\n\nThe modified corpus is then used to fine-tune a pretrained model with RoPE-scaled positional embeddings to support longer context lengths. Experimental results show clear gains on long-context benchmarks such as RULER and LongBench-v2, confirming that entropy-guided data construction improves a model’s ability to utilize distant context. The paper also includes ablations on the entropy threshold, the necessity of the verification step, and the effect of the retrieval window size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The experimental setup in this paper is mostly clear, and the idea of using uncertainty in text and explicitly targeting those regions is interesting. The core motivation—to improve model performance by providing additional, helpful information in the training text—is well-founded. By doing so, the approach encourages the model to make better use of context and, directly or indirectly, to rely on information that is placed farther away from the main document, thereby improving its ability to utilize long-range context. Overall, the idea opens the door to several interesting research directions that explore uncertainty-guided data modification and its role in enhancing context understanding in language models."}, "weaknesses": {"value": "- **W1)** One weakness is the lack of clarity in how high-entropy regions are identified and represented. The paper mentions detecting areas of high predictive entropy, but it is unclear how these local and often volatile entropy values are aggregated into meaningful regions that actually require additional context. In practice, entropy can fluctuate sharply even within short spans—for instance, the first part of a word or phrase often has higher entropy than later parts due to language-level artifacts. Without clear illustrations or examples of how such regions behave during next-token prediction, it is difficult to assess whether they truly capture model uncertainty rather than incidental variation.\n\n- **W2)** Conceptually, the approach still resembles retrieval augmentation, where related text is added to provide more context, and it is not entirely clear whether entropy is the main factor behind the gains. Although the authors compare verified versus unverified document additions, the difference could stem from how the top-k similar documents are chosen or from context-overload effects. It would help to clarify whether documents that reduce entropy tend to rank higher in similarity, and how many are included when verification is skipped. While EntropyLong operates at a finer granularity than RE³SYN, a direct comparison would strengthen the argument that entropy reduction—rather than dependency linking alone—is what drives improvement.\n\n- **W3)**  The paper largely treats high entropy as a sign of poor model performance or uncertainty, but this link is not always straightforward. This claim is somewhat reflected by the language of this paper, specially cases where the authors claim this entropy increase as a theory for poor model performance. Entropy in language can also reflect diversity or ambiguity rather than failure, and its interpretation varies across models and data domains. It would help if the authors discussed whether lower entropy consistently aligns with better predictions or cited supporting evidence for that assumption. From my own experience with large language models, entropy behavior can differ substantially across model families and scales, suggesting that the same thresholds for $\\alpha$ and $\\epsilon$ may not generalize. Including experiments that vary these parameters across different models would clarify how model-dependent the method is. I understand that the selection of such values is document dependant, but I believe the model itself could play a role in this method which explicitly uses entropy  of a language model for targetting points of improvement. \n\n- **W4)** The paper does not provide information or comparisons regarding the computational cost of the entropy-based data curation process. I assume that the lookup and top-k extraction steps could be computationally intensive, and it would be helpful if the authors included at least an approximate cost analysis or comparison with other data construction methods, even if only in the appendix.\n\n- **W5)**  (Minor) The paper does not discuss several closely related approaches that also reuse similar regions of text to improve next-token prediction. Khandelwal et al. (2020, Nearest Neighbor Language Models) retrieve semantically similar examples from the training set using embedding similarity between hidden states, improving predictions by grounding generation in past contexts. In contrast, Liu et al. (2025, Infinigram) rely on n-gram frequency statistics to reuse highly similar text fragments during inference. While these methods apply retrieval at inference rather than training, both demonstrate that referencing similar parts of text can enhance next-token accuracy. Highlighting this connection would make it clearer how EntropyLong extends these retrieval-style ideas into a training-time, entropy-guided framework."}, "questions": {"value": "- **Q1**: I am curious about the placement and ordering of the retrieved, entropy-reducing documents. Table 7 suggests that random or sequential shuffling sometimes yields noticeable gains—especially at 32 K and 128 K context lengths—which might indicate sensitivity to document ordering. Do the authors have an explanation for this behavior?\nAdditionally, connecting to works such as Lost in the Middle (Liu et al., 2023) and LDAM (Chen et al 2025), which study how the position of relevant information affects model performance, it would be interesting to see experiments that vary where these documents are inserted. My intuition is that placing them at the beginning helps the model learn to attend to earlier parts of the context, potentially explaining the stronger performance at longer lengths.\n\n- **Q2**: It is somewhat unclear why the authors choose to append multiple retrieved documents rather than focusing on a single or very limited number that produce the largest entropy reduction. Would using only the top document (K = 1) that most strongly reduces entropy yield similar improvements? Exploring smaller K values could help disentangle the effects of information quantity from information placement, and—when combined with varying document position—might offer clearer insights into how LLMs actually utilize extended context.\n\n- **Q3**: I am having a hard time understanding Figure 2 from description and text. Could the authors please provide an explanation of the setup, formulation for the attention pattern analysis and how it relates to the long entropy method ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U3EXQNWBTA", "forum": "SFXX5Pjl5K", "replyto": "SFXX5Pjl5K", "signatures": ["ICLR.cc/2026/Conference/Submission4655/Reviewer_PDbx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4655/Reviewer_PDbx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761607648094, "cdate": 1761607648094, "tmdate": 1762917492768, "mdate": 1762917492768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EntropyLong, a novel data-construction recipe for the “long-context continual pre-training” stage of LLM development. Given base model M, training corpus D, and retrieval corpus R, the authors present a procedure consisting of 4 steps:\n1. For each document from D, define high uncertainty tokens (the tokens the autoregressive model M is unsure about)\n2. For each uncertain token: \n\n2.a. Form a query by taking the surrounding context\n\n2.b. Embed the query (jina-embeddings-v3)\n\n2.c. Use this query to retrieve 32 documents from R using cosine similarity\n\n3. Filter documents that reduce the uncertainty of the token (only keep C if it did not appear for this doc before)\n4. Shuffle and merge filtered documents into a context\n\nThen, starting with the M checkpoint (LLaMa3-8B) as the initialization, they continue its training with increased sequence length (8K to 128K) as per standard practice with modified RoPE hyperparameter.\n\nThe authors compare their method to baseline methods: Quest (just retrieve related documents) and NExtLong (retrieve related documents, but also unrelated documents for distraction)"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The authors clearly explain their strategy, formulate and test relevant hypotheses."}, "weaknesses": {"value": "The authors claim that dataset release is a key contribution, but no comparison with other datasets is presented in the experiments section. They only compared their method against other published methods.\nOne comparison that could have been done is the comparison with the dataset that was used for training LLaMa-3.1-8B, which has the same context length as the resulting model presented by this work. Another comparison could be made with LongMIT-128K."}, "questions": {"value": "Here are several questions for the authors:\n- How are 100K root docs sampled? Uniformly or using some other random strategy?\n- Are the root documents included in the retrieval corpus?\n- How is underflow or overflow in ConstructSequence handled?\n- How is context deduplicated across root documents (sequences)? How do you know there is no retrieved document that is a part of every - single sequence you train on?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fg8x2PFgnl", "forum": "SFXX5Pjl5K", "replyto": "SFXX5Pjl5K", "signatures": ["ICLR.cc/2026/Conference/Submission4655/Reviewer_2y86"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4655/Reviewer_2y86"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663370926, "cdate": 1761663370926, "tmdate": 1762917492543, "mdate": 1762917492543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EntropyLong, a model-in-the-loop data construction framework for long-context language model training. It identifies high-entropy positions where the model is uncertain, retrieves semantically related contexts, and retains only those that empirically reduce prediction entropy, ensuring genuine long-range dependencies. Using Llama-3-8B with a 128K context window trained on FineWeb-Edu and Cosmopedia, EntropyLong achieves higher scores than Quest and NExtLong on RULER (87.4 avg) and LongBench-v2 (27.6 overall) while preserving short-text performance. Ablations confirm that entropy-based verification is essential and that optimal thresholds balance data quality and quantity, leading to improved long-context reasoning and mitigation of the “lost-in-the-middle” problem."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a novel application for long-context data construction, offering a model-driven perspective on how to identify and verify informative dependencies across distant text segments. \n\nIts originality lies in connecting information-theoretic uncertainty with practical dataset synthesis, moving beyond heuristic concatenation methods. \n\nThe paper is well-organized and clear, making a complex pipeline easy to follow."}, "weaknesses": {"value": "1. The central assumption is that high predictive entropy indicates missing long-range information, which lacks strong theoretical grounding. Empirically, many high-entropy tokens correspond to discourse openings, transitions, or rare words rather than genuine dependency points. Thus, the “verified” concatenations may enhance topical smoothness rather than causal linkage.\n\n2. The paper does not analyze what types of dependencies are actually captured after verification (e.g., factual consistency, narrative continuity, or syntactic linking). Without qualitative inspection or causal tracing, the claim of “true long-range reasoning” remains speculative.\n\n3. The framework requires per-sample forward passes for entropy computation and re-verification, which substantially increases computational cost. The paper does not discuss efficiency trade-offs or scaling to larger corpora.\n\n4. Although the paper frames its contribution as constructing long-context dependencies, the retrieval query for each high-entropy token is limited to a 16-token local window. This narrow context restricts semantic scope to near-sentence continuations rather than true cross-document or distant dependencies, effectively reducing the retrieval process to localized topical or lexical matching. As a result, the constructed samples may improve surface fluency but do not necessarily capture genuine long-range information flow."}, "questions": {"value": "1. How do you ensure that high-entropy tokens truly correspond to informational gaps rather than discourse transitions or stylistic boundaries? Have you examined any qualitative examples of entropy peaks to confirm this assumption?\n\n\n2. The paper treats entropy reduction as evidence of successful dependency capture. Could you clarify whether the reduction tends to occur at the lexical, syntactic, or semantic level? Have you compared this with other dependency metrics (e.g., attention shifts or mutual information)?\n\n3. Does the entropy-based selection bias the resulting corpus toward more homogeneous or easier-to-predict contexts? Have you evaluated the lexical or topical diversity of the constructed dataset versus the raw corpus?\n\n4. Given that each sample requires per-candidate forward verification, how does the total cost compare to standard long-context pretraining? Are there any approximations (e.g., smaller proxy models, cached logits) that could make the process scalable?\n\n5. The experiments focus on a single model (Llama-3-8B). Do you expect the same entropy distribution patterns and improvements to hold for larger or smaller architectures, or for instruction-tuned models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xV0aMLYpi4", "forum": "SFXX5Pjl5K", "replyto": "SFXX5Pjl5K", "signatures": ["ICLR.cc/2026/Conference/Submission4655/Reviewer_dtra"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4655/Reviewer_dtra"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707303310, "cdate": 1761707303310, "tmdate": 1762917492247, "mdate": 1762917492247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}