{"id": "ni3BIqhCzw", "number": 15857, "cdate": 1758256194901, "mdate": 1763355415198, "content": {"title": "On the Generalization of Dynamic GNNs: A Heavy-Tailed Wavelet Perspective", "abstract": "Dynamic graphs exhibit bursty and intermittent dynamics that are poorly captured by standard sequence models. We take a signal–statistical view and show that node-wise temporal signals, once transformed into wavelet space, display Pareto-type heavy tails: a small set of high-magnitude coefficients concentrates a large fraction of the total energy. Building on this observation, we introduce Tail-Aware Masking for Dynamic GNNs (DGNNs): a simple, plug-in mechanism that retains only the top wavelet coefficients (per node) and zeros out the rest before message passing.\n\nOn the theory side, under a mild regularly varying tail assumption with index $\\alpha>2$, we prove that (i) the retained coefficients capture a constant fraction of energy scaling as $\\rho^{1-2/\\alpha}$ for retention ratio $\\rho$, (ii) masking increases an effective tail index of the features, and (iii) the empirical Rademacher complexity and the generalisation gap of the resulting hypothesis class contract at rate $\\mathcal{O}\\!\\big(\\rho^{\\frac{1}{2}-\\frac{1}{\\alpha}}/\\sqrt{nT}\\big)$. These results formalise why sparse, tail-focused representations improve sample efficiency.\n\nEmpirically, on METR-LA we observe clear heavy tails via survival curves and Q–Q plots, validating the modelling prior. Our tail-aware DGNN consistently outperforms its baseline counterpart, yielding substantial reductions in MSE and gains on tail-sensitive metrics, while maintaining training stability through a short warmup. The approach is architecture-agnostic, interpretable (the mask exposes the most informative time–node events), and requires minimal tuning. Together, our findings connect a robust statistical phenomenon of dynamic graph signals to concrete architectural choices and provable generalisation benefits.", "tldr": "", "keywords": ["dynamic F"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d9c8ea86ab84e9b8e9eb41f3bcce32979f5e16d1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the generalization problem of Dynamic Graph Neural Networks (DGNNs) in handling dynamic graphs with bursty and intermittent dynamics. The core innovation is the discovery that node-wise temporal signals, when transformed into the wavelet domain, exhibit Pareto-type heavy-tailed distributions, and the proposal of a Tail-Aware Masking mechanism that retains only the top wavelet coefficients (per node) and zeros out the rest before message passing. Theoretically, under the assumption of a regularly varying tail with index α>2, the paper proves that the retained coefficients capture a constant fraction of energy scaling, masking increases the effective tail index of features. Experimentally, on the METR-LA dataset, the tail-aware DGNN consistently outperforms the baseline, achieving  reduction in MSE and gains on tail-sensitive metrics while maintaining training stability through a short warmup period. However, the paper could still improved in clarity, comprehensiveness of experiments."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper has a clear research motivation, which originates from the discovery of the heavy-tailed distribution of wavelet coefficients. It subsequently constructs a theoretical framework and proceeds with the design of the corresponding method, forming a logical research process."}, "weaknesses": {"value": "1. The clarity of the paper needs significant improvement:\n   - Some abbreviations are used without prior full-form explanations, such as POT, Q-Q plot, and CCDF.\n   - The specific formulation and application of the \"DGNN\" are not clearly elaborated, and details regarding model training and evaluation (e.g., representation learning strategies, loss function designs) are lacking.\n   - The writing formality is insufficient: the pipeline in Section 4.4 only provides a brief procedure without detailed descriptions, and the experimental tasks mentioned in lines 313–323 are ambiguous due to the absence of necessary explanations.\n\n2. The experimental comparisons lack comprehensiveness. Both recent and classic baseline models in the field of dynamic graph learning—especially wavelet-based models (e.g., [1,2,3])—are missing. Only one baseline model is adopted, which weakens the persuasiveness of the experimental results.\n\n3. The paper fails to conduct a time complexity analysis, particularly regarding the wavelet decomposition process applied to dynamic graphs.\n\n\n\n[1] Sun, Ke, et al. \"ModWaveMLP: MLP-based mode decomposition and wavelet denoising model to defeat complex structures in traffic forecasting.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 38. No. 8. 2024.\n\n[2] Jiang, Jiawei, et al. \"Pdformer: Propagation delay-aware dynamic long-range transformer for traffic flow prediction.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 37. No. 4. 2023.\n\n[3] Li, Yaguang, et al. \"Diffusion convolutional recurrent neural network: Data-driven traffic forecasting.\" arXiv preprint arXiv:1707.01926 (2017).\n\n[4] Chen, Chao, et al. \"Easydgl: Encode, train and interpret for continuous-time dynamic graph learning.\" IEEE Transactions on Pattern Analysis and Machine Intelligence (2024)."}, "questions": {"value": "1. How to differentiate the nosie and useful information when using the masking?\n2. As wavelet decoposition is very time-consuming with high computational cost, how does it performs in time complexity as well as running time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fjhfoc5aNP", "forum": "ni3BIqhCzw", "replyto": "ni3BIqhCzw", "signatures": ["ICLR.cc/2026/Conference/Submission15857/Reviewer_rGAK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15857/Reviewer_rGAK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760884261362, "cdate": 1760884261362, "tmdate": 1762926076919, "mdate": 1762926076919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "2qTOAhfSui", "forum": "ni3BIqhCzw", "replyto": "ni3BIqhCzw", "signatures": ["ICLR.cc/2026/Conference/Submission15857/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15857/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763355414384, "cdate": 1763355414384, "tmdate": 1763355414384, "mdate": 1763355414384, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines dynamic graph node time-series through a wavelet lens and observes heavy-tailed Pareto-like distributions in their coefficients. It proposes a Tail-aware masking mechanism that retains only high-magnitude wavelet coefficients before message passing in a DGNN."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**1. Interesting conceptual framing**\n\nThe paper organizes existing wavelet- and heavy-tail-based intuitions into a unified viewpoint on temporal graph signals, though the underlying ideas are not new.\n\n**2. Simple and interpretable mechanism**\n\nThe masking scheme is easy to integrate and interpretable."}, "weaknesses": {"value": "**1. Major methodological flaw**\n\nThis paper repeatedly claim to study “dynamic graphs” with formulation of $\\mathcal{G}_{t}$ (the graph topology is time-varing); yet the quantitative validation is done on METR-LA, whose graph topology does not change over time. \n\nThis leads to a major problem that the heavy-tailed phenomenon they observe is purely due to temporal signal bursts, not structural dynamics. Consequently, the title “Generalization of Dynamic GNNs” and much of the narrative are misaligned with the experiments.\n\n**2. Method novelty is minor**\n\nThe Tail-Aware Masking is effectively quantile-based feature pruning, a trivial operation already present in sparse attention and magnitude-pruning literature. There is no comparison to existing sparsification techniques (attention masking, dropout variants, learned gating).\n\n**3. The theorectical analysis lacks rigor and originality**\n\nIn Sec. 4.5 (Complexity and gneralisation), the derivation of the Rademacher-complexity bound is a sketch; actually, the claimed bound $\\mathcal{O}(\\rho^{1/2-1/\\alpha}/\\sqrt{nT})$ is just a restating of classical i.i.d. heavy-tail results, not a new theoretical contribution for DGNNs.\n\n**4. Missing ablation**\n\nThis paper reports limited visual results. For example, in terms of Retention ratio $\\rho$, the paper simply claims that ``Performance improves as $\\rho$ decreases from 10'', but no numerical evidence is provided. Similarly, other hyperparameters (e.g., warmup length, Huber loss parameter $\\delta$) are discussed descriptively but lack quantitative comparisons or sensitivity analyses."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sfPziWFHB7", "forum": "ni3BIqhCzw", "replyto": "ni3BIqhCzw", "signatures": ["ICLR.cc/2026/Conference/Submission15857/Reviewer_LBoN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15857/Reviewer_LBoN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762436026620, "cdate": 1762436026620, "tmdate": 1762926076549, "mdate": 1762926076549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes dynamic graph signals (time series) using wavelet transforms and reports a heavy-tailed pattern in the coefficients."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The research identifies and formalizes the phenomenon of heavy-tailed distributions in the wavelet coefficients of dynamic graph signals. This discovery provides a previously overlooked perspective for understanding dynamic graph data."}, "weaknesses": {"value": "The paper does not meet the standards of ICLR for the following reasons:\n\nWriting and presentation: The paper is poorly organized and lacks self-containment. Many essential details are missing. For instance, Section 3 is overly concise, and the experimental setup for the reported findings is not described, making it difficult to assess their validity.\n\nTheoretical formulation: The theory is not well developed. The underlying assumptions are unclear, and the role of “dynamic graphs” in the formulation is insufficiently explained.\n\nAlgorithmic contribution: The proposed algorithm, which is said to follow from the theoretical observations, is not presented clearly or in sufficient detail.\n\nOverall, the submission appears incomplete and requires substantial improvement in both technical depth and presentation before it can be considered ready for publication."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "s3ax65sgUZ", "forum": "ni3BIqhCzw", "replyto": "ni3BIqhCzw", "signatures": ["ICLR.cc/2026/Conference/Submission15857/Reviewer_3vf2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15857/Reviewer_3vf2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762737966322, "cdate": 1762737966322, "tmdate": 1762926076172, "mdate": 1762926076172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}