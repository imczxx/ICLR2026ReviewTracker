{"id": "7uaPJ6WAHv", "number": 20705, "cdate": 1758309199370, "mdate": 1763684558649, "content": {"title": "Big-Layers: Enabling end-to-end training", "abstract": "Training deep neural networks on extremely large inputs—such as gigapixel Whole Slide Images (WSIs) in digital pathology—poses significant challenges due to GPU memory constraints. Multiple Instance Learning (MIL) circumvents this limitation by processing patches from a WSI. However, the encoder used to get patch embeddings is usually a generic pre-trained deep neural network model. In this paper, we propose a training strategy that enables training the encoder by dynamically offloading intermediate activations of a layer to CPU RAM, allowing the layer to process inputs that do not fit in the GPU memory. We demonstrate the effectiveness of our approach on PANDA and CAMELYON datasets using popular MIL approaches. Experimental results indicate that our method improves the Quadratic Weighted Kappa (QWK) metric, on PANDA, by 7–15 percentage points compared to ResNet-18 baselines where encoders are kept frozen. Evaluations on external test sets further suggest better generalisation, and in some configurations, our models even outperform foundation-model encoders on TCGA-PRAD. The code will be made publicly available upon publication.", "tldr": "", "keywords": ["Deep learning", "machine learning", "histopathology", "digital pathology", "cancer"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5dfd1e02c5bc94b9b108b3050c1ac82c4e181b6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an approach for enabling end-to-end training on Whole Slide Images by offloading intermediate comptuations to CPU. Their experiments, performed on the Resnet-18 backbone, reveals consistent improvement over a frozen backbone. They propose layer-specific approaches for offsetting GPU memory requirements. Unfortunately, their experiments are solely performed on the CNN backbone, while current SOTA is dominated by vision transformers. For instance, their approach with ABMIL achives a quadratic weighted kappa of 84.60, while using the frozen UNI encoder achieves a performance of ~94 with ABMIL. The generalization of their approach to the self-attention mechanism of vision transformers is not discussed nor intuitive from their proposed algorithm, substantially limiting the utility of this approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The ability to train models end-to-end is of substantial interest to the computational pathology community. \n- The model is evaluated on three MIL methods, indicating that performance is not specific to a specific mechanism. \n- The model is evaluated on a sufficient number of tasks."}, "weaknesses": {"value": "- Performance is only shown on the Resnet-18 encoder, which is substantially behind the state of the art vision encoders (e.g UNI-2, Virchow-2, Conch 1.5). Consequently, even the improved end-to-end model lags behind frozen vision encoders by a wide margin. \n- The way to apply this method to self-attention is not clear from the text, further limiting the relevance of these results to current state of the art. \n- There are no benchmarks against other end-to-end approaches in CPath. There is consequently no evidence that this method provides benefit over existing approaches. \n- There is substantial room in the paper for additional details. An improved description of the method should be provided. \n\nAs it stands, the work is not framed in terms of the state-of-the-art, and I believe there is not sufficient time to include such state-of-the-art. I am recommending reject as a result. However, I would be willing to update my rating if my comments are sufficiently addressed."}, "questions": {"value": "- The code should be provided to help us further evaluate its quality and usability.\n- The benchmark protocol does not need to be in bullet points. The extra space should be used to describe the method in greater detail."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eQjSYJz1nV", "forum": "7uaPJ6WAHv", "replyto": "7uaPJ6WAHv", "signatures": ["ICLR.cc/2026/Conference/Submission20705/Reviewer_zcrD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20705/Reviewer_zcrD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761158826317, "cdate": 1761158826317, "tmdate": 1762934084559, "mdate": 1762934084559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new approach for making predictions on gigapixel histopathology images. Instead of the classical pipeline: (1) tiling slides into patches, (2) embedding patches with pretrained encoders, and (3) aggregating for slide-level prediction; the authors address GPU memory constraints by offloading intermediate activations to CPU RAM when GPU memory is insufficient. This enables joint training of the patch encoder and the slide-level predictor. Concretely, they apply this only to the first layers of a ResNet-18, stopping once sufficient downsampling allows the remainder to fit on the GPU."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper tackles an important limitation of Multiple Instance Learning (MIL) methods in computational pathology that tile whole-slide images into instances. It might be desirable to perform end-to-end training, which usually is not done with current workflows."}, "weaknesses": {"value": "1. The paper is not well motivated within the existing literature. The community’s reliance on pretrained encoders stems not only from memory limits but also from limited labeled data, where pretraining provides substantial gains. The work cites a 2023 pipeline for MIL training, while the GPU memory bottleneck citations are from 2020–2021; the related work feels uneven and insufficiently up to date. \n2. The primary encoder, ResNet-18, is dated for this domain. \n3. The main comparison pits a fully trained ResNet-18 against a frozen ImageNet-pretrained ResNet-18. A fairer baseline would include contemporary frozen encoders pretrained on histopathology patches (e.g., UNI, Virchow, H-Optimus) to assess whether encoder finetuning is truly necessary and beneficial. \n4. The approach is stated to be incompatible with ViTs, which are common encoders in this area. It would be important to evaluate compatibility and performance with larger CNNs (e.g., ResNet-50) and, if possible, discuss extensions to transformer-based models. \n5. The runtime comparison is debatable. Since the baseline reportedly does not benefit from data augmentation; the time to be compared with should take into account the fact that the embeddings can be stored."}, "questions": {"value": "1. Is the method practical with larger backbones such as ResNet-50? \n2. How does the approach compare against strong frozen pretrained encoders (UNI, Virchow-2, H-Optimus)?\n3. Given that the current methods not only respond to memory limits but also to limited labeled data, could the authors specify and demonstrate the use case where their method improves on current SOTA ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hceMJTTuC1", "forum": "7uaPJ6WAHv", "replyto": "7uaPJ6WAHv", "signatures": ["ICLR.cc/2026/Conference/Submission20705/Reviewer_VRyv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20705/Reviewer_VRyv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842318706, "cdate": 1761842318706, "tmdate": 1762934082538, "mdate": 1762934082538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response to All Reviewers"}, "comment": {"value": "We thank all reviewers for their constructive and detailed comments. The major common concerns are addressed below.\n\n### 1. Stronger baselines (foundation models)\nAll reviewers requested comparisons to modern foundation encoders. We have now added experiments with three foundation models: H-Optimus-1, UNI2-h, and Prov-GigaPath. Results are included in the revised tables:\n\n- PANDA / TCGA-PRAD results: **Table 1**  \n- CAMELYON results: **Table 2**  \n- AUC metrics: **Appendix Table 3**\n\nWe observe that the in-domain performance of foundation models on PANDA is state-of-the-art, as expected. However, when applied cross-domain to TCGA-PRAD, the performance drops substantially and is similar to or lower than our end-to-end trained models. This inadequate robustness and domain generalisation is consistent with recent studies on foundation models, as now mentioned in section 1 with references [1-5] below. In contrast, foundation encoders generalise well from CAMELYON17 to CAMELYON16, likely reflecting a smaller domain gap.\n\nWe would like to emphasise that our goal is **not** to claim universal SOTA but to **enable practitioners to train encoders end-to-end**, providing an alternative when frozen features underperform.\n\nMoreover, **training encoders enables the use of domain-generalisation methods**, which we now discuss in the revised section 5.4. Incorporating such methods can further improve out-of-domain robustness and may allow end-to-end trained encoders to outperform foundation models on a wider range of datasets. However, systematically exploring combinations of domain-generalisation algorithms with encoder training across many datasets requires a **community-wide effort**, and we view this work as enabling that future exploration rather than attempting to benchmark all possible combinations.\n\n---\n\n### 2. Runtime comparisons of the precomputed-embeddings baseline and scalability to large models\nReviewers zGhW and VRyv noted that our original runtime comparison did not explicitly include the standard “precompute embeddings once” baseline. The original speed comparison intended to evaluate our layer implementations relative to PyTorch’s native GPU-resident implementations (which do not offload to the CPU), but we agree that reporting all relevant runtime numbers provides a clearer picture. We have now created a **Training epoch comparison** paragraph reporting on three setups tested on PANDA with an NVIDIA RTX 3090 GPU card:\n- **Precomputed-embeddings baseline:** ~20 s/epoch (train MIL head only)\n- **Baseline, on-the-fly embeddings (no encoder backprop):** ~45 min/epoch  \n- **Our end-to-end method (with encoder backprop):** ~200 min/epoch\n\nAlthough our method supports training larger encoders, doing so requires proportionally larger GPU memory and more computation time. Currently, we do not have sufficient numbers of large-memory GPUs to train many high-capacity models such as ResNet-50 at scale. Nevertheless, to demonstrate scalability, we performed a **runtime scaling study for ResNet-50 on an NVIDIA A100** GPU card and reported the results in section 5.3 (with plot in the Appendix).\n\n---\n\n### 3. ViTs, larger backbones, and applicability\nAll reviewers asked about applying our method to ViTs and larger encoders.\n\nWe now explicitly clarify in section 3.3 that our method *can* support self-attention and also outline **two practical implementation paths**:\n\n1. **Naïve self-attention:** treat linear projections as 1×1 convolutions (applied on 1D input) and apply the same sub-tensor offloading strategy as for CNN layers.\n2. **IO-aware tiled attention (FlashAttention):** load Q/K/V blocks (sub-tensors) to GPU, compute block-level attention, and update outputs incrementally (also, this is compatible with the partition-and-offload design in our proposed method).\n\nImplementing and benchmarking ViTs with our method remains an important direction, as doing so would also enable **fine-tuning existing foundation models** using our approach. Exploring this direction requires substantial additional engineering and experimentation, and we therefore leave a full empirical study to future work.\n\n[1] de Jong et al. \"Current pathology foundation models are unrobust to medical center differences.\" arXiv preprint arXiv:2501.18055 (2025).\n\n[2] Kömen, Jonah, et al. \"Do histopathological foundation models eliminate batch effects? A comparative study.\" arXiv preprint arXiv:2411.05489 (2024).\n\n[3] Kömen, Jonah, et al. \"Towards robust foundation models for digital pathology.\" arXiv preprint arXiv:2507.17845 (2025).\n\n[4] Gustafsson, Fredrik K. et al. \"Evaluating computational pathology foundation models for prostate cancer grading under distribution shifts.\" arXiv preprint arXiv:2410.06723 (2024).\n\n[5] Filiot, Alexandre, et al. \"Distilling foundation models for robust and efficient models in digital pathology.\" International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2025."}}, "id": "xOIGBHfls0", "forum": "7uaPJ6WAHv", "replyto": "7uaPJ6WAHv", "signatures": ["ICLR.cc/2026/Conference/Submission20705/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20705/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20705/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763684979740, "cdate": 1763684979740, "tmdate": 1763684979740, "mdate": 1763684979740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an approach to train the patch encoders on gigapixel WSI data with label supervision. This requires backpropagating gradients from the WSI label to the patch encoder layers, for which they propose dynamically offloading intermediate activations of a layer to CPU RAM, allowing the layer to process inputs that do not fit in the GPU memory. The authors show improved performance on PANDA, TCGA-PRAD and CAMELYON in terms of quadratic weighted kappa metric, compared to an ImageNet pre-trained encoder baseline, at a cost in training time."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Significance: The method allows backpropagating supervised signal back to the patch encoders of WSI for potentially significant gains in performance on WSI classification tasks. This type of strategy is interesting and could potentially help design very strong encoders in the future.\n\nClarity: The paper is generally clear."}, "weaknesses": {"value": "The main weaknesses currently have to do with the evaluation of the proposed method. Reinforcing the evaluation either during the rebuttal phase or after resubmission would change my assessment of the paper. \n\n1. The single baseline is a ResNet-18 encoder pre-trained on ImageNet, which artificially inflates the gain in performance from the proposed approach. The field has largely moved to task-specific encoders trained with self-supervised learning (DINO, MoCov3, etc.), or to encoders based on foundation models (UNI, Virchow, GigaPath, etc.) trained on large numbers of WSIs from various histopathology datasets. These encoders should be used as baselines instead. \n\n2. I am not convinced by the fairness of the comparison in terms of runtime. The authors report 45 minutes for one epoch for the baseline on PANDA vs. 200 minutes for the proposed approach. But my understanding is that this is when computing patch \"embeddings on the fly on GPU\" also for the baseline, whereas patch embeddings are usually computed offline and saved once and for all in the standard approach (as no backpropagation is required). Hence training the MIL stage from pre-computed patch embeddings for the baseline could be significantly faster.\n\n3. A nice addition would be to also demonstrate the method on a ViT encoder and on a ResNet-50, to get a sense of the scalability and flexibility of the approach. Experiments are so far limited to ResNet-18."}, "questions": {"value": "The major questions that could be answered to change my initial rating have to do with the weaknesses listed above.\n\nAdditional questions:\n- Could the authors also report AUCs when relevant for direct comparison with the literature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YEMX7ea7v1", "forum": "7uaPJ6WAHv", "replyto": "7uaPJ6WAHv", "signatures": ["ICLR.cc/2026/Conference/Submission20705/Reviewer_zGhW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20705/Reviewer_zGhW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860346975, "cdate": 1761860346975, "tmdate": 1762934081348, "mdate": 1762934081348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}