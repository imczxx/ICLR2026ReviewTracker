{"id": "fWgKnl4itC", "number": 20022, "cdate": 1758301622715, "mdate": 1759897005956, "content": {"title": "Bottlenecked Transformers: Periodic KV Cache Consolidation for Generalised Reasoning", "abstract": "Transformer LLMs have been shown to exhibit strong reasoning ability that scales with inference-time compute, most prominently through token-space “thinking” (i.e., chains of thought). A growing line of work pushes this extra computation into the model’s latent space (adjacent to standard decoding) which we term Auxiliary Latent-Space Computation (ALSC). Existing ALSC methods largely fall into three buckets: (i) token-mediated latent or special-token rollouts, (ii) residual/activation steering, and (iii) memory compression via cache pruning, merging, or summarization. An underexplored alternative is memory consolidation and reconsolidation, two processes in the brain that are responsible for stabilising newly formed memory traces, and, upon recall, transiently rendering established traces plastic such they can integrate new contextual information before restabilising. In a Transformer LLM, this can be seen as analogous to performing in-place global rewrites of incoming KV segments, and rewrites of past segments conditioned on newly observed tokens. In this work, we give a theoretical justification as to why memory (re)consolidation via KV cache rewrites is beneficial for improved reasoning. We do this through the lens of Information Bottleneck (IB) theory, which posits that model generalisation emerges from an optimal balance between input information compression and retention of predictive information in latent representations. We prove using IB theory that Vanilla decoder-only Transformers are inherently constrained in their ability to form task-optimal sequence representations. We then introduce the Bottlenecked Transformer, which augments a decoder-only backbone LLM with a lightweight Cache Processor, an auxiliary Transformer that performs periodic, non-causal, in-place KV rewrites at newline-delimited reasoning step boundaries. The processor consolidates recently written KV entries and reconsolidates a small, top-$k$ attention-selected set of prior entries, conditioned on recent context. We evaluate our Bottlenecked Transformer architecture on seven mathematical reasoning benchmarks, with four backbone LLMs. Our model sees consistent performance gains over vanilla Transformers and pause-token augmented Transformer baselines, with gains of up to +6.6pp for selected tasks and backbones.", "tldr": "", "keywords": ["Information bottleneck", "Generalisation", "Large Language models", "Latent space reasoning", "Representation learning", "Memory consolidation", "KV-cache compression", "Predictive encoding", "Reasoning", "Information theory"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/813b5f75494bce4b01713e2cf87bddacf3f1ad29.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Bottlenecked Transformer, an architecture designed to improve the reasoning capabilities of decoder-only LLMs. The authors first provide a theoretical motivation using Information Bottleneck (IB) theory, arguing that standard autoregressive training incentivizes the model's KV cache to retain unnecessary information from the input sequence, which can hinder generalization. To address this, they propose augmenting a frozen backbone LLM with a lightweight \"Cache Processor.\" This processor is a separate, non-causal Transformer that is invoked periodically (at newline characters) to perform in-place rewrites of the backbone's KV cache. The rewrite mechanism is inspired by the neuroscience concepts of memory consolidation (acting on recent KV entries) and reconsolidation (acting on a small, attention-selected set of prior entries). The processor is trained to rewrite the cache in a way that improves the prediction of the next reasoning step. The authors demonstrate that this approach yields consistent performance gains on seven mathematical reasoning benchmarks across four different backbone models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "**Strong Theoretical Foundation:** The primary strength of this work is its grounding in Information Bottleneck (IB) theory. The authors provide a formal proof that vanilla Transformers are constrained in their ability to form optimal sequence representations for generalization. \n\n**Novel and Bio-Inspired Architecture:** The concept of a separate Cache Processor that performs periodic, in-place KV cache rewrites is highly novel. The design, explicitly inspired by the neural mechanisms of memory consolidation and reconsolidation, is both elegant and intuitive.\n\n**Comprehensive Empirical Validation:** The authors validate their method across seven different mathematical and logical reasoning benchmarks and, crucially, with four different open-source backbone models of varying sizes. The consistent performance improvements over both vanilla SFT and a pause-token baseline demonstrate the robustness and general applicability of the approach.\n\n**Insightful Ablation Studies:** The paper includes well-designed ablation studies. The epoch-matched comparison effectively shows that an epoch of Processor training is a better use of compute than an additional epoch of SFT. The ablation on the reconsolidation budget (k) provides valuable insights into how task characteristics interact with the model's memory mechanism"}, "weaknesses": {"value": "**Significant Computational Overhead:** The practical utility of the method is currently hampered by its high computational cost. The authors report that training the Cache Processor is ~20x slower than standard SFT, and inference is ~45% slower on a 1B parameter model. While acknowledged as a potential engineering issue, this overhead is a major barrier to adoption.\n\n**Weak and Indirect Supervision:** The Cache Processor is trained only via the cross-entropy loss of the next reasoning step. As the authors note, this provides a high-variance and poorly localized signal for learning optimal cache rewrites, which may explain why performance doesn't always improve with Processor size or training duration.\n\n**Simplistic Invocation Trigger:** The Processor is invoked every time a newline token is generated. While simple to implement, this heuristic is not adaptive. It may trigger too often in contexts where newlines are frequent but semantically unimportant, or not often enough in dense reasoning chains that lack newlines.\n\n**Narrow Domain of Evaluation:** The experiments are exclusively focused on mathematical reasoning. While this is an excellent testbed for logical generalization, it's unclear how the core principle—compressing the past to better predict the future—would transfer to other domains like creative writing or summarization, where preserving rich, verbatim details from the context is often essential."}, "questions": {"value": "1. Regarding the computational overhead: Could you elaborate on the specific engineering challenges that prevent compatibility with methods like Flash Attention? Is there a clear path forward to mitigating the significant training and inference latency, or is it a fundamental cost of the non-causal, in-place rewrite operation?\n\n2. The supervision signal for the Processor seems to be a key challenge. Have you considered or experimented with auxiliary loss functions to provide a more direct learning signal? For example, could a variational IB objective be approximated, or could a contrastive loss be used to encourage the rewritten cache state to be more predictive than the original state?\n\n3. The choice to trigger on newline tokens is a simple heuristic. How sensitive are the results to this specific choice? For instance, what would happen if the trigger was changed to a period or another punctuation mark, and have you considered more adaptive triggers based on model uncertainty or prediction error, as suggested in your discussion?\n\n4. The paper makes a compelling case for memory consolidation in the context of mathematical reasoning. How do you see this mechanism applying to domains where reasoning is less about logical abstraction and more about maintaining narrative consistency or tracking complex entity relationships, such as in long-form story generation? Could the \"bottlenecking\" process be detrimental in such cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uEJnQiVka8", "forum": "fWgKnl4itC", "replyto": "fWgKnl4itC", "signatures": ["ICLR.cc/2026/Conference/Submission20022/Reviewer_kt4M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20022/Reviewer_kt4M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760550419862, "cdate": 1760550419862, "tmdate": 1762932920008, "mdate": 1762932920008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Bottlenecked Transformers, augmenting a decoder-only LLM with a small Cache Processor that periodically rewrites selected KV-cache entries at newline-delimited “reasoning step” boundaries to mimic consolidation/reconsolidation and improve generalized reasoning. The authors motivate this with an Information Bottleneck (IB) analysis arguing that vanilla Transformers’ KV caches act as terminal, minimally compressive bottlenecks and thus preserve extraneous input information that can hinder generalization."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Clear theoretical motivation**: The IB framing (Theorems 4.1–4.2) formalizes the KV cache + final hidden state as a terminal bottleneck and links autoregressive training to maximizing both $I(X;Z)$ and $I(Z;Y)$, motivating selective rewrites.\n\n* **Simple, modular mechanism**: A lightweight, layer-aligned processor rewrites (i) recent-step KVs and (ii) top-k recalled past entries by attention mass; gating stabilizes updates. The schedule is practical (trigger on newline).\n\n* **Consistent empirical gains**. Across backbones/tasks, the model usually beats SFT and pause-token baselines; Table 1 and Fig 3 show broad improvements and useful diagnostics."}, "weaknesses": {"value": "* **Scope & novelty relative to cache operators**: While the reconsolidation framing is fresh, the core operation (transform selected cache entries) is close to existing cache-edit/compression lines; novelty hinges on scheduling/selection rather than a fundamentally new cache objective.\n\n* **Supervision signal may be weak**: The processor is trained only via next-step cross-entropy with truncated BPTT, which the authors note causes credit-assignment issues; no explicit IB/MI control is used.\n\n* **Mixed results on OOD and symbol-heavy tasks**: MATH/LogiQA/Gaokao occasionally favor SFT; performance is sensitive to k and language/domain shift, suggesting limited generality without careful tuning."}, "questions": {"value": "* **Ablations on triggers**: Newline ≠ reasoning step in many datasets. How do results change with alternative triggers (e.g., surprise/prediction-error gates, punctuation, tokens-per-step), or with learnable triggering?\n\n* **What is actually rewritten?** Can you provide layer-wise and head-wise analyses of rewrite magnitudes (gate values, ΔK/ΔV norms), and their correlation with downstream accuracy, to demonstrate nontrivial, stable edits?\n\n* **Sensitivity to selection policy**: Beyond top-k by attention mass, evaluate (a) per-layer k, (b) diversity-regularized selection, (c) recency vs. salience trade-offs, and (d) dynamic k conditioned on step difficulty. Table 2 hints that MATH prefers larger k. Can adaptive k close the MATH gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iEygMwxYDi", "forum": "fWgKnl4itC", "replyto": "fWgKnl4itC", "signatures": ["ICLR.cc/2026/Conference/Submission20022/Reviewer_cuPq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20022/Reviewer_cuPq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760761676332, "cdate": 1760761676332, "tmdate": 1762932918929, "mdate": 1762932918929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Bottlenecked Transformers, which add a lightweight non-causal Cache Processor that periodically rewrites the KV cache at newline-delimited “reasoning step” boundaries to consolidate recent tokens and reconsolidate top‑k recalled past tokens for better sequence-level reasoning. The approach is motivated via an Information Bottleneck perspective arguing vanilla decoder-only Transformers over-retain history in the KV cache and thus benefit from in‑place memory edits that preserve predictive information without compressing dimensionality. Concretely, the processor performs periodic, in-place global rewrites of recent KV segments and selected prior entries conditioned on the latest context, acting as sequence-level auxiliary latent-space computation adjacent to standard decoding. Across seven mathematical reasoning benchmarks and four backbone LLMs, the method yields consistent gains over vanilla and pause-token baselines, with improvements up to +6.6 percentage points on selected tasks and models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear problem framing of ALSC at the sequence level and principled positioning of cache rewriting as consolidation/reconsolidation rather than compression, with an architecture that is simple to integrate and keeps KV dimensionality unchanged.​\n\n2. Information-theoretic perspective highlights a plausible failure mode of vanilla decoder-only training (KV states retain unnecessary sequence detail), and motivates a targeted non-causal rewrite to increase predictive efficiency without shrinking memory footprint.​\n\n3. Consistent improvements over SFT and “SFT + pause tokens” across multiple backbones and benchmarks, including sizeable gains on in-distribution math tasks (e.g., +6.6 on SVAMP with Llama‑3.2‑1B; +4.6 on GSM8K with Llama‑3.2‑3B) and thoughtful ablations on epoch-matched budgets and the reconsolidation window k.​\n\n4. Implementation details are thorough (selection policy via attention mass, layer-aligned processor blocks, gated residual rewrites), with reproducibility notes and multiple architectural/training ablations provided.​"}, "weaknesses": {"value": "1. Theorem 4.2 provides a lower bound linking token cross‑entropy to a sum of mutual information terms, but the text then treats autoregressive training as “maximizing both” $$I(S_{0:n};\\hat Z)$$ and $$I(\\hat Z;S_{n+1})$$, which does not follow from a loose bound and is not shown to hold per‑term, weakening the justification for the proposed remedy.\n2. The “terminal bottleneck” claim for the KV cache plus last hidden state is used to argue that the cache retains reconstructive detail that impedes generalization, yet no empirical evidence is provided that the cache can reconstruct inputs or that rewrites reduce $$I(X;Z)$$ while preserving $$I(Z;Y)$$ as required by the IB narrative.\n3. No IB‑style loss or MI estimation is used; reliance on SGD noise and the data‑processing inequality is asserted as a path to compression, but there is no quantification of predictive efficiency $$I(Z;Y)/I(X;Z)$$ before vs. after cache rewrites to support the core thesis.\n4.  Comparisons omit strong ALSC and cache‑operator baselines beyond pause tokens, such as latent rollouts, differentiable cache augmentation, or modern cache merging/compression tuned for reasoning, leaving performance advantages over alternative designs unclear under matched budgets.\n5. The authors notes reconsolidation may require prediction‑error gating, but does not test such triggers or analyze boundary sensitivity (e.g., recent‑step window length R) in the main results. \n6.  Results appear single‑seed with no confidence intervals or significance testing, and there is no contamination analysis despite training on synthetic math mixes that may overlap benchmark styles\n7. Training the processor incurs ~6× memory and ~20× wall‑clock over SFT, while inference adds ~25% memory and ~45% latency despite infrequent invocation, which is a significant overhead for modest accuracy gains on small/medium backbones. \n8. There are no MI proxies/estimates, ablations that track retrieval fidelity vs. abstraction, or causal analyses showing which information is discarded/retained by the processor.\n9. No layer/head‑level probes, attention‑pattern shifts, or KV‑state visualizations that explain why and when reconsolidation helps or hurts (e.g., MATH vs. Gaokao outcomes)."}, "questions": {"value": "1. The IB narrative relies on reducing $I(X;Z)$ while keeping or improving $I(Z;Y)$; can the authors either add a tractable variational IB objective (e.g., CPC/InfoNCE-style bounds) or present calibrated MI proxies that show the intended movement along the efficiency frontier, rather than only accuracy deltas ?\n2. I suggest the authors include strong latent and cache-operator baselines under matched compute, add non‑math and multilingual tasks, and report multi‑seed CIs and contamination checks to substantiate generalization claims\n3. Can the authors run sensitivity analyses for $k$, $R$, and invocation frequency, and report compute-normalized outcomes to justify overheads. \n4. I am curious, if training from scratch with a joint backbone‑processor objective to test whether the “frozen backbone” constraint is the main limiter, and whether end‑to‑end training reaches better predictive efficiency under equal compute?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pPMKAIJuJ6", "forum": "fWgKnl4itC", "replyto": "fWgKnl4itC", "signatures": ["ICLR.cc/2026/Conference/Submission20022/Reviewer_8HNH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20022/Reviewer_8HNH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915483213, "cdate": 1761915483213, "tmdate": 1762932918385, "mdate": 1762932918385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}