{"id": "eXidGkRUFt", "number": 10058, "cdate": 1758159180763, "mdate": 1759897677789, "content": {"title": "<SO$G_k$>: One LLM Token for Explicit Graph Structural Understanding", "abstract": "Large language models show great potential in unstructured data understanding, but still face significant challenges with graphs due to their structural hallucination. Existing approaches mainly either verbalize graphs into natural language, which leads to excessive token consumption and scattered attention, or transform graphs into trainable continuous embeddings (i.e., soft prompt), but exhibit severe misalignment with original text tokens. To solve this problem, we propose to incorporate one special token <SO$G_k$> \nto fully represent the \\textbf{\\underline{S}}tructure \\textbf{\\underline{O}}f \\textbf{\\underline{G}}raph\nwithin a unified token space, facilitating explicit topology input and structural information sharing. Specifically, we propose a topology-aware structural tokenizer that maps each graph topology into a highly selective single token. Afterwards, we construct a set of hybrid structure Question-Answering corpora to align new structural tokens with existing text tokens. With this approach, <SO$G_k$> empowers LLMs to understand, generate, and reason in a concise and accurate manner. Extensive experiments on five graph-level benchmarks demonstrate the superiority of our method, achieving a performance improvement of 9.9–41.4\\% compared to the baselines while exhibiting interpretability and consistency. Furthermore, our method provides a flexible extension to node-level tasks, enabling both global and local structural understanding. The codebase is publicly available\\footnote{The code of our project is available at \\href{https://anonymous.4open.science/r/SOG-8432}{https://anonymous.4open.science/r/SOG-8432}.}.", "tldr": "", "keywords": ["LLM for Graph", "Graph Structure Learning", "Structure Hallucination"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e20e99d1db1e0905f9f37d9d0f2b61cdc445f53c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To address the structural hallucination problem of LLMs)on graph-structured data—where existing Graph-to-Text approaches suffer from high token consumption and attention dilution, and Graph-to-Embedding methods face cross-modal misalignment—this work proposes a novel solution: introducing a **single special token** `<SOGₖ>` . A topology-aware graph tokenizer maps the entire graph topology into a highly selective discrete token from a learnable structural vocabulary of size K, which is then injected into the LLM input sequence, forming a hybrid architecture that blends structural and textual modalities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Innovative and elegant design**: Representing an entire graph via a single virtual global node mapped to a vocabulary index is conceptually clean and computationally efficient.\n- **Unified representation space**: By training on hybrid QA corpora (including k-NN matching, true/false judgment, and description alignment tasks), `<SOGₖ>` is embedded in the **same semantic space** as native LLM text tokens, effectively eliminating the modality gap inherent in Graph-to-Embedding methods.\n- **High efficiency and strong performance**: The approach achieves competitive or superior results with minimal token overhead (just +1 token) and negligible parameter cost.\n- **Interpretability**: The paper includes meaningful analysis showing that each `<SOGₖ>` token occupies a distinct region in the embedding space, suggesting meaningful structural encoding."}, "weaknesses": {"value": "- **Insufficient related work discussion**: To the best of my knowledge, similar ideas have appeared in prior literature(1) *Multi-View Empowered Structural Graph Wordification for Language Models*, which also uses discrete tokens to represent global graph structure; (2) *LangTopo*\n  The paper does not adequately differentiate itself from these works.\n- **Lack of ablation studies**: The proposed “topology-aware graph tokenizer” comprises three key components—(i) anchor node selection (via node degree), (ii) virtual global node , and (iii) self-supervised topological reconstruction—but none are ablated independently to assess their individual contributions.\n- **Missing hyperparameter sensitivity analysis**: The structural vocabulary size K is fixed at 256 without exploring alternatives (e.g., K=64, 128, 512). It remains unclear whether performance saturates, degrades, or improves with larger/smaller K, raising concerns about robustness and scalability."}, "questions": {"value": "1. **Multi-token extension**: The single-token design is indeed attractive. However, has the authors explored using **multiple structural tokens** (e.g., 2–5 tokens) to capture richer or hierarchical graph properties? Could this further improve performance on complex graphs?\n\n2. **Relation to prior work**: How does this method fundamentally differ from *LangTopo* and *Dr.E*? Is the key novelty the **end-to-end joint training with LLMs in a shared token space**, or the **specific topology-aware tokenizer design**? Clarifying this distinction would strengthen the paper’s positioning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IKq08HXoV5", "forum": "eXidGkRUFt", "replyto": "eXidGkRUFt", "signatures": ["ICLR.cc/2026/Conference/Submission10058/Reviewer_g287"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10058/Reviewer_g287"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761417142525, "cdate": 1761417142525, "tmdate": 1762921453723, "mdate": 1762921453723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method that maps each graph’s topology to a single discrete structural token <SOG_k> that can be directly fed into an LLM, avoiding long graph-to-text encodings.\nIt builds these tokens through a topology-aware tokenizer and aligns them with the LLM’s embedding space using structure-based QA fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- One discrete token injects topology into the LLM, drastically reducing prompt length vs graph-to-text.\n\n- Large improvements on multiple MoleculeNet tasks with clear ablations.\n\n- Interpretability analysis is insightful.\n\n- Works with off-the-shelf LLMs (LoRA + token embeddings), no architectural changes to the base model."}, "weaknesses": {"value": "- It remains unclear whether this method scales robustly to heterogeneous or large real-world graphs (e.g., knowledge graphs, citation networks, or synthetic structural datasets).\n\n- The approach is not novel. Some papers [1,2,3,4] have already provided similar approaches. Moreover these baselines are missing.\n\n- Permutation/anchor bias: Tokenizer choices (anchor selection, hop labeling) may introduce ordering biases; robustness to isomorphisms not rigorously tested.\n\n- Not generalizable. Requires fine tuning for each new dataset.\n\n[1] Perozzi, Bryan, et al. \"Let your graph do the talking: Encoding structured data for llms.\" arXiv preprint arXiv:2402.05862 (2024).\n\n[2] Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, and Zhangyang Wang. Llaga: Large language and\ngraph assistant. arXiv preprint arXiv:2402.08170, 2024b.\n\n[3] Chai, Ziwei, et al. \"Graphllm: Boosting graph reasoning ability of large language model.\" arXiv preprint arXiv:2310.05845 (2023).\n\n[4] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson,\nand Bryan Hooi. G-retriever: Retrieval-augmented generation for textual graph understanding and\nquestion answering. arXiv preprint arXiv:2402.07630, 2024"}, "questions": {"value": "- Generalization: How does SOGk perform on non-molecular benchmarks (e.g., social/citation/synthetic graphs) and on node- or link- level tasks?\n\n- Invariance: Which parts of the tokenizer are provably permutation-invariant? What breaks if node ordering or anchor selection is adversarially perturbed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "acE5clunMG", "forum": "eXidGkRUFt", "replyto": "eXidGkRUFt", "signatures": ["ICLR.cc/2026/Conference/Submission10058/Reviewer_tD9C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10058/Reviewer_tD9C"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963734268, "cdate": 1761963734268, "tmdate": 1762921453434, "mdate": 1762921453434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to use a special token <SOG_k> to represent the entire topology of a graph, so that LLM can use that token to make predictions. The method builds a structural tokenizer using GNNs and quantizes the representation into one entry. In addition, the authors construct hybrid structure QA corpora to align the new structural tokens with text tokens using LoRA. On five MoleculeNet-style graph-level classification datasets, their method improves over zero-shot/few-shot/LoRA/soft-prompt LLaMA baselines by a large margin."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem is clearly defined with proper related works.\n2. Using one token for each graph is a convincing idea.\n3. The overall tokenizer pipeline is clear and meaningful.\n3. The authors show strong empirical gains on 3B and 7B models."}, "weaknesses": {"value": "1. All main experiments are on small to moderate molecular graphs, where graph sizes and motif variety are relatively small. It’s unclear how this scales to large real-world graphs.\n2. Compressing a large graph into a single token may lose information.\n3. I did not see the training cost analysis.\n4. Hyperparameter sweeping for the baseline is not reported."}, "questions": {"value": "1. What is the exact size, structural diversity, or negative-sampling strategy of those three QA corpora?\n2. Is there any out-of-vocabulary structure case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZDTTIOdSs3", "forum": "eXidGkRUFt", "replyto": "eXidGkRUFt", "signatures": ["ICLR.cc/2026/Conference/Submission10058/Reviewer_qiqx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10058/Reviewer_qiqx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967239428, "cdate": 1761967239428, "tmdate": 1762921453082, "mdate": 1762921453082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "As LLMs are facing challenges with graphs because of their structural hallucination. Existing approaches have their apparent shortages. This paper proposes a new method that insert a special token called  <SOGk > to represent the structure of graph better. In this paper, the author proposes a special tokenizer to generate a highly selective token based on graph topology, then align the new <SOGk > token with the existing tokens by a set of hybrid structure Question-Answering corpora. The experiments results show it significantly helps LLMs to understand graph datasets better."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper innovatively creates a new method in the graph recognition of large language models (LLMs). Unlike previous methods, which usually convert the entire graph data into another format, the Structure - Oriented Graph (SOG) provides a novel way for LLMs to understand textual graphs through a special token that reflects the overall structure of the graph.\n\nThe advantage of SOG is that it uses a Graph Neural Network (GNN) as an encoder to obtain deeper information within the graph data. To address the drawbacks in converting continuous embeddings, SOG maps the continuous representations into a K - discrete token vocabulary, which is helpful in solving misalignment. It seems to be quite original."}, "weaknesses": {"value": "Method proof missing: This paper only elaborates the main idea of SOG but lacks some basic proof regarding how this method emerged. From an objective perspective, some basic algorithms need to be listed to prove how the components of SOG evidently improve the accuracy and efficiency in graph learning.\n\nThe Hybrid QA method is not always the best way to align a new single token with existing tokens. The chart in this paper shows that in some cases (BBBP, ClinTox), Hybrid QAs are slightly worse than KNN matching alone. This indicates that SOG still needs to find a better way to align its structural token with other tokens on more complex datasets, especially when fine - tuning with description - token."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uN5IX6F9aF", "forum": "eXidGkRUFt", "replyto": "eXidGkRUFt", "signatures": ["ICLR.cc/2026/Conference/Submission10058/Reviewer_kfpk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10058/Reviewer_kfpk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762151140980, "cdate": 1762151140980, "tmdate": 1762921452784, "mdate": 1762921452784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}