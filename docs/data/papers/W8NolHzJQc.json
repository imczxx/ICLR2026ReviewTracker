{"id": "W8NolHzJQc", "number": 15236, "cdate": 1758249195861, "mdate": 1759897319548, "content": {"title": "Swift-FedGNN: Federated Graph Learning with Low Communication and Sample Complexities", "abstract": "Graph neural networks (GNNs) have achieved great success in a wide variety of graph-based learning applications.\nWhile distributed GNN training with sampling-based mini-batches expedites learning on large graphs, it is not applicable to geo-distributed data that must remain on-site to preserve privacy.\nOn the other hand, federated learning (FL) has been widely used to enable privacy-preserving training under data parallelism.\nHowever, applying FL directly to GNNs either results in cross-client neighbor information loss or incurs expensive cross-client neighbor sampling and communication costs due to the large graph size and the dependencies between nodes among different clients. \nTo overcome these challenges, we propose a new federated graph learning (FGL) algorithmic framework called Swift-FedGNN that primarily performs efficient parallel local training and periodically conducts cross-client training.\nSpecifically, in Swift-FedGNN, each client *primarily* trains a local GNN model using only its local graph data, and some randomly sampled clients *periodically* learn the local GNN models based on their local graph data and the dependent nodes across clients.\nWe theoretically establish the convergence performance of Swift-FedGNN and show that it enjoys a convergence rate of $\\mathcal{O}\\left( T^{-1/2} \\right)$, matching the state-of-the-art (SOTA) rate of sampling-based GNN methods, despite operating in the challenging FL setting.\nExtensive experiments on real-world datasets show that Swift-FedGNN significantly outperforms the SOTA FGL approaches in terms of efficiency, while achieving comparable accuracy.", "tldr": "", "keywords": ["federated learning", "graph neural network", "optimization", "communication efficiency"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8dc706e47473b7ac0ac434796eaec4993697b135.pdf", "supplementary_material": "/attachment/0210d5a8ece1ed610375e470e0ceda34c6c90e4f.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Swift-FedGNN, a novel federated graph learning (FGL) framework designed to address the high communication and sampling costs inherent in training graph neural networks (GNNs) on geo-distributed, privacy-sensitive graph data. The core contribution is a hybrid training strategy that primarily relies on efficient parallel local training, punctuated by periodic, time-consuming cross-client training performed only on a randomly sampled subset of clients. To minimize communication and enhance privacy, Swift-FedGNN offloads the aggregation of intermediate node features from remote clients to the central server, thereby avoiding direct client-to-client exchange of raw graph features. The authors provide a rigorous theoretical convergence analysis, demonstrating that Swift-FedGNN achieves an $O(T^{-1/2})$ convergence rate, which matches state-of-the-art sampling-based GNN methods despite the challenging federated setting and the presence of biased stochastic gradients (whose approximation errors are bounded for the first time in this context). Extensive experiments on large-scale real-world datasets show that the proposed method significantly outperforms existing SOTA FGL baselines in efficiency, achieving up to 4x speed-up and a substantially lower total communication cost while maintaining competitive accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This approach constitutes a significant and non-trivial technical modification to existing DGL and FGL paradigms.\n\n2. The quality of this research is high in both its theoretical and empirical contributions.\n\n3. This work holds substantial practical significance as it addresses the key barrier to deploying large-scale GNN training in privacy-sensitive and resource-constrained geo-distributed environments: high communication and sampling complexities."}, "weaknesses": {"value": "1. The paper's main claim rests on successfully balancing communication overhead and information loss, controlled by $I$ and $K$. Although Figures 7a and 7b show the impact of $I$ and $K$ on the computation-to-communication time ratio, they completely lack the corresponding analysis for the final validation accuracy. This omission prevents a full assessment of the accuracy degradation trade-off for efficiency gains, making it impossible to determine the optimal operating point for $I$ and $K$. Thus, the experiments are insufficient to fully support the claim of \"preserving minimal information loss.\"\n\n2. Analysis of Performance-Efficiency Trade-off for $I$ and $K$: The paper discusses the relationship between the correction frequency $I$ and the number of sampled cross-client training clients $K$ with the computation-to-communication ratio (Figure 7a, 7b), but it does not show their impact on the final model validation accuracy. Please provide experimental results showing the sensitivity of the validation accuracy as a function of $I$ and $K$ (e.g., plots of accuracy versus communication cost or wall-clock time for varying $(I, K)$ pairs). This is crucial for fully assessing the method's core trade-off and for guiding optimal hyperparameter settings in practice."}, "questions": {"value": "Please see the above weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9t5QVqIRrE", "forum": "W8NolHzJQc", "replyto": "W8NolHzJQc", "signatures": ["ICLR.cc/2026/Conference/Submission15236/Reviewer_qPZs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15236/Reviewer_qPZs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761701914122, "cdate": 1761701914122, "tmdate": 1762925533251, "mdate": 1762925533251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Swift-FedGNN, a new framework for federated graph learning that achieves efficient training on geo-distributed graphs while reducing both communication and sample complexities. The key idea is to perform frequent local updates on each client and periodic cross-client aggregation on a subset of sampled clients.\nThe paper provides a theoretical convergence guarantee under biased stochastic gradients (Theorem 5.6), and comprehensive experiments on large-scale benchmarks demonstrate faster convergence and lower communication cost compared with strong baselines such as FedSage, FedGNN-G, and LLCG. Overall, the manuscript is technically sound, well-structured, and addresses a practically relevant problem in federated graph learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a concrete and under-studied challenge: how to train GNNs efficiently when graph data are distributed across multiple institutions and cannot be shared due to privacy regulations.\n2. Swift-FedGNN combines parallel local training with periodic cross-client aggregation. Randomly sampled clients share only aggregated neighbor embeddings rather than raw node features. Additionally, this work rigorously analyzes convergence under biased stochastic gradients.\n3. Experiments on multiple datasets (Reddit, OGBN-Products, Cora, Citeseer) show that Swift-FedGNN achieves up to 4× speed-up and an order-of-magnitude reduction in communication with competitive accuracy."}, "weaknesses": {"value": "1. Although the paper only shares aggregated embeddings, could the authors provide further analysis or experiments to verify that these embeddings cannot be reconstructed or used to infer sensitive information?\n2. The communication and computation complexity is currently demonstrated only empirically. It is recommended that the authors provide asymptotic expressions as functions of the number of clients (M), sampled clients (K), communication interval (I), and fan-out (F) to enhance the theoretical completeness of the paper. (*I understand this may be challenging, but I hope the authors can improve this aspect as much as possible. This will not affect my decision, as empirical validation already provides partial support.*)\n3. The offloading mechanism only supports element-wise aggregation (mean/sum/max), and attention-based or heterogeneous GNNs requiring non-element-wise operations (e.g., GAT, HAN) are not discussed."}, "questions": {"value": "1. Could aggregated embeddings still leak information under small client sizes or sparse features? Any empirical test or mitigation (e.g., DP noise)?\n2. What happens when M ≫ 20 clients? Does server-side aggregation become a bottleneck?\n3. What are realistic magnitudes of constants such as $B_{\\Delta G}$ in Theorem 5.6, and do empirical error curves align with theoretical rates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical concerns detected."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HF0nXRSrHi", "forum": "W8NolHzJQc", "replyto": "W8NolHzJQc", "signatures": ["ICLR.cc/2026/Conference/Submission15236/Reviewer_uhnB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15236/Reviewer_uhnB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926113987, "cdate": 1761926113987, "tmdate": 1762925532966, "mdate": 1762925532966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Swift-FedGNN, a communication- and sample-efficient framework for federated graph neural network (GNN) training on geo-distributed graphs where data privacy prevents direct feature sharing among clients. The method primarily performs efficient local training on each client and periodically selects a subset of clients to conduct cross-client training that accounts for dependencies between nodes across clients. By offloading partial aggregation operations to remote clients and the central server, Swift-FedGNN mitigates the information loss of cross-client neighbors while substantially reducing communication and memory overhead. The authors provide a rigorous theoretical convergence analysis, proving that Swift-FedGNN's convergence rate. Extensive experiments on large-scale real-world datasets demonstrate that Swift-FedGNN achieves comparable accuracy to full cross-client training baselines while significantly improving efficiency in terms of training time and communication cost."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important and timely problem in federated graph learning by aiming to reduce communication and sampling overhead while preserving accuracy under privacy constraints. It provides a clear problem formulation and a well-structured methodological design combining local training and periodic cross-client aggregation. The theoretical analysis is good and extends convergence result to a biased-gradient federated GNN setting."}, "weaknesses": {"value": "The paper’s main weakness lies in its limited novelty—while the proposed framework is well-motivated, it largely combines known techniques from federated learning and distributed GNN training, such as periodic communication and hierarchical aggregation, without introducing a clearly new conceptual or algorithmic component. The idea of alternating between local and cross-client updates has been explored in several prior works on communication-efficient federated optimization and graph learning, and the paper does not convincingly demonstrate how Swift-FedGNN differs fundamentally or advances beyond these methods. Moreover, the experimental evaluation, though conducted on two standard benchmarks, is not sufficiently comprehensive to support the claimed generality or superiority. The set of baselines is limited, omitting comparisons with stronger or more recent federated GNN approaches that also target efficiency and scalability. The ablation and sensitivity analyses are relatively shallow, leaving unclear how different design choices impact performance in heterogeneous settings. Finally, the paper’s writing and presentation could be improved for clarity and precision, as some methodological descriptions are verbose yet lack formal rigor or intuitive explanations, which diminishes the overall readability and perceived contribution."}, "questions": {"value": "1. The paper claims to achieve low communication and sampling complexity, but the analysis remains largely empirical. A clearer quantitative or theoretical comparison of communication cost with existing methods such as FedSage+ or FedGNN-PNS would strengthen the claim.\n\n2. The periodic cross-client training strategy relies on fixed intervals and random client subsets, which appear heuristic. The sensitivity of performance to these design choices and whether adaptive or data-driven scheduling could offer improvements remain unclear.\n\n3. The experimental evaluation is limited in scope. Additional benchmarks and comparisons with more recent or stronger federated GNN baselines would help demonstrate the robustness and generality of the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G6mIykBYFB", "forum": "W8NolHzJQc", "replyto": "W8NolHzJQc", "signatures": ["ICLR.cc/2026/Conference/Submission15236/Reviewer_uxRE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15236/Reviewer_uxRE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940412235, "cdate": 1761940412235, "tmdate": 1762925532524, "mdate": 1762925532524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Swift-FedGNN, a federated learning framework for graph neural networks that minimizes sampling and communication overhead in geo-distributed data. It performs local parallel training on each client and periodically conducts cross-client aggregation among a random subset of clients. Remote clients and the server collaboratively pre-aggregate neighbor embeddings so that only aggregated features, not raw data, are exchanged—reducing bandwidth and preserving privacy. The authors prove an $O(T^{-1/2})$ convergence rate matching state-of-the-art GNN methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. This paper targets a relevant and challenging setting where graph data are geo‑distributed with cross‑client dependencies.\n\nS2. The proposed Swift-FedGNN has shown good convergence rates comparable to state-of-the-art methods theoretically and empirically.\n\nS3. Experiments on five benchmarks show up to 4× faster convergence and 21× lower communication cost than existing FGL baselines while maintaining comparable accuracy."}, "weaknesses": {"value": "W1. This paper’s privacy story is incomplete. The approach assumes a trusted server and, for non-element-wise operators (e.g., GAT), raw features or intermediate activations may need to be sent to the server, which undermines the “aggregated-only” guarantee and weakens privacy claims.\n\nW2. The convergence analysis is proved for GCN, whereas the experiments emphasize GraphSAGE/GIN. The paper does not establish conditions under which the analysis extends to these architectures. Moreover, the error neighborhood explicitly grows with depth $L$, but the paper offers no practical prescription for training deeper models without degrading guarantees.\n\nW3. The empirical comparison is limited to three or four methods. Important FGL variants—such as SpreadGNN and FedGCN—are not included in the core efficiency comparisons, despite being cited.\n\nW4. The method section largely describes a clean but straightforward system design. Without a stronger learning-algorithm contribution (e.g., new estimators or privacy mechanisms), the work can read as primarily an engineering improvement.\n\n\n\nMinor typos: \"Communicaiton\" -> \"Communication\" in Figs 6 and 7"}, "questions": {"value": "Q1. The background section cites the Microsoft Academic Graph (MAG, 100 M nodes) as an example of large-scale, geo-distributed graphs, yet experiments stop at ogbn-products (2.4 M nodes). Could the authors clarify why MAG or an equivalently large dataset was not used? Was this due to memory limits, partitioning complexity, or cross-client sampling constraints?\n\nQ2. The main theoretical analysis explicitly targets GCNs. Can the authors discuss whether and how this framework extends to GraphSAGE, GIN, or attention-based models (e.g., GAT)? In particular, how would the error neighborhood term scale with network depth $L$ and non-linear aggregation functions?\n\nQ3. Since the theory demonstrates that gradient bias increases with depth $L$, have the authors observed this empirically? For example, does validation accuracy degrade more rapidly for deeper architectures under the same $I,K$ settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RCGXXgVDDz", "forum": "W8NolHzJQc", "replyto": "W8NolHzJQc", "signatures": ["ICLR.cc/2026/Conference/Submission15236/Reviewer_MwMc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15236/Reviewer_MwMc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970768013, "cdate": 1761970768013, "tmdate": 1762925532200, "mdate": 1762925532200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Swift-FedGNN, a federated graph learning framework addressing cross-client neighbor dependencies and high communication costs in GNN training. Key contributions: periodic cross-client training for sampled clients (most iterations use efficient local training), aggregated neighbor feature exchange, and rigorous theoretical analysis. Experiments on real-world datasets show Swift-FedGNN achieves competitive accuracy while reducing communication overhead significantly compared to baselines, offering an efficient solution for privacy-preserving federated graph learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Innovative Dynamic Training Framework Balances Efficiency and Accuracy\nThe paper proposes a \"local training-dominated, periodic cross-client training-supplemented\" mechanism: clients mainly perform parallel local training to reduce overhead, and only randomly sample a subset of clients for cross-client training at intervals to avoid information loss. This design addresses the inefficiency of traditional FedGNN methods and the accuracy degradation from ignoring cross-client neighbors, achieving a rational trade-off between efficiency and performance.  \n\n2. Rigorous Theoretical Analysis Breaks Traditional Assumptions\nUnlike existing FedGNN works that rely on strong assumptions for convergence analysis, this paper directly bounds the approximation error of biased stochastic gradients in GNNs, a unique challenge in federated graph learning, and reveals the positive correlation between gradient bias and network depth. It further proves Swift-FedGNN achieves an $O(T^{-1/2})$ convergence rate, matching the SOTA of centralized sampling-based GNNs in complex federated scenarios, providing solid theoretical support.  \n\n3. Notable Experimental Efficiency Advantages \nExtensive experiments on real-world datasets show Swift-FedGNN outperforms baselines in efficiency: it reduces communication overhead by 7-21x compared to FedGNN-G on 4/5 datasets, and completes cross-client training in <200ms on ogbn-products. Meanwhile, its validation accuracy is comparable to FedGNN-G, verifying the method’s practical value."}, "weaknesses": {"value": "1. Lack of Validation for GAT, a Widely Used GNN\nThe paper only validates performance on GraphSAGE and GIN. For GAT, it merely notes \"remote clients need to transmit raw features for aggregation\" but provides no experimental data. It fails to discuss whether raw feature transmission undermines the \"aggregation-driven overhead reduction\" advantage or how accuracy is affected, limiting generality for complex GNN architectures.  \n\n2. Insufficient Discussion on Key Hyperparameters\nWhile the paper shows the trend of hyperparameters on performance, it lacks in-depth analysis of critical issues: e.g., optimal I/K criteria across datasets, the adaptive ratio of K to total client count M, and robustness to small parameter fluctuations. This hinders practical parameter tuning and weakens reproducibility.  \n\n3. Narrow Experimental Scenarios \nExperiments only use 10-20 clients and do not test large-scale scenarios, leaving uncertainty about scheduling overhead and server latency at scale. Additionally, data partitioning is limited to METIS-balanced and random modes, ignoring typical non-IID scenarios in federated learning. No analysis of data imbalance’s impact on training limits conclusion generalizability."}, "questions": {"value": "1. For GAT, do you have specific designs to adapt the \"two-layer aggregation\" mechanism? If raw features must be transmitted, what are the expected communication overhead and accuracy loss, and are there plans for supplementary experiments?  \n\n2. Non-IID data is a core federated learning challenge. Do you have preliminary ideas to optimize Swift-FedGNN for non-IID scenarios? Are there plans to supplement related experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4B5J5aWGGJ", "forum": "W8NolHzJQc", "replyto": "W8NolHzJQc", "signatures": ["ICLR.cc/2026/Conference/Submission15236/Reviewer_Aug9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15236/Reviewer_Aug9"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980197876, "cdate": 1761980197876, "tmdate": 1762925531635, "mdate": 1762925531635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles federated graph learning (FGL) on geo-distributed graphs where nodes and edges are partitioned across clients, causing severe cross-client neighbor dependence. Existing methods either ignore these dependencies (hurting accuracy) or fully synchronize them (hurting efficiency and privacy).\nThe authors propose Swift-FedGNN, a mini-batch-based federated GNN algorithm that performs mostly local training and occasionally triggers cross-client training among a sampled subset of clients. In those rounds, each remote client aggregates its own neighbor embeddings and sends only aggregated representations to the server, which re-aggregates and forwards them to the requesting client—avoiding raw data exchange.\n\nThey provide a non-trivial convergence analysis without assuming unbiased gradients, bounding the bias introduced by neighbor sampling and missing cross-client neighbors. The algorithm achieves an $O(T^{-0.5})$ convergence rate matching single-machine sampling-based GNNs. Extensive experiments on five benchmarks (ogbn-products, Reddit, arxiv, flickr, citeseer) show that Swift-FedGNN attains comparable accuracy with 4× speed-up and 7–21× lower communication cost compared to strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear motivation and relevance. The paper addresses a real bottleneck in FGL—communication-heavy cross-client neighbor access—through an elegant and realistic training schedule.\n\nAlgorithmic novelty. The two-stage neighbor aggregation and selective cross-client correction provide a principled and privacy-aware way to integrate distributed neighborhood information.\n\nStrong theoretical grounding. The convergence proof relaxes the common unbiased-gradient assumption, offering bounds that explicitly model bias growth with GNN depth—a fresh theoretical contribution.\n\nEmpirical thoroughness. Evaluations cover multiple datasets, two architectures (GraphSAGE, GIN), partition types, and hyperparameter sensitivity. The improvements in both communication efficiency and convergence speed are significant.\n\nWell-balanced trade-off analysis. The theory and experiments together demonstrate a tunable trade-off between communication frequency $(I, K)$  and residual information loss."}, "weaknesses": {"value": "Limited novelty in aggregation design. While efficient, the idea of “aggregate-then-transfer” resembles known hierarchical aggregation patterns; the real novelty lies more in the federated scheduling and analysis than in the aggregation itself.\n\nEvaluation scope. Although the method is shown on standard benchmarks, all datasets are homophilic. Performance on heterophilic or highly dynamic graphs (e.g., temporal, relational) is unknown.\n\nPrivacy and security claims. The paper asserts improved privacy due to aggregated communication, but lacks formal guarantees (e.g., differential privacy or leakage analysis).\n\nHyperparameter sensitivity. The method depends on correction frequency $I$ and sampling size $K$; although tested, guidance for tuning them across heterogeneous clients is limited."}, "questions": {"value": "Gradient bias quantification: Can the authors provide empirical evidence (e.g., gradient variance plots) to support the theoretical claim that stochastic-gradient bias grows with GNN depth?\n\nGeneralization to heterophilic graphs: How would Swift-FedGNN behave on datasets where cross-client neighbors convey opposing label information?\n\nScalability in extreme settings: What happens when the number of clients $M$ is very large (e.g., hundreds)? Does the sampling of $K$ clients still ensure stable convergence?\n\nPrivacy leakage analysis: Even though raw features are not shared, could aggregated embeddings leak information about rare node attributes?\n\nApplicability beyond node classification: Can the same framework handle link prediction or graph-level tasks where labels are not node-based?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f08YBGrPCP", "forum": "W8NolHzJQc", "replyto": "W8NolHzJQc", "signatures": ["ICLR.cc/2026/Conference/Submission15236/Reviewer_8oJB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15236/Reviewer_8oJB"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission15236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007475698, "cdate": 1762007475698, "tmdate": 1762925531278, "mdate": 1762925531278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}