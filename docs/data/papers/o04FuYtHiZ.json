{"id": "o04FuYtHiZ", "number": 1821, "cdate": 1756945786756, "mdate": 1759898183860, "content": {"title": "ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Dynamic Videos", "abstract": "Spatio-temporal reasoning is essential for understanding real-world environments in various fields, $\\textit{e.g.}$, autonomous driving and sports analytics. While recent advances have strengthened the spatial reasoning abilities of Vision-Language Models (VLMs) through large-scale training data, these models still struggle with kinematic aspects such as traveled distance and speed of moving objects. To bridge this gap, we construct a spatio-temporal reasoning dataset and benchmark for kinematic instruction tuning, referred to as $\\textbf{STKit}$ and $\\textbf{STKit-Bench}$. They consist of real-world videos with 3D annotations that capture object motion dynamics, including traveled distance, speed, movement direction, inter-object distance comparisons, and relative movement direction. To further scale data construction to videos without 3D annotations, we propose an automatic pipeline for generating pseudo-labels via 4D reconstruction at a real-world scale. Building on this kinematic instruction tuning data, we introduce $\\textbf{ST-VLM}$, a VLM enhanced for spatio-temporal reasoning, which achieves strong performance on STKit-Bench. Moreover, ST-VLM generalizes robustly across diverse domains and tasks, outperforming baselines on comprehensive spatio-temporal reasoning benchmarks. Finally, by integrating learned spatio-temporal reasoning with existing abilities, ST-VLM enables complex multi-step reasoning grounded in kinematics.", "tldr": "", "keywords": ["spatio-temporal reasoning", "vision-language models", "kinematic instructions"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f2d43199909a36b834ff6a4a8b0732a36753b08.pdf", "supplementary_material": "/attachment/126771565ebb4a156d818db20df7411bc065f1a0.zip"}, "replies": [{"content": {"summary": {"value": "The paper describes a new vision dataset, that was generated synthetically by augmenting existing datasets with 3D object position and movement information (such as the objects’ movement directions, speeds, etc.). It is based in part on existing datasets that include this type of information in some form and in part on the use of 4D reconstruction methods to video datasets where such information is not included.  The resulting dataset is used to finetune a vision-language model, which performs better at predicting this kind of information than models that were not fine-tuned on this kind of data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a novel dataset. The proposed dataset does what it promises to do. It enables a model finetuned on it to perform comparably well at the task of predicting speeds, directions, etc. of objects shown in the video."}, "weaknesses": {"value": "The dataset focuses on domains like sports and driving, where object position and motion information can be readily extracted. This raises the question what the benefit of distilling this information into an existing vision language model is, instead of relying on existing approaches which would use an external tool that simply extracts such information at inference time. While this question is answered partly in section 6.3 on emergent capabilities, that section is very preliminary and highlights just a few examples qualitatively. \n\nThe method seems to rely on relatively clean scenarios that are free from occlusions, camera motions.  \n\nThe task of predicting speed is inherently ill-posed, suggesting that applying the model to scenarios that are slightly out of the ordinary with respect to object geometry or size, for example showing remote controlled toy cars, would make the model give drastically incorrect results? It would be good to better understand the limitations of this approach considering this."}, "questions": {"value": "Will the dataset and model be publicly released? \n\nHow would the performance of the model vary if the framerate of a video is varied? Is robustness to such variations not important, especially when considering this as a foundational skill of a vision-language model? \n\nWhat about extraordinary scenarios, like videos showing toy cars mentioned above? \n\n(minor) The figures (especially Figure 1 and 2) are good for on-screen viewing but kind of hard to see in a printout."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hHcfd24YTL", "forum": "o04FuYtHiZ", "replyto": "o04FuYtHiZ", "signatures": ["ICLR.cc/2026/Conference/Submission1821/Reviewer_Lsay"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1821/Reviewer_Lsay"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761684438451, "cdate": 1761684438451, "tmdate": 1762915900205, "mdate": 1762915900205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is doing the task that let the vlm understand kinematic spatio-temporal reasoning, such as an object's speed or traveled distance. The authors introduce ST-VLM, a new model trained via kinematic instruction tuning. To enable this, they create the STKit dataset, which contains video-based questions about speed, distance, and movement direction. They also introduce novel pseudo-labeling pipeline that uses 4D reconstruction to generate this kinematic data from unlabeled videos. On STKit-Bench, ST-VLM substantially outperforms baselines like GPT-5 and demonstrates emergent multi-step reasoning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well writing with clear logic and good motivation.\n- The three-stage filtering strategy (rule-based, general model-based, and task-specific model-based) is comprehensive. It demonstrably improves the quality of pseudo-labels.\n- The ablation in Table 6 clearly validates the authors' design choices, showing that both the 3D ground-truth data and the filtered pseudo-label data contribute significantly to the final model's performance.\n- Experiment and dataset building method are detailed."}, "weaknesses": {"value": "- The pseudo-label is not accurate. While the filtering pipeline is effective, a 29% mean error rate remains in the pseudo-labeled traveled distance data.\n- See in questions."}, "questions": {"value": "- How about using data generated in simulator? What is the advantage of simulator data with the 4D model annotated data? In my view is that the 4D model annotated data may not accurate but the video distribution is from the real data, but the simulator data is accurate but is not photo realistic. \n- Recently some work also using vlm do depth estimation such as DepthLM[1]. I am wondering what is their method different with this paper(and I think ST-VLM also can do depth estimation). I am wondering if the authors could provide the results of ST-VLM on depth estimation tasks.\n- Open source plan?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yf0ZPjQA7h", "forum": "o04FuYtHiZ", "replyto": "o04FuYtHiZ", "signatures": ["ICLR.cc/2026/Conference/Submission1821/Reviewer_8N6h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1821/Reviewer_8N6h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877658464, "cdate": 1761877658464, "tmdate": 1762915899667, "mdate": 1762915899667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ST-VLM, a vision-language model enhanced for spatio-temporal reasoning through kinematic instruction tuning using the STKit dataset and STKit-Bench benchmark. It aims to address a key limitation of existing VLMs -- poor handling of dynamic object kinematics -- by leveraging datasets with 3D and 4D information. The paper includes extensive evaluation of state of the art models on STKit-Bench and the ST-VLM shows promising signs of improved spatio-temporal understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed STKIT dataset and benchmark is novel and interesting.\n\n* The paper shows that fine-tuning on the proposed STKIT dataset leads to improved performance over the plan LLaVA-OneVision-7B model on PerceptionTest, MVBench, VideoMME, MLVU, NExT-QA. Which shows improved spatio-temporal understanding.\n\n* The paper includes extensive evaluation of state of the art models on STKit-Bench."}, "weaknesses": {"value": "* The proposed STKIT-BENCH includes a vast majority of questions (92.9%) from the autonomous diving domain. This limits the diversity of the benchmark. Due to the heavy reliance on the autonomous driving domain, the paper should include comparison to popular benchmarks such as \"DriveLM: Driving with Graph Visual Question Answering, ECCV 2024\".\n\n* For 3D understanding of street scenes and answering kinematic questions on distances, it is critical to have access the camera parameters. The proposed model is only able to perform well because the STKIT dataset and benchmark uses the same datasets: Nuscenes and Argoverse. Training on these datasets allows the model to memorize the camera parameters and thus perform well on the evaluation data. The STKIT-BENCH should include datasets such as Cityscapes for true zero-shot evaluation.\n\n* The evaluation in Table 7 and Table 8 should include state of the art approaches such as VideoLLAMA3 and Qwen-2.5-VL.\n\n* The paper should discuss prior work on grounding to fine-grained spatio-temporal visual information in videos such as: \"Look, Remember and Reason: Grounded reasoning in videos with language models, ICLR 2024\"; \"Fine-grained Spatiotemporal Grounding on Egocentric Videos, ICCV 2025\"."}, "questions": {"value": "* The choice of data sources for STKIT-BENCH should be motivated in more detail.\n* The effect of the overlap between STKIT dataset and benchmark should be explain in more detail.\n* Prior work in grounding to spatio-temporal visual information in videos should be explain in more detail."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yLVhIsoSF8", "forum": "o04FuYtHiZ", "replyto": "o04FuYtHiZ", "signatures": ["ICLR.cc/2026/Conference/Submission1821/Reviewer_AMF5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1821/Reviewer_AMF5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762028400916, "cdate": 1762028400916, "tmdate": 1762915899449, "mdate": 1762915899449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to strengthen the spatio-temporal reasoning capability of current VLMs. To this end, the authors introduce STKit, a large-scale dataset with seven types of motion-related question–answer pairs generated through a 4D reconstruction and pseudo-labeling pipeline, and STKit-Bench, a benchmark for quantitative evaluation. By tuning LLaVA-OneVision with these instructions, it achieves performance improvement on STKit-Bench and video understanding benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is well-written.\n2.\tI appreciate the authors’ efforts to curate data for spatio-temporal reasoning.\n3.\tThe results are promising after finetuning the baseline with the curated data."}, "weaknesses": {"value": "1.\tFor STKit-Bench, how to ensure annotation quality, especially annotation related to the distance annotation? Meanwhile, how to ensure its diversity to cover the real-world scenes?\n2.\tBoth training data and testing data come from the same data annotation pipeline. How could the authors deal with domain overlap?\n3.\tThe method does not explicitly model physical kinematics. Therefore, “kinematic instruction tuning” may not be a good description.\n4.\tCould the authors show some evidence that the models learns the spatio-temporal reasoning rather than static correlations?\n5. Will all curated datasets/benchmarks be public to the community?"}, "questions": {"value": "See the questions in Weakness part."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5dZ0CuAwIk", "forum": "o04FuYtHiZ", "replyto": "o04FuYtHiZ", "signatures": ["ICLR.cc/2026/Conference/Submission1821/Reviewer_hL4E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1821/Reviewer_hL4E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065054541, "cdate": 1762065054541, "tmdate": 1762915899304, "mdate": 1762915899304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}