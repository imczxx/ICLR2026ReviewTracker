{"id": "ymgbE674Ay", "number": 19523, "cdate": 1758296950316, "mdate": 1759897034702, "content": {"title": "MSTformer: Multiscale Spatiotemporal Motion-aware Transformer Network for Effective AI-Generated Video Detection", "abstract": "Recent AI-generated videos (e.g., Veo3) are growing increasingly realistic and indistinguishable from real videos. Current existing detectors usually rely on artifacts present in earlier or inferior generations, resulting in poor generalization to the newly published generators. To address the challenge of newly generated videos, we propose a novel dataset, AIDetection, for the AI-generated video detection task. The proposed AIDetection dataset contains 39,298 real and 19,731 generated videos from 27 diverse sources, specifically designed to evaluate cross-generator generalization under out-of-distribution settings. For the real videos, the motion of moving objects and the background show clear distinctions. Based on this observation, in this paper, we introduce a novel Multiscale Spatiotemporal motion modeling Transformer framework (MSTformer) for the AI-generated video detection task, which learns motion-aware discriminative representations from both local and global viewpoints. Specifically, a novel multiscale spatiotemporal downsampling mechanism is designed to capture local motion discrepancies between real and generated videos. Further, to prevent the discriminative cues from being weakened, we also employ a contrastive learning mechanism implemented on multiscale spatiotemporal features, enabling the model to maintain the global discriminative ability. Extensive experiments on three benchmark datasets (i.e. AIDetection, GVF, and GenVideo) demonstrate that MSTformer achieves the superior cross-domain generalization performance. In addition, ablation studies further confirm the effectiveness of multiscale temporal modeling and contrastive learning in enhancing robustness for AI-generated video detection.", "tldr": "", "keywords": ["AI-generated video detection", "Out-of-distribution generalization", "Multiscale spatiotemporal modeling", "Contrastive learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba2c01b37feeb6b49e07138742b53bd83cc85d78.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel multi-scale spatiotemporal motion-aware Transformer architecture (MSTFormer) for detecting high-quality AI-generated videos. The authors have constructed a new dataset, AIDetection, which includes nearly 39k videos from 27 sources, specifically designed to evaluate generalization across different generators. MSTFormer effectively captures the motion differences between real and generated videos through its motion-aware spatiotemporal downsampling module and cross-scale semantic contrastive learning module. It demonstrates outstanding performance on multiple benchmark datasets, particularly showing strong generalization capabilities in out-of-distribution (OOD) settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method performs excellently across multiple OOD test scenarios, demonstrating its robustness on unknown generators.\nComprehensive experiments: Ablation studies and analyses of the effects of different sampling lengths and batch sizes have been conducted to verify the effectiveness of each module.\nGood interpretability: Differences in motion patterns between real and generated videos are visualized using techniques such as optical flow maps."}, "weaknesses": {"value": "1. MSTFormer has limited innovation, as operations for feature pooling already exist in RecoNet and MViT v2. This approach is very similar to these works.\n2. The dataset proposed by the author is very small, with only 39K samples, which is significantly smaller than GenVideo and GenVidBench. This could lead to biased statistical results.\n3. Table 2 presents the test results on three datasets. It can be observed that the results on the three test datasets are close, and AIDetection does not show a significant advantage.\n4. The training set of AIDetection should be used for training, and the test sets of GenVideo and GenVidBench should be used for testing to determine whether the training set of this dataset offers any advantages."}, "questions": {"value": "See the above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "90KcXk3uzj", "forum": "ymgbE674Ay", "replyto": "ymgbE674Ay", "signatures": ["ICLR.cc/2026/Conference/Submission19523/Reviewer_bqV7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19523/Reviewer_bqV7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882226574, "cdate": 1761882226574, "tmdate": 1762931414307, "mdate": 1762931414307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of detecting high-quality AI-generated videos that are becoming visually indistinguishable from real ones. The authors introduce a new dataset AIDetection that covers diverse generators and real sources to evaluate OOD generalization.\nBesides, the authors propose MSTformer which contains two main modules named MSTD and CSCL. Experiments show some improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Writing is clear and the modules are well motivated.\n2. AIDetection dataset is comprehensive for OOD evaluation in AI generated videos detection."}, "weaknesses": {"value": "1. The manuscript only compares MSTformer with MViTv2 and UniFormerV2, omitting recent and more competitive AI-generated video detection methods. As a result, the evaluation appears limited, and the relative performance of MSTformer within the current state of the art remains unclear from my perspective.\n\n2. The ablation study is somewhat incomplete. The authors mainly examine the presence or absence of the MSTD and CSCL modules, while omitting finer-grained analyses. For instance, it would be informative to investigate the impact of different 3D kernel sizes, strides, and downsampling strategies in MSTD (as mentioned around lines 217–218). Similarly, the influence of different cross-scale pair combinations in the CSCL module deserves further exploration.\n\n3. The paper is motivated by the observation that real videos exhibit clear distinctions between object motion and background motion. However, it lacks corresponding interpretability results to support this claim. Providing interpretable analyses or visualizations would help demonstrate that the proposed method indeed captures these motion discrepancies.\n\n4. The paper claims MSTformer is lightweight but computational cost (FLOPs, parameters, runtime) is not reported.\n\n5. AIDetection dataset partly reuses videos from GenVideo and GVD, it's unclear whether these are excluded from training when testing on other datasets."}, "questions": {"value": "I have one additional concern regarding the core hypothesis stated around line 52 — that “generated videos exhibit motion patterns inconsistent with the physical world.” While this assumption is plausible, it lacks direct empirical validation. For instance, a statistical analysis of optical flow distributions could provide supporting evidence for this claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yi5iRJgjmB", "forum": "ymgbE674Ay", "replyto": "ymgbE674Ay", "signatures": ["ICLR.cc/2026/Conference/Submission19523/Reviewer_yYLf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19523/Reviewer_yYLf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097539593, "cdate": 1762097539593, "tmdate": 1762931413118, "mdate": 1762931413118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of detecting AI-generated videos, especially under cross-generator OOD generalization. The authors propose a two-part framework called MSTformer.They also introduce a new benchmark dataset AIDetection, containing videos from multiple commercial and closed-source generators as well as real short videos. This dataset aims to reflect realistic, diverse distributions.\nExperiments on AIDetection, GVF, and GenVideo show consistent improvements across metrics (ACC, F1, AP). For instance, on AIDetection, MSTformer achieves ACC = 91.31, F1 = 91.12, and AP = 97.08; on GenVideo, ACC = 94.32, F1 = 93.50, and AP = 98.50.\nThe paper further reports per-generator ACCs, ablations on the two modules, and sensitivity to parameters τ and λ, number of frames, and batch size."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Clear and targeted motivation.\nThe authors correctly observe that modern video generators largely eliminate spatial artifacts, making motion dynamics the more reliable cue. The LK flow visualizations in Fig. 1 show that generated videos exhibit stronger foreground–background motion correlation.\n\n2.Method design is consistent with the motivation and technically feasible.\nMSTD performs spatiotemporal downsampling via 3D convolution before attention, enlarging the receptive field while preserving local temporal correlation.\nCSCL enforces semantic alignment across multiple scales using supervised contrastive loss, preventing ambiguity in single-scale representations.\n\n3.OOD-oriented evaluation setup.\nThe experiments explicitly test cross-generator generalization under “unknown real sources,” which is highly relevant for real-world robustness.\n\n4.Rich experimental evidence.\nComprehensive comparisons across three datasets, per-generator breakdowns, and one-to-many OOD tests all support the claimed improvements.\nAblation studies confirm that MSTD substantially increases recall (46.56 → 66.81), while CSCL further improves F1 and ACC.\n\n5.Implementation details are sufficiently disclosed.\nFrame sampling, optimizer, training setup, and hardware are all clearly stated, aiding reproducibility."}, "weaknesses": {"value": "1.Inconsistent or unclear dataset statistics.\nDifferent sections report inconsistent counts of real/generated samples and generator sources (e.g., 39,298 vs 19,298 real). The authors should reconcile these and provide detailed splits in the appendix.\n\n2.Misaligned evaluation protocol affects external comparability.\nThe paper explicitly states that, instead of evaluating each generator separately, all test samples are merged.\nThis change makes results not directly comparable with prior works and could blur whether the gains arise from method improvements or mixed distribution effects.\nSuggestion: also report results following the original per-generator protocols (at least in the appendix).\n\n3.Instability on advanced generators (e.g., Sora).\nIn Table 3, the per-generator ACC on Sora (68.60) is lower than MViTv2-S (71.74), indicating limited robustness to complex temporal motion. The authors should analyze failure cases by motion type, scene dynamics, compression, or frame rate.\n\n4.Limited metrics and fixed-threshold evaluation.\nAll main results are based on ACC/Precision/Recall/F1/AP computed at a fixed threshold = 0.5.\nNo ROC-AUC, EER, or calibration analysis is provided.\nSuggest adding AUC, EER, and PR-AUC results and discussing calibration or uncertainty under OOD settings.\n\n5.CSCL lacks empirical justification for “cross-scale augmentation.”\nSection 4.3 describes semantic consistency qualitatively but omits supporting quantitative evidence (e.g., alignment visualization, mutual information, or ablation comparing single-pair vs. multi-pair combinations).\nSuggest including these analyses and reporting contrastive queue statistics in the appendix.\n\n\n6.Reproducibility and openness.\nThe release of code and dataset is not guaranteed. The paper should clarify data licensing and provide code/configs or feature files if raw videos cannot be shared."}, "questions": {"value": ".Please confirm the final statistics of AIDetection (real/generated counts, number of generator sources, and split details).\n2.Can you reproduce results under the original GVF/GenVideo protocols to enable direct comparison?\n3.Why does performance drop on Sora? Is it correlated with scene complexity, motion diversity, or frame rate?\n4.For CSCL, what is the pair selection strategy and contrastive queue size? How sensitive is performance to these choices?\n5.Could you provide ROC-AUC/EER metrics and discuss calibration or temperature scaling?\n6.Have you tested robustness under varying compression levels, frame rates, and resolutions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U911AkUQcH", "forum": "ymgbE674Ay", "replyto": "ymgbE674Ay", "signatures": ["ICLR.cc/2026/Conference/Submission19523/Reviewer_2RW6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19523/Reviewer_2RW6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101889706, "cdate": 1762101889706, "tmdate": 1762931412449, "mdate": 1762931412449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}