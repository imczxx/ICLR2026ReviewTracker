{"id": "hxO6j47C2W", "number": 8277, "cdate": 1758077169401, "mdate": 1759897794734, "content": {"title": "MMGenBench: Fully Automatically Evaluating LMMs from the Text-to-Image Generation Perspective", "abstract": "Large Multimodal Models (LMMs) demonstrate impressive capabilities. However, current benchmarks predominantly focus on image comprehension in specific domains, and these benchmarks are labor-intensive to construct. Moreover, their answers tend to be brief, making it difficult to assess the ability of LMMs to generate detailed descriptions of images. To address these limitations, we propose the MMGenBench-Pipeline, a straightforward and fully automated evaluation pipeline. This involves generating textual descriptions from input images, using these descriptions to create auxiliary images via text-to-image generative models, and then comparing the original and generated images. Furthermore, to ensure the effectiveness of MMGenBench-Pipeline, we design MMGenBench-Test, evaluating LMMs across 13 distinct image patterns, and MMGenBench-Domain, focusing on generative image performance. A thorough evaluation involving over 50 popular LMMs demonstrates the effectiveness and reliability of both the pipeline and benchmark. Our observations indicate that numerous LMMs excelling in existing benchmarks fail to adequately complete the basic tasks related to image understanding and description. This finding highlights the substantial potential for performance improvement in current LMMs and suggests avenues for future model optimization. Concurrently, MMGenBench-Pipeline can efficiently assess the performance of LMMs across diverse domains using only image inputs. All code and data will be released.", "tldr": "", "keywords": ["Image Understanding Benchmark", "Generative Model-Based Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/710398eea8e7611ed37d69f405e8e04beb3d8213.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces MMGenBench, a fully automated pipeline for evaluating LMMs’ image understanding and detailed description. Given an input image, an LMM generates a fine-grained caption; a text-to-image (T2I) model then synthesizes an auxiliary image from that text; finally, a single image encoder (Unicom) computes feature-level similarity (SIM, FID) between the original and synthesized images. Two datasets are provided: MMGenBench-Test (13 image patterns, 1,284 images) and MMGenBench-Domain (~10k images). Results are reported for 50+ LMMs; the best SIM is < 0.6, and a human-alignment study reports 88.27% agreement on 1,850 pairs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Clear, modular pipeline with explicit metric definitions (SIM/FID).\n\n2.Broad coverage (50+ LMMs) with series-wise and pattern-wise analyses.\n\n3.Practical cross-domain evaluation requiring only images."}, "weaknesses": {"value": "1.Metric robustness not established.  Features come from a single encoder (Unicom), and main results default to one T2I model (FLUX.1-dev); no ablations on encoder/T2I/seed variation or rank correlations are reported.\n\n2.Length/style bias. Shorter descriptions tend to receive lower scores.\n\n3.Stochasticity not controlled. Evidence: Randomness is acknowledged (Eq. 2), but no multi-run statistics or confidence intervals are provided.\n\n4.Human alignment under-analyzed. Only a single overall figure (88.27%) is given;"}, "questions": {"value": "1.What is the per-model seed variance of SIM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "COeTEOLi7A", "forum": "hxO6j47C2W", "replyto": "hxO6j47C2W", "signatures": ["ICLR.cc/2026/Conference/Submission8277/Reviewer_iYF5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8277/Reviewer_iYF5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570962699, "cdate": 1761570962699, "tmdate": 1762920211459, "mdate": 1762920211459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MMGenBench, a fully automated evaluation framework for large multimodal models (LMMs) from a text-to-image generation perspective. The core idea is to assess the image understanding and descriptive capabilities of LMMs through a closed-loop pipeline: (1) generating textual descriptions from input images via the tested LMMs; (2) using state-of-the-art text-to-image diffusion models to regenerate images from those descriptions; and (3) quantitatively evaluating the similarity between the original and regenerated images using SIM-Score and FID-Score. To support this pipeline, the authors build two benchmarks: MMGenBench-Test, covering 13 well-defined image patterns (e.g., Natural, Artistic, Motion, Contextual, Symbol, etc.), and MMGenBench-Domain, focusing on the “generated image” domain. Over 50 representative LMMs are evaluated."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a fully automated, cross-domain benchmarking framework that reduces manual labeling cost. It combines LMM-based image-to-text and diffusion-based text-to-image processes into a self-consistent evaluation loop.\n\n2. More than 50 LMMs, both open- and closed-source, are evaluated using unified metrics."}, "weaknesses": {"value": "1. My main concern about this paper lies in the closed-loop evaluation design, which involves three interconnected stages. The first stage is the LMM’s understanding of an image (which is exactly what the paper aims to evaluate); the second stage converts the generated textual description into an image using a text-to-image model; and the third stage measures the similarity between the regenerated image and the original one to assess the LMM’s image comprehension ability.\n\nTheoretically, this approach is valid only if two key assumptions hold: (a) the text-to-image model must be sufficiently powerful to accurately render every object described in the text, and (b) the similarity metric must be capable of reliably quantifying semantic correspondence between images. In practice, however, both assumptions are problematic. Current text-to-image models are still imperfect, which raises doubts about whether the regenerated image can faithfully reflect the textual content. Likewise, existing similarity metrics struggle to capture semantic-level consistency accurately.\n\n2. Although this method is theoretically feasible and can be fully automated, it lacks interpretability. The system outputs only a single numerical score, without providing insight into which aspects of the LMM’s understanding are inaccurate or incomplete. This makes it difficult to diagnose specific weaknesses in the evaluated model."}, "questions": {"value": "1. In Lines 106–107, the authors state that “numerous LMMs excelling in existing benchmarks fail to address the basic tasks of image understanding and description.” Could the authors clarify what specific tasks are referred to here? Please elaborate on what constitutes these “basic tasks” and how they are defined or measured in the proposed framework.\n\n2. What is the difference between MMGenBench-Test and MMGenBench-Domain? Why does MMGenBench-Test require specific annotations and human verification, whereas MMGenBench-Domain does not?\n\n3. In Table 1, GPT-4o, which generally outperforms open-source models on most existing benchmarks, does not achieve the best results here. This raises questions about the reliability and rationality of the proposed evaluation method. The authors should discuss this discrepancy in depth—why does GPT-4o underperform, and why do larger models in the same series not consistently outperform smaller ones? Is this phenomenon caused by limitations or biases in the proposed evaluation pipeline itself, or does it genuinely reflect the intrinsic capability differences between models? This discussion is crucial for demonstrating whether the proposed benchmark is both valid and capable of challenging or redefining existing evaluation systems."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WCpeq8NdEe", "forum": "hxO6j47C2W", "replyto": "hxO6j47C2W", "signatures": ["ICLR.cc/2026/Conference/Submission8277/Reviewer_8ARY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8277/Reviewer_8ARY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834329327, "cdate": 1761834329327, "tmdate": 1762920211146, "mdate": 1762920211146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MMGenBench, a novel evaluation framework designed to assess Large Multimodal Models (LMMs) from the perspective of text-to-image generation. The central contribution is the MMGenBench-Pipeline, which proposes a \"fully automated\" method: an LMM generates a textual description from an input image, this description is then fed to a text-to-image (T2I) model to create an auxiliary image, and finally, the similarity between the original and auxiliary images is quantitatively measured using an image representation model. To support this pipeline, the authors develop two benchmarks: MMGenBench-Test, featuring 1284 images categorized into 13 distinct patterns (e.g., Surreal, Color, Motion), and MMGenBench-Domain, focused on generative images. The paper conducts an extensive evaluation of over 50 popular LMMs, revealing their limitations in generating detailed descriptions and adhering to instructions, even for models that excel in existing benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea of using text-to-image models to \"reconstruct\" an image from an LMM's description is a creative and insightful way to quantify the fidelity and detail of LMM understanding. This \"information compression and restoration\" perspective is a fresh contribution to LMM evaluation. The paper effectively highlights critical shortcomings of current LMMs, such as poor instruction following, inability to generate sufficiently detailed descriptions, and instances of \"safety\" overfitting. These findings are highly significant for guiding future LMM research and development.\n2. The paper conducts a thorough evaluation across more than 50 diverse LMMs, including both open-source and proprietary models. This broad assessment provides a valuable overview of the current landscape of LMM capabilities and weaknesses.\n3. The introduction of MMGenBench-Test, with its 13 distinct image patterns, offers a more granular and structured approach to evaluating LMMs across various visual characteristics. MMGenBench-Domain further extends this to generative images. The inclusion of qualitative examples provides compelling visual evidence for the observed LMM failures, making the paper's points tangible and understandable."}, "weaknesses": {"value": "1. The most significant weakness is the repeated and misleading assertion of a \"fully automated\" benchmark. The construction of MMGenBench-Test explicitly involves \"human check\" and \"manually filtered\" steps. This fundamental contradiction undermines the paper's credibility and its claims of scalability and objectivity in benchmark creation.\n2. The entire pipeline's accuracy and fairness are inherently tied to the performance and potential biases of the underlying GPT-4o, text-to-image models (FLUX, SD3.5, etc.), and the Unicom image representation model. The paper lacks a rigorous analysis or quantification of how these dependencies affect the LMM evaluation results. Without this, it's difficult to ascertain whether observed LMM failures are due to the LMM itself or limitations propagated from the auxiliary models.\n3. The LMMs are specifically prompted to generate an \"image caption-prompt\" for a T2I model, with a word count constraint (20-60 words). This framing might inadvertently bias LMM outputs towards a specific style or level of detail amenable to T2I models, rather than truly assessing their general capability for any detailed image description. The word count limit, in particular, can hinder genuinely comprehensive descriptions for complex images.\n4. While the paper effectively identifies common LMM problems (instruction following, lack of detail, overfitting), the analysis largely remains descriptive. There's minimal deeper investigation into the underlying causes (e.g., specific training data deficiencies, architectural limitations, prompt engineering sensitivity) or potential avenues for mitigation. This limits the actionable insights for model developers seeking to improve LMMs.\n5. While the overall MMGenBench-Test dataset has 1284 images, some of the 13 patterns have relatively few examples (e.g., \"Orientation\" with 114 images, \"Motion\" with 160). This could lead to less robust or generalizable evaluation results for those specific categories."}, "questions": {"value": "1. Could the authors explicitly clarify and rectify the \"fully automated\" claim, particularly concerning the benchmark construction? Please detail the extent of human involvement (e.g., person-hours, number of annotators) in the \"human check\" and \"manually filtered\" stages of MMGenBench-Test creation. How does this manual effort impact the scalability and objectivity claims?\n2. Given the heavy reliance on external models (GPT-4o, T2I models, Unicom), what specific experiments or analyses were conducted (or are planned) to understand and quantify the impact of their potential biases, failure modes, or performance ceilings on the LMM evaluation results? For example, if a T2I model struggles with counting objects, how does this affect the evaluation of an LMM that accurately described the count?\n3. The LMM prompt asks for an \"image caption-prompt\" for a T2I model, with a 20-60 word limit. How might this specific instruction and length constraint influence the LMMs' output compared to a more open-ended request for a general \"detailed image description\"? Have you explored variations in this prompt, and if so, what were the effects?\n4. Beyond SIM-Score and FID-Score, have the authors considered or explored other metrics that could capture more fine-grained aspects of descriptive accuracy, such as object presence, attribute correctness, or relational understanding, perhaps by integrating object detection or semantic segmentation models on the generated images?\n5. For the \"Model Overfitting\" weakness, particularly the \"safety\" example, can the authors provide more quantitative evidence or analysis across the benchmark to show how widespread this issue is among LMMs and its overall impact on their scores?\n6. You state that \"MMGenBench-Domain includes 10,000 images, thereby improving the accuracy of its FID-Score measurement. Therefore, we propose using SIM-Score as the primary metric.\" Could you elaborate on why FID-Score is considered less reliable for MMGenBench-Test (due to fewer images) and why SIM-Score is deemed a more suitable primary metric across both benchmarks, despite FID being a common generative metric?\n7. Please provide more details on the \"human metric based on votes from 9 human experts\" mentioned for pipeline effectiveness. What was the exact task given to humans, how were \"comparable\" cases defined, and what was the inter-annotator agreement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tyckNK6xP6", "forum": "hxO6j47C2W", "replyto": "hxO6j47C2W", "signatures": ["ICLR.cc/2026/Conference/Submission8277/Reviewer_8TQ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8277/Reviewer_8TQ1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931303441, "cdate": 1761931303441, "tmdate": 1762920210778, "mdate": 1762920210778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This benchmark paper presents MMGenBench, an automated pipeline to evaluate the caption capability of LMMs. The key idea is to reconstruct an auxiliary image via powerful text-to-image models, followed by representation-level comparison between the original and generated images. On top of the pipeline, the authors build two benchmarks: MMGenBench-Test, which covers 13 carefully summarized image patterns derived from JourneyDB, and MMGenBench-Domain, which targets the “generated images” domain. Evaluating 50+ popular LMMs shows: (i) current models are far from perfect (best SIM < 0.6); (ii) models strong on existing VQA/caption/ocr leaderboards can still fail at detailed description; and (iii) the proposed pipeline aligns with human judgment in 88.27% of sampled cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation is clear and the idea is interesting. The paper targets a real gap: most current LMM benchmarks emphasize short answers and specific domains, while real applications need long, faithful, instruction-following image descriptions. Meanwhile, the proposed automated evaluation pipeline is scalable.\n\n- The Large-scale empirical study demonstrates interesting insights. Evaluating 50+ LMMs and reporting pattern-wise weaknesses (context, orientation, count, motion) gives the community actionable signals.\n\n- Human alignment check. The 88% agreement suggests the metric is not completely drifting away from human judgment."}, "weaknesses": {"value": "- An important issue is the metric entanglement with the T2I model. The final score is a function of (LMM description quality) × (T2I controllability) × (image encoder). Even though four T2I models are tried, the paper does not quantify how much ranking changes if the T2I model is weaker/safer/biased. A sensitivity analysis is needed.\n\n- Using only Unicom as the image representation back-end makes the whole pipeline hinge on one model’s inductive biases. Showing results with different encoders such as CLIP and DINOv2 would make the claim of “fully automatic and reliable” more convincing.\n\n- Both MMGenBench-Test and Domain come from JourneyDB-like, style-rich, often synthetic images. It is unclear whether the same pipeline will hold for photos, documents, Med-VQA, or low-res, cluttered, user-uploaded images. A small real-photo subset would strengthen the story.\n\n- Since the pipeline favors descriptions that lead to well-conditioned T2I prompts, a model that always outputs long, enumerated, style-heavy prompts may score higher than a model that is actually more faithful but concise. The paper partially discusses instruction-following failure (Fig. 9) but does not experiment with length-controlled outputs."}, "questions": {"value": "How is the consistency of the human annotators? Does the inter-annotator agreements correlate with certain patterns (e.g., Contextual, Orientation, Count), where T2I models naturally struggle?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IHfHXowHZL", "forum": "hxO6j47C2W", "replyto": "hxO6j47C2W", "signatures": ["ICLR.cc/2026/Conference/Submission8277/Reviewer_aasA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8277/Reviewer_aasA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762105234028, "cdate": 1762105234028, "tmdate": 1762920210338, "mdate": 1762920210338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}