{"id": "CsCL9T2PDk", "number": 9579, "cdate": 1758128590884, "mdate": 1759897711098, "content": {"title": "Newton-PINet: A fast physics-informed neural network with Newton linearization for meta-learning nonlinear PDEs", "abstract": "Scientific machine learning has opened new avenues for solving parameterized partial differential equations (PDEs), enabling models to learn a family of PDEs and generalize to unseen instances. In this context, data-driven operator learning methods typically require large training data, while physics-informed neural networks (PINNs) trained with PDE-based loss functions suffer from challenging optimization landscapes and limited generalization, especially for nonlinear PDEs. To address these issues, we propose Newton-PINet, a meta-learning framework for nonlinear PDEs. It (i) introduces a physics-informed multi-layer network with skip connections to a least-squares-computed output layer; (ii) adopts a two-stage learning strategy that first leverages gradient-based training to learn robust representations from the available training tasks, and then performs gradient-free fine-tuning on the output layer for fast task-specific generalization; and (iii) incorporates a Jacobian-free Newton linearization method to speed up the least-squares iteration for nonlinear PDE problems. Newton-PINet achieves relative errors three orders of magnitude lower than recent neural solver baselines on a challenging nonlinear reaction-diffusion  benchmark, even while using 16$\\times$ fewer training tasks and an order of magnitude less training time (under 2 minutes against the several hours these baselines required). This work advances the meta-learning of PINNs toward data-efficient, fast and generalizable physics solvers.", "tldr": "In meta-learning nonlinear PDEs, Newton-PINet achieves high generalization accuracy while requiring fewer training costs than state-of-the-art baselines.", "keywords": ["Physics-informed neural networks; nonlinear PDEs; data-efficient meta-learning; fast generalization"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/475fc0579d2e7ac7d01bf61e1b945aa16c7cc9c4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The Newton-PINet method is a new algorithm that aim at solving parametric PDEs using meta-learning. The method is composed of several components : a specific design of layers, a newton linearization and a regularization in the objective. The paper demonstrate very efficient results in terms of performance and computationnal time both at training and inference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper demonstrate very strong results in term of MSE and computational time, both at training and inference. \n- The problem tackled is a well-known issue of PINNs that is currently studied."}, "weaknesses": {"value": "-\tI feel like the paper is a bit rushed : some references are missing/not well writen, legends of figures are not precise, some numeric application are empty etc... \n-\tThere is a rich content, but it is difficult to follow and understand the method. Maybe pseudo code would help ? \n-\tReferences are old : in the data driven and operator learning section, the most recent reference is from 2021. A very rich literature have emerged on these subject in the 5 past years."}, "questions": {"value": "-\tCould you define specifically what is a task ? I understood it as solving a single instance of a PDE, but I think this should be formally defined at the begining for clarity (maybe in section 2 ?)\n-\tline 157 Could you justify the use of skip connexions ? (done in appendices, but while reading though the main part, I wondered about this point)\n-\tCould you calrify notations ? i=points, s=data, k=iterations (eg line 162)? \n-\tWhat means HOT line 196?\n-\tlines 186-197: Are the loss of the linearization quantified ? When linearizing an expression, one can expect some error, are they quantified? In what extend do they influence the results? \n-\tHow are created the matrices A ? How big is it ? Isn’t inversion costly ? How hard are they to compute.derive? What happens when the matrix are not tractable? \n-\tDo you have any insight about why MSE+LES in Newton PINet works rather well ? The LSE loss term does not incurs a big difference ?\n-\tWhy some cells in tab 2 are missing ? \n-\tCould you ablate the time block strategy ? My point here is to isolate in what extend do the time block strategy helps in the performance. \n- Tab 3, I could not fid the results of table 3 in the work cited (Wei et al. 2025b). Moreover, I couldn't find any reference to PPINNs in this work, which could indicate a mismatch in the references. \n-\tI am not an expert in Newton method so I could not check the entire theory behind the convergence analysis. \n-\tDo you have any insight about why is there such a difference wrt to other baselines ? 3 order of magnitude is a lot of improvement with a very short training time and data requirement (eg on tab 3). Moreover, what justifies the very short training and inference time of the proposed method? Meta-learning methods are usually long to train, due to the inner/outer loop. Additionally, inference can be costly, because of the inner steps required to adapt to new instances. \n- In Fig2 a) and b), why the errors aren't decreasing with respect to the number of iterations? \n- To the best of my knowledge, optimizing a residual loss on the PDE residual, often complicates training. The observe this phenomenon in Fig2)e with PINet. What explains that this effect is much smaller when using Newton-PINet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xUc6LMjmkI", "forum": "CsCL9T2PDk", "replyto": "CsCL9T2PDk", "signatures": ["ICLR.cc/2026/Conference/Submission9579/Reviewer_bRUp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9579/Reviewer_bRUp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757550332, "cdate": 1761757550332, "tmdate": 1762921133824, "mdate": 1762921133824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a meta learning framework for rapid adaptation to new PDEs, introducing a gradient based trained transferable hidden representation and converting learning new PDE tasks into a fast closed-form Jacobian-free Newton linearization-Tikhonov least-squares solve on a skip-connected output layer, achieving gradient-free fine-tuning."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The use of closed-form Tikhonov least-squares speed up adaptation to new PDE solving task is novel to me. Usually, I see people use gradient based fine tuning for learning new PDEs, such as transfer learning. \n2. I am not familiar with meta learning, but the method seems novel and useful and the experimental setup is solid and achieves substantially better results than the baselines."}, "weaknesses": {"value": "Due to the complex loss landscape, PINNs are known to fail easily when the PDE coefficient is large [1]. For example, when the coefficient (e.g., viscosity) in Burgers’ equation is large, will the proposed method also achieve good results?\n\n[1] Krishnapriyan, Aditi, et al. \"Characterizing possible failure modes in physics-informed neural networks.\" Advances in neural information processing systems 34 (2021): 26548-26560."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jWZp5ytx8L", "forum": "CsCL9T2PDk", "replyto": "CsCL9T2PDk", "signatures": ["ICLR.cc/2026/Conference/Submission9579/Reviewer_9rcu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9579/Reviewer_9rcu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940772213, "cdate": 1761940772213, "tmdate": 1762921133607, "mdate": 1762921133607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Newton-PINet, a physics informed meta learning framework for efficiently solving families of nonlinear PDEs. The model combines skip connected PINN architectures with a Tikhonov regularized closed form output layer and a Jacobian-free Newton linearization for nonlinear terms. This design enables gradient free, near instant adaptation to unseen PDEs."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1) Clear motivation and novel combination of PINNs, meta-learning, and Newton linearization.\n\n2) Gradient-free closed-form adaptation drastically reduces computation time.\n\n3) Strong empirical results across diverse PDEs, showing orders-of-magnitude gains in speed and accuracy.\n\n4) Demonstrates high data efficiency and generalization with few training tasks."}, "weaknesses": {"value": "1) The claimed “Jacobian-free Newton” method lacks formal derivation or convergence proof.\n\n2) Construction of the linear system Aw=b is under-specified, hindering reproducibility.\n\n3) No analysis of the stability or sensitivity of the Tikhonov regularization parameter.\n\n4) Meta-gradient propagation through the closed-form solution is not explained."}, "questions": {"value": "1) Question in detail of the Newton Linearization\nThe description of the Jacobian free Newton linearization raises questions about its mathematical rigor.\nThe update rule (in Fig 1 / near line 208)\n(uux) k+1 ≈ u ku k+1 x + u k+1u k x − u ku k\nis presented as a Newton type linearization, yet it differs from the classical Newton Raphson formulation that explicitly involves the Jacobian vector product. The paper claims second order convergence but does not specify the conditions under which this convergence holds.\nA more formal derivation or convergence proof would be necessary to justify calling this method “Newton like” in the strict numerical analysis sense.\n\n2) Question in construction of linear system\nThe paper repeatedly states that the matrices A and b are assembled from PDE residuals, initial conditions, and boundary conditions, but it does not provide an explicit mathematical formulation of these components. In nonlinear PDEs such as the Burgers equation, the residual terms (e.g. burger’s equation) make A depend nonlinearly on both u and w.\n\nIt remains unclear how these nonlinear dependencies are linearized or evaluated when forming A^(k) in each iteration, and whether differential operators are computed analytically or via automatic differentiation. Clarifying this construction is essential for reproducibility and for understanding how the closed form Tikhonov solution applies to nonlinear PDEs. Did I miss some parts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0mrlelDjAe", "forum": "CsCL9T2PDk", "replyto": "CsCL9T2PDk", "signatures": ["ICLR.cc/2026/Conference/Submission9579/Reviewer_Qz5i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9579/Reviewer_Qz5i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963373927, "cdate": 1761963373927, "tmdate": 1762921133193, "mdate": 1762921133193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Newton-PINet is a meta-learning framework for PINNs that pretrains the hidden representation once and adapts to new PDE instances by solving only the output layer with Tikhonov-regularized least squares. To handle nonlinear dynamics, it replaces Picard with a Jacobian-free Newton linearization that achieves quadratic convergence and improves stability. For time-dependent problems it uses temporal domain decomposition, training on short blocks and composing them for long-horizon rollout. Across diverse nonlinear PDE benchmarks, including a 2D Helmholtz case, the method shows strong data efficiency, fast adaptation, and robustness where Picard-based approaches struggle."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear derivation of the Jacobian-free Newton linearization and a contrast with Picard’s first-order vs Newton’s second-order convergence (including a Burgers example and an Appendix proof). \n\n2. Only the output layer is updated at test time via closed-form Tikhonov solves; this, together with time-block training, makes long-horizon inference practical.\n\n3. PINN with the output weights solved by Tikhonov regularization improves nonlinear representation while keeping adaptation inexpensive."}, "weaknesses": {"value": "1. Most studies are 1D; there is one 2D problem (Helmholtz). It remains unclear how the Tikhonov solves and Newton iterations scale in 2D/3D, multiphysics, or turbulence-like regimes. \n\n2. Because adaptation reduces to solving regularized least-squares systems, guidance on low-rank structure/iterative solvers for very large collocation sets would strengthen the practical scaling story."}, "questions": {"value": "1. For high-resolution 2D/3D cases, how do you keep the Tikhonov update solves tractable from a linear-algebra standpoint?\n \n2. Can you quantify the Newton step’s convergence radius and sensitivity to initialization for strongly nonlinear/chaotic settings (e.g., low-viscosity K-S)? Any empirical ablations?\n\n3. Have you tested Neumann/Robin/mixed or discontinuous BCs beyond periodic/Dirichlet? \n\n4. Did you try overlapping blocks, residual re-seeding, or simple filters (e.g., Kalman-style smoothing) to mitigate drift at block boundaries during very long rollouts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1oix0jS5ms", "forum": "CsCL9T2PDk", "replyto": "CsCL9T2PDk", "signatures": ["ICLR.cc/2026/Conference/Submission9579/Reviewer_hUL5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9579/Reviewer_hUL5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999664550, "cdate": 1761999664550, "tmdate": 1762921132770, "mdate": 1762921132770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}