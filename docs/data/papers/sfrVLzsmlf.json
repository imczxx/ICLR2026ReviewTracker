{"id": "sfrVLzsmlf", "number": 14382, "cdate": 1758234213461, "mdate": 1759897373840, "content": {"title": "AMemGym: Interactive Memory Benchmarking for Assistants in Long-Horizon Conversations", "abstract": "Long-horizon interactions between users and LLM-based assistants necessitates effective memory management, yet current approaches face challenges in training and evaluation of memory. Existing memory benchmarks rely on static, off-policy data as context, limiting evaluation reliability and scalability. To address these gaps, we introduce AMemGym, an interactive environment enabling on-policy evaluation and optimization for memory-driven personalization.\nAMemGym employs structured data sampling to predefine user profiles, state-dependent questions, and state evolution trajectories, enabling cost-effective generation of high-quality, evaluation-aligned interactions. LLM-simulated users expose latent states through role-play while maintaining structured state consistency.\nComprehensive metrics based on structured data guide both assessment and optimization of assistants.\nExtensive experiments reveal performance gaps in existing memory systems (e.g., RAG, long-context LLMs, and agentic memory) and corresponding reasons. AMemGym not only enables effective selection among competing approaches but also can potentially drive the self-evolution of memory management strategies.\nBy bridging structured state evolution with free-form interactions, our framework provides a scalable, diagnostically rich environment for advancing memory capabilities in conversational agents.", "tldr": "", "keywords": ["memory", "agent", "long-context"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b09db4d30901ca1dad6fbcaf83ff21ca41996a6a.pdf", "supplementary_material": "/attachment/6d89e016af3167b31963068b104897d59e984418.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces AMemGym, an interactive framework for evaluating and optimizing memory management in long-horizon conversations with LLM-based assistants. It addresses limitations of static, off-policy benchmarks by enabling on-policy evaluation through simulated users and structured data. AMemGym supports memory personalization, role-play-based latent state revelation, and structured state evolution. Experimental results highlight its capability to identify performance gaps and foster the self-evolution of memory systems, making it a scalable and diagnostic tool for conversational assistant development"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a novel on-policy evaluation framework, AMemGym, which effectively addresses limitations in existing memory evaluation systems.\n\n2. The integration of simulated users and structured state evolution ensures reliable and scalable assessments of memory capabilities.\n\n3. Experimental evidence supports the framework’s ability to foster memory self-evolution, offering practical insights for conversational assistant optimization."}, "weaknesses": {"value": "1. The statistical information about the data is insufficiently described. For example, what categories of dialogues are included in the dataset? How many dialogues are there in total? What is the distribution of token lengths across these dialogues? These details are missing throughout the paper and should be explicitly addressed.\n\n2. The \"Memory Implementation\" section on page 6 is somewhat confusing. How exactly is the AWE method implemented? Why is parameter tuning only applied to the AWE method? Does this imply that the RAG and AWI methods do not have relevant parameters to adjust?\n\n3. It is unclear why the paper does not adopt common generation evaluation metrics, such as GPT4Judge or BLEU, for performance assessment. Instead, it uses a self-constructed memory score for the final evaluation. The rationale behind this choice should be explained in more detail, especially given the availability of well-established evaluation metrics.\n\n4. The structure of the appendix appears overly simplistic. It merely lists prompts without clearly explaining at which stage of the process each prompt was used. This lack of context makes the appendix difficult to interpret and detracts from its utility."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "44a582xune", "forum": "sfrVLzsmlf", "replyto": "sfrVLzsmlf", "signatures": ["ICLR.cc/2026/Conference/Submission14382/Reviewer_rDSg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14382/Reviewer_rDSg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761051752154, "cdate": 1761051752154, "tmdate": 1762924797845, "mdate": 1762924797845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **AMEMGYM**, an **interactive (on-policy)** benchmarking and optimization environment for long-horizon conversational memory. A **structured state-evolution blueprint** anchors free-form, LLM-driven role-play and enables **diagnostic** scoring with attribution to **write / read / utilization** stages. Experiments compare native LLMs, RAG, and two agentic-write variants, reveal sizable **on- vs off-policy** ranking shifts, and demonstrate feedback-driven **self-evolution** of memory policy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear motivation**: Off-policy evaluations can induce reuse bias; AMEMGYM offers an on-policy, diagnostically rich setup.  \n- **Methodological novelty**: Persona/state trajectories, exposure utterances, QA variants (with reflection) enable constrained interaction and automated scoring; normalized memory score and stage-wise failure analysis are useful.  \n- **Thorough evaluation**: Quantifies on- vs off-policy discrepancies; characterizes long-horizon degradation of native LLMs; provides granular failure attributions and analyses of frequency, short-term buffers, and top-k.  \n- **Meta-evaluation**: Human ratings on exposure clarity and dialogue consistency support data quality."}, "weaknesses": {"value": "- **External validity of simulated users**: Add a small human-in-the-loop comparison and a systematic study of user-LLM choice.  \n- **Broader baselines**: Include structured memory graphs/event stores, hierarchical compression, and explicit state trackers.  \n- **Leakage control**: Provide anti-leak prompt design and automatic leakage checks.  \n- **Metric reporting**: Add variance/CI and difficulty-conditioned analyses for the normalized memory score.  \n- **Scope of self-evolution**: Jointly evolve retrieval and utilization (e.g., top-k plus utilization prompting), and report stability/convergence."}, "questions": {"value": "1. **Trace reusability**: Must interactions be regenerated for each system, or can exposure prompts and user policy be reused for fair comparison?  \n2. **Noise control**: How is the ratio of non-informative chatter quantified and manipulated as \\(N_i\\) grows?  \n3. **Upper bound (SUB)**: Does the UB solver access all ground-truth states plus a strong reasoner? Would an expert-solver UB better isolate utilization bottlenecks?  \n4. **Privacy**: Guidance for minimizing PII in real-user deployments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gn07x2nNyE", "forum": "sfrVLzsmlf", "replyto": "sfrVLzsmlf", "signatures": ["ICLR.cc/2026/Conference/Submission14382/Reviewer_bGms"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14382/Reviewer_bGms"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761210031953, "cdate": 1761210031953, "tmdate": 1762924797406, "mdate": 1762924797406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work introduces an interactive environment designed to benchmark the memory capabilities of llm assistants. The authors argue that existing benchmarks are flawed because they rely on static, off-policy data. This means the assistant is evaluated on a fixed conversation history it did not create, which fails to capture how an assistant's own responses influence the dialogue and can lead to unreliable evaluations. They first generate the state of the user using structured outputs and then the simulated user uses their attributes in a natural way during the conversation. The framework also introduces diagnostic metrics that decompose memory failures into three stages: write, read, and utilization. They demonstrate that this \"gym\" can be used for agent self-evolution, where an agent uses the environment's feedback to autonomously improve its own memory-writing policy"}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The papers central critique of off-policy evaluation is compelling and well-articulated. The authors provide concrete evidence in table 2 that evaluation rankings of memory systems change when moving from an off-policy to an on-policy setup, proving that the distinction is not just theoretical but has practical consequences.\n2. The introduction of write, read, and utilization failure metrics which gives better insights into failure modes than the usual accuracy metric.\n3. The self-evolution experiment shows that agents can improve its memory policy by learning from the environment's feedback."}, "weaknesses": {"value": "1. The work only evaluates memory for selecting the correct answer using multiple choice questions, but doesn't test the generation capabilities. \n2. The memory is tested using structured key-value pairs and doesn't test the episodic memory or memory where the assistant has to reason over multiple facts."}, "questions": {"value": "1. In section 5, \"Complete Feedback\" is described as including the questions, agent's answers, and ground-truth answers, which are summarized into <feedback.summary>. Could you provide an example of how this summary is formatted? Is it a natural language paragraph, structured JSON, or another format?\n2. How do you ensure diversity in the structured generation? \n\n- Some weird formatting: L373-375"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Vvv3vETHlz", "forum": "sfrVLzsmlf", "replyto": "sfrVLzsmlf", "signatures": ["ICLR.cc/2026/Conference/Submission14382/Reviewer_fnRa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14382/Reviewer_fnRa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803289518, "cdate": 1761803289518, "tmdate": 1762924796913, "mdate": 1762924796913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AMEMGYM, a new interactive, on-policy benchmark designed to  evaluate memory management in long-horizon conversational assistants. AMEMGYM differs from existing static datasets and provides a fully automated environment where LLM-simulated users engage in structured evolving conversations that reveal latent user states through role-play. Through experimental comparisons, they found that on-policy evaluation changed the rankings and scores substantially. They also show that even the latest LLMs struggle to maintain long-term conversational memory, confirming context-length degradation. Agentic-Write External systems performed best which highlights that selective, structured writing yields higher information utilization accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is grounded on a novel motivation: existing benchmarks mostly focus on off-policy memory evaluation which might have a gap between realistic systems.\n* The introduction of write/read/utilization decomposition is a meaningful contribution.\n* The paper also conducted extensive comparisons that shed light on long-context degradation and memory design trade-offs."}, "weaknesses": {"value": "* While the paper positions on-policy evaluation as a core contribution, the empirical and conceptual justification for its advantage over off-policy settings remains underdeveloped. Although Table 2 and Figure 5 show some rank changes between on- and off-policy settings, the paper does not provide insights on why those differences matter. It remains unclear what specific behavioral aspects of “interactive memory” are uniquely captured. In fact, one could argue that well-curated off-policy datasets offer simpler, cheaper, and more reproducible alternatives, and the paper does not convincingly rule out this possibility.\n* The claimed novelty of introducing an on-policy memory evaluation environment is somewhat weakened by the existence of several recent long-horizon, on-policy frameworks: AgentGym, DeepResearch, SWE-Agent. These already feature real-time decision-making, persistent contexts, and memory management. The paper should sufficiently clarify what AMEMGYM contributes beyond these environments.\n* The entire benchmark relies on LLM-simulated users rather than real human interaction data. While this enables scale and control, it raises a question: whether simulated dialogues genuinely capture the noise, inconsistency, and ambiguity of real users. Even though the authors conduct a “meta-evaluation”, it mostly checks internal coherence rather than human-likeness or behavioral realism.\n* Although the paper evaluates several general architectures (e.g. RAG, agentic write, and long-context LLMs), it does not include direct experiments on established open-source memory frameworks such as Mem0 (Chhikara et al., 2025) or A-Mem (Xu et al., 2025), despite citing both as prior work. While the authors’ custom agentic write setups are reasonable for controlled analysis, the paper should better justify why this self-defined configuration was chosen instead of real implementations that are already widely used."}, "questions": {"value": "* Could you provide the cost analysis?\n* Could you also provide results on popular open-source models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cky65sQpOH", "forum": "sfrVLzsmlf", "replyto": "sfrVLzsmlf", "signatures": ["ICLR.cc/2026/Conference/Submission14382/Reviewer_2oDj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14382/Reviewer_2oDj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762133993687, "cdate": 1762133993687, "tmdate": 1762924796520, "mdate": 1762924796520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}