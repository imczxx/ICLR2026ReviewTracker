{"id": "Ubi631nNbI", "number": 17020, "cdate": 1758271240599, "mdate": 1759897204051, "content": {"title": "Detecting Variant Contamination in LLMs via Variance of Generation Distribution", "abstract": "Evaluating large language models (LLMs) is increasingly confounded by variant contamination: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce DVD (Detection via Variance of generation Distribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a memory-adherence state and a perturbation-drift state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains—Omni-MATH and SuperGPQA—by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, DVD consistently outperforms perplexity-based, Min-k% probability, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.", "tldr": "We propose DVD, a variance-based detector that reliably identifies variant contamination in LLM evaluation, outperforming perplexity, edit-distance, and similarity baselines across datasets and models.", "keywords": ["Large Language Model", "Data Contaminment", "Deep Learning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2767f6fb89b7f7b0c804a63b2957e214b98038d1.pdf", "supplementary_material": "/attachment/0c22c7bf9993978f52a0f5f301b6d711d1740cd7.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a sampling-based method to detect variant contamination through the distribution variance of the likelihood of generated tokens. It also provides benchmarks for variant contamination detection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing of this paper is easy to follow.\n\n2. The construction of the benchmark dataset is helpful for evaluating the proposed task."}, "weaknesses": {"value": "1. My main concern is about the introduction of memory adherence and perturbation drift states. Is there any evidence that the model experiences these two states when conducting temperature scaling? Any flag for differentiating these two states? The authors should demonstrate the existence of the states before they design the method. Moreover, I think the introduction of the states should be at the beginning (e.g., introduction section), but in this paper, the detailed explanation is in section 4.\n\n2. I am wondering if there is any evidence that the training corpus of current LLMs contains variant contamination. It is important to validate the motivation with references.\n\n3. The authors only conduct experiments on the self-created benchmark. Although the authors claim that this paper is about variant contamination. More experiments on common data contamination datasets such as WIKIMIA."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZL7oJaPxFP", "forum": "Ubi631nNbI", "replyto": "Ubi631nNbI", "signatures": ["ICLR.cc/2026/Conference/Submission17020/Reviewer_3LXG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17020/Reviewer_3LXG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761415046029, "cdate": 1761415046029, "tmdate": 1762927043930, "mdate": 1762927043930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors introduce the problem of \"variant contamination\", which is a data contamination problem where contaminated samples are semantically equivalent but lexically diverse variants. The authors further propose the Detection via Variance of generation Distribution (DVD) to mitigate this new problem."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of using the model's fluctuation between \"memory adherence\" and \"perturbation drift\" is interesting.\n- The paper has a nice and steady flow, making it easy to understand the overall idea."}, "weaknesses": {"value": "### 1. Problem Definition\n- My major concern is with the problem of \"variant contamination\". I am not fully convinced that variant contamination is a problem and that it needs to be mitigated. First of all, it is ambiguous to draw a line between two sample variants to say which one is contaminated and which differs significantly from the original question. For example, in the case study in Appendix A.2, the first example shows a pair of problem samples. Honestly, I'd say these two questions are pretty different, even if the core reasoning trace required to solve the problem might be similar. In fact, this type of question variation is exactly the way humans are tested in math classes. Students are taught the core reasoning process, and they are tested on similar question types with different settings and numbers. We don't call that \"cheating\" -- that's how we \"learn\".\n- That said, for this problem formulation to make sense, I strongly think there needs to be a concrete way to control the level of semantic variation in generating samples. Could the authors provide at least a high-level way to quantify and control the level of semantic variation?\n\n### 2. Fine-tuning Details\nThe authors simulate the scenario of variant contamination by fine-tuning the models themselves. They use 10 epochs, which, I believe, is an excessively large number of iterations to simulate the real-world contamination environments. Usually, an LLM is trained on a single sample only once or twice. I think 10 epochs will leave a very strong fingerprint on the model and make the task way too easy. Please provide experiments on a setting where the model is trained on the data for 1 epoch.\n\n### 3. Verification of the Core Assumption in Method Design\nWhile the idea of utilizing the model's fluctuation between \"memory adherence\" and \"perturbation drift\" is interesting, whether that is actually the case has not been verified. Please provide a comparison between two plots (i.e., with and without variant contamination), where each plot shows the token log likelihood on the y axis and the token index on the x axis. If the authors' assumption is true, there will be occasional basins and plateaus in the log likelihood trend for variant-contaminated samples.\n\n### 4. Model Scales\nThe authors claim that DVD \"maintains robustness across model scales\" by demonstrating results on 1.5B, 3B, and 7B. While three variants are usually enough, I suggest trying out 32B if resources permit. In my opinion, an LLM smaller than 7B is weak for reasoning tasks and may not be a good specimen for contamination detection.\n\n\n### 5. Minor points\n- Figure 1 lacks a bit of detail to help understand the content. Please include a caption that explains the figure."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "U33zBHN1NQ", "forum": "Ubi631nNbI", "replyto": "Ubi631nNbI", "signatures": ["ICLR.cc/2026/Conference/Submission17020/Reviewer_on2q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17020/Reviewer_on2q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771838443, "cdate": 1761771838443, "tmdate": 1762927043514, "mdate": 1762927043514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies variant contamination, models encounter paraphrased or structurally altered versions of training data that still influence evaluation. The authors prompt LLMs to generate two benchmarks (Omni-MATH and SuperGPQA) by modifying existing datasets to include such variants. They also propose DVD (Detection via Variance of Generation Distribution), which measures the variance of synthetic difficulty across multiple generations under different temperatures. The key claim is that contaminated data cause the model to alternate between memory recall and exploration (drift) states, producing unusually high variance compared with uncontaminated cases. The method is evaluated on the two author-generated benchmarks and compared against perplexity, Min-k%, CDD, and embedding-similarity baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper draws attention to the problem of variant contamination and provides two datasets, which could be valuable resources for the community.\n* The proposed DVD method is conceptually simple. It only requires repeated sampling under temperature perturbations, making it easy to reproduce and extend."}, "weaknesses": {"value": "* The paper assumes that high generation variance reflects alternation between memory and reasoning states, but provides no behavioral or interpretive evidence that such alternation actually exists. The observed variance difference could arise from other factors, eg overfitting noise or instability in fine-tuned logits.\n\n* The models are fine-tuned for **10 epochs** with lr=1e-4 on contaminated data, simulating heavy retraining rather than realistic light contamination. Such strong exposure likely alters the model’s generation dynamics and confounds the claimed relationship between variance and contamination.\n\n* The motivation of the task rests on an implicit assumption: LLMs lack strong generalization ability on paraphrases -> LLMs will not produce confident or consistent outputs for paraphrased inputs. -> This makes previous contamination detectors (perplexity- or similarity-based) cannot distinguish contaminated from clean variants. However, the authors provide neither empirical evidence nor theoretical justification for this assumption. Without controlled contrasts between paraphrased generalization and memorized variants, the distinctiveness of DVD remains speculative."}, "questions": {"value": "* It would be informative to include a controlled experiment with light contamination, eg training for one epoch with a lr=1e-5, to see whether  DVD still distinguishes contamination from clean data. \n\n* It should include some statistical significance tests for table 2.\n\n* It would strengthen the evaluation to include additional contamination detectors, such as TS-Guessing, exposure-based memorization metrics, or membership inference probes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jz2svx94Aj", "forum": "Ubi631nNbI", "replyto": "Ubi631nNbI", "signatures": ["ICLR.cc/2026/Conference/Submission17020/Reviewer_GMqz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17020/Reviewer_GMqz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855371589, "cdate": 1761855371589, "tmdate": 1762927042887, "mdate": 1762927042887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes using the variance of normalized sum of K lowest probability tokens across multiple decodings of the same prompt as a measure to determine whether an example is contaminated or not. \n\nUsing K-min tokens is already an explored aspect in membership inference/contamination detection. However they extend this and present an intuitive explanation behind this measure. The explanation is based on two states that models are assumed to be operating under named \"memory adherence\" and \"perturbation drift\". \n\nThe experimental setup is LoRA finetuning various models on 10 epochs of modified versions of the Omni-Math and SuperGPQA to see if a method can detect contamination in that finetuned model. They experiment with Qwen and LLama models from 1.5B to 8B. The authors create the variants with a verified step which is an extra contribution of the paper. \n\nThey find that DVD(proposed method) achieves higher AUC compared to mink-prob, just using perplexity, cdd or embedding similarity."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The selected topic is an important topic. Variant contamination is an important topic to that should be explored in more detail. This could help us understand model capabilities and generalization better. \nThe way the authors create the variant data for Omni-math and SuperGPQA is quite good, this data can be quite useful in future experiments and can be used by the community. \nThe authors found a creative metric and presented it with an intuitive formulation.\n\nWhile the topic is important and the idea is quite good if the effectiveness can be shown in a generalizable way in the current state the paper needs to be improved significantly."}, "weaknesses": {"value": "The experimental setup seems to be rather challenging. Contamination in pre-training and finetuning are already known to have relatively large differences between them. This paper uses a LoRA finetuning which might even be more different making the results hard o generalize. While pre-training might be prohibitively expensive at least a full finetuning should be possible with the small models at hand. \n\nThere is no statistical tests or confidence intervals that can support the main claim of improvement above the baseline making claims more challenging to be accepted.\n\nWhile CDD is a relatively recent method min-K++ could have been used instead of min-K% as a stronger baseline. \n\nThe results are presented under 10 epochs of finetuning, it is hard to be convinced without seeing results without at least seeing one epochs and also performance of the models on said tasks after finetuning for 1 and 10 epochs.  \n\nThe paper has lots of repetitive parts particularly around the intuition between memory adherence and perturbation drift modes but there is no reference or foundation of behind these modes. While it is useful for authors to share intuition they have developed during the experiments towards a paper if they are going to be this central to the narrative a concrete grounding in the literature or empirical work around these modes could be expected."}, "questions": {"value": "Do you expect the results to generalize to full finetuning? \nShould we expect similar results when there is only one epoch of finetuning instead of 10? \nHow much do the models actually get better after 1 epoch of finetuning on modified samples? How much after 10? \nAre you going to run any significance tests? \nWould including stronger baselines be possible? \nIs the temparature value provided in the paper? Am I missing something?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r3C8E6MzJa", "forum": "Ubi631nNbI", "replyto": "Ubi631nNbI", "signatures": ["ICLR.cc/2026/Conference/Submission17020/Reviewer_VfGj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17020/Reviewer_VfGj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143250862, "cdate": 1762143250862, "tmdate": 1762927042248, "mdate": 1762927042248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}