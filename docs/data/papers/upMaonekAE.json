{"id": "upMaonekAE", "number": 6832, "cdate": 1757997283955, "mdate": 1759897890324, "content": {"title": "SAR: Scene-Action Representation for End-to-End Autonomous Driving", "abstract": "End-to-end autonomous driving systems have made remarkable progress by integrating perception, prediction, and planning into a fully differentiable framework. However, most existing methods either rely heavily on dense intermediate supervision (e.g., segmentation and mapping) or neglect behavior modeling, which leads to significant trajectory deviations and safety risks in highly interactive scenarios. To address these challenges, we propose SAR, a novel end-to-end scene action representation framework that enhances sparse scene modeling through structured behavior injection. Inspired by human driving cognition, SAR decomposes the scene into three complementary components: sparse scene semantics, ego-action awareness, and multi-agent action awareness. These components are fused via a specially designed Scene-Action Transformer to produce a consistent, interpretable, and interaction-aware representation for high-quality trajectory planning. Unlike prior approaches, SAR achieves strong generalization in highly interactive urban scenarios with only a small annotation cost. Experimental results on the nuScenes benchmark show that SAR reduces L2 trajectory error by 47% and collision rate by 41% compared to VAD. It also demonstrates superior robustness on NAVSIM and Bench2Drive, achieving new state-of-the-art performance in both open-loop and closed-loop evaluations. The code will be released soon.", "tldr": "", "keywords": ["autonomous driving", "scene representtation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f4a22af738b310fce3554b133b2bae634b20746.pdf", "supplementary_material": "/attachment/49d7272be9df6da944612e98d9d00f60ee3a3655.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an end-to-end autonomous driving framework SAR that enhances planning performance by designing a dedicated ego-vehicle feature extraction module and an agent motion interaction module. These components improve the model’s ability to capture motion features of surrounding agents, thereby reducing potential collision risks compared to prior works. The proposed method is evaluated on three benchmarks and demonstrates performance advantages over existing approaches. At the same time, the paper lacks some clarity and detail in explaining the design rationale and model details, which I will discuss below."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper further enhances the interaction strategy between agent motion and scene features compared to previous works, enabling more effective capture of critical information in driving scenarios, and the proposed method achieves consistent performance improvements across three benchmark datasets."}, "weaknesses": {"value": "1. Although the proposed interaction design among ego, scene, and agent action features is interesting, similar interaction concepts have been explored in prior works such as VAD [1] and UniAD [2], both of which also include modules for interacting with scene features (e.g., the ego-agent interaction module in VAD). My understanding is that this paper builds upon SSR [3] and further incorporates such an interaction module, which, however, leads to a significant increase in computational cost. As shown in Table 1, the inference speed is notably slower compared to SSR.\n2. In Section 3.5, after obtaining the final scene tokens, the paper does not provide any description of how these tokens are decoded into the final planning trajectory.\n3. In Section 3.3, Line 238 mentions the introduction of a safety threshold δ and a temporal threshold γ. However, the paper does not specify the actual values of these parameters used in the experiments, nor whether the same thresholds are applied across different datasets. Since these are manually designed parameters, I am concerned about the generalizability of this approach across datasets and scenarios.\n\n[1] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In ICCV, 2023.\n[2] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li. Planning-oriented autonomous driving. In CVPR, 2023.\n[3] Peidong Li and Dixiao Cui. Navigation-guided sparse scene representation for end-to-end autonomous driving. In International Conference on Learning Representations (ICLR), 2025."}, "questions": {"value": "1. In Section 3.3, Line 216, the Local Ego Encoder also relies on manually defined interaction rules. Have the authors considered using global BEV features instead of only focusing on the local area around the ego vehicle? In addition, a BEV mask is applied to select nearby features. What is the spatial range of this cropped region, and how was this value determined? I did not find any relevant explanation in the paper.\n2. Regarding the design of the Scene Action Transformer, an intuitive approach might be to use ego action queries to interact separately with the scene queries and agent action queries, and then decode the trajectory. Why do the authors instead choose to use scene queries as the queries in the attention interaction process rather than ego queries? Since the number of scene queries is likely much larger than that of ego action queries, this design choice could lead to a higher computational cost."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Rgaf8nzjYA", "forum": "upMaonekAE", "replyto": "upMaonekAE", "signatures": ["ICLR.cc/2026/Conference/Submission6832/Reviewer_Ddo6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6832/Reviewer_Ddo6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761363805857, "cdate": 1761363805857, "tmdate": 1762919096133, "mdate": 1762919096133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SAR (Scene-Action Representation), an end-to-end autonomous driving framework that enhances sparse scene modeling by injecting ego and multi-agent action awareness into a Scene-Action Transformer. SAR aims to improve trajectory planning in highly interactive scenarios while reducing reliance on dense BEV supervision. It reports SOTA results on nuScenes, NAVSIM, and Bench2Drive datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Good empirical performance: SAR achieves consistent improvements across three benchmarks, showing strong generalization in both open-loop and closed-loop settings.\n\n* Intuitive integration of ego-agent behavior into sparse scene modeling: The idea of injecting ego and agent action awareness into sparse scene queries via a Scene-Action Transformer is simple and intuitive, offering a fresh perspective on behavior-aware scene modeling.\n\n* Comprehensive ablation studies: This paper includes extensive ablations on scene query numbers, token combination, BEV modality, supervision, and hyperparameters, demonstrating methodological rigor."}, "weaknesses": {"value": "- Limited technical novelty.\n\nWhile the combination of sparse tokens and action injection is intuitive, the core components (e.g., cross-attention, deformable attention, token learner, scene-agent-ego transformer) are largely adapted from prior works (SparseDrive, VAD, DiffusionDrive, etc). The architectural contribution feels incremental.\n\n- Insufficient experiments.\n\nAll ablations are merely conducted on nuScenes dataset, which has been challenged many times in previous works [1,2] for its unconvincing open-loop performance for end-to-end AD model evaluation on common straightforward scenarios.\n\n- Poor presentation: \n\n1. Missing technical details about the token learner in Eq. (2), it is hard to guess how to select the informative tokens through a learnable token selector; \n2. In Line-217, missing details about  how to obtain the binary mask M_center; \n3. In Line-233, it seems an additional perception system is employed to predict the potential trajectories of both ego vehicle and all detected agents, which violates the definition of end-to-end AD framework proposed in SAR. Besides, the details about this additional systems are also missing, especially for its efficiency and precision; \n4. In Line-5, how to supervise these three weights during the training phase? Is it only soft attention without any guidance? If so, can you compare them in different scenarios to further verify their differences and explainability? \n5.This paper claims the necessity of sparse scene modeling. However, it still constructs computationally expensive BEV features, which makes the contribution somewhat limited. Is it possible to replace the BEV feature representation process with sparse one as in SparseDrive, SparseAD and DriveTransformer?\n6. In Line-297, missing details about the confidence-based selection mechanism used to identify top-ranked agent motion queries.\n7. Writing and formatting issues: Several grammar/typo issues (e.g., “Scene–Action Transformer” with inconsistent dash spacing, “*DefAttn*” vs. “DefAttn”, missing space in “soft,collision-aware risk embedding”, different input number of same function (DefAttn) as in Eq. (1) and Eq. (7) ).\n8. In Fig.3, it would be better to visualize the surrounding camera images to explain the meaning of highlighted spots.\n\nReferences: \n\n[1] Li Z, Yu Z, Lan S, et al. Is ego status all you need for open-loop end-to-end autonomous driving?[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 14864-14873.\n\n[2] Zhai J T, Feng Z, Du J, et al. Rethinking the open-loop evaluation of end-to-end autonomous driving in nuscenes[J]. arXiv preprint arXiv:2305.10430, 2023."}, "questions": {"value": "See weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wPTG9ymR4p", "forum": "upMaonekAE", "replyto": "upMaonekAE", "signatures": ["ICLR.cc/2026/Conference/Submission6832/Reviewer_GAwo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6832/Reviewer_GAwo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761376174121, "cdate": 1761376174121, "tmdate": 1762919095573, "mdate": 1762919095573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SAR (Scene-Action Representation), an end-to-end autonomous driving framework designed to improve the performance of sparse scene representation models, particularly in highly interactive scenarios. The authors argue that existing sparse methods (like SSR) lack robust behavior modeling.To address this, SAR enhances a sparse scene token representation by \"injecting\" structured behavioral information. The method achieves state-of-the-art results on nuScenes, NAVSIM, and Bench2Drive, showing significant reductions in trajectory error and collision rates compared to prior work."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents strong, state-of-the-art performance across three different benchmarks (nuScenes, NAVSIM, Bench2Drive). The significant reduction in both L2 error (47% vs. VAD) and collision rate (41% vs. VAD) is noteworthy.\n\n2. The ablation study in Table 4 clearly isolates the empirical benefits of the \"Ego action\" and \"Agent action\" components. The finding that the Ego Action Decoder alone drastically cuts the collision rate (from 0.34% to 0.16%) is a key, well-demonstrated result."}, "weaknesses": {"value": "1. The primary concern with this paper is its limited methodological novelty. The baseline Sparse Scene Tokenization is admittedly adopted directly from prior work (SSR). The Scene-Action Transformer is a standard cascaded cross-attention architecture, a common fusion pattern. The core of the \"Ego Action Decoder\" relies on TTC and DCPA, which are classic heuristics from the robotics and motion planning fields. Using them as features is a form of feature engineering, not a novel learned mechanism.\n\n2. The \"Dynamic Risk Awareness\" module relies on predicted future trajectories for both the ego vehicle and other agents (Section 3.3). This re-introduces a modular dependency and the very problem of error propagation (i.e., if the trajectory predictions are wrong, the risk signal will be wrong) that E2E models are meant to solve."}, "questions": {"value": "1. The TTC/DCPA calculations depend on predicted trajectories (Section 3.3). How does the model's performance (especially collision rate) degrade when these underlying trajectory predictions are inaccurate or noisy? Could the model become over-reliant on this heuristic and fail catastrophically when the pre-computed risk signal is wrong?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nBqdwrFpzg", "forum": "upMaonekAE", "replyto": "upMaonekAE", "signatures": ["ICLR.cc/2026/Conference/Submission6832/Reviewer_PoGo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6832/Reviewer_PoGo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815996604, "cdate": 1761815996604, "tmdate": 1762919094937, "mdate": 1762919094937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}