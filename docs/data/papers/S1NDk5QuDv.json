{"id": "S1NDk5QuDv", "number": 9406, "cdate": 1758121380920, "mdate": 1763719332532, "content": {"title": "I Spy With My Model’s Eye: Visual Search as a Behavioural Test for MLLMs", "abstract": "Multimodal large language models (MLLMs) achieve strong performance on vision-language tasks, yet their visual processing is opaque. Most black-box evaluations measure task accuracy, but reveal little about underlying mechanisms. Drawing on cognitive psychology, we adapt classic visual search paradigms---originally developed to study human perception---to test whether MLLMs exhibit the ``pop-out'' effect, where salient visual features are detected independently of distractor set size. Using controlled experiments targeting colour, size and lighting features, we find that advanced MLLMs exhibit human-like pop-out effects in colour or size-based disjunctive (single feature) search, as well as capacity limits for conjunctive (multiple feature) search. We also find evidence to suggest that MLLMs, like humans, incorporate natural scene priors such as lighting direction into object representations. We reinforce our findings using targeted fine-tuning and mechanistic interpretability analyses. Our work shows how visual search can serve as a cognitively grounded diagnostic tool for evaluating perceptual capabilities in MLLMs.", "tldr": "We apply visual search tasks from cognitive science to frontier models --finding that existing models exhibit human-like behaviour for a number of feature types.", "keywords": ["Multimodal Language Model", "Visual Search", "Cognitively Inspired Evaluation", "Pop out"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/944e07347b58f933379f1e78674d477feacf4ab8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates uses visual search paradigms to better understand VLM visual processing. They find that VLMs, like humans, show pop-out effects and capacity limits. Overall, the paper presents some novel results such as the lights prior experiment, but prior work has already covered most of the insights presented here. I find it hard to make a conclusive decision, I really enjoyed reading this paper, it is well put together and motivated, but it is limited in its significance given previous work."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- *Originality* I appreciate taking experiments from psychology and psychophysics and applying them to machine learning models. While I do not think previous work has investigated visual search as intensely, there is previous work that tests models in very similar paradigms.\n- *Quality* The paper is well structured and a great read.\n- *Clarity* The writing is clean and compelling.\n- *Significance* The paper is quite thorough in its investigation, however there is previous work with large overlap, diminishing its significance quite a bit. However, the lights prior results are definitely interesting."}, "weaknesses": {"value": "There is large overlap to [1], which also use conjunctive and disjunctive search to investigate VLM visual processing. While the authors mention this study in the related work, the degree of overlap is not well reflected. [1] already finds human-like capacity constraints in VLMs and furthermore they construct a larger theory that tries to explain VLMs problems in visual processing."}, "questions": {"value": "**Main questions**\n- Line 127 \"Humans are generally highly accurate in visual search tasks, and processing differences between experimental conditions are usually identified using response times. However, by limiting stimulus presentation time (e.g., to 1500ms) we can compare humans and LLMs using accuracy scores alone\", this seems to change the task quite a bit, no? In theory, the models here have no time constraint and you might end up testing humans and models in different paradigms. \n- Figure 2 seems to show that GPT-4o is worse when there are no distractors compared to when there are a small number. Do you have any idea why that might be?\n- Figure 2 seems a bit at odds to Figure 1 in [1]. They seem to report that GPT-4o, or all models in their paper for that matter, is perfect at a simple disjunctive visual search task. However, GPT-4o is not perfect for any number of distractors in your experiments. Do you have any idea on why that might be? \n- I find it hard to take away anything from the mechanistic interpretability analysis. The plots are too coarse and the trajectories look similar to me for almost all conditions, with decodability almost always being higher in the later layers. As such I find the take-away of \"This analysis showed that disjunctive search tasks were resolved at earlier layers relative to conjunctive tasks\" a bit overstated.\n- LIne 424 \"We found that fine-tuning on a conjunctive search task (2 Among 5) improved performance, though not to the level of pop-out, and did so even for larger set sizes absent from the training data\" this mirrors results in [2] and I would be interested if similar constraints to the level of generalization could be seen, like testing the \"2 Among 5\" trained model on the \"Circle Sizes\" data set.\n\n\n**Minor comments**\n- Line 64 \"GPT-4o (OpenAI et al., 2024), which accepts text-image prompt\", isn't that a general feature of vision language models? \n- Figure 7 on the right, what are the error bars here? When I tried to zoom in to see them properly the figure became pixelated, could it be that you are not using vectorized graphics? If so, you might want to consider replacing them. \n- Line 408 \"Disjunctive\" should not be capitalized\n\n\n\n[1] Campbell, Declan, et al. \"Understanding the limits of vision language models through the lens of the binding problem.\" Advances in Neural Information Processing Systems 37 (2024): 113436-113460.\n\n\n[2] Buschoff, Luca M. Schulze, et al. \"Testing the Limits of Fine-Tuning for Improving Visual Cognition in Vision Language Models.\" Forty-second International Conference on Machine Learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iVvFTPyQGN", "forum": "S1NDk5QuDv", "replyto": "S1NDk5QuDv", "signatures": ["ICLR.cc/2026/Conference/Submission9406/Reviewer_xT3L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9406/Reviewer_xT3L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761593445759, "cdate": 1761593445759, "tmdate": 1762921013301, "mdate": 1762921013301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces an evaluation framework for MLLMs using classic visual search paradigms from cognitive psychology. The authors test whether MLLMs exhibit human-like pop-out and feature-binding effects through three controlled experiments: size-based, color-shape-based, and lighting-based visual search."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Good Interpretability and Interesting Findings.\n- Sufficient Experiments."}, "weaknesses": {"value": "- Weak Exploration on Prompt Sensitivity and Robustness\n- Lack of Ablation"}, "questions": {"value": "- The prompt sensitivity is mentioned as a known limitation and provides a fixed set of prompts in the Appendix, but no ablations are included. Have you observed changes in model behavior when rephrasing the prompts or modifying response instructions (e.g., adding reasoning steps)? Would the performance degrade under slightly varied formulations?\n- Some classic visual search studies include target-absent conditions to better differentiate serial from parallel search and to evaluate false positives. Have you considered including such trials in your paradigm, and if so, what are the anticipated challenges in evaluating model responses under that setup?\n- While the use of synthetic stimuli is well-motivated and discussed, the current study stops short of testing generalization to more natural or cluttered scenes.\n- Prompting methods such as CoT that induce reasoning in LLMs have not been adequately evaluated. It would be informative to know whether such formats lead to better localization accuracy or more human-like search strategies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WHTNgFxTs2", "forum": "S1NDk5QuDv", "replyto": "S1NDk5QuDv", "signatures": ["ICLR.cc/2026/Conference/Submission9406/Reviewer_VKVx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9406/Reviewer_VKVx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924138160, "cdate": 1761924138160, "tmdate": 1762921012975, "mdate": 1762921012975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper adapts classic visual search paradigms from cognitive psychology to evaluate the perceptual capabilities of VLMs. The authors use controlled experiments to test whether VLMs exhibit human-like visual search phenomena, specifically \"pop-out\" effects in disjunctive (single-feature) search and capacity limits in conjunctive (multi-feature) search. Using tasks targeting size (Exp 1: Circle Sizes) and feature-binding (Exp 2: 2 Among 5), the authors find that advanced models like GPT-4o demonstrate human-like patterns: accuracy is high and stable regardless of distractor set size in simple disjunctive tasks (e.g., color search), but shows a set-size-dependent decline in conjunctive tasks that require binding shape and color. This suggests a limit in feature-binding capabilities. Experiment 3 (Light Priors) provides further evidence for human-like inductive biases, finding that GPT-4o not only incorporates a \"light-from-above\" prior but also shows a \"surprise\" effect, performing best on novel bottom-lit targets. The paper supports these behavioral findings with fine-tuning experiments and mechanistic interpretability analyses, which suggest that conjunctive tasks rely on deeper layers more than disjunctive search."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. *Strong Core Methods:* The core idea of adapting classic visual search paradigms (disjunctive vs. conjunctive) from cognitive science is a strong, interpretable, and well-grounded method for probing VLM behavior.\n2. *Novelty of Experiment 3:* Experiment 3 (Light Priors) is a clear strength. This is a clever test of a subtle inductive bias well-documented in humans. The finding that GPT-4o exhibits a human-like performance in this task is an interesting contribution.\n3. *Human Baseline:* The comparison with a human baseline provides necessary grounding for the claims about human-like performance patterns."}, "weaknesses": {"value": "1. *Limited scope of contribution*: The current paper should more clearly articulate what methodological or conceptual gap it is filling above and beyond previous work that has evaluated visual search capabilities in VLMs. Is the primary novelty the direct comparison to human performance? \n2. *Underdeveloped Supporting Analyses:* The fine-tuning (Sec 4) and mechanistic (Sec 5) results are presented as key differentiators but are underdeveloped in the main text, with many of the results only found in the appendix.\n    - *Finetuning*: The claim that finetuning mirrors human-like \"unitization\" may be an overstatement. The data (App. H) shows transfer from one shape task (\"2 among 5\") to another (\"T among L\"), but not to the \"Shape-Colour Conjunctive\" task. This suggests domain adaptation (learning shape discrimination) rather than the acquisition of a general, domain-agnostic visual search capability.\n    - *Interpretability Analysis:* Looking at Appendix Fig 20, probe accuracy does not appear to straightforwardly predict final model accuracy. More specifically, shape-color conjunctive probe accuracy is comparably high to disjunctive search probe accuracy in later layers, despite much lower conjunctive search performance. This makes strong conclusions about the relationship between probe accuracy and model performance difficult.\n3. *Human-Model comparison*: The comparison is weakened by the fact that humans had a 1500ms time limit while models had none. This makes direct comparison of \"difficulty\" problematic, given that a strict response deadline was imposed on participants."}, "questions": {"value": "1. Can the authors clarify what the primary new conceptual insight is from Experiments 1 & 2 that was not already established by other related work?\n2. The fine-tuning analysis shows a failure to transfer from \"Shape Conjunctive\" to \"Shape-Colour Conjunctive\" tasks. How does this support the claim of human-like unitization rather than simple domain adaptation (i.e., getting better at shape discrimination)?\n3. The mechanistic analysis shows that conjunctive tasks recruit deeper layers. How does this explain the behavioral finding (i.e., the set-size-dependent decline in accuracy)? Why does processing in later layers correlate with a failure to handle an increasing number of distractors? \n4. In the mechanistic results (Fig 20), there appears to be no significant difference between probe accuracy in the shape-colour conjunctive and disjunctive probes, despite a large difference in behavioral performance. Can the authors comment on this? Have they tested for a direct association between layer-wise probe accuracy and final model accuracy on a given task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n341Aed0Hr", "forum": "S1NDk5QuDv", "replyto": "S1NDk5QuDv", "signatures": ["ICLR.cc/2026/Conference/Submission9406/Reviewer_5cPK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9406/Reviewer_5cPK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146553061, "cdate": 1762146553061, "tmdate": 1762921012586, "mdate": 1762921012586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper adapts visual search paradigms from cognitive science to probe MLLM behavior. Its contributions are as follows:\n1. Evaluating MLLMs on three classic visual search paradigms. The author's findings are consistent with existing literature (i.e., Campbell et al. (2024)).\n2. Supporting finetuning and mechanistic interpretability analyses (details + questions below)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The core idea of adapting classic visual search paradigms (disjunctive vs. conjunctive) from cognitive science is a strong and interpretable way to probe MLLM behavior.\n2. Experiment 3 (Light Priors) is a clear strength. This is a novel test for a subtle, real-world inductive bias (light-from-above), and the result for a human-like \"surprise\" effect (better performance on bottom-lit targets) in GPT-4o is compelling. This is a strong contribution.\n3. The comparison with a human baseline (Appendix I) provides good grounding to make claims about human-like performance patterns."}, "weaknesses": {"value": "1. Justification of contribution: The paper needs a stronger justification for why its contribution is not incremental. As the authors acknowledge, the primary behavioral finding (that MLLMs show set-size-dependent accuracy on conjunctive search tasks) re-demonstrates the finding from Campbell et al. (2024). That paper established this serial-search-like-behavior in what they term ‘feature binding’ tasks. This paper’s contribution replicates their result with different stimuli. It may be that the authors' method of keeping stimulus types constant across tasks (Ref [421]) is an important control that Campbell et al. lacked, but this argument should be clearly made. The related work section should be expanded to explicitly state what methodological or conceptual gap from Campbell et al. this work is filling. The paper should also mention Budny et al. (2025, arXiv:2509.25142) which tests the identical hypothesis (a serial search deficit) by correlating VLM failures with human RT. This paper could address how its set-size-slope analysis provides a different or more robust form of evidence than Budny et al.'s RT-correlation analysis.\n2. Supporting analyses: The fine tuning (Sec 4) and mechanistic interpretability (Sec 5) results are presented as key differentiators but are underdeveloped in the main text, with all substantive results relegated to the appendix. Can you frontload these results? They are important w/r/t differentiating this work.\n(a) Finetuning: The claim that finetuning mirrors training effects in humans (Ref [471]) by creating ‘unitized’ representations (Ref [428]) may be an overstatement. The data (App. H) shows that training on \"2 among 5\" (a shape task) transfers to \"T among L\" (another shape task) but does not transfer to \"Shape-Colour Conjunctive.\" This does not suggest the model learned a general, human-like skill of feature binding; it suggests domain adaptation. Results showing that finetuning on one task generalizes to out-of-distribution tasks (not just OOD set sizes) would be necessary to make this general claim.\n(b) Interpretability analysis: The finding that disjunctive tasks activate earlier layers while conjunctive tasks activate later layers (Ref [407]) is a standard, unsurprising property of deep networks (i.e., simple features are processed early, complex features later). This analysis doesn’t provide a mechanism for the behavioral failure. It also doesn't explain why processing in later layers leads to a set-size-dependent drop in accuracy. Furthermore, it doesn’t seem from the appendix results (Fig 20) that probe accuracy (internal representation) straightforwardly predicts the final model accuracy (behavioral outcome).\n\nSome actionable requests to strengthen the paper: \n1. Formalize “serial search” with slope-based models.\nInstead of eyeballing whether a line on a graph is flat (pop-out) or steep (serial search) the authors could formalize this by including set_size (number of distractors) as a regressor in a statistical model to predict accuracy. This allows them to quantify the slope for each condition and, most importantly, statistically prove that the conjunctive slope is significantly more negative than the disjunctive slope.\n2. Rule out spacing/crowding confounds.\nThe conjunctive failure could be a low-level artifact of clutter/crowding (i.e., the target is, on average, closer to a distractor), not a high-level binding failure. The authors could include min target-distractor distance, mean nearest-neighbor distance, or some metric for clutter as covariates in the models from (A). This would demonstrate that the conjunctive slope persists even after controlling for low-level spatial density. \n3. Implement an error taxonomy to test for misbinding.\nIf the binding problem is the bottleneck, the model should make specific misbinding errors, not just random ones. The authors could partition errors into misbinding (choosing an item with one correct feature, e.g. finding a “red 5” when looking for “red 2”, and pure miss (no feature match or refusals). The misbinding rate should increase with set size in conjunctive tasks. \n4. Address the human-model comparability gap.\nHumans had a 1500ms time limit; models had none (because they are not time constrained). This means difficulty is hard to compare. Consider recording human RT or modifying the perceptual difficulty for models."}, "questions": {"value": "1. **Primary question: Can the authors clarify what the primary new conceptual insight is from Experiments 1 & 2 that was not already established by Campbell et al. (2024)?**\n2. The fine tuning analysis shows a failure to transfer from \"Shape Conjunctive\" to \"Shape-Colour Conjunctive\" tasks. How does this support the claim of human-like unitization rather than simple domain adaptation (i.e., getting better at shape discrimination)?\n3. The mechanistic analysis shows that conjunctive tasks recruit deeper layers. How does this explain the behavioral finding (i.e., the set-size-dependent decline in accuracy)? Why does processing in later layers correlate with a failure to handle an increasing number of distractors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CVEPiCLGfg", "forum": "S1NDk5QuDv", "replyto": "S1NDk5QuDv", "signatures": ["ICLR.cc/2026/Conference/Submission9406/Reviewer_8b6W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9406/Reviewer_8b6W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762196165498, "cdate": 1762196165498, "tmdate": 1762921012151, "mdate": 1762921012151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper adapts classic visual search paradigms (pop-out and conjunctive search) to benchmark multimodal LLMs on synthetic visual tasks and compare model behaviour to human performance.\nIt reports human-like behavioural signatures, simple fine-tuning effects, and preliminary mechanistic analyses of how models represent basic visual features."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear and timely motivation to study multimodal LLMs with structured, cognitively inspired visual search tasks rather than only end-to-end downstream metrics.\n\n- Experimental setups are well controlled and grounded in the visual search literature, with meaningful human baselines and interpretable behavioural metrics."}, "weaknesses": {"value": "- The overall scale of the study feels limited: a relatively small and narrow set of tasks and stimuli is considered, which makes it difficult to assess how robust or general the reported behavioural patterns really are beyond these controlled settings.\n\n- The core experiments appear to focus on a small number of high-profile models, which restricts both generality and reproducibility of the findings. In particular, strong open-weight models such as Molmo (or Pixmo) (Deitke et al., 2025), Qwen and its multimodal variants (Bai et al., 2023), LLaVA (Liu et al., 2023), and DeepSeek-v3 (Liu et al., 2024) are not evaluated.\n\n- The connection to real-world applications is underdeveloped: the paper motivates scenarios such as robotics or medical imaging, but does not explicitly translate each experimental finding into concrete guidance for model choice, system design, or risk assessment in these domains.\n\n- The manuscript structure, especially in the Related Work and Discussion / Conclusion sections, is somewhat monolithic. Breaking these sections into clearer subsections (for example, separating behavioural findings, fine-tuning, interpretability, and limitations) would substantially improve readability and highlight the main takeaways.\n\nReferences:\n\n- Liu, Aixin, et al. \"Deepseek-v3 technical report.\" arXiv preprint arXiv:2412.19437 (2024). \n- Liu, Haotian, et al. \"Visual instruction tuning.\" Advances in neural information processing systems 36 (2023): 34892-34916. \n- Bai, Jinze, et al. \"Qwen technical report.\" arXiv preprint arXiv:2309.16609 (2023). \n- Deitke, Matt, et al. \"Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025."}, "questions": {"value": "- How do the different tasks (for example, size search, digit search, shaded spheres, conjunction search) map onto realistic application scenarios, and what concrete design or safety implications should practitioners draw from the observed behaviours in each case?\n\n- How many distinct models are evaluated in total in the main experiments and in the appendix, and to what extent do the reported behavioural signatures hold consistently across them? A concise summary table of models and high-level findings would help assess generality.\n\n- Is it feasible to add results for a small set of modern open-weight models such as Molmo and Pixmo (Deitke et al., 2025), Qwen-based VLMs (Bai et al., 2023), LLaVA (Liu et al., 2023), or DeepSeek-v3 (Liu et al., 2024), so that the phenomena are not tied to a few proprietary APIs and reproducibility is improved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Dcwg3tswer", "forum": "S1NDk5QuDv", "replyto": "S1NDk5QuDv", "signatures": ["ICLR.cc/2026/Conference/Submission9406/Reviewer_wYKF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9406/Reviewer_wYKF"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9406/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763320517830, "cdate": 1763320517830, "tmdate": 1763320517830, "mdate": 1763320517830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}