{"id": "ABdgMoJhlO", "number": 24705, "cdate": 1758359505859, "mdate": 1759896753628, "content": {"title": "Micro-Macro Retrieval: Reducing Long-Form Hallucination in Large Language Models", "abstract": "Large Language Models (LLMs) achieve impressive performance across many tasks but remain prone to hallucination, especially in long-form generation where redundant retrieved contexts and lengthy reasoning chains amplify factual errors. Recent studies highlight a critical phenomenon: the closer key information appears to the model outputs, the higher the factual accuracy. However, existing retrieval-augmented language models (RALMs) lack effective mechanisms to ensure this proximity — external evidence is injected into reasoning via multi-turn retrieval, but this cannot ensure key information stays close to the outputs. We propose Micro–Macro Retrieval ($M^2R$), a novel retrieve-while-generate framework to fill this gap. At the macro level, $M^2R$ retrieves coarse-grained evidence from external sources; at the micro level, it extracts essential results from a key information repository built during reasoning and reuses them while generating answers. This design directly addresses the key-information–to-output proximity bottleneck, effectively reducing hallucination in long-form tasks. $M^2R$ is trained with a curriculum learning–based reinforcement learning strategy using customized rule-based rewards, enabling stable acquisition of retrieval and grounding skills.  Extensive experiments across different benchmarks demonstrate the effectiveness of $M^2R$, especially in lengthy-context settings.", "tldr": "", "keywords": ["Hallucination", "Long-form Hallucination", "Large Language Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5710516f18d73931026c561ed713487e66cf7b2f.pdf", "supplementary_material": "/attachment/e35b38d1e44c1c2fef1c8be73d7b36e2afd40b24.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to reduce LLM hallucinations in long-context settings, especially in the context of RAG.\nThe key limitation of current RAG approaches is that, either the long context makes it hard for the model to accurately identify key info, or model fails to retrieve intermediate results from its own reasoning chains.\nTo address this limitation, the paper introduces **micro-macro retrieval (M$^2$R)**, a retrieve-while-generate method.\nM$^2$R hinges on the positive correlation between key information proximity to model outputs and factual accuracy, which was reported previous works.\nM$^2$R directly enforces this proximity mechanism into the LLM through curriculum-learning with GRPO, such that *macro retrieval* (also `<think>` phase) implements traditional RAG and maintains a key info repo while *micro retrieval* (also `<answer>` phase) extracts key info from the established repo to ground model outputs.\n\nExperiment evaluation on multi-hop QA benchmarks show that M$^2$R generally outperforms most baselines and its comparative advantage is more evident in challenging scenarios (HotpotQA-2/3Q)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality\n\n   This paper is novel in that it alleviates hallucinations in RAG based directly on empirical insights from [1-2] that key information position methods in long-form generation.\n   The application of GRPO + curriculum learning to realize the insight above also makes sense.\n2. Quality\n\n   The experiment design is sound and directly supports central claims of this paper.\n   The authors have also done an excellent job by releasing the source code with clear documentation.\n3. Clarity\n\n   The paper clearly explains its motivation, key insights (lines 53-55, Appendix B), as well as discussions on the rationale of key designs (lines 120-122, 136-137, 161, 249-256, etc).\n   The case study is useful for readers to understand the method intuitively.\n4. Significance\n\n   This paper could be of significance to the field by grounding in empirical findings of previous works, combining GRPO and curriculum learning and achieve a compelling performance boost in challenging multi-hop QA tasks.\n\nReferences:\n\n[1] Lost in the middle: How language models use long contexts. (2023)  \n[2] Found in the middle: How language models use long contexts better via plug-and-play positional encoding. (2024)"}, "weaknesses": {"value": "(Authors do **not** need to refer to points raised in this section since the main points are already mentioned in *\"Questions\" section*.)\n1. W1: Limited model family\n\n   The paper only involves experiments on Qwen-2.5-3B/7B models; results on more diverse model families could strengthen the paper's arguments.\n2. W2: Missing discussions on costs\n\n   Since the core method introduces additional storage requirements (key info repo) and requires retrieving key info during answer phase, there are concerns regarding whether these components induce heavy storage/time costs."}, "questions": {"value": "**Major questions (that could affect rating)**\n1. **Question 1**: Limited model family\n\n   M$^2$R is only tested on two models of different sizes (3B, 7B) but the same model family (Qwen2.5). This limitation does *not* directly undermine the central claims of the paper, but results on more diverse model families could greatly enhance the general utility of the proposed method.\n2. **Question 2**: Cost analysis\n\n   Inference efficiency is a critical concern in RAG applications. The proposed method requires maintaining a key info repo during macro retrieval and retrieving key infos during micro retrieval. Therefore, a natural concern arises as to whether the performance boost is worth the cost:\n\n   *How does this micro-macro framework affect inference latency, and what are the storage costs of the key info repo?*\n\n   A detailed analysis (either theoretical or empirical) could be useful in deciding whether M$^2$R is usable in practice.\n\n**Minor questions and suggestions (that are not considered to affect rating)**\n1. **Minor question 1**: Additional details for reproducibility\n\n   Although the authors have provided source code and some experiment details in the paper, additional details such as hardware requirements and seeds could be useful for reproductions of results.\n2. **Suggestion 1**: Notations\n\n   The method name, M$^2$R, is not consistently presented: it is usually written in normal font but sometimes written in italics (lines 206-261, 267, etc).\n3. **Suggestion 2**: Paper organization\n\n   Related Work section could help readers set the context, but currently it is placed in the Appendix. Therefore I recommend move Table 1 (M$^2$R prompt template) to the Appendix and move Related Work section to the main body instead.\n4. **Suggestion 3**: Details on training stability\n\n   The paper mentions at lines 242-243 that, directly optimizing macro/micro retrieval leads to poor convergence. Detailed results (preferably placed in the Appendix) could help future researchers gain in-depth understandings regarding the significance of curriculum learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wK0EbahLpA", "forum": "ABdgMoJhlO", "replyto": "ABdgMoJhlO", "signatures": ["ICLR.cc/2026/Conference/Submission24705/Reviewer_vzvj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24705/Reviewer_vzvj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761362634023, "cdate": 1761362634023, "tmdate": 1762943168710, "mdate": 1762943168710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Micro–Macro Retrieval (M2R), a two-level retrieve-while-generate framework designed to reduce hallucination in long-form generation. By combining macro retrieval of coarse evidence during reasoning with micro retrieval of key information during answer generation, M2R ensures that essential evidence remains proximal to output tokens. Trained with a curriculum-based reinforcement learning strategy using rule-based rewards, the method achieves significant improvements in factual consistency and robustness across multi-hop QA and long-context benchmarks compared to strong RAG baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The idea of Micro–Macro Retrieval surprisingly natural and well-motivated — it addresses one of the most persistent issues in RAG systems (long-form hallucination) with a solution that feels both principled and minimal. The “retrieve-while-generate” framing elegantly captures how reasoning and retrieval should co-evolve.\n\n2. The paper is very carefully written. I particularly like how the authors formalize the two retrieval levels and the transition between \\<macro_tool_call>, \\<key_info_save>, and \\<micro_tool_call>. It feels like reading a well-designed system that could actually be implemented in production without hidden tricks.\n\n3. I really appreciate that the system is interpretable by design: it shows that we can literally see the reasoning flow: what it retrieved, what it saved, what it reused. That’s a refreshing contrast to the black-box nature of most retrieval-augmented models. It also feels cognitively aligned with how humans solve tasks: note things down, then recall them precisely.\n\n4. Compared with Self-RAG[1] this work feels like a thoughtful continuation rather than simple imitation. Self-RAG let the model decide when to retrieve; M2R turns that spark into a full reasoning routine. It not only detects when retrieval is needed but also explicitly manages what to keep and reuse, maintaining a long-term internal memory grounded in already verified facts. I find this progression deeply satisfying—the model isn’t merely “asking for help” anymore; it’s learning to remember what it already knows to be true. That evolution from reactive retrieval to proactive self-memory feels like a genuine step forward.\n\n[1] Asai, Akari, et al. \"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.\" The Twelfth International Conference on Learning Representations."}, "weaknesses": {"value": "I think this paper has no particularly obvious weaknesses. Unlike the naïve combination of RAG with RL or GRPO, this work takes a much more principled approach—from memory to retrieval to the overall training strategy, making it a significant step forward in improving RAG performance. However, I do have a few questions that I’d like to raise briefly.\n\n1. In your gradient computation implementation, how did you mask out the information from the retrieval part? Is the positional information handled relative to the retrieval step, or do you directly remove the masked portion?\n2. When invoking retrieval, are macro-retrieval and micro-retrieval mutually exclusive, or can they be used jointly?\n3. When is **key_info_save** called? In your experiments, is it triggered only when the macro retrieval is considered contextually relevant, and thus the macro information is saved? If there are identical or similar **micro_tool_calls**, does the model jointly retrieve them?\n4. If the model chooses to invoke **micro**, but there is no stored memory or relevant content, does that lead to hallucination? How do you handle cases where micro-retrieval fails or retrieves incorrect results?\n5. Are there situations where neither **micro** nor **macro** is used?\n6. What are the overall training costs and latency characteristics? If the database is large or the query is long, does it lead to bottlenecks?\n7. If provides a main figure, it will help more readers to fastly grasp your methodology."}, "questions": {"value": "See above, I am warmly welcome to discuss further detailed on this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pp3iDQ6LT6", "forum": "ABdgMoJhlO", "replyto": "ABdgMoJhlO", "signatures": ["ICLR.cc/2026/Conference/Submission24705/Reviewer_KBYP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24705/Reviewer_KBYP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761496383612, "cdate": 1761496383612, "tmdate": 1762943168530, "mdate": 1762943168530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles hallucination in large language models (LLMs), especially in long-form generation where redundant contexts and extended reasoning amplify factual errors. The authors observe that factual accuracy improves when key information appears closer to the generated output, yet existing retrieval-augmented LMs (RALMs) lack mechanisms to ensure this proximity.\nTo address this, they propose Micro–Macro Retrieval (M2R), a retrieve-while-generate framework combining macro-level external retrieval with micro-level key information reuse from an internal repository built during reasoning. M2R directly maintains evidence proximity to outputs and reduces hallucination in long-context tasks.\nTrained via curriculum-based reinforcement learning with rule-based rewards, M2R achieves consistent gains in factual accuracy and grounding across multiple benchmarks, showing good effectiveness in lengthy-context scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The topic is valuable, especially for mitigating hallucinations in long-form reasoning models.\n\n2. The method is straightforward and conceptually sound.\n\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. The core methodological details are underspecified. If I understand correctly, the approach hinges on constructing and maintaining a key-information repository, and then:\n\n- (1) For macro retrieval: per Lines 060–061, how is “the reasoning process yields answer-aligned evidence” detected, and how exactly is it inserted into the repository(i.e., what constitutes the key and the value)?\n\n- (2) For micro retrieval: what query is used to retrieve answer-related information from the repository?\n\n- (3) Does GRPO merely teach the LLM when/how to invoke the macro- and micro-retrieval tools, rather than optimizing the retrieved content?\n\n2. Empirical coverage is limited. Training is conducted only on Qwen2.5-3B/7B and evaluated on four relatively simple QA datasets. How does the method perform on other model sizes/families and on more challenging reasoning benchmarks? Moreover, Figure 2 shows substantial reward oscillation and a low mean (~0.4), suggesting training stability or sufficiency may be a concern.\n\n3. FlashRAG details and ablations require clarification, including the knowledge base size, and the effects of chunk size and retrieve number on performance. Reporting token statistics of input/output during inference would further clarify whether the approach practically alleviates long-form reasoning constraints."}, "questions": {"value": "Please see the weaknesses,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j0wUrAma7E", "forum": "ABdgMoJhlO", "replyto": "ABdgMoJhlO", "signatures": ["ICLR.cc/2026/Conference/Submission24705/Reviewer_Sm15"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24705/Reviewer_Sm15"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836365492, "cdate": 1761836365492, "tmdate": 1762943168304, "mdate": 1762943168304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A \"retrieve-while-generate\" framework that performs macro retrieval (external sources during reasoning) and micro retrieval (from key-information repository during answer generation) to ensure evidence proximity and reduce hallucination."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong Empirical MotivationLost in the Middle Phenomenon (Liu et al. 2023):\n\nGPT-3.5 accuracy drops from 75% → 55% when answer-bearing evidence moves from context start to middle (Figure 3)\nEmpirically validated across multiple models and tasks\nTheoretical Grounding (Appendix B):\n\nAnalyzes RoPE positional encoding: attention ∝ q^T R_{θ,m-n} k\nHigh-frequency components cancel at large distances Δ\nFormal claim: \"Evidence contribution decreases monotonically with distance\"\nThis foundation is solid — problem is real and well-documented.\n\n2. Explicit Key Information ManagementKey-Value Repository Design:\n\nAdvantages:\n\n- Explicitly separates \"what to remember\" from \"how to answer\"\n- Forces model to extract atomic facts rather than rely on context attention\n- Reduces cognitive load during answer generation\nThis is cleaner than implicit reasoning traces (e.g., ReSearch where key info is buried in <think> text)."}, "weaknesses": {"value": "1. \"Retrieve-While-Generate\" might be Misleading\n\nWhat the Paper Claims:\n\n\"M²R is the first framework to introduce a retrieve-while-generate paradigm during the answer phase.\"\nThis suggests a novel generation mechanism — e.g., retrieval happening during the forward pass.What Actually Happens:Multi-Turn Generation with Tool Calls:\n\nThis is identical to:\n\nOpenAI's function calling\nAnthropic's tool use\nReAct (Yao et al. 2022)\nSelf-RAG (Asai et al. 2023) — which also retrieves during answer generation!\nM²R requires 5-10 sequential model invocations per query.\n\n2. Cost Analysis Completely Absent"}, "questions": {"value": "1. How many model invocations does M²R require per query on average?\n\nPlease report separately for: (a) think phase, (b) answer phase, (c) total\nBreak down by dataset (HotpotQA, MuSiQue, etc.)\nWhat is the range (min/max)?\n\n\n\n2. What is the end-to-end latency in realistic deployment?\n\nTable 2 shows batch inference time (0.67s), but what about non-batched API calls?\nAssuming 100ms per forward pass: how long does a typical query take?\nHow does this scale with question complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k4c4m42D23", "forum": "ABdgMoJhlO", "replyto": "ABdgMoJhlO", "signatures": ["ICLR.cc/2026/Conference/Submission24705/Reviewer_Jaoq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24705/Reviewer_Jaoq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965160202, "cdate": 1761965160202, "tmdate": 1762943168034, "mdate": 1762943168034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}