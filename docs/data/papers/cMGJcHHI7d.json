{"id": "cMGJcHHI7d", "number": 13969, "cdate": 1758226224487, "mdate": 1759897399374, "content": {"title": "Only Brains Align with Brains: Cross-Region Patterns Expose Limits of Normative Models", "abstract": "Normative models of brain regions aim to replicate their representational geometry and are widely used to study neural computation. Model–brain alignment is typically assessed with metrics such as representational similarity analysis (RSA) and linear predictivity (LP). Recent studies, however, show that conclusions from such benchmarks depend strongly on the choice of metric, raising a deeper conceptual problem: what does “brain alignment” truly measure? We address this by testing a broad spectrum of vision models on the BOLD-Moments video fMRI dataset and analyzing the influence of the alignment metric in greater detail. While benchmarks can identify a nominally best model, many other models fall within subject-level variability and are therefore practically equivalent. To move beyond metric dependence, we introduce Alignment Pattern Similarity (APS), a framework that uses brain-to-brain alignment as ground truth for evaluating normative models. For each region, we compare its empirical alignment with other regions against the alignment obtained when replacing that region’s activity with its normative model. Strikingly, while normative models can align well with their target region, their cross-region alignment patterns diverge systematically from those observed in the brain. This reveals a key deficiency: current normative models do not faithfully reproduce brain-to-brain alignment patterns when substituted for real neural data. Furthermore, we show that structural connectivity can predict aspects of these alignment patterns, illustrating how anatomical constraints may additionally guide expectations about functional correspondence. Overall, APS shows great promise to become a principled framework for more robust and biologically meaningful assessments of brain–model alignment.", "tldr": "We expose the limits of brain alignment of SOTA video models, and propose a framework based on cross-region alignment patterns in the brain towards more robust and meaningful assessment of brain-model alignment.", "keywords": ["brain alignment", "benchmarking", "representational similarity analysis", "video models"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9177ed2129359bb5884aee9176a93f9f25ab422a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper benchmarks 47 image and video models on BOLD-Moments fMRI with RSA and linear predictivity, finding that rankings depend on the metric and many models are practically equivalent within subject variability. It proposes Alignment Pattern Similarity, grounded in structural connectivity, to test whether models preserve each ROI’s cross-region similarity profile, and reports that brains align with brains while models generally fail to match these patterns, arguing for stricter anatomy-informed evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The authors raise a very important and timely issue regarding AI-model alignment.\n- The authors propose a solution that I think is quite novel, which is checking the inter-regional similarities between brain regions and with one region replaced by its best-fit model.\n-  Showing that most models fall within subject-level variability is a much-needed report.\n-  I personally find the high APS for brain-brain alignment very interesting (but could the author please provide the baseline elaborated in the weaknesses section below?)."}, "weaknesses": {"value": "- I have a philosophical, high-level concern. I am not sure if requiring the model representation (that has a best fit to one brain region) to have relationships to other brain regions in a way that one brain region relates to those brain regions is an unnecessarily strict requirement.  But I do think it is still an interesting thing to check and report, which could be the author’s point.\n\n- In this line of thought, I think another interesting analysis would be\n1. First, (within a single model) measure the similarity between layers that have high alignments to brain regions of interest. Call that s’_model_RSA(i,j) (or s’_model_LP) for brain regions i and j. For example, say layer 3 corresponds to ROI 1 of the brain, and layer 5 corresponds to ROI 2 of the brain. Then s’_model_RSA(1,2) measures the similarity between layer 3 and layer 5 of the networks.\n2. Measure and report R^2 between S’_model_RSA and brain-to-brain S_RSA (the latter one is from the paper e.g. Figure 5d top left red line).\nWould it be easy to perform this analysis? This is less strict than the author’s requirement. Qualitatively speaking, it is like saying we don’t care exactly how layers talk to each other (and how the ROIs talk to each other), and the only thing we care about is the relationship (RSA/LP value) between these layers and the relationship (RSA/LP value) between these ROIs. We might want these relationships to be similar (high R^2) between the model and the brain.\n\n- Perhaps more importantly, I think the authors should report the baseline for APS, where the ROI connectivity graph is randomly generated.\n\n- I understand that the paper's idea and methodology are not easy to explain, but I think the clarity of the paper can be dramatically improved. Figure 1 is well-intended, but it only makes sense after reading the entire paper. As a reader, it is especially unclear what the cartoon plots under \"Alignment Patterns\" indicate (one would wonder that the x-axis and y-axis are), unless they have read section 3.5 (which I don't think is also clearly written). I am not clear what the best way is to explain the overall pipeline, but I don't think Figure 1 helps much at all.  In section 3.5, the last sentence was nearly impossible to understand."}, "questions": {"value": "Please see the weaknesses section for the question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vDgO2OepQd", "forum": "cMGJcHHI7d", "replyto": "cMGJcHHI7d", "signatures": ["ICLR.cc/2026/Conference/Submission13969/Reviewer_yHdD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13969/Reviewer_yHdD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944581224, "cdate": 1761944581224, "tmdate": 1762924468758, "mdate": 1762924468758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates 47 vision models on the BOLDMoments video fMRI dataset using RSA and linear predictivity (LP) to assess brain-model alignment. The authors identify V-JEPA2 as achieving strongest alignment but show that many models are \"practically equivalent\" within subject-level variability. They introducxe Alignment Pattern Similarity (APS), a novel metric that compares cross-region alignment patterns to anatomical connectivity patterns. The key finding: while normative models can align well with individual brain regions, they fail to reproduce brain-to-brain cross-region alignment patterns, revealing a fundamental limitation of current approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Important conceptual contribution:** APS is a genuinely novel idea that addresses a real gap in the field. The insight that models should preserve relational structure across regions (not just match individual regions) is valuable and biologically motivated.\n2. **Comprehensive model evaluation:** Testing 47 models spanning diverse architectures (CNNs, Transformers), objectives (supervised, self-supervised, multimodal), and modalities (image, video) on a large video dataset is thorough.\n3. **Strong empirical results:** The finding that brain-to-brain APS is high while model-to-brain APS is near-zero is an interesting result."}, "weaknesses": {"value": "1. **Figure quality is severely lacking:** The sketch-based design of figure 1 is not ideal for a publication at a top tier conference. I would recommend redoing it with vector graphics. The other figures have almost illegible axis labels with overlapping/compressed model and ROI names. \n2. **Limited to RSA and LP Despite Known Limitations:** Paper correctly identifies that \"conclusions depend strongly on choice of metric\" yet only uses RSA and LP. Missing modern metrics like CKA[1], RTD[2] and NSA[3]\n3. **APS Validation is Incomplete:** No null model is presented. What is expected APS for random alignment patterns? No p-values or significance tests. Could spatial proximity or retinotopy explain results instead of connectivity? Does APS generalize to NSD or HCP 7T?\n4. **Model-Brain APS Results Are Hard to Interpret:** Low APS could mean: (a) models fundamentally limited, (b) wrong layer selection, or (c) APS too strict but the authors assert (a) without ruling out the other options. Layer-wise analysis of APS is also missing."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9NMaZdKIwA", "forum": "cMGJcHHI7d", "replyto": "cMGJcHHI7d", "signatures": ["ICLR.cc/2026/Conference/Submission13969/Reviewer_NsfX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13969/Reviewer_NsfX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956501109, "cdate": 1761956501109, "tmdate": 1762924468315, "mdate": 1762924468315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper benchmarks tens of image and video models on the BOLD-Moments fMRI dataset using Representational Similarity Analysis (RSA) and Linear Predictivity (LP), argues that many models are practically equivalent despite their rankings because their model→brain alignment falls within the distribution of brain↔brain alignment, and introduces Alignment Pattern Similarity (APS)—a connectivity-grounded test of whether a model preserves each ROI’s cross-region similarity pattern. The key claim is that, although some models score well on RSA/LP, they generally fail to reproduce brain-to-brain cross-region patterns under APS, whereas brains do."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles the growing concern that conclusions about “brain alignment” depend on metric choice, and proposes a relational criterion that goes beyond local fits.\n- The practical-equivalence analysis is a strong way to assess when model-ranking differences are statistically meaningful.\n- Creative attempt to incorporate anatomical priors and cross-region dependencies into model evaluations.\n- It's nice that the paper includes a diverse set of image and video models across supervised and self-supervised objectives."}, "weaknesses": {"value": "- My biggest concern is that the three aims—(1) benchmarking image/video models on the BOLD Moments dataset, (2) demonstrating practical equivalence of several models (in the sense that they lie within the brain-brain similarity range), and (3) introducing APS—feel only loosely connected. Each of these components could stand as an independent contribution, but when presented together, the narrative lacks a clear causal or conceptual throughline. For instance, the benchmark and equivalence analyses establish the limitations of RSA/LP metrics but are not explicitly framed as motivating APS; instead, APS appears as a parallel idea rather than a methodological extension.\n- The practical-equivalence conclusion hinges on the brain↔brain baseline, which is constructed by averaging voxel responses across subjects before comparing to a held-out subject. This raises concerns about functional alignment (are voxels/topographies aligned across individuals?) and the arbitrariness of cross-subject averaging. The authors should: (i) report within-subject baselines, (ii) test hyperalignment or response-based alignment, and (iii) evaluate subject→subject prediction without averaging. Related ideas have appeared in the NeuroAI Turing Test (Feather et al., 2025), which formalizes a similar distributional criterion for model evaluation; citing and differentiating from that work would clarify novelty.\n- The phrase “shifting the focus from single-metric rankings to the stability of rankings” is confusing, since two metrics (RSA, LP) are still used. The authors actually examine whether model orderings are stable given subject variability—i.e., when multiple models are within the brain↔brain alignment range. This should be explicitly defined early to avoid misinterpretation.\n- The statement that “recent benchmarks of video models have relied on a single alignment metric, while metric-comparison studies rarely include modern video architectures” should be supported with citations"}, "questions": {"value": "- How sensitive are the practical-equivalence findings to the brain↔brain baseline (averaged vs. individual subject predictors; with/without hyperalignment; within-subject splits)?\n- Could you compare your equivalence criterion to the NeuroAI Turing Test (Feather et al., 2025) to delineate conceptual differences and overlapping assumptions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U5vvuS0wxj", "forum": "cMGJcHHI7d", "replyto": "cMGJcHHI7d", "signatures": ["ICLR.cc/2026/Conference/Submission13969/Reviewer_jG2B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13969/Reviewer_jG2B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762234258031, "cdate": 1762234258031, "tmdate": 1762924467767, "mdate": 1762924467767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}