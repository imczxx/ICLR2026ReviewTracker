{"id": "1USeVjsKau", "number": 3516, "cdate": 1757456399726, "mdate": 1763719583946, "content": {"title": "ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference", "abstract": "Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We also co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.", "tldr": "", "keywords": ["quantization", "large language models", "model compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c93ef11c88c9e25e304a7d967475d56f18094168.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents ParoQuant, a weight-only post-training quantization (PTQ) method designed for reasoning LLMs. The core idea is to apply scaled pairwise rotation, combining independent Givens rotations with channel-wise scaling, to suppress outliers efficiently. The authors co-design a CUDA kernel to maintain high throughput. Experiments show consistent accuracy gains over AWQ and EfficientQAT, with less than 10% latency overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The motivation that, quantization error accumulation in reasoning models, is clear and important.\n\nThe proposed rotation-based PTQ design is both novel and hardware-efficient.\n\nExtensive experiments on multiple model sizes (up to 70B) and reasoning benchmarks (MMLU-Pro, GSM8K, etc.) demonstrate solid improvement.\n\nPaper is clearly written and well-structured, with good algorithmic detail and ablations."}, "weaknesses": {"value": "While the rotation kernel is claimed to be efficient, the paper lacks quantitative breakdown of runtime and memory overhead (e.g., FLOPs, memory traffic).\n\nThe scalability of independent rotations to larger group sizes or mixed-precision settings (e.g., W4A8) is not discussed.\n\nMore analysis on the trade-off between number of rotations and latency would strengthen the efficiency claim.\n\nThe method seems tailored for linear quantization; extension to vector quantization or activation quantization could be briefly discussed."}, "questions": {"value": "Could the authors provide more detailed profiling on GPU resource usage and explain how ParoQuant scales when group size or model size further increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XWntLRWN8f", "forum": "1USeVjsKau", "replyto": "1USeVjsKau", "signatures": ["ICLR.cc/2026/Conference/Submission3516/Reviewer_kgGQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3516/Reviewer_kgGQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547465752, "cdate": 1761547465752, "tmdate": 1762916777436, "mdate": 1762916777436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ParoQuant, a weight-only post-training quantization method developed to improve the accuracy and efficiency of LLMs. It tackles the challenge of quantization error through a novel combination of independent Givens rotations and channel-wise scaling, which effectively reduces the impact of outliers. Additionally, the work incorporates a custom CUDA kernel to accelerate online scaled pairwise rotations, enabling faster inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear and Well-Founded Motivation: The paper observes that rotating only the top 10% of the most significant weight channel pairs can achieve nearly the same reduction in quantization error as performing a full rotation. This insight eliminates a large amount of redundant computation from full matrix multiplications, leading to a much more efficient quantization process.\n\n2. Methodology with GPU-Aware Design: Building on this motivation, the authors propose a three-step design for the scaled pairwise rotation transform.\n\nStep 1: Replace costly full orthogonal matrix multiplications with a set of decomposed Givens rotations.\n\nStep 2: Eliminate inter-rotation dependencies to allow fully parallel execution on GPUs, resulting in independent rotations.\n\nStep 3: Since a single independent rotation cannot adequately capture complex weight distributions, apply a series of independent rotations combined with channel-wise scaling to improve representation and quantization accuracy.\n\n3. Practical CUDA Implementation: The paper further introduces a co-designed efficient transform kernel that maximizes GPU parallelism by executing computations across three levels:\n\nToken-level: Parallelization over the token dimension of the activation tensor.\n\nChannel-group level: Different CUDA blocks handle different groups of channels.\n\nPair level: Each rotation pair is processed by a separate CUDA thread."}, "weaknesses": {"value": "Overall, I found the paper well-written and technically solid. The following are just minor curiosities rather than critical weaknesses:\n\n1. The 4-bit performance gains appear somewhat modest for certain model sizes and tasks (e.g., Perplexity and AIME). It would be interesting to see whether ParaQuant delivers more substantial improvements at lower bitwidths, such as 3-bit or 2-bit quantization.\n\n2. Do you have any insight into why E-QAT performs particularly poorly on AIME, given that ParaQuant can essentially be viewed as an extension of E-QAT with additional learnable rotations? I am expecting the performance gap between the two methods to be smaller."}, "questions": {"value": "See Above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tSLgNstDaX", "forum": "1USeVjsKau", "replyto": "1USeVjsKau", "signatures": ["ICLR.cc/2026/Conference/Submission3516/Reviewer_xZZn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3516/Reviewer_xZZn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808448333, "cdate": 1761808448333, "tmdate": 1762916776711, "mdate": 1762916776711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely appreciate the reviewers’ thoughtful and encouraging feedback. Across all four reviews, several strengths of our work were consistently highlighted:  \n- **Clear motivation:** a timely focus on the unique sensitivity of reasoning LLMs to quantization error accumulation.  \n- **Methodological novelty:** the scaled pairwise rotation transform, with the insight that independent Givens rotations can efficiently approximate full rotations.  \n- **Strong algorithm-system co-design:** an efficient CUDA kernel that enables practical deployment with low runtime overhead.  \n- **Comprehensive evaluation:** robust accuracy improvements and low overhead across models of many scales, particularly on long-chain reasoning tasks.\n\nWe are grateful for these positive assessments. In response to the reviewers’ suggestions, we have made several revisions and clarifications. All main conclusions of the paper remain unchanged.\n\n**1. Simplified pair selection (Algorithm A1) and updated results**  \nWe replaced the original greedy pairing strategy with random selection. Our theoretical analysis and new experiments show that random pairing performs comparably to greedy pairing while simplifying the PTQ pipeline. All ParoQuant results have been updated accordingly (see response to reviewer *hpqW*).\n\n**2. Added QTIP results on the Qwen3 family**  \nWe extended QTIP comparisons to the Qwen3 models, following the calibration setup used in the QTIP paper for Llama-3 to ensure consistency.\n\n**3. Unified evaluation of QTIP and EfficientQAT**  \nTo ensure fair comparisons, we now evaluate both methods using their pseudo-quantized FP16 models with the same unified evaluation script as ParoQuant, rather than relying on their original real-quantized inference pipelines.\n\n**4. Improved reasoning-task evaluation**  \nFollowing best practices from Liu et al. [1], we refined our evaluation pipeline for reasoning tasks:  \n- Adopted LightEval [2], which provides stronger reasoning-task support than lm-eval [3].  \n- Increased the generation limit from 8k to 32k tokens to avoid truncation.  \n- Added results for Qwen3-4B and MMLU-Pro for DeepSeek-R1-Distill-Llama-8B.  \n- For GPQA-Diamond, AIME-24, and AIME-25, we now report averages over 3 seeds.  \n- GSM8K was removed due to its short-answer format and limited diagnostic value for long-chain reasoning.\n\n**5. Extension to weight–activation quantization (W4A4, W4A8)**  \nAs requested by reviewers *7v1a* and *kgGQ*, we extended ParoQuant to **weight-activation quantization**, including INT4/INT8 and FP4 formats (MXFP4, NVFP4). Across all mixed-precision settings, ParoQuant consistently outperforms baseline methods. Additional results are provided in our response to reviewer *7v1a*.\n\n---\n\n[1] Liu et al. *Quantization hurts reasoning? An empirical study on quantized reasoning models.*  \n[2] https://github.com/huggingface/lighteval  \n[3] https://github.com/EleutherAI/lm-evaluation-harness"}}, "id": "zyqW249z9p", "forum": "1USeVjsKau", "replyto": "1USeVjsKau", "signatures": ["ICLR.cc/2026/Conference/Submission3516/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3516/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3516/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763717996488, "cdate": 1763717996488, "tmdate": 1763717996488, "mdate": 1763717996488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ParoQuant, a novel weight-only PTQ method for reasoning LLMs. It uses a \"scaled pairwise rotation\" transform, combining channel-wise scaling with hardware-efficient Givens rotations to suppress outliers. Through algorithm-system co-design, it achieves high accuracy with low inference overhead. It provides a very systematic solution, and the experiments are very comprehensive."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper clearly identifies and addresses a critical, forward-looking problem: the poor performance of efficient quantization methods on reasoning tasks that require long chains of thought. This focus on error accumulation in generative tasks is timely and important.\n\n2.\tThe proposed \"scaled pairwise rotation\" is a novel and elegant solution. The insight that a full rotation matrix is redundant and can be effectively approximated by a series of independent, parallelizable Givens rotations is the key contribution and is very well executed.\n\n3.\tThe algorithm-system co-design is a major strength. The authors didn't just propose a transform; they designed a custom CUDA kernel that makes the transform viable in practice, demonstrating a deep understanding of both the algorithmic and hardware constraints. The empirical results, showing ParoQuant matching QTIP's accuracy while being ~25% faster, are very compelling."}, "weaknesses": {"value": "1.\tThe greedy pair selection strategy outlined in Algorithm A1, while effective and intuitive, may not be globally optimal. It would be beneficial for the authors to discuss the potential limitations of this greedy approach.\n\n2.\tIn Section 3, when discussing quantization degradation on reasoning tasks, the authors should cite other recent works that have also identified this specific problem (e.g., QSPEC) to better contextualize their motivation.\n\n3.\tIn Figure 3, some text labels in the right-most portion of the diagram are overlapped, which slightly hinders readability."}, "questions": {"value": "1.\tThe number of independent rotations is fixed at K=8 for most experiments. How was this number chosen? Is there a clear point of diminishing returns, and does the optimal value of K change depending on the model architecture or size?\n\n2.\tThe \"pairwise\" rotation in Algorithm A1 is effective but seems conservative. Did the authors consider more fine-grained or alternative rotation structures, such as rotating small blocks of channels against each other? While this might be more expressive, it would likely introduce significant scheduling overhead. A discussion on this potential trade-off between rotation granularity and scheduling efficiency would be insightful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SvCQLKECxP", "forum": "1USeVjsKau", "replyto": "1USeVjsKau", "signatures": ["ICLR.cc/2026/Conference/Submission3516/Reviewer_hpqW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3516/Reviewer_hpqW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832808189, "cdate": 1761832808189, "tmdate": 1762916776175, "mdate": 1762916776175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ParoQuant, a weight-only post-training quantization (PTQ) method designed for reasoning LLMs, where quantization errors can accumulate over long generations.\nParoQuant combines: 1.Independent Givens rotations (pairwise rotations) to suppress outliers efficiently, and 2.Channel-wise scaling to even out magnitude across channels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors convincingly argue that reasoning LLMs are especially sensitive to accumulated quantization errors, providing strong justification for the proposed method’s focus on accuracy stability during long generation.\n\nParoQuant achieves higher reasoning-task accuracy than AWQ and matches the state-of-the-art QTIP while being significantly faster.\n\nThe paper thoughtfully co-designs the quantization algorithm and CUDA implementation."}, "weaknesses": {"value": "Please see my questions."}, "questions": {"value": "How does ParoQuant perform under activation quantization or mixed-precision scenarios?\n\nCould the pairwise rotation be merged offline to further reduce runtime cost?\n\nCan this idea extend to FP4/FP8 formats?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9b0mKtBmgI", "forum": "1USeVjsKau", "replyto": "1USeVjsKau", "signatures": ["ICLR.cc/2026/Conference/Submission3516/Reviewer_7v1a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3516/Reviewer_7v1a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898605583, "cdate": 1761898605583, "tmdate": 1762916775917, "mdate": 1762916775917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}