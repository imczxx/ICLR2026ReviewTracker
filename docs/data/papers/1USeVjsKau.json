{"id": "1USeVjsKau", "number": 3516, "cdate": 1757456399726, "mdate": 1759898083508, "content": {"title": "ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference", "abstract": "Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often lead to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We also co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.", "tldr": "", "keywords": ["quantization", "large language models", "model compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b145e5f4092c6a123d113f3cb419de4bb0f0774.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents ParoQuant, a weight-only post-training quantization (PTQ) method designed for reasoning LLMs. The core idea is to apply scaled pairwise rotation, combining independent Givens rotations with channel-wise scaling, to suppress outliers efficiently. The authors co-design a CUDA kernel to maintain high throughput. Experiments show consistent accuracy gains over AWQ and EfficientQAT, with less than 10% latency overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The motivation that, quantization error accumulation in reasoning models, is clear and important.\n\nThe proposed rotation-based PTQ design is both novel and hardware-efficient.\n\nExtensive experiments on multiple model sizes (up to 70B) and reasoning benchmarks (MMLU-Pro, GSM8K, etc.) demonstrate solid improvement.\n\nPaper is clearly written and well-structured, with good algorithmic detail and ablations."}, "weaknesses": {"value": "While the rotation kernel is claimed to be efficient, the paper lacks quantitative breakdown of runtime and memory overhead (e.g., FLOPs, memory traffic).\n\nThe scalability of independent rotations to larger group sizes or mixed-precision settings (e.g., W4A8) is not discussed.\n\nMore analysis on the trade-off between number of rotations and latency would strengthen the efficiency claim.\n\nThe method seems tailored for linear quantization; extension to vector quantization or activation quantization could be briefly discussed."}, "questions": {"value": "Could the authors provide more detailed profiling on GPU resource usage and explain how ParoQuant scales when group size or model size further increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XWntLRWN8f", "forum": "1USeVjsKau", "replyto": "1USeVjsKau", "signatures": ["ICLR.cc/2026/Conference/Submission3516/Reviewer_kgGQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3516/Reviewer_kgGQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547465752, "cdate": 1761547465752, "tmdate": 1762916777436, "mdate": 1762916777436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ParoQuant, a weight-only post-training quantization method developed to improve the accuracy and efficiency of LLMs. It tackles the challenge of quantization error through a novel combination of independent Givens rotations and channel-wise scaling, which effectively reduces the impact of outliers. Additionally, the work incorporates a custom CUDA kernel to accelerate online scaled pairwise rotations, enabling faster inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear and Well-Founded Motivation: The paper observes that rotating only the top 10% of the most significant weight channel pairs can achieve nearly the same reduction in quantization error as performing a full rotation. This insight eliminates a large amount of redundant computation from full matrix multiplications, leading to a much more efficient quantization process.\n\n2. Methodology with GPU-Aware Design: Building on this motivation, the authors propose a three-step design for the scaled pairwise rotation transform.\n\nStep 1: Replace costly full orthogonal matrix multiplications with a set of decomposed Givens rotations.\n\nStep 2: Eliminate inter-rotation dependencies to allow fully parallel execution on GPUs, resulting in independent rotations.\n\nStep 3: Since a single independent rotation cannot adequately capture complex weight distributions, apply a series of independent rotations combined with channel-wise scaling to improve representation and quantization accuracy.\n\n3. Practical CUDA Implementation: The paper further introduces a co-designed efficient transform kernel that maximizes GPU parallelism by executing computations across three levels:\n\nToken-level: Parallelization over the token dimension of the activation tensor.\n\nChannel-group level: Different CUDA blocks handle different groups of channels.\n\nPair level: Each rotation pair is processed by a separate CUDA thread."}, "weaknesses": {"value": "Overall, I found the paper well-written and technically solid. The following are just minor curiosities rather than critical weaknesses:\n\n1. The 4-bit performance gains appear somewhat modest for certain model sizes and tasks (e.g., Perplexity and AIME). It would be interesting to see whether ParaQuant delivers more substantial improvements at lower bitwidths, such as 3-bit or 2-bit quantization.\n\n2. Do you have any insight into why E-QAT performs particularly poorly on AIME, given that ParaQuant can essentially be viewed as an extension of E-QAT with additional learnable rotations? I am expecting the performance gap between the two methods to be smaller."}, "questions": {"value": "See Above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tSLgNstDaX", "forum": "1USeVjsKau", "replyto": "1USeVjsKau", "signatures": ["ICLR.cc/2026/Conference/Submission3516/Reviewer_xZZn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3516/Reviewer_xZZn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808448333, "cdate": 1761808448333, "tmdate": 1762916776711, "mdate": 1762916776711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ParoQuant, a novel weight-only PTQ method for reasoning LLMs. It uses a \"scaled pairwise rotation\" transform, combining channel-wise scaling with hardware-efficient Givens rotations to suppress outliers. Through algorithm-system co-design, it achieves high accuracy with low inference overhead. It provides a very systematic solution, and the experiments are very comprehensive."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper clearly identifies and addresses a critical, forward-looking problem: the poor performance of efficient quantization methods on reasoning tasks that require long chains of thought. This focus on error accumulation in generative tasks is timely and important.\n\n2.\tThe proposed \"scaled pairwise rotation\" is a novel and elegant solution. The insight that a full rotation matrix is redundant and can be effectively approximated by a series of independent, parallelizable Givens rotations is the key contribution and is very well executed.\n\n3.\tThe algorithm-system co-design is a major strength. The authors didn't just propose a transform; they designed a custom CUDA kernel that makes the transform viable in practice, demonstrating a deep understanding of both the algorithmic and hardware constraints. The empirical results, showing ParoQuant matching QTIP's accuracy while being ~25% faster, are very compelling."}, "weaknesses": {"value": "1.\tThe greedy pair selection strategy outlined in Algorithm A1, while effective and intuitive, may not be globally optimal. It would be beneficial for the authors to discuss the potential limitations of this greedy approach.\n\n2.\tIn Section 3, when discussing quantization degradation on reasoning tasks, the authors should cite other recent works that have also identified this specific problem (e.g., QSPEC) to better contextualize their motivation.\n\n3.\tIn Figure 3, some text labels in the right-most portion of the diagram are overlapped, which slightly hinders readability."}, "questions": {"value": "1.\tThe number of independent rotations is fixed at K=8 for most experiments. How was this number chosen? Is there a clear point of diminishing returns, and does the optimal value of K change depending on the model architecture or size?\n\n2.\tThe \"pairwise\" rotation in Algorithm A1 is effective but seems conservative. Did the authors consider more fine-grained or alternative rotation structures, such as rotating small blocks of channels against each other? While this might be more expressive, it would likely introduce significant scheduling overhead. A discussion on this potential trade-off between rotation granularity and scheduling efficiency would be insightful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SvCQLKECxP", "forum": "1USeVjsKau", "replyto": "1USeVjsKau", "signatures": ["ICLR.cc/2026/Conference/Submission3516/Reviewer_hpqW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3516/Reviewer_hpqW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832808189, "cdate": 1761832808189, "tmdate": 1762916776175, "mdate": 1762916776175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ParoQuant, a weight-only post-training quantization (PTQ) method designed for reasoning LLMs, where quantization errors can accumulate over long generations.\nParoQuant combines: 1.Independent Givens rotations (pairwise rotations) to suppress outliers efficiently, and 2.Channel-wise scaling to even out magnitude across channels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors convincingly argue that reasoning LLMs are especially sensitive to accumulated quantization errors, providing strong justification for the proposed methodâ€™s focus on accuracy stability during long generation.\n\nParoQuant achieves higher reasoning-task accuracy than AWQ and matches the state-of-the-art QTIP while being significantly faster.\n\nThe paper thoughtfully co-designs the quantization algorithm and CUDA implementation."}, "weaknesses": {"value": "Please see my questions."}, "questions": {"value": "How does ParoQuant perform under activation quantization or mixed-precision scenarios?\n\nCould the pairwise rotation be merged offline to further reduce runtime cost?\n\nCan this idea extend to FP4/FP8 formats?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9b0mKtBmgI", "forum": "1USeVjsKau", "replyto": "1USeVjsKau", "signatures": ["ICLR.cc/2026/Conference/Submission3516/Reviewer_7v1a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3516/Reviewer_7v1a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898605583, "cdate": 1761898605583, "tmdate": 1762916775917, "mdate": 1762916775917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}