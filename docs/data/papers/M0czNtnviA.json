{"id": "M0czNtnviA", "number": 11541, "cdate": 1758201170121, "mdate": 1759897568851, "content": {"title": "PRIVET: PRIVacy metric based on Extreme value Theory", "abstract": "Deep generative models are often trained on sensitive data, such as genetic sequences, health data, or more broadly, any copyrighted, licensed or protected content. This raises critical concerns around privacy-preserving synthetic data, and more specifically around privacy leakage, an issue closely tied to overfitting. Existing methods almost exclusively rely on global criteria to estimate the risk of privacy failure associated to a model, offering only quantitative non interpretable insights. The absence of rigorous evaluation methods for data privacy at the sample-level may hinder the practical deployment of synthetic data in real-world applications. Using extreme value statistics on nearest-neighbor distances, we propose PRIVET, a generic sample-based, modality-agnostic algorithm that assigns an individual privacy leak score to each synthetic sample. We empirically demonstrate that PRIVET reliably detects instances of memorization and privacy leakage across diverse data modalities, including settings with very high dimensionality, limited sample sizes such as genetic data and even under underfitting regimes. We compare our method to existing approaches under controlled settings and show its advantage in providing both dataset level and sample level assessments through qualitative and quantitative outputs. Additionally, our analysis reveals limitations in existing computer vision embeddings to yield perceptually meaningful distances when identifying near-duplicate samples.", "tldr": "A sample-level privacy metric for synthetic data", "keywords": ["Generative modeling", "privacy", "extreme value statistics"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d76be5825f985ee7dd70bbee12b9255f63dcbd8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents PRIVET, a sample-level privacy metric for synthetic data that leverages Extreme Value Theory (EVT) applied to nearest-neighbor (NN) distances. The method fits a statistical model (Weibull or Gumbel) to characterize how close synthetic samples are to real training data, identifying those that are unusually similar and may indicate memorization or leakage. PRIVET provides both individual privacy scores and aggregate dataset indices, offering interpretable, quantitative assessments of privacy risk. The approach is modality-agnostic and is validated on genomic and image datasets, where it consistently detects privacy leakage more effectively than existing global-only metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Provides sample-level privacy scores rather than global aggregates.\n2. Grounded in Extreme Value Theory (EVT), offering statistical interpretability.\n3. Scalable and modality-agnostic across images and genomics.\n4. Demonstrates strong empirical validation and thoughtful limitations."}, "weaknesses": {"value": "1.  Embedding dependence: PRIVET’s results in vision tasks vary substantially across different embeddings (e.g., DINOv2 vs. wavelets). Since the metric depends on nearest-neighbor distances, poor or mismatched embeddings can distort similarity relationships and lead to inconsistent privacy estimates. A systematic embedding sensitivity analysis or adaptive embedding selection strategy would strengthen the method’s robustness.\n2. Threshold and tail calibration: The choice of the EVT fitting region and the leak-flagging threshold (τ) are largely heuristic. These parameters influence both the sample-level scores and the number of privacy leaks (NPL), which may not be consistent across datasets. Providing a calibration method (e.g., via null distributions or false-positive control) or ablations on these parameters would clarify interpretability.\n3. Handling of structured or heterogeneous data types: While the method is demonstrated on genomics (Hamming distances) and images (embedding distances), many real-world synthetic data scenarios involve mixed types (categorical, continuous, time series, hierarchical). It is unclear how PRIVET’s nearest-neighbor + EVT framework adapts to such heterogeneous or relational data. The paper could benefit from discussing or testing on more complex data types beyond the two modalities shown."}, "questions": {"value": "1. Parameter sensitivity: How sensitive are PRIVET’s results to the choice of EVT fitting range and the leak-detection threshold (τ)? Have you explored data-driven calibration methods or null-distribution bootstrapping to make  τ more interpretable?\n\n2. Embedding robustness: Since performance varies across embeddings (especially in vision), have you considered adaptive or learned representations that better preserve privacy-relevant distances? Could embedding selection be automated based on EVT goodness-of-fit or validation metrics?\n\n3.  Generality across data types: PRIVET is shown on genetic and image data, two well-structured modalities. How would the approach generalize to mixed or structured data (e.g., tabular datasets with categorical and continuous variables, or time-series data)? Are there distance metrics or representations that could preserve EVT assumptions in such cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZqNTRDpsKl", "forum": "M0czNtnviA", "replyto": "M0czNtnviA", "signatures": ["ICLR.cc/2026/Conference/Submission11541/Reviewer_6mFF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11541/Reviewer_6mFF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11541/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852539473, "cdate": 1761852539473, "tmdate": 1762922634172, "mdate": 1762922634172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRIVET, a privacy evaluation metric for generative models that relies on extreme value theory to model the tail distribution of nearest-neighbor distances.\nBy comparing the NN distance distributions between synthetic, training, and test data, PRIVET provides both dataset-level and sample-level privacy assessments, capable of identifying overfitting and privacy leakage.\nThe authors claim PRIVET to be interpretable, scalable, and domain-agnostic, and provide experiments on genetic data and image data with comparisons to prior metrics such as AATS, CT, FLD, PQMass, and AUTH."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper applies extreme value theory to model the nearest-neighbor distance tails, providing a statistically grounded method for privacy leakage detection.\n\n2. PRIVET can generate interpretable, per-sample privacy scores while remaining consistent with dataset-level statistics, which bridges the gap between global metrics (like AATS, FLD) and local boolean methods (like AUTH).\n\n3. Experiments on both genetic and image datasets demonstrate the versatility of the method, showing good scalability to high-dimensional data and small-sample regimes."}, "weaknesses": {"value": "1. Table 1’s distinction between “interpretable” and “non-interpretable” metrics is not clearly defined. It appears that *interpretability* is equated with having a boolean “leak / not leak” label, but the table also lists a “real value” column without clear justification. Moreover, the rationale for classifying methods such as AUTH as interpretable only at the sample level, but not at the dataset level, is insufficiently explained. \n2. The paper claims that prior metrics like FLD and PQMass fail under low-data conditions or produce noisy estimates, yet this claim is not supported by empirical evidence.\n3. While the experiments cover genetic (binary) and image (continuous) data, the claim of being “domain-agnostic” is incomplete as it lacks a textual or sequential modality.\n4. The evaluation focuses mainly on precision and recall."}, "questions": {"value": "1. Can you provide quantitative or visual evidence supporting the claim that previous metrics (e.g., FLD, PQMass) fail or become noisy in low-data regimes?\n\n2. Since experiments only include genetic and image data, could you discuss or show results on text data to better support the “domain-cross” generalization claim?\n\n3. Would it be possible to add metrics such as ROC-AUC or F1-score to strengthen the evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UKuifxyBp0", "forum": "M0czNtnviA", "replyto": "M0czNtnviA", "signatures": ["ICLR.cc/2026/Conference/Submission11541/Reviewer_L37y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11541/Reviewer_L37y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11541/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991667840, "cdate": 1761991667840, "tmdate": 1762922633651, "mdate": 1762922633651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PRIVET, a privacy audit for generative models that assigns a sample-level privacy-leak score using extreme value theory (EVT) fitted to the lower tail of nearest-neighbor (NN) distance distributions. The method models the empirical CDF of train–train 1-NN distances with a Weibull or Gumbel distribution, then evaluates where each synthetic sample's NN distance to train or test falls within the fitted tail via order-statistics. It reports both local scores and global indices, including the average log-ratio between train-referenced and test-referenced probabilities and a count of suspected leaks under a threshold. Experiments cover controlled genetic-sequence settings with configurable leak rate and copied-bits fraction, a CIFAR-10 \"copycat\" setup using DINOv2 and wavelet embeddings, and a membership-inference style analysis on RBM-generated genomic data. The authors argue PRIVET is interpretable, scalable, domain-agnostic, and more sensitive than baselines such as AATS, CT, FLD, PQMass, and the Authenticity score in several regimes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) Broad empirical coverage. The study spans high-dimensional genetics, computer vision with modern embeddings, and a membership-attack scenario, which together demonstrate versatility across modalities and training regimes, including underfitting.\n2) Interpretability and actionable outputs. The ability to flag specific synthetic samples as probable leaks and to summarize with an estimated number of leaks is useful in governance workflows. The method surfaces when embedding choices compromise detectability.\n3) Comparison to multiple baselines. The paper contrasts PRIVET with several recent metrics and highlights conditions where others become noisy, require matched sizes, or provide only global signals."}, "weaknesses": {"value": "1) Embedding dependence undermines \"modality-agnostic\" positioning. Results show that detectability varies substantially with the representation, with failures for certain image transforms under DINOv2 and improvements with wavelets. The method is only as good as the embedding and distance, which weakens the claim of generality. A guidance section on selecting or learning privacy-aware embeddings, or an adaptive metric learning step, would be valuable.\n2) Figure~1 readability. The numerical annotations in Figure~1 overlap, particularly within dense regions of the scatter, which reduces interpretability and obscures the intended comparison between train-referenced and test-referenced EVT fits. Clear labeling is crucial for a method whose main selling point is interpretability.\n3) Image experiments underperform relative to In-Authenticity in certain regimes} In the CIFAR-10 evaluations (Figure~3), PRIVET does not consistently outperform the In-Authenticity baseline. In some transformations such as JPEG compression and elastic distortion, PRIVET's separation between memorized and novel samples appears weaker. These results directly challenge the general claim that the proposed approach is \"more sensitive\" across modalities.\n4) Lack of empirical scalability evaluation. The paper asserts that PRIVET is scalable because it relies on nearest-neighbor distances and closed-form EVT fits. However, there is no time or memory comparison against competing privacy auditing methods such as AATS, CT, FLD, or PQMass. The experiments do not report runtime on increasing dataset sizes, nor the computational effect of high-dimensional embeddings or approximate nearest-neighbor search. Since practitioners must often audit millions of samples in regulatory contexts, an explicit complexity analysis and empirical runtime study are necessary to substantiate the scalability claim."}, "questions": {"value": "1) Can you revise Figure~1 to avoid overlapping numerical labels and provide clearer visual separation of extreme-tail points? Since interpretability is a core claim, visual clarity is essential for evaluating sample-level privacy scores.\n2) In Figure~3, PRIVET performs worse than In-Authenticity for certain image transformations such as JPEG compression and elastic distortions. What underlying factors drive these failures?\n3) The method's success appears strongly influenced by representation choice (e.g., DINOv2 vs wavelets). Do you have recommendations or automatic adaptation mechanisms to ensure reliable performance across modalities without manual embedding engineering?\n4) You assert PRIVET is scalable, yet runtime and memory comparisons are not reported. How does performance scale relative to baselines as dataset size and embedding dimensionality increase, especially under approximate nearest-neighbor search? Please include empirical runtime curves and complexity analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sSwTnkLF6o", "forum": "M0czNtnviA", "replyto": "M0czNtnviA", "signatures": ["ICLR.cc/2026/Conference/Submission11541/Reviewer_xbm4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11541/Reviewer_xbm4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11541/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042894335, "cdate": 1762042894335, "tmdate": 1762922633282, "mdate": 1762922633282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new privacy metric based on the extension of the nearest neighborhood based statistics that have been used in the past. Simply, the proposed privacy score is defined based on r-th smallest nearest-neighbor (NN) distance to the reference dataset, computed from the M samples of the dataset of interest. In other words, for each synthetic sample, the proposed privacy metric takes the log ratio of overfitting scores between the training and test sets. According to authors, a low value indicates a statistical anomaly and may signal a potential privacy leak."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Another metric that addresses an important privacy problem.\n\nAuthors provide some interesting examples based on pseudo-synthetic data that suits their distance metrics and embeddings."}, "weaknesses": {"value": "Only a single experiment using an RBM-generated synthetic dataset is presented, which limits the generality of the findings. Including additional datasets would strengthen the empirical validation.\n\nMoreover, the current results do not clearly demonstrate whether the proposed approach detects any actual privacy risk. One potential improvement would be to employ a data generator with a controlled privacy parameter—for example, a differentially private generator with varying  epsilon values—and evaluate whether the proposed privacy score correlates with these privacy levels.\n\nFinally, it would be valuable to extend the analysis beyond membership inference attacks. Incorporating sensitive attribute inference or other privacy attack scenarios could help assess whether the proposed score also correlates with stronger leakage in more complex attack models."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LzFvyXUoyn", "forum": "M0czNtnviA", "replyto": "M0czNtnviA", "signatures": ["ICLR.cc/2026/Conference/Submission11541/Reviewer_9Y2e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11541/Reviewer_9Y2e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11541/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762827228781, "cdate": 1762827228781, "tmdate": 1762922632993, "mdate": 1762922632993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}