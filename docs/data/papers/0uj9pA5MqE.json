{"id": "0uj9pA5MqE", "number": 8381, "cdate": 1758080793458, "mdate": 1759897788628, "content": {"title": "Agentic Analogical Reasoning for Large Language Models", "abstract": "Analogical reasoning helps humans grasp new concepts by relating them to familiar\nones. Recent work seeks to improve LLM reasoning by prompting analogical cor-\nrespondences with semantically related scenarios. However, existing approaches\nare single-turn reasoning and may generate unreliable analogical instances, which\nrestricts their effectiveness in complex reasoning tasks. To address these limita-\ntions, we propose a novel Agentic Analogical Reasoning (AAR) paradigm for\nLLM reasoning. This paradigm treats the LLM as an agentic reasoner to integrate\nmulti-turn insights along the reasoning trajectory of iteratively generating analogi-\ncal queries to trigger internal or external knowledge for analogical exemplification,\nand selectively identifying appropriate analogies to conduct further reasoning. To\nequip LLMs with AAR capability, we design an analogical trajectory optimization\nalgorithm including analogical trajectory generation and re-weighted trajectory\ntraining. Furthermore, a mixed training strategy is devised to progressively inter-\nnalize agentic analogical reasoning as an intrinsic capability of LLMs. Finally, we\nconduct extensive experiments on seven reasoning-intensive datasets and achieve\nsignificant performance improvements over prior state-of-the-art (SOTA) methods.\nThe code is available at https://anonymous.4open.science/r/ICLR-8381.", "tldr": "We propose a novel Agentic Analogical Reasoning (AAR) paradigm for LLM reasoning. This paradigm treats the LLM as an agentic reasoner to integrate multi-turn insights along the reasoning trajectory.", "keywords": ["Analogical reasoning", "Large Language Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4516993ff9ab92fea6e82c705721fe023df6177f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel Agentic Analogical Reasoning (AAR) paradigm to enhance LLMs’ analogical reasoning capability. The authors define key processes to complete analogical reasoning. The model generates a sequence of intermediate reasoning steps before producing the final answer.The idea of formulating this process as an ELBO optimization problem is elegant and theoretically sound."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Equations (1)–(6) are clearly presented and easy to follow.\n2. The formalization of analogical reasoning as ELBO optimization is both creative and technically coherent.\n3. The paper includes extensive experiments on reasoning-intensive datasets, showing consistent gains over prior SOTA methods."}, "weaknesses": {"value": "1. **Main Issue**. It is unclear how Equation (7) is concretely implemented in the model. Specifically, since it involves $\\nabla_\\theta (logp(z∣x;θ)+logp(y∣x,z;θ))$, the paper should describe in detail (or provide pseudocode) how this gradient-based optimization is realized in practice.\n2. The paper lacks clear information about training data. It is important to clarify what training corpus was used for model learning.\n3. According to the supplementary material,$ a=A(q)$ where $A$ denotes a retriever. Hence $a$ is not generated by the model parameters $\\theta $. However, in Equation (7),  $a$  is required to be generated by $\\theta $. How is this inconsistency handled during training?\n4. Figures 1 and 2 appear to be unreferenced in main text."}, "questions": {"value": "See weakness. I will **re-evaluate my recommendation after the authors provide clarifications and additional important details**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bs7Ywt6gGB", "forum": "0uj9pA5MqE", "replyto": "0uj9pA5MqE", "signatures": ["ICLR.cc/2026/Conference/Submission8381/Reviewer_m7U5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8381/Reviewer_m7U5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742977169, "cdate": 1761742977169, "tmdate": 1762920288000, "mdate": 1762920288000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Agentic Analogical Reasoning (AAR): a multi-turn “think → analogize → contextualize” loop that treats an LLM as an agent which iteratively forms analogical trajectories using both internal memories and external retrieval. Training maximizes an objective with importance-weighted trajectories (weight ∝ p(y|x,z)), and a mixed/internalization stage converts retrieval-based analogies into self-generated ones so the model increasingly relies on internal analogies. Experiments on seven benchmarks show gains over several baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Broad, fair baselines. Evaluated on 7 datasets with both direct reasoning and RAG-style methods.\n\n2. Component ablations. Shows how removing each module (think / analogize / contextualize, etc.) affects performance.\n\n3. Practical fallback. Uses external analogies when the model can’t retrieve a good one from its own knowledge."}, "weaknesses": {"value": "1. Robustness to bad analogies is unclear. The paper doesn’t show how AAR behaves when analogies are noisy, misleading, or irrelevant.\n2. Single retriever choice. AAR is only evaluated with ColBERT; no evidence it holds with other RAG pipelines (BM25, DPR, Contriever, reranker variants).\n3. Fixed turn budget. The method uses a fixed number of reasoning turns, which may mismatch multi-hop questions that need dynamic depth."}, "questions": {"value": "1. Handling bad analogies: How does AAR detect and reject low-quality or misleading analogies? Any filters, penalties, or stress tests with noisy/adversarial analogies?\n2. RAG variants: Did you test AAR with alternative retrievers/rerankers or retrieval settings (e.g., BM25/DPR/Contriever)? How do results change?\n3. Dynamic turns: Have you explored a confidence-based or utility-based early stopping rule, or training the model to adapt the number of turns per query (e.g., curriculum or cost-aware objective)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eTpODW3Ldg", "forum": "0uj9pA5MqE", "replyto": "0uj9pA5MqE", "signatures": ["ICLR.cc/2026/Conference/Submission8381/Reviewer_AVzv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8381/Reviewer_AVzv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775751563, "cdate": 1761775751563, "tmdate": 1762920287588, "mdate": 1762920287588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Agentic Analogical Reasoning (AAR), a framework that aims to enhance multi-turn reasoning in large language models through iterative analogy generation and contextual refinement. The method incorporates a re-weighted trajectory training strategy intended to help the model learn which analogical paths are effective. While results show consistent improvements on reasoning benchmarks, the evidence does not clearly demonstrate genuine analogical reasoning, as the provided case study reflects domain-specific rule reuse rather than true structural analogy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a well-structured framework that integrates analogy-inspired reflection into multi-turn reasoning, offering a coherent conceptual design beyond simple CoT prompting.\n\nThe introduction of re-weighted trajectory training adds a learnable component that aims to internalize reasoning patterns rather than rely solely on prompt-level heuristics.\n\nThe experiments show consistent, if modest, gains across several reasoning benchmarks, indicating that the framework can be practically applied to enhance LLM reasoning performance."}, "weaknesses": {"value": "The paper runs multi-turn reasoning with repeated analogical steps and retrieval but provides no analysis of inference latency, memory usage, or computational budget. Figure 3 shows diminishing returns beyond five steps, yet the trade-off between accuracy gains and computational cost is never discussed, leaving practical feasibility unclear.\n\nThe process for external analogy retrieval and the construction of the reasoning-case knowledge base are insufficiently described. Key details such as data sources, validation, and usage criteria are missing, making it impossible to assess or reproduce model performance.\n\nIt remains unclear when analogical reasoning is actually beneficial. Reported improvements on mathematical reasoning tasks may stem from longer reasoning chains or template retrieval rather than genuine analogical transfer, raising doubts about the approach’s scope and validity."}, "questions": {"value": "Can you provide concrete evidence that the observed gains come from genuine analogical reasoning rather than retrieval of similar examples or longer reasoning chains?\n\nThe paper introduces an external analogy retrieval mechanism that searches a “knowledge base containing factual and reasoning knowledge,” but details of this knowledge base remain unclear. What are its data sources, scale, and structure, and how is relevance ensured?\n\nCan you provide concrete evidence that the observed gains come from genuine analogical reasoning rather than retrieval of similar examples or longer reasoning chains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MwWp6OlEj0", "forum": "0uj9pA5MqE", "replyto": "0uj9pA5MqE", "signatures": ["ICLR.cc/2026/Conference/Submission8381/Reviewer_XAWq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8381/Reviewer_XAWq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801542254, "cdate": 1761801542254, "tmdate": 1762920287198, "mdate": 1762920287198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a post-training method for LLMs to induce them to be \"agentic analogical reasoners\".  In particular, the LLMs are trained to iterate \"analogical units\", each of which consists of \"thinking\" (generating an analogy query), \"analogizing\" (from internal knowledge or from external sources), and \"contextualizing\" (applying the analogy to the current task).  The authors test this method by post-training two open-weight base models, and evaluating them on several reasoning benchmarks.  The results are compared with those of direct reasoning models, models with RAG, and analogical-prompting models.  The authors find that their method induces improvements in accuracy on most of the benchmarks they tested on, compared with the other models.  The authors also do an ablation study, showing the contribution of each component to the overall results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Analogical reasoning is an important facet of human reasoning, and the authors have devised an interesting way to try to induce a pre-trained model to use analogy in the process of solving problems.  They have shown that their method has significant imrpovement over other methods they tested."}, "weaknesses": {"value": "The authors claim that their method is inspired by Gentner et al.'s structure-mapping theory, and state that their \"contextualizing\" step \"mirrors the core mapping process in structure-mapping theory.\" However, it's not clear that their model is doing anything like structure mapping.  The case studies in the appendix, which show the system \"analogizing\", are not understandable to me, since they are snippets of technical statements -- e.g., \"With serial testing, overall LR+ approximately multiplies: A’s LR+ × B’s LR+, which can substantially raise PPV\" -- what does this mean and how is it an \"analogy\"?\n\nIt would be helpful if the authors explained how these case studies actually illustrate analogies (in a way understandable to readers) and how the system is using a structure-mapping like mechanism to apply these analogies.  Otherwise the authors' claims -- that the accuracy improvements are due to actual analogies and structure mapping -- are not substantiated."}, "questions": {"value": "The paper states: \"External Analogy: When the model emits a special token (e.g., '<analogizing>'), the function A invokes an external retrieval module to search a knowledge base containing factual and reasoning knowledge, returning relevant documents as the analogy a_i.\"  -- Where does this knowledge base come from?  I did not see other discussion of it in the paper -- did I miss something?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yil3Q2uOf6", "forum": "0uj9pA5MqE", "replyto": "0uj9pA5MqE", "signatures": ["ICLR.cc/2026/Conference/Submission8381/Reviewer_zNAp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8381/Reviewer_zNAp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762025415030, "cdate": 1762025415030, "tmdate": 1762920286745, "mdate": 1762920286745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}