{"id": "NXduufyPtY", "number": 17835, "cdate": 1758281047523, "mdate": 1759897151053, "content": {"title": "CNSP: Consistent Null-Space Projection for Principled Prompt-Based Continual Learning", "abstract": "Prompt-based continual learning has recently shown strong empirical progress, yet its theoretical underpinnings remain incomplete. Prior work such as NSP$^2$ provides sufficient conditions for performance preservation for visual prompt tuning via null-space projection and achieves strong empirical results, but its reliance on simplifying assumptions in MHSA and LayerNorm undermines robustness and interpretability. In this paper, we revisit the problem from a matrix-level perspective and propose Consistent Null-Space Projection (CNSP). Our framework introduces: (i) rigorous per-head derivations under MHSA; (ii) a matrix-form characterization of LayerNorm; (iii) a relaxed prompt variance constraint that is more stable in practice; and (iv) refined sufficient conditions enforced via null-space projection that extend naturally to classification heads, ensuring end-to-end task performance preservation. Extensive experiments on multiple benchmarks demonstrate that CNSP consistently improves over NSP$^2$. Our results highlight the importance of principled matrix-level formulations for building robust and interpretable prompt-based continual learning methods.", "tldr": "", "keywords": ["Continual Learning", "Visual Prompt Tuning", "Gradient Projection", "Anti-forgetting"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b1de0a688618f012755b2cee870da18aca291f2a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces CNSP (Consistent Null-Space Projection), a principled framework for prompt-based continual learning with Vision Transformers. Building on NSP^2, the authors strengthen its theoretical foundations by providing per-head sufficient conditions for multi-head attention, a matrix-form treatment of LayerNorm, and a relaxed variance-only constraint for prompt updates. They also ensure end-to-end consistency by incorporating classification head preservation. Experiments show consistent gains in accuracy and reduced forgetting compared to prior methods, including NSP^2."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides an extensive analysis of existing null-space projection method in continual learning (especially NSP^2), and propose corresponding improvements.\n\n2. The proposed method is derived from theoretical basis of feature preservation and head preservation, and seems reasonable."}, "weaknesses": {"value": "1. From my understanding, this work is built on existing null-space projection method NSP^2, which limits its technical novelty.\n\n2. The proposed designs, although very extensive, achieve only marginal improvements over NSP^2.\n\n3. In Table 2 (ablation study), the authors seem to remove major components of null-space projection method. However, as the extension of NSP^2, each individual design should be compared with the counterpart in NSP^2.\n\n4. The authors only consider ViT-B/16 pretrained on ImageNet-21K as the backbone. Does the proposed method also apply to other backbones especially self-supervised checkpoints?"}, "questions": {"value": "My major concerns lie in the effectiveness of individual designs over NSP^2. Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ssn5kvXeue", "forum": "NXduufyPtY", "replyto": "NXduufyPtY", "signatures": ["ICLR.cc/2026/Conference/Submission17835/Reviewer_yBKo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17835/Reviewer_yBKo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760884267931, "cdate": 1760884267931, "tmdate": 1762927672423, "mdate": 1762927672423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of prompt-based continual learning. It analyzes the limitations of a regularization-based method, NSP2, and identifies several key weaknesses, including the lack of rigorous justification for its extension to multi-head attention, the oversimplified treatment of LayerNorm, the unstable invariance assumption on prompts, and the neglect of classification head analysis. To overcome these issues, the paper proposes Consistent Null-Space Projection, which introduces rigorous multi-head analysis, matrix-form LayerNorm modeling, a relaxed distributional constraint, and classification-head preservation to ensure theoretical consistency. Experimental results show that the proposed method consistently outperforms NSP2 in terms of both average accuracy and forgetting across multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces the concepts of feature preservation and head preservation through theoretical analysis, identifying the key factors that maintain the stability of prompt-based continual learning models.\n\n2. In the methodology section, a more rigorous derivation of multi-head attention is presented, leading to a set of sufficient conditions for feature preservation in VPT.\n\n3. The analysis reformulates LayerNorm’s broadcast operations in matrix form, thereby ensuring greater algebraic rigor and theoretical soundness.\n\n4. CNSP achieves performance competitive with state-of-the-art prompt-based continual learning methods, demonstrating both theoretical and empirical advantages."}, "weaknesses": {"value": "1. Since each attention head operates independently, the per-head analysis conducted in this paper essentially follows the same procedure as in NSP2, except that NSP2 did not explicitly denote each head with the superscript (h). As a result, the derivation ultimately reduces to single-head-level results rather than a unified multi-head formulation. This head-level analysis does not provide substantial new insights.\n\n2. The adoption of a right-side nullification form on the second constraint appears questionable. The purpose of the second condition in Eq. (27) is to ensure that the attention output (i.e., the product of the attention matrix and the value matrix V) remains invariant. However, by removing the attention matrix S from the formulation, this invariance is no longer guaranteed, weakening the theoretical justification for the constraint.\n\n3. The claim of “classification head preservation by design” lacks meaningful novelty. This setup does not require any special design, as most continual learning methods already adopt the same practice (i.e., training task-specific heads and concatenating them at inference).\n\n4. The experimental results are limited, consisting mainly of a single comparison with the current state-of-the-art and one ablation study (in the appendix). Moreover, as shown in Table 1, the improvements over NSP2 are marginal and not particularly significant."}, "questions": {"value": "1. In Eq. (23), both gamma and beta are defined as element-wise parameters. However, in Eq. (24), after reformulating them into matrix form, they are expressed as if they participate in matrix multiplication. This seems inconsistent with their element-wise nature. The paper should provide a clear explanation for this formulation choice, as it could directly affect the validity of the subsequent derivations and conclusions.\n\n2. Eq. (25) introduces the variance-invariance assumption, but in transitioning from Eq. (23) to (24), the mean term $\\mu_{P_t+\\Delta P}$ is omitted. It is unclear whether this omission is mathematically equivalent to the original formulation (Eq. 23). If not, then the assumption in Eq. (25), which constrains only the variance while ignoring the mean, may not be strictly valid, potentially weakening the theoretical rigor of the analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KtG8wvgCLs", "forum": "NXduufyPtY", "replyto": "NXduufyPtY", "signatures": ["ICLR.cc/2026/Conference/Submission17835/Reviewer_pGHs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17835/Reviewer_pGHs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733601898, "cdate": 1761733601898, "tmdate": 1762927671947, "mdate": 1762927671947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Consistent Null-Space Projection (CNSP), a novel prompt-based continual learning method. Building upon NSP2, the method systematically analyzes multi-head attention and LayerNorm, thereby enhancing the theoretical foundation for knowledge retention through null-space projection. Extensive experimental evaluations demonstrate that CNSP achieves state-of-the-art performance across multiple benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe motivation behind this paper is both intuitive and clearly articulated. In contrast to previous works, CNSP provides a thorough investigation of multi-head attention, LayerNorm, and the classification head within the context of continual learning. Specifically, the paper introduces a more principled and effective framework for visual prompt tuning with vision transformers.\n2.\tThis paper includes theoretical derivations and proofs.\n3.\tThe paper is written in a clear and well-organized manner."}, "weaknesses": {"value": "1.\tCompared to NSP2, the paper provides a more systematic theoretical analysis of multi-head attention, LayerNorm, and the classification head. However, the experimental results show only modest improvements.\n2.\tThe contribution appears to be somewhat limited and does not fully meet the conference's expectations. The method seems to be an instantiation of orthogonal projection techniques, applied to continual learning in image classification, and is based on visual prompt tuning with vision transformers.\n3.\tMaintaining a separate classification head for each task leads to an increase in the number of parameters as the number of tasks grows. Furthermore, calibrating the logits obtained from independently trained classification heads across different tasks presents a new challenge.\n4.\tThere is a notation error in lines 248 and 654. The correct index should be A(2) instead of A(1)."}, "questions": {"value": "1.\tPlease clarify the core contribution of the work, rather than describing the differences in unexplored components from previous works, such as multi-head attention, LayerNorm, and the classification head.\n2.\tFrom the ablation study in Table 2, the variance preservation loss has minimal impact on the results. Does this suggest that the assumptions in Equation 25 do not significantly affect the derivation of the final constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xUJRwOjZib", "forum": "NXduufyPtY", "replyto": "NXduufyPtY", "signatures": ["ICLR.cc/2026/Conference/Submission17835/Reviewer_3yso"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17835/Reviewer_3yso"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812047453, "cdate": 1761812047453, "tmdate": 1762927671567, "mdate": 1762927671567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CNSP (Consistent Null-Space Projection), a theoretically rigorous framework for prompt-based continual learning in Vision Transformers (ViTs). The work addresses limitations in prior methods (e.g., NSP²) by proposing: (1) Per-head sufficient conditions for multi-head self-attention (MHSA) to ensure feature preservation; (2) Matrix-form LayerNorm modeling to replace scalar approximations; (3) Variance-only constraints for prompt updates (relaxing NSP²’s mean-variance invariance); (4) End-to-end task preservation via classification head constraints. Experiments on CIFAR-100, ImageNet-R, and DomainNet show CNSP consistently outperforms NSP²."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's theoretical foundation is rigorous and innovative in practice."}, "weaknesses": {"value": "1.SVD on $RR^\\top$ may incur latency for large $D$ (e.g., ViT-L/16). Training time/GPU memory vs. NSP² should be reported.\n2.Softmax avoidance (Eq. 20) lacks theoretical justification. The impact of attention-score invariance (Eq. 20) on representational capacity needs analysis.\n3.Classification heads grow linearly with tasks (Appendix E.1). For more tasks, parameter explosion may occur—compression techniques (e.g., prompting heads) should be discussed.\n4.The pseudocode is too long, so it is recommended to put it in the appendix and put more valuable experiments in the main text.\n5.CNSP treats all tasks equally, but catastrophic forgetting varies with task correlation (e.g., CIFAR-100 classes vs. ImageNet-R domains). Experiments use frozen ImageNet-21K ViT-B/16 for all tasks. This bypasses challenges like: Cross-domain pretraining gaps (e.g., medical vs. natural images). Model scaling effects (performance on ViT-L vs. ViT-B).\n6.The experimental results are not convincing, and the gains against NSP^2 is trivial."}, "questions": {"value": "See Section Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JNFZ1LiGMX", "forum": "NXduufyPtY", "replyto": "NXduufyPtY", "signatures": ["ICLR.cc/2026/Conference/Submission17835/Reviewer_XrcC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17835/Reviewer_XrcC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842916630, "cdate": 1761842916630, "tmdate": 1762927671221, "mdate": 1762927671221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Consistent Null-Space Projection (CNSP), a framework for prompt-based continual learning in Vision Transformers. Prior work NSP² derived sufficient conditions for preserving representations under prompt updates, but relied on simplifying assumptions regarding LayerNorm and multi-head self-attention (MHSA), and enforced a strong mean–variance constraint on prompts. This paper revisits the problem using a matrix-level formulation, deriving per-head feature preservation conditions and providing a matrix-form characterization of LayerNorm. The authors further show that variance preservation alone is sufficient, leading to a more stable optimization strategy. They implement prompt updates via right-side null-space projection, ensuring constraints hold during learning. Experiments on CIFAR-100, ImageNet-R, and DomainNet demonstrate consistent improvements over NSP² and competitive performance among prompt-based continual learning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses a clear theoretical limitation in NSP², especially in the treatment of LayerNorm and MHSA.\n- Derivations are technically sound and detailed.\n- Relaxing to variance-only preservation improves stability in practice.\n- Null-space projection is computationally efficient and easy to implement.\n- Consistent empirical gains across benchmarks.\n- Ablation studies effectively demonstrate the necessity of key components."}, "weaknesses": {"value": "- Certain theoretical assumptions are not empirically analyzed (e.g., fixed γ and β in LayerNorm).\n- Performance improvements, though consistent, are modest."}, "questions": {"value": "1. Does null-space projection restrict the expressive capacity of prompts as the number of tasks increases?\n2. How sensitive is the variance-preservation alignment loss to domain shift between tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oOmCP1ys7L", "forum": "NXduufyPtY", "replyto": "NXduufyPtY", "signatures": ["ICLR.cc/2026/Conference/Submission17835/Reviewer_rPJz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17835/Reviewer_rPJz"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971980172, "cdate": 1761971980172, "tmdate": 1763006295927, "mdate": 1763006295927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Consistent Null-Space Projection (CNSP), a framework for prompt-based continual learning in Vision Transformers. Prior work NSP² derived sufficient conditions for preserving representations under prompt updates, but relied on simplifying assumptions regarding LayerNorm and multi-head self-attention (MHSA), and enforced a strong mean–variance constraint on prompts. This paper revisits the problem using a matrix-level formulation, deriving per-head feature preservation conditions and providing a matrix-form characterization of LayerNorm. The authors further show that variance preservation alone is sufficient, leading to a more stable optimization strategy. They implement prompt updates via right-side null-space projection, ensuring constraints hold during learning. Experiments on CIFAR-100, ImageNet-R, and DomainNet demonstrate consistent improvements over NSP² and competitive performance among prompt-based continual learning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses a clear theoretical limitation in NSP², especially in the treatment of LayerNorm and MHSA.\n- Derivations are technically sound and detailed.\n- Relaxing to variance-only preservation improves stability in practice.\n- Null-space projection is computationally efficient and easy to implement.\n- Consistent empirical gains across benchmarks.\n- Ablation studies effectively demonstrate the necessity of key components."}, "weaknesses": {"value": "- Certain theoretical assumptions are not empirically analyzed (e.g., fixed γ and β in LayerNorm).\n- Performance improvements, though consistent, are modest."}, "questions": {"value": "1. Does null-space projection restrict the expressive capacity of prompts as the number of tasks increases?\n2. How sensitive is the variance-preservation alignment loss to domain shift between tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oOmCP1ys7L", "forum": "NXduufyPtY", "replyto": "NXduufyPtY", "signatures": ["ICLR.cc/2026/Conference/Submission17835/Reviewer_rPJz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17835/Reviewer_rPJz"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971980172, "cdate": 1761971980172, "tmdate": 1763598430651, "mdate": 1763598430651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}