{"id": "LhSieiKSc8", "number": 14185, "cdate": 1758229998380, "mdate": 1759897385501, "content": {"title": "Sentiment-weighted advantage updates for portfolio optimization with reinforcement learning", "abstract": "Conventional reinforcement learning (RL) methods for portfolio optimization, such as proximal policy optimization (PPO), rely mainly on historical price data and overlook unstructured market signals like investor sentiment. This paper introduces Sentiment-Augmented PPO (SAPPO), a reinforcement learning framework that integrates daily asset-level sentiment into both the state representation and the policy update. The core innovation is a sentiment-weighted advantage function, where sentiment scores act as dynamic multipliers on advantage estimates, thereby shaping policy gradients in a behaviorally informed manner. This design differs from prior sentiment-aware approaches that inject sentiment only into state vectors or reward shaping, enabling more stable and context-sensitive learning under market nonstationarity. Empirical evaluation on Refinitiv news and NASDAQ-100 stocks shows that SAPPO outperforms vanilla PPO and sentiment-in-state/reward baselines, raising Sharpe ratio from 1.67 to 2.07 and annualized returns from 57\\% to 83\\% with only modest drawdown increase. Extensive ablations confirm that the gains stem from the sentiment-weighted update mechanism rather than from any specific sentiment model. These results highlight the potential of integrating behavioral signals into reinforcement learning for financial decision-making.", "tldr": "", "keywords": ["reinforcement learning", "sentiment", "large language models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68e5fb24ea9b3151e7855d2677cd7c5a027cfa3d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The author introduces a new method of reinforcement learning based algorithm to train an agent for making decisions on portfolio investment. Different with prior work, in this work, sentiment analysis results from news into both the state representation and the objective reward in the PPO algorithm. Significant gain from using the proposed methodology in the expected return is observed against baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Incorporation of sentiment analysis results in the advantage estimation is an interesting exploration. \n- The experiments are comprehensive, and the choice of each component as well as the alternative combinations of modules are well justified through ablation studies. The performance gain achieved by the proposed SAPPO is consistent across all reported experiments."}, "weaknesses": {"value": "- The methodology overall is a bit incremental, where incorporating sentiment analysis to RL based investment planning was widely explored in prior literature, also as discussed by the author. It would be better to also present the performance of those prior approaches mentioned in paper into the baselines, which will give readers better sense of how this work is placed in the current literature. \n- The insights from the study is a bit insufficient, where the expected return/performance is the main focus, but less insights/analysis/observation are conducted on more in-depth aspects, such as to what extent the sentiment information influences the optimal policy learned by the agent? What is the distribution of the sentiment levels? Is the agent's learned policy influence more by the positive or negative news? If the emphasis is on the inclusion of sentiment, is the performance gain consistent across varied RL algorithms (e.g. Q learning, Gradient Policy, DPO, etc.)? \n- The sentiment analysis results are incorporated into the advantage estimation, what if the raw LLM embedding is leveraged? Would that way provide the agent more informative signals? \n- The presentation quality could be improved. Most figures are not vectorized and the font resolution appears low."}, "questions": {"value": "My questions are stated in the Weaknesses above. Having these additional aspects being analyzed or discussed will improve the depth of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lmCHRSYPxK", "forum": "LhSieiKSc8", "replyto": "LhSieiKSc8", "signatures": ["ICLR.cc/2026/Conference/Submission14185/Reviewer_iLLX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14185/Reviewer_iLLX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760737731163, "cdate": 1760737731163, "tmdate": 1762924642030, "mdate": 1762924642030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces sentiment-augmented PPO, a RL framework for portfolio optimization that integrates financial news sentiment into both the state representation and the policy update mechanism. Experiments on multiple US stock datasets demonstrate its SOTA performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core sentiment-weighted advantage function seems a simple yet efficient trick for portfolio optimization.\n2. The proposed method shows robust performance with good generalization ability."}, "weaknesses": {"value": "1. This paper is more like a technical report rather than a research paper for top conferences. The proposed method is a bit trivial with limited technical contribution for me.\n2. Since sentiment is the key point for this work, more experiments with sentiment derived from other text data (e.g., social media, analyst reports) can make this work stronger.\n3. The backtest environment is not realistic without the consideration of slippage and so on.\n4. How practical is the proposed methods for intra-day setting or live trading? Current evaluation only focuses on day-level settings."}, "questions": {"value": "Please see weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bYMhaAOknp", "forum": "LhSieiKSc8", "replyto": "LhSieiKSc8", "signatures": ["ICLR.cc/2026/Conference/Submission14185/Reviewer_5DoB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14185/Reviewer_5DoB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557008600, "cdate": 1761557008600, "tmdate": 1762924641537, "mdate": 1762924641537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Sentiment-Augmented PPO, a reinforcement learning framework that integrates investor sentiment directly into the policy update process of PPO. Unlike previous approaches that inject sentiment as an auxiliary feature or use it for reward shaping, SAPPO modifies the advantage function itself through a sentiment-weighted mechanism, where asset-level sentiment scores modulate policy gradients during learning. This integration aims to create behaviorally informed trading agents that adjust strategies based on collective market sentiment. Using Refinitiv news data processed by a fine-tuned LLaMA 3.3 model, sentiment vectors are appended to the state and used in the advantage computation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. By incorporating sentiment into the advantage computation (Eq. 2), the paper moves beyond prior sentiment-as-feature paradigms and introduces a novel mechanism for modulating policy gradients in a behaviorally grounded way. The approach remains mathematically simple yet produces substantial empirical gains in both Sharpe ratio and cumulative return. \n\n2. The empirical validation is thorough, including diverse baselines (e.g., PPO-Sentiment-State, PPO-Sentiment-Reward, SAC-Sentiment-State), robustness tests on larger portfolios, and expanded time periods"}, "weaknesses": {"value": "1. The computational cost associated with sentiment extraction (LLaMA 3.3 inference, cosine filtering, daily aggregation) is only described qualitatively as efficient, but no quantitative training-time or resource comparison is provided. For practical deployment in high-frequency trading, this detail is crucial.\n\n2. SAPPO exhibits increased turnover (12% vs. 3.5% for PPO) and trading cost (3% of annual portfolio value), which could reduce net profit under realistic transaction fees or liquidity constraints.\n\n3. The paper’s interpretability analysis remains surface-level: while sentiment influences are discussed qualitatively, there is no deeper causal attribution or visualization connecting sentiment shifts to specific trading actions."}, "questions": {"value": "1. How does sentiment weighting affect PPO’s theoretical convergence?\nCan the authors provide a formal derivation or stability proof showing that the sentiment-weighted advantage update (Eq. 2) preserves PPO’s monotonic improvement property? \n\n2. Can turnover and transaction costs be further mitigated?\nGiven SAPPO’s higher daily turnover, have the authors explored regularization methods (e.g., turnover penalties or entropy constraints) to maintain adaptability while reducing trading frequency?\n\n3. How robust is the model to sentiment errors or lags?\nSince sentiment extraction relies on LLaMA-based NLP models, can SAPPO maintain its advantage if sentiment data is noisy, delayed, or misclassified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1CqYV9lVjw", "forum": "LhSieiKSc8", "replyto": "LhSieiKSc8", "signatures": ["ICLR.cc/2026/Conference/Submission14185/Reviewer_VJvd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14185/Reviewer_VJvd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731526371, "cdate": 1761731526371, "tmdate": 1762924641138, "mdate": 1762924641138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Sentiment-Augmented PPO (SAPPO), a simple extension of the Proximal Policy Optimization algorithm for portfolio management that integrates market sentiment into the policy-update step. Instead of adding sentiment as an input feature, SAPPO weights the advantage function by a sentiment score, so that policy updates are scaled up or down depending on whether market sentiment is positive or negative. This modification allows the agent to adapt its risk-taking behavior dynamically without altering PPO’s theoretical structure or stability. Experiments on multiple financial datasets show that SAPPO achieves higher returns and Sharpe ratios than standard PPO and other sentiment-aware baselines, while maintaining robustness across market regimes. The paper argues that this approach is a simple, interpretable, and effective way to fuse qualitative sentiment information with quantitative reinforcement-learning strategies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a clear and timely problem—how to make reinforcement learning for portfolio management more responsive to market sentiment. Its main idea, weighting the PPO advantage by sentiment, is simple, intuitive, and easy to implement. The method keeps PPO’s efficiency and stability while adding useful behavioral awareness. Experiments are thorough and show consistent gains across datasets and conditions. Overall, it’s a practical, well-executed approach with strong empirical results and clear real-world potential."}, "weaknesses": {"value": "While the proposed sentiment-weighted advantage method is simple and effective, it is not fully convincing that this is the best or most principled way to integrate sentiment information into reinforcement learning. Sentiment could be incorporated in several other ways—for example, as input features to the policy or value network, as part of the reward function, or through adaptive risk control mechanisms. Moreover, a classical two-step approach—first predicting future returns using sentiment-enhanced models, then performing portfolio optimization based on those forecasts—remains a natural and well-established alternative. The paper does not clearly explain why direct weighting of the advantage is superior to these more conventional or modular strategies, nor does it provide comparisons against strong sentiment-aware baselines that use such setups. As a result, the contribution, while practical, may appear somewhat incremental or empirically motivated rather than conceptually justified."}, "questions": {"value": "Could the authors elaborate on why weighting the advantage function by sentiment is a more effective or principled integration point than incorporating sentiment through other mechanisms, such as reward shaping, state augmentation, or adaptive risk scaling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E9w1pohool", "forum": "LhSieiKSc8", "replyto": "LhSieiKSc8", "signatures": ["ICLR.cc/2026/Conference/Submission14185/Reviewer_qaGM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14185/Reviewer_qaGM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762025495262, "cdate": 1762025495262, "tmdate": 1762924639134, "mdate": 1762924639134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}