{"id": "nwANzdlMKI", "number": 16172, "cdate": 1758260958530, "mdate": 1759897256852, "content": {"title": "TherapyGym: Evaluating and Aligning Clinical Fidelity and Safety in Therapy Chatbots", "abstract": "Large language models (LLMs) are increasingly used for mental-health support, yet prevailing evaluation methods—fluency metrics, preference tests, and generic dialogue benchmarks—fail to capture the clinically critical dimensions of psychotherapy. We introduce TherapyGym, a framework that evaluates and improves therapy chatbots along two pillars drawn from clinical science: fidelity and safety. Fidelity is operationalized through the Cognitive Therapy Rating Scale (CTRS), adapted into an automatic pipeline that scores both adherence to CBT techniques and therapist competence across multi-turn interactions. Safety is assessed using a multi-label annotation scheme over conversations, covering domain-specific risks(e.g., judgmental behavior, failure to address harm). To mitigate bias and unreliability in LLM-based judges, we further release TherapyJudgeBench, a validation set comprising 116 dialogues and 1,270 expert ratings, enabling systematic auditing and calibration of judge performance against licensed clinicians. Beyond evaluation, TherapyGym functions as a training harness: CTRS- and safety-derived signals serve as rewards in an RL loop where an LLM therapist engages programmable, realistic patient simulations spanning symptom profiles and conversational styles. Empirically, models trained in TherapyGym achieve higher automatic CTRS scores with improvements that transfer to expert human ratings, demonstrating gains in both clinical skill and safety. Our contributions establish a scalable, clinically grounded pathway for developing therapy chatbots that are not merely conversationally fluent but also faithful to evidence-based practice and responsible in high-stakes use.", "tldr": "", "keywords": ["alignment", "llm", "therapy", "evaluation", "safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2cd09918e6417a491f036472a7676b562df3a074.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper present TherapyGym, an evaluation framework to assess two aspects (fidelity and safety) of therapy chatbots. Fidelity means \" how skillfully a therapist implements a treatment model\" e.g. adherence and competence. Authors adopted  Cognitive Therapy Rating\nScale (CTRS) for this aspect. Safety refers to \"requires therapists to avoid harmful behaviors, which in chatbot settings demands additional constraints\". Authors captured 4 \"complementary suite of tests targeting chatbot-specific risks\" to represent the Safety aspect. They simulated multi-turn dialogue between therapist and client to test llms. They also released THERAPYJUDGEBENCH to test different judge models performances. At the end, they did RL finetuning with GRPO to compare the performance differences of models before and after on their evaluation pipeline/human labels."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1 importance of topic. \n- the topic of evaluating therapy chatbot is crucial, especially with many news/cases of users being affected negatively after using chatbot for their own psychotherapy session.\n\nS2 Construct a expert-annotated judge dataset\n- authors recruited two experts to annotate on their simulated conversations, which could treat as a way to train a better judge model in psychotherapy field."}, "weaknesses": {"value": "W1: Biggest weakness is that there are insufficient validations/measures to ensure that the two components of the evaluation pipeline reflect real-world therapy and clients accurately, and to ensure that the evaluation pipeline’s performance is comparable to that of human experts.\n\n(1) Simulation conversations: the authors claimed the conversations are realistic in line 94 (imo which is crucial for a practically important evaluation) but I doubt it:\n\n- Simulated conversations are not good reflective of the real world theory-client conversations\n-  limited to 10 turns while real-world has long conversation in therapy session (usually at least 15mins-1hr).\n- did not do any validation on the quality of the conversation (e.g., Consistency score conducted by [1])\n\nAlso, authors adopt different model choices for simulation (e.g. using o3-mini to implement the patient), which suggests that authors have some criteria for selection, but the paper provides no any explanation/elaboration. For instance, I would expect some human validations comparing different models simulations on role of patient, along with a cost-performance analysis to justify the choices.\n\n(2) llm judge (TherapyJudgeBench)\n- authors did not explain how many experts annotate the tables on the judge bench. I assume authors use the two licensed practitioners labels (as in Table 2). First, the low interrater reliability on some aspects (e.g. \\alpha = 0.2 in Guid.) suggests that there may be issues with providing guidance and standardizing the grades from human experts during data collection. I believe this could seriously undermine the evaluation pipeline and all the claimed performance improvements through RL/GRPO, since we are not sure whether the gold labels are correct (here I assume the authors used human labels since they did not state it clearly). Could you share more on how you avoid this issue (see Q3 i)?\n\n- Second, the authors miss details about the practitioners and how they perform annotation. These details are needed to help readers judge whether their labels are trustworthy.\n\nW2 Lack of novelty and not sure the explicit differences compared with prior works.\n- the simulation between therapist and clients has been explored since 2023 ([1], [2]). There are many important unsolved follow-ups in the field. For instance, how to ensure the simulated multi-turn conversations are consistent. \n- Authors compared different benchmarks in Table 1, but I do not find the comparison is meaningful. First, I do not understand the meaning of each dimensions. (e.g. what is \"Skill Granularity\" or \"safety sensitivity\"?). I would suggest authors provide more examples and definitions of each.\n- More critically, it feels to me the comparison table is a bit biased. It serves as to justify their evaluation pipeline as novel by merely adding the features/experiments authors implemented as dimensions to compare with prior works. I expect they can walk through the reason why we need to include those dimensions (with analysis e.g. what is lacking from the current simulation approach suggested by others). The motive of \"but these typically assess generic traits such as “empathy”, “fluency”, and “helpfulness” without grounding in clinically validated constructs.\" (line 74-75) is good but I do not see how exactly the dimensions authors suggested relate to what they claim.\n\nW3 lack of many important implementation details, metric details and lack of in-depth discussion\n- the writing is rough in discussion section, when compared with the earlier sections. I recommend more in-depth analysis with examples and case studies on the failure modes of models. It could benefit the community more by showing how exactly the models improve (through RL/GRPO) via qualitative analysis and examples.\n- several implementation details  (See q2, q4) and metric calculation (See Q6) are missing\n- how the metric (safety violation rate in Table 4) is calculated? \n\n[1] https://arxiv.org/abs/2401.00820\n[2] https://arxiv.org/abs/2405.19660"}, "questions": {"value": "Q1 why select o3-mini to implement patient? (line 198-199)\n\nQ2 how you get patient profiles? (line 354-355)\n\nQ3 \ni) how you ask clinicians annotate: (table 2)\n\\alpha = 0.2 is very low and it looks like annotators do not have good guidance/ definitions beforehand\nfrom line 315- 318, there are descriptions on instructing the annotators in different stages. can authors provide more info?\nii) what are their background? how long do they practice in CBT?\n\nQ4 definition of each aspects to help reader interpreting\n\nQ5 table 4 -- authors compared between human vs ai. what is the gold label? human vs human has low consistency\n\nQ6 table 3: why it has nan?\n\nQ7 simulation examples? not sure how good the simulated dialogues are. (W1 (1))\n\nQ8 \"Safety is captured through a complementary suite of tests targeting chatbot-specific risks, including judgmental\nbehavior, failure to address harm and speculation about symptoms.\" I'm not sure what it means. What are the suite of tests? Do you mean you do literature search and classify some important safety risks related to counselling?  (line 89-90)\n\nminor: fig 3 numbers are being overlapped by the plotted circles."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FuVBTWjMcf", "forum": "nwANzdlMKI", "replyto": "nwANzdlMKI", "signatures": ["ICLR.cc/2026/Conference/Submission16172/Reviewer_RSRq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16172/Reviewer_RSRq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864999447, "cdate": 1761864999447, "tmdate": 1762926335904, "mdate": 1762926335904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TherapyGym, a novel framework for evaluating and improving therapy chatbots along two clinically critical dimensions: fidelity to CBT principles and safety. The framework consists of three core components: 1) a dataset component (TherapyJudgeBench) comprising 116 multi-turn, synthetic dialogues annotated by licensed therapists using the Cognitive Therapy Rating Scale (CTRS) for fidelity and a four-category scheme for safety; 2) an evaluator component (FidelitySafetyJudge), an LLM-based judge that shows strong agreement with human expert ratings; and 3) an alignment component, which uses the judge's output as a reward signal for  fine-tuning within a simulated patient environment. Empirical results show that models trained in TherapyGym achieve higher automatic CTRS scores, and these improvements transfer to higher ratings from human experts, all while maintaining or improving safety."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The methodology is sound and rigorously designed. The use of the established CTRS scale provides a validated foundation for assessing therapeutic fidelity. The experiments are comprehensive, covering inter-rater reliability among human experts, alignment between LLM judges and humans, and the end-to-end efficacy of the RL fine-tuning pipeline. The decision to exclude CTRS dimensions with low human-human agreement (e.g., Guided Discovery) before reward modeling is a prudent choice that strengthens the reliability of the training signal. The reported metrics (Krippendorff's α, Spearman's ρ) are appropriate for the task."}, "weaknesses": {"value": "As noted, the simulated patients, while based on cognitive models, may behave more ideally and articulately than real-world patients who are often hesitant, ambivalent, or inarticulate. This could limit the framework's ability to evaluate crucial therapist skills like dealing with resistance, navigating ambiguity, and building rapport with a reluctant client. Further, The four safety categories are a good start but may not encompass all emergent risks in therapeutic AI. For instance, the risk of fostering over-dependence or \"AI addiction,\" the model creating an illusion of its own consciousness, giving overly directive advice, or failing to recognize cultural nuances are important safety considerations not currently captured."}, "questions": {"value": "1. The conversations in TherapyJudgeBench are limited to 10 turns. Given that therapeutic alliance and complex issues often unfold over much longer interactions, how do you plan to scale the framework to evaluate longer, multi-session dialogues? Do you foresee challenges in maintaining judge accuracy or reward model stability over extended contexts?\n2. The finding that few-shot prompting performed worst is intriguing. You hypothesize \"prompt dilution and context-length limitations.\" Could you provide more detail or an example of the few-shot prompt structure? Have you explored alternative methods like chain-of-thought or providing scoring rationales from the experts to improve few-shot performance?\n3. Regarding the safety taxonomy, what was the process for selecting these four specific categories? Were other potential risks (e.g., fostering dependency, giving overly simplistic advice for complex trauma) considered and excluded? How extensible is the framework to incorporating new safety labels in the future?\n4. The framework is currently specialized for CBT. What are the main challenges in adapting TherapyGym to other evidence-based therapeutic modalities (e.g., ACT, DBT)? Would it primarily require a new expert-annotated dataset and a different rating scale, or are there more fundamental architectural changes needed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SmFFd4N6qx", "forum": "nwANzdlMKI", "replyto": "nwANzdlMKI", "signatures": ["ICLR.cc/2026/Conference/Submission16172/Reviewer_rjpL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16172/Reviewer_rjpL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988983345, "cdate": 1761988983345, "tmdate": 1762926335286, "mdate": 1762926335286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes THERAPYGYM, a framework to evaluate and improve therapy chatbots along two pillars: clinical fidelity and safety. Fidelity is operationalized via the Cognitive Therapy Rating Scale (CTRS), adapted to an automatic multi-turn scoring pipeline capturing adherence to CBT techniques and competence in delivery. Safety is assessed via multi-label annotations of domain-specific risks (e.g., judgmental language, failure to address self-harm). To address unreliability in LLM judges, the authors introduce THERAPYJUDGEBENCH, a validation set of 116 dialogues with 1,270 expert ratings, intended for auditing and calibrating LLM judge performance against licensed clinicians. Beyond evaluation, THERAPYGYM serves as a training harness: CTRS and safety scores are used as rewards in an RL loop where the LLM therapist interacts with simulated patients exhibiting varied symptom profiles and conversational styles. The authors claim that models trained in THERAPYGYM improve automatic CTRS scores and that these gains transfer to expert ratings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Framing evaluation around clinical constructs (fidelity and safety) is exactly the direction the community needs; CTRS-linked behavioral coding is far more meaningful than generic fluency or preference metrics.\n- A dual-purpose environment (evaluation plus RL training harness) is attractive and pragmatic—closing the loop from measurement to improvement.\n- LLM-judge validation against licensed clinicians via THERAPYJUDGEBENCH is a good step toward quantifying judge reliability.\n- Emphasis on multi-turn, process-level competence with programmable patient simulations acknowledges the interactive nature of therapy."}, "weaknesses": {"value": "- CTRS automation is underspecified. The paper lacks a clear rubric, item mappings (CTS-R/CTRS to the “9 CBT skills”), scoring granularity (turn vs session), aggregation rules, and reliability analyses, making it hard to audit whether CTRS is faithfully operationalized.\n- Human rating setup is thin and ambiguous. Only four raters are mentioned; rating units (turn/skill/session) are unclear; inter-rater reliability is missing; claims that gains “transfer to expert ratings” lack quantitative evidence.\n- Risk of judge gaming (Goodhart’s Law). Using CTRS- and safety-derived judges in RL without robust controls raises concern that models optimize the judge rather than clinical quality; the paper lacks out-of-judge, blinded human evaluations and adversarial tests.\n- Safety taxonomy and coverage are underdefined. The label set, annotation protocol, prevalence, and inter-rater agreement are not specified; multi-turn detection and severity handling are unclear.\n- Patient simulation specificity is weak. How profiles are generated, distributions chosen, and realism validated is not described, risking overfitting to unrealistic interaction patterns."}, "questions": {"value": "- Provide the exact CTRS/CTS-R instrument: cite the scale, list items, show the mapping to the nine skills, specify scoring per turn vs session and aggregation, and report agreement among LLM judges and with human raters.\n- Detail the safety label set and protocol: definitions, guidelines, prevalence, inter-rater reliability, multi-turn detection, and severity scoring; consider releasing a small de-identified subset for benchmarking.\n- Explain the RL setup and anti-gaming controls: policy architecture, reward shaping (fidelity + safety), on/off-policy choice, and safeguards; include out-of-judge, blinded human evaluations and adversarial stress tests to demonstrate generalization.\n- Expand THERAPYJUDGEBENCH details: number/qualification of raters, rating granularity, reconcile the 1,270 ratings with units, and report alignment metrics with confidence intervals (correlations, κ, Bland–Altman).\n- Clarify patient simulation generation and realism checks, and confirm CTRS gains on unseen conversations and human-generated dialogues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2oSxvB8c8Q", "forum": "nwANzdlMKI", "replyto": "nwANzdlMKI", "signatures": ["ICLR.cc/2026/Conference/Submission16172/Reviewer_QqSC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16172/Reviewer_QqSC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995959691, "cdate": 1761995959691, "tmdate": 1762926334415, "mdate": 1762926334415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TherapyGym, a framework for evaluating the fidelity and safety of LLM-based therapists based on the Cognitive Therapy Rating Scale and improving LLM therapist chatbots via reinforcement learning. TherapyGym’s components include a dataset of 116 simulated 10-turn therapist-patient conversations, an LLM evaluator that scores dialogues on fidelity and safety, and an online reinforcement algorithm to align and improve LLM therapists. The authors demonstrate that their RL fine-tuning approach using GRPO leads to higher ratings of LLM therapists from an LLM judge and human therapists within their experimental setup."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work contributes a new evaluation method for LLM therapy chatbots by focusing on clinically relevant measures of performance; namely, adherence and competence in delivering treatment, and the avoidance of harmful behaviors. Their framework uses a clinically validated scale (CTRS) to evaluate and fine-tune LLMs as therapists."}, "weaknesses": {"value": "TherapyGym was not evaluated on a realistic, comprehensive dataset of therapist-patient dialogues (it uses just 116 dialogues, each consisting of only 5 turns per agent).\n\nGeneral-purpose and mental health-specific LLMs are often biased and generate false information. Previous work cited in the introduction specifically cautions against using simulated clients and therapists to avoid this and instead suggests evaluating LLMs on real-world therapist-client interactions, making the reliance on simulated agents a significant drawback of this work. \n\nThis work did not introduce novel algorithms. RL for fine-tuning LLMs, even for therapy, have been used in prior works, and GRPO was recently introduced by DeepSeek.\n\nFidelitySafetyJudge only rates LLM therapists on a subset of CTRS skills with high human reliability. The skills Guided Discovery and Application of CBT Techniques were excluded despite being highly important aspects of an effective therapist intervention.\n\nFidelitySafetyJudge ratings appear much higher than those of human annotators. This bias should be addressed before TherapyGym can be considered an accurate, robust framework for training and evaluating LLMs as therapists. \n\nResults show that even after fine-tuning, LLM therapists do not achieve average CTRS scores above 0.5 from human annotators (rescaled from 0-6 to 0-1)."}, "questions": {"value": "Why is the proposed method not evaluated on more realistic datasets?  This work could benefit from additional evaluation with real-world dialogue datasets (High-and-Low-Quality Therapy Conversation Dataset, HOPE), providing more grounding for TherapyGym as a robust LLM therapist evaluation framework.\n\nGiven the incongruity between the FidelitySafetyJudge and human therapist raters, it is unclear if its evaluations are accurate enough or well-aligned to expert raters. How do the authors envision future improvements to the LLM rater?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vZZxgmhrHj", "forum": "nwANzdlMKI", "replyto": "nwANzdlMKI", "signatures": ["ICLR.cc/2026/Conference/Submission16172/Reviewer_HJ1s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16172/Reviewer_HJ1s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005898313, "cdate": 1762005898313, "tmdate": 1762926333900, "mdate": 1762926333900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}