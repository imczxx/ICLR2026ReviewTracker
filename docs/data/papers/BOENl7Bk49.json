{"id": "BOENl7Bk49", "number": 15256, "cdate": 1758249346112, "mdate": 1759897317675, "content": {"title": "One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation", "abstract": "Learning dexterous bimanual manipulation policies critically depends on large-scale, high-quality demonstrations, yet current paradigms face inherent trade-offs: teleoperation provides physically grounded data but is prohibitively labor-intensive, while simulation-based synthesis scales efficiently but suffers from sim-to-real gaps. We present BiDemoSyn, a framework that synthesizes contact-rich, physically feasible bimanual demonstrations from a single real-world example. The key idea is to decompose tasks into invariant coordination blocks and variable, object-dependent adjustments, then adapt them through vision-guided alignment and lightweight trajectory optimization. This enables the generation of thousands of diverse and feasible demonstrations within several hour, without repeated teleoperation or reliance on imperfect simulation. Across six dual-arm tasks, we show that policies trained on BiDemoSyn data generalize robustly to novel object poses and shapes, significantly outperforming recent baselines. By bridging the gap between efficiency and real-world fidelity, BiDemoSyn provides a scalable path toward practical imitation learning for complex bimanual manipulation without compromising physical grounding.", "tldr": "BiDemoSyn synthesizes diverse, real-world bimanual demonstrations from a single example using vision-guided adaptation and hierarchical optimization, removing the need for simulation or manual data collection in scalable imitation learning.", "keywords": ["bimanual robotic manipulation", "one-shot learning", "demonstration synthesis", "imitation learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ffab34e4ffacaf6b3599f9151463c33ed96bbdaa.pdf", "supplementary_material": "/attachment/771a3e9ffe051758908489bcb2c338a8a4afcb93.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a data generation system for bi-manual robotic manipulation. The system aims at generating N feasible trajectories given one demonstration and N initial object states. By dividing the demonstration into multiple blocks, classifying them into invariant and variable types, and using a vision detection pipeline to adjust the grasping position in variable blocks, the data generation pipeline can adapt to changes of object sizes and shapes. \nExperimental results show that visuomotor policies learned with the proposed pipeline outperform prior methods in success rates, generalization, and data collection efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Low-cost data generation is a significant problem in learning generalizable robotic policies. The system proposed in this paper is effective for some bi-manual tasks and can generalize to some novel object instances.\n- Rich quantitative results and visualizations are presented in experiments. The method is compared with proper baselines (DemoGen and YOTO) and the performance surpasses them. Real-world results are acquired with enough number of tests."}, "weaknesses": {"value": "- The implementation is quite complicated, with many hyper-parameters and options to be decided by the human operator. Several designs imply additional assumptions, which are not explicitly described in the paper.\n    - The deconstruction criteria (4.1) includes some thresholds. The definition of single-arm motion and dual-arm coordination does not make sense. What if two arms move simultaneously but are doing independent tasks in the demonstration (e.g. grasping two objects together)?\n    - The vision-based estimation pipeline (4.2) includes estimating the principal axes from depth. When the object is randomly rotated and partially occluded, this may not work.\n    - The instance-level motion adaptation process introduce a new hyper-parameter $\\lambda$. And, obviously, this simple adaptation strategy only works for similar objects (e.g. cups with similar structures and only differ in width and height), resulting in quite limited object-level generalization.\n- It is unclear how to synthesis visual observation data within each generated trajectories for visuomotor policy learning. For a synthetic trajectory, its initial observation is set by the human using the data collection system, and the robot states and actions are generated by the trajectory optimization system; but how to generate visual observations for each timestep and what kind of visual observations are used (full point cloud or partial point cloud?) are not presented in the paper.\n- Compared with DexMimicGen (Jiang et al., 2024), the technical contribution is quite limited. Vision-based alignment, which is a main contribution, highly depends on the success of the vision estimation pipeline and cannot accurately adapt to very different objects.\n- The system still require the human to reset the initial state for thousands of times and record them. Why does its efficiency comparable to DemoGen (Figure 4)?"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BDjNwXHzTZ", "forum": "BOENl7Bk49", "replyto": "BOENl7Bk49", "signatures": ["ICLR.cc/2026/Conference/Submission15256/Reviewer_2qxQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15256/Reviewer_2qxQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902566790, "cdate": 1761902566790, "tmdate": 1762925556454, "mdate": 1762925556454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission introduces BiDemoSyn, a framework for synthesizing real-world robotic bimanual manipulation data. It takes a single real-world example data, and then generates more through several decomposition steps. Experiments show that it achieves higher synthesis efficiency and better data quality compared to existing data synthesis method, and demonstrates robotic policies trained on generate data can achieve good performance in real world."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation to directly synthesizing robotic manipulation data in real world is interesting and appreciated.\n- The introduced real-world system is comprehensive.\n- Robotic policies trained on generated data achieve decent results."}, "weaknesses": {"value": "- The main method behind BiDemoSyn relies on different components, such as pre-trained vision encoders, planning algorithms, etc, making it a complex system. It's hard to understand what's the difference between existing systems at a high level.\n- The introduced data synthesis system seems to have strong assumptions about the tasks and objects. For example, it assumes static objects which never move during the task execution. Also it's pose estimation probably will not work with deformable objects.\n- The methods require a top-down view to setup the camera to avoid occlusions, which is always feasible for complex tasks or mobile manipulation.\n- Regarding the evaluation metric, the definition of data quality is not appropriate as it is solely based on visual authenticity. In fact, authors should try to deploy generated data on real robots to verify the data quality.\n- Why only point cloud-based policies are considered and experimented? Can the generated data be used to train RGB-only policies?\n- In policy training comparisons, authors should also include results of policies trained on teleoperation data to help understand the gap.\n- Tasks are too simple. There are no distractors, no occlusions, and the background is very clean and has a distinct color.\n-  What are failure cases of the proposed system?"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iSBw1KoBah", "forum": "BOENl7Bk49", "replyto": "BOENl7Bk49", "signatures": ["ICLR.cc/2026/Conference/Submission15256/Reviewer_APLJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15256/Reviewer_APLJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960019163, "cdate": 1761960019163, "tmdate": 1762925555727, "mdate": 1762925555727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BiDemoSyn, a framework for synthesizing bimanual trajectories from single human demonstrations. The motivation of this paper is well-founded, as it focuses on solving a core challenge in the field: bridging the gap between data-collection efficiency and real-world fidelity for imitation learning. This paper combines task decomposition with vision-guided adaptation and trajectory optimization, is empirically shown to produce data that enables visuomotor policies to achieve significant robustness and generalization on complex, contact-rich tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tClear Motivation and Problem Formulation. The paper excels at identifying a critical bottleneck in robotics, the trade-off between data scalability and physical fidelity, and presents a clear, well-structured framework to solve it.\n2.\tSystematic Synthesis via Decomposition, Alignment, and Optimization. The framework's core technical contribution is its systematic three-stage pipeline: it first deconstructs a single demonstration into invariant logic and variable blocks. It then uses vision-based alignment to adapt these variable blocks to new scenes, followed by an optimization stage that ensures the final synthesized trajectory is physically feasible and collision-free. This structured approach generates thousands of diverse, physically-grounded demonstrations from one example, bypassing simulation or repeated teleoperation."}, "weaknesses": {"value": "1.\tThe success of the alignment stage (S2) is critically dependent on the accuracy of the 6D pose estimation. The paper employs a traditional \"geometry-aware processing\" method (image moments and PCA) rather than modern deep-learning-based estimators. The authors state this method yields \"<2% pose estimation errors\" in Appendix B.5, yet attribute 32% of all task failures to \"Orientation Estimation Errors\" in Appendix D.2. This suggests the traditional method is not robust, especially for symmetric or texture-less objects, as the authors admit. Could the authors clarify this discrepancy in error rates and justify why a more robust, modern pose estimator was not employed, given that this stage is the largest single point of failure?\n2.\tThe empirical evaluation, while thorough, is limited to only *six bimanual tasks*. While the framework is presented as a general-purpose synthesis tool, this limited task diversity makes it difficult to assess its true scalability. Furthermore, if a key outcome of this work is the generation of a large-scale, physically-grounded dataset, the value of this dataset for training generalist policies is questionable when it only covers six task domains. Can the authors clarify the effort required to extend BiDemoSyn to entirely new tasks?\n3.\tThe \"One-Shot\" premise is central to the paper's contribution. However, collecting a small handful (e.g., 5-100) of human demonstrations is often feasible and low-cost. A single demonstration may be sub-optimal, contain noise, or fail to capture the full task variance. Does this \"one-shot\" constraint artificially limit the quality and robustness of the \"invariant blocks\" (S1)? Have the authors considered a \"few-shot\" extension  (e.g., 5-100) where the framework could aggregate features or constraints from multiple demonstrations to generate a more robust and generalized reference trajectory?\n4.\tThe \"Deconstruction\" (S1) stage, which segments the trajectory into blocks (B_i) and AEPs, and then categorizes them as invariant/variable, seems to be the most manually intensive part of the pipeline. The paper mentions thresholds (e.g., $\\delta$, $\\zeta$, $\\gamma$) 131313 which appear task-specific. How much manual, task-specific engineering is required to design these decomposition rules? How scalable is this process if the framework were applied to a significantly more complex, long-horizon task with many intermittent contact steps?"}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rle23LPRxP", "forum": "BOENl7Bk49", "replyto": "BOENl7Bk49", "signatures": ["ICLR.cc/2026/Conference/Submission15256/Reviewer_cuHR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15256/Reviewer_cuHR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965240493, "cdate": 1761965240493, "tmdate": 1762925553886, "mdate": 1762925553886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BiDemoSyn, a system for generating real-world bimanual manipulation demonstrations. The key idea is to segment demonstrations into variable and invariable blocks. For variable blocks that depend on object pose, the method uses external vision modules to estimate and align object poses, enabling adaptation across different task configurations. Experiments on six bimanual manipulation tasks show that policies trained with BiDemoSyn-generated data outperform those trained with alternative data collection methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important and practical problem: scalable real-world data generation for bimanual manipulation in the real-world."}, "weaknesses": {"value": "- The contribution appears incremental. Prior work, such as DexMimicGen, has explored synthesizing bimanual manipulation data (though primarily in simulation), and the proposed segmentation strategy resembles those approaches. The main new component here is the integration of external vision modules for pose estimation, which may not be sufficient to justify a strong novelty claim.\n\n\n - Key experimental results require more explanation. For example, in Figure 4, it is unclear what metrics are used to evaluate “demo quality,” making it difficult to interpret the performance comparisons. Additional quantitative or qualitative analysis (e.g., success rate distributions, trajectory diversity metrics) would strengthen the experimental section. Moreover, the distinction between in-distribution and out-of-distribution settings is not clearly defined. It would be helpful for the authors to explicitly describe how these settings are constructed, what constitutes distribution shift in their experiments, and how that shift affects policy performance.\n\n\n - The paper does not sufficiently discuss relevant recent efforts in scalable bimanual and mobile manipulation data generation. In particular, MoMaGen (for long-horizon bimanual mobile manipulation data) is closely related, and a comparison would clarify BiDemoSyn’s contributions and limitations."}, "questions": {"value": "- In Figure 4, what specific metrics are used to evaluate the quality of the generated demonstrations?\n - When segmenting trajectories into blocks, does the process require manual annotation or demonstration labeling? If so, is there a way to automate or learn segmentation boundaries?\n - Have you explored reordering or randomizing block sequences to improve data diversity?\n - Can you clarify the distinctions between in-distribution and out-of-distribution settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "doNXUWZ0n7", "forum": "BOENl7Bk49", "replyto": "BOENl7Bk49", "signatures": ["ICLR.cc/2026/Conference/Submission15256/Reviewer_x4fy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15256/Reviewer_x4fy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985835332, "cdate": 1761985835332, "tmdate": 1762925553429, "mdate": 1762925553429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}