{"id": "GSTMBpvqGk", "number": 7939, "cdate": 1758044929479, "mdate": 1763154908337, "content": {"title": "Cooperative Multimodal Energy-based Model with MCMC Revision", "abstract": "This paper studies the learning problem of the energy-based models (EBM) for multimodal data. Learning EBMs via maximum likelihood estimation (MLE) typically involves Markov Chain Monte Carlo (MCMC) sampling, such as Langevin dynamics; however, noise-initialized Langevin dynamics is often ineffective and hard to mix. More critically, multimodal data contains complex inter-modal dependencies (i.e., relationships shared across modalities), making informative and coherent initializations across multimodalities particularly crucial for multimodal EBM sampling and learning. Notably, Multimodal VAEs, consisting of a shared generator model and a joint inference model, have made progress in capturing such inter-modal dependencies. But, both the shared generator and joint inference models are modelled as unimodal Gaussian (or Laplace), which can be limited in statistical expressivity for complex data and generator posterior distributions. In this work, we investigate the learning problem of the multimodal EBM, shared generator, and joint inference model by interweaving their MLE updates with respective MCMC revisions. With MCMC EBM revision, the shared generator learns to produce coherent multimodal initializations for EBM sampling. The joint inference model provides informative latent initializations as guided by MCMC posterior sampling. Both models serve as complementary initializer models that facilitate effective EBM sampling and learning, leading to realistic and coherent multimodal EBM samples. Extensive experiments demonstrate superior performance for multimodal synthesis quality and coherence compared to various baselines. Analysis, ablation studies, and supplementary experiments further validate the effectiveness and scalability of the proposed multimodal framework.", "tldr": "", "keywords": ["Energy-based Model", "Cooperative Learning", "Multimodality"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76f4a123f611a9356586e1d3712eb1b5df032d1e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method for training Energy-Based Models (EBMs) on multimodal data with a cooperative learning approach. It tries to solve the common problems in multimodal VAEs that arise from poor-quality distribution matching and low-quality sampling. The authors introduce a learning scheme where three models are trained together in a cooperative way. This helps the model generate high quality samples from good initialization points for the EBMs. The final multimodal EBM is capable of generating higher-quality and more coherent modality outputs on datasets such as PolyMNIST and CUB."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tries to address some of the main problems in multimodal VAEs, which are:\n1. Low-quality samples\n2. Coherent conditional and unconditional generation\n\nIn addition, the paper also implements their approach in larger scale dataset provided in the appendix where multimodal VAEs can't scale. The paper demonstrates that a cooperative approach in the EBMs helps the model achieve a superior generative capability. Knowing that Energy-Based-Models are difficult to train with multiple hyperparameters to tune, the authors did a good job of getting decent results using multiple EBMs in a cooperative approach."}, "weaknesses": {"value": "Some of the weaknesses of the paper include:\n\n1. $\\textbf{Clarity of Method}$ \n\nThe paper doesn't explain well the training of the EBMs and how they fuse it with multimodal VAE training. They also don't mention how they perform inference during missing modalities. In general, the paper could be written more effectively to address the core methods, while moving some mathematical repetitions to the appendix. \n\n\n2. $\\textbf{Missing baselines}$ \n\nThe paper compares well with current multimodal VAEs but since misses some related works that should be added as a baseline. [1] introduces using score-based multimodal autoencoders for multimodal VAEs, but it's not cited and compared as a baseline. Realizing the connection between score-based and energy-based models, and the use of additional EBM in that work, the baseline is necessary here. In addition, how does the proposed model perform when one of the EBM models is missing or initialized directly without Langevin sampling? \n\n3. $\\textbf{Limitations of EBMs}$ \n\nEnergy-Based-Models are difficult to train and unstable. Additionally, they require sampling during both training and inference. These makes them generally undesirable. And in these work, multiple EBMs are used which increases that effect. \n\n\n[1] Wesego, Daniel, and Amirmohammad Rooshenas. \"Score-based multimodal autoencoders.\" arXiv preprint arXiv:2305.15708 (2023)."}, "questions": {"value": "Please refer to the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WpBTFRTbuu", "forum": "GSTMBpvqGk", "replyto": "GSTMBpvqGk", "signatures": ["ICLR.cc/2026/Conference/Submission7939/Reviewer_qamK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7939/Reviewer_qamK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766248224, "cdate": 1761766248224, "tmdate": 1762919960894, "mdate": 1762919960894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a cooperative learning framework for multimodal generative modeling that unifies an energy-based model (EBM), a shared latent-variable generator, and a joint inference model within a single probabilistic system. The approach interleaves their maximum-likelihood updates with short-run MCMC refinements, allowing each component to benefit from the others. Specifically, the generator provides coherent multimodal initializations for EBM sampling, the inference model offers informative latent initializations for posterior sampling, and the EBM delivers corrective “revision signals” that refine both models. This self-correcting interaction mitigates poor MCMC mixing and ensures consistent cross-modal representations, enabling stable and efficient training. Experiments on PolyMNIST and CUB datasets demonstrate that the proposed multimodal EBM achieves superior synthesis quality and coherence, outperforming strong baselines such as diffusion-augmented CMVAE in FID and consistency metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work proposes a cooperative framework that combines the complementary strengths of VAE and MCMC, effectively addressing both the limited expressivity of unimodal Gaussian-based VAEs and the slow mixing of noise-initialized MCMC.\n- The method is evaluated across multiple benchmarks, which are familiar with the multimodal VAE community, demonstrating its applicability to various multimodal settings, including image–text and multimodal synthesis tasks.\n- The ablation studies show the best recipit of hyperparameters. The results show that increasing the number of short-run MCMC steps from 10 to 30 yields clear improvements (with diminishing returns beyond 60)."}, "weaknesses": {"value": "- While the paper emphasizes the idea of combining VAE and MCMC in a cooperative manner, this concept itself has already been explored in prior studies [1, 2, 3]. The present work seems to be an incremental extension that adapts this idea to the multimodal setting.\n- There is no comparison between your method and prior methods from the viewpoint of performance-time trade-off. MCMC converges slowly, so I doubt that Diff-CMVAE is superior from that viewpoint. \n- It is understandable that text–image consistency is not typically evaluated for CUB in multimodal VAE research, but recent tools such as CLIP make it possible to measure text–image similarity. Could the authors also report a similar trade-off curve for CUB, as they did for PolyMNIST, to illustrate the balance between fidelity and consistency?\n- The experiments mainly focus on PolyMNIST and CUB, which are relatively small datasets drawn primarily from the multimodal VAE literature. I wonder whether your method works well on datasets that contain larger images or modalities beyond text and image.\n- Although the proposed method outperforms Diff-CMVAE on CUB in FID, there is no direct comparison with recent, more powerful diffusion-based text-to-image models. How does this method position itself when compared to modern diffusion models? The authors claim superiority in terms of FID and modality consistency, but additional analysis is needed to clarify its significance in the broader SOTA landscape.\n\n[1] Hoffman, et al. Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo. ICML2017.  \n[2] Taniguchi, et al. Langevin Autoencoders for Learning Deep Latent Variable Models. NeurIPS2022.  \n[3] Grathwohl, et al. No MCMC for me: Amortized sampling for fast and stable training of energy-based models. ICLR2021."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U42gnVdiSC", "forum": "GSTMBpvqGk", "replyto": "GSTMBpvqGk", "signatures": ["ICLR.cc/2026/Conference/Submission7939/Reviewer_zBpm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7939/Reviewer_zBpm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831580403, "cdate": 1761831580403, "tmdate": 1762919960296, "mdate": 1762919960296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a generative model for problems in which instances come in different modalities, e.g. text and images. The paper is rather dense and brings together a number of concepts. The authors did a very good work in presented their model in a comprehensive manner, given a bit of patience. \n\nI will try to summarize as best as I can what the model is about. To do so I will use as a starting point a basic VAE model that operates on the multimodal setting, instances are structured as $\\bf X = (\\bf x_1, ..., \\bf x_M )$. The latent variable $\\bf z$ will be shared across all modalities. We then have:\n\n* the encoder $q_\\phi(\\bf z | \\bf X) = \\sum_i q_{\\phi_i} (z| \\bf x_i)$  where the modality specific learnt posteriors are modelled as Gaussians $q_{\\phi_i} ({\\bf z}|  {\\bf x_i} )=\\mathcal N(\\mu_{\\phi_i}({\\bf x_i}), \\Sigma_{\\phi_i}({\\bf x_i}) )$ making  $q_\\phi({\\bf z} | {\\bf X})$ a mixture of Gaussians. \n* the joint latent model is: $p_\\omega({\\bf X}, {\\bf z}) = p_\\omega({\\bf X} | {\\bf z}) p_0({\\bf z}) = \\prod p_{\\omega_i}( {\\bf x_i} | {\\bf z} ) p_0({\\bf z})$, where again $p_{\\omega_i}( {\\bf x_i} | {\\bf z}) = \\mathcal N(\\mu_{\\omega_i}({\\bf z}), \\sigma )$\n\nThe authors remark that the particular parametrization of the inferred posterior lacks representation power for the multimodal setting, making it a not very appropriate modelling choise for the posterior. An alternative would have been to use Langevin Dynamics (LD) to sample from the posterior, $p_\\omega({\\bf z} | {\\bf X }) \\propto p_\\omega( {\\bf X | \\bf z}) p_0( \\bf z ) = \\prod p_{\\omega_i}( {\\bf x_i | \\bf z}) p_0( \\bf z ) $, but here too there are issues such as poor mixing and small convergence when the prior $p_0( \\bf z ) $, is not very informative, in addition the product decomposition requires that all modalities are present. \n\nThe authors propose:\n1. to improve upon the representation power of the inferred posterior ${\\bf z}' \\sim q_\\phi(\\bf z | \\bf X)$ by following it up with LD to sample from the posterior  $p_\\omega({\\bf X} | {\\bf z})$ where the $q_\\phi(\\bf z | \\bf X)$ will provide a more informed prior for the LD sampling. They denote the result of this completion/refinement by $M_{\\omega}^{k_z}q_{\\phi}({\\bf z}|{\\bf X})$.\n2. They similarly complete the generation part ${\\bf X}' \\sim p_\\omega( {\\bf X | \\bf z}) p_0( \\bf z )$ by using the sampled ${\\bf X}'$ to initialise one more LD sampling process this time over an energy-based model, $\\pi_\\alpha(.)$, on $\\bf X$. They denote this completion by $M_{\\alpha}^{k_x}q_{\\omega}({\\bf X})$; this produces a richer model than the one operating solely on the product of Gaussians ($p_\\omega({\\bf X} | {\\bf z}) p_0({\\bf z}) = \\prod p_{\\omega_i}( {\\bf x_i} | {\\bf z} ) p_0({\\bf z})$).\n\nThus their model will learn:\n* the latent inference model $q_\\phi(\\bf z | \\bf X)$\n* the joint generative model $p_\\omega({\\bf X} | {\\bf z})$\n* and the energy model $\\pi_\\alpha( {\\bf X} )$\n\neach of these components produce KL terms towards the construction of the final loss function. The authors nicely explain the effect of each KL term and the interplay of the different components of the model. \n\nThe evaluation takes place in two datasets, a multimodal version of MNIST, and a bimodal dataset with a text and an image modality. The authors compare against a large number of baselines, most of them being variants/extensions of the simplified VAE structure given above. They evaluate the coherence of the model over the different modalities, namely whether the different modalities agree semantically, e.g. same digit in PolyMNIST over the modalities, and generation quality as quantified by FID."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* Well written paper with a careful analysis and explanations of the contribution of the different components. \n* Results seem to indicate meaningful improvements over the baselines considered."}, "weaknesses": {"value": "* Computational complexity, since within training we add two MCMC procedures. \n* Somehow limited novelty with respect to the previous variants, namely use the learnt components as more informative priors in the two MCMC procedures, nevertheless they do the work. \n* Baselines mostly revolve around variational models which perform rather badly compared to the proposed method and a Diffusion enhanced VAE."}, "questions": {"value": "* How long does it take to train, in table 1 you give a 1 to 3 seconds / iteration; what should I understand by iteration here? one batch update? what is the computational overhead compared to switching off the MCMC procedures? \n* For the conditional experiments I am not sure I saw much information. What is the conditioning factor? digit id in the case of MNIST or one of the modalities? and what in the case of CUB? on which parts of the model the condition is added? Or is it that one of the modalities is used to produce the latent variable $\\bf z$ by sampling from the respective inferred posterior $q_{\\phi}({\\bf z} | {\\bf x_i})$\n* On the conditional generation results visualisation (figure 8) were the samples produced by the authors code or copied from Palumbo 2023? In the latter case where all the architectural details equivalent? would have been also interesting to have the visualisations of (Diff)-CMVAE. \n* Numerical values FID on PolyMNIST? is this what is shown in table 6 of the appendix? Also in figure 1 each model appears three times, are these different trained models and their respective performances. And why this difference in the presentation of results (FID) table for CUB, scatter plot for PolyMNIST? \n* How is coherence evaluated in the case of CUB? or isn't evaluated? I am not sure I saw something like that whether in the main text or in the appendix\n* Ι would be curious to know how the proposed method operates in standard single modality settings compared to a baseline in which the MCMC is switched off; I guess this is probably explored in past work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wpQvi8I1li", "forum": "GSTMBpvqGk", "replyto": "GSTMBpvqGk", "signatures": ["ICLR.cc/2026/Conference/Submission7939/Reviewer_bVv4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7939/Reviewer_bVv4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855276629, "cdate": 1761855276629, "tmdate": 1762919959773, "mdate": 1762919959773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their valuable feedback. We are encouraged to hear that many of you found (1) our paper to be well-organized and clearly presented (**Reviewers** **fqno**,**bVv4**); (2) our idea to be novel (**Reviewer fqno**), meaningful (**Reviewer bVv4**), and effective in addressing existing limitations in the literature (**Reviewers zBpm, qamK**); and (3) our experiments and ablation studies to be a strong aspect of the work (**Reviewers fqno, bVv4, zBpm, qamK**). We shall carefully update our manuscript based on the discussions here.\n\nBelow, we provide **general responses** for common concerns and respond to each reviewer in their individual comments."}}, "id": "pB1KoQ52Gs", "forum": "GSTMBpvqGk", "replyto": "GSTMBpvqGk", "signatures": ["ICLR.cc/2026/Conference/Submission7939/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7939/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7939/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763154431337, "cdate": 1763154431337, "tmdate": 1763154431337, "mdate": 1763154431337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper learns multimodal EBMs by interweaving MLE with short-run MCMC revisions across three components: an EBM, a shared generator, and a joint inference model, so the generator and inference networks act as complementary initializers for effective EBM sampling. Experiments on CUB and PolyMNIST show that the proposed method achieves better multimodal coherence/quality, and ablations highlight the contributions of the shared generator and joint inference."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well organized and easy to follow: the problem setup, method, and objectives are laid out coherently, and the sectioning and notation make the technical ideas accessible.\n- While prior multimodal VAEs with iterative methods have used diffusion-based methods to improve either generation or inference, this work is the first to make both inference and generation iterative and to feed both revisions back cooperatively into training. This dual, mutually reinforcing loop is a genuine novelty."}, "weaknesses": {"value": "- Unclear global objective across the three losses. Section 3.1.1 specifies the objectives and §3.2.2 discusses their meanings, but it remains unclear what single global quantity the three optimizations jointly minimize/maximize. Consequently, it is not obvious why these particular objectives are necessary, and it seems plausible that other cooperative training objectives for heterogeneous models could be devised to similar effect.\n- Limited and potentially unfair comparison to iterative baselines (e.g., Diff-CMVAE). Because the proposed method, like Diff-CMVAE, requires iterative procedures, comparisons should clearly control for this. However, the paper compares against Diff-CMVAE only on CUB conditional FID; for CUB unconditional metrics, for coherence, and for PolyMNIST, the comparisons are primarily to non-iterative baselines. The authors should include Diff-CMVAE (or comparable iterative multimodal diffusion methods) in these settings as well. In particular, claiming superior FID on PolyMNIST without reporting Diff-CMVAE’s FID is not fair.\n- Lack of ablation-style cost–performance scaling vs. Diff-CMVAE. The method incurs iteration in both inference and generation; Diff-CMVAE iterates only for generation. It is required to quantify how runtime and accuracy scale (e.g., with latent size, image resolution, number of MCMC steps) relative to Diff-CMVAE, to clearly show the additional computation required and the resulting performance trade-offs."}, "questions": {"value": "Please address the above concerns with concrete clarifications and, where appropriate, additional experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M1jHwoJWgs", "forum": "GSTMBpvqGk", "replyto": "GSTMBpvqGk", "signatures": ["ICLR.cc/2026/Conference/Submission7939/Reviewer_fqno"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7939/Reviewer_fqno"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762098750113, "cdate": 1762098750113, "tmdate": 1762919959200, "mdate": 1762919959200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}