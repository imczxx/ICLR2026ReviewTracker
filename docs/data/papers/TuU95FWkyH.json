{"id": "TuU95FWkyH", "number": 11387, "cdate": 1758197933103, "mdate": 1759897578457, "content": {"title": "Bird's-eye-view Informed Reasoning Driver", "abstract": "Motion planning in complex environments remains a core challenge for autonomous driving. While existing rule-based or imitation learning-based motion planning methods perform well in common scenarios, they often struggle with complex, long-tail scenarios. To address this problem, we introduce the Bird's-eye-view Informed Reasoning Driver (BIRDriver), a hierarchical framework that combines a Vision-Language Model (VLM) with a motion planner. BIRDriver leverages the common sense reasoning capabilities of the VLM to effectively handle these challenging long-tail scenarios. Unlike prior methods that require domain-specific encoders and costly alignment, our approach compresses the environment into a single-frame bird's-eye-view (BEV) map, a paradigm that enables the model to fully leverage its knowledge from internet-scale pre-training. It then generates high-level key points, which are encoded and passed to the motion planner to produce the final trajectory. However, a major challenge is that standard VLMs struggle to generate the precise numerical coordinates required for such key points. We address this limitation by fine-tuning them on a composite dataset of three auxiliary types to enhance spatial localization, scene understanding, and key-point generation, complemented by a token-level weighted mechanism for improved numerical precision. Experiments on the nuPlan dataset demonstrate that BIRDriver outperforms the base motion planner in most cases on both Test14-hard and Test14-random benchmarks, and achieves state-of-the-art (SOTA) performance on the InterPlan long-tail benchmark.", "tldr": "", "keywords": ["Autonomous driving", "Key Intent Points"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/58c17f1aa5005f3d23fd7d0c7df53e211e090395.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes improvement over PLUTO hybrid planner by utilizing extra features generated by a VLM using BEV image. They fine tune the VLM to produce key points from BEV scene image using LoRA. These key points summarizes the trajectories of the agents in a compressed way. These are fed into the point encoding module of PLUTO planner to generate the trajectory. Their experiments show improvement on NuPlan and InterPlan over PLUTO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. BEV representation is quite generic and it's generalizable over sensor types, orientation and platforms. This can be applicable to a lot of different architectures and vehicle platforms.\n2. Using key points, instead of dense trajectory representation, is more efficient.\n3. Using weighted SFT loss and spatial localization learning for BEV are also generalizable techniques, that can benefit many works."}, "weaknesses": {"value": "1. The approach is generalizable, but it's only shown with PLUTO planner. It would have made the case stronger if the authors also showed benefit with at least one more planner with BEV based feature augmentation.\n2. The results are shown only using nuPlan, not using reactive simulator like CARLA. This work would benefit from adopting Bench2Drive for benchmarking."}, "questions": {"value": "1. Key point generation could be possible from deterministic or classical learning methods as well, instead of using VLM. That would make the inference latency more practical. Did the authors consider this?\n2. As the authors mention the PDCE loss, did they perform any comparison with their weighted SFT loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gQKMhjb8oi", "forum": "TuU95FWkyH", "replyto": "TuU95FWkyH", "signatures": ["ICLR.cc/2026/Conference/Submission11387/Reviewer_ctTg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11387/Reviewer_ctTg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713736412, "cdate": 1761713736412, "tmdate": 1762922510146, "mdate": 1762922510146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hierarchical motion planning framework that integrates a Vision-Language Model (VLM) with a motion planner. Specifically, the VLM interprets the driving scene from a BEV map and outputs keypoints to represent high-level driving intentions. The motion planner PLUTO is then extended to incorporate these keypoints for trajectory planning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The VLM in BIRDriver requires only the BEV map as input to convey the complete driving scene, eliminating the need for textual descriptions.\n\n2. BIRDriver expresses driving intentions through keypoints, which offer advantages over meta-action, hidden-state, and waypoint-based approaches.\n\n3. BIRDriver introduces a weighted SFT loss and three specialized datasets to enhance the accuracy of keypoint prediction.\n\n4. BIRDriver achieves state-of-the-art performance in closed-loop evaluations on long-tail scenarios of the InterPlan benchmark."}, "weaknesses": {"value": "1. The contribution of this work is limited: using BEV maps as VLM inputs has already been explored in prior work [1], and some studies [4] have even gone further by feeding BEV features directly into LLMs for autonomous driving tasks. Moreover, the authors claim that relying solely on BEV maps eliminates the need to align heterogeneous sensor data across different vehicle platforms. In fact, many existing works [2,3,4,5] have already achieved the same goal by using only map features or multi-view images as inputs.\n\n2. The paper conducts experiments only on nuPlan and its long-tail test set InterPlan, lacking evaluation on broader datasets such as NuScenes and Waymo. If time and computational resources permit, extending experiments to more datasets and comparing with additional VLM-based autonomous driving methods[2,4,5,6,7,8,9,10] would be valuable.\n\n[1] PlanAgent: A Multi-modal Large Language Agent for Closed-loop Vehicle Motion Planning\n[2] Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving\n[3] Asynchronous Large Language Model Enhanced Planner for Autonomous Driving\n[4] Holistic Autonomous Driving Understanding by Bird’s-Eye-View Injected Multi-Modal Large Models\n[5] OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning\n[6] VLP: Vision Language Planning for Autonomous Driving\n[7] Making Large Language Models Better Planners with Reasoning-Decision Alignment\n[8] VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision\n[9] EMMA: End-to-End Multimodal Model for Autonomous Driving\n\n[10] DRIVEVLM: The Convergence of Autonomous Driving and Large Vision-Language Models"}, "questions": {"value": "1. The paper extends the PLUTO architecture to implement the hierarchical framework. Can the proposed architecture be adapted to other existing advanced motion planners as well?\n\n2. Can the proposed framework meet the real-time requirements of autonomous driving motion planning, given that VLM inference may be relatively slow?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Ag8QDWeaz", "forum": "TuU95FWkyH", "replyto": "TuU95FWkyH", "signatures": ["ICLR.cc/2026/Conference/Submission11387/Reviewer_ScDp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11387/Reviewer_ScDp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987755783, "cdate": 1761987755783, "tmdate": 1762922509603, "mdate": 1762922509603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BIRDriver, an hierarchical framework that integrates Vision-Language Models (VLMs) with a motion planner to address the motion planning challenges in complex, long-tail autonomous driving scenarios. Unlike existing methods, BIRDriver utilizes only a single-frame Bird's-Eye View (BEV) as the primary input for the VLM, avoiding the reliance on domain-specific encoders and the complexity of multi-modal data alignment. The framework leverages the VLM's common-sense reasoning capabilities to generate keypoints, which are subsequently passed to the motion planner to produce the final trajectory. To address the difficulty of standard VLMs in generating precise coordinates, the authors designed three auxiliary datasets and a token-level weighting mechanism, significantly improving keypoint prediction accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1、VLM Integration Paradigm: The paper introduces a hierarchical architecture where a Vision-Language Model (VLM) generates high-level keypoints, while a conventional planner executes low-level trajectories. This approach elegantly balances the advanced reasoning capabilities of VLMs with the real-time performance and safety of traditional planners.\n\n2、Dataset Innovation: To address the limitations of Vision-Language Models (VLMs) in spatial localization and scene understanding, the authors meticulously designed three auxiliary datasets. This multi-task learning strategy significantly improved the accuracy of keypoint prediction.\n\n3、Comprehensive Experimental Validation: The method was extensively validated on the nuPlan dataset across three benchmarks: Test14-random, Test14-hard, and InterPlan. Notably, on the InterPlan long-tail test set, BIRDriver significantly outperforms PLUTO and Diffusion Planner, demonstrating its effectiveness in handling complex and rare scenarios."}, "weaknesses": {"value": "1. BEV-Only Input: Relying solely on BEV as input is overly idealistic and impractical. While the dataset currently in use contains BEV information, other datasets may not. More critically, ground-truth BEV information is not available during real-world driving. This would necessitate the model to predict the BEV, which in turn would introduce errors. Such a practical setting is far more complex than one with standard multi-view images, raising concerns about the model's generalizability and practical deployability.\n\n2. Inference Efficiency: The authors' acknowledgement of the VLM's inference efficiency issues in the limitations section is commendable for its honesty. A clarification is needed: What is the inference time for a single VLM call? Motion planning typically requires high-frequency operation (e.g., 10Hz or higher). Is the current framework executed synchronously (with the VLM being called at every step) or asynchronously? If synchronous, the VLM's latency would directly impact the system's reaction time.\n\n3. Keypoints: While the authors advocate for using no more than three keypoints, there is a lack of sufficient theoretical and experimental justification for why three is the optimal number. The experiment in Section 5.3 only compares the RDP algorithm with a method using only the final trajectory point, failing to explore the performance differences among using 2, 3, or 4 keypoints. In certain highly complex scenarios, three keypoints may be insufficient to fully express the complete driving intent, yet the paper does not investigate the relationship between the number of keypoints and performance. Additionally, more details regarding the setting of ε are required.\n\n4. Minor Performance Degradation in Simple Scenarios and Insufficient Analysis: As shown in Table 1, on the Test14-random benchmark, BIRDriver (91.46) performs slightly worse than its base model, PLUTO (91.87), on the CLS-NR metric. Although the margin is small, this raises an important question: Does the intervention of the VLM introduce unnecessary complexity or superfluous decisions in simple, routine scenarios, thereby causing this slight performance degradation? Does this imply the need for an arbitration mechanism between the VLM and the base planner, which would activate the VLM only when a difficult scenario is detected? The authors should provide an analysis and discussion of this phenomenon."}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pSjGAecKP5", "forum": "TuU95FWkyH", "replyto": "TuU95FWkyH", "signatures": ["ICLR.cc/2026/Conference/Submission11387/Reviewer_BksK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11387/Reviewer_BksK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010296417, "cdate": 1762010296417, "tmdate": 1762922509172, "mdate": 1762922509172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BIRDriver (Bird’s-eye-view Informed Reasoning Driver) that contains  hierarchical framework that integrates a Vision-Language Model (VLM) with a motion planner to improve autonomous driving to address long-tail scenarios. Unlike prior approaches that rely on multi-frame camera inputs or large waypoint sequences, BIRDriver uses a single-frame Bird’s-Eye-View (BEV) map and generates a small set of key points (≤3) to convey high-level driving intentions to the motion planner.\nBIRDriver key contributions are:\n\n- In BIRDriver, perception input tokens along with BEV maps are passed as input to VLM which interprets and outputs key points; motion planner generates trajectories based on these points.\n- In BIRDrivier, it predicts relative key points small set of points instead of full trajectories for efficient intention transmission.\n- BIRDriver uses three auxiliary datasets (Key Point, Spatial Localization, Driving Scene Stepwise) and a weighted SFT loss to improve numeric precision."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- BIRDriver combines VLM reasoning with BEV representation for motion planning, that results in reducing reliance on multi-modal alignment.\n- In BIRDrivier, it predicts relative a small set of key points that are  compact, interpretable interface between reasoning and planning.\n- In BIRDrivier, is good at spatial localization and scene understanding that addresses numeric token importance, improving precision in key point generation."}, "weaknesses": {"value": "- BIRDriver latency or inference times remains still, it concerns whether BIRDriver can be applied to real-time applications: VLM inference of BIRDriver remains heavy for real-time onboard deployment; paper only briefly mentions quantization and asynchronous inference as future work.\n- BIRDriver experiments are conducted on limited real-world validation: Experiments are simulation-based  Test14-random, Test14-hard, and InterPlan datasets. It would better to see zero-shot comparisons from simulation to real-world like nuScenes or waymo.\n- Paper lacks detailed discussion on failure cases or robustness under sensor noise and occlusions.\n- BIRDriver mostly focuses on nuPlan benchmarks; does not compare against recent RL-based or hybrid planning approaches like Diffusion Planner.\n- BIRDriver highly relies on BEV maps omiting temporal context beyond 2-second history; may limit reasoning in highly dynamic scenes."}, "questions": {"value": "- How does BIRDriver handle ambiguity in BEV maps, such as occluded agents or missing traffic signals?\n- Could temporal context (multi-frame BEV) improve reasoning without significantly increasing complexity?\n- How sensitive is the system to BEV map quality (e.g., sensor noise, map inaccuracies)?\n- It would be better to see how weighted SFT loss improves compared to reinforcement learning  GRPO like in Diffusion Planner or geometry-aware objectives?\n- What are the most common failure modes observed in InterPlan scenarios?\n- How does the system perform under real-time constraints? Any latency benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gOhDv5lOsV", "forum": "TuU95FWkyH", "replyto": "TuU95FWkyH", "signatures": ["ICLR.cc/2026/Conference/Submission11387/Reviewer_2yL1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11387/Reviewer_2yL1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762039806527, "cdate": 1762039806527, "tmdate": 1762922508767, "mdate": 1762922508767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}