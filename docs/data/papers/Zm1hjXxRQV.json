{"id": "Zm1hjXxRQV", "number": 17141, "cdate": 1758272672264, "mdate": 1759897194358, "content": {"title": "SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting", "abstract": "Continual Learning (CL) requires a model to learn multiple tasks in sequence while maintaining both stability—preserving knowledge from previously learned tasks, and plasticity—effectively learning new tasks. Orthogonal projection has emerged as an effective and popular paradigm in CL, where it partitions the gradient space of previously learned tasks into two orthogonal subspaces: a primary subspace and a minor subspace. New tasks are learned effectively within the minor subspace, thereby reducing interference with previously acquired knowledge. However, existing orthogonal projection methods struggle to achieve an optimal balance between plasticity and stability, as it is hard to appropriately partition the gradient space. In this work, we consider a continual learning paradigm based on Low-Rank Adaptation (LoRA), which has gained considerable attention due to its efficiency and wide applicability, and propose a novel approach for continual learning, called SplitLoRA. We first provide a theoretical analysis of how subspace partitioning affects model stability and plasticity. Informed by this analysis, we then introduce an effective method that derives the optimal partition of the gradient space for previously learned tasks. This approach effectively balances stability and plasticity in continual learning. Experimental results on multiple datasets demonstrate that the proposed method achieves state-of-the-art performance. The code is available at https://anonymous.4open.science/r/SplitLoRA-FB45.", "tldr": "This paper presents SplitLoRA, a method for continual learning that combines orthogonal projection with LoRA. It improves the balance between plasticity and stability by effectively mitigating interference between new and old tasks.", "keywords": ["Continual Learning", "Low-Rank Adaptation", "Gradient orthogonal projection"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa96e8a1177cac6e34381b6b601acfa284f90338.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the stability–plasticity trade-off in continual learning through the gradient projection method, which is a popular approach in this field. Existing projection methods heuristically choose the number of singular vectors, k, but the authors theoretically show that the upper bound of the loss increase can be expressed in terms of stability and plasticity losses. They further demonstrate that these two losses can be written as functions of k and propose a way to find the optimal value of k. The effectiveness of the proposed method, SplitLoRA, is demonstrated on CIFAR-100, ImageNet-R, and DomainNet, where it outperforms the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides solid work on a fundamental issue in continual learning, the balance between stability and plasticity. Expressing the upper bound as a function of stability and plasticity losses, and further relating this to the number of singular vectors, is an interesting and meaningful contribution."}, "weaknesses": {"value": "- Although expressing the upper bound in terms of k and proposing a method to find the optimal $k$ is interesting, the paper lacks depth, as this is the only component studied in detail. The authors argue that existing methods are limited because they require selecting $k$, yet their own method for finding the optimal $k$ also depends on the hyperparameter $\\alpha$.\n\n- The experimental section lacks thoroughness, as it only presents basic results such as comparisons with baselines and limited robustness analysis over different hyperparameter settings.\n\n- Additionally, the title does not accurately reflect the content. The proposed method does not fundamentally rely on LoRA and could be applied to other architectures. It is unclear why the authors emphasize LoRA in naming the method SplitLoRA."}, "questions": {"value": "1. Does the method work without LoRA?\n\n2. The authors should include pseudo-code for the complete end-to-end model training process. For example:\n\n    1. After training task $t-1$ and before training task $t$, obtain the average gradient of the model on the training data for task $t-1$. This average gradient is denoted $G_t^{old}$\n\n    2. Obtain the minor subspace $\\hat{U}_t$ of the gradient $G_t^{old}$.\n\n    3. During training for task t, map the gradient $\\nabla{W}$ onto the minor subspace of the previous task $t-1$ as $\\hat{U}_t^k \\hat{U}_t^{kT} \\nabla{W}$\n\n    4. ..."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aliLChjY6l", "forum": "Zm1hjXxRQV", "replyto": "Zm1hjXxRQV", "signatures": ["ICLR.cc/2026/Conference/Submission17141/Reviewer_swCo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17141/Reviewer_swCo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882225161, "cdate": 1761882225161, "tmdate": 1762927132620, "mdate": 1762927132620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper adopts the concept of Orthogonal projection in the gradient (sub-)space in continual learning (CL). It aims to balance two important aspects in CL, i.e., stability and plasticity. The proposed SplitLoRA partitions the gradient space into two complementary  subspaces via SVD, i.e., a major subspace capturing the most informative and stable directions of previously learned tasks, and a minor subspace containing the remaining, less significant directions, where the learning process takes place for the new task. The paper provides a theoretical analysis of how such subspace partitioning affects model stability and plasticity for CL, then proposes the SplitLoRA method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper well articulated its core idea of splitting orthogonally the gradient space into two complementary subspace via SVD, one for previously learned task and the other for the new task. The motivation is simple, but the authors provided a theoretical analysis on the impact of subspace partitioning on model stability and plasticity for CL. The proposed SplitLoRA methods demonstrated consistent good performance against the SOTA."}, "weaknesses": {"value": "It is arguable that the orthogonal projection might not be the ultimate solution for the catastrophic forgetting, although the proposed SplitLoRA could still match some of our current development of CL. Capacity of the proposed SplitLoRA in terms of the number of well learned tasks is not sufficiently discussed. It seems that the partitioning of the gradient space into previously learned tasks and the new task doesn't impact on the number of tasks to be learned."}, "questions": {"value": "Can the authors investigate the impact of the SplitLoRA method onto the capacity of a specific CL setting? It'd be more convincing to justify the gradient space partition strategy beyond the concern of balancing stability and plasticity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9Arfzj04uj", "forum": "Zm1hjXxRQV", "replyto": "Zm1hjXxRQV", "signatures": ["ICLR.cc/2026/Conference/Submission17141/Reviewer_1gUU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17141/Reviewer_1gUU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960799939, "cdate": 1761960799939, "tmdate": 1762927132259, "mdate": 1762927132259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the continual learning problem and the gradient projection method. Due to the balance between plasticity and stability, the paper explores low-rank adaptation within the continual learning paradigm. The experimental results also demonstrate the effectiveness of the proposed methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper utilizes LoRA in the continual learning framework and uses the gradient to update the process. An upper bound analysis on the loss increase is provided. Although I have not checked the entire proof, based on previous results, Equation 7 is more or less correct. The SplitLoRA algorithm is also presented.”"}, "weaknesses": {"value": "The overall paper is not well written. The main motivation for balancing plasticity and stability is addressed through the use of LoRA. However, LoRA may reduce information, and it is unclear why this method has this effect. Please provide more explanation to clarify the motivation and core contribution of the approach.\n\nThe modified method uses LoRA to replace the corresponding update, which is more akin to a report.\n\nRegarding the dataset and model, they are outdated and not suitable for the current problem. More recent data and new open-source models should be incorporated to demonstrate the effectiveness of the proposed method."}, "questions": {"value": "see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y2p2QCrFAO", "forum": "Zm1hjXxRQV", "replyto": "Zm1hjXxRQV", "signatures": ["ICLR.cc/2026/Conference/Submission17141/Reviewer_kyb6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17141/Reviewer_kyb6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974202865, "cdate": 1761974202865, "tmdate": 1762927131981, "mdate": 1762927131981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SplitLoRA, a Gradient Projection method for continual learning. Specifically, the authors present a theoretical analysis of how subspace projection influences the stability-plasticity trade-off, and then introduce a LoRA-based method to determine a better minor subspace for learning new tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper establishes a theoretical foundation for the proposed method by analyzing gradient projection and minor subspace, and by formulating a corresponding optimization problem.\n2. SplitLoRA attains state-of-the-art performance across three benchmarks under different task configurations."}, "weaknesses": {"value": "1. A major concern is the absence of evaluation metrics. In continual learning, it is critical to assess the stability-plasticity trade-off using both average accuracy (like FAA and CAA reported by authors) and backward transfer (BWT). The omission of BWT weakens the credibility of the empirical results and raises doubts about the robustness of the performance evaluation.\n2. Another issue is the limited selection of baselines in experimental comparison. As mentioned in Section 2.2, Gradient Projection methods in continual learning are closely related to the proposed method. However, I do not notice any Gradient Projection-based methods are included in experiments. Instead, most baselines seem to be prompt-based approaches. Could authors give any clarification on this baseline selection and include latest Gradient Projection approaches for a more comprehensive evaluation?"}, "questions": {"value": "1. In Eq. 7, should the gradient $G_t$ be $\\nabla \\mathcal{L}_t({W_t})$ or $\\nabla \\mathcal{L}_t (W_{t-1})$? \n\nAnd in Eq. 16, should $<\\Delta W_t, G_t>$ be negative, as the optimization follows gradient descent? \n\nAdditionally, should $\\hat{G}_t$ be $G_t^{old}$​? Please correct me if I misunderstand.\n\n2. As stated in Line 304, the ratio $\\alpha$ can vary dynamically. Does the mechanism of relacing it with a fixed hyperparameter have any theoretical justification, such as a bound or convergence guarantee?\n\n3. I recommend the authors provide more fine-grained time cost analysis in Table 4, such as time per iteration or per epoch. I suppose the currently reported time reflects the whole training process, which may be affected by other factors like data loading and evaluation.\n\n4. The authors should double-check their reference. There is a paper labeled \"under review at ICLR 2025\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5xwC7DQUqn", "forum": "Zm1hjXxRQV", "replyto": "Zm1hjXxRQV", "signatures": ["ICLR.cc/2026/Conference/Submission17141/Reviewer_Ceq4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17141/Reviewer_Ceq4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984994859, "cdate": 1761984994859, "tmdate": 1762927131699, "mdate": 1762927131699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}