{"id": "vOYC3RXxo5", "number": 12102, "cdate": 1758205692400, "mdate": 1759897533400, "content": {"title": "DHKR: Dynamic Hierarchical Knowledge Routing for Efficient Low‑Resource Alignment", "abstract": "Domain adaptation for large language models (LLMs) remains a significant challenge, often requiring extensive fine-tuning or inefficient architectural modifications. \nKnowledge-Aligned Domain Shift Tuning (KADA) is a parameter-efficient fine-tuning (PEFT) method that leverages the Lottery Hedge Fund Hypothesis (LHFH) to identify and reuse latent knowledge fragments. \nHowever, KADA is limited by its reliance on a static, predefined set of subnetworks, which restricts flexible adaptation to unseen domains.\n\nInspired by the routing principles of Mixture-of-Experts (MoE) and portfolio balancing strategies, \nwe propose Dynamic Hierarchical Knowledge Routing (DHKR). \nDHKR extends KADA by introducing \\textbf{two-level subnetwork growth}, a hierarchical routing mechanism that dynamically expands both high-level subnetworks ($K$) and their second-level components ($L$) on demand, thereby overcoming the static-size constraints of prior methods.\nTo ensure stability, DHKR employs a composite growth trigger—based on training stagnation, entropy reduction, usage imbalance, and routing stability—together with a multi-level Loss-Free Balancing (LFB) strategy. \nThis transforms KADA’s static mechanism into a self-adaptive, robust knowledge routing system that adjusts capacity and structure to new domains while maintaining stable performance.\n\nExperiments on instruction based benchmarks show that DHKR not only preserves the strengths of KADA but also addresses its key limitations.\nOur work provides a principled framework for dynamic, knowledge-aligned adaptation of LLMs.", "tldr": "Dynamic Hierarchical Knowledge Routing is knowledge alignment tuning framework", "keywords": ["Knowledge Alignment", "Domain adaptation", "Domain Shift Tuning", "Lottery Hedge Fund Hypothesis", "PEFT", "MoE"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2307edf37ac17efa56f1a3003c1339f7e11eea8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Dynamic Hierarchical Knowledge Routing (DHKR), a parameter-efficient fine-tuning framework that extends Knowledge-Aligned Domain Shift Tuning (KADA) by enabling dynamic subnetwork growth for domain adaptation. DHKR uses a two-level hierarchical routing mechanism that expands subnetworks on demand. Experiments on two instruction and QA benchmarks show that DHKR improves adaptability and stability over KADA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces several relevant components, including dynamic subnetwork growth, hierarchical routing, and routing stabilization mechanisms, to effectively enhance the flexibility and efficiency of the original KADA method.\n\n- The runtime analysis demonstrates improved GPU utilization and lower per-iteration runtime, even when training a substantially larger set of parameters, which highlights the method’s computational efficiency."}, "weaknesses": {"value": "- The empirical results (e.g., Tables 1 & 2) are relatively weak, as the proposed method achieves only comparable or worse performance than existing baselines. The rationale for using certain metrics, particularly ROUGE, METEOR, and calibration-based measures, is not clearly justified.\n\n- The evaluation scope is limited to only two datasets; broader testing on more diverse and challenging benchmarks (e.g., reasoning tasks) is necessary to substantiate the method’s effectiveness.\n\n- Additional ablation studies are needed to quantify the individual impact of each proposed component (dynamic growth, hierarchical routing, and stability control).\n\n- The related work discussion is underdeveloped and should be expanded to better connect this work to prior studies, especially the extensive literature on routing and expert selection methods.\n\n- Presentation quality can be improved: some references are missing or incorrectly formatted, and result tables (e.g., Tables 1 & 2) could be reformatted or reorganized to emphasize key findings more clearly."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uBC9HbuwEP", "forum": "vOYC3RXxo5", "replyto": "vOYC3RXxo5", "signatures": ["ICLR.cc/2026/Conference/Submission12102/Reviewer_RPJc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12102/Reviewer_RPJc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879985912, "cdate": 1761879985912, "tmdate": 1762923070245, "mdate": 1762923070245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new framework to improve domain adaptation for large language models, building on parameter-efficient fine-tuning (PEFT) methods such as LoRA and BitFit. This work presents a method, named DHKR, that allows models to grow dynamically and route knowledge hierarchically using PEFT adapters.\n- DHKR extends a previous framework KADA, through a two-level subnetwork growth mechanism inspired by the mixture-of-experts architectures. The system organizes subnetworks hierarchically: the first layer handles domain specialization, and the second layer captures modality-specific nuances. This structure allows new subnetworks to be added when a performance bottleneck is detected. Another knowledge steering layer (KSL) operates on top of a frozen, quantized LLM, combining its outputs with the base representation. \n- To stabilize training and prevent catastrophic forgetting, DHKR employs a training objective that combines cross-entropy loss with regularization terms that prevent overgrowth using a Minimum Description Length penalty. Subnetworks grow progressively, initialized through Net2WiderNet duplication with Gaussian perturbations to maintain diversity.\n\nThe experiments evaluated DHKR on the Meta-Llama-3-8B-Instruct model using Alpaca and OpenBookQA datasets, comparing KADA with other parameter-efficient fine-tuning methods, including QLoRA, AdaLoRA, and BitFit. DHKR began with a single subnetwork and expanded dynamically as needed. Results showed that while DHKR’s initial performance was slightly lower due to gradual growth, it achieved highly stable accuracy, outperforming KADA. Despite having more trainable parameters, DHKR trained faster per iteration than other methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- DHKR achieves a lower Expected Calibration Error compared to KADA (0.02 vs. 0.13), showing that its predictions remain well-calibrated across domains.\n\n\n- Despite having more trainable parameters, DHKR trains 2–3× faster per iteration than AdaLoRA and BitFit. \n\n\n- Its dynamic, two-level subnetwork expansion allows the model to grow capacity."}, "weaknesses": {"value": "- The problem statement is not clearly defined. It would be better to articulate a precise problem that DHKR is meant to solve, such as inefficiency, instability, or failure mode in KADA. \n- While the authors claim that DHKR trains faster than other PEFT methods despite having more trainable parameters, it is unclear whether this improvement is from algorithmic innovations or from system-level optimizations, like kernel fusion and reduced precision casting. Therefore, it is hard to understand the contribution of this paper. \n- The accuracy of the proposed method is roughly on par with (or below) KADA, depending on epoch. The experiments do not report confidence intervals or significance, making it hard to judge the robustness of the trade-offs."}, "questions": {"value": "- Were results averaged across multiple runs or random seeds, and are the reported differences statistically significant?\n- Can the authors provide insight into what knowledge each hierarchical subnetwork learns as the model grows?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r3ruV0tc6c", "forum": "vOYC3RXxo5", "replyto": "vOYC3RXxo5", "signatures": ["ICLR.cc/2026/Conference/Submission12102/Reviewer_m1iw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12102/Reviewer_m1iw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937863665, "cdate": 1761937863665, "tmdate": 1762923069205, "mdate": 1762923069205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key limitation in the PEFT method, KADA. The authors identify that KADA's reliance on a static, predefined set of subnetworks limits its ability to adapt to unseen domains. To overcome this, they propose Dynamic Hierarchical Knowledge Routing (DHKR), a novel framework inspired by Mixture-of-Experts (MoE) routing principles. DHKR extends KADA by introducing a two-level subnetwork growth mechanism that dynamically expands both high-level subnetworks (K) and their second-level components (L) on demand. The authors claim this transforms KADA's static mechanism into a self-adaptive, robust system."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The core contribution is the design of a dynamic, hierarchical PEFT framework. This directly addresses a clear and important limitation in the prior KADA method and offers a more flexible approach to knowledge-aligned domain adaptation.\n- The methodological design is quite principled.  The authors have incorporated sophisticated stability mechanisms, namely the composite growth trigger (monitoring stagnation, entropy, imbalance, etc.) and the multi-level Loss-Free Balancing.\n- The detailed experiments look good."}, "weaknesses": {"value": "- The main weakness of DHKR, in my opinion, is its complexity (both hyperparameters and overhead). Regarding that, I had a few questions: How difficult is it to tune this system? The effectiveness seems highly dependent on getting the thresholds for the composite trigger correct. Furthermore, could the authors please quantify the training and inference overhead (e.g., FLOPs, latency, or wall-clock time) of DHKR? How does it compare to the static KADA-5 baseline and QLoRA?\n- The main KADA baseline is fixed at K=5 subnetworks. This might not be a fair comparison. It is unclear if DHKR's advantage comes from its dynamism or simply from its ability to grow to a larger total capacity. How does DHKR's performance and final subnetwork count (K, L) compare to a stronger static baseline, such as KADA with K=10 or K=15?\n- The composite growth trigger is a central contribution, but its components are not ablated. It would be valuable to see an ablation study that justifies this complex design. For example, what is the performance if growth is triggered only by stagnation or only by usage imbalance?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J8HVGlENpD", "forum": "vOYC3RXxo5", "replyto": "vOYC3RXxo5", "signatures": ["ICLR.cc/2026/Conference/Submission12102/Reviewer_gLpJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12102/Reviewer_gLpJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976065485, "cdate": 1761976065485, "tmdate": 1762923068586, "mdate": 1762923068586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}