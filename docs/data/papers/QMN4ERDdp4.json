{"id": "QMN4ERDdp4", "number": 5402, "cdate": 1757907417995, "mdate": 1759897977751, "content": {"title": "QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models", "abstract": "The demand for efficient deployment of large language models (LLMs) has driven interest in quantization, which reduces inference cost, and parameter-efficient fine-tuning (PEFT), which lowers training overhead. This motivated the development of quantization-aware PEFT to produce accurate yet efficient quantized models.\nIn this setting, reducing quantization error prior to fine-tuning is crucial for achieving high model accuracy. However, existing methods that rely on low-rank adaptation suffer from limited representational capacity. Recent Fourier-related transform (FT)-based adapters offer greater representational power than low-rank adapters, but their direct integration into quantized models often results in ineffective error reduction and increased computational overhead.  \nTo overcome these limitations, we propose QWHA, a method that integrates FT-based adapters into quantized models by employing the Walsh-Hadamard Transform (WHT) as the transform kernel, together with a novel adapter initialization scheme incorporating adaptive parameter selection and value refinement. We demonstrate that QWHA effectively mitigates quantization errors while facilitating fine-tuning, and that its design substantially reduces computational cost. Experimental results show that QWHA consistently outperforms baselines in low-bit quantization accuracy and achieves significant training speedups over existing FT-based adapters.", "tldr": "We propose the first quantization-aware sparse adapter using the Walsh-Hadamard Transform with an initialization strategy that reduces quantization error and facilitates fine-tuning, outperforming low-rank and conventional FT-based adapters.", "keywords": ["quantization", "parameter efficient fine-tuning", "sparse adapter", "walsh-hadamard transform"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38517d8f1d380292b00e6d2c4341cb90b6bb2e38.pdf", "supplementary_material": "/attachment/7683e2f3ad2c8bd03c3e172016cff8c5e121d7b9.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces QWHA, a quantization-aware PEFT framework that integrates a Walsh–Hadamard Transform (WHT)-based adapter (WHA) with a quantization-aware initialization scheme. Unlike existing LoRA-based QA-PEFT methods that rely on low-rank updates or Fourier-transform (DCT/DHT) adapters, the proposed WHA uses a single real-valued WHT kernel with +-1 entries, reducing computation while maintaining high representational capacity. The initialization combines channel-wise parameter allocation (AdaAlloc) and value refinement to minimize layer output error caused by quantization. Experiments on LLaMA and Mistral models across 2–4 bit quantization demonstrate improved accuracy and faster training compared to LoRA and other FT-based baselines."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Addresses a well-defined and practically relevant gap, integrating high-capacity transform adapters into quantized fine-tuning.\n * The WHT choice is analytically and empirically grounded, showing improved energy concentration and better outlier reconstruction compared to sinusoidal transforms.\n * The decomposition into adaptive allocation and refinement provides a tractable solution to an otherwise NP-hard sparse approximation problem.\n * Evaluations across multiple model families and quantization levels are comprehensive, with consistent gains in accuracy and efficiency.\n * Single-transform WHT substantially lowers compute cost, supported by observational runtime and memory measurements."}, "weaknesses": {"value": "* Fine-tuning is limited to Alpaca (instruction-following) and general QA benchmarks; transferability to larger or domain-specific datasets remains unclear.\n * Only GPTQ+MagR quantization is used; robustness to other quantization backends is not shown.\n * While the paper reports overall training runtimes, SVD-based Hessian calibration step may still be costly for larger models and may need a breakdown for the initialization pipeline and its scaling with model size."}, "questions": {"value": "* How sensitive is the allocation to the temperature $t$ and to very small budgets (e.g., $P \\left( r \\right) \\leq 8$))? Any observed failure modes where some channels starve despite the per‑channel budgeting?\n * This is related to one of the weaknesses: can the proposed quantization-aware initialization generalize to other transform bases (e.g., random orthogonal, wavelet) or to rotation-based quantization schemes (e.g., AWQ, QuIP#)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DBLBRMnxCX", "forum": "QMN4ERDdp4", "replyto": "QMN4ERDdp4", "signatures": ["ICLR.cc/2026/Conference/Submission5402/Reviewer_oADz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5402/Reviewer_oADz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887775454, "cdate": 1761887775454, "tmdate": 1762918039810, "mdate": 1762918039810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces QWHA, a novel Quantization-Aware Parameter-Efficient Fine-Tuning (QA-PEFT) method. QWHA utilizes a Walsh-Hadamard Transform (WHT)-based adapter, termed WHA, designed to effectively mitigate quantization errors in Large Language Models (LLMs). The core contributions are a computationally efficient adapter design using WHT, and a sophisticated initialization scheme that combines adaptive parameter allocation (AdaAlloc) to preserve rank and target high-error channels, with a value refinement step to minimize initial error. Experimental results on LLaMA and Mistral models demonstrate that QWHA outperforms existing QA-PEFT methods in low-bit settings (2, 3, and 4-bit) and is significantly faster to train than other Fourier-related transform (FT)-based adapters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel and Well-Motivated Adapter Design (WHA)**: A key strength of this work is the innovative use of the Walsh-Hadamard Transform (WHT) for the adapter, and what makes this choice particularly strong is its tight alignment with the inherent nature of quantization errors. From Section 3.1, we can see that the authors compellingly argue WHT’s square-wave basis functions are far better suited to capturing the abrupt, outlier-driven characteristics of quantization errors than the smooth sinusoidal bases of DCT/DHT employed in prior Fourier-based adapters—and this claim is not just theoretical, as demonstrated in Figure 2(b)’s energy concentration analysis, which clearly validates the superiority of WHT in this context. Additionally, the decision to adopt a single transform (instead of the dual transforms used in earlier work) is both theoretically sound and practically impactful: Section 3.1’s discussion of quantization error’s channel independence justifies the simplification, while Table 13 further shows that this design delivers significant training speedups without compromising performance, making the method much more viable for real-world applications.\n\n- **Effective Quantization-Aware Initialization Strategy**: The proposed two-stage initialization scheme stands out as a notable strength, with its effectiveness evident across Section 3.2, Algorithm 1, Figure 4, Table 2, and Figure 5. AdaAlloc (Eq. 7) offers a principled heuristic that skillfully balances two critical objectives: reducing quantization error by prioritizing high-error channels and preserving the model’s fine-tuning capacity by maintaining a high-rank adapter structure—as illustrated in Figure 4. This is a clear advancement over simplistic magnitude-based or random parameter selection strategies. Complementing this, the subsequent Refinement step (Eq. 9) significantly reduces initial layer output error, as we can see from Table 2 and Figure 5, providing a far stronger starting point for fine-tuning than conventional initialization approaches.\n\n- **Strong and Consistent Empirical Performance**: The method’s performance advantages over existing baselines prove to be robust and reliable, as Table 3, Table 4, and Figure 6 consistently show. Across multiple models (LLaMA-3.1-8B, LLaMA-3.2-3B, Mistral-7B-v0.3) and different bit-widths, QWHA demonstrates clear and consistent superiority. Most impressively, in the highly challenging 2-bit quantization setting, as seen in Table 3, the method achieves a 2-4% absolute improvement on CSQA and GSM8k tasks compared to the next-best baseline—highlighting its exceptional effectiveness in extreme compression scenarios. Figure 6 further reveals that QWHA outperforms CLoQ even at its peak performance, despite using a smaller parameter budget (r=32), which underscores that the proposed adapter’s strength stems from superior representational capacity rather than mere efficient parameter allocation."}, "weaknesses": {"value": "- **Insufficient Hyperparameter Sensitivity Analysis**: From Section 4, Table 8, Table 9, and Equation (7), it’s clear the study relies heavily on several critical hyperparameters without exploring their sensitivity—a notable oversight for a method emphasizing practicality. Notably, the AdaAlloc temperature `t` is fixed at 1 with no ablation to show how adjusting it might alter the parameter allocation strategy (e.g., making it more or less focused on high-error channels). Even more concerning, the adapter scaling factor `α` is set to 4000 for QWHA versus just 1 for CLoQ (per Table 8)—a massive discrepancy that goes entirely unjustified. This raises questions about whether the performance gains stem from the method itself or simply favorable hyperparameter tuning. Additionally, the learning rates in Table 9 appear to be heavily tailored to each experimental setting, which casts doubt on how easily QWHA can be applied to new models or tasks without extensive retuning.\n\n- **Limited Scope of Experimental Evaluation**: The experimental evaluation, as outlined in Section 4’s “Models and Datasets” section, is overly narrow, which undermines the claim of QWHA’s generalizability. Currently, all results are based on instruction tuning with the Alpaca dataset, followed by evaluation on just two tasks: commonsense QA (CSQA) and arithmetic reasoning (GSM8k). While these are relevant benchmarks, they do not capture the full range of use cases for quantized LLMs. It would greatly strengthen the paper to include results on more diverse tasks—such as text summarization (e.g., XSum), code generation (e.g., HumanEval), or long-context understanding—to demonstrate that QWHA’s benefits extend beyond reasoning-focused tasks. Without this, it’s hard to assess whether the method is broadly applicable or limited to specific task types.\n\n- **Lack of Statistical Rigor in Results**: Looking at Tables 3, 4, 10, and 11, it appears all reported accuracy scores are from single runs—an issue that contradicts standard practices in top-tier ML conferences. Fine-tuning LLMs is known to exhibit significant variability based on random seeds, and without reporting mean values and standard deviations across multiple runs, it’s impossible to determine if the reported performance gains are statistically significant or merely due to chance. This lack of rigor makes it difficult to trust the consistency of QWHA’s results, especially in the closely contested low-bit settings where small differences can tip the balance between methods.\n\n- **Inconsistent or Unfair Baseline Comparisons**: From Table 3 and Section 2.3, it’s apparent that the primary performance comparison is somewhat inconsistent. QWHA, which benefits from a dedicated quantization-aware initialization scheme, is pitted against other Fourier-based adapters (LoCA, SSH) that lack this critical component. While the paper acknowledges this mismatch, it still frames this as the main comparison—creating a potentially unfair advantage for QWHA. The more apples-to-apples comparison in Table 4 (where AdaAlloc is applied to all adapter types) paints a more nuanced picture: DCA + AdaAlloc and DHA + AdaAlloc often perform competitively with WHA, especially in the 2-bit LLaMA-3.2-3B GSM8k setting. This complicates the narrative that WHA’s transform kernel is the key differentiator, suggesting the initialization scheme may play a more dominant role than claimed.\n\n- **Weaknesses in Mathematical Formulation and Notation**: Several clarity issues in the mathematical presentation, visible in Algorithm 1, Appendix B.2, and Appendix C.1, hinder reproducibility and understanding. In Algorithm 1, the notation `(p_0, ..., p_d_out) ∈ N_d_out` is non-standard; using `(p_0, ..., p_{d_{out}-1}) ∈ ℕ^{d_{out}}` would make the dimensioning far clearer for readers. Additionally, the justification for WHA’s full-rank property in Appendix B.2 relies on theory for *random* sparse matrices [Coja-Oghlan et al., 2020], but AdaAlloc is a deterministic parameter allocation scheme—creating a tenuous link between the theoretical support and the actual method. While the derivation in Appendix C.1 is mathematically correct, it could be more self-contained by explicitly defining `U` and `Σ` as components of the SVD of `XXᵀ`, rather than assuming readers will infer this.\n\n- **Limited Ablation on Core Method Components**: The paper’s ablation studies, while present, fail to address key questions about its core design choices—something evident from Figure 5, Table 7, and Table 13. For example, the Refinement step’s impact is only measured on *initialization error* (per Figure 5 and Table 7), not on final *task accuracy*. An ablation showing how “WHA + AdaAlloc” performs without Refinement would be critical to understanding whether this step adds meaningful value beyond reducing early-stage error. Similarly, Table 13 compares training times for 1D versus 2D transforms but provides no corresponding accuracy data. Since the use of a single transform is a central design choice, empirical proof that a 2D transform offers no accuracy benefit (not just speed drawbacks) is necessary to justify this decision.\n\n- **Generalization to Other Quantization Schemes**: Despite claiming QWHA is compatible with any quantization scheme, the paper only presents results using GPTQ [Frantar et al., 2023], as seen in Section 4’s “Baselines” section. This gap between claim and evidence weakens the argument for QWHA’s versatility. Quantization methods vary significantly in how they handle outliers, weight distribution, and compression logic—for example, AWQ [Lin et al., 2024] uses activation-aware weighting, while RTN relies on simple rounding. Demonstrating QWHA’s effectiveness with at least one other popular scheme would go a long way toward proving its generalizability, rather than limiting it to a single quantization pipeline.\n\n- **Clarity on Parameter Budget (`r`)**: From Sections 3 and 4, it’s apparent that the paper uses the LoRA-specific term “rank (`r`)” to define the parameter budget for *all* adapters (e.g., “P(r=64)”)—a choice that creates unnecessary confusion. For Fourier-based adapters like WHA, `r` does not have a direct “rank” interpretation, as their structure differs fundamentally from LoRA’s low-rank matrices. Using a more generic term (e.g., “budget size” or “trainable parameter count”) and explicitly reporting the total number of trainable parameters for each method would greatly improve clarity, especially for readers comparing QWHA to non-LoRA baselines.\n\n- **Inference Overhead Analysis is Missing**: Unfortunately, there is no discussion of inference overhead anywhere in the manuscript—an odd omission for a paper that emphasizes efficiency. While the authors likely intend to merge the adapter weights `ΔW` into the frozen quantized weights `W_Q` for zero runtime overhead, the one-time cost of computing `ΔW = F H⁻¹` (including the inverse WHT and sparse-dense matrix multiplication) is never addressed. Additionally, the memory footprint of the sparse `F` matrix compared to LoRA’s compact `A` and `B` matrices is left unmentioned. To complete the efficiency narrative, a brief analysis of these inference-related costs would be essential for practitioners deciding whether to adopt QWHA."}, "questions": {"value": "1. Regarding the adapter scaling factor `α` (Table 8), there is a massive discrepancy between QWHA (α=4000) and CLoQ (α=1)—could you elaborate on why this choice was made? Was `α` tuned independently for each method, and if so, have you investigated how such a large value impacts gradient norms, training stability, or convergence speed for QWHA?\n\n2. The main comparison in Table 3 pairs QWHA (with its dedicated quantization-aware initialization) against other Fourier-based methods that lack this feature, while Table 4 provides a more controlled comparison by applying AdaAlloc to all adapter types. In Table 4, `DCA + AdaAlloc` and `DHA + AdaAlloc` perform quite competitively with WHA—particularly in the 2-bit LLaMA-3.2-3B GSM8k setting. Could you comment on this? Does this suggest that the initialization scheme (AdaAlloc) is the dominant driver of performance, rather than the specific choice of transform kernel (WHT)?\n\n3. The justification for using a single transform (instead of dual transforms) is compelling from an efficiency standpoint, and Table 13 clearly shows the training time benefits. However, to fully validate this design choice, could you provide an accuracy comparison between 1D WHA and a hypothetical 2D WHA? This would empirically confirm that the second transform offers no meaningful accuracy gain in the QA-PEFT context, rather than just faster training.\n\n4. The AdaAlloc method uses a fixed temperature parameter `t=1` (Section 4), but there is no sensitivity analysis provided. Have you tested how varying `t` (e.g., t=0.5, t=2) affects parameter allocation sharpness and final performance? For instance, does a lower `t` lead to over-concentration on a small number of channels, or a higher `t` reduce effectiveness by spreading parameters too evenly?\n\n5. As noted earlier, the reported results lack error bars or confidence intervals from multiple runs. Given the known variance in LLM fine-tuning due to random seeds, could you comment on the stability of QWHA’s performance across different seeds? Have you observed consistent gains, or is there significant variability that might affect the reliability of the reported results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jGX8Tg5ToI", "forum": "QMN4ERDdp4", "replyto": "QMN4ERDdp4", "signatures": ["ICLR.cc/2026/Conference/Submission5402/Reviewer_L6oQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5402/Reviewer_L6oQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923337466, "cdate": 1761923337466, "tmdate": 1762918039475, "mdate": 1762918039475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents QWHA (Quantization-Aware Walsh-Hadamard Adaptation), a novel approach for enhancing fine-tuning efficiency in LLMs under low-bit quantization settings. By integrating Walsh-Hadamard transforms within the quantization-aware training framework, the authors aim to mitigate quantization-related errors, thereby improving model performance without substantial computational overhead. The experiments conducted on several benchmarks substantiate the proposed method's effectiveness, indicating its potential to advance the state-of-the-art in efficient model adaptation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The introduction of a Walsh-Hadamard Transform (WHT) presents a robust method for mitigating quantization error, enhancing the representational capacity of the model while maintaining computational efficiency.\n2. The authors provide thorough experimental validation using different models and a range of quantization levels, supporting the claims of improved performance and efficiency."}, "weaknesses": {"value": "1. The experimental evaluation relies heavily on the Alpaca dataset, primarily focusing on instruction-following tasks. This raises questions about the generalizability of the approach to other domains or more complex datasets, such as those involving natural language understanding or generation tasks.\n2. The dependency on specific hyperparameter configurations (e.g., the temperature parameter in AdaAlloc) is only marginally discussed. There is a lack of sensitivity analyses that would indicate how variations in hyperparameters impact model performance and error rates."}, "questions": {"value": "1. How does QWHA perform on diverse datasets that extend beyond instruction-following tasks? Can you provide insights into its performance on datasets related to summarization or question-answering?\n2. Could the authors elaborate on the decision-making process for hyperparameter settings, particularly the temperature parameter in AdaAlloc? Have you explored the performance implications of varying these settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7xLILZAYiH", "forum": "QMN4ERDdp4", "replyto": "QMN4ERDdp4", "signatures": ["ICLR.cc/2026/Conference/Submission5402/Reviewer_RPse"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5402/Reviewer_RPse"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762261263578, "cdate": 1762261263578, "tmdate": 1762918039190, "mdate": 1762918039190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of deploying Large Language Models (LLMs) efficiently by integrating parameter-efficient fine-tuning (PEFT) with low-bit quantization (QA-PEFT). The authors identify two key limitations in current methods: (1) the limited representational capacity of LoRA-based adapters and (2) the high computational cost and lack of quantization awareness in existing Fourier-related Transform (FT) based adapters. To overcome this, the paper proposes QWHA (Quantization-Aware Walsh-Hadamard Adaptation). This method introduces two main contributions: (1) WHA, a novel adapter that uses a single, computationally efficient Walsh-Hadamard Transform (WHT) kernel, which the authors show is full-rank and effective at capturing outlier-induced quantization errors. (2) A novel quantization-aware initialization scheme, featuring \"AdaAlloc\" for adaptive parameter selection and \"Value Refinement\" to optimize initial values, which is the first (to our knowledge) such scheme for FT-based adapters. Experiments show that QWHA outperforms existing QA-PEFT baselines, particularly in 2-bit and 3-bit settings, while achieving training speeds comparable to LoRA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality & Significance: The problem is well-motivated, sitting at the practical intersection of model compression and adaptation. The paper's primary originality lies in being the first to successfully design a quantization-aware initialization strategy specifically for an FT-based adapter. This bridges a critical gap, combining the superior representational power of FT-adapters with the error-mitigation necessary for low-bit quantization, which LoRA-based methods struggle with.\n\n- Quality: The methodology is concrete and well-supported by a diverse set of analyses. The design choices for WHA (using WHT and a single transform) are justified through clear empirical evidence, including rank analysis, cumulative energy analysis, and outlier component coverage. The two-stage initialization algorithm (AdaAlloc and Refinement) is a non-trivial and well-reasoned approach to solving the complex sparse approximation problem.\n\n- Clarity: The paper is clearly written, systematically identifying the limitations of prior work and presenting its solution. The figures and tables effectively support the authors' claims about why WHA is a suitable kernel and why the proposed initialization is effective."}, "weaknesses": {"value": "- Unclear Ablation of Performance Gains: The main experimental comparison in Table 3 is potentially confounding. QWHA, which includes the proposed quantization-aware initialization (QA Init.), is compared against other FT-based adapters (LOCA, SSH) that do not have this initialization. It is unclear whether QWHA wins because: (A) the WHA kernel is intrinsically superior to DCA/DHA for this task, or (B) WHA is the only FT-based method benefiting from a powerful initialization. A fair comparison would require applying a comparable initialization to LOCA/SSH or ablating QWHA without its initialization.\n\n- Missing Analysis of Initialization Cost: The paper highlights QWHA's efficiency by comparing its training time to CLoQ (Table 5), showing they are similar. However, this ignores the pre-computation cost of the QUANTIZATION-AWARE ADAPTER INITIALIZATION (Algorithm 1). This process involves gathering activations, SVD, and, most notably, solving a least-squares problem (with a matrix inverse) for every output channel of every adapted layer. This complex procedure is computationally non-trivial, whereas LoRA's initialization is virtually instantaneous. Without reporting this initialization time, the claim of \"efficiency\" is incomplete, as the total time-to-model (Initialization + Training) may be significantly higher for QWHA.\n\n- Significant Performance Gap on Reasoning Tasks: While QWHA shows strong performance on CSQA, there is a very large performance degradation on the GSM8k benchmark (Table 3) compared to the 16-bit fine-tuned baseline. For instance, on LLaMA-3.1-8B, the 16-bit baseline achieves 59.74, while the 4-bit QWHA only reaches 56.10. This substantial drop is not fully addressed and may suggest that the proposed adaptation and error-compensation scheme struggles to preserve the complex reasoning capabilities of the model. Further exploration on other reasoning tasks (e.g., AIME, MATH) would be needed to verify if this is a fundamental limitation of the approach."}, "questions": {"value": "- To fairly assess the novelty of the WHA kernel itself, could the authors provide an ablation study comparing WHA, DCA (LOCA), and DHA (SSH) under two conditions: (a) all using the same proposed QA initialization (AdaAlloc + Refinement) and (b) all using a simple baseline initialization (e.g., random selection, zero-init)? This would isolate the true benefit of the WHT kernel versus the initialization scheme.\n\n- Could the authors please report the wall-clock time and memory overhead required for the QUANTIZATION-AWARE ADAPTER INITIALIZATION phase? How does the total deployment time (Initialization Time + Fine-Tuning Time) of QWHA compare to a method like CLoQ?\n\n- Do the authors have an explanation for the significant performance gap between the 16-bit fine-tuned model and QWHA on the GSM8k reasoning task? Does this imply a potential limitation of the WHA adapter in capturing parameters crucial for mathematical reasoning, or is it a general challenge for low-bit QA-PEFT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uhldyoko1Z", "forum": "QMN4ERDdp4", "replyto": "QMN4ERDdp4", "signatures": ["ICLR.cc/2026/Conference/Submission5402/Reviewer_c9iU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5402/Reviewer_c9iU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762633331227, "cdate": 1762633331227, "tmdate": 1762918038848, "mdate": 1762918038848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}