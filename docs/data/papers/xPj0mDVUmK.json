{"id": "xPj0mDVUmK", "number": 8516, "cdate": 1758089026403, "mdate": 1759897778929, "content": {"title": "SHADE: Spectral Hallucination Detection via Dual Spectral Decompositions", "abstract": "Large language models often produce confident but unsupported statements.\nDetecting such hallucinations from internal signals is essential for reliable\nsystems. Existing attention-based methods either summarize weights with local\nstatistics or adopt Laplacians (e.g., \\(D_{\\text{out}}-A\\)) whose guarantees and\napplicability break outside strictly causal, square attention. We seek a single,\nrigorous framework that scales across architectures and attention types while\nyielding interpretable indicators of grounding. We introduce \\textsc{SHADE}, a\nunified spectral approach that models attention with two standard operators: (i)\na random-walk operator \\(L_{\\mathrm{rw}}=I-A\\) that quantifies\ndiffusion/leakiness, and (ii) a degree-normalized cross-operator\n\\(M=D_Q^{-1/2}AD_K^{-1/2}\\) that quantifies query--key coupling. The resulting\nfeatures are mathematically rigorous and physically interpretable, with clear\noperator semantics that map to failure modes associated with hallucination:\n\\emph{diffusion} (from \\(L_{\\mathrm{rw}}\\)) quantifies probability leakiness;\n\\emph{conductance} (via a symmetric/PSD Laplacian, e.g., Chung's) captures\nexpansion/connectedness; and \\emph{energy/alignment strength} (from the SVD of\n\\(M\\)) quantify total coupling and the dominance or fragmentation of coupling\nmodes. The formulation applies unchanged to encoder, decoder, and\nencoder--decoder settings, including rectangular cross-attention and masked\nsub-blocks. Evaluated on GPT-2, FLAN-T5, and Phi-2 across HaluEval and\nTruthfulQA, \\textsc{SHADE} consistently surpasses token-probability and\nLapEigvals-style baselines, delivering strong discrimination and calibration\nalongside interpretable spectra. By grounding hallucination detection in\nstandard spectral operators with physically meaningful interpretations,\n\\textsc{SHADE} offers an explanatory basis for \\emph{where} and \\emph{how}\nhallucinations originate. This suggests two practical avenues: (i) training-time\nregularization to suppress emergent hallucinatory patterns, and (ii) a\ndeployable \\emph{hallucination risk score} with mode-level rationales (by\nlayer/head and prompt span) that end users and developers can act on.", "tldr": "We present a general spectral framework for hallucination detection that transcends architectural boundaries, unifying directed and bipartite spectral analyses to expose complementary attention failures in any transformer configuration.", "keywords": ["hallucination detection", "spectral graph analysis", "alignment", "AI safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aed6f4f0bbe1932185fd27131d94033cdabdc43b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SHADE, a novel framework for detecting hallucinations in Large Language Models by analyzing the internal structure of their attention mechanisms. The method uniquely employs a dual-view approach to diagnose distinct failure modes.\n\nThe first view analyzes the information flow within the attention network, identifying when the model detaches from the source prompt and begins to recycle its own generated content. The second view assesses the query-key coupling, measuring the alignment strength between what the model is looking for and the information it finds. This detects brittle connections, such as when attention collapses onto a few non-informative tokens.\n\nThe authors demonstrate that these two perspectives are complementary and capture different types of structural failures. Through extensive experiments on various models and benchmarks, SHADE is shown to significantly outperform previous detection methods. The framework is notable for being theoretically grounded, applicable across diverse model architectures, and providing interpretable signals for why a model is hallucinating."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses a Timely and Critical Problem: The authors tackle the problem of hallucination in Large Language Models, which remains a significant and pressing challenge for the field. The work's focus on developing principled, internal methods for detecting such failures is highly relevant and contributes to a crucial line of research aimed at improving model safety and reliability.\n\n2. Conceptually Interesting Premise: The core intuition to diagnose attention failures by modeling the mechanism from two distinct structural viewpoints—information flow and query-key coupling—is conceptually novel and interesting. This dual-perspective approach offers a potentially more holistic way to interpret the internal state of the model compared to methods that rely on a single analytical lens."}, "weaknesses": {"value": "1. The core weakness is the inherent ambiguity of its spectral features, which are incapable of distinguishing between attention patterns essential for correct reasoning and pathological ones that lead to hallucinations. For instance, a strong, necessary focus on a key entity to answer a factual question produces the same \"over-concentration\" signal that the method flags as high-risk. Similarly, a simple and correct copying mechanism, often used in summarization, results in a \"rank collapse\", a pattern the method incorrectly associates with pathological over-fitting. Because these features are fundamentally context-blind—describing only the geometric shape of the attention distribution rather than its semantic appropriateness for the task—they cannot serve as reliable, standalone indicators of hallucination.\n\n2. A significant number of hallucinations emerge dynamically over the course of generation. Therefore, for complex outputs, a methodology that relies exclusively on a static analysis of the final attention distribution is theoretically insufficient for accurate detection. Consider a \"snowball effect,\" where an initial, single-step hallucination leads to a cascade of subsequent errors. Given the final length of the text, the original trigger for this failure mode would likely be obscured and thus undetectable by a post-hoc static analysis. This scenario is particularly critical as it more closely mirrors real-world failure cases."}, "questions": {"value": "1. I am skeptical about the practical viability of using attention features as a primary mechanism for hallucination detection. My main concern stems from the significant computational overhead required to access and process attention distributions during the inference phase. For each token generated, this method necessitates retrieving the attention scores from multiple layers and heads, which is a computationally expensive operation. This introduces non-trivial latency and reduces throughput, especially when compared to standard, optimized inference processes that do not require inspecting these intermediate states. Does the potential gain in detection accuracy justify the substantial performance cost? In many application scenarios, such as real-time chatbots or large-scale content generation, inference speed is a paramount constraint. Therefore, a method that significantly slows down generation may not be a feasible solution, regardless of its theoretical effectiveness.\n\n2. A reliance on the final static attention distribution for detection, without the capacity to integrate the temporal dynamics of attention features in long sequences, raises a critical challenge: how can such an approach detect the \"snowball effect\" originating from a single-step hallucination within a long text? This class of cascading hallucination represents a more prevalent and practical scenario, yet the proposed methodology appears to completely circumvent the handling of this realistic failure mode."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EqszMQ6fCQ", "forum": "xPj0mDVUmK", "replyto": "xPj0mDVUmK", "signatures": ["ICLR.cc/2026/Conference/Submission8516/Reviewer_g6mG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8516/Reviewer_g6mG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761628979011, "cdate": 1761628979011, "tmdate": 1762920381085, "mdate": 1762920381085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "If I'm understanding correctly- and this paper has a lot of (what I think is) unnecessary jargon, so this has been difficult- \n\nThe core idea this paper proposes for detecting hallucinations is essentially to use the transformer's attention matrices to compute spectral statistics (eigenvalues, singular values, entropies, etc.) and then feed these handcrafted feature numbers into logistic regression to classify whether the output is a hallucination or not. \n\nThey test a series of models like GPT2/Phi2 on HaluEval and TruthfulQA (which are quite different datasets, given that HaluEval is looking at in-context grounded hallucination while TruthfulQA is moreso factuality without checking against a grounding source), and compare their handcrafted feature logistic regression model against token probability detectors and LapEigvals (a similar spectral baseline that uses a triangular Laplacian). TLDR, the authors method outperforms all them both, though the improvement is something like 1% absolute improvement against LapEigvals on most comparisons.\n\nThe core claim seems to be that simple logistic regression on spectral features from attention matrices detects hallucinations better than token-probability baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The empirical finding seems relatively solid- that SVD-based coupling features consistently outperform token-probability and older spectral baselines. This seems like a relatively lightweight pipeline."}, "weaknesses": {"value": "The main issue with this paper is that it tries to overcomplicate what is essentially a very simple approach. It buries this under heavy and unnecessary math, but it's really just simple linear algebra. \n\nThe baselines comparisons aren't exactly convincing- many, many hallucination detection methods have been proposed, like semantic entropy, self-consistency, embedding-based factuality, surface based classification, etc. Comparing against only 2 baselines with no justification as to why they were selected seems like cherry picking.\n\nFurthermore, there is little analysis on cross-modal or cross-dataset generalization. the logistic regression classifiers are trained per model and per dataset; how extensive this extends to other models and domains remains a question- it can be very convincingly argued that token-probability (uncertainty-based) metrics are much more generalizable across domains and models, and more lightweight to compute, than needing to train the authors' approach across every single model and domain."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UsaHb6CP2u", "forum": "xPj0mDVUmK", "replyto": "xPj0mDVUmK", "signatures": ["ICLR.cc/2026/Conference/Submission8516/Reviewer_WziU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8516/Reviewer_WziU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858046939, "cdate": 1761858046939, "tmdate": 1762920380569, "mdate": 1762920380569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SHADE (Spectral Hallucination Detection), a framework for detecting LLM hallucinations by analyzing the internal attention matrices. The authors critique existing methods like LapEigvals for having limited applicability (e.g., only causal, square attention) and lacking rigorous spectral guarantees. Paper's claim is a \"unified spectral approach\" built on a \"dual\" decomposition:\n1.  **Flow Analysis:** Using a random-walk operator ($L_{rw} = I - A$) to quantify \"diffusion\" or \"leakiness\" (i.e., information flow anomalies).\n2.  **Coupling Analysis:** Using a degree-normalized cross-operator ($M = D_Q^{-1/2} A D_K^{-1/2}$) and its SVD to quantify query-key \"coupling\" (i.e., alignment anomalies)."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper successfully identifies and validates a powerful, architecture-agnostic hallucination detector based on the SVD of the normalized cross-operator $M$. This method consistently achieves near-ceiling performance on HaluEval and strong performance on TruthfulQA.\n2. The paper provides a valuable service by thoroughly dismantling the LapEigvals baseline, pointing out its non-PSD nature, lack of spectral guarantees, and information collapse on the diagonal.\n3.  The SVD Coupling features show excellent discrimination (AUROC) and calibration across all models and datasets."}, "weaknesses": {"value": "1.  The paper's primary weakness is the bait-and-switch between the theoretically-motivated **random-walk Laplacian ($L_{rw} = I - A$)** in Section 3.2.1 and the experimentally-implemented **symmetric normalized Laplacian ($L_{sym}$)** in Section 4.1. The \"flow\" operator that was motivated at length is never tested.\n2.  Because of the weakness above, the central conceptual claim of a \"dual\" framework with \"complementary\" operators is unproven. The paper's own complementarity analysis (Proposition 3.1) is for $L_{rw}$ and $M$, which are not the operators compared in Table 1 or 5.\n3.  Even ignoring the operator-swap, the paper's *empirical* data contradicts its conclusion. The conclusion claims \"low RV coefficients\". However, Table 5 reports an RV coefficient of **0.944** for \"EIGEN-FEATURES\" vs. \"SVD\" on GPT-2/TruthfulQA. This indicates *near-perfect linear association* (i.e., redundancy), the exact opposite of complementarity.\n4.  The $L_{sym}$ operator that *was* tested (\"Eigen-Features\") performed very poorly, and in one case (GPT-2/HaluEval), was no better than chance (AUROC 0.492). This undermines the \"dual\" claim from an empirical standpoint as well."}, "questions": {"value": "1.  Why does Section 3.2.1 motivate the $L_{rw} = I - A$ operator, citing its specific properties, if the experiment in Section 4.1 uses the $L_{sym} = I - D^{-1/2}\\overline{A}D^{-1/2}$ operator for its \"Eigen-Features\"?\n2.  Given this disconnect, how can the paper validate its central \"dual framework\" thesis, since the \"flow\" operator from the theory was never empirically tested against the \"coupling\" operator?\n3.  How does the conclusion claim \"low RV coefficients\" when Table 5 clearly shows an RV coefficient of **0.944** for GPT-2 on TruthfulQA? This high value suggests the features are redundant, not complementary.\n4.  Given that the SVD-coupling path performs exceptionally well and the \"Eigen-Features\" path performs poorly (e.g., 0.492 AUROC) and is theoretically disconnected, wouldn't this paper be stronger if it were simply presented as a single, novel SVD-based method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j9VlxGLiLb", "forum": "xPj0mDVUmK", "replyto": "xPj0mDVUmK", "signatures": ["ICLR.cc/2026/Conference/Submission8516/Reviewer_21hh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8516/Reviewer_21hh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971983141, "cdate": 1761971983141, "tmdate": 1762920379984, "mdate": 1762920379984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SHADE, a universal illusion detection framework based on dual spectral decomposition: the attention \"diffusion/leakage\" is measured by the random walk operator, and the query-key coupling strength and pattern are measured by the SVD of the degree normalized crossover operator. The two operators respectively reveal information flow imbalance and evidence alignment anomaly. The features possess mathematical rigor and physical interpretability, and can be applied to self-attention, cross-attention, rectangular blocks, and various types of Transformers without modification. Experiments on HaluEval and TruthfulQA of GPT-2, Phi-2, and FLAN-T5 show that the SVD coupling characteristics of SHADE are consistently superior to the baselines such as token probability and LapEigvals, demonstrating both high discriminative power and calibration accuracy. It can provide interpretable risk scores at the layer/head/prompt fragment level, support regular expression suppression hallucinations during training or real-time alerts during deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This method provides a cross-architecture, interpretable, plug-and-play unified spectral operator. With just two lines of code, it can be integrated into any Transformer architecture and immediately output a clear physical layer-head-token-level illusion risk score without the need for additional training or external knowledge. Achieve detection performance close to the upper limit directly on the three major models and two benchmarks."}, "weaknesses": {"value": "- All experiments were only for correlation classification and did not prove through intervention (such as suppressing high $\\sigma_1$ or low leakage patterns) that \"once these spectral anomalies are corrected, the hallucination rate will decrease\". Therefore, it is impossible to establish that spectral characteristics are the cause of hallucinations\n\n- Not sufficient ablations. PCA retains the key hyper-parameters such as 85% variance, temperature scaling, and the selection of $\\tau$ with Youden J, and only provides the final values, without showing the sensitivity of the ROC curve to these hyperparameters. Nor has it completely dissolved the respective contributions to performance of \"using only $L_rw$\", \"using only $M$\", and \"combining the two\"."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JmPMy9kZqT", "forum": "xPj0mDVUmK", "replyto": "xPj0mDVUmK", "signatures": ["ICLR.cc/2026/Conference/Submission8516/Reviewer_B93w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8516/Reviewer_B93w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762100370615, "cdate": 1762100370615, "tmdate": 1762920379592, "mdate": 1762920379592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}