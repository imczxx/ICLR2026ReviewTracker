{"id": "9jCca0DsT5", "number": 3214, "cdate": 1757374471157, "mdate": 1759898101752, "content": {"title": "Unrolled Networks are Conditional Probability Flows in MRI Reconstruction", "abstract": "Magnetic Resonance Imaging (MRI) offers excellent soft-tissue contrast without ionizing radiation, but its long acquisition time limits clinical utility. Recent methods accelerate MRI by under-sampling k-space and reconstructing the resulting images using deep learning. Unrolled networks have been widely used for the reconstruction task due to their efficiency, but suffer from unstable training caused by unconstrained hyperparameters and lack of intermediate supervision. In contrast, flow ODEs based on conditional probability Ordinary Differential Equations offer theoretical stability in natural image tasks but are computationally expensive and lack MRI-specific formulations. In this work, we introduce flow ODEs to MRI reconstruction by theoretically proving that unrolled networks are discrete implementations of conditional probability flow ODEs. This connection provides explicit formulations for hyperparameters and clarifies how intermediate states should evolve. Building on this insight, we propose Flow-Aligned Training (FLAT), which derives unrolled hyperparameters from the ODE discretization and aligns intermediate reconstructions with the ideal ODE trajectory to improve stability and convergence. Experiments on three MRI datasets show that FLAT achieves high-quality reconstructions with up to 3x fewer iterations and significantly greater stability than unrolled and diffusion-based generative models.", "tldr": "", "keywords": ["Magnetic Resonance Imaging", "Unrolled Networks", "Conditional Probability Flow", "Ordinary Differential Equations"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a13c37dc547d4eb03435b7b72dc45eb56ab329d8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses accelerated MRI reconstruction from under-sampled k-space by establishing a theoretical equivalence between unrolled networks and conditional probability flow ODEs. It demonstrates that each cascade in an unrolled model corresponds to a forward-Euler step of a conditional probability flow, thereby imposing explicit constraints on the timestep schedule and model hyperparameters. This formulation provides a clear and useful theoretical perspective—viewing unrolled MRI reconstruction as a discretized conditional probability flow—which naturally leads to an ODE-grounded training scheme that is both simple to implement and empirically effective. Building upon this insight, the authors introduce Flow-Aligned Training (FLAT), which (i) enforces ODE-consistent timestep scheduling, (ii) fixes step sizes and weights through the ODE mapping, and (iii) introduces intermediate “velocity alignment” supervision via a composite loss. Experiments show that FLAT achieves higher or comparable PSNR and SSIM using only 12 steps, outperforming diffusion-based baselines that typically require 50–1000 iterations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- **Unified view of unrolled networks and probabilistic flow models.**  \n  The paper presents a significant conceptual advance by unifying *unrolled reconstruction networks* and *conditional probabilistic flow ODEs*. It formally shows that the cascaded updates in unrolled networks can be interpreted as **forward Euler discretizations** of conditional probability flows, thus providing a clear theoretical framework for understanding unrolled MRI reconstruction.  \n  This perspective addresses several long-standing issues in unrolled models, including:  \n  - Unstable or redundant reconstruction trajectories with unclear intermediate meanings.  \n  - Poor interpretability due to backpropagation-based hyperparameter tuning.  \n  \nBy redesigning the unrolled structure from a **probabilistic flow** perspective, the paper achieves better interpretability and stability while maintaining strong reconstruction performance. This work offers conceptual value not only to the unrolled MRI community but also to the broader field of deep iterative reconstruction, potentially inspiring more interpretable and theoretically grounded iterative architectures."}, "weaknesses": {"value": "- **Inadequate diffusion-based baseline comparison.**  \n  The comparison against diffusion-based methods is insufficient. The paper claims that FLAT outperforms traditional unrolled networks and is faster and better than diffusion-based approaches. However, the chosen baseline—**MC-DDPM**—is not representative or competitive. I strongly recommend including at least one **SOTA diffusion-based method**, such as **DDS [1]**, to substantiate the claimed advantages of FLAT over diffusion models.  \n\n\n> [1] Chung, Hyungjin, Suhyeon Lee, and Jong Chul Ye. **\"Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems.\"** *ICLR*, 2024.\n\n---\nWhile the proposed idea is novel and promising, the experimental section requires further strengthening. More robust comparisons and additional analyses (especially addressing the questions below) are necessary to convincingly validate the method. With these improvements, I would be very willing to reconsider my evaluation positively."}, "questions": {"value": "- **Line 226 clarification:** The statement “conditional flow ODE evolving from the under-sampled initialization $x_1 = y$ towards the fully-sampled $x_0$” is not precise. Since $y$ denotes under-sampled k-space measurements, the initialization should more appropriately be $x_1 = A^\\top y$.  \n\n- **Performance on fastMRI Knee dataset:**  \n  Table 1 shows that FLAT does not significantly outperform traditional unrolled networks on the fastMRI Knee dataset and even underperforms in some metrics. Could the authors clarify the reason for this performance gap?  \n\n- **Complex loss function design:**  \n  The proposed training involves a highly complex loss composition. Could the authors analyze the contribution of each component and whether all terms are necessary?  \n\n- **Ablation interpretation (Table 3):**  \n  From Table 3, removing the paper’s three main contributions theoretically reduces FLAT to an E2E-VarNet with the proposed complex loss. The results show that this “degraded” model still performs notably better than a standard E2E-VarNet, suggesting that the improvement may largely stem from the loss function itself. If so, would applying the same loss function to a vanilla E2E-VarNet yield even higher performance—perhaps surpassing FLAT—on the fastMRI Knee dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5BSbUxi3v8", "forum": "9jCca0DsT5", "replyto": "9jCca0DsT5", "signatures": ["ICLR.cc/2026/Conference/Submission3214/Reviewer_hR2V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3214/Reviewer_hR2V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861833933, "cdate": 1761861833933, "tmdate": 1762916603052, "mdate": 1762916603052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new method (FLAT) for viewing unrolled reconstruction approaches as flow models. This insight allowed led to three critical changes in training unrolled networks for reconstruction (1) unrolls viewed as cascaded time steps must satisfy constrains as a trajectory (2) normally free hyper parameters in unrolled methods are fixed to satisfy ODE (3) at intermediate time steps (unrolls) the images are aligned to the desired trajectory not just the final image. The authors show through several experiments that enforcing these constraints on their unrolled network led to SOTA performance compared to other existing methods in MRI reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, this paper provides a novel and useful insight into existing unrolled techniques which I think would be of interest to the broader MRI recon community since unrolled techniques are very popular. The connection between flows and unrolls is interesting and clearly improves performance which is great. The authors did a good job comparing to other SOTA methods."}, "weaknesses": {"value": "I do believe that the paper would benefit from testing their method on various acceleration levels of MRI data. They show results for R=8 but I would also like to see what their performance gains are at higher (and) lower acceleration levels like R=4 and 12. Additionally I would like to know what the wall clock time is for running inference of their method vs. the other methods presented.  They present the number of iterations compared to other techniques, but it would be nice to see the actual timing."}, "questions": {"value": "1.\tI am not sure about the statement in lines 238-239 where it is stated that $p(y|x_t)=N(y|Ax_t,\\sigma^2)$. Isn’t this only true for t=0?\n2.\tIs the initial reconstruction $x^k = y$ the pseudo-inverse reconstruction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AKT5b1XDWR", "forum": "9jCca0DsT5", "replyto": "9jCca0DsT5", "signatures": ["ICLR.cc/2026/Conference/Submission3214/Reviewer_nr56"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3214/Reviewer_nr56"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918496542, "cdate": 1761918496542, "tmdate": 1762916602867, "mdate": 1762916602867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to develop a flow ODE characterization for unrolled networks. The idea is interesting, but unfortunately there are fundamental flaws."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- I think the overall idea of using a flow ODE characterization to describe unrolled networks is great. Hence I really wanted to like this paper. Unfortunately both the theory and execution has substantial flaws."}, "weaknesses": {"value": "1) The proof of the main result is fundamentally flawed:\n- The argument hinges on writing out p(y|x_t). Unfortunately Eq. 1 does not apply to intermediate points on the trajectory, which is well-known in the literature. p(y|x_t) would need to be calculated as (in the authors' notation): \\int p(y|x_0) p(x_0| x_t) dx_0, since we only know the relationship between y and x_0 (i.e. Eq. 1). This breaks down the whole proof. There are many works on approximating this integral in the diffusion inverse problems literature.\n- The authors can see this fails by considering their own definition of x1 = y (incidentally I'm surprised they are trying to come up with a velocity field from k-space to image domain)\n- The second part of the proof that is questionable is the statement \"this velocity aligns with the gradient of the conditional log-density\" Why is this true? This is not shown. \n- Also fundamentally one would expect (5) to have dependence on y in this setup, instead (5) is describing an unconditional flow on the image set, with no knowledge of measurements.\n\n2) The authors seem to be unaware of MRI reconstruction literature. Much of the motivational claims are untrue or incomplete:\n- \"hyperparameters such as step size and weighting coefficients are typically set through heuristics or empirical tuning\" \nAlmost all algorithm unrolling frameworks learn step sizes and weighting coefficients jointly with the proximal operator/regularization neural network. This statement is therefore incorrect. Furthermore, these are considered parameters of the unrolled network, not hyperparameters. This joint learning of regularization and data fidelity parameters is the main advantage of algorithm unrolling over plug-and-play type methods.\n- \"they are typically trained with supervision only at the final cascade\"\nThis is partially true. First off, it seems the authors are not distinguishing between unrolled networks with shared parameters (i.e. each cascade uses the same CNN and step sizes/weighting parameters as in MoDL) vs. those with unshared parameters (as in E2E-VarNet). In the former case, it is easy to train with supervision at the output. Even in that case, one can do weak intermediate supervision by training a single cascade first, replicating it for T cascades, and fine-tuning the T-cascade version (as in MoDL). In the latter case, the typical approach is to first train the shared parameter version, then fine-tune the unshared version on that. There are also works that propose intermediate supervision (e.g. doi: 10.3390/e27090929), but the benefit of this is marginal especially in the first shared setup.\n- Naturally, the erratic behavior observed is related to the unshared version. This is usually not seen in the shared parameter setup, which has much \"flatter\" behavior across cascades.\n\n3) Experiments are performed on either DICOM images or single-coil datasets, which have limited utility in MRI.\n\n4) The proposed algorithm only extends to gradient descent/proximal gradient descent type algorithms, and do not explain the more successful variants based on variable splitting (e.g. ADMM)\n\n5) The authors set \\sigma = 1, but this has a physical meaning in the derivation as the observation noise, so one cannot arbitrarily set it to any value they want.\n\n6) The authors not only use the flow-based loss term, but multiple other loss terms. It is unclear if the comparison networks used the same additional loss terms. An ablation study on the effect of each term in (13) is clearly missing.\n\n7) 12 steps is not a speed-up over existing unrolled networks. MoDL has 10 steps, E2E-Varnet readily has 12 as well.\n\nMinor points:\n- \"Φk(·) is the learned regularizer (often implemented by a CNN)\" \nOften the proximal operator corresponding the regularizer is implemented with a CNN.\n- Flow matching literature typically uses the opposite indexing, with x0 being the noise and x1 being the data distribution.\n- \\sum \\delta_k = -1 is an interesting insight, but this is counter-acted by arbitrarily setting \\sigma = 1. \n- There is something fundamentally wrong with the fully-sampled k-space shown in Fig. 1. Perhaps due to some DICOM processing."}, "questions": {"value": "These are already covered in the weaknesses:\n- How does the method work on multi-coil MRI data?\n- Why is \\sigma = 1? If it is used as the noise level in the dataset, how does it affect the results?\n- What is the effect of each term in the loss function?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HtRwSMr2En", "forum": "9jCca0DsT5", "replyto": "9jCca0DsT5", "signatures": ["ICLR.cc/2026/Conference/Submission3214/Reviewer_jvCY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3214/Reviewer_jvCY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921430341, "cdate": 1761921430341, "tmdate": 1762916602538, "mdate": 1762916602538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper re-interprets unrolling networks as flow matching. It views the iterations derived from optimization algorithms in unrolling networks as time steps in flow matching and aligns intermediate estimation, x_k in unrolling network,with the conditional path x_t in flow matching. Authors formulated it mathematically and claimed the newly derived losses trains unrolling network better. The alignment of optimization iterations in unrolling network to the conditional path in flow matching seems new to me. However the motivation is weak and not well grounded. Please find my concerns below."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written and has a comprehensive literature review.\n2. This paper includes experiments that compares the proposed method to methods that are from different categories."}, "weaknesses": {"value": "1. The authors claim that the core innovation of FLAT defines the velocity alignment between unrolling iterations and flow matching. It is formulated by first defining two different velocities at the k-th timestep:\n\n\ta.  **Ideal Discretized Velocity ($v_{t_k}$):** This is the target velocity, defined as the discrete temporal derivative using the ground truth.\n\t    $v_{t_{k}} = (x_{t_{k}}^{\\*} - x^{(k+1)}) / (t_{k} - t_{k+1})$, $x_{t_{k}}^{*}$ is the linearly interpolated ground truth at time $t_k$. $x^{(k+1)}$ is the network's prediction from the *previous* iteration.  \n\tb.  **Network's Predicted Velocity ($v^{(k)}$):** This is the velocity predicted by the network at the current step.\n\t    $v^{(k)} = (x^{(k)} - x^{(k+1)}) / (t_{k} - t_{k+1})$, $x^{(k)}$ is the network's output at the current step $k$.   \n\n        Why is this ideal ode path is better than original path derived from optimization iterations? If this is true, how much influence can this loss terms make?\n\n2. As a following-up questions, how you select the hyper-parameters? Could you explain why there are so many other loss terms? How you balance the weights for each term? Why the weights for the velocity alignment term is so small? Can we only keep the velocity alignment term? What if we have the other loss terms for other unrolling methods.\n\n3. Not sure if it is right to replace $\\nabla_{x_t} \\log p(x_t)$ with the velocity field $v_\\theta(x_t, t)$, as the unrolling network actually does not learn this and has no generative modelling for it. The unrolling network to me just learns a discriminative model. Would you further comment on this?\n\n4. If this velocity alignment help us in training unrolling network, how would the number of discrete steps affect the performance? The more the better?\n\n5. Which specific diffusion model based method you used for comparison? DDPM and DDIM samplers seems to be very broad. Or would you specify how you implement the method using diffusion models?"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PGwpCGtb2f", "forum": "9jCca0DsT5", "replyto": "9jCca0DsT5", "signatures": ["ICLR.cc/2026/Conference/Submission3214/Reviewer_AHYE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3214/Reviewer_AHYE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762170324127, "cdate": 1762170324127, "tmdate": 1762916602202, "mdate": 1762916602202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}