{"id": "Ix4WA6AXPe", "number": 9492, "cdate": 1758124584892, "mdate": 1759897717040, "content": {"title": "Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models", "abstract": "Large language models may encode sensitive information or outdated knowledge that needs to be removed, to ensure responsible and compliant model responses. Unlearning has emerged as an efficient alternative to full retraining, aiming to remove specific knowledge while preserving overall model utility. Existing evaluations of unlearning methods focus on (1) the extent of forgetting of the target knowledge (forget set) and (2) maintaining performance on the retain set (i.e., utility). However, these evaluations overlook an important usability aspect: users may still want the model to leverage the removed information if it is re-introduced in the prompt. In a systematic evaluation of six state-of-the-art unlearning methods, we find that they consistently impair such contextual utility. To address this, we augment unlearning objectives with a plug-in term that preserves the model's ability to use forgotten knowledge when it is present in context. Extensive experiments demonstrate that our approach restores contextual utility to near original levels while still maintaining effective forgetting and retain-set utility.", "tldr": "We find existing unlearning methods suppress contextual utility on the forget set and propose context-aware unlearning to fix it.", "keywords": ["LLM Unlearning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7bd791414f89a60ce08977de8dfacc970d3d087e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper argues that standard LLM unlearning pipelines—optimized to reduce recall on a forget set while preserving generic utility on a retain set—silently degrade a third axis the community rarely measures: the model’s ability to use the “forgotten” information when it is explicitly reintroduced in the prompt (their “Contextual QA” setting)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper isolates “contextual utility” as a distinct deployment requirement—use it if the user provides it—and shows that popular unlearning objectives deform internal representations enough to suppress even context-grounded use.\n- The contextual variant rescues RMU from near-zero Contextual QA (≤0.05) to ~0.97–0.99 while keeping retain-set utility virtually unchanged; NPO/UNDIAL see double-digit LLM-Judge gains too.\n- Evaluation targets real usage rather than proxy memorization."}, "weaknesses": {"value": "- The proposed fix is a straightforward KL-consistency term to the original model on contextual prompts, conceptually akin to the KL regularizers widely used in RLHF/DPO-style pipelines the paper itself cites as inspiration. The core contribution is thus problem framing + evaluation, not a new optimization principle.\n- Because the KL target is p_{orig}, any pre-existing issues in the original model’s grounding (hallucinations, refusal idiosyncrasies) are inherited by construction. The paper does not compare against anchoring to a teacher ensemble or to gold references, nor does it analyze when p_{orig}​ miscalibrates contextual evidence.\n- Seems like Grad Difference, DPO, GradAscent are not being considered as baselines (as shown in Table 1), but the corresponding results are missing in table 2."}, "questions": {"value": "- Why is KL anchored to p_{orig}​ rather than an ensemble/teacher oracle or gold references when available? Have you observed cases where p_{orig} provides incorrect contextual distributions, and how does the method behave then?\n- How does the method fare when context contains both correct and subtly incorrect spans (a common RAG issue)?\n- What happens if distractor facts outnumber correct snippets, or if retrieval returns off-topic but lexically similar passages?\n- Have you tried a teacher-free variant (e.g., KL to a masked version of p_w that conditions on evidence spans) to avoid hard dependence on p_{orig}?\n\nI am willing to adjust my score if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sHqrV1XiYX", "forum": "Ix4WA6AXPe", "replyto": "Ix4WA6AXPe", "signatures": ["ICLR.cc/2026/Conference/Submission9492/Reviewer_3U3v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9492/Reviewer_3U3v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760498821116, "cdate": 1760498821116, "tmdate": 1762921071501, "mdate": 1762921071501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper illustrates a gap in current LLM unlearning evaluations: the lack of consideration for contextual recoverability (i.e., the ability of a model to recall forgotten knowledge when the context is reintroduced). To address this, the authors propose a new evaluation setup, Contextual QA, and demonstrate that several existing methods (RMU, NPO, UNDIAL, DPO, GradAscent, GradDiff) perform poorly under this setting. They further introduce a context-aware objective that augments existing unlearning losses with a KL-consistency term, encouraging alignment of the unlearned model’s conditional distribution when contextual cues are provided."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a clear problem formulation and introduces an insightful new evaluation axis, showing that standard unlearning methods can suppress the model’s ability to utilize externally supplied facts. The proposed fix is simple, practical, and easy to integrate, requiring minimal additional hyperparameters while yielding tangible empirical gains."}, "weaknesses": {"value": "- The TOFU dataset uses fictitious entities designed to be independent, but real data typically exhibit strong interconnections among entities and attributes. The authors should also evaluate existing unlearning methods on datasets like PISTOL (which explicitly models data interconnectivity) and on real-world pretraining data to test whether ContextQA performance remains suppressed when partial contextual links to the forget set persist.\n\n- The current ContextQA setup appears to append ground-truth answers cleanly and explicitly (despite paraphrase in ablation study) to the prompt, essentially testing whether the model can copy or condition on directly supplied facts. In realistic RAG scenarios, contextual evidence is often long, noisy, and embedded within paragraphs. The authors are encouraged to assess unlearning behavior under such more realistic retrieval settings.\n\n- The proposed mechanism trains the model to respond affirmatively to semantically similar cue, which expand the attack surface for prompt-based extraction. This design potentially conflicts with ongoing efforts to enhance the robustness and safety of unlearning methods, and the authors did not evaluate the method's robustness to this (and any other) attack. Considering the stringent safety and compliance requirements inherent to unlearning methods, the proposed approach raises legitimate concerns regarding its practical deployability."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yqjAb6OMhL", "forum": "Ix4WA6AXPe", "replyto": "Ix4WA6AXPe", "signatures": ["ICLR.cc/2026/Conference/Submission9492/Reviewer_tTjH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9492/Reviewer_tTjH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694352095, "cdate": 1761694352095, "tmdate": 1762921071009, "mdate": 1762921071009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors argues that an unlearned model should still be able to utilize ground truth if it's provided in the context. To address this, the authors add a context term to the overall optimization objective, encouraging the model to produce correct outputs when forget examples are paired with their ground-truth context."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents an interesting problem with a simple yet effective solution. Experimental results indicate that adding the context term noticeably improves model performance when ground truth is paired with forget samples in the prompt."}, "weaknesses": {"value": "My primary concern is the practical value of an unlearning approach that explicitly retain the knowledge and allow its recovery via prompt-based context. Since unlearning demand is driven by critical concerns such as privacy and compliance requirements, retaining such information, even conditionally, may still violate regulations or enable easier extraction by attackers. Should introducing context be more of a potential attack (i.e., vulnerability of existing unlearned model) than unlearning objective? Also should a safer alternative to reintroduce knowledge through weights rather than via prompting to avoid making recovery trivial to exploit? The paper should also consider this risk and test robustness on attack methods (including but not limited to quantization attack, prompt attack and other information extract attacks).\n\nReported baselines induce parameter changes for coarse-knowledge unlearning rather than precisely unlearning knowledge in a fine-grained manner. I'm wondering if recently proposed method that uses activation steering to precisely steer knowledge representation would still suffer the same context-suppression issue? Please include such method as baseline and evaluate whether it has contextual recoverability issue?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4toyTXpIZp", "forum": "Ix4WA6AXPe", "replyto": "Ix4WA6AXPe", "signatures": ["ICLR.cc/2026/Conference/Submission9492/Reviewer_xDu6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9492/Reviewer_xDu6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759547674, "cdate": 1761759547674, "tmdate": 1762921069650, "mdate": 1762921069650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies that existing LLM unlearning methods, while effective at forgetting target knowledge, often harm a model’s ability to use that knowledge when it is reintroduced in context. To address this, the authors propose context-aware unlearning, which adds a KL-divergence regularization term aligning the unlearned model’s contextual responses with the original model. Experiments on Gemma-2B-IT and Qwen3-8B show that this approach restores contextual utility to near-original levels while maintaining effective forgetting and model utility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed context-aware objective is modular, requires minimal changes, and can be plugged into various unlearning methods.\n- The approach yields substantial improvements (LLM-Judge ≈ +0.9) in contextual QA performance without harming forgetting or utility.\n- The authors conduct extensive experiments across multiple methods, and forget ratios, supported by both quantitative and qualitative analyses."}, "weaknesses": {"value": "- For handling outdated knowledge, knowledge editing is generally more appropriate than unlearning. Moreover, realistic cases where a model must “re-use” forgotten information are rare.\n- The problem is more accurately described as studying how unlearning affects in-context learning ability, rather than as a practical need to recover forgotten knowledge.\n- Experiments focus mainly on two small- to mid-sized instruction-tuned models (Gemma-2B-IT and Qwen3-8B) and synthetic benchmarks (TOFU), limiting generalizability.\n- The paper is empirically strong but offers limited theoretical justification or formal guarantees for why contextual utility preservation works."}, "questions": {"value": "- How much existing unlearning methods harm the model’s in-context learning ability, and how much improvement the proposed context-aware unlearning brings in terms of general ICL performance, beyond the specific Contextual QA task?\n- The paper frames its motivation around responsible AI and compliance. Yet, if the method restores access to forgotten information when provided in prompts, could this re-enable access to sensitive content that was intentionally removed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eLHvpWDYFI", "forum": "Ix4WA6AXPe", "replyto": "Ix4WA6AXPe", "signatures": ["ICLR.cc/2026/Conference/Submission9492/Reviewer_WCwE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9492/Reviewer_WCwE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762072008811, "cdate": 1762072008811, "tmdate": 1762921069356, "mdate": 1762921069356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}