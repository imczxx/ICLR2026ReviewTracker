{"id": "fCZf20wK6p", "number": 23478, "cdate": 1758344387767, "mdate": 1759896812594, "content": {"title": "NDAD: Negative-Direction Aware Decoding for Large Language Models via Controllable Hallucination Signal Injection", "abstract": "Large language models (LLMs) have recently achieved impressive progress in knowledge-intensive and reasoning tasks. However, their tendency to produce fabricated or factually inconsistent content remains a fundamental challenge to their practical deployment. To address this issue, we propose Negative-Direction Aware Decoding (NDAD), a novel decoding method that identifies and exploits hallucination signals as repulsive directions in the model’s representation space, thereby improving factual adherence without retraining. Specifically, NDAD elicits hallucination-leaning signals by selectively masking critical attention heads, which exposes unstable hypotheses that the model would otherwise amplify during generation. To regulate the influence of these signals, NDAD employs two complementary weights: a global alignment weight measuring how well the induced signal aligns with the layer’s native activations (thus quantifying its referential utility) and a local weight estimating whether low-probability tokens in the masked distribution are likely to evolve toward the final output. Based on the weights, we derive a latent hallucination distribution that serves as the negative direction. A lightweight gradient-descent step then subtracts mass from hallucination-prone regions of the output distribution, adjusting the final logits while preserving the model’s high-confidence predictions. Extensive experiments across multiple LLMs and diverse benchmark datasets demonstrate that NDAD consistently enhances factual reliability without requiring additional training or external knowledge.", "tldr": "", "keywords": ["Large Language Models;Contrastive Decoding;Robustness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cdaedf409a5793ae1fb1e13bc968dc27e6a3b3c4.pdf", "supplementary_material": "/attachment/211c39b71df1ae82faa5c4539d03a49c7ceb32f3.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Negative-Direction Aware Decoding (NDAD), a raining-free decoding strategy designed to mitigate hallucinations in LLMs. The core contribution lies in its approach of actively identifying and leveraging \"hallucination signals\" as a repulsive force during generation. Rather than solely promoting factual content, NDAD isolates hallucination-prone directions by strategically masking influential attention heads. The method then employs a dynamic weighting mechanism, integrating both global consistency and local divergence measures, to controllably steer the model's output away from these identified negative trajectories via a gradient-descent adjustment. Experiments demonstrate that NDAD consistently enhances factual reliability across diverse LLMs and benchmark datasets, offering a lightweight yet potent solution to a critical challenge in model deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method is training-free and operates at inference time, making it a practical and computationally accessible approach for improving factuality compared to methods requiring model fine-tuning\n- The dual-weighting scheme that considers both global consistency (alignment with layer activations) and local divergence (evolution of low-probability tokens) provides a non-trivial method for modulating the hallucination signal.\n- The paper provides ablation experiments that investigate the necessity of its core components, such as the global and local weights and the importance-guided masking strategy, strengthening the justification for its specific design choices."}, "weaknesses": {"value": "- The performance improvements over the baseline are often marginal, questioning the method's practical significance.\n- The method introduces a number of new hyperparameters (number of masked heads, number of layers, top-I tokens, evolution rate α) that likely require careful tuning for different models and tasks, potentially limiting its out-of-the-box utility.\n- The paper does not analyze the computational overhead or potential impact on generation latency. Furthermore, evaluation is focused on factuality, with no assessment of whether the method harms other text quality aspects like coherence or fluency.\n- All models tested are in the smaller 7B-13B parameter range. The method's viability and computational cost on much larger models (e.g., 70B+) remain unproven."}, "questions": {"value": "1. What is the computational overhead (e.g., latency increase per token) of NDAD compared to greedy decoding and the baseline?\n2. Have you evaluated whether NDAD negatively impacts other text qualities, such as fluency, coherence, or stylistic appropriateness?\n3. How do you expect NDAD's performance and computational cost to scale when applied to much larger models (e.g., 70B+ parameters)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KPfWddlsD7", "forum": "fCZf20wK6p", "replyto": "fCZf20wK6p", "signatures": ["ICLR.cc/2026/Conference/Submission23478/Reviewer_ehY7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23478/Reviewer_ehY7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833305810, "cdate": 1761833305810, "tmdate": 1762942676875, "mdate": 1762942676875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The following paper proposes a novel approach to hallucination reduction in Large Language Models (LLMs). It decerns a direction responsible for the hallucinations by masking out most influential attention heads responsible for the factuality. This direction is then subtracted from the original output logits of the model as a way to reduce the hallucinations in the final predictions. The paper showcases the effectiveness of the proposed method on a number of LLMs and datasets. It shows that proposed approach increases the accuracy on a number of benchmarks compared to existing hallucination reduction methods and studies the impact of the components driving the hallucinations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper is well written, the method and the experiments  is clearly explained\n+ The authors report results on a number of datasets and models - they show that proposed methods consistently outperform the existing baseline approaches."}, "weaknesses": {"value": "**Discussion on limitations of the work**\n\nThe paper is well written but there are clearly some insights missing on the limitations of the proposed work. \n\n1) The paper relies heavily on an existing algorithm that identifies heads responsible for factuality but it is unclear if the heads responsible for the factually are consistently the same for all tasks and sample sizes and what's the accuracy in identifying such heads. It is unclear how the number of heads and layers are chosen. It is hard to tell from Figure 4 what number of heads and layers one should choose.\n\n2) It is also unclear how the results are aggregated for single token vs multi-token generation. When we say logits_L is this the logits for the final answer, a single token or aggregation of logits across multiple tokens ?\n\n3) It is also unclear how we set the threshold to select top I tokens ?\n\n\nDescriptions of the figures and tables. \n\nThe descriptions are quite vague and high level. It might be important to mention in the titles what are we exactly measuring e.g. Overall Accuracy or the Factuality Accuracy. Figure 1 is great but the description is very vague and it requires reading the rest of the paper to understand it. \n\nIt would we also interesting to describe the choice of combining global and local weights. Why do we use the product of W_global * W_local vs additive influence W_global + W_local"}, "questions": {"value": "See weaknesses\nTable 1 shows that some of the baseline hallucination mitigation approaches worsen the accuracy. Why is that the case ? Isn't the purpose of those methods to increase the accuracy ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7JoHPBjqgE", "forum": "fCZf20wK6p", "replyto": "fCZf20wK6p", "signatures": ["ICLR.cc/2026/Conference/Submission23478/Reviewer_pT8A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23478/Reviewer_pT8A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869523053, "cdate": 1761869523053, "tmdate": 1762942676692, "mdate": 1762942676692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a training-free method to reduce hallucination named NDAD. The method aim to reduce LLM hallucinations, its core ideas is to steer output distributions away from hallucination-prone directions. First, the method masks critical attention heads (selected based on metrics such as importance scores and entropy) to generate a \"hallucination signal,\" then uses dynamic weighting (global weight for signal-logit alignment and local weight for low-probability token evolution) to control this signal's impact. Finally,  the output logits are modified by gradient descent to penalize the KL divergence from the negative distribution. The paper provides tests across multiple LLMs and different benchmarks, the results show that the method has similar results to baselines such as  DoLa, SLED."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method presents a unique and compelling perspective by \"subtracting negatives\"—inducing a hallucination signal and then repelling the model from it.\n\n2. The approach demonstrates broad applicability, validated across multiple model families (Llama, Mistral, Qwen) and sizes (7B, 13B, 8B) on a diverse set of tasks (TruthfulQA, GSM8K).\n\n3. Rigorous studies validate the core design choices, including the efficacy of the masking strategy and the necessity of both the global and local weighting components."}, "weaknesses": {"value": "1. My biggest concern is that the method is not practical at all. The proposed method requires a second, modified forward pass at every decoding step. In the meanwhile, the gradient-descent step is problmetic as well. This will result in inference-time latency and definitely problemetic in many inference settings(may lead to KV-cache related issues). All make the method impractical.\n\n2. The motivation and calculation for the local weight (evolution trajectories, one-hot vectors) lack intuitive, step-by-step clarity\n\n3. The introduction of several new parameters combined with the observed performance curves suggests the method may be highly sensitive to tuning. It can be critical for real-world deployment\n\n4. The choice of using an arbitrary squared transformation for the final weight scores is not sufficiently justified. \n\n5. Another concern is that even with introduction of overhead and hyper-parameters, the performance gain is very small and even worse compared with existing method"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XyaTPrRtSc", "forum": "fCZf20wK6p", "replyto": "fCZf20wK6p", "signatures": ["ICLR.cc/2026/Conference/Submission23478/Reviewer_vTbm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23478/Reviewer_vTbm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967099745, "cdate": 1761967099745, "tmdate": 1762942676440, "mdate": 1762942676440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Negative-Direction Aware Decoding (NDAD), a training-free decoding strategy to reduce hallucinations in large language models. NDAD first intentionally disrupts a model’s most fact-relevant attention heads to induce a “hallucination-leaning” prediction, which reveals how the model would start to generate unreliable content. It then identifies the most dangerous parts of that signal — the token directions that are likely to grow into the final answer — and treats them as a negative direction. During actual decoding, NDAD pushes the model’s output distribution away from this negative direction via a small KL-based adjustment to the final logits. Across multiple benchmarks (factual QA, open-ended QA, and chain-of-thought math reasoning) and several popular LLMs, NDAD matches or outperforms prior decoding-time methods like SLED and Activation Decoding, without extra training data, external retrieval, or modifying model weights."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The perspective is new: instead of “pulling the answer toward the correct truth,” NDAD “pushes the model away from being wrong.” It first induces a hallucination signal — the direction the model tends to move toward incorrect content after targeted corruption — and then explicitly repels the final output probabilities away from that direction. This reframes the problem from boosting truthfulness to actively repulsing hallucination, which is conceptually distinct.\n2. The method introduces a weighted mechanism that combines global consistency and local evolution, instead of bluntly suppressing all probabilities. After combining these two views, it applies normalization and squared weighting to emphasize high-confidence risky directions. This fine-grained control avoids the typical failure mode where aggressive down-weighting breaks fluency and makes the sentence fall apart.\n3. The approach shows consistent gains across multiple tasks, and is particularly strong on tasks that require reasoning and multi-step thinking."}, "weaknesses": {"value": "1. Runtime comparison with baselines. he method requires extra computation at inference (masking heads, computing global/local weights, doing the gradient-style correction), but the paper does not systematically report the runtime or memory overhead versus standard decoding or simpler contrastive methods. \n2. Regarding the reliability and causality of the “critical attention head” selection process: Can the authors provide more fine-grained evidence, for example, whether the heads identified as critical tend to attend to high-confidence knowledge sources (such as factual spans in the context, entity mentions from the question, numerical cues, etc.) rather than merely contributing to fluency or syntax?"}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u6bVQTwImQ", "forum": "fCZf20wK6p", "replyto": "fCZf20wK6p", "signatures": ["ICLR.cc/2026/Conference/Submission23478/Reviewer_VbaU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23478/Reviewer_VbaU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986512051, "cdate": 1761986512051, "tmdate": 1762942676164, "mdate": 1762942676164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}