{"id": "FOZ4FwC7YX", "number": 12636, "cdate": 1758209175293, "mdate": 1763054763887, "content": {"title": "Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play", "abstract": "Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present Speech-DRAME, a unified framework that contributes at three levels: (i) Speech-DRAME-EvalBench, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models (SEMs), (ii) DRAME-Eval, a fine-tuned evaluation model that substantially outperforms zero-shot and few-shot ALLMs, and (iii) Speech-DRAME-RoleBench, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: Archetype Evaluation, a top-down approach measuring adherence to broad role archetypes, and Realism Evaluation, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, DRAME-Eval achieves stronger agreement with human ratings (Pearson correlation improves from 0.480 to 0.629 in archetypes, and from 0.390 to 0.625 in realism).\n By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.", "tldr": "Speech-DRAME introduces a bilingual benchmark for speech role-play, combining Archetype (top-down) and Realism (bottom-up) evaluation, aligning speech evaluation models with human judgments to outperform zero-shot audio LLM judges.", "keywords": ["Speech Role-play", "Role-play Evaluation", "Speech Evaluation Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6d20575ef0200e4876a4484d27269bc7240d369b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Exceeded the page limit. Desk Reject."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Exceeded the page limit. Desk Reject."}, "weaknesses": {"value": "Exceeded the page limit. Desk Reject."}, "questions": {"value": "Exceeded the page limit. Desk Reject."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VUbof7XCtJ", "forum": "FOZ4FwC7YX", "replyto": "FOZ4FwC7YX", "signatures": ["ICLR.cc/2026/Conference/Submission12636/Reviewer_xKtL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12636/Reviewer_xKtL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875010051, "cdate": 1761875010051, "tmdate": 1762923479041, "mdate": 1762923479041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Dear Area Chairs and Reviewers,\n\nThank you for evaluating our submission. Due to a formatting oversight on our part, the paper exceeded the page limit by two lines, and we fully understand the resulting desk-reject recommendations. We would therefore like to formally withdraw the paper.\n\nWe would especially like to thank the two reviewers who provided detailed and thoughtful feedback despite the formatting issue. Their comments were highly constructive, clearly engaged with the technical substance of our work, and have already helped us identify several important directions for improvement. We are truly grateful for the time and care they invested in offering such comprehensive critiques.\n\nThank you again for your understanding and for the reviewers’ valuable efforts.\n\nSincerely,\nSpeech-DRAME Authors"}}, "id": "5bGO9yS9iB", "forum": "FOZ4FwC7YX", "replyto": "FOZ4FwC7YX", "signatures": ["ICLR.cc/2026/Conference/Submission12636/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12636/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763054763231, "cdate": 1763054763231, "tmdate": 1763054763231, "mdate": 1763054763231, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is desk-rejected since at the time of submission, the main text exceeds 9 pages."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper is desk-rejected since at the time of submission, the main text exceeds 9 pages."}, "weaknesses": {"value": "The paper is desk-rejected since at the time of submission, the main text exceeds 9 pages."}, "questions": {"value": "The paper is desk-rejected since at the time of submission, the main text exceeds 9 pages."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oSabdGzfza", "forum": "FOZ4FwC7YX", "replyto": "FOZ4FwC7YX", "signatures": ["ICLR.cc/2026/Conference/Submission12636/Reviewer_MmWZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12636/Reviewer_MmWZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957370156, "cdate": 1761957370156, "tmdate": 1762923478774, "mdate": 1762923478774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Speech-Dream, a framework for benchmarking speech role-play, which includes an evaluation benchmark for training and testing speech evaluation models, a fine-tuned evaluation model (DREAM-Eval) that outperforms zero-shot and few-shot Audio LLMs, and a speech role-play benchmark for comparing speech foundation models. Speech-Dream proposes two complementary evaluation strategies: Archetype Evaluation (based on synthetic data) and Realism Evaluation (based on real speech data). Compared to Audio LLMs, DREAM-Eval achieves stronger alignment with human ratings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes two complementary evaluation strategies: Archetype Evaluation (synthetic data-based) and Realism Evaluation (real human speech-based).\n2. It builds a comprehensive framework, including datasets, evaluation models, and benchmarks, and evaluates multiple proprietary and open-source models.\n3. The fine-tuned Qwen2Audio model achieves superior evaluation quality compared to general-purpose Audio LLMs."}, "weaknesses": {"value": "1. Realism-based evaluation suffers from domain mismatch, as its training data also contains synthetic speech, reducing its usability despite being based on real human speech.\n2. Realism Evaluation shows poor alignment with human perception (Spearman correlation of only 0.375), indicating limited reliability.\n3. The framework only supports single-turn evaluations, failing to capture the coherence of multi-turn narratives.\n4. The paper lacks audible demo cases for the speech role-play generation task.\n5. Some parts of the paper are less concise, with redundant content, such as lines 083 to 101 in the introduction."}, "questions": {"value": "Please address the issues described in the Weaknesses section. Resolving these concerns could improve the paper’s evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0FGp70aofy", "forum": "FOZ4FwC7YX", "replyto": "FOZ4FwC7YX", "signatures": ["ICLR.cc/2026/Conference/Submission12636/Reviewer_QBf4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12636/Reviewer_QBf4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984623143, "cdate": 1761984623143, "tmdate": 1762923478467, "mdate": 1762923478467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Speech-DRAME, a framework for evaluating speech role-play tasks using both generative models and evaluation models (SEMs). The authors aim to build a comprehensive benchmark, consisting of the EvalBench and RoleBench, for evaluating role-play performance in speech generation. They also introduce a dual evaluation strategy (Archetype-based and Realism-based evaluations) and argue that their method provides an improvement over previous evaluation techniques based on zero-shot large language models (ALLMs)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel Framework**: The proposed framework offers an interesting approach by integrating role-play generation with a dual evaluation strategy. The introduction of EvalBench and RoleBench datasets provides a clear framework for the evaluation of speech-based role-playing tasks.\n2. **Detailed Benchmark Design**: The inclusion of both Archetype and Realism evaluation strategies ensures that the proposed method addresses both large-scale and fine-grained human perception of speech quality.\n3. **Relevance to the Community**: The focus on improving role-play evaluation and the framework's potential to influence future work in speech generation and assessment makes it relevant to the community."}, "weaknesses": {"value": "1. **Over-reliance on the Appendix**: A significant amount of important information is placed in the **Appendix**, which makes it difficult to follow the main arguments and understand the contributions in the body of the paper. A well-written paper should be **self-contained**, with all critical information included in the main text.\n2. **Clarity of Motivation**: The paper lacks a clear motivation regarding the limitation of **zero-shot ALLMs** as evaluation judges. There is insufficient discussion of the limitations of using ALLMs for this task.\n   - In line 45-46, the statement **\"this strategy\"** is vague and lacks clarity regarding what exactly is being referred to.\n   - The statement in **Lines 131-132** (\"Yet studies consistently reveal high prompt sensitivity and shallow capture of paralinguistic cues\") is **unsupported** and should be backed by references or experimental evidence.\n3. **Clarity of Realism-Based Evaluation**:\n   - The **Realism-based role-play evaluation** section is not explained clearly. The paper mentions it but does not provide enough details about how it differs from the Archetype evaluation or what specific criteria it evaluates.\n   - The paper also mentions the **dual evaluation strategy** but does not sufficiently explain the distinctions and advantages of each evaluation approach (Archetype vs. Realism).\n4. **Fine-tuning Data Quality and Diversity**:\n   - While the paper mentions the use of **fine-tuning**, it does not address the **quality control** or **diversity** of the fine-tuning data. Given that the fine-tuned model is trained on data from the same distribution, the results may be overly optimistic and not fully generalizable.\n   - In **Line 240**, the paper mentions that data is collected from **speech foundation models in D.4**, but some of the models (e.g., Qwen-2.5-Omni) are being used for evaluation, which could affect the fairness of the evaluation.\n\n\nSeveral other weaknesses:\n\n1. **Use of TTS-Generated Data for Archetype**\n   - The decision to use **TTS-generated role-play data** instead of **real human speech** in the **Archetype-based evaluation** is not justified sufficiently. Given that real human speech data is available (e.g., from firefighter media sources) and the main claim line053, it is unclear why synthetic data was chosen.\n\n2. **Evaluation Across Unified Prompting Template**:\n\n   - The paper claims that **all evaluations follow a unified prompting template**, but this approach may not fully reflect the model's ability to handle diverse inputs and scenarios. This could limit the generality of the evaluation.\n\n   - **Suggested improvement**: The authors should discuss the potential limitations of using a single prompting template and consider testing the model across varied inputs to better assess its performance."}, "questions": {"value": "The paper presents an interesting framework with potential to advance the field of speech role-play evaluation. However, the paper could benefit from clearer explanations and justifications for certain methodological choices, particularly regarding the use of TTS-generated data, the zero-shot ALLM evaluation model, and the definition of key terms. Additionally, a stronger comparison with prior work and more detailed experimental design would strengthen the paper.\n\nAdditionally, the **experimental design** could be further detailed, particularly with regard to **fine-tuning data** and its potential impact on generalizability.\n\nFrom my perspective, the **training of evaluators** (e.g., the DRAME-Eval model) is one of the most valuable aspects of this paper, but there is a need for further exploration of its **generalization across different domains** and **evaluation metrics**."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7ABYzlaegl", "forum": "FOZ4FwC7YX", "replyto": "FOZ4FwC7YX", "signatures": ["ICLR.cc/2026/Conference/Submission12636/Reviewer_f2WH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12636/Reviewer_f2WH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998247163, "cdate": 1761998247163, "tmdate": 1762923478209, "mdate": 1762923478209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}