{"id": "kDhAiaGzrn", "number": 23089, "cdate": 1758339425496, "mdate": 1759896832638, "content": {"title": "VGR: Visual Grounded Reasoning", "abstract": "In the field of multimodal chain-of-thought (CoT) reasoning, existing approaches\npredominantly rely on reasoning on pure linguistic space, which inherently suffers\nfrom language bias and is largely confined to math or science domains. This narrow\nfocus limits their ability to handle complex visual reasoning tasks that demand comprehensive understanding of image details. To address these limitations, this paper\nintroduces VGR, a novel reasoning multimodal large language model (MLLM) that\ncan replay the visual memory during thinking just like humans. Unlike traditional\nMLLMs, VGR first thinks the question and detects relevant regions that may help\nsolve problems, then, the visual memory from the critical area is extracted to assist\nreasoning. To achieve this, we curate a large-scale SFT dataset called VGR-SFT\nthat contains reasoning data with mixed vision grounding and language deduction.\nThis teaches VGR to think and actively choose grounding areas for key information before answering, and we propose a dynamic visual memory replay stage to\nintegrates the corresponding information into the reasoning process, enhancing\nmultimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline show\nthat VGR achieves superior performance on multimodal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and +12.9 improvement on ChartQA.", "tldr": "We presents VGR, an MLLM with fine-grained visual perception, addressing language bias in traditional multimodal reasoning by detecting relevant image regions for precise answers, using reasoning data and an inference pipeline with visual replay.", "keywords": ["VLM", "MultiModal", "Cot"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40ec46a3967c5948eb7e79a579c88d8c93babff0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces VGR, a multimodal large language model framework that augments chain-of-thought reasoning with selective visual grounding via a dynamic visual memory replay mechanism. VGR detects task-relevant image regions during reasoning, retrieves compressed high-resolution visual features from a maintained visual memory pool, and injects them into the LLM input to improve fine-grained visual-linguistic inference. The experiments on eight visual reasoning benchmarks demonstrate that VGR significantly outperforms existing vanilla, SoTA VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- VGR is a well-motivated and thoughtfully designed approach. The dynamic visual memory replay mechanism is a clever way to enhance the model's ability to focus on relevant image regions during reasoning, which is crucial for fine-grained visual-linguistic tasks.\n- The experiments are well-executed and comprehensive, covering a wide range of visual reasoning benchmarks and a series of ablation studies. The results convincingly demonstrate the effectiveness of VGR, with significant performance improvements over existing methods."}, "weaknesses": {"value": "- The novelty of this paper is not clearly articulated, especially in relation to very similar prior work on CogCoM [1] and Chain-of-Focus [2]. Both CogCoM [1] and Chain-of-Focus [2] also focus on enhancing visual reasoning by incorporating dynamic, bounding box-based image retrieval mechanisms. The authors should clearly differentiate VGR from these existing approaches and highlight the unique contributions of their method. The reviewer recognizes L060-061 as an attempt to do so by saying \"our dataset empowers models to autonomously attend to arbitrary visual regions during reasoning.\", but the same can be said about CogCoM and Chain-of-Focus.\n- The experiment section could be strengthened by providing a bit more qualitative analysis into why VGR outperforms other methods. For example, sample a handful of examples (e.g., 10-20) where VGR succeeds but other methods fail, and analyze what aspects of VGR's design contribute to its success in those cases. This would provide deeper insights into the strengths of the proposed approach.\n\n[1] CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning. https://arxiv.org/abs/2402.04236\n[2] Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL. https://arxiv.org/abs/2505.15436"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aaNfXLb9JB", "forum": "kDhAiaGzrn", "replyto": "kDhAiaGzrn", "signatures": ["ICLR.cc/2026/Conference/Submission23089/Reviewer_qwzK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23089/Reviewer_qwzK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890255449, "cdate": 1761890255449, "tmdate": 1762942506884, "mdate": 1762942506884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VGR, a multimodal reasoning framework that lets an MLLM “replay” visual memory on demand during chain-of-thought. It depicts a mechanism when the model emits a special replay signal with a bounding box, VGR fetches the corresponding image tokens from a high-resolution feature map and appends them into the ongoing context for grounded reasoning. The authors also curate VGR-SFT, a supervised dataset mixing reasoning traces with explicit region grounding produced via a cold-start model, rejection sampling, and a smaller “annotator” model for scale. On LLaVA-NeXT-7B backbones, VGR reports strong gains on perception-heavy benchmarks while using ~30% of the visual tokens versus the baseline, with ablations isolating the contributions of replay and a detection loss for box accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Simple, modular mechanism with clear intuition.**\nTurning visual inspection into an explicit token-level replay step is elegant and easy to graft onto LLaVA-style stacks. The control token and parser logic are well specified.\n\n2. **Efficiency/performance tradeoff.** \nThe expand-then-compress feature design + selective replay cuts visual tokens by ~70% yet improves perception tasks\n\n3. **Data contribution.**\n VGR-SFT explicitly couples reasoning with self-proposed RoIs, avoiding manual box bias and encouraging “ground-then-reason” behavior. The pipeline (cold start → reject sampling → annotator) is thoughtful."}, "weaknesses": {"value": "1. **Replay supervision and parser brittleness.**\n\n The method depends on the model outputting valid coordinates via \\<sot\\>[x1,y1,x2,y2]\\<eot\\>; while a detection loss is added, the paper doesn’t quantify (i) the rate of invalid boxes and (ii) sensitivity to coordinate quantization / resolution scaling. Metrics for “replay success rate” and its effect on final accuracy would strengthen the claim.\n\n2. **Comparisons to other zoom-then-behavior baselines.**\n\nZoomeye/Chain-of-Spot and guided-search methods aim at similar zoom-then-reason behavior. While the appendix notes a comparison in cost and accuracy,  the main paper doesn’t present head-to-head accuracy/latency plots vs. these strategies under identical token budgets. A direct comparison table would help isolate the value of feature replay vs. re-encoding crops [1-3]\n\n3.  **Latency comparison.**\n\nThe paper provides a comparison on vision token cost, but as the method usually requires additional bounding box tokens and recall of the visual feature map, adding a inference time latency cost comparison compared with LLaVA-Next and other observe-then-reason baseline will help to understand the efficiency of the method.\n\n\n[1]. Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL\n\n[2]. Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models\n\n[3]. ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration."}, "questions": {"value": "1. What fraction of generations emit at least one valid replay signal? What’s the average number of replay events per instance?\n\n2. How often are boxes invalid or out of bounds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9SVX6DYjCf", "forum": "kDhAiaGzrn", "replyto": "kDhAiaGzrn", "signatures": ["ICLR.cc/2026/Conference/Submission23089/Reviewer_ysjS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23089/Reviewer_ysjS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912405290, "cdate": 1761912405290, "tmdate": 1762942506642, "mdate": 1762942506642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VGR (Visual Grounded Reasoning), a novel multimodal large language model (MLLM) designed to address language bias and limited visual detail understanding in existing multimodal chain-of-thought (CoT) reasoning approaches. VGR introduces a dynamic visual memory replay mechanism that enables the model to actively detect and retrieve visual tokens from key image regions during reasoning, mimicking human visual cognition. The authors also construct a large-scale supervised fine-tuning dataset (VGR-SFT) through a three-stage pipeline (cold-start generation, reject sampling, and annotation model scaling) to teach the model visual grounding and reasoning integration. Extensive experiments on benchmarks like MMStar, ChartQA, and AI2D show that VGR outperforms the LLaVA-NeXT-7B baseline with only 30% of the image tokens, validating its efficiency and effectiveness in visual-linguistic reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The proposed dynamic visual memory replay mechanism is innovative, as it enables on-demand retrieval of key visual regions during reasoning, effectively mitigating language bias and enhancing fine-grained visual understanding.\n\n(2) The VGR-SFT dataset construction pipeline is rigorous and scalable, avoiding manual annotation bias through model-generated grounding areas and ensuring data quality via reject sampling and rewriting.\n\n(3) The experimental design is comprehensive, including extensive comparisons with state-of-the-art models, ablation studies on core components (e.g., detection loss, visual memory replay), and efficiency analysis, providing solid evidence for the method's superiority."}, "weaknesses": {"value": "(1) The choice of the 72B MLLM for cold-start data generation lacks comparative analysis; the authors do not explain why smaller models (e.g., 13B or 34B) were not considered, nor do they demonstrate the advantages of the 72B model in generating initial visual grounding data.\n\n(2) The use of Doubao1.5-VL's API for reject sampling lacks detailed parameter settings (e.g., temperature, top-p, response timeout) and verification criteria, making it difficult for readers to reproduce the data filtering process.\n\n(3) The setting of β=2 in the detection loss (combining L1 and GIoU loss) is not supported by ablation experiments; the authors fail to test other β values (e.g., 1, 3, 4) to verify whether 2 is the optimal choice.\n\n(4) The comparison with ZoomEye is incomplete, as it only reports results on V* Bench and HR-Bench 8K, lacking performance data on other key benchmarks (e.g., TextVQA, InfoQA) and detailed analysis of reasoning logic differences between the two methods.\n\n(5) The combination experiments of visual encoders and LLMs are limited; the authors only test a few combinations (e.g., Qwen2.5+SigLIP, Vicuna+CLIP) and do not explore more advanced encoders (e.g., InternViT-6B, EVA-CLIP) or LLMs (e.g., LLaMA 3), limiting the demonstration of the framework's generalizability.\n\n(6) The rationale for setting the maximum number of cropped images to 64 during test-time token scaling is unclear; the authors do not explain whether this threshold is determined by empirical testing or theoretical analysis, nor do they show performance changes when exceeding this threshold.\n\n(7) The contribution of each subset in the VGR-SFT dataset (e.g., AI2D, GQA, ChartQA) to the model's overall performance is not analyzed; it is impossible to determine which data types drive the performance improvement, hindering the understanding of dataset design rationality.\n\n(8) The trigger mechanism for replay signal generation during inference is not detailed; the authors do not clarify how the model decides when to retrieve visual memory (e.g., based on semantic cues in the reasoning chain or statistical thresholds), leading to ambiguity about the core logic of dynamic replay.\n\n(9) Key training hyperparameters are missing, such as batch size, number of training epochs, weight decay, and learning rate scheduling strategy for both pre-training and fine-tuning stages, which is critical for reproducibility.\n\n(10) The model's performance on low-resolution images (e.g., below 336×336) is not evaluated; given that real-world images often have varying resolutions, this omission limits the assessment of the model's practical applicability.\n\n(11) There is no comparison with cutting-edge MLLMs like GPT-4V, Gemini Pro Vision, or Claude 3 Opus; the authors only compare with open-source models, failing to demonstrate VGR's competitiveness against commercial state-of-the-art systems.\n\n(12) The quantification of language bias reduction is insufficient; the authors claim to mitigate language bias but do not use specific metrics (e.g., bias score, fairness indicators) to measure the degree of reduction, making the claim lack objective support.\n\n(13) The details of high-resolution cropping in visual memory pool construction are vague; the authors do not specify key parameters such as crop overlapping ratio, number of crops per image, and selection criteria for crop positions, affecting the reproducibility of the visual memory module.\n\n(14) The annotation error rate of the VGR-SFT dataset is not reported; the authors do not explain how they identified and handled annotation errors during data curation, raising concerns about data quality.\n\n(15) Inference speed on different hardware platforms (e.g., A100, RTX 3090, RTX 4090) is not provided; the authors only report average reasoning time per question on specific benchmarks, failing to reflect the model's deployment feasibility on resource-constrained devices.\n\n(16) Performance in multi-turn reasoning scenarios is not tested; the authors only evaluate single-turn question answering, while real-world applications often require continuous reasoning based on previous interactions.\n\n(17) The specific thresholds for format verification and correctness verification in reject sampling are unclear; for example, the ANLS threshold for closed-ended tasks and the semantic alignment score threshold for open-ended tasks are not specified, making it difficult to replicate the data filtering process.\n\n(18) The detailed structure of the MLP in the detection head is missing; the authors do not mention the number of layers, hidden size, activation function, or output dimension, which is essential for understanding the region detection mechanism.\n\n(19) Generalization ability on cross-domain images (e.g., medical images, remote sensing images, satellite images) is not evaluated; the model is only tested on conventional VQA and OCR datasets, limiting the assessment of its applicability to specialized fields.\n\n(20) The basis for selecting the MLLM used in data rewriting is not explained; the authors do not compare different models (e.g., LLaMA 2, Mistral) for rewriting effectiveness, leading to uncertainty about the optimal choice for this task.\n\n(21) The selection of 2×2 and 4×4 pooling strategies for visual token compression lacks theoretical support; the authors do not explain why these pooling sizes are chosen over others (e.g., 3×3), nor do they provide ablation results on pooling strategies.\n\n(22) The impact of model parameter size (7B vs 13B) on performance is not analyzed in depth; the authors only report basic results but fail to discuss how parameter scaling affects the trade-off between performance and computational cost.\n\n(23) The balance of the VGR-SFT dataset across different task types is not addressed; the authors do not clarify whether the data distribution is balanced (e.g., proportion of OCR vs. general VQA tasks) or how imbalance is handled if present.\n\n(24) Metrics for evaluating the accuracy of region selection in dynamic visual memory replay are missing; the authors do not specify how they measure whether the model selects the correct key regions, making it difficult to assess the effectiveness of the grounding mechanism.\n\n(25) The handling of duplicate samples in the training data is not explained; the authors do not mention whether duplicate samples exist, how they were detected, or how they were processed (e.g., removal, merging), which may affect training stability.\n\n(26) Performance in few-shot learning scenarios is not tested; the authors only use full-scale training data, failing to demonstrate the model's ability to adapt to low-data regimes, which is important for real-world applications.\n\n(27) The comparison with Chain-of-Spot is insufficient; the authors do not discuss differences in reasoning logic, computational complexity, or performance on different task types, limiting the understanding of VGR's advantages over similar interactive reasoning methods.\n\n(28) Latency introduced by visual memory replay is not discussed; the authors do not quantify the additional time cost of retrieving and processing visual tokens, which is critical for real-time applications.\n\n(29) Interpretability analysis is limited; beyond region annotation, the authors do not provide other interpretability methods (e.g., attention visualization, reasoning chain decomposition) to explain how the model integrates visual and linguistic information.\n\n(30) Detailed license information for datasets is missing; the authors only state that datasets are publicly available but do not specify license types (e.g., MIT, CC BY-SA) or any restrictions on use, raising potential copyright concerns.\n\n(31) The fusion method of pre-training data (LLaVA-558K) and fine-tuning data (LLaVA-NeXT-770K + VGR-SFT) is not clarified; the authors do not explain whether the data is concatenated, mixed in batches, or processed with different weights, affecting reproducibility.\n\n(32) Robustness on complex scenarios (e.g., occluded images, blurred images, low-light images) is not evaluated; the authors only test on standard datasets, failing to demonstrate the model's ability to handle real-world noise.\n\n(33) The normalization method of bounding box coordinates in GIoU loss is not explained; the authors do not specify whether coordinates are normalized to [0,1] based on image size or other standards, leading to ambiguity in loss calculation.\n\n(34) The relevance between visual cues and language reasoning in the dataset is not evaluated; the authors do not measure how well the annotated visual regions align with the reasoning chain, affecting the assessment of dataset quality.\n\n(35) Training time and computational resource consumption are not reported; the authors do not specify GPU hours, number of GPUs used, or total training time, making it difficult for researchers with limited resources to replicate the work.\n\n(36) The potential of combining VGR with RL methods (e.g., GRPO) is not explored; the authors only use supervised fine-tuning, failing to discuss whether RL can further enhance the model's reasoning and grounding capabilities.\n\n(37) The impact of reasoning chain length on performance is not analyzed; the authors do not test whether longer or shorter reasoning chains affect accuracy or efficiency, limiting the understanding of optimal reasoning chain design.\n\n(38) The generalization ability of the annotation model during dataset scaling is not evaluated; the authors do not measure how well the 14B annotation model performs on unseen data types, raising concerns about dataset quality during scaling.\n\n(39) The impact of reducing visual tokens by 70% on the model's ability to capture complex visual information is not discussed; the authors do not clarify whether the token reduction leads to information loss in complex scenes (e.g., dense objects, complex layouts).\n\n(40) Performance in multilingual scenarios is not tested; the authors only use English questions and images, failing to demonstrate the model's applicability to non-English languages, which is important for global use."}, "questions": {"value": "*To facilitate discussions during the Rebuttal phase, authors are advised to respond point-by-point (indicating the question number).*\n\n(1) Could you provide a comparative analysis of different cold-start models (e.g., 13B, 34B, 72B MLLMs) to justify why the 72B model was chosen for initial data generation? Please include metrics such as data quality (e.g., grounding accuracy), generation speed, and reject rate.\n\n(2) What are the specific parameter settings (e.g., temperature, top-p, max response length) and verification criteria of Doubao1.5-VL used in the reject sampling pipeline? Could you provide the exact prompts used for format verification, correctness verification, and visual grounding verification?\n\n(3) Why is β set to 2 in the detection loss? Could you conduct ablation experiments with different β values (e.g., 1, 3, 4) and present the results to confirm that 2 is the optimal choice?\n\n(4) Could you extend the comparison with ZoomEye to more benchmarks (e.g., TextVQA, InfoQA, DocVQA) and provide detailed metrics including accuracy, reasoning time, and visual token usage? Also, please analyze the fundamental differences in reasoning mechanisms between VGR and ZoomEye.\n\n(5) Have you tested more advanced visual encoders (e.g., InternViT-6B, EVA-CLIP-18B) or LLMs (e.g., LLaMA 3 8B/70B, Mistral 8X7B) with the VGR framework? If so, please provide the experimental results; if not, please explain the reasons and discuss the potential impact on performance.\n\n(6) What is the rationale for setting the maximum number of cropped images to 64 during test-time token scaling? Could you present performance curves with varying numbers of crops (e.g., 20, 40, 64, 80) to demonstrate the trade-off between token count and performance?\n\n(7) Could you analyze the contribution of each subset in the VGR-SFT dataset (e.g., AI2D, GQA, ChartQA) to the model's performance on different benchmarks? Please present ablation results showing the model's performance when trained on individual subsets.\n\n(8) How does the model determine when to generate the replay signal during inference? Please explain the trigger mechanism in detail, including whether it relies on semantic cues, statistical thresholds, or other factors, and provide illustrative examples.\n\n(9) Could you provide complete training hyperparameters, including batch size, number of training epochs, weight decay, learning rate scheduling (e.g., warm-up steps, decay rate), and optimizer type (e.g., AdamW, SGD) for both pre-training and fine-tuning stages?\n\n(10) Have you evaluated the model's performance on low-resolution images (e.g., 224×224, 112×112)? Please present the results and discuss how resolution affects the model's grounding and reasoning capabilities.\n\n(11) Could you provide a comparison with commercial state-of-the-art MLLMs (e.g., GPT-4V, Gemini Pro Vision, Claude 3 Opus) on the same benchmarks? If direct comparison is not feasible, please explain the constraints and provide indirect evidence (e.g., relative performance to open-source baselines vs. commercial models).\n\n(12) How do you quantify the reduction of language bias in VGR? Please define specific metrics (e.g., bias score, fairness gap) and present comparative results between VGR and the LLaVA-NeXT baseline on these metrics.\n\n(13) Please detail the high-resolution cropping strategy in visual memory pool construction, including parameters such as crop size, overlapping ratio, number of crops per image, and selection criteria for crop positions. Could you also explain how this strategy balances detail preservation and computational efficiency?\n\n(14) What is the annotation error rate of the VGR-SFT dataset? How did you detect and handle these errors during data curation? Please provide statistics on error types (e.g., incorrect bounding boxes, misaligned labels) and their distribution across datasets.\n\n(15) Could you report the model's inference speed (e.g., tokens per second, questions per minute) on different hardware platforms (e.g., A100, RTX 3090, RTX 4090, CPU)? Please also include memory consumption (e.g., GPU VRAM usage) for different model sizes (7B, 13B).\n\n(16) Have you tested the model's performance in multi-turn reasoning scenarios? Please design a set of multi-turn tasks (e.g., sequential visual reasoning, follow-up questions) and present the results, including accuracy and consistency across turns.\n\n(17) What are the specific thresholds for format verification and correctness verification in reject sampling? For example, what ANLS threshold is used for closed-ended tasks, and what semantic alignment score threshold is used for open-ended tasks? Please justify these threshold choices.\n\n(18) Please provide the detailed structure of the MLP in the detection head, including the number of layers, hidden size, activation function, input/output dimensions, and initialization method. Could you also explain how this structure is optimized for bounding box regression?\n\n(19) Have you evaluated the model's generalization ability on cross-domain images (e.g., medical images from ChestX-ray14, remote sensing images from NWPU-RESISC45)? Please present the results and discuss the challenges of adapting VGR to specialized domains.\n\n(20) What is the basis for selecting the MLLM used in data rewriting? Could you compare the rewriting effectiveness of different models (e.g., LLaMA 2 7B, Mistral 7B, Qwen2 7B) using metrics such as reasoning coherence, format compliance, and ground-truth alignment?\n\n(21) Why are 2×2 and 4×4 pooling strategies chosen for visual token compression? Could you conduct ablation experiments with other pooling sizes (e.g., 3×3, 5×5) and present results on performance, computational cost, and information preservation?\n\n(22) How does the model's parameter size (7B vs. 13B) affect the trade-off between performance and computational cost? Please provide detailed metrics including training time, inference speed, memory usage, and accuracy across benchmarks for both sizes.\n\n(23) Is the VGR-SFT dataset balanced across different task types? Please provide a detailed breakdown of data distribution (e.g., percentage of OCR, general VQA, science QA tasks) and, if imbalanced, explain how you addressed it (e.g., oversampling, weighted loss).\n\n(24) What metrics do you use to evaluate the accuracy of region selection in dynamic visual memory replay? Please define the metrics (e.g., IoU with ground-truth regions, precision/recall of key region selection) and present comparative results between VGR and the baseline.\n\n(25) How did you handle duplicate samples in the training data? Please explain the detection method (e.g., hash-based, semantic similarity) and processing strategy (e.g., removal, merging), and provide statistics on the number of duplicates found and processed.\n\n(26) Have you tested the model's performance in few-shot learning scenarios? Please design experiments with varying training data sizes (e.g., 1K, 10K, 50K samples) and present results on key benchmarks, comparing VGR with other few-shot multimodal models.\n\n(27) Could you provide a more in-depth comparison with Chain-of-Spot, including reasoning logic, computational complexity (e.g., number of forward passes), performance on spot-based visual search tasks, and adaptability to high-resolution images?\n\n(28) Does visual memory replay introduce additional latency during inference? If so, could you quantify the latency overhead (e.g., percentage increase in reasoning time) and discuss potential optimizations (e.g., precomputing visual tokens, parallel processing)?\n\n(29) Besides region annotation, could you provide other interpretability analyses (e.g., attention heatmaps, reasoning chain decomposition, visual-linguistic alignment scores) to explain how the model integrates visual and linguistic information for reasoning?\n\n(30) Could you provide detailed license information for all datasets used in the study (e.g., LLaVA-558K, VGR-SFT subsets)? Please specify any restrictions on use, redistribution, or commercial application to address copyright concerns.\n\n(31) How are the pre-training data (LLaVA-558K) and fine-tuning data (LLaVA-NeXT-770K + VGR-SFT) fused during training? Please explain whether the data is concatenated, mixed in batches, or assigned different weights, and justify the chosen method.\n\n(32) Have you evaluated the model's robustness on complex scenarios (e.g., occluded images, blurred images, low-light images)? Please use standard robustness benchmarks (e.g., ImageNet-C, VQA-C) and present results on accuracy degradation compared to clean images.\n\n(33) Please explain the normalization method of bounding box coordinates in the GIoU loss calculation. Are coordinates normalized to [0,1] based on the original image size, cropped patch size, or other standards? Please provide a mathematical formulation if applicable.\n\n(34) How do you evaluate the relevance between visual cues and language reasoning in the VGR-SFT dataset? Please define a relevance metric (e.g., semantic similarity between region labels and reasoning steps) and present statistics on the dataset's relevance distribution.\n\n(35) Could you report the detailed computational resource consumption for training VGR, including the number of GPUs used (e.g., 8×A100), total GPU hours, power consumption, and training time for pre-training and fine-tuning stages separately?\n\n(36) Have you explored combining VGR with reinforcement learning (RL) methods like GRPO? Please provide preliminary results (if any) on whether RL can further improve the model's reasoning accuracy, grounding precision, or efficiency. If not, please discuss the technical challenges and potential benefits.\n\n(37) How does the length of the reasoning chain affect the model's performance? Could you conduct ablation experiments with varying reasoning chain lengths (e.g., short, medium, long) and present results on accuracy, efficiency, and language bias reduction?\n\n(38) How do you evaluate the generalization ability of the 14B annotation model during dataset scaling? Please present metrics such as grounding accuracy, reasoning coherence, and reject rate on unseen data types compared to the cold-start 72B model.\n\n(39) Does reducing visual tokens by 70% lead to information loss in complex visual scenes (e.g., dense objects, complex layouts)? Please design experiments with such scenes and present comparative results between VGR and the baseline (using full tokens) on detail-rich benchmarks.\n\n(40) Have you tested the model's performance in multilingual scenarios? Please evaluate VGR on non-English benchmarks (e.g., VQA in Chinese, German OCR tasks) and present results on accuracy, grounding precision, and language bias compared to English-only performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XzScUnmDGs", "forum": "kDhAiaGzrn", "replyto": "kDhAiaGzrn", "signatures": ["ICLR.cc/2026/Conference/Submission23089/Reviewer_R29m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23089/Reviewer_R29m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920278978, "cdate": 1761920278978, "tmdate": 1762942506448, "mdate": 1762942506448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the VGR framework, designed to support visual reasoning through visual memory replay during the thinking process. The main contribution of VGR lies in its visual memory replay module, which retrieves and replays visual information from memory. During inference, the model can request projections of image patches from memory, enabling it to revisit relevant visual regions and perform more focused reasoning. This approach allows the model to start with a smaller set of snapshot embeddings (around 30% of the original LLaVa) without sacrificing quality.\n\nTo train VGR, the authors first use an existing model to “cold-start” a dataset: given an image and a question from existing benchmarks (e.g., AI2D), the model generates a reasoning chain and answer. A rejection strategy is then applied to filter out incorrect samples using formal verification, correctness checks, and visual grounding validation. A smaller model is trained on the verified cold-start data and later used as an annotator to create a large-scale dataset. This larger dataset is then used with SFT methods to train the final VGR model.\n\nThe evaluation section is well structured, consisting of two main parts: (1) main results demonstrating the strong performance of the VGR framework across a range of datasets — both those included in the cold-start data and new, out-of-domain benchmarks such as MMStar and TableVQA — and (2) an extensive series of ablation studies exploring different components of VGR, including its reasoning process, memory module, and loss function."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Overall, the paper is clearly written and methodologically solid. The assumptions are well stated, and the authors do an excellent job of detailing their experimental setup and reasoning process. The evaluation section is one of the strongest aspects of the work, especially the ablation study, which provides valuable insight and helps position the paper as a meaningful contribution to the field."}, "weaknesses": {"value": "There are a few points that could use more clarification.\nFirst, the paper doesn’t clearly separate in-domain and out-of-domain benchmarks in Table 2. I tried to cross-check the datasets listed in Table 1 myself to identify which ones were new, but I would still like the authors to explicitly confirm whether the evaluations indeed include out-of-domain data. Without this distinction, it’s hard to fully assess the framework’s generalization ability.\n\nSecond, the main comparisons focus on relatively smaller VLMs. It would be interesting to see how VGR performs when integrated with larger models. If there are challenges or limitations in doing so, it would be helpful for the authors to discuss them. Otherwise, including such results would strengthen the paper’s claims about scalability.\n\nFinally, as shown in Table 10, VGR’s main drawback appears to be its computational cost (e.g., 1s runtime for LLaVa-NeXT vs. 7s for VGR). Apart from a brief note in Appendix A.2, the paper doesn’t explore this issue in depth. A more thorough discussion of runtime and hardware costs would make the evaluation more balanced and practical."}, "questions": {"value": "Q1) Why did the authors choose not to formulate the memory access as a tool call, rather than introducing special tokens? To me, that seems like a potentially more straightforward approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oQ9HCK52Op", "forum": "kDhAiaGzrn", "replyto": "kDhAiaGzrn", "signatures": ["ICLR.cc/2026/Conference/Submission23089/Reviewer_CQgV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23089/Reviewer_CQgV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974309660, "cdate": 1761974309660, "tmdate": 1762942506169, "mdate": 1762942506169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VGR (Visual Grounded Reasoning), a framework that aims to enhance multimodal reasoning in vision-language models by incorporating a visual memory replay mechanism. The key idea is to allow the model to explicitly reference relevant visual regions during the reasoning process through a controlled replay signal (<sot>[x1,y1,x2,y2]<eot>). This design reduces the number of visual tokens, improves efficiency, and enhances interpretability. The authors conduct extensive experiments on multiple benchmarks (e.g., AI2D, ChartQA, MMStar), showing consistent improvements over LLaVA-NeXT baselines with fewer visual tokens. The paper also provides detailed ablation studies and efficiency analyses. While the empirical results are strong, the conceptual novelty is somewhat limited, and certain aspects of the method and presentation could be improved."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper includes a large number of experiments across diverse benchmarks with fair comparison to strong baselines, along with clear ablations and efficiency metrics.\n\n2. The proposed replay mechanism and token-efficient design are well implemented and practically useful for multimodal reasoning systems."}, "weaknesses": {"value": "1. The work is technically solid; however, the contribution in terms of novelty could be clarified further. The concept of visual CoT (chain-of-thought grounded in image regions) has been explored in prior studies (e.g., VisualCoT[1], SketchPad[2], Refocus[3]), which are currently only briefly discussed. Including a more detailed comparison could help position the paper within the existing literature. Among these, approaches most similar to “visual CoT” could be highlighted more explicitly, both conceptually and empirically.\n2. The paper adopts LLaVA’s any-resolution encoding to handle arbitrary image sizes. It would be helpful to provide additional discussion on why this design choice is preferred over an alternative strategy such as cropping the image into regions, which may be more architecture-agnostic and computationally straightforward. Clarifying this could strengthen the reader’s understanding of the method’s design trade-offs.\n3. The title emphasizes “Visual Grounded Reasoning.” It may be useful to clarify the scope of reasoning in this context. Prior studies (e.g., DeepSeek-R1[2]) suggest that reasoning ability in large language models often arises from reinforcement learning or dedicated reasoning-phase training, whereas supervised fine-tuning primarily encourages pattern learning or imitation. Explicitly situating the current work within this context could help manage expectations regarding the type of reasoning capability demonstrated.\n4. The proposed replay mechanism assumes that accurately predicting relevant visual regions contributes to improved reasoning. Currently, the paper does not examine how often the replay regions are correctly predicted, nor how reasoning performance is affected when the replay predictions are inaccurate. A quantitative analysis, such as tracking replay prediction accuracy during training and correlating it with task performance, could provide stronger evidence for the causal role of replay. This would also help determine whether the model genuinely relies on visual replay or mainly benefits from additional supervision signals.\n5. This paper only conducts experiments on the LLaVA-Next backbone. It would be better to conduct experiments on other backbones, such as Qwen-VL or InternVL."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fEo49Cyvbw", "forum": "kDhAiaGzrn", "replyto": "kDhAiaGzrn", "signatures": ["ICLR.cc/2026/Conference/Submission23089/Reviewer_joFM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23089/Reviewer_joFM"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135669957, "cdate": 1762135669957, "tmdate": 1762942505939, "mdate": 1762942505939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}