{"id": "IWQNhR6poq", "number": 12143, "cdate": 1758205945372, "mdate": 1763724268819, "content": {"title": "Improved probabilistic regression using diffusion models", "abstract": "Probabilistic regression models the entire predictive distribution of a response variable, offering richer insights than classical point estimates and directly allowing for uncertainty quantification. While diffusion-based generative models have shown remarkable success in generating complex, high-dimensional data, their usage in general regression tasks often lacks uncertainty-related evaluation and remains limited to domain-specific applications. We propose a novel diffusion-based framework for probabilistic regression that learns predictive distributions in a nonparametric way. More specifically, we propose to model the full distribution of the diffusion noise, enabling adaptation to diverse tasks and enhanced uncertainty quantification. We investigate different noise parameterizations, analyze their trade-offs, and evaluate our framework across a broad range of regression tasks, covering low- and high-dimensional settings. For several experiments, our approach shows superior performance against existing baselines, while delivering calibrated uncertainty estimates, demonstrating its versatility as a tool for probabilistic prediction.", "tldr": "We propose a new loss function for diffusion models that enables flexible noise modeling, improving both performance and uncertainty quantification in regression settings.", "keywords": ["Diffusion models", "Uncertainty Quantification", "Probabilstic Regression"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c32c02574c80f3c4a7e7bf475cc9035e475cea18.pdf", "supplementary_material": "/attachment/e7b1eff4deb1236c5193d499526bd1bbcf8e85e1.zip"}, "replies": [{"content": {"summary": {"value": "To enable better uncertainty quantification and calibrated probabilistic predictions across regression tasks, this paper introduces a diffusion-based probabilistic regression framework that learns the full distribution of diffusion noise instead of predicting only its mean (as in standard DDPM/DDIM). \n\nThe key points of this paper includes: \n\n1. Reformulate diffusion regression as learning the distribution $p_\\epsilon(\\epsilon_t | x_t)$, using strictly proper scoring rules (e.g., energy score or kernel score) to ensure consistency and calibration.\n2. Propose several parameterizations of the diffusion noise distribution with:\n- Univariate Gaussian\n- Gaussian Mixture\n- Multivariate Gaussian (low-rank + diagonal)\n3. Demonstrate that this formulation yields closed-form reverse sampling, preserving the tractability of DDPM.\n4. Empirically validate across UCI benchmarks, autoregressive prediction, and monocular depth estimation, showing consistent performance and better uncertainty calibration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is conceptually novel and rigororous, and it connects diffusion modeling with the broader literature on probabilistic scoring rules and distributional regression. The proposed method is versatile and general, which just need no or at least minimal architectural modification: existing diffusion backbones (e.g., DDPM, DDIM, U-Net) can be reused. Results across multiple domains support generality and robustness, and overall the paper is well-written, with clear derivations and equations linking the proposed loss, parametrizations, and reverse process formulation."}, "weaknesses": {"value": "The paper still exposes several limitations, which is listed below and hope the authors could address:\n\n1. While three variants are explored (univariate, mixture, multivariate), the paper lacks deeper insight into when and why each performs best. It would benefit if the authors provide with guidance to the readers on how to choose these parameterizations, maybe according to the data distribution? \n\n2. Although claimed efficient, explicit runtime comparisons (e.g., against DDPM or nonparametric) are limited. \n\n3. It remains unclear whether these advantages persist at scale (e.g., large diffusion backbones, complex regression scenarios).\n\n4. The theoretical results and contribution are incremental relative to Bortoli et al. (2025) and is conceptually close to their work. Although the authors categorize it as concurrent work, Bortoli et al. (2025) appears in ICML 2025, which is hardly considered as concurrent. The paper could more sharply differentiate its contributions beyond being a computationally efficient parameterized alternative."}, "questions": {"value": "1. Why specifically use the energy and kernel scores?\n\n2. Are there any parameterization trade-offs, e.g., how does performance scale with K? \n\n3. Any observed training instabilities not reported due to modeling the full noise distribution?\n\n4. Could this approach extend naturally to discrete or hybrid data (e.g., language, tabular categorical features), like done in CARD (Han et al., 2022)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rbgmDhcnQ5", "forum": "IWQNhR6poq", "replyto": "IWQNhR6poq", "signatures": ["ICLR.cc/2026/Conference/Submission12143/Reviewer_vvhz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12143/Reviewer_vvhz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952553999, "cdate": 1761952553999, "tmdate": 1762923103201, "mdate": 1762923103201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a diffusion based framework for probabilistic regression that, instead of predicting only a point estimate, learns the full conditional distribution of the per step diffusion noise using strictly proper scoring rules. Concretely, it parameterizes the noise distribution (e.g., diagonal Gaussian, Gaussian mixtures, and multivariate Gaussian with efficient approximations) so the model can balance expressivity and compute, while still admitting closed form reverse transitions for straightforward sampling. Across diverse tasks—UCI tabular regression, autoregressive flow/weather prediction, and monocular depth—the approach reports better predictive accuracy and notably improved calibration/uncertainty estimates compared to standard diffusion baselines. Overall, it reframes diffusion regression as learning a flexible noise distribution to produce calibrated predictive distributions end to end."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Flexible noise modeling with proper scoring rules yields calibrated predictive distributions instead of mere point estimates.\n2. Closed-form reverse transitions allow efficient sampling and easy integration with standard diffusion samplers.\n3. Modular parameterizations (diag Gaussian, mixtures, multivariate/low-rank) trade off accuracy vs. compute without redesigning the pipeline.\n4. Broad empirical scope (tabular, autoregressive flow/weather, depth) shows consistent CRPS/calibration gains over diffusion baselines."}, "weaknesses": {"value": "1. The paper's core premise—learning the full noise distribution $ p_{\\theta}^{\\epsilon}(\\cdot|x_t) $ via proper scoring rules—was concurrently proposed by Bortoli et al. (2025). This work's primary contribution is thus the specific instantiation with (mixture) Gaussian heads. This is further narrowed by the fact that the simplest case (univariate Gaussian) is, as the authors note, conceptually equivalent to prior work on variance learning.\n\n2. The reported improvements are inconsistent and, in some cases, negligible. On UCI benchmarks, CRPS gains vary widely, from substantial (e.g., Naval: $ -32 % $) to nonexistent (e.g., Wine: $ 0 % $). The method can also underperform the baseline in RMSE on some datasets (e.g., Yacht). Likewise, on depth estimation, the CRPS gains are marginal (e.g., $ \\approx -0.45 % $ on KITTI, $ \\approx -2.5\\% $ on DIODE) and even show a slight regression on ETH3D ($ \\approx +0.7\\% $). This mixed evidence weakens the claim of general applicability.\n\n3.  The method's calibration claims are undermined by the reliance on a post-hoc hyperparameter, $ \\tau $. The authors concede the model is \"over-conservative\" and that achieving near-nominal coverage—as well as the largest CRPS/ES gains—requires a small $ \\tau \\approx 0.05 $. This is a significant, unprincipled deviation from the underlying DDIM framework, and no method for selecting this critical parameter is proposed.\n\n4. The empirical evaluation lacks the rigor expected for a high-impact venue. The authors admit to only \"minor hyperparameter tuning\" for comparators and an inability to conduct an \"extensive statistical evaluation\" of their own hyperparameters due to cost. Furthermore, computational comparisons are dismissed as \"rough estimates\". Without a rigorous and fair benchmark, the reported performance margins are difficult to interpret confidently."}, "questions": {"value": "See the weakness section and the following:\n\n1.  Given the method is demonstrably \"over-conservative\" at $\\tau=1$, how can the reliance on an unprincipled, post-hoc $\\tau \\approx 0.05$ to correct calibration be justified? Can the authors provide a principled selection rule for $\\tau$ and re-evaluate all key metrics using it, rather than presenting ad-hoc results?\n\n2.  How do the authors explain the highly variable, and in some cases negligible or negative, empirical gains (e.g., 0% CRPS on Wine, RMSE regression on Yacht, marginal/negative CRPS on KITTI/ETH3D)? Can rigorous ablations be provided to demonstrate that these gains are not mere artifacts of added parameters, especially for the depth estimation tasks?\n\n3.  Given the admission of \"minor hyperparameter tuning\" and \"not optimized\" timing, how can the claims of superiority be validated? Can the authors strengthen the paper by providing comparisons against properly tuned, strong UQ baselines (e.g., variance-learning DDPMs, MDNs) under a fair, matched-compute framework?\n\n4.  How can the framework be considered \"principled\" when it currently lacks practical guidance for model selection and the claims of epistemic uncertainty separation are purely qualitative? Can the authors substantiate these claims by providing a quantitative heuristic for head selection and either a formal analysis or a quantitative benchmark for the UQ decomposition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vnkd94VCbm", "forum": "IWQNhR6poq", "replyto": "IWQNhR6poq", "signatures": ["ICLR.cc/2026/Conference/Submission12143/Reviewer_wwWQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12143/Reviewer_wwWQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957403037, "cdate": 1761957403037, "tmdate": 1762923102721, "mdate": 1762923102721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends diffusion models to handle the probabilistic regression setting - that is, when one models the full conditional distribution of the outputs given the inputs, not just the conditional mean. This is done by proposing a proper scoring rule based objective, and adapting the diffusion model accordingly using a certain Gaussian mixture component. The framework is evaluated on UCI datasets as well as some PDE/weather type benchmarks, and for depth estimation."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 4}, "strengths": {"value": "This paper is somewhat tangential to my own research, so some comments may not be perfectly informed. With this said, there are several things I really like about this paper:\n* **Very important problem.** The ability to model the full distribution of inputs automatically is a major advantage of classification setups as opposed to regression, as cross-entropy loss learns the full softmax scores and not just the largest score, and is critical to LLM success. In fact, I am surprised by is that it is not solved, as I would have thought that donig this knd of modeling successfully would be necessary to do video generation to the kind of fidelity we have today.\n* **Evaluations on multiple qualitatively different domains.** The paper is much stronger by evaluating on toy datasets, climate modeling, and computer vision simultaneously.\n* **Approach via scoring rules.** This is not the most obvious thing to do, which to me would have been something that leverages distribution matching via optimal transport."}, "weaknesses": {"value": "My main concerns are:\n* ***No available code.*** The authors claim to have attached code the the submission, but I am unable to access any of the two anonymized repositories because they say \"The requested file is not found.\" for every single file except the readme.\n* **Diffusion presentation is too heavy.**, especially related to diffusion model aspects. There were multiple times where I though details could have been moved to an appendix. In particular 2.2 jumps straight into formulation and could be eased, with details moved to an appendix.\n* **Not enough review on scoring rules.** To many readers of this paper, this part will be new, and it needs to be reviewed in a lot more detail. I know their definition and setup by memory, and for me these aspects were barely comprehensible. For many others coming from a diffusion background this will be the new and exciting part.\n* **Use of VI and mixture models.**.In other settings, it is well known that variational inference is hard to get working with mixture models, which exhibit all kinds of things like mode collapse and other problems. This is why VAEs for instance are rarely used with mixture model components. Does this aspect of the modeling really work?\n* **Not obvious early-enough that mixture models will be used.** Readers should know this from the abstract and introduction, because it's a very important signal about potential performance.\n* **Not enough ablations,** I would have appreciated it if it was easier to understand what parts of the pipelines are necessary, and what is qualitatively lost if one for instance drops the mixture model component, or if one keeps it but trains without scoring rules. Right now the best I can see is numbers related to performance, which is not rich enough to tell what is going on, see next point.\n* **Way too much reliance on table-based evaluation and relative comparisons.** Tables give information about relative performance of models, but reveal nothing about how well they work in an absolute sense and can therefore hide serious performance problems.\n* **Evaluation details are hard to read.** For example, the acronym CRPS is never defined, and is not as widespread as RMSE so it should be defined.\n* **No sanity checks on visualizable toy examples.** I would have liked to see, for instance, something like a a 1D time series example that can be plotted, to sanity check the model's behavior.\n* **Tables contain no +- error bars.** This makes it impossible to assess how noisy experiments are.\n* **Unclear how many random seeds.** This is only revealed in the sea surface example, which uses 5 runs, which is rather small. This can be acceptable on basis of limited compute, but not otherwise - so a justification should be included if this is the reason.\n* **Unclear how plots (as opposed to tables) actually evaluate how good the distribution is.** We should be comparing the true distribution with the learned distribution, for instance via Wasserstein distance or some other metric, or by plotting both side-by-side in a 1D example where this is possible (and listing something like a Kolmogorov-Smirnov distance)."}, "questions": {"value": "Please see weaknesses and address as appropriate, taking extra care to point out anywhere I might have an error so I can take another look. \n\nIn addition, below I include not just questions, but also some detailed comments:\n* Typo: \"the UCI benchmark\" -> \"UCI benchmarks\"\n* In what sense does p_z represent a prior? What's the likelihood here?\n* In (2) parentheses are not the right size and should be made larger\n* In 2.2, why opt for a discrete presentation rather than write down the SDE, which is quite a bit cleaner?\n* I am confused about why the forward process, as written, is Markovian. For T=3 it reads like p(x_3 | x_0) p(x_1 | x_2, x_0) p(x_2 | x_3, x_0). Why is it that we can write  p(x_1 | x_2, x_0) and don't need additional dependence on x_3? Note that a factorization like this is not true generically, as can be seen by considering for instance the Kalman smoothing equations. I don't necessarily think there is an error here, instead I think I am confused and misunderstanding the details of the setup, so if there is a standard reference you can point me to I would appreciate this so I can better follow. Relatedly, it is also not clear to me whether this level of detail is actually needed here.\n* Why not formulate the method in terms of calibration and refinement, as opposed to scoring rules? There should be an equivalent way to think about the work from this perspective, and I am guessing it would come out cleaner and easier to follow."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lshil0j5pp", "forum": "IWQNhR6poq", "replyto": "IWQNhR6poq", "signatures": ["ICLR.cc/2026/Conference/Submission12143/Reviewer_QhqQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12143/Reviewer_QhqQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972465084, "cdate": 1761972465084, "tmdate": 1762923102326, "mdate": 1762923102326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Main revision update"}, "comment": {"value": "We sincerely thank all reviewers for their constructive feedback, valuable suggestions, and the time invested in evaluating our work.\nWe have carefully addressed all raised points and provide below a consolidated summary of the revisions made in the updated manuscript. In addition to this overview and the individual point-by-point responses, all modifications in the revised version of the paper are highlighted in blue for clarity.\n\n**Model selection**\n\nBased on our results, we provided some guidance to practitioners on how to select one of our proposed methods (univariate, multivariate, mixture) for a specific experiment. This is described in the conclusion lines 477 - 480.\n\n**Additional runtime analysis**\n\nFollowing the reviewers’ suggestions, we include a detailed computational runtime analysis in Appendix D.1 (Table 11). Because extensive efficiency ablations for large-scale models such as Marigold (~900M parameters) are computationally prohibitive, we focus on the two PDE tasks; given the minimal architectural and training differences, these results directly transfer to the remaining experiments. The analysis shows no significant runtime differences between DDPM and the univariate or multivariate normal variants. For the mixture normal model, runtime increases with the number of components $K$, whereas the sample-based method of Bortoli et al. (2025) [1] exhibits substantially higher computational cost. Overall, our method achieves improved performance with essentially no additional computational overhead, except in settings involving large mixture sizes.\n\n\n**Additional correlation analysis for uncertainties**\n\nTo further validate the uncertainty estimates beyond individual qualitative examples, we added a quantitative analysis of predicted epistemic and aleatoric uncertainty for the KS equation in Appendix G. Specifically, we study the behavior of AU and EU together with the predictability of the solution—measured via the Spearman correlation between the mean prediction and the ground truth—for increasing autoregressive steps $s$. An additional figure summarizes this behavior, averaged over the entire test set, and confirms that the uncertainty estimates behave as expected for the KS equation (see lines 2167–2190).\n\n\n**Additional univariate visualizations**\n\nAs a sanity check for our models, we provided additional visualizations of single-step predictions of the PDE tasks. This allows to more clearly assess the predictions of the models and the behavior of generated samples and the predictive mean.\n\n\n**Comparison with improved DDPM***\n\nAs suggested by Reviewer wwWQ, we added a diffusion model as a baseline, which models the variance. While the approach from Bortoli et al. [1] also learn the variance, we agree that it is interesting to use more standard methods for this as well. Thus, we decided to use improved DDPM (iDDPM) [2] for this, an established diffusion model that immediately builds upon DDPM, using the same architecture but a very different loss functions. For a more indepth description and results on the KS equation, see Appendix F. The results show, that iDDPM improves the DDPM baseline, but performs worse than our univariate and mixture models.\n\n**New Section 2.3 on Scoring Rules**\n\nFollowing the suggestion of Reviewer QhqQ, to extend on our introduction of scoring rules, we moved the introduction to from 3.1 to 2.3 where we hope to provide a more gentle introduction now.\n\n[1] Bortoli, Valentin De, Alexandre Galashov, J Swaroop Guntupalli, et al. \"Distributional Diffusion Models with Scoring Rules.\" The Forty-second International Conference on Machine Learning, 2025.\n\n[2] Nichol, A.Q. and Dhariwal, P., 2021, July. Improved denoising diffusion probabilistic models. In International conference on machine learning (pp. 8162-8171). PMLR."}}, "id": "2p57rWQPIq", "forum": "IWQNhR6poq", "replyto": "IWQNhR6poq", "signatures": ["ICLR.cc/2026/Conference/Submission12143/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12143/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission12143/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724766330, "cdate": 1763724766330, "tmdate": 1763724766330, "mdate": 1763724766330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}