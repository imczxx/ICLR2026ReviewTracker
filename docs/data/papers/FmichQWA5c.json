{"id": "FmichQWA5c", "number": 12575, "cdate": 1758208731845, "mdate": 1759897500960, "content": {"title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents", "abstract": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. \nUnlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult.\nOceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making.\nAgents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers.", "tldr": "We introduce OceanGym, the first comprehensive benchmark for underwater embodied agents, designed to advance AI in one of the most demanding real-world environments.", "keywords": ["Embodied AI", "Benchmark", "Agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4d671abfe28634b8cfbb8cdd82cc7853a941bb82.pdf", "supplementary_material": "/attachment/d3f123f9460909c259a048901905995190e65984.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents **OceanGym**, a simulation benchmark for evaluating **multimodal large language model (MLLM) agents** in underwater environments. Built on Unreal Engine and HoloOcean, it integrates RGB and sonar sensing with discrete movement-based actions. The agent operates under a **memory-augmented Markov process**, where a fixed-size sliding window stores recent textual summaries and actions generated by an MLLM. The benchmark includes **85 test scenarios** spanning perception and decision-making tasks, such as object localization, inspection, and navigation under shallow and deep-water conditions. Quantitative results compare different MLLM backbones on perception accuracy and distance-based decision metrics. Overall, OceanGym provides a unified platform for studying embodied multimodal reasoning and language-driven control in simulated underwater settings, combining perception, memory, and decision-making within a single evaluation framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important and timely problem—applying AI and multimodal reasoning to underwater robotics and autonomous exploration, which has significant real-world value.\n2. The manuscript is clearly structured and easy to follow. The motivation, design choices, and experimental setup are described in a coherent and readable way.\n3. The authors have implemented a complex simulation and benchmarking platform that integrates Unreal Engine, multimodal sensors, and an MLLM-based agent framework. This represents a considerable amount of engineering work and system integration."}, "weaknesses": {"value": "1. The environment and control formulation are overly simplified, which may limit the realism and potential sim-to-real applicability of OceanGym. In particular, the **discrete “movement-based” action space** (instead of physically grounded velocity or thrust control) and the **idealized environment dynamics** (absence of current, drag, or inertia) create a noticeable gap between simulation and real-world AUV behavior.\n2. The paper designs the agent’s memory as a fixed-size sliding window. This setup inherently limits the agent’s ability to capture **long-horizon temporal dependencies**, since any information beyond the most recent K steps is forgotten. As a result, the analysis and claims regarding the agent’s memory or its ability to “remember” become theoretically weak because of this design weakness.\n3. The benchmark includes only 85 test scenes, and the discrete percentages in Table 1 (e.g., 33.33 %) suggest that each condition was evaluated with only one or very few samples. This limited evaluation scope introduces high result variance and reduces the credibility of the reported averages. Consequently, the dataset lacks the diversity needed to claim robust or generalizable agent performance."}, "questions": {"value": "1. See Weaknesses.\n2. In the Memory Transfer experiment, the paper states that the agent leverages “knowledge and experience accumulated from previous tasks.” Could the authors clarify whether this experience is derived from **the agent’s own previously sampled trajectories** (correct or not?) or from **teacher/expert trajectories**? The current description suggests self-generated experience, but it is not clear.\n3. Your memory mechanism is a fixed-size sliding window of textual summaries and past actions generated by an MLLM$((m_t={(d_{t-k},a_{t-k})}_{k=1}^K))$. Conceptually, this looks close to “detect objects → record what was seen → remember recent actions.”\n**Have you evaluated a non-MLLM classical baseline** that uses (i) a conventional CV detector for RGB/sonar, plus (ii) a **rule-based summarizer** to produce the same kind of per-step text summary (object, pose/status, etc)? This pipeline may be more accurate and efficient since you do not leverage MLLM much. I found [1] has a similar implementation to my description.\n\n[1]. Zhang, Hongxin, et al. \"Building cooperative embodied agents modularly with large language models.\" *arXiv preprint arXiv:2307.02485* (2023)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0n19Z2nqWm", "forum": "FmichQWA5c", "replyto": "FmichQWA5c", "signatures": ["ICLR.cc/2026/Conference/Submission12575/Reviewer_dEvG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12575/Reviewer_dEvG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760936040472, "cdate": 1760936040472, "tmdate": 1762923427049, "mdate": 1762923427049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OceanGym, a benchmark for underwater embodied agents, constructed upon UE. The paper introduces 8 tasks, including perception and decision-making tasks. The paper benchmarks 4 MLLMs and human performance, revealing a large gap."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-motivated, designing environments and benchmarks to study underwater embodied agents is important."}, "weaknesses": {"value": "- The contribution of the proposed platform is questionable.\n  - Underwater setting is challenging because of the dynamic ocean currents, salinity, ... but all these factors are not modeled.\n  - How the sensor data is simulated is unclear.\n  - How the proposed platform could align with real-world conditions is unclear, like what the real underwater robots is being referred to when designing the agents in the paper?\n  - What's the contribution compared to previous work, like HoloOcean mentioned in the paper?\n\n- Missing details for the benchmark.\n  - There are no task statistics in the main paper.\n  - There is no example of what the sensor data looks like, like the sonar images.\n  - How human performance is evaluated is unclear.\n  - Human performance on the benchmark is 100 and nearly 100 for perception tasks, and not reported for decision-making tasks.\n\n- Missing related literature discussion\n  - There is much more related literature in the embodied simulations and embodied agents domain, just naming a few. [1][2][3]\n  - Appendix A.2 More Related Works is still empty.\n\n[1] BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation\n[2] ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation\n[3] HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J5k8u12WG1", "forum": "FmichQWA5c", "replyto": "FmichQWA5c", "signatures": ["ICLR.cc/2026/Conference/Submission12575/Reviewer_usJW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12575/Reviewer_usJW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943909265, "cdate": 1761943909265, "tmdate": 1762923426751, "mdate": 1762923426751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new embodied environment, the OceanGym, that focuses on underwater scenarios where perception and planning are all difficult. The paper then tests the environment using MLLM agents that are given only the current agent state and the perception result."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The proposed challenge has very useful practical applications as well as adequate difficulty for the community to explore.\n\n2. The tasks and metrics are all well-formed and can be adapted easily by other embodied planning methods, making development easy on the environment."}, "weaknesses": {"value": "1. MLLM performing poorly can be anticipated. However, the paper lacks evidence that the poor performance comes from the difficulty of underwater setting instead of internal weakness of MLLMs themselves. Maybe MLLM just can't handle the planning even not in underwater environments.\n\n2. Results show that adding sonar on top of RGB makes MLLM perform worse. This suggests potentially ill-designed prompts of MLLM baselines.\n\n3. The environment only simulates the lighting, not other extreme conditions underwater, which could mean large deployment gap.\n\n4. There could be more in-domain vision-based methods tested on the environment."}, "questions": {"value": "1. Difficulties underwater include not only perception, but also low-level controlling / disturbance, e.g. due to water currents. Can your environment simulate this in some way?\n\n2. Have you tried training a vision module, e.g., that transforms a poorly lighted image to a well-lit one, or with depth prediction / bounding box prediction?\n\n3. How do the baselines perform without the lighting constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dTWNFabKAH", "forum": "FmichQWA5c", "replyto": "FmichQWA5c", "signatures": ["ICLR.cc/2026/Conference/Submission12575/Reviewer_C1Qr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12575/Reviewer_C1Qr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972017010, "cdate": 1761972017010, "tmdate": 1762923426420, "mdate": 1762923426420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript proposes a benchmark environment for underwater embodied agents. The manuscript claims that the proposed environment will provide a basis for developing agents that are robust to challenging perceptual settings (e.g., low-light and low-visibility conditions, forced reliance on unconventional state estimation signals such as sonar) and difficult planning/reasoning tasks in the underwater environment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and well-organized\n- The manuscript proposed a novel benchmark for a lesser studied subfield of embodied AI\n- The manuscript provides a reasonable set of initial experiments that provide sufficient headroom for subsequent research"}, "weaknesses": {"value": "- L88-89: Whereas the ultimate goal is to provide a pathway for transferring learned skills to real-world underwater vehicles, I am missing sufficient justification that the real-world underwater robotics community can really benefit from this environment. Put a different way, the manuscript lacks experiments illustrating that agents developed to achieve high performance in the proposed benchmark will be successful in the real world settings.\n- Table 1, Section 3.3p1: The addition of sonar information in most cases seems to decrease the model performance. This yields a potential paradox in leveraging the proposed benchmark environment for developing better agents (see comment above): is is worthwhile to develop better agents in the proposed benchmark? Would agents that model vision + sonar information in the benchmark more effectively also perform better in real world environments or on other benchmarks?"}, "questions": {"value": "- Section 3.3p1: Could the authors provide some more rationale about why the proposed benchmark may be well-suited for the ICLR community, specifically? I feel as if the fit would be stronger if the manuscript provided some methodological/algorithmic insights as well, e.g., by providing stronger baselines that are better-suited to processing these multisensory information (as opposed to taking foundation models off the shelf and displaying low performance)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b7wDP2QUy4", "forum": "FmichQWA5c", "replyto": "FmichQWA5c", "signatures": ["ICLR.cc/2026/Conference/Submission12575/Reviewer_dKUd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12575/Reviewer_dKUd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980040289, "cdate": 1761980040289, "tmdate": 1762923426118, "mdate": 1762923426118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}