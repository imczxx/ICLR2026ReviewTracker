{"id": "tguzk54PbV", "number": 10650, "cdate": 1758178546404, "mdate": 1759897637829, "content": {"title": "Adversarial Reinforcement Learning for Large Language Model Agent Safety", "abstract": "Large Language Model (LLM) agents can leverage tools such as Google Search to complete complex tasks.\nHowever, this tool usage introduces the risk of indirect prompt injections, where malicious instructions hidden in tool outputs can manipulate the agent, posing security risks like data leakage.\nCurrent defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks.\nHowever, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections.\nTo address this limitation, we propose Adversarial Reinforcement Learning forAgent Safety (ARLAS), a novel framework that leverages adversarial reinforcement learning (RL) by formulating the problem as a two-player zero-sum game.\nARLAS co-trains two LLMs: an attacker that learns to autonomously generate diverse prompt injections and an agent that learns to defend against them while completing its assigned tasks.\nTo ensure robustness against a wide range of attacks and to prevent cyclic learning, we employ a population-based learning framework that trains the agent to defend against all previous attacker checkpoints.\nEvaluated on BrowserGym and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower attack success rate than the original model while also improving their task success rate.\nOur analysis further confirms that the adversarial process generates a diverse and challenging set of attacks, leading to a more robust agent compared to the base model.", "tldr": "Co-trains two LLMs with adversarial reinforcement learning to improve LLM agent safety, where an attacker learns to autonomously generate diverse prompt injections and an agent learns to defend against them.", "keywords": ["large language model", "LLM agent", "safety"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/502ad31c7e63e8e1695d3d5e8caa4b2c77d94bdf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes ARLAS, an adversarial reinforcement learning framework to improve LLM agent security against indirect prompt injection attacks. Two LLMs are trained in a two-player game: an attacker that learns to generate malicious prompt injections and an agent that learns to defend while completing tasks. Experiments on BrowserGym and AgentDojo with Gemma 3 and Qwen 3 models show that ARLAS reduces attack success rates and slightly improves task performance compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It is good to see the exploration of co-training of attacker and agent models.\n2. I like the evaluation on out-of-distribution tasks for broader assessment.\n3. The iteration-wise performance comparison shows training effectiveness."}, "weaknesses": {"value": "1. The code is neither provided nor promised for release.\n2. The method requires white-box access to the agent, limiting practical applicability.\n3. Only two small models are tested. Larger ones (e.g., 32B) are only used for data collection, with no report of compute resources.\n4. Unrealistic threat model: the attacker knows the defender’s function names, which is unrealistic.\n5. Incorrect prompt injection setup: The agent is told to follow user's instruction, but the user told agent to follow the instructions on webpage. This contradicts the definition of indirect prompt injection, where external instructions should be ignored. I would expect the evaluation on the dataset used in [1]. It is also better to compare with their method.\n6. The authors should clarify the differences between BrowserGym and AgentDojo.\n7. Lack of baselines: The author mention [2] in their related works but a comparison is missing.\n\n[1] Hines, Keegan, et al. \"Defending against indirect prompt injection attacks with spotlighting.\" arXiv preprint arXiv:2403.14720 (2024).\n[2] Wen, Tongyu, et al. \"Defending against Indirect Prompt Injection by Instruction Detection.\" arXiv preprint arXiv:2505.06311 (2025)."}, "questions": {"value": "See weaknesses above and the following questions:\n\n1. How is the attack success rate measured in BrowserGym? Is it evaluated using attacks generated by your own attacker model, and if so, how are baselines evaluated for fairness?\n\n2. How do the authors explain that the Attack Success Rate on the OOD dataset (AgentDojo) is similar to SPAG, which isn’t designed for security? Does this suggest the BrowserGym results are overfitted to the trained attacker and don’t generalize to real-world attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "32hclh7nSz", "forum": "tguzk54PbV", "replyto": "tguzk54PbV", "signatures": ["ICLR.cc/2026/Conference/Submission10650/Reviewer_bZWN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10650/Reviewer_bZWN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760600845976, "cdate": 1760600845976, "tmdate": 1762921904023, "mdate": 1762921904023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ARLAS (Adversarial Reinforcement Learning for Agent Safety), a framework that enhances the robustness of LLM-based agents against indirect Prompt Injections (PIs). It co-trains two LLMs: an attacker that learns to generate diverse prompt injections and an agent that learns to defend against them formulated as a two-player zero-sum RL game. A population-based training strategy ensures robustness against all prior attacker strategies. Experiments on BrowserGym and AgentDojo show that ARLAS is effective against indirect PIs, and produces diverse attacks over training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The core idea of the paper and leveraging adversarial reinforcement learning to combat indirect Prompt Injections (PIs) is interesting and seems not to be explored before by others.\n\n- The use of the population-based training in the methodology also is intuitive and a wise choice which reflects its effectiveness in the results as well compared to the iterative training."}, "weaknesses": {"value": "- The authors claim in the abstract that ARLAS improves robustness and also enhances task success. However, this claim is not supported throughout the paper and raises some concerns. The apparent improvement in task success rate largely stems from the imitation learning stage; as shown in Figure 4 (right), all settings exhibit lower task success rates compared to ARLAS w/o AL in the absence of attacks. In fact, the true capabilities decrease (first-row, right images) from 59.1 to 56, and the ART baseline performs slightly better. So it cannot be concluded that ARLAS improves or even retains performance. When attacks are introduced (e.g., the second-to-last row in the same figure), the higher numbers for ARLAS settings are somewhat misleading because the authors label a task as unsuccessful if the agent leaks personal information. For instance, the w/o AL baseline achieves 26 while ARLAS achieves 35, but this difference simply reflects that ARLAS leaks information less frequently, not that it has better capabilities. This metric does not capture the true capability degradation resulting from adversarial training. The authors should have reported task success rates on a completely benign set of tasks to quantify the actual performance drop caused by the training.\n\n\n- The use of an imitation learning stage raises some questions. It seems primarily intended to warm up weaker base models before adversarial training, but this choice may not be ideal. Starting directly with stronger, more capable models could have reduced the need for such a stage and provided a fairer assessment of ARLAS’s effectiveness without relying on preliminary fine-tuning.\n\n\n- To me, the improvements over some baselines, particularly the ART baseline, appear marginal. In the absence of attacks, ART actually achieves a higher task success rate than ARLAS (57 vs. 56). In terms of Attack Success Rate, the robustness gains are also modest, with only a small reduction from 39 to 34. On AgentDojo, the task success rate likewise appears higher for ART. Overall, the added value of ARLAS compared to existing methods such as ART is not entirely clear to me.\n\n- Given the adversarial nature of this work and in line with ICLR’s guidelines, it would have been preferable to include a dedicated Ethics Statement as well as the Reproducibility section in the paper."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fbCgb91sWD", "forum": "tguzk54PbV", "replyto": "tguzk54PbV", "signatures": ["ICLR.cc/2026/Conference/Submission10650/Reviewer_4eDA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10650/Reviewer_4eDA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761272209538, "cdate": 1761272209538, "tmdate": 1762921903512, "mdate": 1762921903512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Adversarial Reinforcement Learning for Agent Safety (ARLAS), a framework for improving the robustness of LLM-based agents against indirect prompt injection attacks. ARLAS jointly trains two LLMs — an attacker that learns to generate diverse prompt injections, and an agent that learns to resist them while completing its assigned task. The training process is formulated as a two-player zero-sum Markov game, combining imitation learning and population-based adversarial RL. Experiments on BrowserGym and AgentDojo show moderate improvements in attack success rate and task success rate over baselines such as automated red teaming (ART) and SPAG."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n\n- Addresses an important and timely problem — indirect prompt injection attacks in tool-using LLM agents.\n\n- The idea of adversarial co-training of attacker and agent is conceptually appealing and well-aligned with the goal of automated red-teaming.\n\n- The method is clearly described and reasonably easy to reproduce (details and hyperparameters are provided).\n\n- Population-based learning for adversarial stability is a reasonable design choice."}, "weaknesses": {"value": "Weaknesses\n\n1. Limited novelty.\n\nThe framework largely combines known elements — adversarial self-play, GRPO-style RL updates, and population-based training — without introducing a fundamentally new idea. Similar approaches (e.g., SPAG, Rainbow Teaming, ReST) have explored self-play adversarial setups for LLMs; ARLAS feels like a straightforward adaptation to the prompt injection domain.\n\n2. Shallow empirical validation.\n\n- Experiments are restricted to synthetic web environments (BrowserGym, AgentDojo) and do not convincingly demonstrate real-world security gains.\n\n- Reported improvements (1–2% ASR/TSR gains) are small and lack strong statistical justification.\n\n- Attack “diversity” is only measured via embedding distance, which does not necessarily correlate with semantic or behavioral diversity.\n\n3. Weak analysis and insights.\n\n- The paper focuses on metrics without providing qualitative insights into what kinds of attacks or defenses emerge.\n\n- No ablations or theoretical analysis on why population-based learning helps stability."}, "questions": {"value": "1. How does ARLAS generalize to multimodal or non-textual attack surfaces (e.g., visual prompt injections)?\n\n2. Can you provide examples of qualitatively different attacks generated during training to support the “diversity” claim?\n\n3. How sensitive are the results to the choice of the base LLM or imitation data quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7ZMe9Ak2or", "forum": "tguzk54PbV", "replyto": "tguzk54PbV", "signatures": ["ICLR.cc/2026/Conference/Submission10650/Reviewer_QfgA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10650/Reviewer_QfgA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879376931, "cdate": 1761879376931, "tmdate": 1762921903078, "mdate": 1762921903078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose to train an attacker and defender language models with RL for robustness against indirect prompt injection. The attacker generates an attack prompt that can lead the defender model to leak user private information and the prompt is injected in the environment's response. Meanwhile, the goal of the defender is to successfully complete a task even with the indirect prompt injection. Since both models are co-trained with zero-sum reward, the attacker can adaptively discover new type of attack and the defender becomes robust to various kinds of attacks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- I think it is a  novel framework that can train a language model agent robust indirect prompt injection.\n\n- The proposed method shows strong improvement of attack success rate and task completion rate compared to other baselines."}, "weaknesses": {"value": "- Typo: I think you might intend to refer to Figure 3 in line 394 and Figure 4 in line 403, $r^k_T$ should be $r^g_T$ in line 254.\n\n- It is not clear what criteria is used to select different checkpoints of attackers to get diverse attack strategy.\n\n- I think the biggest problem is an unrealistic problem setup. If the leakage of private data is an issue, we can simply bypass this issue by not providing any user information to the language model. If some intermediate steps require such information, then user can decide whether to provide the information or not."}, "questions": {"value": "- Instead of checkpointing attackers, can we save previously generated successful attack prompt instead? \n\n- Even after training the defender, it still show lower task success rate because of indirect prompt injection compared to the same model without any attack. Does it mean the defender still struggles to generalize to similar type of attacks in unseen task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yIvhNGj5WY", "forum": "tguzk54PbV", "replyto": "tguzk54PbV", "signatures": ["ICLR.cc/2026/Conference/Submission10650/Reviewer_bTAE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10650/Reviewer_bTAE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037059658, "cdate": 1762037059658, "tmdate": 1762921902503, "mdate": 1762921902503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}