{"id": "leoXWCu6CO", "number": 7460, "cdate": 1758023145463, "mdate": 1759897851611, "content": {"title": "Dynamic Drone-Assisted Pickup and Delivery Routing", "abstract": "We investigate the dynamic drone-assisted pickup and delivery problem (DAPDP), which concerns real-time, on-demand routing decisions in scenarios where new paired orders arrive stochastically throughout the day. By leveraging a fleet of trucks each equipped with a drone, operators can split tasks between ground vehicles and aerial vehicles, aiming to minimize total travel costs while respecting constraints on time windows, capacity, and drone flight endurance. We propose a deep reinforcement learning (DRL) approach based on deep Q-learning, to decide dynamically which newly arrived orders to dispatch and how to integrate drone sorties effectively. Our experiments on a large, real-world-inspired dataset demonstrate substantial performance gains over greedy, random, and lazy dispatch baselines, yielding 10.6\\%, 22.6\\%, and 37.2\\% savings, respectively, in total travel cost. \nAdditionally, our value-based RL learns subset selection decisions that co-adapt with a paired sub-solver, yielding near-oracle performance and outperforming classical and PPO baselines.", "tldr": "", "keywords": ["Reinforcement learning", "autonomous systems", "vehicle routing", "optimization"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/30e355fa2b456dac395fbc1a5535517cd5ad8878.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a Deep Q-Network (DQN) framework to address the Dynamic Drone-Assisted Pickup and Delivery Problem (DAPDP), where new paired orders arrive throughout the day. The system decides dynamically which requests to dispatch, coordinating a fleet of trucks and onboard drones. The authors model the problem as an MDP and train a value-based agent to select subsets of requests for dispatch. Experimental results on large-scale, real-world-inspired datasets show better performance over heuristic and RL baselines.\n\nThe problem setting is relevant and timely, and the combination of DRL with combinatorial routing optimization is technically interesting. However, despite numerical improvements, the presentation, clarity, and scientific rigor of the paper are significantly undermined by weak methodological exposition, poor appendix structure, and lack of qualitative visualization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tRelevant and timely topic – The paper tackles a timely and complex problem at the intersection of dynamic logistics and heterogeneous fleet coordination (trucks and drones). The dynamic, rolling-horizon setting with stochastic order arrivals is highly relevant to real-world last-mile delivery. \n2.\tEffective Co-Adaptation: A key insight is that the DRL agent learns to co-adapt with the static sub-solver, selecting request subsets that are easier to solve within the time budget, which is a non-trivial and valuable learned behavior.\n3.\tEmpirical results – Demonstrated measurable cost reduction across realistic datasets, achieving near-oracle performance under certain settings.\n4.\tComprehensive ablation studies – Includes sensitivity analysis on learning rate, discount factor, and solver time limits, providing valuable insights into training behavior."}, "weaknesses": {"value": "1. Lack of qualitative visualization\nDespite focusing on spatial routing, the paper contains no figures showing truck and drone trajectories, launch/rejoin points, or deferral maps. This omission makes it impossible to assess whether the learned policy demonstrates interpretable or practically meaningful behavior. The results remain purely numerical and fail to provide intuition about what the agent has actually learned.\n2. Figure 3 – Conceptually unclear and unsupported\nThe explanation of Figure 3 (penalty landscape) is inadequate and purely illustrative. The figure provides no empirical content (no learning curves, convergence plots, or variance analysis). The authors claim the “dense signal proved more stable than a flat FAIL reward” but present no numerical evidence. A more useful figure would compare training stability or performance with vs. without this penalty structure.\n3. Appendix quality and structure\n•\tThe appendix contains numerous editorial and structural problems:\no\tSelf-referential typos such as “Equation equation 25.”\no\tOver-fragmented subsections (e.g., C.3.1, C.3.2) that add little substance.\no\tRedundant or verbose content that reads like documentation rather than a formal academic supplement.\n4. State definition inconsistencies\n•\tThe state representation is described twice — in Section 3.1 (compact features for DQN input) and Appendix C.1 (environment-level details) — but with inconsistent terminology and no mapping between them.\n•\tThe paper never defines the state formally (e.g., st=(⋯ )s_t = (\\cdots)st=(⋯)), nor specifies which variables are normalized or aggregated.\n•\tThis ambiguity compromises reproducibility and obscures the MDP’s structure.\n5. Incomplete reward specification\n•\tThe reward is defined in pieces across Section 3.1 and Appendix C.2.3, without a single unified formula.\n•\tThe interaction between the base reward and penalty terms is unclear.\n•\tSince the reward function defines the MDP, this version undermines theoretical completeness and reproducibility.\n6. Unreferenced static model\n•\tAppendix A reproduces the standard static DAPDP without citing foundational sources.\n•\tAcknowledging prior work is necessary for academic integrity and to clarify what is new in this formulation.\n7. Writing and style inconsistencies\n•\tReinforcement learning is written in full in Section 3.1 but later abbreviated as reinforcement learning (RL) in Appendix C.5 — inconsistent usage.\n•\tSection C.5.1 introduces AlphaGo without prior context or explanation. The reference appears abrupt and unnecessary.\n•\tSeveral sentences require stylistic polishing for academic tone and precision.\n•\tSection 2 ('Positioning and Novelty') is overly dense and complex, which makes it difficult to follow. Consider simplifying the language and structure to improve readability.\n•\tThe conclusion section should be polished for consistency in tone and tense."}, "questions": {"value": "Visualization and interpretability\n\tCan the authors include visualizations (maps or trajectory diagrams) showing the spatial behavior of their learned policy?\n\tFor example, how do drone launch points and truck paths differ between the DQN policy and baselines?\n\n\tReward formulation\n\tWhat is the exact unified mathematical expression for the reward function used in training, including penalty terms?\n\tHow is the penalty integrated into the Q-learning target update?\n\n\tState representation\n\tPlease provide a formal definition of the MDP state space s_t and observation mapping.\n\tWhich features are normalized or transformed before being input to the neural network?\n\n\tFigure 3\n\tCould the authors provide quantitative evidence (e.g., learning curves, variance reduction) to justify the claim that the dense penalty improves training stability?\n\tOtherwise, consider removing or replacing Figure 3 with empirical results.\n\n\tStatic DAPDP references\n\tThe static MILP model in Appendix A is standard in prior literature. Which works did the authors build upon or modify? Please cite them.\n\n\tAppendix structure\n\tWill the authors reorganize Appendix C to reduce redundancy and clarify environment vs. learning-level definitions?\n\n\tAlphaGo reference\n\tWhy is AlphaGo mentioned? If used as an analogy, could the authors connect it more explicitly to the reinforcement learning discussion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "na"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iRLGpuI4jo", "forum": "leoXWCu6CO", "replyto": "leoXWCu6CO", "signatures": ["ICLR.cc/2026/Conference/Submission7460/Reviewer_AYVe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7460/Reviewer_AYVe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761109894856, "cdate": 1761109894856, "tmdate": 1762919574268, "mdate": 1762919574268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzed the dispatching problem for the last-mile delivery using drone-truck integration. A MDP is formulated and experiment is conducted on a simulated data-set."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The problem is novel and interesting. It’s also more complicated compared to existing last-mile delivery problem.\n+ The formulation of the problem is relatively complete and clear."}, "weaknesses": {"value": "- The major contribution of the paper seems to be in the formulation, not sure it meets the expectation for a ICLR paper.\n- The assumption that orders come randomly is not realistic. Last-mile orders, especially for food and grocery delivery, has strong spatial and temporal patterns.\n- Grammar. “the agent must time dispatch decisions” on page 2."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GYxv9smTYd", "forum": "leoXWCu6CO", "replyto": "leoXWCu6CO", "signatures": ["ICLR.cc/2026/Conference/Submission7460/Reviewer_fXco"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7460/Reviewer_fXco"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761227858048, "cdate": 1761227858048, "tmdate": 1762919573597, "mdate": 1762919573597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the dynamic drone-assisted pickup and delivery problem (DAPDP), where a fleet of trucks equipped with drones serves dynamically arriving paired pickup–delivery requests under time-window, capacity, and endurance constraints. The authors propose a deep Q-learning (DQN)–based approach to decide which new orders to dispatch and how to coordinate drone sorties. A paired ALNS (adaptive large neighborhood search) sub-solver is used for route construction. Experiments on a “real-world-inspired” dataset with up to 200 customers reportedly show that the method outperforms greedy, random, and PPO baselines and achieves performance close to a clairvoyant oracle."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The topic — dynamic pickup-and-delivery with drones — is practically relevant and fits the growing interest in combining reinforcement learning with combinatorial logistics optimization.\n2. The empirical results, if valid, suggest that learning-based decision rules might yield efficiency gains in dynamic dispatching environments.\n3. The integration of an RL agent with an optimization-based sub-solver is conceptually interesting and aligns with decision-focused learning paradigms."}, "weaknesses": {"value": "1. Lack of technical novelty.\nThe proposed approach relies on a standard deep Q-learning framework with minor heuristic adjustments. There is no clear algorithmic or theoretical innovation beyond existing DQN formulations or hybrid RL–metaheuristic approaches.\n2. Insufficient methodological rigor.\nThe MDP formulation and environment definition are vague and incomplete. Key components — such as state representation, transition dynamics, and reward specification — are not defined rigorously, making it difficult to assess reproducibility or correctness.\n3. Unjustified design choices.\nThe integration of ALNS into the framework is insufficiently explained. It remains unclear why ALNS is chosen, how its neighborhoods are designed, what hyperparameters or termination criteria are used, or how it interacts with the learning policy. Without such detail, the claimed near-optimal performance is not verifiable.\n4. Unrealistic or underspecified experimental setup.\nSeveral assumptions are overly restrictive or inconsistent with realistic dynamic pickup-and-delivery settings — most notably the “all requests must be served” constraint. Furthermore, the dataset description lacks transparency: the origin and realism of the 200-customer scenario are unclear, and there is no evidence that the environment captures meaningful stochasticity or dynamism.\n5. Questionable baselines and results.\nThe claim that the proposed DQN approach achieves performance within 1% of a clairvoyant baseline is implausibly strong and not supported by proper justification. Details on the oracle baseline, its computational budget, and its use of future information are missing. No ablation or sensitivity analysis is provided to understand why such high performance is achieved.\n6. Limited generalization and insight.\nThe study provides no conceptual or methodological insights that would generalize beyond this specific problem. As a result, the contribution is primarily empirical and lacks depth expected for ICLR."}, "questions": {"value": "1. What are the hyperparameters and termination criteria for the ALNS sub-solver, and how is its performance benchmarked?\n2. How is the clairvoyant (oracle) baseline implemented, and what future information does it assume access to?\n3. Could the authors provide details about the dataset generation process and justify why 200 customers represent a realistic operational scale?\n4. Have the authors tested generalization across different levels of dynamism or uncertainty in request arrivals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EhgH4aYmEH", "forum": "leoXWCu6CO", "replyto": "leoXWCu6CO", "signatures": ["ICLR.cc/2026/Conference/Submission7460/Reviewer_vyDR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7460/Reviewer_vyDR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761676811412, "cdate": 1761676811412, "tmdate": 1762919573014, "mdate": 1762919573014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates the dynamic drone-assisted pickup and delivery problem and proposes a deep reinforcement learning (DRL) approach based on deep Q-learning, to decide dynamically which newly arrived orders to dispatch and how to integrate drone sorties effectively.  However, it is not clear how to use the DQN to solve the challenges in the dynamic drone-assisted pickup and delivery problem, such as the dynamic orders and the cooperation of trucks and drones. Additionally, this work lacks the experiments of comparing with the baselines in the truck-drone delivery."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work models the dynamic drone-truck collaborative delivery problem with time windows aligns with emerging research topics in the current logistics industry, boasting high application value and cutting-edge relevance.\n2. This paper applies the standard DQN to learn dispatching decisions, while the complex routing problem is optimized by a dedicated traditional optimizer ALNS. This is a highly practical and effective choice, and the experimental results also demonstrate the superiority of this method."}, "weaknesses": {"value": "1. The generalization experiments explore scenarios with different urban distributions. What is the model's generalization performance when the request scale of test instances (e.g., 300 requests) is much larger than that during training (100-200 requests)?\n2. The experimental results do not include a comparison of inference time. Adding this comparison would enable a better understanding and evaluation of the method, as it is important to know whether ALNS optimization is time-consuming.\n3. The addition of shaped rewards is mentioned in Section 4.5.1, but no specific details are provided.\n4. Regarding the reward_for_decision in Section 3.2.2, the reward for one step is evenly distributed among each node, which seems unreasonable. For example, if A, B, and C are selected—where A is far from B and C, while B and C are close to each other—this single action returns a large negative reward. A, B, and C receive the same penalty, but in this case, the priority should be to avoid selecting A as much as possible."}, "questions": {"value": "1. It is not clear how to use the DQN to solve the challenges in the dynamic drone-assisted pickup and delivery problem, such as the dynamic orders and the cooperation of trucks and drones.\n\n2. Could you compare this method with the state-of-the-art method in the truck-drone delivery problems? This work lacks the experiments of comparing with the baselines in the truck-drone delivery."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "459JRAhaPO", "forum": "leoXWCu6CO", "replyto": "leoXWCu6CO", "signatures": ["ICLR.cc/2026/Conference/Submission7460/Reviewer_ecQu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7460/Reviewer_ecQu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752082180, "cdate": 1761752082180, "tmdate": 1762919572398, "mdate": 1762919572398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a deep Q learning approach for a dynamic drone-assisted pickup and delivery routing problem. The authors attempt to address some of the main challenges of this last-mile delivery problem, such as dynamic requests,  coordination of the ground vehicle and the drone, etc. The static version of the problem is formulated as a Markov Decision Process, with the state space, the action space being a binary variable (dispatch or defer), and the reward being reflective of the total distance as cost, along with the constraint violation penalty after solving the subproblem. The approach was compared against 7 other baseline methods. Parametric study, ablation studies, and hyperparameter sensitivity studies were also performed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem addressed in this manuscript is both highly relevant and inherently challenging within the context of Last Mile Delivery (LMD) — a domain that plays a critical role in modern logistics and e-commerce operations. Efficiently optimizing routes and resource allocation in this stage directly impacts delivery speed, operational cost, and customer satisfaction, making it a central focus of contemporary research in operations research and transportation systems. The authors have effectively captured the complexity of this problem through a well-formulated optimization framework that accurately reflects the real-world constraints and dynamic nature of LMD scenarios. The mathematical formulation is rigorous and thoughtfully constructed, providing clear insights into the trade-offs and decision variables involved. Furthermore, the detailed problem description enhances the manuscript’s clarity and accessibility, allowing readers to fully appreciate the technical depth and practical significance of the proposed approach. Overall, the problem formulation and presentation demonstrate a strong alignment with real-world logistics challenges and contribute meaningfully to advancing optimization methodologies for last-mile delivery systems. The problem considered in this manuscript is a very relevant and difficult problem in Last Mile Delivery. The optimization formulation of the problem, along with the detailed description, is well appreciated."}, "weaknesses": {"value": "1. Even though the practicality of this work is unquestionable, and the authors have formulated the static version of the problem as an MDP, there is no novelty in the methodology used (simple deep Q learning). Even though it can be argued that there is no need for a more novel, sophisticated approach, given the focus of ICLR, I feel  ICLR might not be the right platform for showcasing this work, and is more apt for an optimization-related platform.\n\n2. The writing style of the manuscript can also be significantly improved. At present, the writing style resembles more of a project report than an academic paper. For example, the meaning of many terms in the state space is not clear. I encourage the authors to improve the clarity of writing there, and also in many other parts of the manuscript.\n\n3. The current choice of baseline appears relatively weak and limits the strength of the comparative analysis. To provide a more convincing evaluation, the study would benefit from incorporating stronger and more representative baselines. In particular, since the Oracle method assumes complete knowledge of the system — an unrealistic assumption in practical settings — it would be more appropriate to replace or complement it with a Mixed Integer Linear Programming (MILP) formulation. MILP-based methods serve as a more rigorous and interpretable benchmark, offering a well-established optimization standard against which the proposed approach’s performance can be meaningfully compared. Including such a baseline would not only strengthen the empirical validation but also highlight the practical advantages and limitations of the proposed method under realistic conditions."}, "questions": {"value": "1. What is 60s ALNS?\n\n2. How is the sub-problem solved during every decision-making step? \n\n3. The experimental implementation details are not provided. Can the authors please provide how the environment is implemented using the dataset? For example, the programming platform, or other simulation environment, if any."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Abrn2az1ze", "forum": "leoXWCu6CO", "replyto": "leoXWCu6CO", "signatures": ["ICLR.cc/2026/Conference/Submission7460/Reviewer_k2s8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7460/Reviewer_k2s8"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762788581575, "cdate": 1762788581575, "tmdate": 1762919571768, "mdate": 1762919571768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}