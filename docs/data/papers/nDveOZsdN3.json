{"id": "nDveOZsdN3", "number": 10532, "cdate": 1758174818301, "mdate": 1763655578044, "content": {"title": "$\\mathrm{D^3}$: Divide, Describe, and Diffuse: Prompt-Enriched, Scene-Aware Dataset Condensation for Object Detection", "abstract": "Dataset condensation (DC) seeks to compress large datasets into small synthetic ones for efficient training. Recent work applies text-to-image diffusion models to DC, but their naïve use in object detection creates a representation bottleneck: conditioning on short captions yields sparse, single-object scenes with limited spatial coverage, failing to capture the multi-object co-occurrence and layout diversity essential for detector generalization.\nWe present $\\mathrm{D^3}$ (Divide, Describe, Diffuse), an optimization-free framework that produces dense and semantically diverse synthetic images tailored for detection. $\\mathrm{D^3}$ constructs a scene-to-object dictionary from dataset statistics, which guides a Large Language Model to generate enriched captions grounded in realistic contexts. To further enhance fidelity and spatial variety, we design two structured prompting strategies: prompt merging, which combines multiple enriched captions, and spatial partitioning, which allocates sub-prompts to different image regions.\nOn MS COCO and PASCAL VOC, $\\mathrm{D^3}$ achieves state-of-the-art performance under severe data constraints. Notably, using only 0.5\\% of VOC data, $\\mathrm{D^3}$ reaches 18.8\\% mAP (vs. 8.5\\% for the strongest baseline), demonstrating its effectiveness in overcoming the representation gap of prior DC methods.", "tldr": "", "keywords": ["Dataset Condensation", "Object detection"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f0d15260db590991d05da617c81b4fbe6b1fce7c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes D³ (Divide, Describe, and Diffuse), a prompt-enriched and scene-aware dataset condensation framework for object detection. The method first augments textual prompts through scene-aware caption enrichment, then synthesizes dense and semantically diverse images via a text-to-image diffusion model. Experiments on VOC and COCO demonstrate consistent improvements over recent dataset condensation and coreset-based methods under low-data regimes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed framework combines LLM-guided text augmentation and diffusion-based image synthesis in a conceptually clean way.\n\n2. The pipeline is modular, optimization-free, and potentially extensible to other multimodal generation settings.\n\n3. Quantitative results show stable improvements over prior methods (DCOD, UniDD) across multiple detectors and datasets."}, "weaknesses": {"value": "1. Lack of quantitative validation for the “semantic coverage” hypothesis (Sec. 3.1).\n\nThe authors claim that detection performance mainly depends on semantic coverage rather than object count. However, this hypothesis lacks quantitative evidence. The observed performance drop could also arise from visual inconsistencies (e.g., illumination, scale mismatch, blending artifacts) rather than semantic sparsity. To substantiate this claim, the authors should:\n\nDesign a controlled experiment that fixes the number of pasted objects while varying their semantic diversity, or introduce a CLIP embedding–based diversity metric and analyze its correlation with mAP. Such analysis would convincingly link semantic coverage to detection performance.\n\n2. Weak spatial control compared with existing controllable diffusion models.\nThe proposed “augment in text space, realize in image space” paradigm is elegant, but its spatial controllability remains weak. Existing controllable diffusion models such as ControlNet, T2I-Adapter, and GLIGEN already provide explicit conditioning via bounding boxes, depth, or pose, ensuring spatial coherence and realistic layouts. In contrast, D³ relies solely on linguistic prompting, which makes object placement and scale less reliable—an issue especially critical for object detection tasks. Incorporating layout-aware conditioning or lightweight structural priors could substantially strengthen the framework.\n\n3. Outdated captioning model and limited linguistic diversity (Sec. 4.1).\nThe paper uses BLIP for caption extraction, which often yields single-object or coarse scene descriptions. However, modern captioners such as ControlCap (2024) and the Qwen series (e.g., Qwen3-VL) have demonstrated stronger multi-object reasoning and controllable caption generation, often mitigating the very limitations D³ attempts to address via SACE. Thus, the reliance on BLIP introduces unnecessary bias. A more robust design would involve modern multimodal LLMs or controllable captioners (e.g., ControlCap, Qwen-VL) that can directly produce scene-rich, attribute-aware, and region-grounded captions. Additionally, using datasets with human-written captions (e.g., Visual Genome, COCO Captions) could provide a better foundation than automatically generated text.\n\n4. Limited data budget and low absolute performance.\n\nAt 0.5% data budget, the model achieves 18.8% mAP on VOC, far below the ≈70% attainable with full data, and only marginally higher than baselines. True dataset condensation should aim to preserve near-full performance with reduced samples, which is not realized here.\nThe authors should: Extend experiments to ≥5% or 10% budgets, Report mAP–budget curves, and Include multiple runs with standard deviation to establish robustness.\n\n5. Conceptual ambiguity between “dataset condensation” and “synthetic data generation”\n\nThe contribution remains conceptually ambiguous. Classical dataset condensation focuses on compressing existing data while preserving task performance;D³, however, generates entirely new synthetic images via text-to-image diffusion, which belongs more to synthetic dataset generation than condensation. Given that the absolute mAP (e.g., 16.4% on COCO vs. ≈40% with full training) remains far below realistic expectations, it is unclear whether the framework achieves meaningful “condensation.” Clarifying this distinction would help the reader properly interpret the paper’s positioning and novelty."}, "questions": {"value": "Please refer to Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YvkPulK7Xl", "forum": "nDveOZsdN3", "replyto": "nDveOZsdN3", "signatures": ["ICLR.cc/2026/Conference/Submission10532/Reviewer_TL4i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10532/Reviewer_TL4i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881327137, "cdate": 1761881327137, "tmdate": 1762921813933, "mdate": 1762921813933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents D3, a framework for dataset condensation in object detection that leverages text-to-image diffusion models. It addresses the key limitation of prior methods—sparse, single-object scenes—by enriching prompts with scene-aware object lists and using structured composition strategies to generate denser, more diverse synthetic images. The method achieves state-of-the-art results under extreme data constraints on COCO and VOC."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a well-motivated approach, effectively addressing the core bottleneck of sparse object density in T2I distillation by introducing a scene-to-object dictionary and structured prompting to generate semantically richer, denser synthetic images, leading to significant performance gains under extreme compression."}, "weaknesses": {"value": "1. D³ lacks any explicit mechanism to control or optimize for object scale. Both the scene dictionary and spatial partitioning focus on object presence and rough location, neglecting relative size. This may lead to an unrealistic scale distribution in generated images, potentially under-representing small objects.\n\n2. The \"Top-K\" frequent objects used in the scene dictionary inherently favor common categories, exacerbating the long-tail problem. The paper offers no strategy (e.g., oversampling rare classes or targeted prompting) to ensure balanced representation of all categories in the distilled dataset.\n\n3. Technically, D³ functions more as an integration of existing models. Its heavy reliance on LLMs (like GPT-4.1) for prompt enrichment introduces significant computational cost and API dependency.\n\n4. The study does not explore performance at higher compression ratios (e.g., 5%, 10%, 20%, 50%), leaving the method's scalability and upper performance bound unclear."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VVYX4bSnlL", "forum": "nDveOZsdN3", "replyto": "nDveOZsdN3", "signatures": ["ICLR.cc/2026/Conference/Submission10532/Reviewer_ZLz9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10532/Reviewer_ZLz9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883763144, "cdate": 1761883763144, "tmdate": 1762921812885, "mdate": 1762921812885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of dataset condensation (DC) for object detection, identifying a key \"representation bottleneck\" in prior text-to-image (T2I) methods. The authors argue that naïve conditioning on simple captions produces sparse, single-object images that lack the multi-object co-occurrence and layout diversity essential for training robust detectors. They propose $D^3$ (Divide, Describe, Diffuse), an optimization-free framework that generates dense, semantically diverse synthetic data. The pipeline first Divides the dataset by building a Scene-to-Object Dictionary (SOD) from co-occurrence statistics. It then Describes complex scenes by using an LLM, guided by the SOD, to perform Scene-Aware Caption Enrichment (SACE). Finally, it Diffuses these scenes by using a Multi-Caption Composition (MCC) strategy—specifically a region-aware prompt—to guide a T2I model. The resulting images are pseudo-labeled by a frozen detector. On MS COCO and PASCAL VOC, $D^3$ achieves state-of-the-art results, more than doubling the mAP of the strongest baseline at extremely low data ratios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's core strength is its precise diagnosis of why prior T2I-based DC methods fail for detection. The concept of the \"representation bottleneck\" caused by simple captions is a key insight. This is powerfully supported by the motivating Copy-Paste experiment, which demonstrates that scene coherence and semantic coverage are far more important for detector training than just the raw count of pasted instances.\n2. The core idea of \"augmenting in text space, then realizing in image space\" is a highly effective and novel solution to the problem. The $D^3$ pipeline is clever and logical. In particular, the Scene-to-Object Dictionary (SOD) is a smart mechanism to ground the LLM's creativity in the dataset's actual statistics, ensuring the generated scenes are not just dense, but also realistic.\n3. The results are not incremental but represent a massive leap forward, more than doubling the mAP of the previous SOTA (UniDD) in low-data regimes. The high quality of the $D^3$-generated dataset is further validated by its strong generalization: it successfully trains not only the default detector (Faster R-CNN) but also other advanced architectures (DINO, DiffusionDet) and even transfers effectively to other tasks like segmentation and pose estimation."}, "weaknesses": {"value": "1. The framework's knowledge is hard-capped by the capabilities of the frozen pseudo-labeler. The T2I model could generate a perfect object, but if the frozen detector (e.g., Faster R-CNN) fails to recognize it, that object effectively does not exist in the final dataset. This creates a hard ceiling on the condensed data's quality, which can never surpass the knowledge of its labeler.\n2. The paper's core claim is that its linguistic layout control (MCC-RA) is a key driver of performance. A critical missing baseline would be a comparison against explicit layout control methods. For instance, sampling realistic bounding box layouts from the dataset and using a layout-to-image model (like GLIGEN) would also capture \"multi-object co-occurrence and layout diversity\" and would serve as a more direct and challenging baseline.\n3. The \"optimization-free\" claim is misleading, as the framework is a complex, feed-forward cascade of numerous, large-scale, pre-trained models. It requires a VLM (BLIP), a scene classifier (ResNet-50), a large LLM (GPT-4.1), a SOTA T2I generator (SDv3), and a detector (Faster R-CNN). This makes the pipeline computationally expensive to run (approx. 33s per image) and difficult to reproduce, especially for those without API access or massive compute resources."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EFc4qmoERg", "forum": "nDveOZsdN3", "replyto": "nDveOZsdN3", "signatures": ["ICLR.cc/2026/Conference/Submission10532/Reviewer_sUci"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10532/Reviewer_sUci"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948066661, "cdate": 1761948066661, "tmdate": 1762921812427, "mdate": 1762921812427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Dataset Condensation (DC) for object detection is challenging due to the computational cost of bi-level optimization and the information bottleneck of simple text-to-image (T2I) generation models. The proposed framework, D³ (Divide, Describe, Diffuse), addresses this by using a learned scene-to-object dictionary and a two-stage caption enhancement pipeline (including Scene-Aware Caption Enrichment and Multi-Caption Composition) to generate content-rich, spatially diverse synthetic images. This method achieves state-of-the-art performance, significantly outperforming baselines on PASCAL VOC and MS COCO under extreme data constraints."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. While prior DC methods for detection relied on computationally prohibitive bi-level optimization or naïve Text-to-Image (T2I) generation, this work introduces a novel generative paradigm focused on enriching the conditioning signal (captions) rather than heavily optimizing the synthetic images or networks.\n\n2. The results are compelling, achieving state-of-the-art performance under extreme data constraints (0.25% storage budget). The claim of \"more than doubling the strongest baselines\" on both PASCAL VOC and MS COCO provides strong quantitative evidence of the method's effectiveness and superiority over existing methods like DCOD and UniDD."}, "weaknesses": {"value": "1. The authors state in the introduction that D3 achieves more than twice the improvement compared to UniDD. However, it is important to note that UniDD uses SDv1.5 as its baseline, whereas D3 is built upon SDv3.0. Therefore, this comparison is not entirely fair.\n2. The annotations corresponding to the images generated by the authors are derived from the pre-trained detection model Fast R-CNN, which inherently limits their accuracy. Moreover, Faster R-CNN does not represent the state-of-the-art in object detection performance."}, "questions": {"value": "1. In my view, the images ultimately generated by the authors, as shown in Figure 5, bear a strong resemblance to the Mosaic technique—a common data augmentation method used in YOLO series. Mosaic combines four distinct images into a single composite, allowing the model to perceive targets across varying scales and backgrounds. This observation raises an intriguing question: Does the performance improvement reported by the authors’ method primarily stem from the effects of data augmentation?\n2. The experiments presented by the authors in Section 3 are quite interesting. However, as noted earlier, the proposed approach bears a strong resemblance to Mosaic. It would be beneficial if the authors could supplement this section with experimental comparisons or results from Mosaic to provide a more comprehensive evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "prFD78a8WG", "forum": "nDveOZsdN3", "replyto": "nDveOZsdN3", "signatures": ["ICLR.cc/2026/Conference/Submission10532/Reviewer_Cxgn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10532/Reviewer_Cxgn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762480126020, "cdate": 1762480126020, "tmdate": 1762921811823, "mdate": 1762921811823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of changes"}, "comment": {"value": "We sincerely thank all reviewers for their thorough evaluations and valuable comments. We have made the following revisions (highlighted in blue):\n\n- Minor revisions in Section 3.1 to accurately reflect the intended goal and claim\n- In Appendix A.5, the Limitations section has been updated to DISCUSSION, LIMITATIONS, AND FUTURE WORK. All potential improvements raised by the reviewers have been integrated into this section.\n\nWe are grateful for the comments that have greatly improved the quality of the manuscript. We hope our updates and explanations adequately respond to your comments."}}, "id": "whjKjAZg3K", "forum": "nDveOZsdN3", "replyto": "nDveOZsdN3", "signatures": ["ICLR.cc/2026/Conference/Submission10532/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10532/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10532/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763655479979, "cdate": 1763655479979, "tmdate": 1763655479979, "mdate": 1763655479979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}