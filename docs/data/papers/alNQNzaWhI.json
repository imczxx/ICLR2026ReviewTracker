{"id": "alNQNzaWhI", "number": 2463, "cdate": 1757096914501, "mdate": 1763691100040, "content": {"title": "From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents", "abstract": "Retrieval-augmented generation agents development is hindered by the lack of process-level supervision to effectively guide agentic capabilities like task decomposition, retriever invocation, and stepwise decision-making. While reinforcement learning offers a potential solution, it suffers from sparse rewards and the limited reasoning capabilities of large language models (LLMs). Meanwhile, existing data synthesis methods only produce chain-of-thought rationales and fail to model environmental interactions.\nIn this paper, we propose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG agent development. EviPath comprises: (i) Abductive Subtask Planning, which decomposes the problem into sub-questions and iteratively plans an optimal solution path based on the dependencies between them; (ii) Faithful Sub-question Answering, which uses supporting evidence to construct a proxy environment to generate reasoning thoughts and answers for each sub-question; and (iii) Conversational Fine-Tuning, which formats the complete agent-environment interaction trajectory into a dialogue format suitable for Supervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and tool-use capabilities directly from synthesized data. Extensive experiments on widely-used question-answering benchmarks show that an 8B parameter model trained with EviPath-synthesized data significantly and consistently outperforms state-of-the-art baselines with a double-digit absolute EM gain of 14.7% in open-domain question answering.", "tldr": "Evidence-anchored Reasoning Path Synthesis for Training RAG-based Question Answering Agents", "keywords": ["Data Synthesis", "RAG", "Agent", "Question Answering"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f01544e57e33ee70d5cb7df16b3a1434d9687538.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG agent, which comprises three stages: 1) Abductive Subtask Planning: decomposes the problem into sub-questions and iteratively plans an optimal solution path based on the dependencies between them; 2) Faithful Sub-question Answering: uses supporting evidence to construct a proxy environment to generate reasoning thoughts and answers for each sub-question, and 3) Conversational Fine-Tuning: formats the complete agent-environment interaction trajectory into a dialogue format suitable for SFT. Experiments show the effectiveness of various models trained on data synthesized through EviPath. The paper also answers six research questions based on many ablation studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tries to transform the traditional forward trajectory synthesis paradigm to an abductive reverse process.\n2. The introduction of intermediate answer synthesis can effectively boost the quality of synthetic trajectories.\n3. The figures shown in this paper are easy to understand and can clearly present the pipeline of the proposed method.\n4. The main experiments and ablation studies are comprehensive."}, "weaknesses": {"value": "1. My main concern lies in the scalability of the methods in this paper. As EviPath heavily relies on the presence of a large number of questions, golden evidence, and answers, these elements can be found in common multi-hop QA datasets. However, in real-world scenarios, especially in specific domains, it is often challenging to obtain a substantial amount of high-quality answers and evidence, and sometimes there may not be enough questions available. In such cases, EviPath may prove to be inadequate.\n\n2. The intermediate processes in this paper are all synthesized in a simulated environment based on gold evidence. The entire process assumes that the synthesized intermediate answers and thoughts are correct, which introduces several issues: 1) Can gold evidence truly simulate a real retrieval environment? 2) Are the synthesized intermediate thoughts and answers genuinely accurate? 3) Models trained in this manner have only encountered positive environmental feedback. Once they generate errors during inference or receive incorrect external signals, the model is prone to failure.\n\n3. There are doubts about the fairness of the baseline comparisons: 1) The paper introduces the signal of gold evidence. Do all baselines adhere to the same setup? 2) The training in this paper is conducted on 265K data points. Have all baselines been trained on the same amount of data? 3) How would methods like HippoRAG, IRCoT, EfficientRAG perform on models such as GPT-4o/DeepSeek-V3/DeepSeek-R1, given that prompt-based baseline models often have weaker backbones?\n\n4. The datasets tested in this paper only involve retrieval and do not seem to include web search and knowledge graph as shown in Figure 2.\n\n5. The datasets used to test the generalization in RQ3 still exhibit strong similarities to the training set, which may not be sufficient to draw conclusions."}, "questions": {"value": "1. What are the reproduction details of RAG Agents and concurrent works in Table 1, including the composition and scale of the training data, as well as the information contained in the prompts?\n\n2. Is there an analysis regarding the robustness of the trained models, such as their behavior when encountering situations not seen during training (e.g., generating incorrect intermediate answers, encountering retrieval errors)?\n\n3. Given that LLMs today already possess strong foundational abilities in multi-hop QA, and RL algorithms can enable models to self-explore trajectories relying solely on the final reward and engage in self-reflection and correction, is it necessary to synthesize a large amount of trajectory data in this scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ae3ZfmLEZO", "forum": "alNQNzaWhI", "replyto": "alNQNzaWhI", "signatures": ["ICLR.cc/2026/Conference/Submission2463/Reviewer_jQ3j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2463/Reviewer_jQ3j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760816942301, "cdate": 1760816942301, "tmdate": 1762916247769, "mdate": 1762916247769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Reply to all reviewers"}, "comment": {"value": "Dear Reviewers,\n\nThank you for your acknowledgement of the contribution of our manuscript and the constructive suggestions. \n\nWe would like to clarify that, although our training data are synthesized in simulated environments, ***all experiments*** except RQ4 ***are conducted under an “open-domain” setting***.  (See Section 5.2 for our original claim). \n\nLike all baseline methods, ***during evaluation***, RAG agents developed with EviPath data ***also need to interact with “real environment”***, and *handle noisy/incomplete retrieval results*. \n\nHence, we believe that the excellent performance reported in our tables demonstrates the good quality of our synthetic data, and hence, proves the transferability of RAG agents to be applied in ***“real-world”***, ***“open-domain”*** scenarios. \n\nIn addition, we would like to kindly point out that most of the ablation studies you requested in the review *have already been included in our original manuscript*. These include, the removal of golden evidence (Table 1), the removal of training trajectories for the planner/executor (Table 1), and reducing the amount of training samples (Figure 5).\n\nWe hope that our clarifications and experimental evidence help address your concerns about our work, and we would be very grateful if you could consider updating your overall assessment accordingly. We will provide detailed, point-by-point responses to the specific weaknesses and questions in the following sections.\n\nThank you very much for your time. We are looking forward to receiving your feedback. \n\nRegards,\n\nThe Authors"}}, "id": "Ps02e4Fgkj", "forum": "alNQNzaWhI", "replyto": "alNQNzaWhI", "signatures": ["ICLR.cc/2026/Conference/Submission2463/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2463/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2463/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763373310697, "cdate": 1763373310697, "tmdate": 1763373310697, "mdate": 1763373310697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the lack of step-wise supervision in current retrieval-augmented generation (RAG) agents and proposes EviPath, an evidence, anchored reasoning path synthesis framework grounded in abductive reasoning. By applying abductive reasoning, **EviPath** reversely constructs multi-step reasoning trajectories from existing QA pairs and supporting evidence, encompassing three stages: subtask planning, sub-question answering, and conversational fine-tuning. Experimental results demonstrate that models trained on EviPath-synthesized data achieve significant improvements on multi-hop QA benchmarks, validating the effectiveness of data-driven process supervision in enhancing the reasoning capabilities of RAG agents."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper innovatively introduces abductive reasoning into the process of reasoning trajectory generation, providing a complementary paradigm for RAG training that differs from traditional CoT and RL methods.\n2. It constructs a dataset containing 265k high-quality reasoning trajectories and provides two types of structured prompt templates, planner and executor, offering effective support for process-level supervision.\n3. The paper conducts a systematic comparison with 24 baseline methods in the experiments."}, "weaknesses": {"value": "**1. The theoretical depth and verifiability of the contribution on abductive reasoning are insufficient.**\n\na. The paper claims that “We are the first to formulate the synthesis of reasoning paths for RAG agents as an abductive reasoning problem” and describes the process of constructing reasoning trajectories backward from the final answer as “abductive subtask planning.” However, in terms of methodological implementation, the actual process still relies on a large language model to generate sub-question decompositions based on the final answer and the evidence set, and subsequently produce the thought and action fields.\n\nb. The paper mentions that golden evidence for sub-questions is selected based on a cosine similarity threshold of 0.9, but it does not analyze the rationale or sensitivity of this threshold. The absence of threshold ablation or robustness experiments makes this step appear arbitrary, and its impact on factual precision and recall remains unclear. Although the paper refers to an “optimal, dependency-aware reasoning plan” and “minimally sufficient w.r.t. F̂,” it does not provide objective criteria for determining what constitutes “optimality” or “minimal sufficiency,” nor does it explain how the search space is pruned or how the process recovers from failures. Consequently, the so-called abductive reasoning in this paper resembles an LLM-driven controlled trajectory reconstruction process rather than a formally constrained abductive reasoning algorithm with verifiable optimality.\n\n**2. Strong reliance on the“golden evidence subset F̂”**\n\nBoth key stages of EviPath assume access to a golden evidence subset F̂ that is highly relevant to the question. In Stage 1 (Abductive Subtask Planning), the model directly uses the final answer and F̂ to reverse-generate sub-questions. In Stage 2 (Faithful Sub-question Answering), the framework bypasses real retrieval by constructing a simulated environment, treating the complete set of supporting facts F as the retrieval corpus and selecting, for each sub-question, the subset F̂ᵢ most similar to its intermediate answer.\n\nHowever, in realistic open-domain RAG settings, retrievers often make errors and rarely have access to true golden facts. The authors construct this RAG agent training paradigm entirely based on simulated environments and synthetic data, yet the experiments do not verify its robustness or transferability in real-world open-domain scenarios, nor do they provide any evaluation of the constructed environment's quality.\n\n**3. Ablation studies are insufficient in depth and coverage**\n\nThe paper does not present behavioral case studies comparing planner-only and executor-only settings. For example, it remains unclear whether there are interpretable failure cases such as “retrieving the correct evidence but the planner following an incorrect reasoning path” or “the planner generating a correct plan but the executor producing a wrong answer.” The paper also does not explicitly explain why the planner is considered the main bottleneck compared to the executor. Overall, it lacks qualitative analysis and interpretable examples to substantiate these claims.\n\n**4. Lack of reproducibility and resource availability**\n\nThe paper does not provide code, and critical implementation details in the main text and appendix (such as generation parameters including temperature, top-p, max tokens, and n) are insufficiently described, making it difficult for reviewers to independently reproduce the results.\n\nThe dataset section only reports the overall scale (265k trajectories) but does not present specific cases or any quality evaluation metrics to demonstrate the validity and reasonableness of the synthesized trajectories."}, "questions": {"value": "**1. Comparison with RL-based methods**\n\nThe paper repeatedly claims that the proposed EviPath + SFT approach significantly outperforms RL-based RAG agents such as GRPO, Search-R1, and Graph-R1. However, it does not clearly specify whether these RL baselines were evaluated under the same model scale, training budget, and data size. Given the reported performance gap of up to +14.7 EM, the authors should further clarify the fairness of these comparisons.\n\n**2. On the difference in SFT data density**\n\nCould the performance improvement of EviPath primarily stem from denser supervision (265k complete reasoning trajectories) rather than the method itself? The authors are encouraged to conduct controlled experiments, such as reducing the number of trajectories or removing parts of intermediate annotations, to verify the true source of the observed performance gains.\n\n**3. On the definition and substance of “agentic RAG”**\n\nThe paper repeatedly emphasizes that EviPath trains RAG agents with agentic reasoning capabilities, yet the overall pipeline is entirely offline, involving only trajectory synthesis without real environmental feedback, state updates, or tool interaction. The authors should clearly define what constitutes agentic reasoning in the context of this paper and clarify how EviPath fundamentally differs from a workflow-based RAG system."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bb8p5k3aVR", "forum": "alNQNzaWhI", "replyto": "alNQNzaWhI", "signatures": ["ICLR.cc/2026/Conference/Submission2463/Reviewer_SxGi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2463/Reviewer_SxGi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903808419, "cdate": 1761903808419, "tmdate": 1762916247275, "mdate": 1762916247275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EviPath, a framework for synthesizing evidence-anchored reasoning trajectories to train RAG agents. Unlike reinforcement learning methods (which suffer from sparse rewards) or prior CoT synthesis (which lacks environmental grounding), EviPath introduces a three-stage abductive reasoning approach: Abductive Subtask Planning (ASP), Faithful Sub-question Answering (ESA), and Conversational Fine-tuning (CFT). Experiments on HotpotQA, MuSiQue, and 2WikiMultihopQA show consistent improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Reformulates reasoning path synthesis as an abductive reasoning problem, offering a new angle on data synthesis for agentic LLMs.\n\n2. The proposed Planner–Executor decomposition aligns well with the modular RAG agent architecture and avoids costly RL rollouts.\n\n3. Extensive experimental evaluation and good performance across multiple QA datasets, including both text-based and KG-based ones."}, "weaknesses": {"value": "1. \"We are the first to formulate the synthesis of reasoning paths for RAG agents as an abductive reasoning problem\". I think this is a bit strong and overclaimed. Previous works have made relative exploration (e.g., hotpotQA)\n\n2. Real RAG agents in deployment must handle dynamic, noisy, and incomplete search environments. Evaluating only on static corpora overestimates stability and underestimates retrieval uncertainty."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h1q5WI7xSb", "forum": "alNQNzaWhI", "replyto": "alNQNzaWhI", "signatures": ["ICLR.cc/2026/Conference/Submission2463/Reviewer_NVxW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2463/Reviewer_NVxW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007672623, "cdate": 1762007672623, "tmdate": 1762916246951, "mdate": 1762916246951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EviPath, a framework for synthesizing evidence-anchored reasoning trajectories to train RAG agents. Unlike reinforcement learning methods (which suffer from sparse rewards) or prior CoT synthesis (which lacks environmental grounding), EviPath introduces a three-stage abductive reasoning approach: Abductive Subtask Planning (ASP), Faithful Sub-question Answering (ESA), and Conversational Fine-tuning (CFT). Experiments on HotpotQA, MuSiQue, and 2WikiMultihopQA show consistent improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Reformulates reasoning path synthesis as an abductive reasoning problem, offering a new angle on data synthesis for agentic LLMs.\n\n2. The proposed Planner–Executor decomposition aligns well with the modular RAG agent architecture and avoids costly RL rollouts.\n\n3. Extensive experimental evaluation and good performance across multiple QA datasets, including both text-based and KG-based ones."}, "weaknesses": {"value": "1. \"We are the first to formulate the synthesis of reasoning paths for RAG agents as an abductive reasoning problem\". I think this is a bit strong and overclaimed. Previous works have made relative exploration (e.g., hotpotQA)\n\n2. Real RAG agents in deployment must handle dynamic, noisy, and incomplete search environments. Evaluating only on static corpora overestimates stability and underestimates retrieval uncertainty."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h1q5WI7xSb", "forum": "alNQNzaWhI", "replyto": "alNQNzaWhI", "signatures": ["ICLR.cc/2026/Conference/Submission2463/Reviewer_NVxW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2463/Reviewer_NVxW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007672623, "cdate": 1762007672623, "tmdate": 1763633416941, "mdate": 1763633416941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}