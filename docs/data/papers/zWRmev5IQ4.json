{"id": "zWRmev5IQ4", "number": 1944, "cdate": 1756969518201, "mdate": 1759898177297, "content": {"title": "DiTraj: Training-free Trajectory Control For Video Diffusion Transformer", "abstract": "Diffusion Transformers (DiT)-based video generation models with 3D full attention exhibit strong generative capabilities. Trajectory control represents a user-friendly task in the field of controllable video generation. However, existing methods either require substantial training resources or are specifically designed for U-Net, do not take advantage of the superior performance of DiT. To address these issues, we propose **DiTraj**, a simple yet effective training-free framework for trajectory control in text-to-video generation, tailored for DiT. Specifically, first, to inject the object's trajectory, we propose foreground-background separation guidance: we use the large language model to convert user-provided prompts into foreground and background prompts, which respectively guide the generation of foreground and background regions in the video. Then, we analyze 3D full attention and explore the tight correlation between inter-token attention scores and position embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled 3D-RoPE (STD-RoPE). By modifying only foreground tokens' position embedding, STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening cross-frame attention among them and thus enhancing trajectory control. Additionally, we achieve 3D-aware trajectory control by regulating the sparsity of position embedding. Extensive experiments demonstrate that our method outperforms previous methods in both video quality and trajectory controllability.", "tldr": "", "keywords": ["Text-to-video generation", "Controllable video generation", "Trajectory control", "Training-free"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b0b577ed129bb99c6ba197122fd05f5b555bd487.pdf", "supplementary_material": "/attachment/a02d3a2a4024968697f7b0b7b88f60d10d7145dc.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents DiTraj, a training-free framework for trajectory control in text-to-video generation tailored for DiT models. The method introduces two key innovations: foreground-background separation guidance via LLM prompt decomposition and inter-frame Spatial-Temporal Decoupled 3D-RoPE. STD-RoPE modifies position embeddings to enhance cross-frame attention among foreground tokens, improving trajectory alignment for large movements. Additionally, 3D-aware control is achieved by regulating position embedding density. Experiments on DiT-based models demonstrate superior performance in video quality and trajectory controllability compared to training-free and training-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) DiTraj fills a critical gap in training-free trajectory control for DiT-based video models, eliminating the need for expensive fine-tuning or inference-time optimization. Its compatibility with existing DiT architectures enhances real-world applicability.\n\n(2) Quantitative results and user studies consistently show DiTraj outperforms baselines in both trajectory alignment and video quality. The ablation study validates the contributions of foreground-background separation and STD-RoPE.\n\n(3) The analysis of 3D full attention mechanisms and the diagnosis of poor large-movement control as a result of low cross-frame attention scores demonstrate technical depth. STD-RoPE’s design to decouple spatial and temporal dimensions addresses this issue logically.\n\n(4) Readable Presentation: The manuscript is well-structured, with clear explanations of methodologies and intuitive visualizations."}, "weaknesses": {"value": "(1) The core idea of injecting control via cross-attention masks (foreground-background separation) follows standard practices in conditional diffusion models. There are no obvious bottlenecks or difficulties in extending it to the field of trajectory controled video generation.\n\n(2) Ambiguity in STD-RoPE Mechanism: While STD-RoPE improves performance, modifying position embeddings to eliminate spatial discrepancies raises concerns: the altered embeddings no longer encode true spatial positions, yet the paper does not clarify their semantic meaning or address potential side effects.\nUnderjustified Design Choices: The decision to retain temporal dimensions while aligning spatial embeddings is empirically effective but lacks theoretical justification. Why does preserving temporal embedding alone ensure motion coherence?"}, "questions": {"value": "(1) The foreground-background separation guidance relies on LLM prompt decomposition. How robust is this process to ambiguous user prompts (e.g., prompts with multiple foreground objects or vague scene descriptions)?\n\n(2) STD-RoPE modifies position embeddings to \"align\" spatial dimensions on different positions across different frames, making these embeddings no longer represent actual spatial coordinates. What do the modified embeddings encode, and how do they avoid confusing the model’s understanding of scene layout?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3c9dC3hTQD", "forum": "zWRmev5IQ4", "replyto": "zWRmev5IQ4", "signatures": ["ICLR.cc/2026/Conference/Submission1944/Reviewer_CB3N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1944/Reviewer_CB3N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1944/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761033024441, "cdate": 1761033024441, "tmdate": 1762915964868, "mdate": 1762915964868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free method for trajectory control in video diffusion models. This was achieved by manipulating the attention control for the foreground and background videos. Specifically, it proposes a STD-RoPE, an extension of the RoPE, to enhance such an separation. The paper shows good contributions in motivating the proposed problem and designing a corresponding solution to address that, which is appreciated. The paper offers a good experimental analysis to compare with existing works on both training-based and training-free approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The motivation of the proposed work sounds reasonable as the paper relays the trajectory control to the separation of foreground and background videos. By analyzing the characteristics of the full attention, it makes sense to achieve such separation via the masking operation in full attention, which can link to individual foreground and background. \n\nThe paper also shows a good analysis on why the STD-RoPE was used based on the analysis of attention map. \n\nThe paper also shows an extensive experimental comparisons with both training-based and training-free based methods. This demonstrates that the competitive of the proposed method."}, "weaknesses": {"value": "The paper shows several weaknesses that need to be carefully addressed, mainly around several unclear technical details. \n\n1. The principle around STD-RoPE is not very well presented. The paper mentions that the position embedding of video tokens with the bounding box was first selected as the anchor, then position embeddings with all other frames were modified to align with the anchor. If the spatial dimensions are aligned, that seems to contradict to the movement of the trajectory? This part is not well demonstrated in the paper. \n\n2. The Algorithm 1 was not illustrated with details as it was briefly mentioned in STD-RoPE section. It will be nice to offer more details to complement the previous comment on how this STD-RoPE was designed to separate the foreground and background movements associated with the bounding box. \n\n3. When describing the cross-attention in Eqn. (4), it will be nice to provide more clear details on what is the QKV here. This will better explain the injection of the trajectory. It also seems to be not clear on how this trajectory was encoded, especially when the bounding box was used. \n\n4. There are some prior works on point-based trajectory control, it appears the paper does not seem to discuss sufficient relevant works in this direction. What are the main advantage of using bounding box, instead of the point-based control? Can the proposed method still apply to the point-based trajectory control?\n\n5. The paper shows an ablation study in Table 3, where different mechanisms were investigated. It looks like the trajectory control will decrease the video quality. What is the main reason for that? \n\n6. In Fig. 7, the integration of SG yields notable alterations in the video layout. This does not seem to be clear in the results of the given Figure 7. Could you explain what is the notable difference in this layout?\n\n7. By looking at the used instruction template input into the LLM from the Appendix, that was really complex. How does the paper comes with this instruction template and how will others replicate the instruction template?"}, "questions": {"value": "1. What are the main advantage of using bounding box, instead of the point-based control? Can the proposed method still apply to the point-based trajectory control?\n\n2. How is the bounding box trajectory was encoded in the cross-attention process?\n\n3. How this STD-RoPE was designed to separate the foreground and background movements associated with the bounding box, especially on the spatially alignment operation. \n\n4. Why will the trajectory control decrease the video quality? Which is the main impacting factor from the proposed designs?\n\n5. How does the paper comes with this instruction template and how will others replicate the instruction template?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JvB8jxlosz", "forum": "zWRmev5IQ4", "replyto": "zWRmev5IQ4", "signatures": ["ICLR.cc/2026/Conference/Submission1944/Reviewer_ddfm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1944/Reviewer_ddfm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1944/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533949258, "cdate": 1761533949258, "tmdate": 1762915964415, "mdate": 1762915964415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel framework for precise object trajectory control in text-to-video (T2V) generation, specifically designed for Diffusion Transformer (DiT)-based models. The authors leverages an LLM to decompose user prompts into foreground (object) and background descriptions, which are used to guide video generation via a cross-attention mask based on bounding-box trajectories. A method to modify position embeddings of foreground tokens across frames is proposed, aligning spatial dimensions to enhance cross-frame attention while preserving temporal coherence, thereby improving trajectory control. This is complemented by an R-token mask to avoid artifacts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper innovatively combines prompt decomposition based on large language models with positional embedding operations in deep image processing, offering novelty compared to methods focused on U-Net. STD-RoPE represents an innovative application of 3D-RoPE to trajectory control.\n2. The methodology employs a step-by-step exposition supplemented by illustrative diagrams to enhance comprehension. The writing is precise and structurally rigorous."}, "weaknesses": {"value": "1.The foreground-background separation relies on an external large language model (Qwen3). If the model misinterprets the prompt, errors may be introduced.\n2. This method requires predefined trajectories. Some metrics set in the paper are not reasonable, and it fails to demonstrate whether this approach leads to reduced video quality. Video quality metrics should be added (such as fvd). Compared to prior work, the visualized scenes do not exhibit complex motion trajectories. Although the paper mentions optimization for large trajectories, the actual visualizations do not show significantly complex or varied trajectory movements.\n3.Experiments focus on single-object scenarios with simple backgrounds. Performance in cluttered or multi-object environments remains unverified.\n4.Text in images appears overly blurred and too small. Font formatting lacks consistency, resulting in an unprofessional appearance."}, "questions": {"value": "1. How does the method handle cases where the LLM generates inaccurate foreground/background prompts? Could a rule-based fallback be integrated to reduce dependency? How exactly is the LLM used for separation? What is the predefined prompt? Does it need modification?\n2. Is there a plan to extend DiTraj for interactive trajectory editing during generation, rather than relying on pre-specified boxes?\n3. The paper tests on Wan2.1 and CogVideoX. How portable is DiTraj to other DiT-based models (e.g., Sora-inspired architectures)?Could the performance improvement stem from changes in the video model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Bkm2boUihU", "forum": "zWRmev5IQ4", "replyto": "zWRmev5IQ4", "signatures": ["ICLR.cc/2026/Conference/Submission1944/Reviewer_Kuu7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1944/Reviewer_Kuu7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1944/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890963883, "cdate": 1761890963883, "tmdate": 1762915964214, "mdate": 1762915964214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors focus on achieving training-free trajectory control for Diffusion Transformer (DiT)-based video generation models through the control condition of bounding boxes. To this end, the authors propose the framework, DiTraj, which primarily consists of two key modules: foreground-background separation guidance and inter-frame Spatial Temporal Decoupled 3D-RoPE (STD-RoPE). Specifically, the model is guided to focus on the foreground and background regions. Furthermore, by observing and modifying the 3D full attention and 3D-RoPE mechanisms, the authors achieve trajectory control of the object. Extensive experiments demonstrate the effectiveness of the proposed framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The issue focused on by the paper is important. How to achieve user-friendly and efficient control for video generation models, including DiT-based models, is a topic of widespread interest.\n\n2. The analysis of the underlying mechanism is clear. The paper conducts a detailed analysis of the 3D full attention map and 3D-RoPE within the DiT architecture and proposes a specific method for training-free trajectory control."}, "weaknesses": {"value": "1. [Major] Limited diversity of experiments to demonstrate the robustness of the framework: In the qualitative comparison, the provided visualization examples are primarily limited to simple scenarios with a single foreground object undergoing significant motion. This raises concerns about the method’s applicability and robustness in several other key scenarios, such as cases with multiple foreground objects, situations involving small-magnitude object motion, or more complex scenes.\n\n2. [Major] Potential limited scope of method’s applicability: The proposed method achieves trajectory control by modifying the 3D-RoPE of tokens. However, this approach may perform very poorly or be nearly unusable in certain scenarios. For example, consider a case where multiple initially interacting foreground objects are intended to gradually separate (e.g., two penguins hugging each other and then moving in different directions). This is because when foreground objects have complex interactions, it may become extremely difficult to accurately disentangle and modify the 3D-RoPE for tokens corresponding to each individual object.\n\n3. [Minor] The paper may overclaim the effectiveness of its ‘3D-aware’ trajectory control. The demonstrated ‘3D-aware’ control is primarily limited to cases of object scaling (zooming in and out), and does not encompass all the 3D motion cases. For instance, it is unclear whether the method can handle a common 3D scenario, such as making a foreground vehicle follow a specific trajectory to drive in a circle to show the vehicle in different directions."}, "questions": {"value": "1. Regarding the robustness and applicability of the framework (related to Weaknesses 1 & 2): \n\n(1) On multi-object scenarios: The experiments primarily show single-object trajectory control. Could the authors elaborate on how the proposed framework, particularly the M^cross mask and STD-RoPE modification, would theoretically handle scenarios with multiple foreground objects? It is recommended to include a discussion or a qualitative example of a multi-object scene in their rebuttal.\n\n(2) On handling complex interactions: Another key challenge arises when objects interact physically, such as the example of “two penguins hugging each other and then separating.” Could the authors explain how to address instances where the bounding boxes overlap or interact? It is recommended to include a discussion and some qualitative examples of complex interactions in their rebuttal.\n\n2. Regarding the claims of ‘3D-aware’ control (related to Weakness 3):\n\nThe term ‘3D-aware’ may suggest an understanding of object geometry and viewpoint. The current method demonstrates excellent control over object scaling to simulate depth. To clarify the scope of this claim, could the authors comment on whether their method can handle viewpoint changes that require rendering an object from different viewpoints? For example, could it generate a car driving in a circle while showing its front, side, and rear views accordingly? Perhaps rephrasing the contributions as  ‘2.5D’ or ‘scale control’ would be more appropriate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CWWCeDmNj4", "forum": "zWRmev5IQ4", "replyto": "zWRmev5IQ4", "signatures": ["ICLR.cc/2026/Conference/Submission1944/Reviewer_ZZxd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1944/Reviewer_ZZxd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1944/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138685837, "cdate": 1762138685837, "tmdate": 1762915963961, "mdate": 1762915963961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}