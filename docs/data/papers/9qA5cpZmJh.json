{"id": "9qA5cpZmJh", "number": 2818, "cdate": 1757260048664, "mdate": 1763505334946, "content": {"title": "Entropy Proxy for LLM Memorization Score", "abstract": "Large Language Models (LLMs) are known to memorize portions of their training data, sometimes reproducing content verbatim when prompted appropriately. Existing memorization research rarely explores how training data influences memorization and often limits the experimental setup to a binarized memorization vs non-memorization catagory.  In this work, we investigate a fundamental yet under-explored question in the domain of memorization: How to quantitatively characterize memorization difficulty using intrinsic properties of training data in LLMs? Inspired by early studies using compression algorithms to filter out simple memorization cases, we explore the link between training data compressibility and memorization.   Through experiments on a wide range of open models without various setups, we present the Entropy–Memorization Law. It suggests that at the set-level, data entropy (estimator) is linearly correlated with memorization score. We also further investigate EM Law with several dimensions:  visualizing vocabulary size as an implicit factor, and applying the law to data with disparate semantics.", "tldr": "", "keywords": ["Memorization", "Entropy", "LLMs"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ad90e59314d67d53ed7b2219a044f484c6ad1f26.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The main question of the paper is to quantitatively characterize memorization difficulty of training data by an LLM using the intrinsic property of the data itself, such as entropy. The paper focuses on approximating memorization score, according to discoverable memorization, using the entropy of a set of instances. The key contribution is the entropy-memorization law, which suggests that data entropy is linearly correlated with memorization score. The law is derived based on empirical observation of entropy vs edit distance between ground-truth generation and LLM response."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The key strength of the paper is the connection between data entropy to memorization score incurred by an LLM. The observation that lower entropy data is at higher risk of memorization leakage has implications in future experiments. Experimental results look convincing -- I like the part where the authors realize that instance-level entropy estimate becomes noisy and requires further analysis, such as extending to set-level analysis.\n\nOverall, the paper is potential to quantify the difficulty of memorization from the aspect of training data."}, "weaknesses": {"value": "The paper lacks from preciseness. I elaborate below.\n\n- There is no formal statement of entropy-memorization law. Rather, the readers have to understand the law from Figure 5, which has an additional axis of unique token counts, making it difficult to get the main point.\n- Some choices look arbitrary and not well justified. For example, edit distance only looks into syntactic similarity, but does not consider the intrinsic difficulty of generating a response. A response with very low generation probability by the LLM may have lower edit distance with the ground-truth and therefore, tagged as memorized -- essentially, the paper neglects the intrinsic difficult of token generation by the LLM and its connection to memorization. \n- The choice of not including post-trained LLMs looks unjustified. Anyway, the proposed methodology is agnostic to training strategy.\n- I do not get why filtering trivial memorization is needed. There could be cases when the response needs to contain part of the prompt. Why the authors think that memorization is unlikely in those scenarios?\n- The authors find that instance-level entropy cannot explain memorization well. However, memorization is usually concerned at the level of individual instance. Moreover, the relation with sample space being smaller or large does not make sense to me. Is the analysis conducted for a single instance? If there are multiple instances, I do not understand why sample space range would be upper bounded by 50?\n- In Equation 1, independent token probability is considered. I do not know if *point probability* is a standard term. Is there a way to extend to conditional token probability, since this is how autoregressive LLMs work? Does it help with removing noise by looking at instance-level entropy? More specifically, the relation to vocabulary size has to be explained well. \n- Line 77 and Line 455-456 look conflicting with each other. Which one is correct? Low entropy = high memorization, or low entropy = low memorization?\n- Line 76, the readers do not know the definition of $r$."}, "questions": {"value": "Address the points in the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M7tqjDk29D", "forum": "9qA5cpZmJh", "replyto": "9qA5cpZmJh", "signatures": ["ICLR.cc/2026/Conference/Submission2818/Reviewer_BafX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2818/Reviewer_BafX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760873957822, "cdate": 1760873957822, "tmdate": 1762916390060, "mdate": 1762916390060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "JhdxFN4IRm", "forum": "9qA5cpZmJh", "replyto": "9qA5cpZmJh", "signatures": ["ICLR.cc/2026/Conference/Submission2818/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2818/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763505333929, "cdate": 1763505333929, "tmdate": 1763505333929, "mdate": 1763505333929, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how intrinsic properties of training data influence memorization in large language models. It proposes using data compressibility as a proxy to estimate how difficult a text snippet is for a model to memorize. The authors first test instance-level metrics such as zlib compression and empirical entropy, but these show weak correlation with memorization because each sequence is too short to provide a stable estimate.\n\nThey introduce a set-level approach that groups sequences by their memorization score and calculates entropy across each group’s aggregated tokens. This greatly increases sample size and produces strong linear correlations between entropy and memorization across multiple model families and training datasets. The result is presented as the Entropy–Memorization Law: text with higher entropy tends to have higher memorization difficulty.\n\nThe paper also examines the effect of token vocabulary size, sequence length, sampling strategy, and different semantic domains, and finds that the relationship remains consistent. A normalized entropy variant is presented to separate vocabulary size from token distribution uniformity. The authors argue that these findings can help assess privacy risks in training data and contribute to understanding memorization behavior in language models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Strengths - \n\n- The paper takes logical next steps to explore the space and runs experiments to verify their proposed law under varying conditions\n- The results generalize to varying data subsets of the chosen dataset\n- The paper tests across several model families and datasets corresponding to them."}, "weaknesses": {"value": "Weaknesses - \n\n- Does filtering data already create some bias? Will the estimator fit when you have high overlap between prompt and continuation?\n- The authors claim it can be used to audit models but applying the law in practice will require computing entropy over trillions of tokens and then identifying sequences which are at risk of memorization however the paper does not talk about the effect of dilution/repetition which may be the primary cause of some sequences being memorized/not-being memorized as they are seen several times or seen at the start and never seen again so the law should account for position in training as well to actually be impactful. Repetitions are handled during entropy computation I suppose.\n- The paper is overall very poorly written with grammar issues and repetition/figures are hard to read and seem rushed. \n- A lot of the ideas seem to overlap with [1] especially around instance level and have been shown to not be strong predictors such as compressibility. [1] is already cited but these points are not brought up\n\n[1] Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon (https://arxiv.org/abs/2406.17746)"}, "questions": {"value": "Questions - \n\n- Line 157 - Isn't the opposite true? If there is more randomness the compressiblity should be low.\n\nSome Writing Nits - \n\n- Line 83 - \"A model-agnostic is compute-efficient.\" - Issue with writing\n- Line 107 is redundant and broken\n- Line 445 - Audition -> Auditing?\n\nRelevant Citations - \n\n- Understanding Memorisation in LLMs: Dynamics, Influencing Factors, and Implications (https://arxiv.org/abs/2407.19262) - Explores memorization with synthetic data and discusses effect of entropy on memorizability"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TkammbS8eZ", "forum": "9qA5cpZmJh", "replyto": "9qA5cpZmJh", "signatures": ["ICLR.cc/2026/Conference/Submission2818/Reviewer_H99e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2818/Reviewer_H99e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489208878, "cdate": 1761489208878, "tmdate": 1762916389887, "mdate": 1762916389887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of quantitatively measuring how difficult an example is to memorize. Building upon existing notions of example compressibility, they show that a naive approach to estimating the instance-level entropy can be too noisy to adequately correlate with the memorization score. To address this shortcoming, they propose a level-set based estimator of the instance entropy. In particular, they show that this level-set based estimator correlates linearly with memorization scores  Next, they show that memorized examples comprise an exponentially smaller subset of the token space and once the entropy is normalized with respect to the total token support, the trends reverse. Finally, the authors establish that the identified entropy/memorization law is consistent across different semantic clusters of the pretraining data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper attempts to solve an important and interesting problem for large language models: how easily are different sequences memorized? Although there are various intuitive heuristics present in the prior work, this paper focuses on deriving a quantitative description to predict what examples are faster to memorize versus generalize. Thus, I believe that the problem statement posed is interesting. The paper also conducts a methodological investigation, exploring various facets of their metric which increases the readers understanding of memorization in LLMs."}, "weaknesses": {"value": "(1) Issues with clarity: I found the paper at some point difficult to understand. For example, after investigating the failures of instance-level entropy calculation, the paper introduces their level-set based estimator. However, I found this transition and the motivation of the level-set entropy calculation to be difficult to motivate. The paper notes that the level-set aggregates more samples than the instance level methodology. However, this still feels incomplete: there are many potential ways by which to aggregate the sequences to generate less noisy entropy estimates. Why are level-sets of the memorization scores the appropriate mechanism by which to aggregate. The lack of justification for this choice, and the lack of ablation of other choices feels scientifically incomplete and leaves the possibility of better estimators/proxies of memorization. \n\n(2) Entropy Metric Seems Post-Hoc: As far as I can tell, the metric introduced here does not have predictive power. Since the entropy estimates depend on the memorization score, it does not appear possible to actually use the entropy to predict which examples are likely to be memorized. If this is indeed the case, it appears to be a significant weakness of the method: what would be the motivation for computing the entropy proxy if I need to compute the memorization score before-hand anyway. I would definitely appreciate if the authors could dispel my misunderstanding here, or explain what was their intended use-case for the entropy proxy.\n\n(3) Insufficient Analysis/Explanation of Results: I found it quite confusing, for example, that the entropy-memorization law seems to reverse once normalized entropy is introduced. Although the authors do note this, they don't provide any explanation or hypotheses for why the trend becomes negative. It would significantly enhance the interpretability of their results and their contributions if they would provide some more interpretation of this finding. Similarly, the experiment with the semantic consistency was not entirely explained."}, "questions": {"value": "(1) Please add some more explanation about why aggregation across the level sets was preferred rather than other aggregation techniques to denoise the entropy estimates?\n\n(2) Please clarify whether the entropy proxy can be used to predict which sequences are likely to be memorized a-priori (without training the model). If this is not possible, please clarify what is the intended usage of this proxy.\n\n(3) Please clarify why does the entropy-memorization law reverse after normalization (or if there is some interpretation/analysis of this)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gQmxDKwui4", "forum": "9qA5cpZmJh", "replyto": "9qA5cpZmJh", "signatures": ["ICLR.cc/2026/Conference/Submission2818/Reviewer_jV3X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2818/Reviewer_jV3X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673467101, "cdate": 1761673467101, "tmdate": 1762916389750, "mdate": 1762916389750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores whether training data entropy or compressibility can serve as a proxy for LLM memorization difficulty. The authors propose the Entropy–Memorization (EM) Law, stating that the entropy of training data (computed via token-level statistics) correlates linearly with a memorization score (computed using Levenshtein distance). They perform experiments on multiple open LLMs (OLMo, OpenLLaMA, Pythia) and datasets, finding a strong linear correlation (Pearson’s correlation $\\sim$ 0.9)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper explored a different perspective of connecting intrinsic property of training data to memorization.\n2. Experiments have been conducted on multiple LLMs."}, "weaknesses": {"value": "1. The paper needs an extensive amount of re-writing. It reads more like an exploratory report at present. There is lack of coherence across all the sections and the writing of the paper feels like has been done in a rush. For example, \n\n- Page 2 line 107 - \"Choices of LLM and Training Corpus lama (Geng & Liu, 2023),\" -- this ends here, followed by the next line on the next page as \"We selected four family of pre-trained only LLMs:..\"\n\n- Line 122 - \"...we repeatedly randomly sample a sequence (length > |p+ s|) from the dataset until the number reaches the required number.\" -- What is meant by number reaching the required number?\n\n2. Line 77 states that \"It suggests that higher entropy correlates with a higher memorization score in LLMs.\" -- does the setup really hint at that? Doesn't the increase in distance indicate that as entropy increases, the model reproduces the data less accurately? So higher-entropy data are harder to memorize, and the model tends to fail on them, which is intuitive as well. It seems like more than higher memorization score, this metric hints at how difficult the memorization is. The paper began with memorization difficulty and gradually entered into memorization score for some reason, which is not clear.\n\n3. The connection between data entropy/compressibility and memorization has been suggested in [a]. It is not clear how different things in this paper are with the cited paper.\n\n4. There are no other quantitative metrics used like perplexity for comparison with entropy.\n\n5. The pseudcode of algorithms are provided with no explanation provided. The experiment section is written in a form of a list -- one after another finding, with lack of intuition or reasoning as to why one would be interested in it. \n\n\n\n[a] Carlini, Nicholas, et al. \"Extracting training data from large language models.\" 30th USENIX security symposium (USENIX Security 21). 2021."}, "questions": {"value": "Same as Weaknesses above and the following:\n\n1. Since edit distance doesn't capture semantic similarity/dissimilarity, how will this method perform if the Levenshtein distance is changed to some other metrics like BertScore or some other form of embedding?\n\n2. What theoretical explanation justifies calling it an Entropy-Memorization law rather than an empirical correlation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6v2fEgmvVT", "forum": "9qA5cpZmJh", "replyto": "9qA5cpZmJh", "signatures": ["ICLR.cc/2026/Conference/Submission2818/Reviewer_BzzB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2818/Reviewer_BzzB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739546550, "cdate": 1761739546550, "tmdate": 1762916389598, "mdate": 1762916389598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}