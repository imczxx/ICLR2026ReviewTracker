{"id": "771P34sqnn", "number": 18312, "cdate": 1758286287321, "mdate": 1762946084374, "content": {"title": "FaceCoT: A Comprehensive Benchmark for Face Anti-Spoofing with Chain-of-Thought Reasoning", "abstract": "Face Anti-Spoofing (FAS) typically depends on a single visual modality when defending against presentation attacks such as print attacks, screen replays, and 3D masks, resulting in limited generalization across devices, environments, and attack types. Meanwhile, Multimodal Large Language Models (MLLMs) have recently achieved breakthroughs in image–text understanding and semantic reasoning, suggesting that integrating visual and linguistic co-inference into FAS can substantially improve both robustness and interpretability. However, the lack of a high-quality vision–language multimodal dataset has been a critical bottleneck. To address this, we introduce FaceCoT (Face Chain-of-Thought), the first large-scale Visual Question Answering (VQA) dataset tailored for FAS. FaceCoT covers 14 spoofing attack types and enriches model learning with high-quality CoT VQA annotations. Meanwhile, we develop a caption model refined via reinforcement learning to expand the dataset and enhance annotation quality. Furthermore, we introduce a CoT-Enhanced Progressive Learning (CEPL) strategy to better leverage the CoT data and boost model performance on FAS tasks. Extensive experiments demonstrate that models trained with FaceCoT and CEPL outperform state-of-the-art methods on multiple benchmark datasets.", "tldr": "FaceCoT is a large-scale vision-language benchmark dataset with Chain-of-Thought (CoT) annotations designed for Face Anti-Spoofing (FAS).", "keywords": ["Multimodal Large Language Models", "Chain-of-Thought", "Face Anti-Spoofing"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/45586972c84a6fe6c585845314b8e2e91add4bea.pdf", "supplementary_material": "/attachment/869fdf87c471be2de32e6fbd27edcb8122b66188.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces FaceCoT, a multimodal VQA dataset and CoT reasoning framework for face anti-spoofing task. The dataset contains 1.08 million samples across 14 attack types, including a verified subset (FaceCoT-Gold100K) and a reinforcement learning–expanded set (FaceCoT-Silver982K). The authors further propose CEPL, a two-stage training strategy: (1) visual enhancement pre-training using CoT data, and (2) multi-task joint training that combines CoT reasoning with binary classification. Experiments on 11 public FAS benchmarks show clear performance gains, demonstrating the effectiveness and generalization ability of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a clear and forward-looking problem. Introducing CoT reasoning into face anti-spoofing is novel and enhances model interpretability.\nThe FaceCoT dataset is large and diverse, offering rich vision-language annotations and a strong basis for future research.\nThe proposed CEPL framework adopts a clear two-stage training strategy that effectively balances reasoning and classification, improving both accuracy and interpretability."}, "weaknesses": {"value": "The paper approaches the problem from the perspective of Chain-of-Thought (CoT) and interpretability, yet all evaluations of interpretability are presented as qualitative descriptions, lacking quantitative metrics or user studies to assess the usefulness or accuracy of the explanations.\n\nThe multi-task joint training lacks rigorous empirical validation. The authors emphasize the importance of MJT, but no ablation comparing “VEP-only” or removing MJT is provided, making it unclear whether the multi-task mechanism itself contributes to the improvement.\n\nThe choice of LoRA fine-tuning lacks experimental justification. In CEPL, the first stage uses full-parameter fine-tuning, while the second stage adopts LoRA. Although the authors claim it prevents catastrophic forgetting, there is no quantitative comparison or verification, making this conclusion largely speculative.\n\nThe consistency between CoT reasoning and final predictions is not validated. The evaluation relies solely on binary classification metrics, which cannot demonstrate whether the generated reasoning aligns with the final decision or whether CoT supervision truly contributes to model reasoning behavior."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fWMaVLMoWd", "forum": "771P34sqnn", "replyto": "771P34sqnn", "signatures": ["ICLR.cc/2026/Conference/Submission18312/Reviewer_uuxn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18312/Reviewer_uuxn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761278997083, "cdate": 1761278997083, "tmdate": 1762928031343, "mdate": 1762928031343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "u5myfIgPeo", "forum": "771P34sqnn", "replyto": "771P34sqnn", "signatures": ["ICLR.cc/2026/Conference/Submission18312/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18312/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762946083415, "cdate": 1762946083415, "tmdate": 1762946083415, "mdate": 1762946083415, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the core issues of poor generalization and interpretability of unimodal methods in the face anti-forgery (FAS) task by proposing the FaceCoT dataset and the accompanying CEPL training strategy. FaceCoT is the first large-scale visual question answering dataset for FAS, containing 1.08 million samples, covering 14 attack types, and innovatively introducing a six-layer structure called Chain of Thought (CoT) annotation. The CEPL strategy effectively improves model performance through visually enhanced pre-training and multi-task joint training. Experiments show that this method achieves state-of-the-art performance on 11 benchmark datasets (with an average AUC improvement of 4.06%). However, the paper lacks significant verification of the method's innovation and experimental depth."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Data Scale and Quality: 1.08 million samples, covering 14 attack types, with annotation quality ensured through manual verification and reinforcement learning optimization (GPT-4o + manual verification + RL optimization).\n2. Accurate Problem Identification: Targets the bottlenecks of generalization and interpretability in the FAS domain, proposing a multimodal solution that points in the right direction.\n3. Extensive Experimental Coverage: Tested on 11 benchmark datasets, demonstrating strong cross-domain performance.\n4. Extensive Explanation of Interpretability: The appendix provides extensive CoT output examples to enhance the credibility of the results."}, "weaknesses": {"value": "1. Lack of innovation in dataset construction methods:\nThe RL annotation model directly uses the existing VRFT framework without any improvements tailored to the specific characteristics of the FAS task.\nCompared to works such as ForgerySleuth (Sun et al., 2024) and BusterX (Wen et al., 2025), the methodological advantages are not demonstrated.\n2. Validation of visual encoder improvements is lacking:\nFeature visualization comparison before and after pre-training is lacking.\nNo feature space analysis is performed to demonstrate the effectiveness of fine-grained feature learning, and no corresponding ablation experiments are performed.\n3. Gaps in CoT module contribution analysis:\nThe necessity and contribution of each module in the six-layer CoT architecture are not verified.\nUnable to distinguish whether the performance improvement is due to the CoT design or a simple data scale effect.\n4. Incomplete baseline comparison:\nLack of comparison with FAS methods based on open-source MLLMs such as LLaVA.\nNo significant advantages over simple baselines are demonstrated."}, "questions": {"value": "1. Innovation question: Compared with recent work such as ForgerySleuth and BusterX, what substantial innovations does this paper offer in terms of RL annotation model design and multimodal fusion methods? Please provide a detailed comparative analysis.\n2. Feature Learning Verification: How can we prove that the visual encoder indeed learns better fine-grained features through CoT supervision? Please provide direct evidence such as feature visualization and attention analysis.\n3. CoT Architecture Necessity: Are all six layers of CoT modules indispensable? Please provide module-level ablation experiments to quantify the contribution of each module to the final performance.\n4. Method Advantage Verification: Compared with directly applying existing MLLMs (such as LLaVA) to the FAS task, what are the advantages of this method? Please provide corresponding comparative experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bDXqMg7pIU", "forum": "771P34sqnn", "replyto": "771P34sqnn", "signatures": ["ICLR.cc/2026/Conference/Submission18312/Reviewer_HErr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18312/Reviewer_HErr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761476389000, "cdate": 1761476389000, "tmdate": 1762928030987, "mdate": 1762928030987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FaceCoT, a large CoT-annotated resource for face anti-spoofing (FAS), and a two-stage training scheme called CoT-Enhanced Progressive Learning (CEPL). Stage-1 performs full-parameter SFT on CoT text (termed “Visual Enhancement Pre-training”), and Stage-2 conducts multi-task joint training for CoT generation and binary classification while inheriting the Stage-1 vision encoder. The approach reports strong cross-domain results on 11 benchmarks and showcases interpretable CoT outputs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.FaceCoT dataset: A large, structured CoT dataset tailored to FAS (three major attack types, 14 subtypes) with a clear six-part template (Caption, Facial Description, Facial Attributes, Reasoning, Spoofing Description, Conclusion).\n2.Methodological advantage: CEPL’s stage-wise optimization reduces task interference compared to single-stage multi-task training. \n3.Breadth of empirical validation: The method is evaluated across 11 public FAS datasets, achieving the best scores and highlighting robustness in cross-domain settings."}, "weaknesses": {"value": "1.Stage-1 motivation vs. evidence attribution.\nStage-1 is described as “Visual Enhancement” aimed at strengthening the vision encoder, yet the implementation performs full-parameter SFT (vision encoder, connector/resampler, and LLM all updated). Without module-freezing or representation probes, improvements cannot be causally ascribed to the vision side (language-side learning or template alignment may also drive gains). The text and Fig. 4 explicitly present this setup, but current ablations (CEPL vs. single-stage; with/without RL) do not isolate the source of the benefit. More controlled analyses are needed to substantiate the “visual representation” claim. \n2.Potential shortcut / label-template leakage from category-specific hints.\nThe paper provides tailored hints per spoofing type (e.g., “Photographing a printed photo constitutes spoofing.”), and also enforces exact match of <Conclusion> to the ground truth during verification. This design risks a shortcut from hint→label rather than genuine visual analysis. Clarification is needed on whether such hints are visible to the detector during training (beyond the caption-model annotation phase) and how performance behaves without hints. \n3.Cross-domain evaluation lacks finer-grained decomposition.\nThe paper emphasizes robustness to unseen attack modalities (e.g., HKBU-MARs-V1+ and HiFiMask include transparent, plaster, and resin masks) and shows strong overall gains. However, it does not provide per-attack-type breakdowns and significance tests. Per-attack-type granular evaluations are needed to verify whether improvements are uniform across subtypes. \n4.Instructional baselines and CoT ablations are not yet sufficient.\nTo separate the value of task instruction from the value of the multi-field CoT structure, include an instructional baseline that asks only the task question (no intermediate CoT fields). This will clarify whether CoT structure—not just task prompting—drives the observed gains."}, "questions": {"value": "1 Stage-1 claims to enhance visual representations, but it updates all modules. Can you provide controlled evidence (e.g., module freezing) that attributes gains to the vision encoder rather than the language side? \n2 Precisely when/where are hints injected? Are they present during detector training or only in the caption-model pipeline? Please report with-hint vs. no-hint performance. \n3 Please add an instruction-only baseline (only the task question; no intermediate fields) to quantify each field’s contribution. \n4 For cross-domain claims on unseen attacks (e.g., transparent/plaster/resin masks in HKBU-MARs-V1+ and HiFiMask), provide per-attack-type results with dispersion and significance to substantiate uniform gains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tudgBoax27", "forum": "771P34sqnn", "replyto": "771P34sqnn", "signatures": ["ICLR.cc/2026/Conference/Submission18312/Reviewer_eFWt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18312/Reviewer_eFWt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879707226, "cdate": 1761879707226, "tmdate": 1762928030563, "mdate": 1762928030563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a dataset called FaceCoT, the first visual question-answering dataset specifically designed for face anti-spoofing (FAS), containing 14 attack types as well as structured CoT annotations. Besides, the paper proposes a CoT-Enhanced Progressive Learning strategy (CEPL), which optimizes the visual encoder and performs multi-task joint optimization through staged training. Experimental results show the good performance of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.This paper is easy to follow.  \n\n2.This paper presents an open-sourced well large scale of VQA dataset for FAS task, which will benefit the FAS research community.  \n\n3.The proposed CoT structure with CEPL strategy sounds reasonable and improves the reasoning and detection performance."}, "weaknesses": {"value": "1.The technical contribution of this paper is limited, as the basic SFT and RL are conventional without specific design for FAS task.  \n\n2.During the data expansion, unlabeled spoofing data is utilized to perform RL of the caption model. With simple format and accuracy rewards, it is hard to handle semantic errors, as well as the unseen spoofing types, which raises my concerns about the accuracy of the expended annotation."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BhfReYeaUF", "forum": "771P34sqnn", "replyto": "771P34sqnn", "signatures": ["ICLR.cc/2026/Conference/Submission18312/Reviewer_nPFN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18312/Reviewer_nPFN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917310604, "cdate": 1761917310604, "tmdate": 1762928029950, "mdate": 1762928029950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}