{"id": "PKidr9Ruli", "number": 14419, "cdate": 1758234855036, "mdate": 1763349920108, "content": {"title": "On Natural Ways to Generate and Their Provable Power", "abstract": "Diffusion language models have recently emerged as a competitive alternative to autoregressive language models. Beyond next-token generation, they are more efficient and flexible by enabling parallel and any-order token generation. However, despite empirical successes, their computational power and fundamental limitations remain poorly understood. In this paper, we formally study whether non-autoregressive generation in Masked Diffusion Models (MDM) enables solving problems beyond the reach of Auto-Regressive Models (ARM). Our results show that MDM with sufficiently large context length is computationally universal with decoding steps matching the optimal parallel time complexity in PRAM. However, when controlling for other factors, MDM's flexibility to generate in any-order does not expand what ARM can already solve. To address this, we propose a new form of generation called any-process generation, which extends MDM with capabilities to remask, insert and delete tokens. Theoretically and empirically, we demonstrate these capabilities enable scalability to significantly harder reasoning problems that are otherwise intractable for ARM and vanilla MDM. Additionally, they prove essential for generation tasks where objects naturally evolve through non-sequential processes, crucial for extending current LLMs beyond natural language to domains such as coding and science.", "tldr": "", "keywords": ["masked diffusion models", "autoregressive models", "expressiveness"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3c69c9cb6b1880d1b924903673be70b593b3209.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces any-process masked diffusion (AP-MDM), a rewriteable-canvas variant of masked diffusion that augments the usual unmask head with insert, delete, remask, and stop decisions. It first argues, by construction and by complexity intuition, that autoregressive (AR) decoders and standard masked diffusion (MDM) share the same fundamental hardness on combinatorial and structured tasks: with a write-once canvas, you either pay exponential space (beams, branch caching) or exponential time (restarts). AP-MDM changes the computational regime: by allowing in-place edits and dynamic length, it turns generation into an anytime control process that can backtrack without storing branches. The formal upshot is polynomial working memory (space) with time that scales to instance hardness; you still do not beat worst-case exponential time, but you avoid the classic space blow-up of AR/MDM search.\n\nOn the learning side, AP-MDM keeps the strong denoising core of MDM but trains the edit heads via self-supervised surrogates (no ELBO over variable length) to remask, insert, delete, and halt. This decomposes hard global mapping into local repair decisions, which the paper shows is data-efficient in practice. At inference, a simple edit policy iterates these heads; compute is anytime and grows with problem difficulty. Empirically (Sudoku, Dyck/bracket languages, and related structure-heavy settings), AP-MDM achieves higher accuracy at much lower memory than AR or vanilla MDM under realistic budgets, illustrating the claimed time–space tradeoff: it “spends steps” instead of “storing branches.”"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Big-picture is spot on: AR and standard MDM appear different yet share the write-once-canvas weakness; the paper names it clearly and proposes a simple, natural fix drawn from code and biology, a rewriteable, variable-length canvas.\n2. Claims are precisely scoped; the contribution addresses space, not time. Framing the shift as N to NSPACE is apt; the constructions make the point transparent, and the proofs align with the reported experiments. More baselines/ablations (see weaknesses) would be welcome, but they are not necessary for credibility.\n3. The writing shows taste and control. The title and opening pose the question cleanly; the narrative moves from diagnosis to design to consequences with clarity; examples make the mechanism feel inevitable. I came away having learned a lot.\n4. The design is consequential. It introduces a generation scheme in which test-time compute scales via edits and dynamic length, with implications for training targets, halting criteria, and even what counts as a step in generative modeling.\n5. It lands at the right moment. Recent advances supplied the prerequisites: variable-length backbones that keep state coherent under edits (FlexMDM, DAEDAL), stable credit assignment and scheduling for where and when to edit (P2, ReMDM, R3), and practical stopping. This paper assembles those pieces into a coherent, learnable edit process."}, "weaknesses": {"value": "1. No ELBO for edits + scheduling and halting schemes are opaque/potentially suboptimal: Learning relies on surrogate labels and augmentation design. The induced trajectories may not reflect human or task-native edit patterns, so calibration and likelihood reporting are unavailable. (see Questions below)\n2. Curious to see compute-normalized baselines (within space budget). With (possibly suboptimal) scheduling learned from augmentation and no cache of failures, how strong is AP-MDM beyond the space advantage? \n2. Would be interesting to ablate rewrite-only with sufficiently large generation length. This would clarify which ops carry the gains."}, "questions": {"value": "1.\tA key implication is that the edit trajectory now matters, not just the final text. What should count as an optimal trajectory, and how should we obtain it: human or naturally logged edits for training, or algorithmic schemes such as minimum edit distance, maximum expected information gain per step?\n2.\tIf edit trajectories become first-class data, would you formulate any-process training with an ELBO? Do you see likelihood-based evaluation and probabilistic queries over edits becoming standard practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "txtnvkNxpP", "forum": "PKidr9Ruli", "replyto": "PKidr9Ruli", "signatures": ["ICLR.cc/2026/Conference/Submission14419/Reviewer_qEHt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14419/Reviewer_qEHt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537151560, "cdate": 1761537151560, "tmdate": 1762924827664, "mdate": 1762924827664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new framework for generative modeling, in which the model is allowed to perform extra operations such as remasking and insertion (rather than solely unmasking/next-token prediction). This more flexible generation process allows computational circuits of higher complexity to be representable, which the authors compare rigorously against masked diffusion models and autoregressive models. Furthermore, the authors demonstrate experimentally on simple tasks that their method appears much more robust  and seems to learn more efficient representations/solutions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. I think it is a good idea to investigate various generation strategies and their complexities/capacities in a way that is fairly model-agnostic. The authors do exactly this; by adopting a complexity-theoretic language they are able to show separations in a way that does not depend on parameterization. The reason for improvement is clear, and they present it clearly.\n\n2. The experimental results are convincing. The authors' proposed modification to the generation process sounds reasonable and makes intuitive sense, but without definitive empirical proof it doesn't mean as much. In this case, I feel that the authors are able to demonstrate (despite the small scale) that their method is learning qualitatively better solutions (as evidenced by e.g. length generalization in Figure 4b). Of course, while it would be nice to see how things fare at scale, this is the first step. My philosophy is that if a change in the model only slightly increases metrics on synthetic tasks instead of showing clear separations, it isn't as interesting to scale it up. The experiments in this paper certainly indicate that something new is here, but the question of how it does at large remains open."}, "weaknesses": {"value": "I think it is hard to decipher from the theorem statements (and also from the proofs, at least for me) exactly *which* types of problems are hard for unidirectional sequential models but easy for your type of model. Sudoku makes natural sense, and it was no surprise to see the proposed method do so well on that task. However, it feels like there is a big gap between the complexity-theoretic statements and what is demonstrated empirically. \n\nIn my life I have found it hard to convince people to run large-scale experiments to test modeling changes on the general basis of expressivity and larger capacity; for me it is always much more successful to have a crystal clear problem instance (such as modeling code with parenthesis, ...) where it is clear to demonstrate (both theoretically and empirically) that other methods fail and your method is the fix. Having an example/mascot problem like this gives you a kind of language to explore other problems in, and it helps tie together the different perspectives for your reader.\n\nReading this paper, I felt it difficult to really track the expressivity improvement between the theory and the experiments. The extra flexibility is clear to see intuitively, but the whole point of the theory + empirics is to qualify and quantify exactly what the gains of your method are. Sorry for the long rant, but my main point is that it would be more convincing and gratifying to read if the connections between the theoretical improvements and the experimental results could be explored at a finer scale."}, "questions": {"value": "1. The authors are looking through a perspective of expressivity/capacity, which is fine and often the right starting point. I am, however, interested if the authors found any qualitative differences in training dynamics/hyperparameter choices/... between their method and ARM or MDM? Anecdotally, does it feel similar training them? I ask because sometimes when a model is, for example, way more robust to learning rate, it's indicative of what kinds of extra structure its baking into its solution.\n\n2. It would be nice to see experiments at larger scale. This is always the case though, isn't it :)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A7NUrN1ref", "forum": "PKidr9Ruli", "replyto": "PKidr9Ruli", "signatures": ["ICLR.cc/2026/Conference/Submission14419/Reviewer_Fhff"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14419/Reviewer_Fhff"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959796617, "cdate": 1761959796617, "tmdate": 1762924827326, "mdate": 1762924827326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies alternative generation paradigm that breaks the AR hegemony. It argues \"natural\" process that include editing and complete rewriting the intermediate states offer concrete benefits over AR and Mask-dLM. It first formalizes Mask-dLM's power and limits, then proposes Any-Process dLM supporting masking, unmasking, insertion and deletion. AP-dLM is a natural ability-superset model to existing alternatives and strictly expands solvable problems under practical resource bounds. Primitive empirical results on Sudoku, parity and graph editing confirms the proposed method as a proof-of-concept."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1) The clarity of the process-level framing and the proposed AP-dLM is good.\n2) The demonstrated time-optimal PRAM simulation for Mask-dLM and time-space-optimal one for AP-dLM is mind-freshing. \n3) The experiments are compellingly testifying the core claimed benefits at a small scale."}, "weaknesses": {"value": "1) Please pardon me if I'm being too harsh on this - it claims to have solved everything but really nothing in practice - one of the major issues in language modeling is always about scaling. There's little, if any, discussion on how the proposed method could even scale-up to a medium level. If we break down language generation into a few rather heterogeneous mini-procedures, the two most important steps are:\n\n - Context Encoding, meaning that how the context information is collected and compressed;\n - Action Prediction, meaning how the representation vectors are then transformed into further actions that alters the context.\n\nThe real question in NLG ever since the N-gram era really has been that, given a sequence of N tokens that can be sufficiently long, how to balance between being practically efficient in encoding context and predict the next growth of context that holds the **monotonicity** of context to some extent, such that we can somewhat efficiently reuse the previous computation when the context is altered by the generation step. I don't think the community is unaware of the unnaturalness of AR all along, most scaled-up practices uses it only because we can easily train very large and thus generally capable AR models at ease.\n\n2) Continuing the previous discussion, talking about data efficiency alone without carefully check how efficient can the proposed model utilize the data can be less meaningful. External validity beyond toy settings remains to be shown.\n\n3) A lot of existing efforts in building non-Mask/insertion-based dLMs that are moderately scaled up, such as Insertion Transformer, InDIGO, Non-Monotonic Sequential Text Generation, XLNet, Levenshtein Transformer and InsNet are missing, showing the potential insufficient literature review by the authors."}, "questions": {"value": "1) I don't think that when insertion operations and deletion operations are included, their conceptual \"addresses\" can be trivially provided by any existing position encoding without having to trigger the complete re-encoding of context. This caused my confusion when trying to understand THM 10. Can the authors provide more details on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8aw15H9ddb", "forum": "PKidr9Ruli", "replyto": "PKidr9Ruli", "signatures": ["ICLR.cc/2026/Conference/Submission14419/Reviewer_zY1v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14419/Reviewer_zY1v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971068588, "cdate": 1761971068588, "tmdate": 1762924826931, "mdate": 1762924826931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}