{"id": "0V0bQi24YC", "number": 1486, "cdate": 1756886832563, "mdate": 1759898206403, "content": {"title": "Video-LevelGauge: Investigating Contextual Positional Bias in Video Language Models.", "abstract": "Large video language models (LVLMs) have made notable progress in video understanding, spurring the development of corresponding evaluation benchmarks. However, existing benchmarks generally assess overall performance across entire video sequences, overlooking nuanced behaviors such as contextual positional bias, a critical yet under-explored aspect of LVLM performance. We present **Video-LevelGauge**, a dedicated benchmark designed to systematically assess positional bias in LVLMs. We employ standardized probes and customized contextual setups, allowing flexible control over context length, probe position, and contextual types to simulate diverse real-world scenarios. In addition, we introduce a comprehensive analysis method that combines statistical measures with morphological pattern recognition to characterize bias. Our benchmark comprises 438 manually curated videos spanning multiple types, yielding 1,177 high-quality multiple-choice questions and 120 open-ended questions, validated for their effectiveness in exposing positional bias. Based on these, we evaluate 27 state-of-the-art LVLMs, including both commercial and open-source models. Our findings reveal significant positional biases in many leading open-source models, typically exhibiting head or neighbor-content preferences. In contrast, commercial models such as Gemini 2.5 Pro show impressive, consistent performance across entire video sequences. Further analyses on context variation, context length, model scale, and multi-modal reasoning provide insights for mitigating bias and guiding model enhancement.", "tldr": "", "keywords": ["Contextual Positional Bais", "Video Benchmark", "Large Video Language Model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5447888345957c5bff22e348fdefcd4dfbec0b35.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors address the problem of **contextual positional biases** in large video-language models (LVLMs), where the content of a clip is interpreted inconsistently depending on its position within a video. While this issue has been explored in language models, it has not been studied in multimodal video-language settings, making this work both unique and novel. The paper introduces **Video-LevelGauge**, a benchmark designed to evaluate contextual positional biases in LVLMs across diverse tasks and video types. Beyond standard accuracy, it proposes novel **statistical metrics** to quantify such biases. Using these metrics, the authors further characterize **morphological patterns** in LVLMs, enabling the identification of specific phenomena underlying positional bias. Extensive experiments and analysis conducted with Video-LevelGauge provide valuable insights to guide future LVLM development and contextual positional bias mitigation. Overall, the paper is well-written and presents notable contributions to a previously underexplored area of LVLM research."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Video-LevelGauge incorporates a comprehensive coverage of six video-language tasks spanning diverse video types, including egocentric, media, and synthetic videos, enabling extensive evaluation of existing LVLMs for contextual positional biases across a multitude of settings.\n- Video-LevelGauge introduces three statistical metrics: $P_{mean}$, which captures the magnitude of positional bias, and $P_{ran}$ and $P_{var}$, which measure the volatility of model behavior. Together, these metrics provide a holistic assessment of positional biases in LVLMs.\n- The proposed morphological recognition provides a grounded approach to identifying potential root causes of contextual positional biases, providing a framework to diagnose and characterize positional biases in LVLMs.\n- Thorough analysis is performed on Video-LevelGauge using the proposed metrics across multiple factors, including context length and video type, revealing several insightful observations that could inform future efforts toward mitigating these biases."}, "weaknesses": {"value": "- For the positional bias metric, the relative score is intuitive and easy to understand. However, greater details could be provided on what the score entails: How is $RS_i$ computed if $S_{meta} = 0$, and what does $RS_i$ reflect in this case? Additionally, for the five types of morphological patterns, several categories are not straightforward to understand. In particular, the morphological phenomena of “lost in the middle”, “neighbor bias”, and “volatile” could be somewhat confusing. I would suggest for the authors to perhaps provide additional elaboration for each of these types in Section 3.4, and possibly how they correlate with model performance.\n- Although the authors validated against multimodal information leakage within each probe instance, the paper does not seem to address potential leakage between the probe clip and background video(s) during evaluation. Background videos with similar contexts to the probe clip could lead to cross-video leakage, allowing the LVLM to answer the query correctly by referring to background video content, which may be unintended.\n- Evaluation results for individual tasks (e.g., OCR, AP, etc.) only report $P_{ran}$, which measures the worst-case variation, but omit $P_{mean}$, which captures the average performance across instances. Including $P_{mean}$ would provide a clearer view of the overall positional bias for each task."}, "questions": {"value": "- “$\\nearrow \\text{if MSE}_1 \\leq 3 \\text{ and }k > 0.5$” [Appendix A.5] - Could the authors elaborate what the neighbor bias entails, and how the trend reflected by this condition correlates with this bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vbFvcCV63C", "forum": "0V0bQi24YC", "replyto": "0V0bQi24YC", "signatures": ["ICLR.cc/2026/Conference/Submission1486/Reviewer_sdzE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1486/Reviewer_sdzE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761244666749, "cdate": 1761244666749, "tmdate": 1762915783482, "mdate": 1762915783482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark suite for evaluating contextual positional bias of large video language models (LVLMs) considering long contexts. The data collection involves three stages,\n1. QA Generation: They collect videos from existing video-text resources including ego-centric videos, and generate frame-wise captions using GPT-4o. These captions are combined with crowd-sourced task definitions, and then an LLM generates question-answer pairs.\n2. QA Refinement: The generated question-answer pairs are filtered by GPT-4o, considering hallucinations and cases where the question does not require visual information in order to be answered. Human validators filter out the invalid question-answer pairs before proceeding to next step.\n3. Distractors: LLMs generate distractors (i.e., incorrect answer choices) for the collected question-answer pairs, later, again refined by human validators.\n\nAdditionally, the benchmark is divided into 7 sub-categories, which are OCR, Attribute Perception, Object Reasoning, Count Problem, Relationship Recognition, Action Reasoning, and Instructed Description. The benchmarked models are tested under different conditions, which are denoted as customized contexts. The set of customized contexts within this work include multiple videos inputs, long videos, multimodal interleaved input and lastly template video with ImageNet mean pixel values. This work introduces 3 different metrics to measure the positional bias, where all 3 metrics are centered around the relative score (RS) measure. These 3 metrics are, average relative score ( $P_{mean}$ ), difference between maximum and minimum relative score ( $P_{ran}$ ), and the variance in relative scores ( $P_{var}$ ). The evaluations on the proposed benchmark include 27 LVLMs including both open-weight and proprietary models where the model scale is ranging up to 108 billion parameters. Further studies investigates the effect of context type, context length and model size on positional bias."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- A novel benchmark for assessing the positional bias in video-language models.\n- Focuses an important aspect which could help to assess robustness of LVLMs under different positional settings, which could help further developing more trusthworthy LVLMs.\n- Detailed experimentation: 27 models, further beneficial studies on the effect of different design choices."}, "weaknesses": {"value": "- The presentation needs to be improved,\n\t- In Figure 3, the entire set of sub-tasks should be illustrated. This is not feasible.\n\t- In Figure 3, the tasks should be renamed by following the terminology already exist in the literature. I don't understand why someone should pose these tasks as *reasoning* tasks, where they are *recognition* tasks in fact. For instance, please see [Fig 1](https://arxiv.org/pdf/2306.13394) in MMU work to see the difference between reasoning and recognition type of tasks. So, the tasks should be renamed as,\n\t\t- Object Reasoning -> Object Recognition\n\t\t- Count Problem -> Object Counting (because actions can be counted also as well)\n\t\t- Action Reasoning -> Action Recognition\n\t\t- Attribute Perception -> Attribute Recognition (for consistency)\n\t- In Fig1(a), what do the numbers next to the tick and x marks?\n\t- Fig 2 is cluttered, I think it would be better if the only metric is positional variance.\n- Human refinement process remains entirely opaque. There are no details on this process, even how many human validators participated in the refinement process.\n- The proposed metric is not a standard metric used to evaluate the models, so I think this part should be expanded in the main text. This is such an important element of this paper but it only takes up 20 lines in the main text currently.\n\t- For instance, why should one use the relative score metric, and why should the standalone accuracy should be in the denominator?\n\t- What is the position $i$ ? What is the unit, seconds, or frame? Is this position absolute or relative? Are these positions randomly sampled for each individual example? Do these positions guarantee that the question-answer pairs remain valid for the video with custom context?\n\t- The morphological recognition (MR) term is confusing because morphology is a term which exists in NLP literature. Additionally, MR term currently seems unclear in the main text, and it is not motivated well enough."}, "questions": {"value": "- I think it would be good to show visual examples of customized contexts in the appendix part.\n- There is some typo on Fig2: GTP-4o-latest should be GPT-4o-latest.\n- There is also white text on the last figure (Fig. 22) which can be barely seen on the background: `\"Please output the questions and reference answers in the following JSON format: [ {'question': 'xxx', 'answer': 'xxx’}, {'question': 'xxx', 'answer': 'xxx’},\"`"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ahh9IJRD2a", "forum": "0V0bQi24YC", "replyto": "0V0bQi24YC", "signatures": ["ICLR.cc/2026/Conference/Submission1486/Reviewer_TpSR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1486/Reviewer_TpSR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589689596, "cdate": 1761589689596, "tmdate": 1762915783366, "mdate": 1762915783366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Video-LevelGauge, a benchmark exploring positional bias in large video language models (LVLMs). The benchmark covers a range of models for comprehensive evaluation and reveals that existing open-sourced ones suffer significant positional bias, exhibiting unstabe video understanding affected by the position of target content. The paper also provides a deep analysis on models’ behavior across divers perspectives, such as context length and model scales."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important and underexplored problem in video understanding — positional bias in LVLMs. Overall, the paper is well written and easy to follow.\n- The benchmark is well structured, featuring diverse videos and mechanisms that prevent single-frame or blind-bias predictions. The positional bias metric is clearly defined and suitable for evaluation.\n- The paper provides extensive experimental results across 27 leading LVLMs with in-depth analyses, offering valuable insights into model behavior."}, "weaknesses": {"value": "- While the benchmark design and evaluation criteria are solid, the results and analyses feel somewhat lukewarm and unsurprising. In Section 4.2, the main conclusion that positional bias tendencies vary across models and depend largely on training methods and model scale is expected. It is natural that larger models exposed to diverse video data perform better, and this conclusion could likely apply to other evaluation criteria beyond positional bias. I would appreciate a deeper, more tailored analysis of *why* such biases arise and *what* model characteristics contribute to distinct bias patterns (e.g., MR types). For instance, why do MiniGPT4-Video and InternVL3 (8B) show head preference? Are their training datasets skewed toward early-frame cues? \n- Consequently, it seems somewhat trivial that longer videos are more challenging and lead to lower performance. However, it remains unclear whether this truly amplifies positional bias or if the observed drop is simply due to increased video complexity.\n- The paper primarily identifies the problem of positional bias but does not propose or discuss concrete directions for mitigation. Including such discussion would make the contribution more complete and valuable for future work.\n- (Minor) Discussing inconsistent and biased video understanding found in prior works [1, 2] could also strengthen this analysis.\n\n**References** \n\n[1] A Closer Look at Temporal Sentence Grounding in Videos: Dataset and Metric, arxiv 2021\n\n[2] On the Consistency of Video Large Language Models in Temporal Comprehension, CVPR 2025"}, "questions": {"value": "See the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EHapERVH31", "forum": "0V0bQi24YC", "replyto": "0V0bQi24YC", "signatures": ["ICLR.cc/2026/Conference/Submission1486/Reviewer_ZbYf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1486/Reviewer_ZbYf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634282860, "cdate": 1761634282860, "tmdate": 1762915783001, "mdate": 1762915783001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Video-LevelGauge, a benchmark for evaluating contextual positional bias in LVLMs by inserting standardized probes into different positions of video contexts. The benchmark systematically assesses 27 LVLMs, revealing that commercial models exhibit less positional bias than open-source ones. The study also explores the effects of context type, length, and model size on positional bias, providing insights for future improvements."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel Contribution: Highlights contextual positional bias, an underexplored issue in LVLMs.\n- Well-Designed Benchmark: Includes standardized probes, flexible context configurations, and comprehensive metrics.\n- Comprehensive Evaluation: Thorough analysis of 27 LVLMs, revealing actionable insights on bias patterns."}, "weaknesses": {"value": "- No Mitigation Strategies: While the paper introduces contextual positional bias, it does not propose or evaluate methods to mitigate it.\n- Lack of Task-Specific Examples: The paper does not provide clear illustrations of how positional bias affects specific tasks, making the findings less interpretable.\n- Narrow Scope: Focuses solely on positional bias without addressing other LVLM limitations, such as hallucination or temporal reasoning errors."}, "questions": {"value": "Please see Weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gHBuTrxqd8", "forum": "0V0bQi24YC", "replyto": "0V0bQi24YC", "signatures": ["ICLR.cc/2026/Conference/Submission1486/Reviewer_3Lzq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1486/Reviewer_3Lzq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772680709, "cdate": 1761772680709, "tmdate": 1762915782634, "mdate": 1762915782634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}