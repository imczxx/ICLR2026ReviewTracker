{"id": "klJuJp6iqu", "number": 15235, "cdate": 1758249185606, "mdate": 1763001156157, "content": {"title": "FairOCL: Fair Gradient Aggregation for Online Continual Learning", "abstract": "Online continual learning (OCL) aims to enable neural networks to learn sequentially from streaming data while mitigating catastrophic forgetting, a key challenge in which learning new tasks interferes with the retention of previously acquired knowledge. Although most existing approaches rely on memory buffers to replay past samples, training jointly on mixed data from different tasks often leads to gradient conflicts, which undermine model performance. To address this, we propose FairOCL, a framework that draws inspiration from fair resource allocation in communication networks. FairOCL formulates gradient aggregation across tasks as a constrained utility maximization problem and enforces fairness in the optimization process, allowing principled control over task prioritization. Extensive experiments on several standard benchmarks show that FairOCL achieves consistent improvements over state-of-the-art methods. Our code will be released.", "tldr": "We propose FairOCL, a framework that draws inspiration from fair resource allocation in communication networks, allowing principled control over task prioritization.", "keywords": ["online continual learning", "fair resource allocation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1befc08f32935a8a64f4c88b6116f9e54f74b410.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper consider the online continual learning, where the data only appears once. The authors proposed the fairness-based gradient alignment approach to alleviate the catastrophic forgetting."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors conducted experiments on various dataset, including two challenging datasets, TinyImageNet and ImageNet-100."}, "weaknesses": {"value": "1. Although the proposed algorithm appears technically sound, the paper’s contribution seems incremental, as it primarily combines two existing methods (i.e., FairGrad and MKD) and closely resembles FairGrad in both structure and writing style. For example, Section 4.1 in FairOCL is almost identical to Section 4 in FairGrad, Section 3.2 in FairOCL mirrors Section 3.2 in FairGrad, and the method described in Section 4.2 of FairOCL is highly similar to that in Section 4.2 of FairGrad.\n2. The authors claim that prior works rely on computationally intensive hyper-gradient calculations for stability, whereas their proposed method avoids this issue. However, no concrete justification or empirical evidence supporting this claim is provided.\n3. The literature review omits several important works in continual learning, particularly those focused on gradient-based approaches, which limits the paper’s ability to position its contribution within the broader research context.\n4. The EMA/MKD method itself is a well-known and effective technique for continual learning. It would strengthen the paper if the authors discussed how FairOCL improves upon or differs from the conventional EMA-based approaches.\n5. Since the proposed method is based on gradient alignment, it would be valuable for the authors to provide empirical observations or visualizations illustrating how gradient alignment behaves in practical experiments.\n6. The results presented in Table 1 appear to be directly copied from Table 3 of [R1]\n    - [R1]: Rethinking momentum knowledge distillation in online continual learning  \n7. The reported results of average forgetting in Table 2 differ substantially from those presented in the original manuscripts of the compared methods. The authors should carefully discuss the reasons behind these discrepancies. For example, the results for POCL, DER++, ER, and GSA vary significantly from Table 3 in the referenced paper. Moreover, the results of POCL on ImageNet-100 are missing without explanation.\n8. The experimental comparisons seem unfair. Given that FairOCL combines FairGrad and MKD, it should be compared against similarly enhanced baselines. Many existing methods could also integrate MKD to improve performance, so limiting comparisons to naive baselines weakens the validity of the claimed advantages.\n9. Finally, as the paper emphasizes computational efficiency and gradient-based learning, it would be beneficial to include comparisons of both accuracy and computational cost with other gradient-based methods to substantiate the claims made in the introduction."}, "questions": {"value": "1. The plots in Figure 5 for $\\alpha = 0.0$ and $\\alpha = 1.5$ appear almost identical to Figures 1(a) and 1(b) in the FairGrad paper, despite the differing experimental setups (multi-task learning and continual learning). The authors should clarify this similarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XH9OSm2paw", "forum": "klJuJp6iqu", "replyto": "klJuJp6iqu", "signatures": ["ICLR.cc/2026/Conference/Submission15235/Reviewer_fPYP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15235/Reviewer_fPYP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760872683764, "cdate": 1760872683764, "tmdate": 1762925531976, "mdate": 1762925531976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "KxwFKYAjSZ", "forum": "klJuJp6iqu", "replyto": "klJuJp6iqu", "signatures": ["ICLR.cc/2026/Conference/Submission15235/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15235/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763001155128, "cdate": 1763001155128, "tmdate": 1763001155128, "mdate": 1763001155128, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a principled and tunable approach to fair continual learning by casting gradient aggregation as an α-fair utility maximization problem, drawing inspiration from network resource allocation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This framing provides a theoretical knob to balance plasticity and stability. \nIt also connects fairness directly to the optimization dynamics.\nThe inclusion of a teacher-student EMA distillation mechanism to mitigate feature drift is a reasonable practical addition to help stabilize representations without significantly increasing memory requirements."}, "weaknesses": {"value": "1. My main concerns is whether the particular way of formulating this as a utility maximization problem makes more sense than the classical ERM formulation. The Equation 7 seems ill posed. Specifically, G^\\top G \\omega=\\omega^{-1/\\alpha} is the stationarity condition of a non-convex problem.  Existence/uniqueness requires symmetric positive definite G^\\top G and strictly positive \\omega; in practice G^\\top G is often singular/ill-conditioned, and \\omega^{-1/\\alpha} is undefined at \\omega_i\\le 0. Even when relaxed to least-squares, it’s solving a noisy non-linear system that may have multiple or no solutions and is highly initialization-sensitive.\n\n2. It's not clear whether the new updates will not harm past tasks as the fairness weighting is a soft heuristic. This is unlike gradient projection or gradient alignment techniques that explicitly constrain the updates. \n\n3. Sensitivity to large gradient norms: The fairness mechanism seems to be dependent on gradient magnitudes. It's possible tasks with large gradient norms can dominate the ones with smaller norms. So is the fairness scale dependent?"}, "questions": {"value": "It's not clear whether the new updates will not harm past tasks as the fairness weighting is a soft heuristic. This is unlike gradient projection or gradient alignment techniques that explicitly constrain the updates. \n\nSensitivity to large gradient norms: The fairness mechanism seems to be dependent on gradient magnitudes. It's possible tasks with large gradient norms can dominate the ones with smaller norms. So is the fairness scale dependent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1j0RDIkFBf", "forum": "klJuJp6iqu", "replyto": "klJuJp6iqu", "signatures": ["ICLR.cc/2026/Conference/Submission15235/Reviewer_YgXt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15235/Reviewer_YgXt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761677447330, "cdate": 1761677447330, "tmdate": 1762925531526, "mdate": 1762925531526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce FairOCL, a new way to combine gradients more fairly. They borrow the idea of fairness from communication networks, where multiple users share limited resources. Here, each task is treated like a user, and the gradient direction is the shared resource. FairOCL finds a balance so that no old task dominates or gets ignored. They also add a knowledge distillation trick (using a “teacher” model) to reduce the bias toward recently seen data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. FairOCL is simple to implement, integrates smoothly with standard replay-based training, and introduces minimal computational overhead compared to existing methods.\n2. The paper provides intuitive connection between resource allocation and gradient aggregation in online continual learning. This analogy offers a principled way to think about balancing task influence during replay."}, "weaknesses": {"value": "1. Limited novelty – The idea of reweighting or balancing gradients has been explored before in continual learning. FairOCL mainly repackages this concept under a fairness interpretation.\n2. Outdated experimental setup – The evaluation relies mostly on small-scale benchmarks (CIFAR-10/100, TinyImageNet) and older baselines (ER, DER++, DVC). Many stronger recent OCL methods (e.g., DyTox, CODA-Prompt, Co2L, CLS-ER, or MEMO) are missing.\n3. Fixed parameter α – The fairness level is tuned manually per dataset; there’s no adaptive or principled selection.\n4. Limited realism of fairness evaluation - The paper could better explain how fairness in gradients translates into fairness in learned features. The proposed fairness concept is tested only under balanced class distributions, where every task contributes similarly sized datasets. It would be more meaningful to evaluate FairOCL under imbalanced or long-tailed continual learning settings, where resource sharing constraints are more realistic and the notion of fairness has stronger practical relevance.\n5. Missing Literature - Many prior gradient reweighing methods are not cited in this work eg: [1],[2]\n\ncitations:\n[1] Gradient Reweighting: Towards Imbalanced Class-Incremental Learning (Jiangpeng He, Fengqing Zhu)\n[2] Rethinking Gradient Projection Continual Learning: Stability/Plasticity Feature Space Decoupling (Zhen Zhao; Zhizhong Zhang; Xin Tan; Jun Liu; Yanyun Qu; Yuan Xie)"}, "questions": {"value": "1. Could you better differentiate FairOCL from prior gradient reweighting methods?\n2. How sensitive is performance to α, and could you provide guidance or heuristics for practitioners to choose it? Do different α values correspond to specific stability–plasticity trade-offs that could be described more clearly in the paper?\n3. Could you elaborate on how fairness at the gradient level might translate to fairness in learned features or task retention? Is there qualitative evidence (e.g., visualization, theoretical argument) showing that gradient balance leads to more uniform feature preservation?\n4. The fairness principle is evaluated on balanced datasets. How would FairOCL behave under imbalanced or long-tailed task distributions?\n5. What unique advantage does the fairness-based formulation offer beyond a change in interpretation. For instance, does it provide different gradient solutions or improved theoretical guarantees?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bmHi3XKhTy", "forum": "klJuJp6iqu", "replyto": "klJuJp6iqu", "signatures": ["ICLR.cc/2026/Conference/Submission15235/Reviewer_9iqb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15235/Reviewer_9iqb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929032830, "cdate": 1761929032830, "tmdate": 1762925530909, "mdate": 1762925530909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the issue of conflicting gradients in online continual learning (OCL). They propose FairOCL, a framework based on fair resource allocation in communication networks. FairOCL formulates gradient aggregation across tasks as a constrained utility maximization problem and enforces fairness in the optimization process, allowing principled control over task prioritization. Proposed technique is novel and the empirical evaluation is exhaustive."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The idea to frame gradient aggregation in online continual learning as an \\alpha‐fair utility maximisation problem inspired by communication networks is a novel and principled approach.\n2.  The authors recognise that fair aggregation alone is insufficient due to task recency bias. The inclusion of an EMA‐based teacher and a KL‐divergence loss provides a simple and effective method that improves retention.\n3. The authors evaluate FairOCL on four benchmarks with varying buffer sizes and compare against strong replay baselines and state‐of‐the‐art methods. Across all datasets and memory sizes, the proposed method achieves higher average accuracy and lower forgetting. The ablation study demonstrates that FairOCL is robust to the choice of α and distillation temperature.The experiments show the method's stability."}, "weaknesses": {"value": "1. The problem of fairness in continual learning is not well-motivated. How does the problem of gradient conflict translate to fairness? Arguments or specific use cases that demonstrate the presence of gradient conflict and resulting issue of fairness is not provided. \n2. This work seems very close in writing to the reference  Wu et al. (2024). Although authors point out the difference clearly, there could be more effort to dealign introduction and other sections.\n3. Important tradeoffs may not be analysed. For instance, are there situations where ensuring fairness lead to increase in forgetting or decrease in accuracy. Intuitively, it seems like ensuring fairness tends to have a long term gain and hence, for the first few tasks, I expect the proposed method not to perform well. However, as the task advances (long task sequences), the proposed method may do better and better. However, no such discussions are available and this hinders the intuitive understanding of the work.\n4. How does fairness affect stability Vs plasticity ? This needs more discussions."}, "questions": {"value": "1. The introduction states that joint optimisation on current and replayed data produces gradient conflicts, but the paper does not provide examples quantifying how often such conflicts occur, which tasks are adversely affected, or how uniform averaging produces imbalanced retention. Including case studies or synthetic examples illustrating this would strengthen the problem statement and motivate the fairness formulation.\n2. Equation (5) introduces a ball constraint parameterised by ε, but ε is neither fixed nor further discussed. It would improve readability to define all variables and symbols and to be consistent in subscripts and superscripts.\n3. The authors say that Eq. (7) is solved via a constrained nonlinear least‐squares formulation and that the overall cost remains comparable to other methods. A small table reporting the number of iterations and runtime per batch on each dataset would support the claim that the training budget is comparable.\n7. The paper argues that fairness in gradient aggregation yields balanced retention across past tasks, yet no explicit fairness metrics are reported. Plotting per‐task improvements using matrices such as Jain’s fairness index would show if the method is actually fair. Similarly, it would be useful to report task‐wise accuracy at the end of training to verify that earlier tasks do not deteriorate relative to later ones.\n8. alpha is a tunable parameter; however this being a hyperparameter becomes an issue. Shiuld this be learned from data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kVX992DGoC", "forum": "klJuJp6iqu", "replyto": "klJuJp6iqu", "signatures": ["ICLR.cc/2026/Conference/Submission15235/Reviewer_itRt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15235/Reviewer_itRt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762320016156, "cdate": 1762320016156, "tmdate": 1762925529514, "mdate": 1762925529514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}