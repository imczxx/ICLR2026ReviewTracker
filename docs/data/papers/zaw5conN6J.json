{"id": "zaw5conN6J", "number": 11662, "cdate": 1758202893555, "mdate": 1763067632887, "content": {"title": "SoundReactor: Frame-level Online Video-to-Audio Generation", "abstract": "Prevailing Video-to-Audio (V2A) generation models operate offline, assuming an entire video sequence or chunks of frames are available beforehand. This critically limits their use in interactive applications such as live content creation and emerging generative world models. To address this gap, we introduce the novel task of frame-level online V2A generation, where a model autoregressively generates audio from video without access to future video frames. Furthermore, we propose SoundReactor, which, to the best of our knowledge, is the first simple yet effective framework explicitly tailored for this task. Our design enforces end-to-end causality and targets low per-frame latency with audio-visual synchronization. Our model's backbone is a decoder-only causal transformer over continuous audio latents. For vision conditioning, it leverages grid (patch) features extracted from the smallest variant of the DINOv2 vision encoder, which are aggregated into a single token per frame to maintain end-to-end causality and efficiency. The model is trained through a diffusion pre-training followed by consistency fine-tuning to accelerate the diffusion head decoding. On a benchmark of diverse gameplay videos from AAA titles, our model successfully generates semantically and temporally aligned, high-quality full-band stereo audio, validated by both objective and human evaluations, at low per frame token-level latency ($26.6$ms for the head NFE=1, $30.3$ms for NFE=4 with $30$FPS, $480$p videos using a single H100.).\nDemo samples are available at https://anonymous-sr-submission.github.io/.", "tldr": "We introduce the novel task of frame-level online video-to-audio generation and propose SoundReactor, which, to the best of our knowledge, is the first simple yet effective framework explicitly tailored for this task.", "keywords": ["Frame-level online video-to-audio generation", "video-to-audio generation", "autoregressive models", "diffusion models", "multimodal generative modeling for sound and audio"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/03d86941d892db74699a2cd6bd860375fe8c4fa2.pdf", "supplementary_material": "/attachment/63ab449fc045796b70310206eea100c292b7dde2.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes SoundReactor, a novel framework for frame-level online video-to-audio (V2A) generation. To enforce causality, the model employs a causal decoder-only transformer with a diffusion head on top. To improve generation speed, the authors apply Easy Consistency Tuning (ECT). For conditioning, visual features are extracted from DINOv2 grid representations, while audio tokens are obtained from a VAE trained from scratch. The model demonstrates strong performance and low latency compared to the baseline V-AURA on gameplay videos."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis is the first work to explicitly address a novel online, frame-level V2A generation, and the design choices are technically sound.\n2.\tThe model achieves both high efficiency and strong performance in OGameData.\n3.\tThe experiments are extensive, with comprehensive ablations supporting the design choice."}, "weaknesses": {"value": "1.\tReal-time generation is most impactful in long or interactive settings. However, the experiments are limited to 16-second clips, which may not convincingly demonstrate online efficiency, since prior works (e.g., SpecVQGAN) already handle sequences of ~10 s or more.\n2.\tIt would be helpful to include an ablation using a non-causal or offline transformer to quantify how much the causal constraint actually contributes whether positively or negatively to the overall performance.\n3.\tGiven the frame-level generation setup, does the model also produce a long silence when no visual events occur? This aspect is not clearly discussed."}, "questions": {"value": "1.\tCould this framework be extended to multi-condition generation (e.g., adding text prompts to control style or sound context)?\n2. If a short irrelevant segment (e.g., a 1-second advertisement) appears before the actual video (e.g., 8 seconds of a bird singing), will the subsequent audio generation be influenced by the earlier context? How quickly can the model adapt to such context shifts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "waR0lMN7zH", "forum": "zaw5conN6J", "replyto": "zaw5conN6J", "signatures": ["ICLR.cc/2026/Conference/Submission11662/Reviewer_ygRm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11662/Reviewer_ygRm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867465937, "cdate": 1761867465937, "tmdate": 1762922721161, "mdate": 1762922721161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "NSI1dbHeJu", "forum": "zaw5conN6J", "replyto": "zaw5conN6J", "signatures": ["ICLR.cc/2026/Conference/Submission11662/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11662/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763067631697, "cdate": 1763067631697, "tmdate": 1763067631697, "mdate": 1763067631697, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper, SoundReactor, introduces the first frame-level online Video-to-Audio (V2A) generation model that can generate synchronized audio in real time from video frames. Unlike prior offline V2A models, which require access to the entire video frames before producing audio, SoundReactor operates causally, generating audio sequentially, one frame at a time, without future frames. The framework consists of three core components: (1) Video token modeling; (2) Audio token modeling; (3) Multimodal transformer with diffusion head Training is performed in two stages: first, diffusion pretraining under the EDM2 framework to model robust denoising and temporal coherence, and second, consistency fine-tuning (ECT) to accelerate inference. Evaluated on the large-scale OGameData250K dataset, SoundReactor significantly outperforms the previous works in both audio quality and audio-visual alignment. Remarkably, it achieves real-time operation (≤ 32 ms per frame at 30 FPS), while maintaining comparable perceptual quality. Overall, SoundReactor establishes a new paradigm for causal, real-time V2A generation, providing a foundation for interactive, multimodal, and live content generation in the future."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well written and supported by extensive and systematic experiments.\n\n2. This paper introduces a new frame-level online V2A generation paradigm. This enables real-time audio generation synchronized with streaming video frames, which is crucial for interactive applications such as gaming, virtual environments, and world models in robotics.\n\n3. This paper achieves end-to-end causality from vision to audio, enabling frame-by-frame generation with minimal latency."}, "weaknesses": {"value": "1. While the authors claim frame-level causality, the sampling can involve autoregressive decoding with a diffusion process, which may introduce temporal lookahead latency.\n\n2.  Although “dinov2-vits14-reg” is used for efficiency, there’s no comparison to other vision encoders, except DINOv2's series. It’s unclear how much grid (patch) features extracted by DINOv2[1] actually contribute to performance.\n\n\n\n[1] Oquab, Maxime, et al. “DINOv2: Learning robust visual features with-out supervision.” Transactions on Machine Learning Research, 2024\n[2] Evans, Zach, et al. \"Fast timing-conditioned latent audio diffusion.\" Forty-first International Conference on Machine Learning. 2024.\n[3] Evans, Zach, et al. \"Stable audio open.\" ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025."}, "questions": {"value": "1. During video token modeling, each frame’s features are concatenated with their temporal difference from the previous frame $(\\hat{V}i - \\hat{V}{i-1})$ to inject temporal cues. Could this differencing introduce a risk of future leakage? What would happen if you simply concatenated the previous frame features or injected sequence information using positional encoding instead?\n\n2. Does the proposed framework ensure scalability for other-resolution videos and high-frequency audio (e.g., 96 kHz)? \n\n3.How would the inference speed of the offline model change if it were limited to only the current and past frames instead of the full video sequence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JJ6Ch1FAgq", "forum": "zaw5conN6J", "replyto": "zaw5conN6J", "signatures": ["ICLR.cc/2026/Conference/Submission11662/Reviewer_29S2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11662/Reviewer_29S2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945934051, "cdate": 1761945934051, "tmdate": 1762922720690, "mdate": 1762922720690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel task—frame-level online video-to-audio (V2A) generation—where the model generates audio autoregressively from video frames without access to future frames, a setting critical for real-time applications like live content creation and interactive world models. To address this, the authors propose SoundReactor, the first framework explicitly designed for this task, featuring:\n* A causal, decoder-only transformer operating on continuous audio latents.\n* A novel visual conditioning scheme using DINOv2 grid features aggregated per frame with temporal differencing to ensure end-to-end causality and synchronization.\n* A two-stage training pipeline: diffusion pre-training followed by consistency fine-tuning (via ECT) to accelerate inference.\nEvaluated on the OGameData dataset, SoundReactor generates high-quality, full-band stereo audio with strong semantic and temporal alignment, achieving low per-frame token-level latency (26.6–30.3 ms on H100). Both objective metrics and human evaluations confirm its superiority over offline AR baselines like V-AURA, which violate causality. The model also demonstrates zero-shot generalization to longer sequences via NTK-aware RoPE extension. Ablation studies validate key design choices, including the diffusion head size, CFG strategy, and vision conditioning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Well-Designed Causal Framework: The proposed SoundReactor is the first framework explicitly designed for this challenging online setting, enforcing strict end-to-end causality—including causal vision encoding—which is essential for real-time operation without access to future frames.\n* Effective and Efficient Architecture: The model leverages a lightweight DINOv2 encoder for efficient visual feature extraction, continuous audio latents via VAE for high-quality reconstruction, and a decoder-only transformer with a diffusion head, achieving a strong balance between audio quality and low per-frame latency.\n* Strong Empirical Performance: Comprehensive experiments on a diverse gameplay dataset demonstrate that SoundReactor generates high-quality, full-band stereo audio with strong semantic and temporal alignment, outperforming strong offline AR baselines like V-AURA under the causal constraint.\n* Practical Inference Acceleration: The integration of Easy Consistency Tuning (ECT) significantly reduces the number of function evaluations (NFE) in the diffusion head (down to 1–4 steps) while maintaining audio quality, making the model highly suitable for real-time applications."}, "weaknesses": {"value": "* Limited Evaluation on Long-Form Sequence Generation: The method demonstrates frame-level generation and alignment capabilities, which should theoretically offer advantages over many existing diffusion-based models for long-sequence generation, as those often rely on concatenation of shorter segments. However, the current evaluation only extends to 16-second sequences, which is not particularly long. It would be valuable to test and compare the performance on significantly longer audio sequences to better validate the scalability and stability of the proposed online generation approach.\n* Limited Generalizability to Real-World Datasets: While the model performs well on the curated OGameData dataset (comprising gameplay videos), its performance on the more diverse and uncurated VGGSound dataset is noticeably weaker, particularly in terms of semantic alignment (IB-Score) and audio quality (KL_PaSST). This suggests the model may not generalize as effectively to real-world, in-the-wild video content.\n* Incomplete Inference Latency Comparison: To the best of our knowledge, other models also have implementations that focus on inference acceleration. It would be beneficial to supplement the experiments with comparative inference speed analyses against these accelerated baselines, providing a more comprehensive understanding of the practical efficiency advantages of the proposed method."}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HENZbl9OIp", "forum": "zaw5conN6J", "replyto": "zaw5conN6J", "signatures": ["ICLR.cc/2026/Conference/Submission11662/Reviewer_184U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11662/Reviewer_184U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996822095, "cdate": 1761996822095, "tmdate": 1762922719190, "mdate": 1762922719190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}