{"id": "YiV2rJOIUJ", "number": 3342, "cdate": 1757405217785, "mdate": 1759898094858, "content": {"title": "Fast to Train, Fast to Sample: Stable Velocity for Flow Matching", "abstract": "We revisit flow matching from a variance-centric perspective. \nAlthough conditional flow matching (CFM) is theoretically elegant, its use of single-sample conditional velocities introduces high variance, which can destabilize optimization and slow convergence. \nWe demonstrate that this behavior induces two distinct regimes: a high-variance regime that hinders training and a low-variance regime where conditional and true velocities are nearly identical, thereby enabling analytical sampling shortcuts.\nMotivated by these insights, we introduce the \\textbf{Stable Velocity} framework to improve both the training and sampling processes of flow matching. \nFor training, we propose \\textit{Stable Velocity Matching (StableVM)}, a variance-reduced objective that preserves CFM's global optima while significantly improving stability and convergence in the high-variance regime. \nFor sampling, we introduce \\textit{Stable Velocity Sampling (StableVS)}, a ``free lunch'' acceleration method that leverages the low-variance regime to achieve faster generation without requiring finetuning.\nExperiments on SiT-XL trained on ImageNet, as well as on several large pretrained models (SD3, SD3.5, Flux, and Wan2.2), show consistent improvements in training convergence and more than $2\\times$ faster sampling while maintaining high fidelity.", "tldr": "", "keywords": ["Flow Matching", "Stochastic Interpolants", "Variance Reduction", "Efficient Sampling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03d8f46047986580afaef7b5ab7c901db5a3218f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to improve both the training stability and sampling efficiency of flow matching models. The authors propose decomposing the training process into two distinct regimes: a high-variance regime (corresponding to large injected noise) and a low-variance regime (corresponding to small injected noise).\n\nTo stabilize training in the high-variance regime, the paper introduces Stable Velocity Matching, which approximates the true velocity by sampling multiple data points. To accelerate inference in the low-variance regime, it proposes Stable Velocity Sampling, which treats the generated data as the ground truth, effectively linearizing the trajectory into a straight line for faster sampling. The paper provides experimental results for both unconditional and text-to-image generation to validate the proposed methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clarity of Exposition: The paper is well-written and logically structured. Figure 2, in particular, is highly effective. It intuitively illustrates the core differences between high-variance regime and low-variance regime, allowing readers to quickly grasp the paper's key insights.\n\n- Experimental Validation: The paper's key insight—the two-regime decomposition—is clearly supported by Figure 1. The experimental results also demonstrate that the proposed method improves generation quality."}, "weaknesses": {"value": "- Limited Novelty: The paper's contributions appear derivative. Stable Velocity Matching is analogous to using a stable target [1] , and Stable Velocity Sampling strongly resembles DDIM or other straight-line samplers. Since diffusion models are a special case of flow matching, the method seems to be a straightforward adaptation and combination of established diffusion techniques, lacking fundamental novelty.\n\n- Lack of comparison with baselines. The paper lacks a comparison against current state-of-the-art (SOTA) method for efficient and high quality generation. In fact, the proposed method is only compared with variances of itself. To properly contextualize its contributions, the method must be benchmarked against SOTA approaches in terms of both generation quality and efficiency.\n\nReference: [1] Yilun Xu, Shangyuan Tong, and Tommi Jaakkola. Stable target field for reduced variance score estimation in diffusion models"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PHKOkC3OEr", "forum": "YiV2rJOIUJ", "replyto": "YiV2rJOIUJ", "signatures": ["ICLR.cc/2026/Conference/Submission3342/Reviewer_HXfF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3342/Reviewer_HXfF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761494721135, "cdate": 1761494721135, "tmdate": 1762916680745, "mdate": 1762916680745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits flow matching from a variance-centric perspective. Motivated from the large variance issue of vanilla conditional flow matching, the authors propose Stable Velocity Matching, a variance-reduced objective that preserves CFM’s global optima while provably improving stability. Extensive experiments are conducted to verify the efficiency of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well organized, starting from observation of large variance in vanilla CFM and then introducing variance reduction techniques. Theotical analysis is also provided to show the benefits of proposed method. The authors conducted extensive experiments to demonstrate the acceleration over vanilla FM."}, "weaknesses": {"value": "1. The motivation in Sec 2 is not very clear to me. I understand that the conditional velocity may have large variance but it doesn't necessarily undermine training stability because the Monte Carlo estimator is to estimate the gradient instead of loss itself. Therefore I suggest the authors compute the variance in gradient using conditional velocity.\n\n2. I don't understand the difference between the proposed method and STF as mentioned in Appendix B. For STF, it first samples a batch $(x_0^i)\\_{1 \\leq i \\leq n}$ from data distribution, and then samples $x_t$ by applying the transition kernel to the \"first\" training data $x_0^1$. For the proposed method, it first samples $(x_0^i)\\_{1\\leq i\\leq n}$, and theh samples $x_t$ from the mixture model, which is equivalent to choosing a random index $i$ and applying transition kernel to $x_0^i$. Since the order of $(x_0^i)$ doesn't matter, I don't see the differene between the two methods. Could the authors explain more about this?\n\n3. For experiments, the authors only compared the proposed method with vanilla FM and didn't mention even most related methods like STF. If the authors want to claim the advantages or differences of the proposed method over STF, I think a numerical comparison is necessary."}, "questions": {"value": "Please see weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v3TaUlbLlP", "forum": "YiV2rJOIUJ", "replyto": "YiV2rJOIUJ", "signatures": ["ICLR.cc/2026/Conference/Submission3342/Reviewer_kXZ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3342/Reviewer_kXZ3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761715376425, "cdate": 1761715376425, "tmdate": 1762916680444, "mdate": 1762916680444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes variance in Conditional Flow Matching, identifying high-variance and low-variance regimes. It proposes Stable Velocity Matching, an unbiased variance-reduced training objective using multiple reference samples, and Stable Velocity Sampling, a training-free acceleration method exploiting analytical PF-ODE solutions in the low-variance regime. Experiments demonstrate sampling speedup on large pretrained models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The observation regarding variance regimes is well-motivated. The validation experiment in Figure 1 effectively demonstrates that the split point ξ shifts toward 1 as dimensionality increases, supporting the analysis.\n2. The experimental validation is comprehensive and involves substantial computational resources."}, "weaknesses": {"value": "1. The proposed accelerated sampling method (StableVS) is only compared to a \"default sampler\". What is the \"default sampler\" mentioned in Figure 4? It lacks comparison with other advanced samplers such as DPM-Solver++, etc.\n2. Based on Figure 3, the convergence speedup from StableVM appears marginal.\n3. The StableVS sampler's performance depends critically on the split point $\\xi$. The paper lacks a principled method or heuristic for choosing this value, which appears to be a \"magic number\" hyperparameter.\n4. Does the two-regime variance structure hold true in latent space? It lacks a similar experiment as Figure 1. For latent spaces generated by different VAEs, how would one determine the split point $\\xi$?"}, "questions": {"value": "1. What is the computational cost that Stable Velocity Matching adds compared to the single-sample Monte Carlo estimate? How does this cost scale as the reference batch size $n$ increases?\n2. How was the split point $\\xi$ chosen?\n3. How will the proposed memory bank solve the sparsity problem for text-conditional generation?\n4. What are the generation results (FID, IS) for your ImageNet-trained SiT-XL model when using a low NFEs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Un025dQR1y", "forum": "YiV2rJOIUJ", "replyto": "YiV2rJOIUJ", "signatures": ["ICLR.cc/2026/Conference/Submission3342/Reviewer_7N2v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3342/Reviewer_7N2v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903872820, "cdate": 1761903872820, "tmdate": 1762916680050, "mdate": 1762916680050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Stable Velocity, a framework to improve both training stability and sampling efficiency in flow-matching and stochastic-interpolant generative models. The key insight is that Conditional Flow Matching (CFM) suffers from high variance in its conditional velocity estimates, leading to slow convergence and unstable training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper clearly articulates how CFM variance affects optimization and links it to training and inference regimes."}, "weaknesses": {"value": "1. There have been several recent works that significantly enhance training and inference efficiency at scale, such as MeanFlow [2] and REPA (Representation Alignment for Generation) [1]. Compared to these approaches—which demonstrate substantial speedups and scalability gains—the improvements reported in this paper appear relatively minor. Consequently, the overall contribution of this work appears incremental relative to existing approaches. \n\n[1] REPRESENTATION ALIGNMENT FOR GENERATION: TRAINING DIFFUSION TRANSFORMERS IS EASIER THAN YOU THINK\n[2] Mean Flows for One-step Generative Modeling\n\n2. Multiple papers have proposed importance sampling for times for diffusion models for reducing the variance of the diffusion ELBO, empirically showing improved results:\nHuang, Chin-Wei, Jae Hyun Lim, and Aaron C. Courville. \"A variational perspective on diffusion-based generative models and score matching.\" Advances in Neural Information Processing Systems 34 (2021): 22863-22876.\nSong, Yang, et al. \"Maximum likelihood training of score-based diffusion models.\" Advances in neural information processing systems 34 (2021): 1415-1428.\nVariance reduction techniques that make use of control variates rather than importance sampling are not discussed either\nJeha, Paul, et al. \"Variance reduction of diffusion model's gradients with Taylor approximation-based control variate.\" arXiv preprint arXiv:2408.12270 (2024).\nA comparison to some of these methods would make the paper stronger.\n\n3. The best FID value for SiT-XL/2 reported in prior work is 2.06, yet Table 1 in this paper only presents FID values at selected training iterations without showing the final or best-achieved score. To fairly assess the improvement brought by the proposed method, the authors should include the best FID attained during training and compare it directly with the SiT-XL/2 benchmark result."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PNxZ3y2bys", "forum": "YiV2rJOIUJ", "replyto": "YiV2rJOIUJ", "signatures": ["ICLR.cc/2026/Conference/Submission3342/Reviewer_kKja"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3342/Reviewer_kKja"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979208626, "cdate": 1761979208626, "tmdate": 1762916679823, "mdate": 1762916679823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}