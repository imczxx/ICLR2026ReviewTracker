{"id": "0psy3EVkT7", "number": 2225, "cdate": 1757034779635, "mdate": 1759898161772, "content": {"title": "GeoMoLa: Geometry-Aware Motion Latents for Learning Robust Manipulation Policies", "abstract": "Learning latent actions for robotic manipulation heavily relies on extracting motion patterns from visual sequences, yet effective action abstractions require understanding three-dimensional geometric transformations. Here, we introduce GeoMoLa (Geometry-Aware Motion Latents), which learns discrete latent action codes by predicting how point clouds evolve during manipulation rather than reconstructing visual observations. This four-dimensional objective -- spatial geometry changing through time -- forces latent representations to encode actual physical motion rather than appearance patterns. GeoMoLa achieves state-of-the-art performance using only single-view RGB-D input, while existing methods require multi-view reconstruction, succeeding across diverse manipulation benchmarks. Our ablations reveal that geometric prediction is the key to driving performance, quantitatively validating that manipulation depends on spatial understanding. \nFurthermore, the learned codes exhibit effective motion abstraction: applying them to novel scenes produces physically consistent transformations regardless of visual context. Our real-world experiments also confirm this robustness capability, achieving robust manipulation with minimal demonstrations in cluttered environments where geometric reasoning determines success. Thus, we demonstrate that effective latent actions for robot control can better emerge from understanding motion through its three-dimensional effects rather than pixel-level patterns.", "tldr": "", "keywords": ["robotics", "3D", "semantics learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f807131418f88917482f8626d2db18c8dce626b3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a self-supervised way of learning discrete action codes through predicting how point clouds evolve during manipulation rather than reconstructing visual observations. The paper argues that the spatial geometry changing through time is important and useful to spatial understanding. Experiments on different benchmarks show superior performances. The paper claims they are the first framework that models robot manipulation as continuous four-dimensional process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed method only uses single-view RGB-D inputs, while still achieving competitive performance. This is important in real-world deployment of robots.\n* The intuition behind the proposed method makes sense. The learned results show good interpretability also hints the effectiveness of the method."}, "weaknesses": {"value": "* The methodology section is quite hard to follow. There is few logical connection between each subsection. The writing mostly consists of plain descriptions of models, without many explanation, which makes the framework hard to understand.\n* The logical connection of Figure 2 is unclear. What is the relation between (a) and (b)? Do they share any module?\n* It is hard to see how the four-dimensional geometry changing is imposed in the training objective. It looks like from line 223 the constraint is imposed through predicting future latent point map features. And the latent features are obtained through a finetuned diffusion model described in Appendix D. The point map VAE is initialized by an RGB VAE, which does not make too much sense to me since 3d coordinates are different modalities than RGB. And Appendix D has some undefined variables such as z^t.\n\nOverall the writing of the paper seems a big issue for me. It is quite confusing so that I find it hard to evaluate the correctness of the architecture. Although the paper has good intuition and seemingly good results, I would rate a borderline rejection."}, "questions": {"value": "Which specific loss function imposes the constraint that \"encode actual physical motion rather than appearance patterns\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sJFPXQgE9W", "forum": "0psy3EVkT7", "replyto": "0psy3EVkT7", "signatures": ["ICLR.cc/2026/Conference/Submission2225/Reviewer_voeQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2225/Reviewer_voeQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524777344, "cdate": 1761524777344, "tmdate": 1762916151223, "mdate": 1762916151223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GeoMoLa. It learns discrete motion codes by predicting future 3D geometry (pointmaps) and RGB from current RGB‑D observations and language. \n\nA VQ‑VAE discretizes vision‑language features into codes; a pointmap/RGB latent diffusion model is trained to forecast future observations conditioned on those codes; and a 3D denoising transformer uses the codes to generate 6‑DoF action chunks. \n\nExperiments on RLBench, CALVIN, and six real‑robot tasks show consistent gains over 2D/3D baselines, with ablations indicating geometry prediction is the main driver of performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Method is well motivated: VQ‑VAE for discrete codes; pointmap diffusion for future geometry; 3D‑aware transformer with relative 3D attention and cross‑attention to latents for action generation.\n\n2. Good experimentation: solid benchmarks and real‑world evaluation with low demo counts, plus clean ablation identifying geometry prediction as the key contributor."}, "weaknesses": {"value": "1. The main contribution to me is tying discrete latents to future 3D geometry prediction and then using them to condition 3D‑aware action diffusion. Might better tone down “first 4D” phrasing and draw distinction vs dynamic Gaussian / NeRF‑style approaches.\n\n2. Baselines are not strong enough. For example, in RLBench experiments, RVT2 is not included. It gets 100% on close jar and 80% on stack block.\n\n2. Besides, the presentation is not very clear. Latent motion / latent action is not consistent and thus confusing to readers. For example,  fig2 has (a) Geometry-Aware Latent Action Learning, and (b) Latent-Conditioned Action Generation. If I get it right, the latent action is to motion latents in the title. But it could mean latent embedding learned for robot action space. Thereby it is unclear to the latent action learning actually until reading much more in depth."}, "questions": {"value": "What is the motivation of deriving latent action from rib and language using minigpt? \nWould it be more natural to have depth / point map as input to latent action as well? considering they are assumed available in both training and inference"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9Lku9yGL4w", "forum": "0psy3EVkT7", "replyto": "0psy3EVkT7", "signatures": ["ICLR.cc/2026/Conference/Submission2225/Reviewer_ViXr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2225/Reviewer_ViXr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045531657, "cdate": 1762045531657, "tmdate": 1762916150958, "mdate": 1762916150958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The presented work introduces a novel approach to learn latent actions. Rather than learning the latent actions to predict future images, the authors propose learning the latent action to predict both future RGB and pointclouds. Thanks to the pointcloud prediction, the authors claim that the latent actions better capture the 3D geometric of the task. \n\nThanks to a better geometric component in the learned latent actions; when exploited for policy learning, the learned policies lead to better policies (higher task success rates).\n\nThe performance of the model was evaluated both in simulation (CALVIN, RLBench) and real robot. The authors also present ablation studies on the impact of learning the latent actions with and without pointcloud prediction and with and without RGB prediction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**\nThe presented work is original in learning latent action representations with the additional 3D geometric embeddings. While previous works [1] propose learning the latent action embeddings with only future RGB prediction, the presented work proposes learning the embedding with both RGB and pointcloud. \nThe authors argue that the pointcloud might led to better capturing the geometry, a reasoning that is sound.\n\n**Quality**\nThe work makes a good job in evaluating the performance of the model under multiple evaluations and present a useful ablation to visualize the real impact of adding 3D geometric prediction in the latent action pretraining.\n\n**Clarity**\nThe work is easy to read and to follow.\n\n[1] Ye, S., Jang, J., Jeon, B., Joo, S., Yang, J., Peng, B., ... & Seo, M. (2024). Latent action pretraining from videos. arXiv preprint arXiv:2410.11758."}, "weaknesses": {"value": "- Weak improvements due to the 3D geometry. While the authors show in Tab 3., a performance improvement thanks to the 3D geometry, the improvements are not large (2% increase in RL Bench and max 5% in CALVIN). Also the variations on the performance increase among tasks, makes it wonder when does the 3D geometry helps and when does not. Authors could consider exploring some simple “demo tasks”, one where 3D geometry does not help and one where 3D geometry is essential and find out if the latent embeddings with the 3D geometry is useful.\n- Another interesting analysis could be done in comparing the performance enhacement individually in tasks that require 3D translations informations and tasks that require rotation information. Is the pointcloud-based latent action embeddings equally useful for both?\n- Figure 1 is not very informative. Being the first figure of the paper, authors could try to improve the first figure to give a better grasp of the main idea. While it is able to give the general idea of “3D better”, it lacks details and it is too general to be valuable. Authors could consider including information regarding the latent action embedding and how is different from previous latent action embeddings.\n- 3D diffuser actor reported performance is lower than in their paper. While the original paper claim an average success rate of 81.3%, in your work the performance is 77%. Is there a reason for this mismatch?"}, "questions": {"value": "- 3D diffuser actor reported performance is lower than in their paper. While the original paper claim an average success rate of 81.3%, in your work the performance is 77%. Is there a reason for this mismatch?\n\n- What could be the reason of observing not very large performance enhancement when training the latent actions with pointcloud prediction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gqlV7UYDB1", "forum": "0psy3EVkT7", "replyto": "0psy3EVkT7", "signatures": ["ICLR.cc/2026/Conference/Submission2225/Reviewer_ixNB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2225/Reviewer_ixNB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762741306543, "cdate": 1762741306543, "tmdate": 1762916150523, "mdate": 1762916150523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}