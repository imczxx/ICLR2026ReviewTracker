{"id": "cVX3VqO8BO", "number": 4717, "cdate": 1757752486589, "mdate": 1759898018156, "content": {"title": "UniHM: Unified Dexterous Hand Manipulation with Vision Language Model", "abstract": "Planning physically feasible dexterous hand manipulation is a central challenge in robotic manipulation and Embodied AI. Prior work typically relies on object-centric cues or precise hand-object interaction sequences, foregoing the rich, compositional guidance of open-vocabulary instruction. We introduce UniHM, the first framework for unified dexterous hand manipulation guided by free-form language commands. \nWe propose a Unified Hand-Dexterous Tokenizer that maps heterogeneous dexterous-hand morphologies into a single shared codebook, improving cross-dexterous hand generalization and scalability to new morphologies. Our vision language action model is trained solely on human-object interaction data, eliminating the need for massive real-world teleoperation datasets, and demonstrates strong generalizability in producing human-like manipulation sequences from open-ended language instructions. To ensure physical realism, we introduce a physics-guided dynamic refinement module that performs segment-wise joint optimization under generative and temporal priors, yielding smooth and physically feasible manipulation sequences. Across multiple datasets and real-world evaluations, UniHM attains state-of-the-art results on both seen and unseen objects and trajectories, demonstrating strong generalization and high physical feasibility.", "tldr": "we introduce UniHM, the first framework for synthesizing unified dexterous hand manipulation sequences guided by free-form language commands.", "keywords": ["Hand Manipulation Synthesis;Multimodal Large Language Model;"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b828d4fcf4164f7c71551239330ccaf630b3c903.pdf", "supplementary_material": "/attachment/f96d33ad9a495ab70a22f8758499a5b2cf80a5be.zip"}, "replies": [{"content": {"summary": {"value": "This paper explores the use of multimodal large models for cross-embodiment dexterous manipulation. The main contributions are as follows:\n1. Constructed a dexterous manipulation dataset encompassing multiple heterogeneous dexterous hands, based on human manipulation datasets.\n2. Proposed a language-guided, multimodal large model-based framework for cross-embodiment dexterous manipulation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe proposed dataset features highly aligned grasp poses with semantic annotations for cross-embodiment manipulation, which is quite a contribution to the community.\n2.\tProposes and validates a viable methodology for applying Multimodal Large Language Models (MLLMs) to the challenging problem of high-DoF dexterous manipulation generation.\n3.\tAchieving promising results in real-world deployment on high-DoF dexterous embodiments using only human demonstration videos, providing a positive signal for addressing the issue of expensive data collection."}, "weaknesses": {"value": "1.\tThe selected tasks are relatively simple. On one hand, many demonstrated actions (e.g., opening a drawer) can be performed by simpler, lower-cost two-fingered grippers, failing to highlight the advantage of dexterous hands. On the other hand, the tasks designed for dexterous hands (e.g., picking and placing objects) involve overly simplistic semantics, lacking part-level manipulation or interactions with clear subsequent intent (e.g., \"handing the scissor handles to the user\" or \"grasping the teapot lid\", which are feasible in OakInk). We expect to see complex manipulation sequences that are hard to do for non-dexterous hands and feature richer semantic hierarchies.\n2.\tThe evaluations in simulation are primarily analytical metrics. I believe success rates for the manipulations are essential."}, "questions": {"value": "1.\tIn real-world deployment in open-world scenarios, high-fidelity 3D assets of the objects being manipulated are unavailable. Could this affect the performance of the physics-guided dynamic refinement?\n2.\tThe experiments just include comparisons with several human motion generation models. What about the performance of language-guided Hand-Object Interaction models [1] or language-guided grasp models [2, 3] (which can extend to the setting of this paper) on the current dataset.\n\n[1] Cha, Junuk, et al. \"Text2hoi: Text-guided 3d motion generation for hand-object interaction.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n[2] Wei, Yi-Lin, et al. \"Grasp as you say: Language-guided dexterous grasp generation.\" Advances in Neural Information Processing Systems 37 (2024): 46881-46907.\n\n[3] Zhong Y, Huang X, Li R, et al. Dexgraspvla: A vision-language-action framework towards general dexterous grasping[J]. arXiv preprint arXiv:2502.20900, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fCXAkz8aWQ", "forum": "cVX3VqO8BO", "replyto": "cVX3VqO8BO", "signatures": ["ICLR.cc/2026/Conference/Submission4717/Reviewer_uKvV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4717/Reviewer_uKvV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879189886, "cdate": 1761879189886, "tmdate": 1762917531743, "mdate": 1762917531743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified pipeline for language-conditioned, sequential dexterous-hand manipulation. The method comprises: (i) a Unified Dexterous-Hand Tokenizer (VQ-VAE) that maps heterogeneous hand morphologies into a shared discrete codebook and decodes back to hand-specific joint trajectories; (ii) a VLM-based generator that, given RGB-D input, an instruction, object point clouds, and tokenized history, autoregressively produces manipulation tokens; and (iii) a physics-guided dynamic refinement that enforces contact and temporal smoothness while adhering to the generator’s intent, yielding physically feasible, executable trajectories. Experiments on DexYCB and OakInk show consistent improvements over strong motion-generation baselines (TM2T, MDM, FlowMDM, MotionGPT3) and higher real-robot success rates; ablations demonstrate each module’s contribution."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a strong framework that incorporates a cross-dexterous-hand representation, language-conditioned sequence generation, and a physics-guided dynamic trajectory refinement module.\n\nThe authors perform extensive experiments across multiple datasets, covering both seen and unseen settings, with comprehensive evaluations and ablations."}, "weaknesses": {"value": "Tokenizer evaluation limited to a single hand. Although a unified dexterous-hand tokenizer is proposed, both the HOI and real-world experiments appear to use only one hand type. A broader evaluation across multiple robot hands would more convincingly validate the tokenizer’s generality and cross-hand transfer.\n\nUnderspecified sequence-generation metrics. The paper introduces a manipulation sequence generator, but the evaluation protocol for sequences is insufficiently detailed. Please clearly define the quantitative metrics and how they are computed.\n\nMissing text–motion alignment evaluation. Given the language-conditioned setup, include explicit Text–Motion Alignment assessments is necessary. Please provide both quantitative measures and qualitative analyses (e.g., human judgments of instruction adherence).\n\nMinor issues.\n\nLine 075: “Language -guided” → “Language-guided”.\n\nLine 115: missing space before “introduces”.\n\nLine 377: only five metrics are listed"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mB3pctwoGs", "forum": "cVX3VqO8BO", "replyto": "cVX3VqO8BO", "signatures": ["ICLR.cc/2026/Conference/Submission4717/Reviewer_ZSGv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4717/Reviewer_ZSGv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920226909, "cdate": 1761920226909, "tmdate": 1762917531386, "mdate": 1762917531386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors propose a novel framework for manipulation that generalizes across different dexterous-hand morphologies. Furthermore, the proposed method can generalize over unseen objects. The method consists of three main stages: first, the motion is tokenized in a morphology-independent way. Second, a token sequence is generated based on combined text, perception, and token history. Finally, the motion is generated by using physics-aware decoding. The authors report results that show that their method is superior to baselines on existing datasets, as well as in real-world evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is considering a relevant problem of generalization to different hand morphologies. The authors proposed a comprehensive framework that shows good results across multiple metrics and in real-world robot setups. They successfully manage to combine and benefit from existing models, such as PointSam and CLIPort, and integrate them into their framework. Overall, the approach is well-defined, and evaluation supports the claims."}, "weaknesses": {"value": "The main weakness of the approach is the complexity of the method. Currently, the approach consists of multiple parts and many different pre-trained models that need to be finetuned.  \n\nThe authors do not provide code. It is unclear which simulation environment they have used.\n\nThe authors mention some terms without providing sufficient explanation in their context or a reference to related work. Such terms are MANO poses (line 153), vector-quantization operator (line 187), knowledge distillation (line 203).\n\nSmaller writing comments and typos:\n- In the introduction, the authors listed 4 core capabilities, and 4 contributions. However, they almost identically, and their repetition is not well motivated. Therefore, the authors should either differentiate them better, or combine them in order to improve legibility.\n- Line 045: No space before AffordDexGrasp\n- Line 272: No space between sentences (generation.A practical)\n- Table 1, 2, and 4: The arrow pointing to right next to Diversity is unintuitive, and its meaning might confuse readers, who did not read the evaluations section in detail."}, "questions": {"value": "Have you tried different VLMs?  \n\nHave you tried others pretrained models instead of PointSam and CLIPort? \n\nHave you tried different capacities of the Codebook?\n\nWhich simulation engine have you used? In the Appendix D it is mentioned that Sapien is used for visualization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "aorhmbgi1b", "forum": "cVX3VqO8BO", "replyto": "cVX3VqO8BO", "signatures": ["ICLR.cc/2026/Conference/Submission4717/Reviewer_7Tpz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4717/Reviewer_7Tpz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938981246, "cdate": 1761938981246, "tmdate": 1762917531072, "mdate": 1762917531072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a VLM approach for unified dexterous manipulation. Their approach consists of multiple components. First, a unified tokenizer is learned for multiple hand morphologies using a VQ-VAE codebook. The tokenizer is learned in such a way that all morphologies share the same codebook but different encoder and decoders. For the VLM training, qwen 0.6B is used as base modell. The VLM is trained to ouptut the hand configurations given the object target trajectory and the point cloud of the object. For inference, the point cloud is inferred using PointSAM and the target trajectory using CLIPort. Finally, the hand poses are refined via optimization using a cost function that aligns contact normals with surface normals of the point cloud as well as produces smooth motions. The approach is tested on several benchmark datasets as well as real robot tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- First VLM for unified (multi-embodiment) dexterous manipulation\n- The VLM can be trained purely from human data\n- Performance seems to be competitive\n- Real robot experiments are convincing"}, "weaknesses": {"value": "- Some sections are unclear and missing detail (see questions)\n- The paper would benefit from further ablations. E.g. the different parts of the cost function. Or the benefits of using a unified latent space. Here, the (maybe naive) alternative would be to learn everything in a single space (e.g. MANO) and retarget the output of the VLM afterwards. Such a comparison would be insightful. \n- There is no further information on the costs of the optimization of the grasp. How long does it take? Is it real-time capable?\n\nHowever, I think the approach is interesting and shows a promising performance. The strengths do outweight the weaknesses."}, "questions": {"value": "- I got confused in section 3.3 as its unclear how the mathematical objects in Eq. 7 and 8 look like. Could you please add more information here and clarify what T_tar and P_obj is (also formally, what is the dimensionality)?\n- Is the target trajectory the 6D pose trajectory of an object or the 3D positions of a keypoint?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NWyUXS2ToV", "forum": "cVX3VqO8BO", "replyto": "cVX3VqO8BO", "signatures": ["ICLR.cc/2026/Conference/Submission4717/Reviewer_s9Mv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4717/Reviewer_s9Mv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112873577, "cdate": 1762112873577, "tmdate": 1762917530609, "mdate": 1762917530609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}