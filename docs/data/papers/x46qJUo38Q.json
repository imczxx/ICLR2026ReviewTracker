{"id": "x46qJUo38Q", "number": 9605, "cdate": 1758129698245, "mdate": 1759897709540, "content": {"title": "Multi-Operator Few-Shot Learning for Generalization Across PDE Families", "abstract": "Learning solution operators for partial differential equations (PDEs) has become a foundational task in scientific machine learning. However, existing neural operator methods require abundant training data for each specific PDE and lack the ability to generalize across PDE families. In this work, we propose MOFS: a unified multimodal framework for multi-operator few-shot learning, which aims to generalize to unseen PDE operators using only a few demonstration examples. Our method integrates three key components: (i) multi-task self-supervised pretraining of a shared Fourier Neural Operator (FNO) encoder to reconstruct masked spatial fields and predict frequency spectra, (ii) text-conditioned operator embeddings derived from statistical summaries of input-output fields, and (iii) memory-augmented multimodal prompting with gated fusion and cross-modal gradient-based attention. We adopt a two-stage training paradigm that first learns prompt-conditioned inference on seen operators and then applies end-to-end contrastive fine-tuning to align latent representations across vision, frequency, and text modalities. Experiments on PDE benchmarks, including Darcy Flow and Incompressible Navier Stokes variants, demonstrate that our model outperforms existing operator learning baselines in few-shot generalization. Extensive ablations validate the contributions of each modality and training component. Our approach offers a new foundation for universal and data-efficient operator learning across scientific domains.", "tldr": "", "keywords": ["PDEs", "Neural Operator", "AI for science", "Scientific Machine Learning", "Operator Learning", "Multimodal Machine Learning", "Domain Adaptation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c061ec2e5d8cff0c7f3e4cb0727f912d61b5353b.pdf", "supplementary_material": "/attachment/37f1f1b5ce7ed04db2e32622ddb379efb8c49db3.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes MOFS, a multimodal framework for few-shot operator learning across PDE families. The method combines (i) self-supervised pretraining of a shared FNO encoder with spatial and frequency reconstruction, (ii) text-conditioned operator embeddings derived from statistical summaries of PDE fields, and (iii) a memory-augmented prompting mechanism that retrieves prior PDE examples to enhance inference. The training pipeline involves two stages: prompt-conditioned supervised learning and contrastive fine-tuning. Empirical evaluations on Darcy Flow and Incompressible Navier–Stokes benchmarks are presented, with claims of superior few-shot generalization compared to standard neural operator baselines such as FNO, DeepONet, and UNet."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Clarity of the high-level idea: The paper is decently organized and describes the main components of MOFS: spatial-frequency pretraining, textual embeddings, and memory-augmented multimodal fusion. The architecture and training pipeline are broken down step by step.\n\n- Ambitious goal: Tackling cross-operator few-shot generalization is a challenging and relevant direction in operator learning, where standard neural operator methods typically overfit to a single PDE family.\n\n- Some empirical performance gains: Reported results on Darcy Flow and Incompressible Navier–Stokes show lower relative L2 error compared to baseline methods in a few-shot setting.\n\n- Ablation studies: The authors attempt to evaluate contributions of different components (e.g., without text, vision, memory, pretraining)."}, "weaknesses": {"value": "The technical novelty and empirical evidence fall short in several critical areas:\n\n- Lack of genuine novelty:\nMost components of the proposed framework are borrowed or adapted from existing multimodal and meta-learning architectures (e.g., cross-modal attention, CLIP-style text embeddings, memory retrieval). There is little actual innovation at the algorithmic or theoretical level. The “integration” of standard components does not by itself constitute a significant research contribution.\n\n- Weak baselines and missing comparisons:\nThe experiments only compare against fairly basic baselines (FNO, DeepONet, UNet). These are insufficient to support the paper’s strong claims of “new foundation” for operator generalization. Crucially, the method should be compared against stronger and more recent approaches such as: (1) Koopman neural operator as a mesh-free solver of non-linear partial differential equations, (2) Solving High-Dimensional PDEs with Latent Spectral Models. Without these baselines, it’s unclear whether MOFS provides any real improvement over the state of the art.\n\n- Overly complex architecture with limited justification:\nThe method introduces a very large number of components (pretraining objectives, text embeddings, cross-attention, memory retrieval, soft prompts, multiple loss functions). However, the ablation study is superficial — it shows that “removing components hurts performance,” but does not explain why or which components are essential versus redundant. This creates the impression of “architecture inflation” rather than targeted innovation.\n\n- Very limited evaluation scope:\nThe evaluation is confined to Darcy Flow and Navier–Stokes variants with fairly simple variations. This is insufficient to substantiate the strong generalization claims. No high-dimensional PDEs, irregular domains, or more complex physics are tested.\n\n- No robustness or efficiency analysis:\nGiven the model’s complexity, it likely has high computational overhead. The paper does not provide inference latency, memory footprint, or training cost. This is critical for a method claiming to improve “few-shot efficiency.”\n\n- Lack of theoretical grounding:\nUnlike some prior works on operator generalization and meta-PDE learning, the paper does not provide any theoretical insight into why multimodal prompting should yield improved generalization. The method is essentially heuristic.\n\n- Weak justification for text embeddings:\nThe paper uses simple field statistics to generate textual descriptions. It’s not clear how this weak textual signal actually helps operator learning. There is no ablation isolating textual conditioning’s effect in a controlled way beyond a single table entry.\n\n- Writing tone overstates contributions:\nThe abstract and introduction repeatedly claim “first framework,” “universal foundation,” etc., without sufficient empirical or theoretical support."}, "questions": {"value": "- Missing baselines:\nPlease compare against the following state-of-the-art PDE operator learning methods: (1) Koopman neural operator as a mesh-free solver of non-linear partial differential equations, (2) Solving High-Dimensional PDEs with Latent Spectral Models. These are essential baselines to evaluate whether MOFS offers any meaningful improvement.\n\n- Component justification:\nWhy is textual conditioning necessary? What specific structure is learned from the textual descriptions that is not already captured in the numerical fields?\n\n- Complexity analysis:\nPlease provide a breakdown of computational cost — both during training and inference — compared to a standard FNO. How does the multimodal fusion and memory retrieval affect scalability?\n\n- Generalization beyond toy PDEs:\nHow does the method perform on more challenging PDEs (e.g., advection–diffusion, wave equations, or higher-dimensional problems)? Are there failure cases?\n\n- Ablation clarity:\nThe current ablations are too coarse. Please conduct more fine-grained analyses (e.g., only pretraining vs. only memory vs. only text, different prompt lengths, memory size sensitivity, etc.).\n\n- Unclear effect of memory buffer:\nHow large is the memory, how is it updated, and how sensitive is the model to its size and quality? What happens if the retrieved prompts are noisy or irrelevant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SXdNAf7itz", "forum": "x46qJUo38Q", "replyto": "x46qJUo38Q", "signatures": ["ICLR.cc/2026/Conference/Submission9605/Reviewer_5i8H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9605/Reviewer_5i8H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760852828015, "cdate": 1760852828015, "tmdate": 1762921147687, "mdate": 1762921147687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MOFS (Multi-Operator Few-Shot Learning), a multimodal neural framework for learning across families of PDE operators. The key idea is to generalize to unseen PDEs using only a few demonstration pairs by combining (i) frequency-aware FNO pretraining, (ii) text-conditioned embeddings derived from field statistics via BERT, and (iii) a memory-augmented prompting mechanism for cross-modal attention. The authors adopt a two-stage training scheme: supervised few-shot learning followed by contrastive fine-tuning to align latent representations across modalities. Experiments on variants of Darcy Flow and Incompressible Navier-Stokes equations show that MOFS achieves lower relative $L^2$ errors than FNO, DeepONet, and UNet baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to follow.\n\n2. The attempt to use textual statistics as priors and align them with spectral/visual features is interesting and novel within the operator-learning literature."}, "weaknesses": {"value": "1. Lack of conceptual novelty.\n    -    Most components—FNO encoder, contrastive learning, text conditioning, memory-based prompting—are directly adapted from existing architectures (e.g., FNO, CLIP, Flamingo) with minimal innovation specific to operator learning. The paper does not articulate why multimodal fusion or text embeddings are theoretically beneficial for PDEs beyond empirical combination.\n\n2. Weak empirical reuslts.\n    - Although large and diverse public benchmarks such as PDEBench, PDEArena, and Well datasets are available, the experiments only use small subsets (Darcy Flow and simple Navier–Stokes variants) with low resolution and few configurations.\n    - The baselines are outdated. More relevant and competitive comparisons such as UFNO, AFNO, CViT, and recent PDE foundation models including MPP, Poseidon, DPOT, PDEformer are missing. Given the rapid progress in neural operator research, the presented experiments do not provide convincing evidence of state-of-the-art performance\n\n3. Incomplete experimental details\n    - The paper lacks essential information such as the explicit PDE formulations, discretization resolutions, hyperparameter settings (e.g., learning rate schedules, training epochs, optimizers), model sizes, and computational costs. Without these critical details, it is impossible to evaluate the fairness of comparisons or ensure reproducibility of the reported results."}, "questions": {"value": "1. Dataset and setup:\n   - What specific PDE equations and boundary conditions are used for each dataset? What are the spatial resolutions and data size for the pretraining stage?\n   - What are the exact hyperparameter settings (learning rate schedules, optimizers, batch size, number of epochs, etc.) and model sizes? \n\n2. How are the text descriptions generated for unseen PDEs in few-shot evaluation? Given the small number of samples, how stable or reliable are the computed statistics?\n\n3. How large is the memory buffer in practice, and how is it maintained during training and inference? Does memory retrieval improve results beyond simply enlarging model capacity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ccXkYonH8Q", "forum": "x46qJUo38Q", "replyto": "x46qJUo38Q", "signatures": ["ICLR.cc/2026/Conference/Submission9605/Reviewer_4cLZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9605/Reviewer_4cLZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761407298519, "cdate": 1761407298519, "tmdate": 1762921147412, "mdate": 1762921147412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper gives a new method for few-shot learning of PDE surrogates via operator networks. It first consists of a pretraining reconstruction stage (spatial and spectral), followed by two stages of supervised operator learning. The first stage does few shot learning by using the inputs, embeddings, similarly retrieved \"prompts\", and a text embedding of a language description of the statistics of the PDE data.  The second stage minimizes the spatial prediction loss again, but now also with contrastive terms encouraging similar embeddings within PDE datasets and a term encouraging sufficient spread of the embeddings in a memory buffer.  Experiments are run on a few selections from PDEBench."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Exploring operator learning techniques which can generalize via few-shot learning across different PDEs is an important area of research.  Using pretraining approaches for such methods is a promising way forward to more efficient and broadly applicable models for the fast numerical surrogate solutions for PDEs."}, "weaknesses": {"value": "The paper is unclear overall in its presentation of the method and gives little motivation for each of the many components.\n\nThe use of a language model to encode the statistics of the samples for each PDE dataset feels particularly unmotivated.  What is the additional benefit of a natural language representation of these scalar statistics compared to dealing with them directly?  The paper writes that this captures both \"physical statistics and linguistic priors.\"  It is unclear what a linguistic prior means here when the descriptive sentence is entirely composed of a label of the PDE dataset and its numerical statistics.\n\nThere are almost no details given around the experiment implementations, making any attempted reproduction of these results nearly impossible."}, "questions": {"value": "1. In the pretraining phase, why is only the spatial component masked?  \n\n2. Why does the frequency loss for pretraining only look to reconstruct the input frequencies and not the output function's frequencies as well?\n\n3. What are the settings of all method parameters used for the experiments?  E.g. masking ratio, architecture sizes (lengths, widths, number of modes, etc).  Which layers are frozen during pretraining?  What were the corresponding hyperparameters/architecture choices for the models compared against?\n\n4. The paper claims \"physics supervision\" is used in the second few-shot stage, is this referring to the L2 supervised loss of the output of the model with the ground truth field?  Typically this would instead refer to the satisfaction of certain physical constraints, such as the PDE operator itself or some other conservation law."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iBj8Rubltx", "forum": "x46qJUo38Q", "replyto": "x46qJUo38Q", "signatures": ["ICLR.cc/2026/Conference/Submission9605/Reviewer_XqoS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9605/Reviewer_XqoS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949873542, "cdate": 1761949873542, "tmdate": 1762921147071, "mdate": 1762921147071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce MOFS, a multi operator for few-shot learning. Aiming for generalizations to unseen PDE operators. Leveraging pre-training using a combination of spatial and frequency losses, a few-shot fine-tuning and an end-to-end contrastive learning phase based on a multimodal approach involving text-conditioned embedding and memory embedded prompting, this model enjoys high performance when compared to other classic neural operator architectures such as FNO or DeepOnet."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tVery innovative multi-modal approach\n-\tStrong performance is achieved on the benchmark PDEs considered."}, "weaknesses": {"value": "-\tThe comparisons with other models, such as DeepOnet or FNO, are a bit unclear; do we consider a similar number of  parameters for these models ?\n-\tNot much infration is given regarding how expensive the various phases of training are, in particular the contrastive learning.\n-\tThe text conditioning is hard coded, and could possibly hinder the generalization of the method to more complex PDEs."}, "questions": {"value": "-\tCan the format of the text used generalize to more complex PDEs ? would we need to consider more complex statistics ?\n-\tWhat if the PDE name and the various statistical data where embedded in a different way (say, some labels); and the model then conditioned on them.would the performance be diminished ?\n-        How expensive is the whole training procedure in comparison to that of a classic neural operator model ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hPfKHl5aYr", "forum": "x46qJUo38Q", "replyto": "x46qJUo38Q", "signatures": ["ICLR.cc/2026/Conference/Submission9605/Reviewer_7Nu4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9605/Reviewer_7Nu4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998299792, "cdate": 1761998299792, "tmdate": 1762921146743, "mdate": 1762921146743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}