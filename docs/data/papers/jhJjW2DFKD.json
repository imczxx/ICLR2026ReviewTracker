{"id": "jhJjW2DFKD", "number": 1041, "cdate": 1756829761312, "mdate": 1759898231402, "content": {"title": "ARC-Decode: Risk-Bounded Acceptance for Sampling-Based Speculative Decoding", "abstract": "As larger language models deliver stronger capabilities, their autoregressive inference becomes increasingly expensive. \\emph{Speculative decoding} accelerates generation by letting a fast draft process propose tokens that the target model verifies in parallel. Yet under sampling ($T>0$), observed speedups consistently lag behind those under greedy decoding: verification expends compute on low-value branches, and strict equality rejects drafts that would induce only negligible changes in the next-step conditional distribution.\nA key limitation under sampling is this \\textbf{over-rejection of low-risk drafts, which depresses acceptance rates and limits acceleration.}\nTo address this gap, we propose **ARC-Decode** (**A**cceptance with **R**isk **C**ontrol), a training-free method that augments speculative decoding and requires no extra forward passes. Our method ensures soft acceptance while guaranteeing that accepting non–top-1 drafts causes only negligible next-step distributional shifts, as measured by Jensen–Shannon divergence. ARC-Decode combines (i) confidence-based pre-verification filtering that preserves high-probability branches while enforcing prefix closure and leaf safety, and (ii) a risk-bounded acceptance criterion using an analytic upper bound on the next-step distribution shift from embedding and logit differences. Integrated into the state-of-the-art EAGLE-3 pipeline, ARC-Decode increases accept length per cycle and reduces verification compute, achieving up to **1.6**$\\times$ end-to-end speedup over EAGLE-3 under sampling with negligible quality change across benchmarks.", "tldr": "ARC-Decode is a training-free method that accelerates speculative decoding under sampling via entropy-aware pruning and risk-bounded acceptance, increasing accept length to achieve up to 1.6× speedup with negligible quality degradation.", "keywords": ["LLMs", "Speculative decoding", "Sampling-based decoding", "Inference acceleration", "Risk-bounded acceptance"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10c16318f0e3a1b0a597504584e613fa60a49a1c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents ARC-Decode, an augmentation for the EAGLE-3 speculative decoding method. ARC-Decode uses an entropy-guided pre-verification strategy for pruning draft trees, and claims to apply \"a risk-bounded soft-acceptance rule that provably controls next-step distributional divergence\".\n\nExperiments with three models - Llama-3.1-8B, Qwen3-8B, Vicuna-13B - demonstrate that ARC-Decode improves the speedup of EAGLE-3."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper's topic - speculative decoding - is an important technique in the current LLM literature."}, "weaknesses": {"value": "- The paper is fundamentally flawed. It seems the authors are not familiar with how speculative decoding works, and many errors in the paper seem like hallucinations from undisclosed LLM usage.\n\n  - Line 34-35: where is \"strict equality\" used in verification for the $T>0$ scenario?\n\n  - Line 40 (and Section 3.3): In the scenario $T>0$, it is not necessary to accept the top-1 draft token to begin with.\n\n  - Line 56-57: per-token computation is determined solely by model architecture and size, and has nothing to do with the model being reasoning-specialized or not.\n\n  - Line 131: please illustrate which part of the EAGLE-3 paper mentions \"removing distribution-matching constraints\"?\n\n  - Speculative decoding (both for greedy decoding and for sampling) does not change the target model's output distribution. Indeed, in the sampling scenario, the exact output may differ even when the distribution does not change. However, this should not affect the performance on downstream tasks. If there is a notable difference, it means either 1) the algorithm changes output distribution, or 2) there is an implementation error, or 3) the evaluation set is too small to mitigate random variation, and thus unsuitable for evaluating speculative decoding methods in the sampling setting.\n\n  - Alternatively, no one says the authors can't do research on inference speedup methods that change the output distribution, so long as they 1) clearly explain how their methods differ from speculative decoding, and 2) show that their methods do not degrade output quality for widely used models and benchmarks. For this purpose, the benchmarks used in the current paper (MT-Bench, HumanEval, GSM8K, Alpaca) and one of the models (Vicuna) are outdated and probably contaminated. The most commonly used benchmarks right now include but are not limited to MMLU-Pro GPQA, MATH, IFEval, AIME, Live(Code)Bench, SWE-Bench, etc.\n\n- Line 195-203: in speculative decoding with $T>0$, verification requires the distribution over vocabulary, not just the sampled sequence. This is obviously intractable for a sequence with 1024 tokens. While I understand why the authors would think of such an approximation, it's neither theoretically nor empirically justified. Theoretically, how much error can result from this approximation? Empirically, many LLM applications (e.g. frontier math reasoning, coding) require very precise generation, where a single erroneous token can lead to completely wrong results. And as far as I know, the mentioned metrics - BERTScore, sentence embedding, NLI - are not trained to capture such nuances.\n\n- The technique proposed in Section 3.2 - entropy-guided pre-verification pruning - is nothing new. See [1,2].\n\n[1] AdaEDL: Early Draft Stopping for Speculative Decoding of Large Language Models via an Entropy-based Lower Bound on Token Acceptance Probability. ENLSP@NeurIPS 2024.\n\n[2] Draft Model Knows When to Stop: Self-Verification Speculative Decoding for Long-Form Generation. EMNLP 2025."}, "questions": {"value": "- Line 53-54: ICLR is a conference for academic contributions, not hearsay. Where does this \"reportedly\" come from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BFpv4iCiJw", "forum": "jhJjW2DFKD", "replyto": "jhJjW2DFKD", "signatures": ["ICLR.cc/2026/Conference/Submission1041/Reviewer_537d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1041/Reviewer_537d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760778250176, "cdate": 1760778250176, "tmdate": 1762915660565, "mdate": 1762915660565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ARC‑Decode, a *training‑free* plug‑in for speculative decoding under sampling. The method has two pieces. First, an entropy‑guided pre‑verification pruning that filters low‑mass branches in the draft tree while enforcing prefix closure and leaf safety. Second, a risk‑bounded soft acceptance rule that accepts non–top‑1 draft tokens when a calibrated upper bound on the next‑step Jensen–Shannon (JS) divergence is below a tolerance. The bound combines an embedding‑side Lipschitz surrogate (via tied embeddings and whitening) with a logit‑margin surrogate; the method uses the tighter of the two and requires no extra forward passes at verify time. Integrated into EAGLE‑3, ARC‑Decode raises accept length and end‑to‑end speedup across MT‑Bench, HumanEval, GSM8K, and Alpaca with near‑parity quality. For Llama‑3.1‑8B on Alpaca, the paper reports 2.28× speedup vs. autoregressive and ~1.6× over EAGLE‑3 under matched sampling (T=1)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is simple to integrate. Entropy‑guided pruning uses root entropy and path mass to keep a contiguous backbone (Fig. 3b), and the Local Tolerance Score (LTS) accepts when the tighter of an embedding‑bound and a logit‑margin bound is below a calibrated threshold (Fig. 3c). No extra forward passes over EAGLE‑3 are needed at inference time.\n2. Results are consistent. Across three backbones (Llama‑3.1‑8B, Qwen‑3‑8B, Vicuna‑13B), ARC‑Decode increases accept length and speedup on all four tasks. Example: on Llama‑3.1‑8B MT‑Bench, τ grows from 3.35 to 4.49 and speedup from 1.84× to 2.40×; on Alpaca, speedup rises from 1.42× to 2.28× (Table 2, p.7). Accuracy remains at parity or slightly better than EAGLE‑3 per Table 3 (p.8). The ablation (Table 4, p.8) shows LTS drives most of the gain, while pruning contributes modestly and reduces verification workload.\n3. The appendix provides a clear statement and proof of Theorem 1, calibration pseudocode (Alg. 1), and a useful coverage diagnostic (Fig. 6) that examines normalized JS after accepting non–top‑1 tokens. This improves transparency of the calibration‑based guarantee."}, "weaknesses": {"value": "1. The motivation to accept some potentially correct draft tokens is intuitive and important. However, Arc-Decode takes its benefit **at the cost of dropping the lossless property**. While the authors provide some experiments to demonstrate Arc-Decode will not harm the performance, I still doubt its generalizability in diverse scenarios (role-play, multi-lingual long context and so on). If I were a model provider, the cost of performance degradation is unaffordable. Some block-level / tree-level verification methods [1, 2] that can improve the efficiency without sacrificing performance are preferred.\n2. The paper does not cite or compare against **Fuzzy Speculative Decoding**, which *explicitly* accepts based on a divergence threshold between target and draft distributions and offers a user‑tunable accuracy–runtime trade‑off. This is very close in spirit to ARC‑Decode, which uses a *bound* instead of the actual divergence. Without a direct comparison to FSD on the same setups, the novelty and advantage of LTS remain unclear. Please add FSD as a baseline and discuss differences in compute and quality control. \n3. The guarantee hinges on (i) weight tying to compute token‑embedding differences, (ii) a *local* Lipschitz assumption for the embedding‑to‑logit map, (iii) a softmax Lipschitz constant on $\\mathbf{l}2$, (iv) active‑vocabulary truncation with smoothing, and (v) calibration‑to‑test exchangeability. In real deployments, some backbones do not strictly tie input/output embeddings; local Lipschitz constants can vary sharply across contexts; the tail outside the top‑K union can carry non‑trivial mass; and exchangeability is brittle across domains (e.g., math/code vs. chat). The paper absorbs many of these into constants fitted by quantiles, which converts the guarantee into a data‑dependent heuristic. The appendix acknowledges bucketed calibration as a fix, but no *multi‑domain* evaluation is shown. This weakens the “risk‑bounded” claim.\n4. The paper omits a comparison with methods that *already* relax verification: training-based **Judge Decoding** and training-free lenience speculative decoding. Meanwhile, the experiments are limited in small-scale LLMs (<=13B). The efficiency gain in large-scale LLMs (>=70B) remains vague.\n\nIf the authors clearly address these concerns, I am willing to increase my score.\n\n[1] Sun, Ziteng, Uri Mendlovic,Yaniv Leviathan, Asaf Aharoni, Jae Hun Ro, Ahmad Beirami, and Ananda Theertha Suresh. \"Block verification accelerates speculative decoding.\" *arXiv preprint arXiv:2403.10444* (2024).\n\n[2] Weng, Yepeng, Qiao Hu, Xujie Chen, Li Liu, Dianwen Mei, Huishi Qiu, Jiang Tian, and Zhongchao Shi. \"Traversal Verification for Speculative Tree Decoding.\" *arXiv preprint arXiv:2505.12398* (2025)."}, "questions": {"value": "1. The reported performance of baseline Eagle-3 is lower than its official reported number. For example, the reported MAT and speedup of L31-8B on MT-bench with temperature=1 is 4.24 and 3.07x, while the manuscripts report 3.35 and 1.84x. Could you please explain the distinct gap? Why the `max_draft_tokens` is set to 32 (not official 60)?\n2. I wanna in some extremely difficult tasks, can Arc-Decode still keep the output performance. \n3. I find that in the ablation study, the speedup of ARC (prune-only) is almost same to the Eagle-3 baseline. Does this mean we do not need the `entropy-guided pre-verification pruning`?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Cfe3aqM4Yl", "forum": "jhJjW2DFKD", "replyto": "jhJjW2DFKD", "signatures": ["ICLR.cc/2026/Conference/Submission1041/Reviewer_9tWv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1041/Reviewer_9tWv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546494930, "cdate": 1761546494930, "tmdate": 1762915660234, "mdate": 1762915660234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates speculative decoding in the sampling regime, i.e. when tokens are drawn from the target distribution with temperature T > 0. While lots of progress has been made in the literature for greedy decoding, the sampling regime still remains tricky due to the high rejection rate, degrading the performance of most approaches in the literature. The authors propose their approach called ARC-Decode by enhancing current approaches with the following training-free mechanisms:\n\n1. A confidence-based filtering applied before verification that prunes low probability branches, avoiding unnecessary compute from the target model which the authors show is one of the main bottlenecks in this regime.\n2. A theoretically motivated soft acceptance scheme, allowing non-top 1 tokens to be accepted if the incurred divergence is controlled. This is motivated by the authors’ observation that empirically, many more tokens could be accepted without damaging the output quality.\n\nIn combination, these two enhancements lead to significant speedups over prior works in the high temperature setting, and the authors show that their approach remains robust across a range of temperatures."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This work tackles an aspect of speculative decoding that is kind of under-appreciated in my opinion, trying to make it work more efficiently under sampling. This is an important problem and indeed makes it more difficult to deploy speculative decoding in real scenarios. Any improvements in such a setting are very useful for the literature.\n2. The method performs well on a variety of benchmarks and the authors also compare against arguably the strongest baseline in the form of EAGLE-3, making it convincing that their approach really does add something on top of a strong model. The lossy approach to verification is also carefully checked on several benchmarks, which is important to ensure that no quality degradation occurs."}, "weaknesses": {"value": "1. The structure of the text and the ease of reading could definitely be improved. The acceptance scheme in speculative decoding when considering the sampling scenario is not really defined anywhere. How does one actually accept or reject? In the experimental section, speculative sampling is mentioned but also not clearly described. I am very familiar with speculative decoding in the greedy setting but still I found it unclear at times what exactly the problem setting is here. Similarly, Section 3.3 would benefit from clearer writing. What is a non-top-1 draft here? Non-top-1 under target I assume? But why only accept top-1 drafts, I thought we were dealing with sampling from the target? I’m also not sure how relevant it is to have the derivation of the upper bound to the Jensen-Shannon divergence here in the main text, it ended confusing me more than helping. In general, it would help to have a clear statement of the criterion employed in the end.\n2. The speedups over Eagle are quite nice, but I would like to understand better what framework was used here. You report tokens/s (which to be fair, many SD papers don’t even do) but according to the numbers, this suggests that the autoregressive baseline runs around 40 tokens/s for Llama-3.1-8B? This does seem a bit slow, e.g. https://github.com/meta-pytorch/gpt-fast reports 94 tokens/s, which would still be faster than your final model even. What quantization did you use? How does it change results? The mentioned framework can go up to 140 tokens/s with 8bit quantization. I do acknowledge that this line of work rarely is precise when it comes to optimized baselines but I would still like to clarify this."}, "questions": {"value": "1. How does the baseline perform when temperature is varied? Does ARC outperform them across most temperatures?\n2. From Table 2, it’s quite clear that speedups over Eagle3 can vary quite a bit from benchmark to benchmark, e.g. Alpaca seems to benefit a lot from your approach, while HumanEval does less so. Do you have an intuition why? Which kinds of benchmarks do you believe are better targets for your approach?\n3. In Figure 4, it would be nice to add at least one baseline speculative decoding approach such as EAGLE-3. How would its speedup curve look w.r.t. temperature? It would really drive home the point made in the intro to show how the performance gets worse and worse with higher temperature."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hLkMGW00wg", "forum": "jhJjW2DFKD", "replyto": "jhJjW2DFKD", "signatures": ["ICLR.cc/2026/Conference/Submission1041/Reviewer_G1p8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1041/Reviewer_G1p8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907842243, "cdate": 1761907842243, "tmdate": 1762915660046, "mdate": 1762915660046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the persistent inefficiency of speculative decoding under sampling, where strict token-level verification leads to over-rejection of low-risk drafts and limits speedup. The authors propose ARC-Decode, a training-free, plug-in method that increases acceptance length while maintaining generation quality. The approach combines two components: (1) an entropy-guided pre-verification pruning mechanism that filters low-value draft branches using a depth-aware confidence score, and (2) a risk-bounded acceptance rule based on a Jensen–Shannon divergence upper bound derived from embedding and logit differences. Together, these provide a provable safety guarantee for soft acceptance. Integrated into the EAGLE-3 pipeline, ARC-Decode achieves consistent improvements across models (LLaMA-3.1-8B, Qwen-3-8B, Vicuna-13B) and benchmarks (MT-Bench, HumanEval, GSM8K, Alpaca), delivering up to 1.6× end-to-end speedup under sampling with no measurable loss in accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a theoretically grounded risk-bounded acceptance criterion with Jensen–Shannon divergence guarantees, providing a principled safety rule that directly addresses a key inefficiency in speculative decoding, the over-rejection of low-risk drafts under sampling.\n- The proposed Local Tolerance Score (LTS) effectively translates the theoretical bound into a practical acceptance rule using lightweight features such as embedding and logit differences.\n- The approach is training-free and integrates seamlessly with existing speculative decoding pipelines, making it directly applicable to real-world inference systems."}, "weaknesses": {"value": "- Theoretical sections, particularly those deriving the Lipschitz-based JS bound, are difficult to follow and would benefit from clearer intuition or intermediate explanations.\n- The approach depends on calibrated constants estimated from a small held-out set, but the paper does not analyze how these parameters generalize across model scales, domains, or prompt distributions.\n- The entropy-guided pruning module is interesting, but it would strengthen the paper to include a comparison with alternative pruning or filtering strategies used in prior speculative decoding work.\n- The paper mentions but does not empirically compare against related soft verification methods such as Medusa or Judge Decoding, which would clarify the trade-offs between fidelity and speed."}, "questions": {"value": "- How stable is the calibration parameter when prompt length, domain, or model configuration changes?\n- Could you provide empirical plots comparing the true Jensen–Shannon divergence with the estimated upper bounds (U_emb and U_logit) to assess the tightness of the approximation?\n- Have you tried alternative pruning rules, and how do they affect decoding speed or acceptance rate?\n- How does total or cumulative risk behave as accepted output length grows?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GXg7NfxpJn", "forum": "jhJjW2DFKD", "replyto": "jhJjW2DFKD", "signatures": ["ICLR.cc/2026/Conference/Submission1041/Reviewer_QtDt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1041/Reviewer_QtDt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979435525, "cdate": 1761979435525, "tmdate": 1762915659876, "mdate": 1762915659876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}