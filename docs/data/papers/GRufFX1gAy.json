{"id": "GRufFX1gAy", "number": 10634, "cdate": 1758178174032, "mdate": 1763748586081, "content": {"title": "InnoGym: Benchmarking the Innovation Potential of AI Agents", "abstract": "LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present \\textbf{InnoGym}, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide \\textbf{iGym}, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.", "tldr": "", "keywords": ["innovation", "agent", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f81e321ac7efc787803e14564254bed33c7f0b8d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces InnoGym, a framework and benchmark to quantify the innovation potential of AI agents along two axes: performance gain over the best known solutions and novelty relative to a curated Known Solution Space. The benchmark component iBench comprises 18 Improvable Tasks selected and standardized through a two‑stage curation pipeline. The system component iGym is a unified SDK for long‑horizon, tool‑centric execution. Experiments on 10 tasks evaluate MLAB, CodeAct, and AIDE and analyze time budget, base model, and temperature effects."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Formalization \\(T=(P,S,V,D)\\) and metrics \\(G,N\\) are clear and they enabling evaluation beyond correctnes. This is useful for measuring breakthroughs. Feasibility gating \\(C(s)\\) ensures novelty is only credited to valid solutions. \n\n+ Dataset curation is well-documented. The authors provide details of two‑stage filtering for resource availability and evaluator quality \nwith clear visible/hidden split of artifacts.\n\n+ The iGym SDK offers great abstractions for tools/environments, recovery+concurrency, with support for workflow‑ and agent‑mode paradigms."}, "weaknesses": {"value": "- The novelty metric is under‑specified and potentially subjective. \\(D(\\cdot,\\cdot)\\) is not clearly defined in Sec. 2.2, and Sec. 4.1 uses to a rubric without a precise equation mapping rubric scores to \\(N\\). Aggregation/normalization for \\(N\\) is described qualitatively and thresholds for declaring “innovative success” are not formalized (Sec. 4.1). \n\n- The experimental scope and baseline coverage are limited. Comparisons involve only three agent frameworks (Table 2). Table 2 also includes many missing entries.\n\n- The results lack statistical rigor. Table 2 reports point estimates only (no confidence intervals or multiple seeds). Fig. 5 lacks error bars or variability reporting. No hypothesis tests or bootstrapping are provided for framework comparisons (Sec. 4.2).\n\n- The optimum set notation mixes set and element: “\\(S^*\\in\\arg\\max_{s\\in S}V(s)\\), \\(V^*=V(s\\in S^*)\\)” (Sec. 2.1). The transition from continuous \\(D\\) (Sec. 2.2) to discrete rubric scoring (Sec. 4.1) lacks a formal link.\n\n- The \"Known Solution Space\" is said to be comprehensive, but per‑task counts/diversity statistics are not reported (Sec. 4.1). These is also no audit of profile quality (manual verification or inter‑rater agreement) is shown. Since \\(S_{\\text{known}}\\) is finite, coverage can bias novelty (p. 18)."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YM5Jo9Oxeq", "forum": "GRufFX1gAy", "replyto": "GRufFX1gAy", "signatures": ["ICLR.cc/2026/Conference/Submission10634/Reviewer_UjRg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10634/Reviewer_UjRg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858289867, "cdate": 1761858289867, "tmdate": 1762921889919, "mdate": 1762921889919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes InnoGym, a benchmark that aims to evaluate the innovation potential of AI agents beyond just producing the correct answers. Agents are evaluated based on both performance gains and novelty (methodological differences). The authors include 18 engineering tasks with iGym as the execution environment. Experiments show the different behavior across different methods. Further analysis reveals the impact of prior knowledge, execution time, base models, and sampling temperature."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tEvaluating the model performance on novelty, besides performance gain, is interesting and important. In the context of the benchmark, since the solution space can be grounded with known solutions, the evaluation of novelty can be quantified.\n\n2.\tThis paper is well-written and easy to follow, with clear figurative illustrations.\n\n3.\tThe authors show good comparison with existing benchmarks in Table 1 to consolidate the motivation.\n\n4.\tThe analysis, e.g., prior solution, is well-executed."}, "weaknesses": {"value": "1.\tThe definitions in Section 2.3 are very strong and not well-grounded. For example, SWE-Bench is defined as a “solved problem”. However, the optimal solutions to the bugs are clearly not defined or estimated. It is also an unsolved/improvable task for agents. On the other hand, for improvable tasks, how is achieving a new state-of-the-art performance defined as novelty? For example, this might be achieved by simply extending the training or using more in-domain data.\n\n2.\tThe evaluation of novelty is much dependent on the distance function. However, there is not enough detail about how the function is implemented. Is it done by the Comparison prompt shown in B.2? It is unclear what distance function can measure the similarity in solutions to extract a novelty score. For example, two code files might be semantically dissimilar but have the same logic. See BRIGHT (https://arxiv.org/abs/2407.12883) and EquiBench (https://arxiv.org/pdf/2502.12466) for further understanding of this question.\n\n\n3.\tExtending from 2, there is no human grounding for the novelty/differences in the solution. It remains unclear about the qualification of the LLM-as-a-judge. The authors can either improve the robustness by breaking the solution down into tools/methods and showing what patterns are captured by the judge. Alternatively, the authors can show the correlation between judges and humans.\n\n\n4.\tDo we actually need novelty in some ML competition tasks at this stage, especially when all methods have very low scores in Table 2? The quality of the novelty evaluation is bounded by the candidate solutions. However, the human candidate solution set is not acquired with a novelty objective. Simple methods that are executed well but not necessarily novel can be a good solution for some of these tasks\n\n5.\tGiven that the current definition of novelty is more like diversity, which covers part of the whole space of innovation (e.g., not covering creating new research direction), it would be good if the authors could show some pragmatic usefulness of the current definition, e.g., diverse solutions can make a better ensemble method."}, "questions": {"value": "1. One small concern is that there is no specific instruction on novelty given to the model / agentic framework. I am wondering how such instruction would influence the novelty scores.\n\n2. The citations to MLAB, CodeAct, and AIDE seem missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XcYbsQlL0Z", "forum": "GRufFX1gAy", "replyto": "GRufFX1gAy", "signatures": ["ICLR.cc/2026/Conference/Submission10634/Reviewer_Rk9e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10634/Reviewer_Rk9e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885670974, "cdate": 1761885670974, "tmdate": 1762921889547, "mdate": 1762921889547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes InnoGym, a benchmark and execution framework to rigorously evaluate AI beyond correctness, to include ‘innovation’. The authors formalize innovation as a combination of performance gain over the best-known solutions and methodological novelty, and introduce an evaluation pipeline grounded in this definition. They curate 18 improvable real-world scientific and engineering tasks, standardize executors and validators. Relatedly, the authors also propose iGym, a unified tool-and-execution environment to support agent evaluation on its innovation. Empirical studies across LLM agents show that while some systems produce novel solutions, they rarely surpass human baselines and often fail due to robustness limitations. An insight here is that novel ideas alone are insufficient, i.e. the key bottleneck for current agentic systems is reliably executing and scaling those ideas into verified performance improvements."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The creativity evaluation framework, suite of tasks and gym could be a helpful resource for the AI community.\n\nThe approach of evaluating LLM and agents’ creativity through their generation solution and performance is sound and intuitive."}, "weaknesses": {"value": "The creativity evaluation framework seems to involve heavy amount of computation for each candidate solution as an individual AI model as training and evaluation have to be done for each of them.\n\nSeveral key details are missing in the main text of the paper which can weaken its clarity and reproducibility, for example, what exactly are the distance functions used for each task (seems to be B.2 COMPARISON PROMPT but not mentioned in main text), how are the tasks processed to be use for creativity measure (e.g. some of the tasks are classification based) and meaning of the three methods used in the experiments (e.g. MLAB, CodeAct, AIDE)."}, "questions": {"value": "Formatting in page 14 seems off.\n\nPlease see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xWTYMPtwAr", "forum": "GRufFX1gAy", "replyto": "GRufFX1gAy", "signatures": ["ICLR.cc/2026/Conference/Submission10634/Reviewer_ELHx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10634/Reviewer_ELHx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913270803, "cdate": 1761913270803, "tmdate": 1762921889040, "mdate": 1762921889040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InnoGym, a benchmark with 18 tasks from real-world engineering and scientific domains to evaluate AI agents on both performance and solution novelty. A core contribution is its novelty metric, which quantifies methodological differences from prior approaches. The paper also tested three popular agents in the community with their proposed benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses an important and underexplored aspect: evaluating the novelty of agent-generated solutions, going beyond mere correctness.\n- Constructs a benchmark with 18 real-world \"improvable\" tasks.\n- Provides iGym, a unified and reproducible execution environment that supports long-horizon agent workflows and could benefit the broader research community (if open-sourced).\n- Conducts systematic evaluation of three popular agent frameworks (MLAB, CodeAct, AIDE), offering concrete insights into current capabilities and limitations."}, "weaknesses": {"value": "- **Novelty metric reliability**: The embedding distances of text are highly sensitive to presentation style—identical solutions with different implementations could yield large distances. Conversely, highly novel ideas (e.g., dropout, residual connections) may appear minor in textual or architectural distance but represent significant conceptual leaps. These are known challenges in novelty estimation; it is not reasonable to expect that this paper solves them completely, but the paper should discuss them and clarify the authors’ design choices.\n\n- **Heavy dependence on S_known**: Novelty scores are inherently limited by the completeness and diversity of collected prior solutions. A method may appear highly novel due to missing references. The paper does not address how sensitive results are to its coverage.\n\n- **Lack of methodological detail**:\n  - The distance function is not clearly defined in Section 2.1. While N(s) = ... D(s, h) is stated, experiments (Section 4.1) reveal that novelty is computed via LLM-based profiling and scoring across six dimensions (0–4 scale), not raw distance. Also, what exactly distance function used here?\n  - **Validator construction**: It is unclear whether validators are rule-based, script-based, or LLM-powered. How are complex constraints (e.g., format, feasibility, domain-specific rules) reliably checked?\n  - **Evaluator normalization**: The process of converting relative scores to absolute ones and “adjusting until leaderboard consistency” is confusing. What adjustments were made? Were they manual or automated? How is consistency verified across languages and environments?\n\n- **Agent selection and representativeness**:\n  - The three evaluated frameworks (MLAB, CodeAct, AIDE) lack citations.\n  - Many benchmark tasks are optimization-focused, yet the selected agents are not known to specialize in optimization. Stronger baselines like OpenEvolve or other evolutionary/optimization-oriented agents could be considered."}, "questions": {"value": "see above weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fLNgVnwACL", "forum": "GRufFX1gAy", "replyto": "GRufFX1gAy", "signatures": ["ICLR.cc/2026/Conference/Submission10634/Reviewer_kLJD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10634/Reviewer_kLJD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945729670, "cdate": 1761945729670, "tmdate": 1762921888611, "mdate": 1762921888611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers: Summary of Revisions in Paper for Each Reviewer"}, "comment": {"value": "Dear all reviewers,\n\nThank you for your thoughtful reviews!\n\nWe sincerely appreciate the reviewers’ feedback and questions, which have helped us improve the clarity and completeness of the manuscript. **We emphasize that the core methodology and main experimental conclusions remain unchanged. Most revisions are confined to the Appendix, while changes in the main text are minimal and purely for clarification and readability.** A revised draft of the manuscript has been uploaded, with added or modified content highlighted in green.\n\nThese revisions primarily focus on clarifying dataset construction details and refining the Novelty evaluation protocol.\n\n**For Reviewer kLJD:**\n\n- Clarified that the distance function $D$ uses an **Agent-as-Judge** approach rather than embeddings. (Page 3, Lines 113-115)\n- Added citations for the three Agent frameworks. (Page 6, Line 294)\n- Detailed the implementation method of the distance function $D$. (Page 6, Lines 301-316; Page 20, Appx. E.1)\n- Added a detailed description of the experimental setup. (Page 6, Lines 318-323)\n- Included statistics on the size and diversity of the known solution set $S_{known}$ for each task. (Page 16, Appx. C and Tab. 3)\n- Conducted experiments comparing the distance function $D$ with human evaluation. (Page 20, Appx. E.2-E.4 and Tab. 9, 10, 11, 12)\n- Added a detailed description of **Solution Collection**. (Page 22, Appx. F.1)\n- Added a detailed description of **Evaluator Normalization**. (Page 24, Appx. F.2)\n- Added a detailed description of **Validator Construction**. (Page 25, Appx. F.3)\n\n**For Reviewer ELHx:**\n\n- Clarified that the distance function $D$ uses an **Agent-as-Judge** approach rather than embeddings. (Page 3, Lines 113-115)\n- Detailed the implementation method of the distance function $D$. (Page 6, Lines 301-316; Page 20, Appx. E.1)\n- Added citations for the three Agent frameworks. (Page 6, Line 294)\n- Added an overview of the three Agent frameworks. (Page 17, Appx. D.1)\n\n**For Reviewer Rk9e:**\n\n- Added citations for the three Agent frameworks. (Page 6, Line 294)\n- Detailed the implementation method of the distance function $D$. (Page 6, Lines 301-316; Page 20, Appx. E.1)\n- Added a detailed description of the experimental setup. (Page 6, Lines 318-323)\n- Included statistics on the size and diversity of the known solution set $S_{known}$ for each task. (Page 16, Appx. C and Tab. 3)\n- Added a detailed explanation of the **Task Taxonomy**. (Page 16, Appx. C.1)\n- Added an overview of the three Agent frameworks. (Page 17, Appx. D.1)\n- Added experiments on explicitly prompting agents for innovation. (Page 19, Appx. D.3, Lines 1017-1023 and Tab. 7)\n- Conducted experiments comparing the distance function $D$ with human evaluation. (Page 20, Appx. E.2-E.4 and Tab. 9, 10, 11, 12)\n- Added a detailed description of **Solution Collection**. (Page 22, Appx. F.1)\n\n**For Reviewer UjRg:**\n\n- Corrected the definition of formula $V$. (Page 3, Line 120)\n- Detailed the implementation method of the distance function $D$. (Page 6, Lines 301-316; Page 20, Appx. E.1)\n- Added a detailed description of the experimental setup. (Page 6, Lines 318-323)\n- Added error bars to Figure 5. (Page 8, Fig. 5)\n- Added a detailed explanation of the **Task Taxonomy**. (Page 16, Appx. C.1)\n- Included statistics on the size and diversity of the known solution set $S_{known}$ for each task. (Page 16, Appx. C and Tab. 3)\n- Added statistical analysis for Table 2. (Page 18, Appx. D.2 and Tab. 4, 5)\n- Added submission success rates for the main experiments. (Page 18, Appx. D.3 and Tab. 6)\n- Conducted experiments comparing the distance function $D$ with human evaluation. (Page 20, Appx. E.2-E.4 and Tab. 9, 10, 11, 12)\n- Added a detailed description of Solution Collection. (Page 22, Appx. F.1)\n\nBest Regards,\nAuthors of InnoGym"}}, "id": "AedREsadd4", "forum": "GRufFX1gAy", "replyto": "GRufFX1gAy", "signatures": ["ICLR.cc/2026/Conference/Submission10634/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10634/Authors"], "number": 20, "invitations": ["ICLR.cc/2026/Conference/Submission10634/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763748498264, "cdate": 1763748498264, "tmdate": 1763748498264, "mdate": 1763748498264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}