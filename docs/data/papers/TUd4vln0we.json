{"id": "TUd4vln0we", "number": 4818, "cdate": 1757771654807, "mdate": 1759898011449, "content": {"title": "PerturbFormer: Adversarial Graph Transformers for Scalable and Resilient Representation Learning", "abstract": "We propose PerturbFormer, a unified framework for graph representation learning that integrates multi-scale structural embeddings, contrastive self-supervised pretraining, and heterophily-aware transformer architectures. To address challenges posed by sparse features and noisy topologies, the model introduces adversarial propagation, where generative modules synthesize structural perturbations and discriminators enforce semantic consistency. The framework leverages degree-normalized attention and relative positional encoding to adaptively model diverse graph environments. Efficiency is achieved through graph-free condensation and hardware-aware optimization. Empirical evaluations demonstrate strong performance across homophilous and heterophilous benchmarks, with enhanced robustness under structural noise and temporal dynamics. This work contributes a scalable and resilient solution for graph-based learning in complex domains.", "tldr": "PerturbFormer trains robust graph transformers by having a GAN perturb edges while a confidence-aware residual module self-corrects, beating SOTA on large homophilous and heterophilous benchmarks with fewer parameters.", "keywords": ["Adversarial Graph Learning", "Transformer Architectures", "Multi-scale Embeddings", "Generative Pretraining", "Adaptive Signal Calibration", "Vertex Classification", "Computational Efficiency"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/129e21f23f2d1f5c8daa6bc359bb38053472ab19.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes PerturbFormer, a transformer-based framework for graph learning that integrates multi-scale structural embeddings, contrastive pretraining, and adversarial propagation. By combining degree-normalized attention with generative perturbations, it achieves robust, efficient, and state-of-the-art performance across diverse graph tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper proposes PerturbFormer, a new graph learning framework capable of handling multiple tasks, demonstrating good applicability across various domains.\n2. The paper provides comprehensive experimental analyses, including evaluations on different tasks, ablation studies, efficiency assessments, and insightful visualizations."}, "weaknesses": {"value": "1. The first major weakness of this paper lies in its writing quality. The manuscript is poorly written, and I suspect that the use of LLMs goes beyond typos and grammar (a review request has been submitted). \n   (1) The Introduction fails to clearly convey the motivation and logical foundation of the proposed methods, while it lists up to six “limitations” the paper aims to address—many of which are exaggerated, such as the distillation limitation that is never discussed later.  (2) The Related Work section enumerates six points, but each is overly simplistic and resembles LLM-generated text. (3) In the Methodology section, the authors merely stack several modules without explaining their interconnections, and even use terms that sound machine-generated, such as “residual refinement” vs. “residual correction” or “feature extraction” vs. “feature synthesis,” which severely hampers readability. (4) Figure 1 provides little informative content and looks like an LLM generated it.\n\n2. The PerturbFormer framework appears to be a combination of multiple existing modules and techniques (e.g., multi-resolution representation, contrastive alignment, adversarial propagation), yet their relationships are unclear. The approach PerturbFormer lacks novelty.\n\n3. The experimental setup is confusing. For instance, PCQM4Mv2 is a graph regression dataset, but the paper incorrectly describes it as a node classification task. Table 3 lists datasets without any description or references.\n\n4. The reported results are hard to believe: PerturbFormer achieves the best performance on all tasks and datasets, yet no standard deviation over multiple runs is provided, and no code for reproducibility.\n\n5. Given that PerturbFormer integrates multiple modules, an analysis of its computational complexity is essential. However, no theoretical discussion is offered, and the results in Table 8 are unclear—comparing runtime across different methods and datasets is not meaningful.\n\nOverall, the paper is logically inconsistent and poorly written, the proposed method lacks novelty, and there is concern that the use of LLMs in the writing and content generation exceeds acceptable limits, without any disclosure."}, "questions": {"value": "Please respond to the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dxUKQ5WZhz", "forum": "TUd4vln0we", "replyto": "TUd4vln0we", "signatures": ["ICLR.cc/2026/Conference/Submission4818/Reviewer_4Gtn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4818/Reviewer_4Gtn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760863995747, "cdate": 1760863995747, "tmdate": 1762917592855, "mdate": 1762917592855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses key challenges in graph-based semi-supervised learning, including heterophily, robustness to structural noise, and scalability. It proposes PerturbFormer, an integrated framework that synthesizes multi-scale feature augmentation, contrastive self-supervised pretraining, generative adversarial networks, and optimized transformer architectures. The methodology involves enhancing node representations through multi-hop structural embeddings, adversarial contrastive pretraining, and a Graphormer with degree-normalized attention. Core contributions include a novel adversarial propagation mechanism for dynamic perturbation synthesis and confidence-weighted residual refinement. Experimental evaluation on benchmarks demonstrates improved classification accuracy and robustness compared to state-of-the-art baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.\tThe framework is novel as it combines feature synthesis, contrastive alignment, and GAN-based perturbations to enhance robustness.\n2.\tConvergence guarantees for residual propagation are provided.\n3.\tThe paper is easy to follow."}, "weaknesses": {"value": "1.\tClaims of outperforming prior work in low-homophily settings are not rigorously justified with comparative analysis; evidence is limited to benchmark results in Table 2 without discussing fundamental gaps.\n2.\tThe GAN-based perturbation lacks analysis of critical issues like model training stability. There is no experimental curves or metrics on adversarial optimization progress."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LRbJQRGbIX", "forum": "TUd4vln0we", "replyto": "TUd4vln0we", "signatures": ["ICLR.cc/2026/Conference/Submission4818/Reviewer_gH8g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4818/Reviewer_gH8g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477719212, "cdate": 1761477719212, "tmdate": 1762917592508, "mdate": 1762917592508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors summarize six problems in the graph learning domain. To address these issues, they propose PerturbFormer, which integrates multiple techniques (including contrastive learning, attention mechanisms, and adversarial machine learning). In the experiments, the authors report that it demonstrates strong performance."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The field of graph learning is a valuable area.\n- The paper utilizes large-scale graph datasets for evaluation."}, "weaknesses": {"value": "- The writing should be substantially improved, as it is very challenging to follow. The subsequent weaknesses stem from this aspect.\n- The motivation presented in the paper appears somewhat disorganized. The authors introduce SIX problems in Section 1, but these lack clear organization and detailed explanations.\n- The proposed method seems to simply combine existing techniques without introducing significant novelty.\n- The writing and clarity of formulas in Section 3 should be enhanced. Additionally, I strongly recommend that the authors provide an overview diagram of the method."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zjgyB0E3RE", "forum": "TUd4vln0we", "replyto": "TUd4vln0we", "signatures": ["ICLR.cc/2026/Conference/Submission4818/Reviewer_7g8J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4818/Reviewer_7g8J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939311365, "cdate": 1761939311365, "tmdate": 1762917592104, "mdate": 1762917592104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PerturbFormer, a graph representation learning framework that combines many components, such as multi-scale structural embeddings, contrastive pretraining, a heterophily-aware Graphormer and an adversarial propagation mechanism. The core idea is to jointly train a generator that perturbs the graph topology and a discriminator that enforces semantic consistency in node representations. An adaptive residual correction module further refines predictions using node-level confidence scores. The authors evaluate PerturbFormer on a wide range of  datasets and tasks. The results are powerful."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper effectively integrates many components into a single end-to-end framework.\n2. This paper is well-organized and easy to follow.\n3. The results show strong performance."}, "weaknesses": {"value": "1. The motivation is mentioned briefly but remains underdeveloped. Neither the Abstract nor the Introduction sufficiently clarifies the problem’s significance or the specific gap, and the narrative leans toward describing components.\n2. It remains unclear how much each component contributes to the reported gains. The current ablation (Table 4) removes modules singly and therefore does not assess interaction effects.\n3. While the method is described in detail, key hyper-parameters are not reported."}, "questions": {"value": "1. In Table 5, PerturbFormer shows remarkable robustness. Is this primarily due to the GAN regularizer, the confidence mechanism, or their combination?\n2. The contraction condition (Eq. 26) assumes $\\lVert \\tilde{A}_{\\tau} \\rVert_{2} < 1$, but Appendix C shows this often fails in heterophilous graphs. How to address this in practice? Do you apply spectral normalization, and if so, does it degrade performance on homophilous graphs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9OmMWGeyvy", "forum": "TUd4vln0we", "replyto": "TUd4vln0we", "signatures": ["ICLR.cc/2026/Conference/Submission4818/Reviewer_EZoD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4818/Reviewer_EZoD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988516632, "cdate": 1761988516632, "tmdate": 1762917591827, "mdate": 1762917591827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}