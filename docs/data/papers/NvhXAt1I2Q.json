{"id": "NvhXAt1I2Q", "number": 4345, "cdate": 1757665524408, "mdate": 1759898038546, "content": {"title": "VTBench: Comprehensive Benchmark Suite Towards Real-World Virtual Try-on Models", "abstract": "While virtual try-on has achieved significant progress, evaluating these models towards real-world scenarios remains a challenge. A comprehensive benchmark is essential for three key reasons: (1) Current metrics inadequately reflect human perception, particularly in unpaired try-on settings; (2) Most existing test sets are limited to indoor scenarios, lacking complexity for real-world evaluation; and (3) An ideal system should guide future advancements in virtual try-on generation.\nTo address these needs, we introduce the **V**irtual **T**ry-on **Bench**mark (**VTBench**), the first-ever hierarchical try-on benchmark suite that systematically decomposes virtual image try-on into hierarchical, disentangled dimensions, each equipped with tailored test sets and evaluation criteria. VTBench exhibits three key advantages: 1) Multi-Dimensional Evaluation Framework: The benchmark encompasses five critical dimensions for virtual try-on generation (*e.g.,* overall image quality, texture preservation, complex background consistency, cross-category size adaptability, and hand-occlusion handling). Granular evaluation metrics of corresponding test sets pinpoint model capabilities and limitations across diverse, challenging scenarios. 2) Human Alignment: Human preference annotations are provided for each test set, ensuring the benchmark’s alignment with perceptual quality across all evaluation dimensions. 3) Valuable Insights: Beyond standard indoor settings, we analyze model performance variations across dimensions and investigate the disparity between indoor and real-world try-on scenarios. To foster the field of virtual try-on towards challenging real-world scenarios, VTBench will be open-sourced, including all test sets, evaluation protocols, generated results, and human annotations.", "tldr": "", "keywords": ["Virtual Try-on", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/42f911ee6b66d56a796e5a43870400066cc33eb9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a hierachical virtual try-on benchmark for virtual try-on evaluation on multple aspects, including both the overall and localized texture quality of generated images. The authors use CLIP, DINO and QWen models to construct specific metrics for each aspect and address the image evaluation under the unpaired setting. The proposed benchmark also includes human preference labels to test the metrics' perceptual alignment with human."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The motivation of this paper precisely targets the major challenge in virtual try-on evaluation: coarse similarity metrics that are inconsistent with human perception and unsuitable for texture detail evaluation.\n\n2) Font texture similarity is a novel metric that has been overlooked in prior work, but is very important in real-world setting where it's necessary to preserve brand's text logo.\n\n3) The proposed benchmark is valuable to the virtual try-on community."}, "weaknesses": {"value": "1) Using VLM model to determine size fitness is not convincing. It is difficult to evaluate if a generated garment fits the original body shape because of clothing-body occlusion in the clothed model image. In addition, size fitness itself can also be decomposed into three categories: oversized/loose fitting, normal fitting and tight fitting. The authors provide human alignment scores in the experiments, but it's better to see more evidence showing that the size evaluation in VLM is accurate. Perhaps some visualizations on randomly-selected triplets and their QWen responses can be helpful.\n\n2) The hand consistency metric evaluates hand/joint structure but does not include various hand artifacts that are common in many virtual try-on images. I would suggest changing the name to avoid confusion or including hand artifacts detection (e.g., skin color incosistency and blurry finger edge).\n\n3) I suggest the author provide more details of the data filtering criteria and human evaluation, including sources and number of human annotators."}, "questions": {"value": "1) When evaluating background semantic consistency, what's the justification of choosing DINO instead of QWen VLM?\n\n2) How is the overall rank calculated in Table 1? I assume it is not a simple average since the six metrics all have different magnitudes.\n\n3) The sentence at L417 is cut off.\n\n4) Figure 3 and Figure 5  show results on five dimensions. I suggest the authors also include results of fidelity. It is important as it serves as a baseline to show how mismatched FID/KID is with human judgment."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The proposed dataset contains human subjects."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "601TMsE2le", "forum": "NvhXAt1I2Q", "replyto": "NvhXAt1I2Q", "signatures": ["ICLR.cc/2026/Conference/Submission4345/Reviewer_Nr7j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4345/Reviewer_Nr7j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761158680123, "cdate": 1761158680123, "tmdate": 1762917308690, "mdate": 1762917308690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VTBENCH , a comprehensive benchmark suite for evaluating image-based virtual try-on (VTON) models. The authors argue that existing evaluation methods (like FID/KID) align poorly with human perception and lack consideration for real-world complex scenarios. VTBENCH addresses these issues with a hierarchical evaluation framework that decomposes VTON quality into three main categories and six dimensions: General Image Quality (Similarity, Aesthetics), Garment Preservation (Texture, Size), and Auxiliary Consistency (Background, Hand) ."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper's most outstanding strength is its large-scale human preference annotation study. The results  robustly demonstrate that the proposed new metrics (e.g., for cross-category, texture, hand, and background consistency) are highly correlated with human perceptual judgments , a feature broadly lacking in existing metrics. The VLM-based \"Size Fitness\" metric and the OCR-based \"Font Texture Similarity\" (FTS) metric  are highly innovative. They elevate evaluation from low-level pixel similarity to high-level semantic and logical correctness.\n\n- The curation of four new specialized test sets (CBC, FTF, CSF, HOC) is a significant contribution, specifically targeting common failure cases like hand occlusions , complex backgrounds , and cross-category try-ons  that are ignored by previous benchmarks.\n\n- The comprehensive benchmark analysis of 16 SOTA models provides valuable insights to the community regarding the pros and cons of different architectures (GAN vs. UNet vs. DiT) ."}, "weaknesses": {"value": "- **Questionable Efficacy of Visual Texture Metric**: The paper computes the cosine similarity between CLIP or DINO embeddings of the original garment and the cropped garment region from the generated image  to judge visual texture. However, CLIP and DINO are trained via contrastive or self-supervised learning, which are not inherently designed to enhance fine-grained details. For example, generative methods like IP-Adapter, which use CLIP as an image encoder, often fail to restore reference image details. Furthermore, AnyDoor uses DINO as an encoder but still requires a high-frequency filter to ensure detail consistency. This casts doubt on the ability of CLIP or DINO to reliably measure fine-grained texture.\n- **High Dependency of Background Consistency Metric on Masks**: In the measurement of background consistency , most try-on methods are mask-based, and the masked regions differ between methods. The fit and extent of a given mask will heavily influence the paper's background consistency metric. When evaluating the 16 baselines on the CBC dataset, was a standardized, pre-computed mask provided to all models?\n- **Limited Scope of Hand Consistency Metric**: The Hand Consistency metric  focuses on the consistency of the model's preserved regions, but focusing only on hands is somewhat limited. Since try-on images are often half- or full-body, the hand region has a low pixel ratio. Other elements like the model's hair, face, skin tone, and body shape are visually more critical to the realism of the result and should also be evaluated for consistency."}, "questions": {"value": "- When evaluating the 16 baselines on the CBC dataset, was a standardized, pre-computed mask provided to all models? If so, how was this mask generated? If not (i.e., each model used its own internal mask), how does this metric fairly compare different models (as the score might reflect the mask's size more than the background preservation quality)?\n\n- Why did the authors choose to focus exclusively on hand consistency? Are there plans to expand this consistency evaluation to other non-edit regions that are critical for perceptual realism, such as facial fidelity, hair details, and skin tone consistency?\n\n- Given the known limitations of CLIP/DINO in capturing high-frequency details (as noted in the \"Weaknesses\" section), can the authors provide more evidence that $E_{\\epsilon}$  reliably measures fine-grained texture (rather than just style or the overall shape)? Have the authors considered supplementing this with other detail-focused metrics (e.g., high-frequency variants of LPIPS or SigLIP which is used in FLUX Redux) to enhance the \"Texture Fidelity\" dimension's evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xhFTdc5iqz", "forum": "NvhXAt1I2Q", "replyto": "NvhXAt1I2Q", "signatures": ["ICLR.cc/2026/Conference/Submission4345/Reviewer_bco4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4345/Reviewer_bco4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792185933, "cdate": 1761792185933, "tmdate": 1762917308332, "mdate": 1762917308332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VTBench, a benchmark for evaluating virtual try-on (VTON) models. The key contributions are three-fold: 1 A hierarchical evaluation framework that decomposes virtual try-on quality into six fine-grained dimensions; 2 It introduces several unpaired evaluators to overcome the lack of paired ground-truth images; 3 It provides four custom test datasets with human preference annotations, where a model comparison study is conducted to evaluate different VTON models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ This work establishes a foundation for future research toward realistic and perceptually aligned virtual try-on systems.\n+ It evaluates 16 state-of-the-art models across multiple paradigms, offering valuable comparative insights for the community.\n+ It tailors existing models for virtual try-on evaluation from different perspectives."}, "weaknesses": {"value": "- The aesthetic metric correlates poorly with humans, significantly discrediting the reliability of its use in evaluating and comparing VTON models.\n- It lacks comparison with commonly used full-reference and no-reference image quality assessment metrics.\n- Its claim to guide the development of future VTON models is somewhat overstated, since the benchmark itself doesn’t propose novel generative methods or optimization strategies."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gmSNwcpZ7V", "forum": "NvhXAt1I2Q", "replyto": "NvhXAt1I2Q", "signatures": ["ICLR.cc/2026/Conference/Submission4345/Reviewer_ALsj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4345/Reviewer_ALsj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793811287, "cdate": 1761793811287, "tmdate": 1762917308087, "mdate": 1762917308087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}