{"id": "nY8looE4lO", "number": 456, "cdate": 1756740510129, "mdate": 1759898260196, "content": {"title": "Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation", "abstract": "Current autoregressive diffusion models excel at video generation but are generally limited to short temporal durations. Our theoretical analysis indicates that the autoregressive modeling typically suffers from temporal drift caused by error accumulation and hinders parallelization in long video synthesis. To address these limitations, we propose a novel planning-then-populating framework centered on Macro-from-Micro Planning (MMPL) for long video generation. MMPL sketches a global storyline for the entire video through two hierarchical stages: Micro Planning and Macro Planning. Specifically, Micro Planning predicts a sparse set of future keyframes within each short video segment, offering motion and appearance priors to guide high-quality video segment generation. Macro Planning extends the in-segment keyframes planning across the entire video through an autoregressive chain of micro plans, ensuring long-term consistency across video segments. Subsequently, MMPL-based Content Populating generates all intermediate frames in parallel across segments, enabling efficient parallelization of autoregressive generation. The parallelization is further optimized by Adaptive Workload Scheduling for balanced GPU execution and accelerated autoregressive video generation. Extensive experiments confirm that our method outperforms existing long video generation models in quality and stability. Generated videos and comparison results are in the Anonymous Demo page.", "tldr": "", "keywords": ["Long Video Generation", "Diffusion", "Autoregressive"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/15cc7405cedbef7c83a8257b0442aec8524cbaab.pdf", "supplementary_material": "/attachment/adb2eb221084d5ac2406b126a83f917cc5be4a03.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a macro-micro planning strategy for long video generation. The micro planning phase initially predicts a sparse set of pre-planning frames, which represent the early, mid, and terminal frames of each segment. Then, the macro planning phase predicts the intermediate frames within each segment, conditioned on the predicted first frames from the micro planning. This hierarchical setup can be further parallelised, enabling the natural utilisation of a multi-gpu setup for further latency reduction. The proposed network achieves comparable performance to other inference optimisation methods such as Self-forcing and CausVid on VBench, and demonstrates significantly superior preference rate with human evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is simple and easy to implement. It’s an independent strategy that can be easily combined with self-forcing and other inference speed-up techniques."}, "weaknesses": {"value": "- The linked anonymous demo page cannot be opened. \n- The overall paper writing is poor, with many inconsistent notations and visualisations.\n    - Figure 2’s caption only describes the limitations of current AR methods, without mentioning the proposed method. \n    - Figure 3 also raises questions. Should micro-planning predict frames across all segments, or just segment 1? Similarly, macro-planning should jointly optimise for three frames rather than autoregressively as indicated in the figure caption.\n    - In Section 3.1, M is defined as the Micro planning network or process, but it is also defined as a set containing three predicted frames. This contradicts the example provided in Appendix Figure 11, where the planning frames for early, midpoint, and terminal can be more than one frame. \n    - Similarly, M+ is defined as the macro-planning network or processing, but it is also defined as the number of predicted frames.\n    - Equation 2 is incorrect because the conditioning generative process conditioned on x^1 is not included in the expanded formula.\n    - In Figure 6, t_i represents an timestep or an inference step?\n- The proposed method is built on top of WAN 2.1 14B, while the important baselines, CausVid and Self-forcing, are both distilled to 1.3B models. This makes it unfair to compare them directly. Is it possible to directly report results on the 1.3B model as well, and ensure that all models generate the same length of output?"}, "questions": {"value": "- L192: earyl → early\n- L355: t_a=2,3,4 is inconsistent with Fig 11 t_a=2,3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NHoMdgPnPO", "forum": "nY8looE4lO", "replyto": "nY8looE4lO", "signatures": ["ICLR.cc/2026/Conference/Submission456/Reviewer_R1mP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission456/Reviewer_R1mP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761493308239, "cdate": 1761493308239, "tmdate": 1762915524136, "mdate": 1762915524136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Macro-from-Micro Planning (MMPL) for long video generation with autoregressive (AR) diffusion backbones. The core idea is to (1) plan sparse keyframes per segment (Micro Planning) from the first frame of a segment, (2) chain these micro plans across segments (Macro Planning) by seeding the next segment with the previous segment’s planned endpoints, and then (3) populate all in-between frames in parallel across segments, enabled by the fixed planning anchors. To further increase throughput on multi-GPU setups, the authors add an adaptive workload scheduler with two modes that trade memory peak vs. throughput by choosing whether the next segment starts from the mid-point or terminal planned frame of the previous segment. The method is implemented by fine-tuning Wan2.1-T2V-14B on a curated ~50k video set with LLM-generated captions, and evaluated on VBench-long plus a small user study on 19 MovieGen prompts. Reported results show higher VBench scores and strong user preference, as well as ~3× wall-clock speedups with 4 GPUs for 60-second videos under their pipeline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Making long-video AR generation as plan-then-populate with keyframes per segment, then parallel population conditioned on those anchors, is a clean, modular perspective. While planning and hierarchical generation exist, the specific coupling of joint keyframe prediction from the segment’s first frame plus segment-level AR chaining is crisply articulated and differs from step-jump or packed-context approaches.\n2. The two-mode scheduler (minimum memory peak vs. maximum throughput) provides an explicit, practical deployment knob that many papers hand-wave. The interplay between when to start the next segment and how to reuse/skip frames is well described.\n3. The error-accumulation analysis connects AR drift to exposure bias and imitation learning regret bounds, motivating the reduction from frame-wise to segment-wise dependency depth under MMPL. While mostly qualitative/upper-bound reasoning, it grounds the need for planning anchors and shorter causal chains.\n4. The stability tooling around re-encode/decode across segment boundaries and an appendix noise-initialization scheme for smoother transitions addresses practical artifacts (color shift/flicker) that often undermine long-video demos."}, "weaknesses": {"value": "1. Questionable novelty boundary vs. prior “planning/parallel AR” lines. The closest prior they cite, FramePack-Plan, already reduces error via step-wise frame jumping and context compression; other works use hierarchical/story planning or parallelized AR decoding. The paper states three innovations (two-level plan, one-pass segment keyframe prediction, then parallel population), but the empirical study does not isolate what is fundamentally new versus combinations/engineering of known techniques. A finer related-work positioning and head-to-head comparisons against frame-jump/packed-context methods are missing.\n2. The macro vs. micro contribution is not ablated: we see ablations for positioning of planning frames and training policies, but not for “Micro only”, “Macro only”, “Population parallelism only”, or “Scheduler off”. This makes it hard to credit the gains to the planning hierarchy versus downstream parallel population and scheduling.\n4. implementations and backbones vary (e.g., Wan-14B fine-tuned for the proposed method, distilled 1.3B baselines among others). The paper says “comparable scale,” but the exact compute/training budgets and data curation (LLM captions; selection of top 1% licensed clips by LAION aesthetic scoring) could favor the proposed system’s data quality. A deeper fairness audit (same base model, same finetuning budget) is needed.\n5. The choice of mid-point vs. terminal seeding trades memory vs. throughput, yet we lack measured curves (utilization, peak VRAM, effective FPS) to guide practitioners.\n6. Missing or thin ablations on stability modules. The re-encode/decode stitching and noise-interpolation initialization (Appx.) look helpful, but there are no quantitative ablations isolating their impact (e.g., color-shift metrics, boundary flicker metrics, LPIPS/temporal warping vs. off). Without this, it’s hard to know how much they contribute versus planning alone."}, "questions": {"value": "1. Can you provide direct comparisons to FramePack-Plan (and other planning/packed-context or parallel-AR methods) under the same backbone, training budget, and prompts? Ideally include Micro-only, Macro-only, Parallel-only, and Scheduler-off ablations to quantify the unique value of each component. \n2. Please include stage-wise wall-clock breakdowns (Micro Plan, Macro chain latency, Content Population, re-encode/decode, inter-GPU transfer) and utilization curves for 1/2/4/8 GPUs, with peak VRAM and throughput across different ta/tb/tc settings and segment sizes. How sensitive is the speedup to segment granularity and anchor spacing? \n3. You mention inevitable prefix delays before parallelism ramps. Can you quantify this overhead and propose a look-ahead or speculative planning scheme to reduce it? Do you consider asynchronous seeding (e.g., starting s+2 from xtb of s before s+1 finishes)?\n4. Please add ablations toggling re-encode/decode and the noise-interpolation initialization (Fig. 13) with objective boundary stability metrics (e.g., temporal color histograms near joins, t-LPIPS at boundaries). Also report any failure cases (banding, ghosting)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ygWavfSWh4", "forum": "nY8looE4lO", "replyto": "nY8looE4lO", "signatures": ["ICLR.cc/2026/Conference/Submission456/Reviewer_b7Dw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission456/Reviewer_b7Dw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863083906, "cdate": 1761863083906, "tmdate": 1762915523991, "mdate": 1762915523991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores long-form autoregressive video generation, where accumulated frame-level errors and strict sequential inference cause temporal drift and poor scalability. It introduces Macro-from-Micro Planning (MMPL), a hierarchical “plan-then-populate” scheme that first predicts sparse keyframes for every short segment (Micro) and then chains these plans across segments (Macro), enabling parallel synthesis of intermediate frames while suppressing error propagation. Extensive comparisons on 30-s clips from the VBench-long benchmark and a 29-participant user study show MMPL outperforming open-source baselines (SkyReelsV2, MAGI, CausVid, Self Forcing) in subject consistency, motion smoothness, aesthetic quality, and human preference, while 4-GPU parallel inference cuts wall-clock time to 33 %. The manuscript is clearly structured."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper tackles an important and practical problem—generating long, high-quality videos with autoregressive models—whose limitations in temporal drift and sequential bottlenecks are well analyzed.\n\n- The authors propose an elegant two-level planning framework that decouples long-range dependency modeling from dense frame generation, achieving both consistency and parallelism without architectural surgery.\n\n- Experiments are good, combining automatic metrics, human evaluation, ablations, and efficiency analysis on multi-GPU setups, demonstrating superiority.\n\n- The writing is good, figures are informative."}, "weaknesses": {"value": "My major concern about this paper lies in the rather limited performance improvement demonstrated by the proposed technique. In other words, the claimed state-of-the-art results may largely stem from an unfair comparison. Specifically, the method is trained on an exceptionally strong baseline model—Wan-2.1—yet Table 1 does not include any comparison with Wan-2.1 itself. One possible reason for this omission might be that Wan-2.1 cannot generate videos as long as those presented in this work. However, I believe it is essential to conduct detailed ablation studies on short 5-second videos to demonstrate that the improvements reported in Table 1 indeed originate from the proposed MMPL mechanism, rather than simply from extending a 5-second model to 30 seconds. Another observation reinforcing my concern comes from the relatively weak ablation results in Table 2. For instance, even after removing the modules, the model still achieves SOTA performance on motion smoothness and aesthetic quality in Table 1. Meanwhile, for background consistency and imaging quality, the MMPL approach does not achieve SOTA at all. These results make me question the true effectiveness and generalizability of the proposed technique."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rzdth2Rece", "forum": "nY8looE4lO", "replyto": "nY8looE4lO", "signatures": ["ICLR.cc/2026/Conference/Submission456/Reviewer_k4xv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission456/Reviewer_k4xv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888058601, "cdate": 1761888058601, "tmdate": 1762915523888, "mdate": 1762915523888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper proposes a planning-then-populating framework MMPL. MMPL first creates a global video storyline via two stages: Micro Planning, which predicts keyframes within short segments to guide local generation, and Macro Planning, which links these micro plans autoregressively to ensure long-term consistency. Then, Content Populating generates all intermediate frames in parallel across segments, significantly improving efficiency. Experiments show the method achieves both quality and stability in long-video generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- MMPL proposes a hierarchical autoregressive planning pipeline for long-video generation.\n- MMPL could parallelly synthesizes frames for multiple video segments guided by pre-planned keyframes.\n- MMPL incorporates a workload scheduling strategy to minimize the overhead of the proposed pipeline."}, "weaknesses": {"value": "- **Decoupled planning and generation pipelines.** MMPL introduces a typical divide-and-conquer strategy to schedule the long-video generation task. I consider a key challenge lies in the fidelity of motion modeling during the planning stage when complete frames are unavailable (i.e., MMPL seems not to be an end-to-end training method). The authors should report a dynamic degree metric and compare both subject and camera motion against standard full or causal attention baselines. \n- **Incomplete VBench metrics.** The authors should report all quality, semantic and overall metrics, even though MMPL is a lightweight fine-tuning method built on WanX-2.1 14B. Additionally, I believe the dynamic degree warrants significant comparison, as it typically involves a trade-off between image quality and object motion.\n- **Contrained workload scheduling.** Although MMPL employs a divide-and-conquer strategy to distribute computation tasks, it is inherently multi-GPU friendly. However, on a single GPU, the proposed method requires device kernel–based scheduling, which limits its potential for parallel generation.  Consequently, the authors could further optimize MMPL for single-GPU execution."}, "questions": {"value": "- See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eIQmZ8k967", "forum": "nY8looE4lO", "replyto": "nY8looE4lO", "signatures": ["ICLR.cc/2026/Conference/Submission456/Reviewer_431X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission456/Reviewer_431X"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934739489, "cdate": 1761934739489, "tmdate": 1762915523773, "mdate": 1762915523773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}