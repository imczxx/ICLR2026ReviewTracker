{"id": "T5hD0as3jb", "number": 9977, "cdate": 1758153863989, "mdate": 1759897682286, "content": {"title": "Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models", "abstract": "Vision–language models (VLMs) extend large language models (LLMs) with vision encoders, enabling text generation conditioned on both images and text. However, this multimodal integration expands the attack surface by exposing the model to image-based jailbreaks crafted to induce harmful responses. Existing gradient-based jailbreak methods transfer poorly, as adversarial patterns overfit to a single white-box surrogate and fail to generalise to black-box models. In this work, we propose **U**niversa**l** and **tra**nsferable jail**break** (**UltraBreak**), a framework that constrains adversarial patterns through transformations and regularisation in the vision space, while relaxing textual targets through semantic-based objectives. By defining its loss in the textual embedding space of the target LLM, UltraBreak discovers universal adversarial patterns that generalise across diverse jailbreak objectives. This combination of vision-level regularisation and semantically guided textual supervision mitigates surrogate overfitting and enables strong transferability across both models and attack targets. Extensive experiments show that UltraBreak consistently outperforms prior jailbreak methods. Further analysis reveals why earlier approaches fail to transfer, highlighting that smoothing the loss landscape via semantic objectives is crucial for enabling universal and transferable jailbreaks.", "tldr": "", "keywords": ["Vision-language model", "Jailbreak", "Transferability"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac7739bd9c1c82fb85a461dfd15ed02ab7793f30.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces UltraBreak, an optimization-based attack framework for universal, transferable jailbreaks for VLMs. Unlike previous attacks that overfit models, UltraBreak achieves cross-model transferability and cross-target universality with one adversarial image. Experiments show UltraBreak outperforms prior methods in transferability and universality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The experiments are comprehensive, including open-source and closed-source victim models. Quantitative results show UltraBreak has higher ASR than baselines on AdvBench and SafeBench. Ablation results assess each component's contribution.\n+ The paper is clear, well-structured, with logical flow from motivation to analysis. Mathematical formulations are precise."}, "weaknesses": {"value": "1.\tThe selected baselines are relatively outdated. For instance, typography-based methods, apart from FigStep, should also consider more recent benchmarks such as MM-SafetyBench [1]. Furthermore, when it comes to optimization methods, several advanced approaches like AdvDiffVLM [2], SSA-CWA [3], AnyAttack [4], and M-Attack  [5] have not been taken into account. Incorporating these would provide a more comprehensive evaluation and enhance the robustness of the study.\n2.\tThe paper reveals that UltraBreak is the first to achieve effective universality and transferability against VLMs using a single surrogate model. Of the five open-source models tested, three are from the Qwen family (Qwen-VL-Chat, Qwen2-VL, Qwen2.5-VL). Though the authors say they come from distinct training pipelines, their shared architecture might lead to common vulnerabilities, possibly inflating transferability measurements.\n3.\tThe paper states that when evaluating commercial models like GPT-4.1-nano and Gemini-2.5-flash-lite, the most harmful targets were excluded. This indicates that test conditions varied and were less challenging than those for open-source models. The 32.26% average ASR may not accurately reflect the attack's effectiveness against advanced safety measures.\n4.\tThe paper highlights high ASR, like 71.83% on SafeBench, but lacks analysis of the ~28% failures. Understanding why some attacks fail or succeed, such as the comparison between VAJM and UltraBreak on GLM-4.1V, is essential. The brief mention of dataset toxicity as a reason is speculative. A detailed failure analysis is vital for assessing threats and guiding defenses.\n5.\tThe paper states the benefit of a smoother loss landscape. However, excessive smoothness can lead to a shallow minimum, which might not be optimal for attack purposes. As the temperature parameter τ increases, the landscape becomes smoother, but the model's focus may shift to irrelevant outputs. Thus, a moderate level of smoothness is necessary, and the idea that the smoother the better is flawed, challenging the method's theoretical basis.\n\n[1] MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models\n[2] Efficient generation of targeted and transferable adversarial examples for vision-language models via diffusion models  \n[3] How robust is Google’s Bard to adversarial image attacks?  \n[4] AnyAttack: Towards large-scale self-supervised generation of targeted adversarial examples for vision-language models  \n[5] M-Attack: A Simple Baseline Achieving Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4"}, "questions": {"value": "1. Given that recent methods like M-Attack report success rates over 90%, how does UltraBreak compare to these more advanced and effective baselines?\n2. Considering that much of your open-source evaluation involves the Qwen model family, could the high transferability partly be inflated by similarities in architecture?\n3. Since the evaluation on commercial models excluded the most harmful targets, how can we fairly assess the threat?\n4. How do you reconcile the main claim that a smoother loss landscape is better with your own finding that excessive smoothness hampers the attack's success?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "as3dTPBsYe", "forum": "T5hD0as3jb", "replyto": "T5hD0as3jb", "signatures": ["ICLR.cc/2026/Conference/Submission9977/Reviewer_QY1w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9977/Reviewer_QY1w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761295234927, "cdate": 1761295234927, "tmdate": 1762921414437, "mdate": 1762921414437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new VLM jailbreaking algorithm that tries to develop universal and transferable adversarial attacks on VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strengths\n    - I think the paper is targetting and important problem—the development of universal and transferable jailbreaks on image models. Previous work showed this was challenging.\n    - The method, as explained in the introduction, broadly makes sense to me and is intuitive. I like the shift from a log-likelihood to semantic based loss.\n    - The paper is clear and well written.\n    - The ablation studies show the components make sense and help performance.\n    - I liked the extra analysis in Fig 2, and it made sense."}, "weaknesses": {"value": "- Weaknesses\n    - I think the exposition could be tightened up in places (see questions below).\n    - Ideally I'd love to see a big stronger jailbreak evaluation using something like StrongReject.\n    - Ideally I'd love to see some additional target models, like Claude Sonnet 4.5, GPT-5.\n    - \"UltraBreak consistently outperforms all gradient-based baselines across target models and both test sets. One exception is ...\" Please tone down the writing e.g., with \"tends to outperform\"\n    - It would be nice to add a baseline like Best-of-N jailbreaking to see how well you can do without needing something transferable or universal. You could look at a \"universal and transferable ASR gap\""}, "questions": {"value": "- Questions\n    - Minor: \"Extensive experiments on benchmark datasets demonstrate that UltraBreak surpasses prior gradient-based methods by over 50% on black-box models and unseen targets, establishing strong universality across targets and transferability across models.\" I don't know what the 50% number is or refers to—can you make this more precise?\n    - As I understand, it seems like the approach used is a __token-level__ semantic loss. I can see how this would improve the loss function (many tokens might have similar meanings), but I think a better thing would be a sentence level embedding loss. I think you're trying to approximate this using the cosine similarity to future embeddings and weighted loss, but is that right?\n    - Does the loss function work autoregressively and require model sampling, or do you simply plug in the target completion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mmZZb5YfRt", "forum": "T5hD0as3jb", "replyto": "T5hD0as3jb", "signatures": ["ICLR.cc/2026/Conference/Submission9977/Reviewer_EMMq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9977/Reviewer_EMMq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761353715189, "cdate": 1761353715189, "tmdate": 1762921414094, "mdate": 1762921414094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes specific optimizations for two important properties in VLM attacks—universality and transferability. The authors introduce an adversary attention semantic loss and a total variation loss. The combination of these two modules effectively mitigates the shortcomings observed in previous works, namely the issues of surrogate overfitting and weak transferability.\n\nOverall, the paper presents effective results, and the experimental findings are well-explained. The ablation studies are also thorough and provide clear insights into the contribution of each component.\n\nHowever, since my reading in the area of VLM safety is limited, I am not fully confident in assessing how novel this work is compared to prior research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is logically well-organized, and the motivation is clear.\n\n2. The proposed method achieves strong performance, though the degree of novelty is uncertain."}, "weaknesses": {"value": "1. Figure 1 is hard to follow. I suggest adding a more explicit flow in the caption, or introducing a small algorithm box to walk through the pipeline step by step.\n\n2. There are minor typos—for example, line 199 uses w*t,j; I believe this should be $w_{t,j}$, right?\n\n3. It’s unclear how the method deals with the potentially spiky loss landscape induced by the total variation loss."}, "questions": {"value": "1. Is the plain average an appropriate aggregate? A simple mean can be distorted by one relatively poor result among several strong ones, which could unfairly lower the overall ranking. Have you considered alternatives such as rank-based aggregation before averaging?\n\n2. What is the contribution of Targeted Prompt Guidance to overall performance? Please provide analysis/ablations to quantify its importance and interactions with the two loss modules.\n\n3. Since the method adds two loss terms, what is the computational overhead compared to baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p5BjBgWQwh", "forum": "T5hD0as3jb", "replyto": "T5hD0as3jb", "signatures": ["ICLR.cc/2026/Conference/Submission9977/Reviewer_KaW7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9977/Reviewer_KaW7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761472780642, "cdate": 1761472780642, "tmdate": 1762921413760, "mdate": 1762921413760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces UltraBreak, a method to train universal and transferable image jailbreaks against VLMs (universal meaning they work against different prompts, and transferable meaning they work against different models not used during attack training).\n\nThe method uses gradient optimization to train the attack image. They introduce a number of components to the attack training algorithm to promote transferability:\n1. Semantic adversarial target. Instead of calculating the loss as cross entropy to a specific harmful completion, they use a cosine similarity loss in a custom output embedding space.\n2. Input space constraints (random transformation of attack image in batch, a projection, and a total variation regularization loss).\n\n\nWhile prior works have failed to find image jailbreaks that transfer, UltraBreak has impressive results. Table 1 shows that the attacks transfer very well between models. Table 2 contains useful ablations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "## Originality\n\nThis is the first paper, that I know of, to present a method that can produce image jailbreaks that transfer between models. The idea of constraining the input space is not novel, but the semantic loss (and implementation) is new to me.\n\n## Quality\n\nThe quality of experiments is good. The leave one out ablations also give insight into which components of the algorithm are important. I was initially skeptical about the need for attention in the semantic loss, but the results in Figure 3 were convincing.\n\n## Clarity \n\nOverall the paper is well written and easy to follow.\n\n## Significance\n\nThere have been many works that have shown VLMs are vulnerable to image jailbreaks. With this being said, almost all assume a white-box threat model. Finding attacks that transfer to other models is an important step that shows image jailbreaks to VLMs are a real concern that require specific mitigations when deploying VLMs, even in a black box manner. In this sense, the findings of the paper have reasonable significance."}, "weaknesses": {"value": "I think there should be more focus on transfer to frontier models. The bottom section of table 1 has some good results in this area. The paper would be improved by adding results with more current frontier models. \n\nI think some of the language in the introduction is too strong. For example you state \"We present UltraBreak, the first jailbreak framework to achieve effective cross-target universality and cross-model transferability against VLMs.\" This could be interpreted as meaning no prior work has achieved cross-target transfer, but this is false, for example [1] and [2] achieve this.\n\nIn addition it is worth noting [1] is related but not cited, in particular they also use input space constraints and seem to find similar high level features (Figure 6 bottom).\n\nNit: Line 160 typo \"Figure 1 summaries our approach\" \n\n[1] Bailey, Luke, et al. \"Image hijacks: Adversarial images can control generative models at runtime.\" _arXiv preprint arXiv:2309.00236_ (2023).\n\n[2] Qi, Xiangyu, et al. \"Visual adversarial examples jailbreak aligned large language models.\" _Proceedings of the AAAI conference on artificial intelligence_. Vol. 38. No. 19. 2024."}, "questions": {"value": "1. Can you provide any results on more frontier models, e.g. GPT-5 and Claude 4?\n2. Although the ablation is convincing, can you provide more intuition as to why the attention mechanism is needed in the semantic loss?\n3. What embedding matrix do you use in equation (3)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vjy5x8sWe4", "forum": "T5hD0as3jb", "replyto": "T5hD0as3jb", "signatures": ["ICLR.cc/2026/Conference/Submission9977/Reviewer_EXWG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9977/Reviewer_EXWG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759951109, "cdate": 1761759951109, "tmdate": 1762921413358, "mdate": 1762921413358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}