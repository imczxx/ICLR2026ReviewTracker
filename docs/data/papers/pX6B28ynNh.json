{"id": "pX6B28ynNh", "number": 21849, "cdate": 1758322658624, "mdate": 1759896899968, "content": {"title": "CMT-Benchmark: A Benchmark for Condensed Matter Theory Built by Expert Researchers", "abstract": "Large language models (LLMs) have demonstrated remarkable progress in coding and mathematical problem-solving; however, evaluation on advanced research-level problems in the hard sciences remains scarce. \nTo fill this gap, we present \\cmt, a dataset of 50 original problems covering condensed matter theory (CMT) at the level of an expert researcher. The topics cover analytical and computational approaches commonly used in quantum many-body physics as well as classical statistical mechanics. This dataset was designed and verified by a panel of expert researchers from around the world. \nWe built the dataset through a collaborative environment that challenges the panel to write and refine difficult problems that the panel would like their research assistants to be able to solve, with topics including Hartree-Fock mean-field theory, exact diagonalization, quantum Monte Carlo, density matrix renormalization group, quantum statistical mechanics, classical statistical mechanics, and model building. We evaluate different LLMs by programmatically checking LLM-generated solutions against expert-supplied ground truth. \nFor this, we developed machine-grading mechanisms that are suitable for advanced physics research problems. \nFor example, we handle non-commuting operators that are essential for quantum many-body problems by symbolic manipulation and normal ordering. \nOur evaluations show that frontier models struggle with all of the problems in the dataset, highlighting a gap in the physical reasoning skills of current LLMs. Notably, experts identified strategies for creating increasingly difficult problems by interacting with the LLMs and exploiting common failure modes. \nWhile the highest-performing model, GPT5, correctly solves 30\\% of the problems, average performance across 17 models (GPT, Gemini, Claude, DeepSeek, and Llama classes) is only 11.4$\\pm$2.1\\%.  Moreover, our benchmark contains 18 problems that {\\it not a single one} of the 17 models can correctly solve, and 26 problems that are solved by {\\it at most} one model. \nThese currently unsolvable problems span the fields of Quantum Monte Carlo, Variational Monte Carlo, and Density Matrix Renormalization Group. \nThe answers sometimes violate fundamental symmetries or have unphysical scaling dimensions. We believe that this benchmark set provides valuable guidance for the future development of language models, aiming to achieve the goal of AI research assistants and tutors.", "tldr": "", "keywords": ["large language model", "statistical mechanics", "benchmark", "evaluation", "numerical methods", "scientific problem solving", "condensed matter physics", "quantum physics"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/df2f248c695e8e6c0145ca3604288bda3d55e57b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a new and very difficult benchmark for physics problems in condensed matter theory. It evaluates state-of-the-art LLMs and finds that they cannot solve most of these problems.The problems are categorized into different categories and created by experts from across the world. The benchmark is intended to help develop a research assistant grade AI assistant in this field."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- new benchmark dataset for a field in which data is lacking\n- should enable the development of stronger models for physics problems\n- the problems are checked and created by experts"}, "weaknesses": {"value": "- small dataset: there are only 50 problems because they are manually created by experts\n- no other weaknesses to be found"}, "questions": {"value": "- do the problems have different difficulty levels or are they all at approximately the same level?\n- do the problems generalize well, is an LLM that is able to solve these problems expected to generally be good at solving physics problems?  - How do you assess the coverage of these 50 problems, are there redundancies or gaps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LJklbOlL2F", "forum": "pX6B28ynNh", "replyto": "pX6B28ynNh", "signatures": ["ICLR.cc/2026/Conference/Submission21849/Reviewer_VTk8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21849/Reviewer_VTk8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748324733, "cdate": 1761748324733, "tmdate": 1762941956385, "mdate": 1762941956385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CMT-Benchmark, a dataset of 50 expert-level problems in condensed matter theory (CMT). The benchmark was built by an international panel of expert researchers at the level expected of strong grad students. The authors then built automated evaluation infrastructure, including a novel parser that can handle non-commutative operator expressions in CMT. The paper found that all frontier models have uniformly low performance in the benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. I really like the creation of the benchmark process. By asking the human experts from different countries to submit the questions, i think this benchmark truly captures what it means to be an expert in CMT. Thus, it will be more convincing to believe that the progress in this benchmark will imply the progress in CMT research. I think this is a significant contribution to the community. \n\n2. The automated parsing and grading system is carefully-designed and quite impressive, particularly the handling of non-commutative operator algebra through symbolic manipulation and normal ordering.\n\n3. The four detailed case studies (Sections A.1-A.4) provide valuable insights into specific LLM limitations: language-geometry gaps, over-reliance on textbook heuristics, failure to apply fundamental principles, and weak spatial reasoning."}, "weaknesses": {"value": "1. The paper only tests LLM capabilities without access to tools like web search, code executions or symbolic/numerical computation packages. However,  we know that even human research assistants don't work in isolation without any tool use. It would be interesting to test whether tool-augmented agents improve performance.\n\n2. Requiring answers in boxed LaTeX environments and prohibiting new variables may artificially hurt model performance. The authors note some models (particularly Gemini 2.5 Pro) \"occasionally disregard the formatting instructions,\" leading to parsing failures. How much does the strict format requirement degrade performance compared to free-form responses that could be human-evaluated on a subset?\n\n3. While the authors claim problems are original, there's no systematic verification that similar problems (or solution strategies) don't appear in training data or online.\n\n4. With only 50 problems and some categories having very few examples (PEPS has 3, VMC has 2), per-category performance can have large uncertainty."}, "questions": {"value": "1. Can you design experiments to show that how much of the poor performance is due to formatting constraints versus actual physics reasoning failures?\n\n2. Can tool-augmented agents substantially improve performance on this benchmark?\n\n3. Is there evidence of training data contamination for any problems? How to design the experiments to test this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R3A61Z3W2f", "forum": "pX6B28ynNh", "replyto": "pX6B28ynNh", "signatures": ["ICLR.cc/2026/Conference/Submission21849/Reviewer_qany"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21849/Reviewer_qany"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949762967, "cdate": 1761949762967, "tmdate": 1762941956003, "mdate": 1762941956003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CMT-Benchmark, a dataset of 50 problems and expert-annotated answers in condensed matter theory. A variety of large language models are evaluated on this benchmark dataset, and they show a high gap to a satisfying physical reasoning skills."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Create a very high-quality dataset in condensed matter domain. This dataset could potentially be very useful in evaluating LLM's capability in solving scientific research problems.\n- Provide comprehensive benchmarking of multiple LLMs including GPT, Claude, DeepSeek and Llama family models.\n- The writing of this paper is good, providing details in data curation process and necessary background knowledge."}, "weaknesses": {"value": "- To improve the quality of experiments, authors are encouraged to analyze the failure cases, demonstrating LLM models consistently fails on which types of problems and makes which types of mistakes.\n- As the benchmark is highly domain-specific, authors are encouraged to also evaluate the performance of web search agents (e.g., OpenAI & Tongyi DeepResearch) to show if LLM models could solve the problem through searching public Internet knowledge."}, "questions": {"value": "Though it is released close to the ICLR submission date, I am interested in the comparison of CMT-Benchmark to another public benchmark CMPhysBench [1], which is also used to evaluate LLM in condensed matter physics. What are the major differences between them in the collected problems and answer annotation process?\n\n[1] Wang, Weida, et al. \"CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics.\" arXiv preprint arXiv:2508.18124 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ozt8kfPQ26", "forum": "pX6B28ynNh", "replyto": "pX6B28ynNh", "signatures": ["ICLR.cc/2026/Conference/Submission21849/Reviewer_u4aj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21849/Reviewer_u4aj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967737409, "cdate": 1761967737409, "tmdate": 1762941955589, "mdate": 1762941955589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper curated a benchmark including 50 problems on condensed matter that are generated by domain experts. The benchmark covers a wide range of topics in the field and requires high levels of understanding and expertise to solve the problems. The LLMs perform badly on the benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The curated benchmark involves experts inputs and is carefully designed and evaluated.\n\n2. The benchmark covers a wide range of topics in the condensed matter field."}, "weaknesses": {"value": "1. The models evaluated are all general purpose LLMs that have not been fine tuned in the field of condensed matter. Therefore, they are unlikely to perform well on this challenging benchmark. The authors may want to fine tune a model on relevant tasks and then evaluate it on the benchmark to see if performance can be improved.\n\n2. The benchmark only includes question and answer, without reasoning process. For such complicated tasks, few-shot prompts and a CoT guide may improve the performance. I think this would be valuable to test."}, "questions": {"value": "1. Have the authors tried to fine tune some LLMs to learn to solve problems as challenging as the benchmark?\n\n2. Would it possible to include reasoning process into the benchmark that can serve as the prompt or a reasoning benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y25cFAF07T", "forum": "pX6B28ynNh", "replyto": "pX6B28ynNh", "signatures": ["ICLR.cc/2026/Conference/Submission21849/Reviewer_Fftv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21849/Reviewer_Fftv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979773984, "cdate": 1761979773984, "tmdate": 1762941955260, "mdate": 1762941955260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}