{"id": "ji8OFKRYwX", "number": 5814, "cdate": 1757937055774, "mdate": 1759897951901, "content": {"title": "Towards Efficient Chain-of-Thought Reasoning via Adaptive-Budgeting based Policy Optimization", "abstract": "Recently, Chain-of-Thought (CoT) reasoning has become a key problem-solving\ncapability for advanced large language models (LLMs) to address difficult tasks\nsuch as the mathematical ones. However, balancing the efficiency and performance of long CoTs still remains an intractable challenge. In this paper, we observe that assigning adaptive token budgets for different examples during training\nis an viable way to tackle with the above issue. Motivated by this, we propose a novel reinforcement learning scheme, termed Adaptive-Budgeting based\nPolicy Optimization (ABPO). Based on the popular GRPO, our ABPO redefines\nthe RL training as an adaptive curriculum learning process, where example pools\nare curated to categorize training examples into three types, namely the mastered,\nlearning and hard ones, respectively. As the training progresses, ABPO will adaptively schedule the examples with proper length budgets, and the example pools\nwill alse be dynamically updated based on the model status. In this way, we can\nassign adaptive token lengths for different examples during RL training, achieving\na good balance between efficiency and performance of CoTs. To validate ABPO,\nwe apply it to three representative LLMs, and conduct extensive experiments on\na bunch of CoT reasoning benchmarks. The experimental results not only show\nthe substantial efficiency improvements with minimal performance loss, e.g., reducing token length by 78.3% while improving 2.0% performance of DeepSeek-R1-Distill-Qwen-1.5B on average, but also show our obvious advantages over\nthe compared methods, e.g., reducing 59.4% length and increasing 8.3% performance on average than HAPO, respectively. Our code is anonymously released at https://anonymous.4open.science/r/AnonymizeABPO-5380/", "tldr": "", "keywords": ["Large Language Models", "Chain of Thought", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/12a22335c2f04d7f140d837aa15f9dcff59856ea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ABPO (Adaptive-Budgeting based Policy Optimization), a reinforcement learning framework to improve efficiency in chain-of-thought (CoT) reasoning for large language models (LLMs). The core idea is to dynamically allocate token budgets to training samples according to model performance and sample difficulty. ABPO extends Group Relative Policy Optimization (GRPO) by maintaining three example pools (mastered, learning, and hard) and adaptively updating token budgets during training. Experiments on four mathematical reasoning benchmarks (MATH500, AMC, AIME, OlympiadBench) and multiple base models (DeepSeek-R1-Distill-Qwen 1.5B/7B, DeepScaleR-1.5B) demonstrate that ABPO can reduce CoT length by over 70% while maintaining or slightly improving accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Comprehensive experimental design:**  \n  The paper includes diverse models and datasets, as well as ablation studies on curriculum learning, adaptive budgeting, and review mechanisms.  \n- **Clear motivation:**  \n  It addresses a real inefficiency issue (“over-thinking”) observed in long-CoT reasoning LLMs.  \n- **Insightful analysis and visualization:**  \n  The visualizations clearly show that ABPO dynamically adjusts reasoning length based on task difficulty.  \n- **Empirical consistency:**  \n  Across all settings, ABPO achieves substantial efficiency gains (up to −78% CoT length) while maintaining comparable or slightly higher accuracy."}, "weaknesses": {"value": "- **Limited novelty:**  \n  The idea of adaptive length control resembles prior works (e.g., HAPO, L1-Max) and mainly builds upon existing RL frameworks (GRPO).  \n- **Modest gains:**  \n  Although CoT length reduction is impressive, the improvement in accuracy is small (≈ +1–2%).  \n- **Methodological complexity:**  \n  The multi-pool adaptive scheme introduces additional hyperparameters and scheduling logic, which may complicate implementation compared to simpler fixed- or curriculum-budget baselines.  \n- **Motivation ambiguity:**  \n  It remains unclear whether optimizing CoT length should be a primary training objective, or if this efficiency truly translates to better general reasoning ability."}, "questions": {"value": "1. Does the difficulty categorization (mastered / learning / hard) require a **pre-evaluation step before training**, or is it performed online during training?   \n2. How robust is ABPO across random seeds and datasets, given that adaptive updates depend on the model’s current accuracy estimates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NRNzeB8M4t", "forum": "ji8OFKRYwX", "replyto": "ji8OFKRYwX", "signatures": ["ICLR.cc/2026/Conference/Submission5814/Reviewer_idTW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5814/Reviewer_idTW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761437068959, "cdate": 1761437068959, "tmdate": 1762918278050, "mdate": 1762918278050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies efficient Chain-of-Thought (CoT) distillation, aiming to reduce the training cost and improve reasoning capability of smaller models distilled from large reasoning models. Instead of focusing on architecture or optimization, the authors take a data-centric perspective—investigating how to select high-quality reasoning data for CoT distillation.\n\nThey introduce a data selection framework that prioritizes training samples with diverse reasoning paths and useful intermediate steps, based on the concept of step utility (how much each reasoning step contributes to the final correct answer). Through systematic experiments across arithmetic reasoning, commonsense reasoning, and instruction datasets, they show that carefully selected reasoning samples significantly outperform random or length-based sampling for the same training budget."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Novel data-selection view of CoT distillation: Previous works often focus on model-side compression or prompt engineering, while this paper reframes the problem from a data selection perspective, emphasizing “which reasoning traces to distill” rather than “how to distill.”\n\n* Well-designed methodology: The introduction of step utility and diversity-based filtering provides a concrete and interpretable criterion for sample selection, improving both efficiency and generalization.\n\n* Comprehensive experiments: Multiple datasets (GSM8K, SVAMP, CSQA, StrategyQA) and multiple student sizes are tested. The paper also includes ablation studies that validate the effect of each selection criterion."}, "weaknesses": {"value": "* Step-utility estimation overhead: Computing step-level utility still requires running reasoning traces and reward evaluation, which may offset the claimed training efficiency in large-scale settings.\n\n* Limited evaluation on complex reasoning: Most benchmarks are short-form arithmetic or commonsense reasoning; the approach’s scalability to long CoT tasks (e.g., MATH, GPQA) or multimodal reasoning remains untested.\n\n* Ablation depth and interpretability: The ablation studies show performance differences but lack deeper analysis of why certain data characteristics (e.g., step diversity vs. accuracy) dominate the improvements."}, "questions": {"value": "See the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sWDrggOdoK", "forum": "ji8OFKRYwX", "replyto": "ji8OFKRYwX", "signatures": ["ICLR.cc/2026/Conference/Submission5814/Reviewer_meC4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5814/Reviewer_meC4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655840477, "cdate": 1761655840477, "tmdate": 1762918277771, "mdate": 1762918277771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the efficiency–performance trade-off in chain-of-thought (CoT) reasoning for large language models (LLMs). The authors propose Adaptive-Budgeting based Policy Optimization (ABPO), an extension of GRPO that adaptively assigns token budgets to training samples according to their difficulty and the model’s learning progress. The approach divides training data into mastered, learning, and hard pools, dynamically updating these categories and their corresponding length budgets. Experiments on multiple LLMs (DeepSeek-R1-Distill-Qwen-1.5B/7B, DeepScaleR-1.5B-Preview) and reasoning benchmarks (MATH500, AMC, AIME, OlympiadBench) show significant reductions in CoT length with minimal or no loss in accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Targets an important and underexplored problem: balancing reasoning efficiency with accuracy in LLMs.\n\n* Proposes a simple and implementable mechanism that achieves strong empirical gains.\n\n* Comprehensive experimental evaluation across multiple model scales and benchmarks.\n\n* Qualitative examples effectively illustrate adaptive behavior."}, "weaknesses": {"value": "* The adaptive budgeting rule is heuristic—thresholds and increments are hand-picked, and the categorization process (mastered/learning/hard) lacks theoretical grounding.\n\n* No formal justification for why this curriculum structure leads to optimal or stable policy improvement.\n\n* Limited ablation on key hyperparameters (τ₀, τ₁, d); unclear robustness.\n\n* The approach does not fundamentally modify GRPO—it primarily wraps a training schedule around it. It seems to be ineffective if we do not revisit the same question again?"}, "questions": {"value": "1. How sensitive is ABPO to the choice of τ₀, τ₁, and d? Could these hyperparameters be learned or adapted automatically?\n\n2. How does ABPO behave when the pool thresholds are poorly chosen—does training diverge or stagnate?\n\n3. I recently came across this paper Optimizing Anytime Reasoning via Budget Relative Policy Optimization (https://openreview.net/pdf/8136e4668a09f8c47a2454d9e72728d4fdea055e.pdf), which shares very similar motivation and method. Instead of dynamically setting budget, this paper uses the dense reward framework to let the model learn the optimal budget by itself. It would be nice if you can discuss your differences and how your method is fundamentally better than the baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mG9wDx7ve9", "forum": "ji8OFKRYwX", "replyto": "ji8OFKRYwX", "signatures": ["ICLR.cc/2026/Conference/Submission5814/Reviewer_Yb4b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5814/Reviewer_Yb4b"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888495194, "cdate": 1761888495194, "tmdate": 1762918277493, "mdate": 1762918277493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ABPO, a reinforcement-learning framework that adapts the token budget per example to balance CoT efficiency (shorter traces) and accuracy. It builds on GRPO by (i) adding a budget-aware reward, and (ii) organizing data into dynamically updated mastered/learning/hard pools with curriculum-style scheduling and periodic review. Experiments across multiple math benchmarks and three base models show improved length–accuracy trade-offs relative to fixed-budget."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is generally well-written and structured, followed by the motivation and explanation.\n2. The separation of the token allocation decision from the generation process is a clean and novel architectural contribution.\n3. The experimental results validate the central hypothesis: the model learns to assign smaller budgets to easier problems and reserves larger budgets for more difficult ones."}, "weaknesses": {"value": "1. The reward design is based on a hard cutoff binary scheme, which does not distinguish between reasoning traces that are slightly longer than the target and those that are significantly longer. I suggest incorporating a soft budget penalty, similar to the L1-Max penalty, to encourage smoother control over reasoning length.\n2. The paper introduces a number of hyperparameters( $\\tau_0$, $\\tau_1$, $\\lambda$, $\\alpha$, $d$, and $t_0$) without providing evidence of tuning.\n3. The authors evaluate with $K = 3, 5, 10$ rollouts. However, it is common practice to perform 16 or 32 rollouts for a fair evaluation and to compute pass@1 scores. Using only 10 rollouts may not be sufficient.\n4. The proposed method only outperforms the DeepSeek-1.5B model in Table 1, but performs worse than the base DeepSeek-7B and DeepScaleR-1.5B models.\n5. In Table 2, the authors compare only against DeepSeek-1.5B, while DeepScaleR exhibits stronger baseline performance. I recommend evaluating the method based on DeepScaleR, and for fairness, performing fixed-budget comparisons for both ABPO and DeepScaleR—for example, by applying early stopping to DeepScaleR under equivalent token budgets.\n6. In Figure 3, presenting only a single point is insufficient for fair comparison. To better demonstrate performance–efficiency trade-offs, the authors should ensure similar token usage between ABPO and L1-Max, and include results for multiple token budgets (e.g., 512, 1024, 2048 tokens)."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UaDNtXcbpa", "forum": "ji8OFKRYwX", "replyto": "ji8OFKRYwX", "signatures": ["ICLR.cc/2026/Conference/Submission5814/Reviewer_q95z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5814/Reviewer_q95z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762198930891, "cdate": 1762198930891, "tmdate": 1762918277216, "mdate": 1762918277216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}