{"id": "OJh7oBCYhL", "number": 7282, "cdate": 1758014014953, "mdate": 1759897862160, "content": {"title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context", "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision–Language–Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively.\nIn this work, we introduce *cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands.* To address this new setting, we present **RoboOmni**, a *Perceiver-Thinker-Talker-Executor* framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. \nTo address the absence of training data for proactive intention recognition in robotic manipulation, we build **OmniAction**, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance. All datasets, code, and real-world demonstration videos will be released publicly.", "tldr": "", "keywords": ["Speech", "Robotic Manipulation", "Omni-Modal LLMs", "Proactive Intention Recognition"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1c1c438f66a3eeea602a15880b13cd19cce9355.pdf", "supplementary_material": "/attachment/1101393b4f41ceca5240f79e4ee86581b3f1298a.zip"}, "replies": [{"content": {"summary": {"value": "The paper defines a new setting—cross-modal contextual instructions—where robots infer and confirm user intent from speech prosody, environmental sounds, and vision rather than explicit text, and introduces RoboOmni, a unified Perceiver–Thinker–Talker–Executor model that ingests audio+vision and outputs both dialogue and actions via discrete action tokens (FAST+) in a single autoregressive stream. To supply training data, this paper synthesize OmniAction (≈140k episodes; 5k+ speakers; 2.4k sound events; 640 backgrounds) by GPT-4o–based dialogue rewriting plus multi-speaker TTS with timbre cloning and mixed sound events/backgrounds. They report large gains on a LIBERO-based synthetic-audio benchmark (85.6% vs 25.9% best baseline) and better performance on a small real-speech test with direct audio instructions (76.6% vs 73.8 for π0), along with lower per-inference latency than ASR→VLA cascades; pretraining on OmniAction accelerates SFT."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Timely problem framing. Positioning “contextual instructions” (latent intent from audio+vision, incl. paralinguistics) fills a realistic gap in current VLA assumptions about explicit commands. \n\n- Dataset contribution. OmniAction is large and richly varied across speakers, events, and backgrounds; the pipeline is described with reasonable detail (filtering, GPT-4o rewriting/validation, TTS, overlap mixing, SNR control). \n\n- Strong performance. Big margins on OmniAction-LIBERO-TTS (overall 85.6% vs 25.9 best text/ASR baseline) and competitive performance on real-speech direct commands; end-to-end modeling halves inference latency vs ASR cascades."}, "weaknesses": {"value": "- Synthetic dominates the “new setting.” The core results for contextual instructions are on TTS-synthesized audio and GPT-generated dialogues; the only real-speech evaluation uses direct instructions (not contextual), so the key claim—proactive intent inference from real, messy context—remains under-validated in live conditions. \n\n- Engineering integration over algorithmic novelty. The method mainly composes existing ingredients (Qwen2.5-Omni style encoders, autoregressive LLM “Thinker”, FAST+ action tokens) with a unified likelihood; there’s little new learning principle or analysis specific to proactive intent estimation or uncertainty handling. \n\n- Limited ablations on why it works. We see per-type breakdowns and a pretraining vs from-scratch SFT curve, but no modality ablations (e.g., remove environmental sounds, prosody, or vision), no leave-one-out fusion studies, and no tight control to disentangle dataset scale vs architecture as the main driver. \n\n- External dependence and compute. The pipeline leans on GPT-4o for scripting/validation and large-scale TTS/voice cloning.\n\n- Evaluation scope and safety. Real-robot demos are qualitative and small-scale; there’s no user study on over-eagerness, mis-attribution of speaker intent, or false-positive “proactive” actions, despite identity/overlap being central to the setting."}, "questions": {"value": "- Can you report real-speech, contextual-instruction results (not just direct commands) with humans, including failure/over-intervention rates? \n\n- Which modality drives gains? Please add leave-one-modality-out and prosody/identity ablations to separate benefits of paralinguistics vs vision vs text. \n\n- How sensitive is performance to the FAST+ tokenizer vs continuous action heads, and to chunk length? Any degradation in dexterous or contact-rich tasks? \n\n- What is the wall-clock training and energy cost compared to ASR→VLA cascades tuned for similar accuracy? \n\n- How do you mitigate biases/artifacts from GPT-4o/TTS synthesis (e.g., sentiment exaggeration, timbre leakage) when transferring to real audio? Any domain-gap diagnostics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qD2kRDW4NZ", "forum": "OJh7oBCYhL", "replyto": "OJh7oBCYhL", "signatures": ["ICLR.cc/2026/Conference/Submission7282/Reviewer_2Ge8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7282/Reviewer_2Ge8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740651872, "cdate": 1761740651872, "tmdate": 1762919407306, "mdate": 1762919407306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets proactive intention estimation, action planning, and execution for embodied agents in realistic human-robot interaction. It introduces a large-scale synthetic multimodal dataset built on Open-X with off-the-shelf text and audio models, and proposes RoboOmni, a unified framework that combines omni-modal perception with low-level action generation. Experiments suggest strong performance on proactive intention recognition and end-to-end planning/execution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The scenarios are well-chosen, practically relevant, and aligned with next-generation robots operating in realistic human-robot interaction settings.\n- The constructed dataset is likely to be valuable for the community, providing broad multimodal supervision that can stimulate research on proactive robotic behavior."}, "weaknesses": {"value": "- Neither the dataset’s realism/relevance nor the model’s performance appears to be evaluated by humans. This limits claims about practical usability and interaction quality. While Section 5.3 covers direct human audio instructions, this differs from the complex multimodal contexts central to the paper’s claims. \n- The dataset may inherit biases from off-the-shelf models used during curation. This paper does not analyze or mitigate such biases. \n- The setup, tuning, and fairness of baseline comparisons are insufficiently described, making it hard to interpret performance gaps. Please see the Question section for details."}, "questions": {"value": "- Please elaborate on the procedure for “removing trivial samples with low-information visual states.” How do you quantify or detect low information? What thresholds and ablation evidence support the chosen criteria?\n\n- Does the model feed its own intermediate outputs back into the pipeline? If so, are those outputs fed back as text or as audio?\n\n- Were baselines in Table 1 tuned on RoboAction or a simplified variant (e.g., audio converted to text)? If not, low performance may reflect distributional mismatch rather than model limitations. Similarly, were the planner and VLA components of the baselines in Figure 8 tuned for these tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aOYbreM9u2", "forum": "OJh7oBCYhL", "replyto": "OJh7oBCYhL", "signatures": ["ICLR.cc/2026/Conference/Submission7282/Reviewer_xDAu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7282/Reviewer_xDAu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836933639, "cdate": 1761836933639, "tmdate": 1762919406906, "mdate": 1762919406906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript proposes a multimodal fusion framework that integrates text, speech/dialogue, vision, and environmental sounds to support robotic reasoning and decision-making. Compared with using text as the sole communication bridge, the unified end-to-end approach achieves stronger performance with lower control latency, supported by both simulated and real-world experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Clear writing and well-structured presentation.\n\n- Promised release of a robotic dataset, extensive experiments across diverse tasks, and large-scale training.\n\n- An “omni” framework for cross-modal contextual instructions that explicitly includes environmental event/background sounds; comparisons to ASR-based pipelines highlight the benefits of an end-to-end model that can handle overlapping speech.\n\n- Strong empirical results, with ≈60% average success rates exceeding other baselines."}, "weaknesses": {"value": "- Limited discussion of the method and training for fusing multimodal sensory inputs.\n\n- Ablations on modality contributions are missing: it remains unclear how much each cue (prosody/identity, non-verbal audio, vision) contributes. Please add drop-modality ablations (e.g., audio-w/o-prosody, no non-verbal, no vision) and alignment-window studies; current results separate instruction types but not modalities within the architecture.\n\n- Task descriptions are brief; it is hard to understand the challenges without consulting references.\n\n- Baseline fairness: many baselines rely on ASR (Whisper) or ground-truth text, which can be disadvantaged when paralinguistic information matters. One can retain a language-based architecture but add a separate “translation module” to detect and convert such information [1] into prompt context (e.g., simple classifiers for emotion or dialogue overlap, then embed tokens like “[overlapped_dialogue] [disappointed]”). This increases complexity but is a straightforward way to extend text-based models to richer context.\n\n- Minor: the related-work section could be expanded for a more comprehensive background; prior work [2, 3] also explores multimodal decision-making including environmental sound.\n\nReferences\n\n[1] Yamakawa, N., Takahashi, T., Kitahara, T., Ogata, T., & Okuno, H. G. (2011, June). Environmental sound recognition for robot audition using matching-pursuit. In International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems (pp. 1-10). Berlin, Heidelberg: Springer Berlin Heidelberg. \n\n[2] Zhao, X., Li, M., Weber, C., Hafez, M. B., & Wermter, S. (2023, October). Chat with the environment: Interactive multimodal perception using large language models. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 3590-3596). IEEE. \n\n[3] Liu, Z., Chi, C., Cousineau, E., Kuppuswamy, N., Burchfiel, B., & Song, S. (2024). Maniwav: Learning robot manipulation from in-the-wild audio-visual data. arXiv preprint arXiv:2406.19464."}, "questions": {"value": "- Following W1 and W2: environmental event sounds can be sparse compared to speech or vision—will the model ignore this feature during fusion? How is this addressed and evaluated? Also, how is synchronization among modalities handled? What is the performance change if environmental sounds are removed?\n\n- The experiments show large gains over baselines. Could the authors provide insights into why those baselines fail and why RoboOmni succeeds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wb4aesEHd7", "forum": "OJh7oBCYhL", "replyto": "OJh7oBCYhL", "signatures": ["ICLR.cc/2026/Conference/Submission7282/Reviewer_GVRu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7282/Reviewer_GVRu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970674430, "cdate": 1761970674430, "tmdate": 1762919406526, "mdate": 1762919406526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RoboOmni, a multimodal large language model framework that infers robot intentions from speech, sounds, and visual cues instead of explicit commands. It introduces a new setting called cross-modal contextual instructions and builds a large dataset, OmniAction, for training. Experiments show RoboOmni outperforms baselines in success rate, speed, and intention recognition in both simulation and real-world tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces a novel and practical setting, cross-modal contextual instructions, reflecting more natural human-robot interaction without explicit commands.\n\n2. Proposes RoboOmni, a unified framework that integrates intention recognition, interaction confirmation, and action execution using multimodal large language models.\n\n3. Builds a large, diverse dataset (OmniAction) with rich multimodal signals to support training and evaluation.\n\n4. Demonstrates strong empirical performance, outperforming text- and ASR-based baselines in multiple metrics across simulation and real-world environments."}, "weaknesses": {"value": "1. Generalization Beyond Scripted Contextual Cues: A primary concern is the potential for the model to overfit to the specific structures of the six contextual instruction types synthesized for the OmniAction dataset. Since the dataset was generated by prompting GPT-4o to convert atomic instructions into structured dialogues, the model may be learning to recognize these semi-scripted patterns rather than developing a more general, robust capability for open-world intent inference. The impressive performance might not fully transfer to the messiness of real, unscripted human interactions that do not conform to these six categories.\n\n2. Rigidity of the \"Infer-Confirm-Act\" Protocol: The paper frames the proactive confirmation loop as a key feature. However, this rigid protocol may not always be optimal. A truly intelligent agent should be able to modulate its interaction strategy based on its confidence in its own inference. In unambiguous cases, directly executing the inferred task would be more efficient and fluid. The current framework appears to lack this dynamic capability, which limits its social adaptability.\n\n3. Simplification of the Embodied Action Space: The work's main novelty is in perception and reasoning, while the \"Executor\" component relies on a standard 7-DoF action representation. The evaluation of success is task-level, which may obscure nuances in the quality of physical execution (e.g., smoothness, safety, precision)."}, "questions": {"value": "1. Could you clarify if the Talker and Executor modules are designed to operate sequentially or if they can be interleaved? Can the model generate a sequence that combines speech and action, for instance, to provide narrative feedback during execution (e.g., \"Okay, I am now picking up the red cup... a bit heavy... and placing it on the table.\")? If not, how do you see this capability being integrated in future work?\n\n2. The use of GPT-4o for dialogue generation is clever and scalable. However, could you discuss potential linguistic artifacts or biases that this synthetic approach might introduce? For instance, did you observe any repetitive conversational structures or phrasings from the LLM that might not be representative of authentic human speech, and how might such artifacts affect the model's real-world performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u0kIKxYciA", "forum": "OJh7oBCYhL", "replyto": "OJh7oBCYhL", "signatures": ["ICLR.cc/2026/Conference/Submission7282/Reviewer_1JVX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7282/Reviewer_1JVX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762889766203, "cdate": 1762889766203, "tmdate": 1762919406155, "mdate": 1762919406155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}