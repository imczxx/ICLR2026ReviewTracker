{"id": "xao0xuDoK0", "number": 2099, "cdate": 1756988857372, "mdate": 1759898169679, "content": {"title": "Accuracy Law for the Future of Deep Time Series Forecasting", "abstract": "Deep time series forecasting has emerged as a booming direction in recent years. Despite the exponential growth of community interests, researchers are sometimes confused about the direction of their efforts due to minor improvements on standard benchmarks. In this paper, we notice that, unlike image recognition, whose well-acknowledged and realizable goal is 100\\% accuracy, time series forecasting inherently faces a non-zero error lower bound due to its partially observable and uncertain nature. To pinpoint the research objective and release researchers from saturated tasks, this paper focuses on a fundamental question: how to estimate the performance upper bound of deep time series forecasting. Going beyond classical series-wise predictability metrics, e.g., ADF test, we realize that the forecasting performance is highly related to window-wise properties because of the sequence-to-sequence forecasting paradigm of deep time series models. Based on rigorous statistical tests of over 2,800 newly trained deep forecasters, we discover a significant exponential relationship between the minimum forecasting error of deep models and the complexity of window-wise series patterns, which is termed the accuracy law. The proposed accuracy law successfully guides us to identify saturated tasks from widely used benchmarks and derives an effective training strategy for large time series models, offering valuable insights for future research.", "tldr": "This paper discovers the accuracy law, which describes inherent relationship between time series complexity and best forecast performance achieved by SOTA models.", "keywords": ["deep learning", "machine learning", "time series"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e00993e4e387f6c9d77749dc12ac37e7943af8d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reveals the Accuracy Law in the field of time series forecasting through extensive experiments, offering an empirically grounded and insightful principle with practical significance for the community."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The manuscript is well-written and effectively conveys its core ideas, enabling readers to clearly follow the authors’ reasoning. Moreover, the proposed predictability metric for time series is thoughtfully and ingeniously designed. Although the formulation is heuristic in nature, its validity is robustly supported by comprehensive experimental evidence presented in the paper."}, "weaknesses": {"value": "**Clarity of Experimental Setup:**  \n\nThe description of the experimental setup lacks sufficient detail in several key aspects:  \n\n1. The train/validation/test split ratios for the datasets are not disclosed, making it difficult to assess reproducibility and fairness of the evaluation.  \n\n2. It remains unclear whether the proposed *complexity* metric is computed on the test set alone or across the entire dataset. Regardless of the choice, a more thorough discussion of data partitioning is warranted. For instance, certain time series may exhibit high predictability in the training segment but low predictability in the validation or test segments—a scenario that could hinder model convergence and degrade test performance. The absence of such analysis weakens the practical utility and interpretability of the proposed *Accuracy Law*. \n \n3. In Figure 5 (right), the term *Forecastability* is used without a formal definition or reference to its computation, which compromises clarity.\n\n**Limited Generalizability of the Accuracy Law:**  \n\n1. While the authors present an impressive and commendable effort in formulating the **Accuracy Law**, its generalizability is constrained by the dependence of the complexity measure—and consequently the scaling exponent α—on the specific input–output window configuration. Given the prohibitive cost of conducting exhaustive experiments across all possible window settings, it is unreasonable to expect the research community to independently re-estimate α for every new forecasting horizon. At a minimum, the paper should provide pre-computed α values for commonly used input–output window pairs (e.g., 96→{96, 192, 336, 720} and 512→{96, 192, 336, 720}). Alternatively, the authors should propose a lightweight or analytical approximation method to estimate α efficiently, thereby enhancing the law’s practical applicability.\n\n   In practice, recent time series models (e.g., xPatch, TimeMixer++) have achieved performance gains primarily on longer forecasting horizons such as 192, not on the 96→96 setting. Providing α only for 96→96 merely confirms a well-known fact, which—despite the paper’s good intent—limits its contribution.\n\n2. For Section 4.3, Practice 2: Guiding Large Time Series Models, the base model’s prediction task is not inherently a 96-to-96 forecasting setup. When the input sequence is sufficiently long, restricting the input length to only 96 time steps becomes inappropriate, leading to two evident issues:\n\n   1. Since the current accuracy law is exclusively defined for the 96-to-96 setting, its utility in guiding the construction of benchmarks (Usage 1) is severely limited. In the context of the present work, benchmark design should, at a minimum, not impose arbitrary constraints on input length—examples such as GIFT-Eval and the FEV Leaderboard underscore this need.\n\n   2. Similarly, the applicability of this accuracy law to training strategy design (Usage 2) is also limited, as the actual training samples may not conform to a fixed 96-to-96 format. Moreover, the experimental setup described here lacks sufficient clarity; a detailed explanation—particularly regarding the choice of training sequence lengths—should be provided in the appendix, accompanied by further discussion on how varying input lengths impact model training."}, "questions": {"value": "1. Please elaborate on the experimental setup and provide further discussion regarding dataset splitting and its complexity.\n\n2. Include commonly used α values or a quick method for estimating α.\n\n3. Provide further explanatory notes on the two usages in Practice 2.\n\nSpecific rationale is provided in the \"Weaknesses\" section.\n\nI am willing to adjust my score based on the authors' response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b2etDu0Fbi", "forum": "xao0xuDoK0", "replyto": "xao0xuDoK0", "signatures": ["ICLR.cc/2026/Conference/Submission2099/Reviewer_DZKi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2099/Reviewer_DZKi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747162237, "cdate": 1761747162237, "tmdate": 1762916022660, "mdate": 1762916022660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, observing less and less improvement made by recent work on time series forecasting tasks, propose a question: \"How sature is time series forecasting task? What's the upper bound?\" To study this, authors conceptually rely on the \"internal inherity\" of time series, and define the accuracy law of time series forecasting, providing insights for the community about how and what direction we should move on."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The most significant pro I consider is the question proposed. Though recently there has been talks and papers on why the time series community has gone wrong, for example we have seen questions on foundation models that lack context[1], we have seen papers pointing out that hyperparameters are very sensitive for time series forecasting[2], etc. However, I do think this paper proposes an important question: the \"upper bound\" reachable for the time series forecasting datasets perhaps has been reached, and further modification of model architecture targeting these datasets might not be that meaningful. Especially they show, for example, ETT datasets, Electricity, Weather and Exchange-Rate has been saturated.\n2. The proposed accuracy law, based on the \"pattern complexity\" of windows, makes sense on theory and shows reasonable forecasting correlation coefficient on benchmarks."}, "weaknesses": {"value": "1. The authors mainly focus on univariate forecasting with only time series contexts, without other contexts like [1] mentioned.\n2. I think the small progress in the recent 2-3 years made on time series forecasting benchmarks can already provide the insight that these benchmarks are not reliable. Of-course it would be better for someone like the authors to provide in-depth analysis and clearly show that these benchmarks have been saturated."}, "questions": {"value": "1. Some previous work similarly splits the prediction loss into Bias and Variance; i.e. Bias means the \"upper-bound\" of forecasting accuracy or \"lower-bound\" of prediction loss, Variance is determined by noise, model learning paradigm, etc., they claim that bias decreases with longer input horizon[3]. What's your comment on this? Would you find out that longer input horizon could lead to higher prediction upper-bound?\n2. Some claim that context is needed for time series forecasting. For example, consider these two cases: stock prediction and virus spread condition prediction. At some point they might look very similar, so a foundation model could give similar prediction, leading to non-accurate prediction[1]. However, in practice we use other contexts: e.g. we use alphas for stock prediction, and we use some other data and physics model for virus spreading prediction. What's your opinion on this?\n3. Recently the community thinks that we have to use new benchmarks with more contexts and modalities other than time series alone for forecasting. Forecasting from time series alone is not reliable nor reasonable. Do you think this would be a good approach? What does the \"accuracy law\" for these cases might look like?\n\nRef:\n\n[1] Fundamental limitations of foundational forecasting models: The need for multimodality and rigorous evaluation, NeurIPS 24 workshop talk.\n\n[2] Position: There are no Champions in Long-Term Time Series Forecasting, Arxiv.\n\n[3] Scaling Law for Time Series Forecasting, NeurIPS 24 poster."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9z3p4g72S5", "forum": "xao0xuDoK0", "replyto": "xao0xuDoK0", "signatures": ["ICLR.cc/2026/Conference/Submission2099/Reviewer_VUQD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2099/Reviewer_VUQD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924155357, "cdate": 1761924155357, "tmdate": 1762916022522, "mdate": 1762916022522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper notices that recent studies have achieved minor improvements on standard benchmark datasets. To pinpoint the research objective and release researchers from these saturated tasks, this paper considers it essential to estimate the performance upper bound of deep time series forecasting. Based on a series of experiments, the paper empirically finds and formally defines an accuracy law for deep time series forecasting. This law reveals that the minimum forecasting error (MSE) achievable by deep models exhibits an exponential relationship with the window-wise pattern complexity of the time series. By leveraging the proposed accuracy law, this paper identifies saturated tasks in widely used benchmark datasets and derives an effective training strategy to improve the generalization capability of large-scale time series models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper newly defines window-wise pattern complexity and series pattern complexity.\n2. The figures are visually appealing, the overall organization is reasonable, and the references are appropriate.\n3. This paper has good writing."}, "weaknesses": {"value": "Please see the Questions."}, "questions": {"value": "1. The proposed Accuracy Law in Eq. 4 and the key Figure 4 are solely derived from experiments on three models. I believe that both the experimental and theoretical evidence are insufficient:  \n(1) The proposed accuracy law is essentially a linear association fitted based on the currently limited experimental results, relying heavily on the performance of the selected models. In fact, we have not yet obtained a true forecasting performance upper bound that is relevant to time series predictability. If additional models and datasets are included in the statistical analysis, the key parameter α in Eq. 4 would change, and consequently, the accuracy law line would also shift. Therefore, I argue that relying on the proposed accuracy law cannot effectively identify saturated forecasting tasks. For instance, if the models used to fit the accuracy law line perform poorly, many models may fall below the line, but this does not necessarily indicate that the forecasting task for that specific series pattern complexity has reached saturation.  \n(2) According to the proposed accuracy law, “the minimum forecasting MSE achieved by all feasible deep models admits an exponential relation with the complexity.” This implies that time series with the same pattern complexity should exhibit similar or nearly identical lowest forecasting errors. However, as shown in Figure 4, even for univariate time series with the same pattern complexity, their lowest forecasting errors vary substantially. Moreover, as pattern complexity increases, forecasting errors become more scattered. This observation conflicts with the claimed accuracy law, and the paper does not provide any explanation for this phenomenon.  \n(3) The current experiments use only three models, which are not fully representative. Selecting different types of time series models may yield different results. For example, models can be based on different frameworks such as MLP-based or Transformer-based, and consequently, some model types may be more sensitive to dataset characteristics, the length of the prediction horizon and past observation. To conduct a systematic and comprehensive empirical evaluation, I suggest adopting a broader range of models and conducting more in-depth analysis.  \n(4) The calculation of the proposed series complexity is not clearly explained. Are the divided windows overlapping or non-overlapping?  \nWhat is the stride used? The current demonstrations and conclusions are based on the input-96-predict-96 setting. How do different partitioning schemes or window sizes affect the computed series pattern complexity?\n2. What is the theoretical basis for the complexity definition? Why use the Fast Fourier Transform (FFT) instead of other methods?\n3. Others:  \n(1) The paper states C_min=0, how many such special time series exist, and what are their characteristics?  \n(2）The paper states C_max=309, but Figure 4 shows that the maximum value on the x-axis is 302.  \n(3) Is there any model that performs relatively worse than others on low pattern complexity series but performs better on high pattern complexity series?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ShlCCVK8fM", "forum": "xao0xuDoK0", "replyto": "xao0xuDoK0", "signatures": ["ICLR.cc/2026/Conference/Submission2099/Reviewer_B8Rp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2099/Reviewer_B8Rp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934484594, "cdate": 1761934484594, "tmdate": 1762916022404, "mdate": 1762916022404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work attempts to provide a accuracy law that relates the relationship between the minimum forecasting error of deep models,  and the complexity of time-series window wise patterns (mainly measured as the variance of the amplitude spectrum distribution). The work is timely and is in a good direction as there have been several papers that develop sophisticated deep time series forecasting models and can only show marginal gains in accuracy on the standard 7-8 datasets such as ETT, electricity, traffic etc."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A key use case of the accuracy law as claimed in the work is to test whether saturation wrt accuracy has been achieved for a given dataset, so as to guide the research community towards the need to move to new benchmarks. The work is timely and is in a good direction as there have been several papers that develop sophisticated deep time series forecasting models and can only show marginal gains in accuracy on the standard 7-8 datasets such as ETT, electricity, traffic etc."}, "weaknesses": {"value": "While the work has good motivation, I find the claims to be particularly strong, and not well supported by the theoretical analysis. There are also some concerns regarding the experimental setup.\n\nDespite the claim of finding a novel accuracy law, the actual law proposed in the paper does not seem to have solid theoretical backing or novelty. The work first transforms the time window in to the frequency domain, and then extracts the amplitude spectrum. The variance of this spectrum is deemed to represent the complexity of time window. This measure is then related via a exponential relation to the best MSE achieved for that time series.\n\nThis idea is intuitive, but as such not that novel. Several works in time series decompose the data into frequency domain, and it is also straightforward to see the variance of the amplitude spectrum can affect the complexity. \n\nThere are some issues that need clarity when testing whether this idea is valid. First, the tests are only done on univariate time series on the LOTSA dataset. Using random sampling over LOTSA dataset, the work claims to create a balanced, diverse and representative dataset to test their law. It is not clear, in a rigorous manner, what “balanced”, “diverse” and “representative” means. How can creating time series on LOTSA dataset be representative of all types of time series found in real world, and how can one be sure that real world time series diversity is represented in the created dataset?  This issue seems like a major limitation which affects the generality of the claim. \n\nAnother setting which is hardcoded is the input-96-predict-96 setting. Why this setting was chosen, how does the accuracy law’s performance varies when this time window size varies. One can assume that if the input size increases, then the prior tests for time series complexity, such as ADF, would start working. Currently I could not see this analysis. \n\nOverall, while the claimed law is intuitive, and it is indeed required to establish a metric for complexity of time series, the claims of the work appear overstated, and the accuracy law’s testing is limited by several assumptions made in the experimental setup, which may not hold in wide range of real world settings."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a9FyJcmqrX", "forum": "xao0xuDoK0", "replyto": "xao0xuDoK0", "signatures": ["ICLR.cc/2026/Conference/Submission2099/Reviewer_cwRd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2099/Reviewer_cwRd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061799455, "cdate": 1762061799455, "tmdate": 1762916022246, "mdate": 1762916022246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a measure of complexity for time series forecasting tasks. This is verified by showing a log-linear relationship between their proposed measure of complexity and minimum accuracy over 3 deep forecasting models. Specifically, train 3 models on a single univariate time series, and take the minimum (log) MSE/MAE over these 3 models as the dependent variable, and the complexity of that time series as the independent variable, and do a linear regression. Based on this relationship which the paper terms an accuracy law, the paper identifies amongst popular benchmark datasets, which ones are saturated, and which can be further improved on. They also propose a divergence metric inspired by their measure of complexity, and use it to compute the divergence between datasets. Using this, they construct a benchmark with a higher divergence from existing pre-training datasets. Finally, they show that using the measure of complexity to sample more complex time series during pre-training can lead to a model which performs better on this benchmark."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of having a measure of complexity which can guide us in developing better benchmarks and training datasets is very promising. The proposed measure of complexity clearly has some ability to be predictive in whether a time series is forecastable or not. The paper also does some interesting work on dataset selection and promisingly shows that when trained on more complex time series, performance improves."}, "weaknesses": {"value": "* The construction of figure 4 and the collection of data points for the proposed law is problematic. The idea is to regress the best possible accuracy on the measure of complexity, however, each data point is only the best metric over 3 models of different architectures. Ideally this should be the best metric over an extensive search of model classes. In fact, the formal statement of the accuracy law claims to be \"the minimum forecasting MSE achieved by all feasible deep models\", this is a huge overstatement and is not true. \n* Along similar lines, there is a huge extrapolation risk between how the datapoints are collected and real world scenarios. The data points are collected in a very artificial setting where 1 deep model is trained on 1 time series, but this is not the case for most real world models, and increasingly so when the paradigm of cross dataset training is becoming more popular. Considerations need to be made about how to measure forecastability, or perhaps we need to be considering conditional forecastability.\n* I'm confused as to why the relationship between complexity and performance has been termed an \"accuracy law\", when it seems that the experiment is simply verifying that the proposed measure of complexity fulfills the desired relationship between complexity and forecastabillity. I presume that the name is inspired by scaling laws, but the term scaling laws comes about from measuring the scale of an aspect of the system and performance, where the way we measure the dependent variable is fixed. However, here, the way the dependent variable is measured is changing based on the independent variable. For example, let's say we want to propose a complexity law, where the independent variable is the measure of complexity of some aspect of the system, e.g. dataset, model arch, and the dependent variable is performance, measured in a fixed manner and not changing based on the complexity measure.\n* Given the extrapolation risk, I'm not sure if we can make the conclusion made in 4.2 Practice 1. It also seems to make a presupposition that (deep learning) research is mainly about model design.\n\nMinor\n* Some claims made in the introduction are very contentious\n  * \"For instance, image recognition in computer vision (Deng et al., 2009), a popular and long-standing area since 2000, has a quantifiable and widely accepted goal, that is, to achieve 100% accuracy.\"  - Even in imagenet, 100% can't be achieved due to labelling errors and inherent ambiguity in images.\n  * \"Drawing inspiration from other tracks of the machine learning community that never or rarely suffer from such unclear research objectives\"  - I'm very surprised by this statement."}, "questions": {"value": "* Could more information about the set up for figure 4 be provided, exactly how the models were trained, what are the model sizes, more information about the time series that were picked for this experiment.\n* How are the data points in Figure 6 obtained?\n* I'm confused about the divergence metric and figure 7. For Divergence(x, y), are x, y from the same dataset, i.e. TimeBench, or is x from TimeBench and y from Gift-Eval?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "81fmQg5SZe", "forum": "xao0xuDoK0", "replyto": "xao0xuDoK0", "signatures": ["ICLR.cc/2026/Conference/Submission2099/Reviewer_RH7o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2099/Reviewer_RH7o"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762114709044, "cdate": 1762114709044, "tmdate": 1762916021904, "mdate": 1762916021904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}