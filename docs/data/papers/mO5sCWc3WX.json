{"id": "mO5sCWc3WX", "number": 1087, "cdate": 1756836597695, "mdate": 1759898228591, "content": {"title": "Test-Time Anchoring for Discrete Diffusion Posterior Sampling", "abstract": "We study the problem of posterior sampling using pretrained discrete diffusion foundation models, aiming to recover images from noisy measurements without retraining task-specific models. While diffusion models have achieved remarkable success in generative modeling, most advances rely on continuous Gaussian diffusion. In contrast, discrete diffusion offers a unified framework for jointly modeling categorical data such as text and images. Beyond unification, discrete diffusion provides faster inference, finer control, and principled training-free Bayesian inference, making it particularly well-suited for posterior sampling. However, existing approaches to discrete diffusion posterior sampling face severe challenges: derivative-free guidance yields sparse signals, continuous relaxations limit applicability, and split Gibbs samplers suffer from the curse of dimensionality. To overcome these limitations, we introduce **Anchored Posterior Sampling (APS)** for *masked diffusion* foundation models, built on two key innovations---*quantized expectation* for gradient-like guidance in discrete embedding space, and *anchored remasking* for adaptive decoding. \nOur approach achieves state-of-the-art performance among discrete diffusion samplers across linear and nonlinear inverse problems on the standard benchmarks.", "tldr": "We introduce Anchored Posterior Sampling (APS) for masked diffusion foundation models, built on two key innovations: (1) quantized expectation for gradient-like guidance in discrete embedding space, and (2) anchored remasking for adaptive decoding.", "keywords": ["inverse problems", "generative modeling", "discrete diffusion", "masked diffusion", "image editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/85cb53d41c59488c4d5af6021dcccb90623df9db.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Anchored Posterior Sampling (APS) for masked discrete diffusion models. APS is built on two key innovations: (1) quantized expectation, which enables gradient-like guidance in discrete embedding space, and (2) anchored re-masking, an adaptive strategy for decoding the most informative tokens early. Experiments demonstrate that APS achieves competitive performance among discrete samplers on linear and nonlinear inverse problems and enables training-free stylization, often outperforming continuous diffusion baselines at a lower computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- To the best of my knowledge, this work is original. It tackles a significant challenge regarding posterior sampling with discrete diffusion models with a novel combination of ideas: Quantized Expectation provides a principled, gradient-like signal that is far more effective than prior derivative-free or relaxation-based methods; Anchored Remasking​ adapts the concept of \"anchor tokens\" from language modeling to the visual domain for inverse problems.\n- As far as I checked, this work is methodologically sound, grounded in theoretical derivations (Thm 3.1 & 3.2) that provide a solid foundation for the empirical contributions. The experimental evaluation is comprehensive and rigorous.\n- The paper is well-written and well-structured. The appendix provides substantial additional detail, ensuring reproducibility."}, "weaknesses": {"value": "- While the paper provides theoretical bounds (Thm 3.1 & 3.2), the connection between these bounds and the final Algorithm 1 (APS) feels somewhat loose. The method's core strength lies in the engineering innovation of ​Quantized Expectation​ and the heuristic of ​Anchored Remasking, which are motivated intuitively but not derived directly as the solution to optimizing the presented bounds.\n- The comparison of runtime (Table 6) is valuable, but it lacks a breakdown. The 100 inner optimization steps per reverse diffusion step represent a significant overhead. The paper does not ablate the number of inner steps or analyze the trade-off between performance and compute.\n- The generality of APS across different discrete diffusion architectures (e.g., MaskGIT) is not demonstrated. To strengthen the claim of generality, the authors should apply APS to at least one other masked discrete diffusion model (even if smaller) on a subset of tasks. This would show that the innovations are not overly specific to MMaDA's architecture."}, "questions": {"value": "- The paper highlights the speed of APS compared to continuous methods, but the 100 inner-loop steps per diffusion step represent a significant cost. Could you provide an ablation study showing the performance (e.g., PSNR/LPIPS) versus the number of inner steps (e.g., 10, 30, 50, 100) for a key task like 4x super-resolution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QZvcgMRuQ4", "forum": "mO5sCWc3WX", "replyto": "mO5sCWc3WX", "signatures": ["ICLR.cc/2026/Conference/Submission1087/Reviewer_Lv1F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1087/Reviewer_Lv1F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812025314, "cdate": 1761812025314, "tmdate": 1762915674020, "mdate": 1762915674020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel posterior sampling method for masked discrete diffusion models, aiming to solve inverse problems like super-resolution and deblurring without task-specific retraining. The method builds on two key ideas: quantized expectation for gradient-like guidance in discrete space, and anchored remasking for adaptive decoding. Theoretical bounds (LDDPS and LAPS) are derived to support training-free inference. Extensive experiments on FFHQ and ImageNet show strong results, outperforming discrete baselines and competing with continuous diffusions."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed methods, quantized expectation and anchored remasking, empirically address challenges in discrete diffusion posterior sampling, like non-differentiability and suboptimal decoding order. This makes the method practical and efficient.\n\n- I like the solid theoretical explanations, which have clear derivations of upper bounds that enable reusing pretrained models. This is valuable for further investigation.\n\n- The experiments demonstrate superior performance on linear and nonlinear tasks, with notable improvements in LPIPS and PSNR. The extension to stylization and editing adds practical utility.\n\n- The writing is clear and well-organized."}, "weaknesses": {"value": "The method's generalization across different tokenizers, datasets, and noise levels could be further explored. For instance, it relies heavily on LFQ tokenization. Could you discuss more on results for varying noise strengths and multimodal tasks to highlight robustness?"}, "questions": {"value": "Could you discuss more on results for varying noise strengths and multimodal tasks to highlight robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rPkEQVcAiW", "forum": "mO5sCWc3WX", "replyto": "mO5sCWc3WX", "signatures": ["ICLR.cc/2026/Conference/Submission1087/Reviewer_9oTX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1087/Reviewer_9oTX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820510886, "cdate": 1761820510886, "tmdate": 1762915673908, "mdate": 1762915673908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops an inference time only algorithm for solving inverse problems using pre-trained masked diffusion models. The paper proposes the optimization of a variational objective at test-time together with various tricks such as expected codebook quantization and anchored remasking."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is timely and tackles a problem that could be highly interesting for the ICLR community. \n- The paper presents experiments on inverse problems and stylization and the comparisons are performed against many baselines."}, "weaknesses": {"value": "My first concern is regarding the methodological aspect of the paper. Overall I found the mathematical details to be somewhat poorly presented and the theoretical derivations seem to be disconnected from what the authors actually implement in practice. \n- The authors define a joint probability distribution $p \\_\\varphi(x, z_{0:1} | y)$ in line 206. Given the definition in equation 6, this is not a probability distribution: the tokenwise conditional is $p \\_\\varphi(z^l _s | z_t, y)$ does not even integrate to one (over $z^l \\_s$), it ingrates to $q(y|x \\_\\varphi(z \\_t))$. Furthermore, following this definition we would have that the conditional is equal to , why does this make sense?  $q(y|x \\_\\varphi (z \\_t))^L q(z \\_s| z\\_t, x \\_\\varphi(z_t))$. The authors are trying to tilt the unconditional backward distribution but this is wrong. \n- Algorithm 1 in the appendix seems to be disconnected from the derivations in the main appendix. First, the authors seem to be only considering a reconstruction and perceptual loss. There doesn't seem to be anything loss term related to the pre-trained model as one would expect from the theory but also in general from variational inference methods (see G2D2). Next, even assuming that the authors want to implement the $L \\_{APS}$ loss, this would require drawing a random timestep at each gradient step and then optimizing the associated term. This is not what Algorithm 1 does as the timesteps are scanned in decreasing order. Overall the theoretical framing does not match the actual implementation, despite the authors claiming for example in line 302 that the objective (7) is optimized at each time step. \n\nRegarding the experimental evaluation it seems that the authors use the perceptual and L1 reconstruction loss only for their method and not the other baselines (at least it seems to me that they mention nowhere that they use the same losses for the other methods. The code is not provided so I am unable to check this in the codebase). Furthermore, they seem to have performed tuning over the weights for both terms. In my opinion this is unfair for the other methods and biases the results as the losses should boost the performance. Second, it also seems that the comparison with existing diffusion baselines is not done using the same base model, which is also concerning and biases the results \n\nMy overall opinion of this paper is that it is not yet ready for publication due to the many inconsistencies in the methodology and also the experimental section."}, "questions": {"value": "- Why not compare against the same baselines on all the tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Mx1ogzKVeh", "forum": "mO5sCWc3WX", "replyto": "mO5sCWc3WX", "signatures": ["ICLR.cc/2026/Conference/Submission1087/Reviewer_nSN9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1087/Reviewer_nSN9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866733390, "cdate": 1761866733390, "tmdate": 1762915673785, "mdate": 1762915673785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel approach for posterior sampling based on a pretrained discrete diffusion model. The authors present two theorems demonstrating: \n\n1) An objective for training a discrete diffusion model capable of posterior sampling;\n\n2) An alternative form of this objective that enables the reuse of a pretrained discrete diffusion model for posterior sampling without requiring backpropagation through it.\n\nFurthermore, the authors propose two additional techniques: Quantized Expectation for differentiable likelihood evaluation and Anchored Remasking for adaptive unmasking. Finally, the paper reports experimental results that showcase the model’s capabilities."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clearly written and provides detailed, step-by-step explanations, supported by extensive references to prior work.\n\n- The proposed methods — Quantized Expectation and Anchored Remasking — have a notable positive effect on the final model’s performance.\n\n- The experimental evaluation is comprehensive and effectively demonstrates the model’s efficacy across multiple scenarios."}, "weaknesses": {"value": "Both theorems presented in the paper contain errors due to incorrect operations involving probabilistic distributions. Specifically, in Equation (6) (page 4), the authors incorrectly parameterize the conditional distribution as follows:\n\n$$p_\\phi(z^l_s \\mid z_t, y) := q(z^l_s \\mid z^l_t, x_\\phi(z_t)) q(y \\mid x_\\phi(z_t))$$\n\nThe fundamental issue lies in the fact that the distribution on the left-hand side ($p_\\phi(z^l_s \\mid z_t, y)$) is defined over $z^l_s$ conditioned on $y$, whereas the right-hand side ($q(z^l_s \\mid z^l_t, x_\\phi(z_t)) q(y \\mid x_\\phi(z_t))$) represents a joint distribution over $z^l_s$ and $y$. In probabilistic modeling, these two forms cannot be interchanged. This inconsistency becomes critical because both theorems explicitly rely on treating these as probabilistic distributions. The issue appears in the Proof of Theorem 3.1 (the transition from Equation 10 to the equation immediately following it, page 16) and in the Proof of Theorem 3.2 (the transition from Equation 13 to Equation 14, page 20). \n\nWhile the final empirical procedure performs well, this success can be attributed to the structure of the training objective $\\mathcal{L}_{\\text{APS}}$ (Theorem 3.2, page 5) rather than to the theoretical results. Specifically, the right-hand term of the objective encourages the lightweight model’s output to align with a particular $y$, whereas the left-hand term regularizes the model by constraining it to remain close to the pretrained diffusion model. \n\nIn conclusion, the current theoretical framework does not rigorously justify the proposed practically effective method. Given that the theoretical contribution constitutes a central component of the paper, this issue substantially undermines the overall validity of the work."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oYKiEM542V", "forum": "mO5sCWc3WX", "replyto": "mO5sCWc3WX", "signatures": ["ICLR.cc/2026/Conference/Submission1087/Reviewer_8wfe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1087/Reviewer_8wfe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914739450, "cdate": 1761914739450, "tmdate": 1762915673629, "mdate": 1762915673629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}