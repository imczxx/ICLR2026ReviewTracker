{"id": "bDgcc1gPOu", "number": 4233, "cdate": 1757643542805, "mdate": 1759898045740, "content": {"title": "AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance", "abstract": "As governments move to regulate AI, there is growing interest in using Large Language Models (LLMs) to assess whether or not an AI system complies with a given AI Regulation (AIR). However, there is presently no way to benchmark the performance of LLMs at this task. To fill this void, we introduce AIReg-Bench: the first benchmark dataset designed to test how well LLMs can assess compliance with the EU AI Act (AIA). We created this dataset through a two-step process: (1) by prompting an LLM with carefully structured instructions, we generated 120 technical documentation excerpts (samples), each depicting a fictional, albeit plausible, AI system — of the kind an AI provider might produce to demonstrate their compliance with AIR; (2) legal experts then reviewed and annotated each sample to indicate whether, and in what way, the AI system described therein violates specific Articles of the AIA. The resulting dataset, together with our evaluation of whether frontier LLMs can reproduce the experts’ compliance labels, provides a starting point to understand the opportunities and limitations of LLM-based AIR compliance assessment tools and establishes a benchmark against which subsequent LLMs can be compared. The dataset and evaluation code are available at https://anonymous.4open.science/r/aireg-bench-5259/.", "tldr": "", "keywords": ["benchmark dataset", "large language model", "AI regulation", "AI safety", "responsible AI", "legal AI", "law", "compliance"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35bcbaf47b7125ee0674096b6148de2cb620e298.pdf", "supplementary_material": "/attachment/b59c440f6d1f20929af32fdf2e9607bc9b780458.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces AIReg-Bench, a benchmark dataset specifically designed to evaluate how effectively LLMs can assess compliance with the European Union's AI Act (AIA). Gpt-4.1-mini was used to generate 120 excerpts of plausible, fictional technical documentation for high-risk AI systems, with the aim of mimicking what a provider would create. Then, a small handful of legal experts were involved to annotate these samples on a Likert scale to determine their probability of compliance. Ten LLMs, including models from GPT, Gemini and Grok families, were evaluate to understand whether they are able to closely approximate human compliance judgments.\n\nWhile the paper has the merit of proposing an open benchmark to foster further quantitative research into LLM performance on AI regulation compliance tasks, it also highlights the challenges of producing technical documentation for high-risk AI systems, which would justify LLM-driven sample generation and admit inherent subjectivity in legal compliance assessments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  Creation of a new, open-source benchmark dataset specifically designed to quantitatively evaluate and compare how well LLMs can assess compliance with the EU AI Act.\n\n2.  Cost-saving opportunity for AI Regulation (AIR) compliance assessments based on a GenAI pipeline for producing fictional, yet plausible samples of technical documentation that an AI provider might use to demonstrate compliance with AIR.\n\n3. Evaluation of ten LLMs to determine the level of approximation of human expert judgments on compliance."}, "weaknesses": {"value": "1. Risk of unreliability and lack of domain-specific knowledge.    The reliance on synthetic data generation and the inherent challenges of legal benchmarking introduce several limitations, particularly the risk associated with building an entirely artificial dataset in a critical domain like AIA compliance. While it's acknowledged that  reliance on synthetic data is due to a current unavailability of l technical documentation for high-risk AI systems, it cannot however be overlooked that LLMs often lack domain-specific tacit knowledge and may hallucinate facts or references.  These issues pose risks when, as in this case, the core evaluation data is derived from automated generation rather than real-world artifacts.\n\n2. Small team rather than a large consensus of legal experts.   Another big concern is the number of LLM generators -- just one, gpt4-mini -- as well as number of legal \"experts\". Particularly,  the latter highlights limited scale of expert validation, which cannot be fully justified by the fact that there are \"very few potential annotators\" with the necessary expertise in the EU AI Act who are willing and able to conduct extended annotation tasks.\n\n3. Limited scope: The benchmark is currently scoped only to a subset of the AIA’s requirements for HRAI systems  \n\n4. Simplification of the assessment process: The benchmark condenses the complex compliance assessment process -- which in practice often involves a long, multi-turn dialogue between legal teams, technical staff, and regulators -- into a single-turn interaction based on a fixed set of synthetic artifacts"}, "questions": {"value": "See above Weaknessess, and consider addressing points 2 to 4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LAma2rcRUL", "forum": "bDgcc1gPOu", "replyto": "bDgcc1gPOu", "signatures": ["ICLR.cc/2026/Conference/Submission4233/Reviewer_Lk7K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4233/Reviewer_Lk7K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826889692, "cdate": 1761826889692, "tmdate": 1762917242391, "mdate": 1762917242391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces AIRReg-Bench, a benchmark designed to assess how well current frontier AI systems can evaluate compliance with the EU AI Act (AIA). In particular, the authors create synthetic scenarios based on 120 excerpts from AIA Articles. While these samples are synthetically generated (GPT-4.1-mini), they are additionally scored by domain and legal experts, resulting in a total of 360 scores across eight scenarios. Subsequently, ten state-of-the-art AI systems are tested, showing overall decent agreement with the median human compliance score, particularly Gemini-2.5 Pro, which achieved a $\\kappa_w$ of around 86% on the five-point Likert scale. The results generally indicate that existing models are, in some cases, already close to providing near expert-level advice on AI regulation matters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The AIA is becoming increasingly relevant, and work on it seems timely.\n- The main strength of the paper, in my opinion, is the strong inclusion of subject-level legal experts. This not only demonstrates significant effort but also lends credibility to the individual scenarios, including their scores and ratings. The results are therefore quite indicative of the alignment of frontier systems with experts on this matter.\n- Full release of the corresponding dataset could be very useful for future work in this area (especially using the labels)."}, "weaknesses": {"value": "- The inter-rater agreement is 0.651, which is “on the good side of not good.” Values in this range are not atypical, especially for quite heterogeneous data, and I have encountered them myself; however, it is below what is classically seen as reliable scoring (0.67). I don’t think this is a particularly disqualifying issue by itself, but the lack of analysis thereof is, in my view, problematic (besides the fact that the number is mentioned rather late in the discussion when it should really be part of the dataset description). In particular, it is unclear whether models perform better or worse on subsets where humans show high/low variance.\n- One of the motivations for the work seems to be the financial and time aspects of regulatory compliance (“estimated that these assessments can take up to two and a half days (European Commission, 2021) and cost EUR 7,500 for each AI system”). Two things immediately come to mind: (1) this is not particularly expensive for putting a High-Risk AI system on the market, and (2) it is questionable whether it is generally advisable to aim to replace this compliance process with AI systems. That is not to say they cannot play a role in this process, but perhaps not in the zero-shot fashion evaluated in this benchmark. From talking to people currently working on the standardization processes of the AI Act, it seems they are more interested in systems that can handle the basic “grunt work.” What would be interesting here, for example, is to examine cases where expert opinions do not align with model opinions - how would an expert judge this as a starting point for a compliance process for such a system?\n- Given the current presentation, the diversity of the dataset is also not particularly evident (from just reading the paper and appendix). Therefore, I cannot make any statements about the actual diversity. The authors should consider providing a significantly enhanced description of the individual cases and respective examples in the main work. Also, consider not referring to them simply as “Use 1-Use 8”; almost any one- or two-word title (e.g., Traffic control, Hiring, etc.) would be more descriptive. Similar uncertainties exist regarding, for example, whether experts and models agree more or less on compliant versus non-compliant cases. Without this information, it is difficult to make meaningful judgments about the provided data.\n- The pipeline seems acceptable (it is mostly prompt engineering) but still appears to produce a notable amount of lower-quality samples. While the current benchmark mitigates this by providing expert-level judgment on these samples, this could hinder the generative capabilities of future benchmarks."}, "questions": {"value": "Besides the points raised above, I have the following questions:\n\n- Why did you not post-filter the scenarios that are rated more highly by experts? In particular, what is the value of having about one-third of your benchmark scenarios based on data rated as more unrealistic by experts (assuming Likert < 3 as subpar samples)? Building on this (and the comment above), it would generally be helpful to include more dataset statistics in the main paper. The number above I had to roughly approximate based on the ratings proportion; furthermore, not a single full example of a case with respective scores is provided in the paper.\n- Can you provide more detail on the specific Article 10, Use 8 (and Article 12, Use 2) cases that show a particularly high discrepancy between model and annotator? Overall can you provide quantitative insight where models seem to struggle / disagree more with experts and why?\n- Do you have any explanation why we observe such strong contrast between o3-mini and o3 w.r.t. over and under-estimation (Table 3)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uTrvvygmbN", "forum": "bDgcc1gPOu", "replyto": "bDgcc1gPOu", "signatures": ["ICLR.cc/2026/Conference/Submission4233/Reviewer_BGtH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4233/Reviewer_BGtH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840229718, "cdate": 1761840229718, "tmdate": 1762917242115, "mdate": 1762917242115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors benchmark large language models' ability to assess compliance with AI regulation, focusing specifically on the EU AI Act's requirements for high-risk Systems (Articles 9, 10, 12, 14, 15). They construct a dataset of fictional technical documentation excerpts generated with GPT-4.1-mini across multiple use cases and varying compliance profiles. From these, 120 samples are selected (approximately one-third compliant and two-thirds non-compliant) and annotated each by three legal annotators. Using this benchmark, the authors evaluate ten frontier LLMs and find that several models closely approximate human legal annotators' judgments of regulatory compliance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The paper is carefully written and overall in very good shape.\n* It is clearly structured and easy to follow.\n* The topic is highly timely and relevant.\n* I checked the Appendix and the repo and both are great in terms of transparency/reproducability."}, "weaknesses": {"value": "* The main limitation lies in the limited external validity. Although the authors acknowledge and attempt to mitigate the fact that the technical documentation excerpts are fictional (Section 6.2), this remains a substantial constraint that cannot truly be alleviated. \n* To me, the study reveals how closely LLMs can replicate the annotations of legal annotators on LLM-generated material. The truly interesting question would be whether LLMs can identify genuine non-compliance among real developers and deployers using authentic, non-pre-selected data. This question cannot be answered with the current study design and I find the result of the paper very limited in novelty and impact.\n* The paper appears to have been developed with legal input, yet it lacks a clear legal interpretation framework or taxonomy of expected non-compliance types. This omission further weakens external validity, as the instances of non-compliance planted in the LLM-generated excerpt originate from the LLM process itself. Including even a brief qualitative analysis of the most frequent or illustrative non-compliance patterns would substantially strengthen the work's legal validity.\n* I am not fully convinced by the novelty claim that this is the first benchmark of LLMs for AI regulation compliance. For example, Guldimann et al. (2024) already present an entire benchmarking suite for AI Act compliance of LLMs. The authors should more clearly position their work relative to such prior efforts.\n* The introduction and motivation would benefit from concrete examples of the types of violations assessed.\n* The evaluation design using the median human annotator as a reference point is not ideal. In related literature, stronger baselines are common. I recommend that the authors consider applying the Alternative Annotator Test proposed by Calderon et al. (2025).\n\n\n\n* Minor: typo line 12: Systems Systems; and 1153 )) instead of )"}, "questions": {"value": "* Why did you use gpt-4.1-mini for the technical documentation generation?\n* Can compliance with the selected AIA articles truly be assessed from technical documentation alone?\n* Which articles/violation caused the highest annotator disagreement?\n* Are differences between models statistically significant?\n* Why were no open-weight models included in the evaluation?\n* Since legal annotators could use external sources, did any rely on LLMs for assistance? Some justifications in the repo appear LLM-like to me.\n* Given the technical nature of the documents, is this annotation task really best suited to legal experts?\n* Have you considered that, in real use, developers might try to game such LLM-based compliance systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pyAoxunixu", "forum": "bDgcc1gPOu", "replyto": "bDgcc1gPOu", "signatures": ["ICLR.cc/2026/Conference/Submission4233/Reviewer_AXmy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4233/Reviewer_AXmy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898328143, "cdate": 1761898328143, "tmdate": 1762917241687, "mdate": 1762917241687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The goal of the paper is to address the issue of there being no standardized method for quantitatively evaluating and comparing the performance of LLMs for AIR compliance assessments through their open dataset AIReg-Bench."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The strengths of the paper is that is addresses an issue of high interest and significance, the ideas presented are original, the paper is very clear and straightforward to read and follow, and sufficient details are provided for claims that the authors make. Also, the steps listed for reproducibility is provided correctly. Overall, it is an excellent paper in terms of scientific contributions, impact, importance and necessity."}, "weaknesses": {"value": "One of the weaknesses of the paper is the presentation, such as how it is structured. The better way in which the paper should have been structured is first the problems are detailed, followed by a brief introduction to the solution and then thorough details about the solution. After the details about the solution are provided, the paper should then continue with explaining how the solution addresses the problem and provide evidence/support accordingly. The worry I have is that the presentation of the paper could diminish the significance and contributions of the paper. \n\nAnother weakness is the types of people that were involved in the study, When the authors say legal experts, they mention how those legal experts are a team of law school students, law graduates, and qualified lawyers. The problem is that law school students are not legal experts. There is still a lot that they need to do before they become experts in law. In some extent the same applies for law graduates, but law graduates is more acceptable than law students. I understand the benefit of having a diverse set of legal individuals, but this could hinder the quality of the paper."}, "questions": {"value": "What is the legal practice of the lawyers? There is a big difference in knowledge, approach and perspectives on issues between a criminal lawyer, family lawyer, and an immigration lawyer. \n\nIf the paper is not accepted to the conference, I would suggest performing the study again with just qualified lawyers. It would make the paper much more valuable. \n\nDid an ethics board review and approve the study? If so, this needs to be added in the ethics statement. Also, the ethics statement could be worded better."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6dZBxyoZ8Q", "forum": "bDgcc1gPOu", "replyto": "bDgcc1gPOu", "signatures": ["ICLR.cc/2026/Conference/Submission4233/Reviewer_D1j8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4233/Reviewer_D1j8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762302083952, "cdate": 1762302083952, "tmdate": 1762917241356, "mdate": 1762917241356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}