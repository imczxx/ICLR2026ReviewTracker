{"id": "FpGuLa3S9S", "number": 2263, "cdate": 1757043732750, "mdate": 1762917407069, "content": {"title": "Autoregression with Self-Token Prediction", "abstract": "\\begin{abstract}\nNext-token prediction has been highly effective in language, but its extension to continuous modalities is challenging: regression over correlated latents tends to collapse into near-identity mappings, while discretization via vector-quantized encoders introduces quantization artifacts. Mask-based prediction with diffusion heads mitigates these issues, yet suffers from a train–inference mismatch, inability to use key–value caching, and poor scalability to long sequences. To overcome these limitations, we propose \\emph{self-token prediction}, which conditions each token on ground-truth references during training, ensuring consistency with causal inference while avoiding identity collapse. This design supports key–value caching and parallel generation, enabling scalable, high-fidelity synthesis across text, audio, image, and video. Built on this paradigm, \\textsc{OmniAR} unifies heterogeneous modalities in a shared omni-token space, achieving efficient and high-quality generation, including real-time and theoretically endless video generation.", "tldr": "We propose self-token prediction, a paradigm that conditions each token on ground-truth references during training, ensuring consistency with causal inference while avoiding identity collapse.", "keywords": ["diffusion models", "generative models", "AIGC", "AR", "autoregressive. omnimodal", "multimodal"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/8e27d54b5b2be78a6749e27bb6d8dd32a407373a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes self-token prediction for omnimodal autoregressive modeling, addressing the tendency of next-token regression on continuous latents to collapse into identity copying via a two-stream, asymmetric attention design where only reference tokens serve as context. The approach is paired with grouped causal attention to maintain causality while enabling parallel token prediction within groups, a JEPA regularizer that aligns causal features to a full-context EMA teacher across modalities, and a lightweight time reparameterization schedule to stabilize token-level flow sampling. The resulting system (OMNIAR) aims to retain KV-cache–friendly, causal decoding while scaling to text, images, audio, and video. Empirical evaluation covers ImageNet conditional image generation and classification (reporting FID/IS/Accuracy), text-to-image (GenEval), image captioning on COCO via BLIP, and video generation with VBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Proposed to adopt multiple techniques to improve performance, such as grouped causal attention that allows parallel token prediction, JEPA loss and a lightweight time reparameterization schedule.\n- The model is causal, autoregressive and KV-cache–friendly."}, "weaknesses": {"value": "- Line 76-78: “although the fraction of masked tokens is sampled stochastically during training(see Fig. 1, middle), each forward pass uses only a single sampled mask ratio; consequently, training sees only isolated masking configurations rather than the progressive unmasking dynamics that occur during generation.” The authors claim that masked-based models have train-inference mismatch due to a single sampled mask ratio for the sequence. However, I want to point out that each sequence will be applied masking with different masking ratios in different training iterations, so the model is able to see the same sequence with different masks throughout the training process.\n- The attention map in Fig. 3 is unclear which tokens belong to the same group for grouped causal attention.\n- Table 1 (a) is a system-level comparison with other models, however, the authors only compared with models that have small model size. In table 2, it seems that models are able to scale up to 5.5B. Why not also scale up the proposed models in table 1 for class-conditional image generation and compare between the model with self-token prediction and MAR-H (FID: 1.55 w/ CFG)?\n- No inference speed analyses in this work, although adopting techniques like parallel token prediction and kv-caching. A small table with throughput/latency vs. other system-level baselines would strengthen the empirical case.\n- In Eq. (9), the ∆tt notation is confusing. It should be written as $t \\cdot \\delta t$.\n\nLacking references to: \nMELLE [A], IMPACT [B], AudioMNTP [C], and GLM-4-voice [D], which are all autoregressive speech/audio generation models.\n\n[A] Meng, Lingwei, et al. \"Autoregressive Speech Synthesis without Vector Quantization.\" CoRR (2024).\n\n[B] Huang, KP, et al. \"IMPACT: Iterative Mask-based Parallel Decoding for Text-to-Audio Generation with Diffusion Modeling.\" ICML 2025\n\n[C] Yang, SW, et al. \"Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction.\" ICML 2025\n\n[D] Zeng, Aohan, et al. \"Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot.\" arXiv preprint arXiv:2412.02612 (2024)."}, "questions": {"value": "Why does JEPA loss encourage modality alignment? Is it because tokens of each modality use the same loss and hence are projected to the same target space?\n\nWith the linear interpolation you use, do the authors agree that Eq. (5) is equivalent to rectified flow. If the authors agree with this, please cite rectified flow’s original paper [E].\n\n[E] Liu, Xingchao, Chengyue Gong, and Qiang Liu. \"Flow straight and fast: Learning to generate and transfer data with rectified flow.\" arXiv preprint arXiv:2209.03003 (2022)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mRBabCunfE", "forum": "FpGuLa3S9S", "replyto": "FpGuLa3S9S", "signatures": ["ICLR.cc/2026/Conference/Submission2263/Reviewer_KPNS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2263/Reviewer_KPNS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818539681, "cdate": 1761818539681, "tmdate": 1762916168316, "mdate": 1762916168316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "fCv5NZ16fG", "forum": "FpGuLa3S9S", "replyto": "FpGuLa3S9S", "signatures": ["ICLR.cc/2026/Conference/Submission2263/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2263/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762917405449, "cdate": 1762917405449, "tmdate": 1762917405449, "mdate": 1762917405449, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces \"Self-Token Prediction,\" a novel paradigm designed to overcome the limitations of next-token prediction and masked prediction for continuous data (images, video, audio). The core idea is to condition the prediction of each token on ground-truth \"reference\" tokens during training, using an asymmetric grouped causal attention mechanism. This design prevents the model from learning an identity function and ensures training-inference consistency."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The \"Self-Token Prediction\" paradigm is a genuine innovation. It elegantly solves the identity collapse problem in continuous autoregression and the train-test mismatch of masking, two significant challenges in the field."}, "weaknesses": {"value": "1. The explanation of the Self-Token Prediction, which is the paper's core contribution, is not sufficiently clear. Figure 3 (right) is critical for understanding the information flow between reference and prediction tokens but lacks detailed annotations. \n2. The experimental proof is currently insufficient to fully support the paper's ambitious motivation of building a unified model for \"omnimodal AR understanding and generation.\" This is the most significant shortcoming of this paper. There is a notable lack of quantitative results for: text-video, text-audio, and image-audio generation. Quantitative results for video captioning and audio captioning are absent.\nThe current experiments leans heavily on image, leaving the \"omni\" claim only partially validated.\n3. What is \"KL-VAE\"? I dont know which previous paper introduces this item."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "My4YUJcych", "forum": "FpGuLa3S9S", "replyto": "FpGuLa3S9S", "signatures": ["ICLR.cc/2026/Conference/Submission2263/Reviewer_uyCc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2263/Reviewer_uyCc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889341959, "cdate": 1761889341959, "tmdate": 1762916166561, "mdate": 1762916166561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents OmniAR, a unified generative framework extending next-token prediction to continuous modalities such as images, audio, and video. It introduces self-token prediction, where each token is conditioned on a ground-truth reference during training, preserving causal consistency and avoiding identity collapse. This approach supports key–value caching and parallel generation, enabling efficient, scalable, and high-fidelity synthesis across multiple modalities, including real-time and theoretically endless video generation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Introduction of a unified self-token prediction framework.\n- OmniAR proposes a new training paradigm in which each token receives an explicit reference token during training, effectively bridging autoregressive and mask-based prediction."}, "weaknesses": {"value": "1. Insufficient ablation and comparative analysis.\n- The paper introduces a new generative modeling paradigm, but the ablation studies are limited. In Section 5.1 (Efficiency of Self-Token Prediction), it would be valuable to include a direct comparison among Next-token prediction, Mask-based prediction, and Self-token prediction on a common benchmark such as ImageNet. Furthermore, Equation (6) introduces an additional JEPA loss, yet there is no ablation isolating its contribution, making it difficult to assess how much this component improves the final performance.\n\n\n2. Lack of theoretical and empirical justification for “real-time and endless video generation.”\n- The claim regarding real-time and theoretically endless video generation is insufficiently supported. Stronger evidence is needed to demonstrate concrete inference efficiency gains, such as reductions in sampling steps or wall-clock time compared with existing baselines. Moreover, if the proposed approach improves long-sequence generation, experiments and quantitative evaluations on long video generation should be provided to substantiate the claim.\n\n\n3. Limited discussion of related generative frameworks.\n- The experiment section could be expanded to discuss several recent generative modeling frameworks that are closely connected to this work, including ACDiT [1], GIVT [2], MAGViT-2 [3] and ResGen [4]. Including these work in Table 1 would help position OmniAR more clearly within the broader landscape of contemporary generative modeling research.\n\n\nReferences\n\n[1] Li et al., ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer, 2025.\n\n[2] Tschannen et al., Generative Infinite-Vocabulary Transformers, 2024.\n\n[3] Yu et al., Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation, 2024\n\n[4] Kim et al., Efficient Generative Modeling with Residual Vector Quantization-Based Tokens, 2025."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iLjCbIcUUr", "forum": "FpGuLa3S9S", "replyto": "FpGuLa3S9S", "signatures": ["ICLR.cc/2026/Conference/Submission2263/Reviewer_g7Xx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2263/Reviewer_g7Xx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991889447, "cdate": 1761991889447, "tmdate": 1762916166308, "mdate": 1762916166308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}