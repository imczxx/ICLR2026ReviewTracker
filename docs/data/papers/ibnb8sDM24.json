{"id": "ibnb8sDM24", "number": 10006, "cdate": 1758155619615, "mdate": 1763270780752, "content": {"title": "MultiChartQA-R: A Benchmark for Multi-Chart Question Answering in Real-World Reasoning Scenarios", "abstract": "Existing benchmarks for chart analysis primarily focus on single-chart tasks, whereas multi-chart benchmarks are scarce and limited to simplistic question types, making it difficult to comprehensively evaluate the reasoning and decision-making capabilities of multimodal large language models (MLLMs) in realistic scenarios.\nWe present MultiChartQA-R, a benchmark designed to evaluate multi-chart question answering capabilities, ranging from fundamental abilities to decision-making applications, with four progressively complex reasoning tasks that encompass real-world scenarios: cross-chart trend comparison, complementary data integration, anomaly and causal analysis, and strategy recommendation. The benchmark consists of versions in three major languages, each containing 695 chart–code pairs and 2,160 QA pairs, with extensibility to additional languages. We further propose a flexible multiple-choice evaluation metric that can be adjusted based on real-world scenarios, along with an extended dataset consisting of 512 charts and 1,212 QA pairs, designed to study retrieval and scaling behavior as the number of charts increases.\nOur evaluation of 13 representative MLLMs (4 proprietary models and 9 open-weight models) reveals significant performance gaps compared to human, especially in cross-chart visual perception, data integration, and aligning with human preferences. Additionally, our experiments reveal interesting multilingual characteristics of multi-chart question answering.", "tldr": "", "keywords": ["datasets and benchmarks", "multi-chart question answering", "cross-chart reasoning", "chart understanding and reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/69cb2da892a7a75cc42533caeca557af803da977.pdf", "supplementary_material": "/attachment/d79b99e56dce7b764673e825f8d79797ed34b0fd.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes MultiChartQA-R, aiming to evaluate question-answering and reasoning abilities in multi-chart scenarios. It covers four progressively complex tasks: cross-chart trend judgment, complementary data integration, anomaly and causal/pattern analysis, and strategy recommendation. The dataset reportedly includes English, Chinese, and Spanish versions, and introduces a tunable multiple-choice evaluation metric, MFβ, to balance “accuracy” and “error avoidance.” The authors evaluate 13 multimodal large models and compare them with human performance, claiming that models show a significant gap from humans in cross-chart perception and data integration, as well as certain multilingual differences. The data are derived from multiple charts in public industry reports or dashboards, reconstructed via “reverse chart-to-code” generation and further verified through human review and RAG-based correction. Tasks 3 and 4 are model-generated and human-refined. The paper also presents overall statistics and comparison tables with existing benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Starting from the four-step chain of “correlation → utilization → depth → application,” the paper decomposes multi-chart linkage analysis common in real business scenarios into task categories. The task design has some intuitive rationale, especially in isolating “strategy recommendation” as a decision-oriented task that helps capture application scenarios closer to end-user needs. \n2. By translating rendering code terminology and aligning it with RAG terminology, the authors generate multilingual chart versions and perform consistent QA translation. The process is clearly described and can be extended to additional languages. \n3. The benchmark covers 13 models (both proprietary and open-source, 7B–78B), reports multilingual results, and includes ablation analyses such as “irrelevant chart interference” and “expanding number of charts,” providing reference value."}, "weaknesses": {"value": "1. The paper acknowledges the existence of prior multi-chart and multi-image reasoning benchmarks (e.g., MultiChartQA). Its main additions are the “strategy recommendation task” and multilingual support, but it lacks strong evidence that these four task types are inherently closer to real-world needs or that strategy recommendation remains truly open-ended after being formatted as multiple-choice. Although the comparison table claims broader coverage and more flexible evaluation, no controlled experiment is provided to demonstrate empirical improvement over MultiChartQA under identical models and question types. \n2. The abstract and Table 1 give inconsistent impressions about whether “each language includes 695 charts and 2160 QA pairs” or whether these totals refer to the entire dataset. The abstract states that “each language version contains 695 chart-code pairs and 2160 QA,” while Table 1 lists “Total Questions 2160, Unique Charts 695” without clarifying whether this is per-language. The relationship between per-language and overall counts should be clearly unified and specified. \n3. Tasks 3 and 4 are generated through a multi-step process of “extracting gold charts → model-generated questions and correct/distractor options → human review,” enhanced by retrieved external knowledge for reasoning. If the generation model shares data or stylistic overlap with the evaluated models, it may introduce “style leakage” or “distractor distribution bias.” The paper only reports high human quality scores but lacks quantitative comparisons to assess such risks (e.g., cross-model/cross-family stability, or how different generators affect task difficulty). \n4. The paper defines constraints w_e = 2w_h and w_e|E| + w_h|H| = 1 but does not explain the source of these coefficients, nor provide sensitivity analysis or comparative justification (e.g., whether different w_e:w_h ratios produce stable model rankings or could be data-driven). Comparisons with existing multiple-choice metrics (e.g., weighted F1, Com2) are mostly relegated to the appendix, leaving the main text short of empirical validation. \n5. Table 3 reports multi-model results but lacks statistical significance tests, confidence intervals, or repetition counts. The conclusion that “English has no advantage” is a strong inference requiring stricter controls (e.g., same chart language vs. different question/prompt language combinations). Although the discussion mentions two experiment settings, the main text omits key results and reproducibility details. \n6. Figure 1’s caption inconsistently refers to “Task4&5,” while the text only defines four tasks; several English expressions and formatting issues need refinement. Such inconsistencies, though minor, weaken the authority expected from a benchmark paper. \n7. Compared to existing multi-chart benchmarks, the proposed work still relies on a multiple-choice format. The inherent open-endedness of “strategy formulation/open QA” is discretized into fixed options, potentially reducing external validity for real applications. The paper does not convincingly demonstrate any “new diagnostic dimensions” for model capability or “direct contribution” to training/alignment research within the community."}, "questions": {"value": "Questions are those listed under the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XOtrWKRpfq", "forum": "ibnb8sDM24", "replyto": "ibnb8sDM24", "signatures": ["ICLR.cc/2026/Conference/Submission10006/Reviewer_5FDr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10006/Reviewer_5FDr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400366786, "cdate": 1761400366786, "tmdate": 1762921425973, "mdate": 1762921425973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "**Title: Withdrawal of Submission**\n\nDear Area Chair and Reviewers,\n\nThank you very much for your time and effort in reviewing our manuscript. We have carefully read all the reviews. After thorough consideration, we have decided to withdraw our paper from ICLR 2026.\n\nThe reviewers' comments are very insightful and have helped us identify several areas for significant improvement. We plan to incorporate this valuable feedback to substantially revise our work.\n\nWe sincerely appreciate the opportunity to submit to ICLR 2026 and are grateful for the high-quality feedback provided by the reviewers.\n\nBest regards,\n\nThe Authors"}}, "id": "vHdpQT8WgX", "forum": "ibnb8sDM24", "replyto": "ibnb8sDM24", "signatures": ["ICLR.cc/2026/Conference/Submission10006/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10006/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763270780085, "cdate": 1763270780085, "tmdate": 1763270780085, "mdate": 1763270780085, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new benchmark MultiChartQA-R, which contains questions rangeing from fundamental abilities to decision-making capabilities. It also provides a new evaluation metric, $MF_{\\beta}$, which is more flexible in application, followed by some observations out of experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The overall writing logic of this article is very clear.\n- It provides a novel benchmark which covers more complex questions compared to existing benchmarks.\n- It proposes a new metric to evaluate the capability of anomaly and pattern analysis and strategy recommendation.\n- This paper evaluates the performance of many MLLMs in multi-chart settings, and extracts some conclusions were summarized from the experiment."}, "weaknesses": {"value": "There is a core concerns. Solving these concerns may influence my rating.\n- The questions that evaluate the capability of anomaly and pattern analysis and strategy recommendation are actually turned into a true-or-false or selection question, where intuitively these should be a question of testing the model's generative capabilities. This change weakens the comprehensiveness of the model performance evaluation. Also this is away from Question Answering benchmark.\n\nThere are some other concerns but are not as important as concerns above:\n- How to define easy and difficult items in interfering items.\n- In the experiments, there are no methods that are designed for chart settings. This is a very important aspects to measure the validity of this benchmark. \n- The first two tasks are manually annotated. This may lead to differences between the performance of the model and humans during the final evaluation. Why not just annotate this using other models, followed by human check?"}, "questions": {"value": "Please refer to section Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qf8EV6VlHy", "forum": "ibnb8sDM24", "replyto": "ibnb8sDM24", "signatures": ["ICLR.cc/2026/Conference/Submission10006/Reviewer_wQLy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10006/Reviewer_wQLy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643199073, "cdate": 1761643199073, "tmdate": 1762921425317, "mdate": 1762921425317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MultiChartQA-R, a multilingual benchmark for multi-chart question answering covering four progressively complex reasoning tasks: cross-chart trend inference, complementary data integration, anomaly and pattern analysis, and strategy recommendation. The benchmark includes 695 chart-code pairs and 2,160 QA pairs across English, Chinese, and Spanish, and also proposes a flexible multiple-choice evaluation metric to better assess multi-option reasoning behavior. The authors evaluate 13 proprietary and open-weight MLLMs and show substantial performance gaps compared to humans, particularly in cross-chart perception and data integration. The authors also provide extensive studies to test model robustness against different aspects."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The methodology is generally sound. The construction pipeline combines chart-to-code reconstruction, controlled QA synthesis, multilingual chart/code translation with RAG assistance, and manual review for a subset of data. The metric is reasonable for multi-option multiple-choice settings. Evaluation covers a broad set of models. Claims about model limitations are supported by experiments.\n- The discussion is particularly interesting, involving isolated experiments that evaluate models’ retrieval robustness when having irrelevant visual information and mixing questions and charts in a single conversation — both of which mimic real-world scenarios. The robustness on language mixing is also well-experimented that both multilingual charts and multilingual text scenarios are covered."}, "weaknesses": {"value": "- The multilingual analysis is interesting, especially the distinction between varying languages in charts vs. varying languages in QAs/prompts, but the current discussion feels shallow. Instead of only stating which settings lead to larger fluctuations, I would like to see more rigorous analysis. Concretely, different models show different degrees of performance fluctuation, and some fluctuations appear small. Which differences are statistically significant? Also, some models seem less robust when chart text languages change, while others struggle more when QA language changes. It would be valuable to analyze and justify why this is the case.\n- Section 4.3 and Appendix G are poorly written and difficult to follow. It would help to include a visual illustration or clearer explanation highlighting the differences among the settings. Providing error bars or variance estimates for accuracy curves would clarify statistical significance.\n- The human evaluation process is vague. While the authors state that 30% of the data was inspected, it is unclear how human performance numbers were obtained. More detail on evaluator background, how answers were collected (e.g., did annotators see exactly the same inputs as models?), and the number of annotators per question would help assess reliability.\n- While the benchmark aims to test reasoning-heavy abilities, models already appear to saturate on the latter two question types (relative to human performance), suggesting these tasks may not be sufficiently challenging.\n- Chart type distribution appears imbalanced. Although 14 chart types are included, 9 chart types represent <5% of the dataset, while the remaining 5 chart types account for >95%, with bar charts (bar and stacked bar) alone exceeding half of the dataset. This limits representativeness. Also, single chart looks relatively simple based on the authors’ provided chart images in the supplementary material."}, "questions": {"value": "- Figure 2 is visually complex and difficult to read (arrows, fonts, layout). Could the authors revise or simplify this figure?\n- In Table 4, adding irrelevant charts hurts trend inference and data integration the most, but has limited effect on anomaly/pattern attribution and strategy recommendation. Could the authors explain why? If retrieval is the bottleneck for the first two tasks, does this imply the latter two may not strongly require chart grounding? I am curious about authors' thoughts.\n- How do the authors ensure translation validity across languages? More detail on multilingual quality control would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KLA93Tdnji", "forum": "ibnb8sDM24", "replyto": "ibnb8sDM24", "signatures": ["ICLR.cc/2026/Conference/Submission10006/Reviewer_XBqy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10006/Reviewer_XBqy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865689087, "cdate": 1761865689087, "tmdate": 1762921424817, "mdate": 1762921424817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MultiChartQA-R, a new benchmark dataset for evaluating the multi-chart question-answering capabilities of Multimodal Large Language Models (MLLMs). The proposed benchmark is available in three languages, consisting of 2,160 QA pairs across 695 charts, structured into four reasoning tasks of increasing complexity: cross-chart trend comparison, complementary data integration, anomaly and causal analysis, and strategy recommendation. The authors also proposed a flexible multiple-choice evaluation metric. The authors evaluate 13 MLLMs and find significant performance gaps compared to humans in this benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well motivated. It addresses the current limitations on chart-related benchmarks, which are mostly limited to single charts. To address this, the authors have proposed a multi-chart benchmark. \n\n2. The four-task structure is also well-motivated, targeting real-world use cases. \n\n3. Multilingual extension. \n\n4. The evaluation of 13 models provides a strong baseline for the community.\n\n5. Proposed a flexible evaluation metric."}, "weaknesses": {"value": "1. The paper claims to use charts from real-world reports. However, the authors used the chart-to-code technique, where an LLM converts each chart into Python code. While this is an innovative solution, the benchmark does not test reasoning on real-world data. Rather, it seems more of an LLM-generated synthetic benchmark that mimics the appearance of real-world charts. \n\n2. Moreover, synthetic data generation was also followed in complex tasks (Task 3 and Task 4) by using a reasoning model. But no information about this model was provided. \n\n3. Missing details regarding the RAG web sources and the model being used. \n\n4. The evaluation pipeline for Task 2 uses DeepSeek-V3.1. Why this model is used? What is the limitation of this model? Why Regex not used? Human evaluation was also needed in this step to justify the selection of DeepSeek. \n\n5. Justify the novelty of the proposed benchmark in comparison this: https://arxiv.org/abs/2410.14179\n\n6. Lack of justification of the effectiveness of the proposed evaluation metric in comparison to other metrics. Compare its effectiveness over Precision, Recall, F1 etc."}, "questions": {"value": "Check the weaknesses. Address them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UATwtVNpIO", "forum": "ibnb8sDM24", "replyto": "ibnb8sDM24", "signatures": ["ICLR.cc/2026/Conference/Submission10006/Reviewer_Apjk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10006/Reviewer_Apjk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762178759593, "cdate": 1762178759593, "tmdate": 1762921424346, "mdate": 1762921424346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}