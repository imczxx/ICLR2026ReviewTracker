{"id": "cKOrvcUYYf", "number": 5536, "cdate": 1757918892628, "mdate": 1763067670673, "content": {"title": "Reinforce Your Layout: Online Reward-Guided Diffusion for Layout-to-lmage Generation", "abstract": "In this work, we tackle the layout-to-image generation task by proposing a novel online reinforcement learning (RL) framework that directly optimizes diffusion models to achieve consistency between images and layouts. We introduce RLLay, a method that overcomes a major limitation that lies in existing methods with their reliance on indirect side guidance—rather than direct supervision on layout alignment—which constrains these models' ability to accurately position and scale image content. Given a prompt, our approach generates multiple candidate images and ranks them using a reward model based on Intersection-over-Union (IoU) to quantify alignment between predicted and target layouts. To effectively utilize this ranking signal, we introduce a pairwise preference-based optimization strategy that fine-tunes the diffusion model by maximizing the likelihood of higher-ranked samples relative to lower-ranked ones (hard-negatives). Experimental results show that our RL-based fine-tuning significantly improves both spatial layout fidelity and text-image alignment, establishing a promising direction for more controlled and layout-aware image generation.", "tldr": "", "keywords": ["reinforcement learning", "diffusion", "layout to img", "cmputer vision"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/adbc70d293753bb8213f86bb8ef7c94aba9af343.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work studies the layout-to-image generation problem. The main efforts in this work is to use the reinforcement learning to improve the layout-to-image generation performance. The proposed method applies the online policy training strategy on a pre-trained layout-to-image generation model. The author first rank the difficulties of the samples into four levels and uses a curriculum training scheme by starting from easy samples to very hard samples. In addition, the author proposes to use SDE instead of ODE during the sampling of RL candidates. Afterward, the model is optimized by a custom RL loss. In the experiment, the author shows that the proposed approach can be applied on different layout-to-image model and the results show that the performance on LayoutSAM-Eval benchmark has improved."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* This work studies the application of reinforcement learning specific for layout-to-image generation, which can be a promising direction for layout-to-image generation quality improvement.\n* The proposed method has been applied on two different layout-to-image models, indicating it generalizability."}, "weaknesses": {"value": "Major issues:\n* The reviewer is quite confused about the motivation part in sec 3.2.1. For example:\n    * “base model fails to satisfy complex layout constrains, so parallel sampling yields candidate pairs of uniformly poor quality with barely separable preference/reward signals, making policy improvement difficult.” What kind of complex layout constrain does base model usually to fail and has the proposed method efficiently address these complex layout generation? And why does uniformly poor quality makes policy improvement difficult? Is there any evidence that current L2I provides uniformly poor quality and the policy improvement is difficult? \n    * “the wide distributional span within a single dataset further amplifies training instability.” The reviewer does not understand the relation between the data distribution span and the training instability. And why does the curriculum training is a suitable solution for this problem. \n* The methodology of the proposed method is not well elaborated. For example\n    * In the difficulty level definitions, what does “no preferences/relations” mean? How is containment and overlap defined (like instance wise or region size wise). \n    * The author claims in ln 206-209 that “policy continues to improve under heavy overlap and complex relations without collapsing.” The reviewer wonders is there any evidence that the audience can see why non-curriculum training can collapse.\n    * The current rewards is purely based on the IoU, but in the L2I problem, the object fidelity is also important. The reviewer wonders why the author designed reward solely based on IoU.\n    * The sample pairing in Eq 3 is also proposed without a sufficient justification.\n* The reviewer also has questions on the sampling modification in Sec 3.2.3. It seems the SDE adopted in ln 264 is just the ODE + adding gaussian noise. This would be a naive fall back from simplified ODE back to the original DDPM’s SDE. However, there are quite a few heuristic designs added here, like Eq5 and Eq6. The reviewer cannot quite be convinced by the propose solutions. In addition, the ln 275-277 is confusing, the reviewer does not understand what this sentence is conveying. Moreover, the ln 289-290 that Eq6 is “providing well-behaved action-probability estimates for preference-based and policy-gradient updates.”, but the reviewer cannot find any evidence to support this statement. \n* On the loss function in Sec 3.2.4, the analysis in Eq 10 seems to ignore the second term in the Eq 7. There are two issues here:\n    * Even if ignoring the second term in Eq 7, the derived partial derivative show be -\\beta[1 - \\sigma(-\\beta \\Delta log p)]. \n    * The reviewer does not think the second term in Eq 7 should be ignored. It seems here the author’s ignoring of second term is based on the use of the approximation in ln 316. However, if use the original expansion of KL, there will be one more \\Delta log p term in the second part. The reviewer is not convinced on using the approximation and treat it as a constant independent of  \\Delta log p.\n* There are plenty of things proposed in this work but the corresponding discussion are quite scarce. Currently there are so many factors that it makes the reviewer very confused where the most important performance gain is from. However, the current ablation study is only comparing against to other RL strategy and how these RL strategies are applied in the L2I problem is also not clear in the manuscript.\n* The chose baselines is just Instance Diffusion and it has been released quite long ago, the reviewer suggests the author adding more comparison to more recent works on L2I problem. Also, the VQA accuracy for GLIGEN and Instance Diffusion is missing in Table 2.\n\nMinor issues:\n* As a CV paper, the reviewer would have expectation to see visual comparison against existing methods to show the advantages of the proposed method, and the effectiveness of each proposed componets, but the current manuscript provides insufficient visual results. \n* Some of the equations are missing numbers."}, "questions": {"value": "Please see my weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fqtC6zbbnW", "forum": "cKOrvcUYYf", "replyto": "cKOrvcUYYf", "signatures": ["ICLR.cc/2026/Conference/Submission5536/Reviewer_fnUK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5536/Reviewer_fnUK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750654914, "cdate": 1761750654914, "tmdate": 1762918119312, "mdate": 1762918119312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "R06jlBZdNz", "forum": "cKOrvcUYYf", "replyto": "cKOrvcUYYf", "signatures": ["ICLR.cc/2026/Conference/Submission5536/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5536/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763067669750, "cdate": 1763067669750, "tmdate": 1763067669750, "mdate": 1763067669750, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents RLLay, an online RL framework for layout-to-image generation that fine-tunes diffusion models to better follow spatial layouts. Built on a new reinforcement learning algorithm ARPO, it uses curriculum scheduling, hard-negative mining, and preference-guided rewards for stable, sample-efficient training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. To the best of my knowledge, this is the first RL-based pipeline for layout-to\u0002image generation.\n\n2. Extending the flow ODE to a variance-time SDE (with stability notes) to enable preference/RL learning is a nice technical touch."}, "weaknesses": {"value": "1. Table 1 is somewhat confusing, as the process for determining these ratios is not described. The settings for CreatiLayout and HicoNet differ, suggesting that the ratios were not selected arbitrarily, yet the paper offers no explanation for the rationale behind these choices.\n\n2. The paper employs a “from easy to hard” curriculum strategy based on layout complexity, aiming to progressively train the model to handle increasingly complex layouts. While this approach appears reasonable, the experiments do not include per-difficulty comparisons against baselines to validate whether the method indeed improves performance on harder layout constraints.\n\n3. The work lacks several important ablations: (1) an analysis of how varying the number of extreme preference pairs per (prompt, layout) affects results, and (2) curriculum learning ablations—such as experiments without curriculum learning or with different difficulty boundaries.\n\n4. Compared with the selected baseline, Instance Diffusion, the reported improvements are limited; moreover, the Intersection-over-Union (IoU) score on the COCO benchmark is notably lower."}, "questions": {"value": "1. Figure 1 contains several noticeable issues. In the first row, the layout appears to omit a bounding box for the notebook; the green box should be labeled “desk lamp”; and the capitalization of “Pen holder” is inconsistent with the other category labels. Furthermore, including the corresponding real image (i.e., the source of this layout) side by side would make the visualization clearer and the comparison more intuitive.\n\n2. It is unclear whether the proposed approach supports multi-instance layouts—for example, a layout containing multiple instances of the same object, such as five cats. The paper should clarify whether the method can accurately interpret and generate such cases, and ideally include experiments or visual examples demonstrating its capability in handling multi-instance scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QfOB8famob", "forum": "cKOrvcUYYf", "replyto": "cKOrvcUYYf", "signatures": ["ICLR.cc/2026/Conference/Submission5536/Reviewer_XTmo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5536/Reviewer_XTmo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825933801, "cdate": 1761825933801, "tmdate": 1762918118996, "mdate": 1762918118996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RLLay, an online reinforcement learning framework for layout-to-image generation, aiming to improve spatial alignment between generated images and target layouts. The key technical component is ARPO (Advanced Relative Policy Optimization)—a pairwise preference-based RL algorithm that fine-tunes diffusion models through online sampling and ranking by a layout reward (mean IoU).\nRLLay further incorporates (1) curriculum learning by layout difficulty to stabilize online optimization, and (2) hard negative mining to construct more informative preference pairs.\nExperiments on LayoutSAM-Eval and COCO benchmarks show consistent improvement in spatial accuracy and fidelity over diffusion and preference-optimization baselines such as DPO, DSPO, and GRPO."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The staged “easy-to-hard” layout scheduling is practical and empirically improves stability—a rare but useful consideration in RL-based diffusion.\n2. Table 4 clearly compares DPO, DSPO, GRPO, and ARPO, providing quantitative evidence of the proposed algorithm’s effectiveness."}, "weaknesses": {"value": "1. The “ARPO” update essentially combines a logistic pairwise loss from DPO-style optimization with a KL regularizer, which makes it conceptually close to DSPO and GRPO. The paper claims that ARPO brings stability advantages, but it does not provide either theoretical justification or empirical evidence (e.g., variance plots or training dynamics) to support this claim.\n\n2. The related work section misses several latest layout-to-image generation studies, such as 3DIS-FLUX, as well as other recent approaches that incorporate curriculum learning, which has become a common and effective strategy in multimodal understanding. A more comprehensive discussion of these works would better contextualize the contribution.\n\n3. In Figure 2, the examples of “Medium,” “Hard,” and “Very Hard” layouts appear almost identical, and the “Very Hard” case shows a layout that does not match the target image. It is unclear whether these examples are actual training samples or illustrative placeholders. Please clarify the data source or design intention here.\n\n4. The level definitions described in the main text are inconsistent with those shown in Figure 2. This inconsistency should be resolved to ensure clarity about how difficulty levels are defined and applied in practice.\n\n5. The visualizations and quantitative benchmarks mostly involve layouts with five or fewer objects (see Fig. 1 and Sec. 4.1.2). Since the paper’s main claim concerns improved layout fidelity, it would be valuable to include evaluations on denser or more occluded scenes (e.g., ≥ 10 objects or high-overlap layouts) to better demonstrate robustness.\n\n6. The reward function relies solely on IoU, which measures spatial overlap but overlooks other important aspects such as appearance consistency (e.g., object size, color, texture, and contextual realism). Incorporating more comprehensive reward components could lead to more balanced improvements.\n\n7. All reported evaluations depend entirely on automatic metrics (IoU, FID, CLIP, VQA). Given that the paper focuses on preference alignment, it would be helpful to include a small-scale user study or human evaluation comparing perceptual layout quality to strengthen the empirical claims.\n\n8. The paper lacks qualitative analyses—there are no visual comparisons or case studies illustrating how RLLay improves generation quality beyond numerical metrics."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NONPdfkHBS", "forum": "cKOrvcUYYf", "replyto": "cKOrvcUYYf", "signatures": ["ICLR.cc/2026/Conference/Submission5536/Reviewer_TmJD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5536/Reviewer_TmJD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920707330, "cdate": 1761920707330, "tmdate": 1762918118620, "mdate": 1762918118620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RLLay, an online reinforcement-learning framework for layout-to-image diffusion models. It directly optimizes layout fidelity using an IoU-based reward with online policy updates, combining curriculum learning, hard negative mining, and a stochastic extension of SD-3 sampling for differentiable log-probabilities. Experiments on LAYOUTSAM-Eval and COCO show consistent improvements in spatial alignment, fidelity, and semantic accuracy over diffusion and RL-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[+] RLLay directly optimizes layout fidelity with a clear IoU-based reward, improving spatial control without auxiliary guidance.\n\n[+] The easy-to-hard curriculum and extreme pairing strategy stabilize training and enhance gradient efficiency.\n\n[+] Strong baselines (HiCO, CreatiLayout, DPO, DSPO, GRPO) and metrics show consistent quantitative and qualitative gains.\n\n[+] Figures and tables are well-organized and visually support the claims."}, "weaknesses": {"value": "[-] Heavy reliance on GroundingDINO may introduce bias, but robustness to detection errors is not analyzed.\n\n[-] Effects of λ, β, and κ on stability and alignment are not systematically studied.\n\n[-] Missing analyses on out-of-distribution layouts or potential reward overfitting."}, "questions": {"value": "1. How sensitive is RLLay to the accuracy of the GroundingDINO reward model?\n1. Please clarify how trajectory-level log-probabilities are computed and accumulated.\n1. Are there qualitative cases where optimizing IoU leads to perceptual artifacts or “metric gaming”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jBYZkuBvFH", "forum": "cKOrvcUYYf", "replyto": "cKOrvcUYYf", "signatures": ["ICLR.cc/2026/Conference/Submission5536/Reviewer_8bi3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5536/Reviewer_8bi3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996228573, "cdate": 1761996228573, "tmdate": 1762918118267, "mdate": 1762918118267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}