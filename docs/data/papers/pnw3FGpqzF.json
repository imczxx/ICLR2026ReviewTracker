{"id": "pnw3FGpqzF", "number": 24941, "cdate": 1758362247489, "mdate": 1759896741614, "content": {"title": "Knowledge-Enhanced Tabular Data Generation", "abstract": "Tabular data generation methods aim to synthesize artificial samples by learning the distribution of training data. \nHowever, most existing tabular data generation methods are purely data-driven. \nThey perform poorly when the training samples are insufficient or when there exists a distribution shift between training and true data.\nIn many real-world scenarios, data owners are often able to provide additional knowledge beyond the raw data, such as domain-specific description or dependencies among features. \nMotivated by this, we categorize the types of knowledge that can effectively support tabular data generation, and incorporate selected knowledge as auxiliary information to guide the generation process. \nTo this end, we propose KTGen, a $\\textbf{K}$nowledge-enhanced $\\textbf{T}$abular data $\\textbf{Gen}$eration framework.\nKTGen leverages auxiliary information by training a correction network in the latent space produced by a VAE, aligning the generated data with the auxiliary information. \nOur experiments demonstrate that, when training on limited, biased data, incorporating auxiliary information makes the distribution of synthetic samples closer to the true data distribution, and also improves the performance of downstream models trained on the synthetic samples.", "tldr": "", "keywords": ["Tabular data generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9daa4802f466c09f5273a48aac19d663258053bf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces KTGen, a method to leverage auxiliary information (correlation between features and per feature distribution). The idea is to adapt TabSyn by adding a corrector network, which is trained to minimize a loss based on correlation and a loss based on feature-wise statistics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Importance of the problem: synthetic data generation is an important problem in the space of tabular data. Furthermore, the scarcity of observed data is also a realistic scenario that is worth tackling. Incorporating metadata or additional information is a promising research avenue to palliate this problem."}, "weaknesses": {"value": "Novelty: KTGen largely builds on the methodology introduced by Tabsyn. The contribution comes from the addition of the corrector network and the loss terms to satisfy intra feature correlations and feature wise statistics, which is not particularly novel.\n\nSoudness: Ideally, we would like the samples from the generator to have the same correlation structure as the provided one and the same marginals as in the additional information. Regarding the marginals, the losses introduced in Section 4.2 do not directly enforce this (L_dist). Indeed, they instead encourage the features of an individual sample to be close to the mode of the provided distribution. This problem comes from the fact that the loss is defined per sample, and is not a distance between two distributions (such as a Wasserstein distance).\n\nRealism of the assumption: the setup assumes that the data owner will still provide global per-column distributions, (cross-feature correlations, and mixture-model fits from a much larger candidate set. It’s unclear that organizations who refuse to share the raw data would give exact correlation matrices and GMM parameters.\n\nExperiments:  the paper does not compare against simple non deep-learning baselines that could possibly use this extra information, for example Gaussian copula variants. It is not clear if the 20 samples are actually useful, or if the feature distribution information + correlation structure are enough.\n\nAblations: there’s no study of how sensitive KTGen is to the lambda weights in the correction loss, to noise schedule choices in diffusion, to VAE capacity, or to the quality/accuracy of the provided statistics (e.g. what if the shared correlation matrix is noisy or partially redacted?). this makes it hard to assess robustness."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UR1JACXc6H", "forum": "pnw3FGpqzF", "replyto": "pnw3FGpqzF", "signatures": ["ICLR.cc/2026/Conference/Submission24941/Reviewer_TBjU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24941/Reviewer_TBjU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761598165615, "cdate": 1761598165615, "tmdate": 1762943253385, "mdate": 1762943253385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes KTGen, a framework for tabular data generation that leverages auxiliary knowledge beyond the raw training data. The method builds upon the TabSyn latent-diffusion backbone, introducing a correction network in the latent space trained to align synthetic data with external statistical knowledge. Specifically, KTGen uses two types of auxiliary information: (1) pairwise correlations among features and (2) marginal feature distributions modeled by Gaussian mixtures. The correction network minimizes the discrepancy between these statistics computed on generated samples and those provided by the knowledge source, while regularizing the latent deviation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-organized and easy to follow. \n\n2. The work targets a key limitation of current tabular generative models: their inability to exploit external knowledge that data providers often possess."}, "weaknesses": {"value": "1. **Overclaimed scope**\nThe paper categorizes external knowledge into unstructured textual and statistical types, but only implements the latter. No mechanism is provided for textual or semantic knowledge, making the claimed scope broader than what the method achieves.\n\n2. **Limited applicability of incorporated knowledge**\nKTGen only uses low-order statistics: pairwise correlations and marginal distributions, leaving open how the method could handle higher-order feature interactions, causal dependencies, or richer domain priors.\n\n3. **Potentially biased evaluation**\nThe main fidelity metrics (KL/Wasserstein per column, KS) directly correspond to the optimized objectives, naturally favoring KTGen. More comprehensive metrics such as feature correlation errors, C2ST, α-precision, or β-recall are not reported, so the overall generative fidelity and diversity remain unclear.\n\n4. **Unclear training data setup**\nAlthough the paper emphasizes the small-sample scenario, it does not clearly state the number of samples used for training the generator in each experiment. A varying sample size would clarify KTGen’s robustness and where its advantage diminishes."}, "questions": {"value": "1. How many training samples are used in Figure 3 and Table 2? \n2. Will the backpropagation of loss (4) occupy a lot of VRAM since multiple forward passes are involved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9NYhgqCu9C", "forum": "pnw3FGpqzF", "replyto": "pnw3FGpqzF", "signatures": ["ICLR.cc/2026/Conference/Submission24941/Reviewer_AK2p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24941/Reviewer_AK2p"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853762867, "cdate": 1761853762867, "tmdate": 1762943252970, "mdate": 1762943252970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a methods for tabular data generation that are augmented by human knowledge in the form of text and/or statistical information. Knowledge-enhanced Tabular data Generation (KTGen) is proposed which consists of a VAE with a diffusion model on the latent space and a correction model that aligns synthetic data with auxiliary information. The approach is evaluated on data from the UCI repository and scikit-learn and compared with several baseline models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The strengths of the paper include:\n- The focus on tabular data is practically and theoretically important. \n- The model is tested on a number of datasets. \n- The model is compared with several baseline models."}, "weaknesses": {"value": "The weaknesses of the paper include;\n- The intended technical contribution is unclear. The approach appears to be to train a constraint network on top of the standard VAE+diffusion model approach to synthesizing data. The innovation here does not seem significant enough to warrant publication. \n- The datasets are extremely modest. UCI and scikit-learn are not compelling in terms of either scale or realism."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mpmnoqjkYk", "forum": "pnw3FGpqzF", "replyto": "pnw3FGpqzF", "signatures": ["ICLR.cc/2026/Conference/Submission24941/Reviewer_vQ9t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24941/Reviewer_vQ9t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938367984, "cdate": 1761938367984, "tmdate": 1762943252721, "mdate": 1762943252721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes KTGen that improves synthetic tabular data quality by integrating external statistical knowledge into the generation process. KTGen combines a VAE that maps data to latent space, a score-based diffusion model that denoises latent representations, and a correction network that aligns generated samples with auxiliary knowledge such as feature dependencies or marginal statistics. The model is designed for settings where real data are limited or biased, but aggregated statistical information remains accessible. Experiments on eight datasets show that KTGen produces higher-quality synthetic samples and stronger downstream task performance than existing tabular generation baselines under biased and small-sample conditions."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- $\\textbf{Novel perspective}$: \nKTGen introduces a clear and meaningful direction for knowledge-enhanced tabular data generation, bridging the gap between purely data-driven models and knowledge-guided generative approaches.\n\n\n- $\\textbf{Methodological clarity}$: \nThe paper is well-structured; the distinction between semantic-level and data-level knowledge is conceptually neat, and the correction network is technically sound."}, "weaknesses": {"value": "- $\\textbf{Fairness of comparison}$: \nEven in the “unbiased” setting, KTGen accesses global statistical summaries derived from the full dataset, whereas baselines only see the limited training subset. This introduces an inherent information advantage and makes the performance comparison not strictly fair.\nEven considering the setting assumed by the paper, where only limited data are available but certain statistical characteristics of the full dataset can be accessed, the comparison is not entirely fair, because the baseline models were not designed to operate under such knowledge-available but data-restricted conditions, and thus cannot leverage the same type of auxiliary information that KTGen uses.\nHowever, the idea is promising, and if it shows strong performance under fair conditions, I would raise my score.\n\n- $\\textbf{Unclear evaluation scope}$: \nThe paper does not explicitly separate experiments that test the generative quality from those that test knowledge-utilization benefit, which complicates interpretation.\n\n- $\\textbf{Missing baselines with knowledge access}$: \nNone of the baseline models are adapted to the “knowledge-available” assumption; therefore, we cannot isolate whether KTGen’s gain comes from its architecture or from richer priors."}, "questions": {"value": "Have you tested KTGen under domain-shift conditions to verify its ability to generalize beyond the training distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no ethic concerns"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tB9i9goURR", "forum": "pnw3FGpqzF", "replyto": "pnw3FGpqzF", "signatures": ["ICLR.cc/2026/Conference/Submission24941/Reviewer_QDhf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24941/Reviewer_QDhf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006182057, "cdate": 1762006182057, "tmdate": 1762943252398, "mdate": 1762943252398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}