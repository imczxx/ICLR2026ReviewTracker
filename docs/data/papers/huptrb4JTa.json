{"id": "huptrb4JTa", "number": 21584, "cdate": 1758319286297, "mdate": 1763594484072, "content": {"title": "ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations", "abstract": "Neural theorem proving has advanced rapidly in the past year, reaching IMO gold-medalist capabilities and producing formal proofs that span thousands of lines. Although such proofs are mechanically verified by formal systems like Lean, their excessive length renders them difficult for humans to comprehend and limits their usefulness for mathematical insight. Proof simplification is therefore a critical bottleneck. Yet, training data for this task is scarce, and existing methods—mainly agentic scaffolding with off-the-shelf LLMs—struggle with the extremely long proofs generated by RL-trained provers. We introduce ProofOptimizer, the first language model trained to simplify Lean proofs without requiring additional human supervision. ProofOptimizer is trained via expert iteration and reinforcement learning, using Lean to verify simplifications and provide training signal. At inference time, it operates within an iterative proof-shortening workflow, progressively reducing proof length. Experiments show that ProofOptimizer substantially compresses proofs generated by state-of-the-art RL-trained provers on standard benchmarks, reducing proof length by 87% on miniF2F, 57% on PutnamBench, and 50% on Seed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check faster in Lean and further improve downstream prover performance when reused as training data for supervised finetuning.", "tldr": "we train a model to simplify AI-generated Lean proofs", "keywords": ["ai for math", "proof simplification"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c2bf9188cfdb421f6f2780fc5cfbdcf3f0af9b1.pdf", "supplementary_material": "/attachment/be4475a6a45fe5a7a4b16a65aecfc99950220f31.zip"}, "replies": [{"content": {"summary": {"value": "ProofOptimizer is a system that optimizes the length of long Lean proofs, such as those produced by SOTA prover models. ProofOptimizer is the first work to use reinforcement learning to tackle this problem setting, and they show by comprehensive experiments that they improve upon existing baseline, reaching substantial proof reduction."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "ProofOptimizer is the first work to use RL to shorten formal proofs. The experiments in Section 4 conclusively show the advantage of this method over existing baselines such as Gemini-2.5-Pro on a pipeline like that of ImProver.\n\nI am convinced by the analysis in Section 5 that optimized proofs are easier to learn for a model, and faster to compile. The former is important in improving the data quality in existing SFT datasets for Lean, and the latter is important considering the multi-day runs that AlphaProof, Seed-Prover, and Aristotle required for IMO problems. These establish the importance of the problem setting, which has not been analyzed in prior work."}, "weaknesses": {"value": "The training distribution seems limited to high-school competition problems, which seems to limit the usability of ProofOptimizer for integrating long proofs of other types of results into libraries like Mathlib, as authors mention on L43–44."}, "questions": {"value": "While proofs produced by Seed-Prover are very redundant and, to a human, look easy to simplify, I am very curious whether the ProofOptimizer pipeline can also be applied to human-written proofs like those in Mathlib, or helping verbose, model-written proofs (like those written by Aristotle) be merged into Mathlib, which is a key factor limiting such models. Relatedly, I am curious whether the model could do better by training on data outside Goedel-Pset or proofs written by Goedel-Prover-V2, such as human-written proofs or diffs in Mathlib commits. I recognize this may be a big ask and I don’t expect a complete answer in the author rebuttal stage.\n\nReduction @64x2 seems much better than @128 in Figure 3. Is there any further analysis on the tradeoff between iteration count and k? Relatedly, why the specific schedule of 6 iterations of k = 64 and 2 iterations with k = 1024?\n\nI am interested in how the Lean-specific tokenizer works specifically, because it seems central to interpreting the numbers like min@k and for reproducibility. Or if the code is not long, is there code for the tokenizer? (You can probably put this as a small explanation in the Appendix)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CZcpftakNn", "forum": "huptrb4JTa", "replyto": "huptrb4JTa", "signatures": ["ICLR.cc/2026/Conference/Submission21584/Reviewer_As7x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21584/Reviewer_As7x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678447987, "cdate": 1761678447987, "tmdate": 1762941844773, "mdate": 1762941844773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ProofOptimizer, the first LLM designed to simplify formal proofs in the Lean theorem-proving system without relying on human-annotated simplification data. Addressing a critical bottleneck in neural theorem proving, where state-of-the-art (SoTA) reinforcement learning (RL)-trained provers (e.g., Seed-Prover, Goedel-Prover-V2) generate correct but excessively long, inscrutable proofs. ProofOptimizer integrates three core components: a symbolic Lean linter to remove redundant steps, a 7B-parameter LM fine-tuned for simplification, and an iterative inference-time workflow for progressive shortening. Trained via expert iteration (STaR-like iterative refinement using verified simplifications as training data) and online RL (rewarded for proof length reduction while preserving correctness), ProofOptimizer achieves promising results on miniF2F and PutnamBench with shortened proofs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Address an important issue**: The paper targets proof simplification, which is an under-addressed problem in neural theorem proving. Unlike prior work (e.g., ImProver) that relies on agentic scaffolding around off-the-shelf LLMs and fails on long RL-generated proofs, ProofOptimizer is the first LLM trained specifically for simplification.\n- **No Human Demonstrations**: By leveraging Lean’s mechanical verification to generate training signals (via expert iteration) and RL rewards (based on length and correctness), the model avoids the scarcity of human-annotated proof-simplification pairs, making its approach scalable and practical.\n- **Strong Empirical Performance**: The results are compelling and well-validated across benchmarks.\n- **Multiple Practical Benefits**: The paper goes beyond length reduction to demonstrate downstream value: simplified proofs speed up Lean execution and improve supervised fine-tuning of base provers."}, "weaknesses": {"value": "- **Diversity Collapse in RL**: The paper acknowledges that RL training (ProofOptimizer-RL) improves single-sample performance (@1) but reduces multi-sample diversity (@32), limiting the model’s ability to explore alternative simplifications. This tradeoff is not fully resolved, and no mitigation strategies are proposed.\n- **Computational Cost**: The iterative shortening workflow and RL training are computationally expensive: the authors note ~3000 H100 GPU hours per IMO problem. This high cost may limit adoption, especially for smaller research teams, and the paper does not discuss efficiency optimizations.\n- **Dependence on Lean**: ProofOptimizer is designed exclusively for Lean, with no discussion of adaptability to other formal systems (e.g., Isabelle, Coq)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "39puQcY7EW", "forum": "huptrb4JTa", "replyto": "huptrb4JTa", "signatures": ["ICLR.cc/2026/Conference/Submission21584/Reviewer_gPDN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21584/Reviewer_gPDN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821612631, "cdate": 1761821612631, "tmdate": 1762941844198, "mdate": 1762941844198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces ProofOptimizer, a 7B LLM for Lean proof shortening trained from Qwen2.5-7B-Instruct through the following stages:\n(1) Fine-tuning on five Lean-related tasks \n(2) Expert Iteration (with a 0.8len(x) threshold as the filter) on the Proof Simplification dataset built by collecting and extracting theorems from Goedel-Pset and generating proofs with Goedel-Prover-V2-32B \n(3) RL using a variant of GRPO with a reward $R(x, y) = max (|y|/|x| - 1, 0)$ for shorter proof $y$ given the prompt proof $x$.  \n\nWhen further Integrated with several inference-time techniques, ProofOptimizer manages to significantly reduce the length of the proofs generated by Goedel-Prover-V2-32B by 72.5% and 23.8% on miniF2F and PutnamBench, respectively. Besides, the ProofOptimizer can be used to optimize the training data for fine-tuning a theorem prover to achieve 2% performance gain."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The proof shortening performance of ProofOptimizer is impressive. Seed-Prover’s IMO 2025 proofs are shortened by 50%, which makes the proof much easier for human to go through.\n\n- The task and all the techniques used in the pipeline are clearly stated, and the results are presented in the easiest way for the readers to acquire and understand.\n\n- The performance evaluation and ablation studies are comprehensive. Almost all the questions I came up during reading are answered in the paper."}, "weaknesses": {"value": "1. The overall conceptual novelty of this work is limited. The task of shortening Lean proof can be seen as a code simplification task on the functional programming language Lean. The main contribution is on engineering implementation. \n\n2. The significance of pursuing the \"shortest proof\" is not entirely clear. In the introduction, the authors claim that \"RL-trained provers often generate proofs that are correct but excessively long and inscrutable\", where I do not agreee with the last \"inscrutable\" description. Both Kimina-Prover and DeepSeek-Prover-V2 follow a declarative proof style, so do the subsequent Goedel-Prover-V2 and Seed-Prover. Compared to the inscrutable imperative(or procedural) style proofs such as the IMO-2024 solutions made by AlphaProver, declarative style proofs explicitly claim every intermediate goal in the proof, which are understandable for people who are familiar with the basic grammars of the formal language. According to Figure 11, a large part of the red@k improvement of ProofOptimizer comes from using a single tactic to substitute the whole proof, which could adversely reduce the readability of the proof.\nIn my opinion, the ideal proof rewriter should do more than shortening the proof. It should not only remove all reduntant tactics, but also make sure the proof granularity is not more coarse-grained than typical natural langauge proofs by human. In other words, advanced tactics that omit necessary details should not be used in the rewrite."}, "questions": {"value": "1. What do you think is the \"best\" proof for a theorem in the formal system? In my opinion, since a single tactic can comprise complex constructions of proofs (such as brute force tactics mentioned in section 5.2) and can thus be time-costing, it seems to me that shorter execution time is a better metric for evaluating a proof than the proof length. Can you share your understanding on this topic?\n\n2. For non-thinking models, we can directly shorten the proofs in the training data and fine-tune the model on the shortened data, as section 5.1 does. What shall we do when it comes to thinking models that only output the proof at the end of the response?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PXuZBhAC7u", "forum": "huptrb4JTa", "replyto": "huptrb4JTa", "signatures": ["ICLR.cc/2026/Conference/Submission21584/Reviewer_vYF8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21584/Reviewer_vYF8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915457773, "cdate": 1761915457773, "tmdate": 1762941843884, "mdate": 1762941843884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The goal of this work is to shorten LLM generated proofs, which can be long and difficult to understand. This work trains a new LM to shorten a given proof, using reinforcement learning. The reward signal is a mixture of the Lean correctness reward and the length of the new proof output by the model.\n\nFirst, this work introduces a Lean linter, which removes some obviously inefficient portions of proofs. Afterwards, to train an LM to shorten proofs, they use expert iteration, or RL. The initial model is Qwen-2.5-7B-Instruct, which is then further trained on mathematical/formal theorem proving-related tasks before expert iteration/RL. The training dataset is constructed using some problems from Goedel-Pset. The problems’ natural language solution is turned into a proof sketch. Then, each step in the proof sketch is used as a theorem (and the solution is given by Goedel-Prover-V2-32B), resulting in 145K theorem-proof pairs.\n\nLet M be the proof-shortening model. Within each round of expert iteration, for each theorem-proof pair, several shorter versions of the proof are sampled from M, and the shortest correct proof is included in the SFT dataset within this round of expert iteration. The length of this proof should be less than 80% the length of the original proof, so that the model will learn some non-trivial simplifications. In reinforcement learning, the reward is the relative shortening (e.g. 0.1 if the proof is 10% shorter) if the proof is correct and 0 otherwise. The algorithm is GRPO, with no advantage normalization.\n\nAfter performing 3 rounds of expert iteration (obtaining ProofOptimizer-ExpIt), the proof length with 1 generation, and minimum proof length among 32 generations, both improve significantly. However, when performing RL starting from 2 rounds of expert iteration, min@1 improves compared to ProofOptimizer-ExpIt, but the min@32 is worse. Their proof shortening models (both with expert iteration and RL) significantly outperform Gemini-2.5-Pro.\n\nAdditionally, this work finds that iteratively shortening the proof using repeated applications of their model - in each round, they generate several proofs and take the shortest one. There is a 57.2% proof length reduction on Putnam Bench, though some of the longer proofs are more difficult to simplify. There is a significant improvement from using more samples per iteration. Using a large sampling budget, there is around 40-50% length reduction on Seed Prover’s IMO proofs, using iterative applications of the proof-shortening model trained with expert iteration.\n\nThis work also finds that training on simplified proofs can help obtain better theorem proving models - when training their base model on proofs generated by Goedel-Prover-V2, they find that applying the proof-shortening model to these proofs before SFT can improve pass@32 on miniF2F. An additional benefit is that these proofs can execute/be verified more quickly."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Results with expert iteration are strong, as well as with iterative shortening of proofs using the expert iteration model."}, "weaknesses": {"value": "1. The @1 metrics with RL are strong, but the @32 metrics with RL are not much better than the @1. So this may be a disadvantage since the @32 metrics also tend to be important (since it is possible to use parallel sampling together with the Lean verifier)."}, "questions": {"value": "1. Line 158 - is there a typo in the definition of relative shortening, i.e. should be (|x| - |y|)/|x| instead?\n2. Why do you believe that min@32 is not significantly different from min@1 with RL? Is this due to an entropy collapse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GzTamsSbE7", "forum": "huptrb4JTa", "replyto": "huptrb4JTa", "signatures": ["ICLR.cc/2026/Conference/Submission21584/Reviewer_524q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21584/Reviewer_524q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762238943150, "cdate": 1762238943150, "tmdate": 1762941843630, "mdate": 1762941843630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}