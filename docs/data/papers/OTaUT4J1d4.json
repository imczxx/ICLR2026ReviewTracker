{"id": "OTaUT4J1d4", "number": 3950, "cdate": 1757572480011, "mdate": 1763543798002, "content": {"title": "SpecExit: Accelerating Large Reasoning Model via Speculative Exit", "abstract": "Despite their strong performance on reasoning tasks, large reasoning models (LRMs) often suffer from overthinking, producing unnecessarily long outputs and incurring high end-to-end latency, a significant limitation to their real-world deployment. To address overthinking, early-exit mechanisms have been proposed to terminate reasoning before typical completion, showing that this approach can effectively shorten generation length with minimal impact on accuracy. However, their reliance on probing mechanisms introduces a detection overhead that limits their end-to-end latency gains and compromises their generalizability across diverse problems. Inspired by the use of hidden states in speculative decoding, we propose \\textbf{SpecExit}, a novel framework that predicts both future tokens and an early-exit signal directly from a lightweight draft model without probing overhead. Our method offers significant improvements, achieving up to 66\\% generation length reduction and 2.5× end-to-end speedup compared to the speculative decoding baseline, without compromising accuracy. Our method leverages the inherent signals from hidden states to provide effective early-exit signals, suggesting broader use of hidden states for efficient reasoning. Our code is available at: https://anonymous.4open.science/r/SpecExit-B802.", "tldr": "", "keywords": ["efficient reasoning", "speculative decoding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4e4c3c97917d248454702fe27bd4ac2a12243d1b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SpecExit, an early-exit framework that integrates with speculative decoding to predict both future tokens and reasoning sufficiency signals from a lightweight draft model, achieving up to 2.5× faster inference and 66% shorter reasoning without accuracy loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Elegant integration of early-exit prediction into speculative decoding with zero probing overhead.\n- Uses hidden-state signals (confidence/progress/remaining) to dynamically determine reasoning sufficiency.\n- Demonstrates consistent 2×–2.5× speedup and 66% token reduction without hurting accuracy.\n- Framework is general, modular, and easily deployable on vLLM or PyTorch inference pipelines.\n- Well-designed ablation studies confirm the effectiveness of multi-signal fusion and signal smoothing."}, "weaknesses": {"value": "- Model-specific training requirement: The MTP-based signal extraction head must be retrained for each target model due to differences in hidden-state representations, limiting plug-and-play generality.\n- Limited model family evaluation: Experiments are conducted only on two reasoning model families, leaving generalization to other architectures or general-purpose LLMs unclear.\n- Signal & smoothing dependency: The method relies on multiple signals and smoothing strategies (e.g., EWMA) to remain stable, indicating insufficient robustness of raw signals."}, "questions": {"value": "- Since the MTP and signal heads must be retrained for each model due to architectural differences, how costly is this process in practice? Could a lightweight or partially shared training strategy (e.g., freezing most of the MTP layer or distilling from a pretrained head) reduce the retraining overhead across models?\n-  Can the authors provide results or at least discussion on how SpecExit might generalize to other model architectures, such as Phi, Mistral, or LRMs?\n- How sensitive is SpecExit’s performance to the chosen thresholds for confidence, progress, and remaining length? like use different thresholds to do some ablation studies. Why do you choose these thresholds in STEER method (like confidence > 0.8 && progress > 0.3 && remaining < 200)?\n- Can the authors identify and analyze typical scenarios where the control signals conflict (e.g., rising confidence but low progress), and show how the EWMA controller behaves in these situations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ACqJdl5THr", "forum": "OTaUT4J1d4", "replyto": "OTaUT4J1d4", "signatures": ["ICLR.cc/2026/Conference/Submission3950/Reviewer_8nc3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3950/Reviewer_8nc3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760670476475, "cdate": 1760670476475, "tmdate": 1762917109632, "mdate": 1762917109632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work introduces SpecExit, a reasoning-aware early exit framework for reducing the overall latency of the reasoning process. The method involves using a combination of speculative decoding and multi-task learning, to generate confidence signals from the draft model which guide the system to pre-empt the reasoning process eﬃciently. Evaluation across a range of tasks, the work claims to achieve 66% reduction in average generation length and 2.5x speedup in end-to-end latency, compared to the baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper combines speculative decoding, multi-task learning, and confidence-based exit criteria in a coherent way to address reasoning efficiency.\n\n2. Demonstrates significant inference latency reduction with minimal accuracy degradation across tasks.\n\n3. The analysis of different signal types (Confidence / Progress / Remaining Reasoning Length) and smoothing methods (No Smoothing / Momentum / Sliding Window / EWMA) is thorough and informative.\n\n4. The method offers a general framework applicable to existing reasoning models without modifying their core architecture."}, "weaknesses": {"value": "- Line 267: The notion of “recover token” is unclear. How does it differ from the reasoning-end marker  (\"\\</think>\")?\n- Lines 363–364: The variant using combined signals (Confidence + Progress + Remaining Length) is denoted as SpecExit*, yet Table 1 reports results for SpecExit. Which configuration do these results correspond to?\n- Table 1: Ambiguity exists between “Think” (line 333) and “Vanilla” (line 339). If they refer to the same baseline, HUMANEVAL+ accuracies (90.9 vs 88.4) are inconsistent.\n- Table 1 caption should define what boldface indicates (best overall or best among comparable methods).\n\n---\n- Lines 313–314 mention that the speculative decoding baseline is trained on the same dataset as EAGLE3, but the draft model configuration is unspecified.\n- What is the relationship between the draft and target models (same family, smaller variant, or identical)? Without this detail, reproducibility and interpretation of results are limited.\n\n---\n- Please quantify additional compute (e.g., FLOPs or GPU hours) for:\n    - Dataset construction, which involves multiple reasoning traces per prompt to determine optimal lengths.\n    - Multi-task learning with additional loss heads.\n- These costs should be weighed against runtime improvements.\n---\n\n- Does the target LLM architecture affect training of the MLP heads? Since the dataset is derived from a specific model’s reasoning traces, how is overfitting avoided? Are exit heads transferable across models within the same family?\n---\n\n- The inference flow involving the MTP mechanism, lightweight draft model, MLP head, and target model is confusing. Please clarify whether:\n    - The draft model includes MTP heads,\n    - These predictions are passed to the target for verification, and\n    - The target model then generates extended outputs via a linear layer.\n- A schematic or algorithmic diagram would make the inference pipeline clearer.\n---\n\n- What is the motivation for three auxiliary losses (L_conf, L_prog, L_rem). The rationale for choosing these specific losses is unclear. What intuition or empirical observation led to defining these signals? Figure 5 shows dataset-dependent variation for Progress and Remaining signals, while Confidence remains consistently low. What happens if only Progress and Remaining signals are used?\n- The abstract claims an “average generation length reduction of 66%,” but results suggest this is the maximum observed, not the mean. The phrasing should be corrected to “up to 66% reduction.”"}, "questions": {"value": "- Please provide detailed configurations of the draft model and its relation to the target model (architecture, size, tokenizer compatibility).\n\n- Can the MLP exit heads trained on one model generalize to another of similar architecture?\n\n- How large is the training overhead compared to runtime gains?\n\n- Could the authors clarify whether the method assumes the draft and target models belong to the same family?\n\n- Why were the three particular auxiliary losses (L_conf, L_prog, L_rem) chosen, and is there theoretical justification for their complementarity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kCgkDqYdOK", "forum": "OTaUT4J1d4", "replyto": "OTaUT4J1d4", "signatures": ["ICLR.cc/2026/Conference/Submission3950/Reviewer_UuSi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3950/Reviewer_UuSi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906826581, "cdate": 1761906826581, "tmdate": 1762917109372, "mdate": 1762917109372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SpecExit, a novel framework designed to accelerate inference in Large Reasoning Models (LRMs) by tackling the \"overthinking\" problem, where models produce unnecessarily long reasoning chains, leading to high latency. The core idea is to integrate an early-exit mechanism directly into the speculative decoding process. Unlike prior methods that rely on costly probing of the target model, SpecExit extends the lightweight draft model to predict not only future tokens but also three auxiliary signals: confidence, reasoning progress, and remaining reasoning length. These signals are learned via multi-task training on data where the minimal sufficient reasoning length is heuristically identified. During inference, these signals are used to dynamically terminate the reasoning process at natural breakpoints (e.g., paragraph ends) without any modification to the target model or extra probing overhead. Experiments on various reasoning benchmarks with models like Qwen3-4B and DeepSeek-R1-Distill show that SpecExit can reduce generation length by up to 66% and achieve end-to-end latency speedups of up to 2.5x compared to a standard speculative decoding baseline, with minimal impact on task accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The primary strength of this work lies in its novel and elegant integration of two distinct lines of research: speculative decoding for per-token acceleration and early-exit mechanisms for reducing reasoning length. Instead of treating these as separate problems, the authors propose a unified framework. The key insight—to offload the exit-signal prediction to the draft model and leverage its hidden states—is highly original. This avoids the well-documented overhead of probing-based early-exit methods, which is a creative and impactful conceptual leap."}, "weaknesses": {"value": "1. The author should introduce some basic knowledge of speculative decoding to facilitate readers who are not familiar with speculative decoding to understand the content of the paper.\n\n2. Figure 2 only shows the forward process of one token. I don't understand how to predict the vocab logits and signals of multiple tokens simultaneously. The author needs to explain in detail how the MTP layer in the paper is designed. Is it exactly the same structure as Medusa or EAGLE?\n\n3. The author combines speculative decoding with exit prediction, but I believe there is no coupling relationship between the two. According to the method designed in this paper, adding an MLP layer directly to the hidden feature of the token can also predict the exit signal without the need for an MTP layer.\n\n4. The author can supplement the experimental results without speculative decoding (only predicting the exit signal of each token)."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WtrS2jyFwt", "forum": "OTaUT4J1d4", "replyto": "OTaUT4J1d4", "signatures": ["ICLR.cc/2026/Conference/Submission3950/Reviewer_rpsy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3950/Reviewer_rpsy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991775220, "cdate": 1761991775220, "tmdate": 1762917109139, "mdate": 1762917109139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **SpecExit**, a *reasoning-aware early-exit framework* designed to reduce overthinking in large reasoning models (LRMs).\nInstead of relying on explicit probing phrases (e.g., “Final Answer is”), SpecExit leverages **hidden states** from a lightweight **draft model** to jointly predict (i) future tokens and (ii) early-exit signals—**confidence**, **reasoning progress**, and **remaining reasoning length**—in a single forward pass.\nThe method removes probing overhead, integrates smoothly with **speculative decoding**, and dynamically stops generation at semantically coherent boundaries such as paragraph delimiters.\nExperiments on GSM8K, MATH500, AIME, GPQA, HumanEval+, and ARC-Challenge show up to **66 % shorter reasoning chains** and **2.5 × latency reduction** without accuracy loss.\nAblation studies confirm that multi-signal integration with EWMA smoothing yields the best balance between efficiency and correctness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Novel integration of early-exit and speculative decoding via hidden-state supervision.\n* Eliminates probing overhead while maintaining accuracy.\n* Multi-signal design (confidence + progress + remaining length) improves robustness across tasks.\n* Experiments on six benchmarks and two reasoning-optimized backbones.\n* Ablations on signal types, smoothing strategies, and segmentation markers.\n* Reproducibility and open-source release fully documented.\n* Clear contextualization within reasoning-efficiency literature (DEER, EAGLE, RL-based control)."}, "weaknesses": {"value": "* Lacks a **formal theoretical analysis** of why hidden-state features correlate with reasoning sufficiency.\n* Dependence on explicit *<think>* / *</think>* markers may limit generalization to free-form reasoning.\n* Limited model family evaluation; Experiments are conducted on two reasoning model families."}, "questions": {"value": "1. How sensitive is SpecExit to the chosen thresholds (confidence > 0.8, progress > 0.3)? Could these be learned dynamically?\n2. Have you tested **cross-model transferability** of exit heads (e.g., trained on Qwen, applied to DeepSeek)?\n3. How would SpecExit behave with **long-context**?\n4. Could you quantify **energy / GPU-hour savings** corresponding to the reported 2.5× latency reduction?\n5. Can you provide a theoretical account—stating assumptions and guarantees—that explains why functions of intermediate hidden states serve as reliable proxies for ‘reasoning sufficiency’ (e.g., monotonicity/calibration of the confidence and progress heads)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "INPJl6mKzH", "forum": "OTaUT4J1d4", "replyto": "OTaUT4J1d4", "signatures": ["ICLR.cc/2026/Conference/Submission3950/Reviewer_4V7z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3950/Reviewer_4V7z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088894878, "cdate": 1762088894878, "tmdate": 1762917108908, "mdate": 1762917108908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}