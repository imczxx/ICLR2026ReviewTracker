{"id": "5rKsNFg1m9", "number": 15521, "cdate": 1758252281293, "mdate": 1759897301488, "content": {"title": "Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution", "abstract": "Recent advances in large language models (LLMs) have enabled agents to autonomously perform complex, open-ended tasks. However, many existing frameworks depend heavily on manually predefined tools and workflows, which hinder their adaptability, scalability, and generalization across domains. In this work, we introduce $\\textbf{Alita}$—a generalist agent designed with the principle of $\\textit{Simplicity is the ultimate sophistication,}$ enabling scalable agentic reasoning through $\\textit{minimal predefinition}$ and $\\textit{maximal self-evolution}$. For minimal predefinition, Alita is equipped with only one component for direct problem-solving, making it much simpler and neater than previous approaches that relied heavily on hand-crafted, elaborate tools and workflows. This clean design enhances its potential to generalize to challenging questions, without being limited by tools. For $\\textit{Maximal self-evolution}$, we enable the creativity of Alita by providing a suite of general-purpose components to autonomously construct, refine, and reuse external capabilities by generating task-related model context protocols (MCPs) from open source, which contributes to scalable agentic reasoning. Notably, Alita achieves 72.73\\% pass@1 and 86.06\\% pass@3 accuracy, which ranks top 1 among all open-source frameworks temporarily, on the GAIA benchmark, 74.00\\% and 52.00\\% pass@1, respectively, on Mathvista and PathVQA, outperforming many agent systems with far greater complexity. Our code is open-sourced.", "tldr": "", "keywords": ["Generalist agent", "Self-evolution"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7d71a761bfc3ee6d3bca9fcc854f695a3b71ea5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Alita, a generalist agent system built on the idea of \"minimal predefinition + maximal self-evolution.\" Instead of packing the agent with tons of pre-built tools like most systems do, Alita starts with just a web browsing agent and creates tools on-the-fly using Model Context Protocols (MCPs). On the GAIA benchmark, it gets 72.73% pass@1 accuracy, beating OpenAI's Deep Research at 67.36%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear idea, small starting system. The “minimal preset + self-evolution” design cuts manual plumbing and avoids a big tool zoo.\n\n- Good results. On GAIA, Alita beats many strong baselines; results on MathVista and PathVQA are also competitive.\n\n- Transfer to other stacks. Reusing Alita’s MCP tools inside another agent gives clear gains, which suggests a nice path for sharing skills across systems."}, "weaknesses": {"value": "- Scalability over time is untested. As the MCP store grows, retrieval speed, conflicts, and tool choice errors may rise. No long-run or high-load study.\n\n- Ablations are missing. We don’t know how much each module (brainstorming, web search, env recovery, retries) actually helps.\n\n- Key method details are thin. How MCPs are checked, stored, deduped, and retrieved is not clear enough to reproduce or scale.\n\n- Safety and compliance. The agent searches the web, pulls code, and runs it. The paper does not detail sandbox rules, license checks, data contamination, or supply-chain guards.\n\n-"}, "questions": {"value": "- What about cost? Pass@3 means 3x cost. If baselines had the same budget, would Alita still win?\n\n- Do Alita-made MCP tools help on tasks beyond GAIA (e.g., other domains or data sources)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vDEZHJnCCH", "forum": "5rKsNFg1m9", "replyto": "5rKsNFg1m9", "signatures": ["ICLR.cc/2026/Conference/Submission15521/Reviewer_RaUK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15521/Reviewer_RaUK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901802932, "cdate": 1761901802932, "tmdate": 1762925803997, "mdate": 1762925803997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a generalist agent framework that emphasizes two design principles: Minimal Predefinition (using only one core capability, the web agent, and a small set of general-purpose tools) and Maximal Self-Evolution (enabling the agent to autonomously construct, refine, and reuse external capabilities). Instead of relying on hardcoded tools or workflows, it generates and manages new tools by itself through “MCP brainstorming”. The generated MCPs can be cached for future reuse. The proposed method achieves superior results on GAIA, MathVista, and PathVQ datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Simple design and flexibility. The framework only uses a small set of tools, avoiding labour-intensive tool definition. This also makes it more flexible in different domains. \n2. The method outputs baselines on several benchmarks while being less complex. The authors also showed that the generated MCPs (tools) can be reused in other scenarios, such as other agent frameworks, smaller LLMs."}, "weaknesses": {"value": "1. Although the framework does not rely on predefined tools, it still needs to generate task-specific tools and manage the resulting tool set. Therefore, the advantage of simplicity is only evident at the initial stage. In essence, compared with other agent frameworks, this approach merely adds a tool creation module. From this perspective, the contribution and novelty of the method are rather incremental.\n\n2. The experiments are not sufficiently thorough. Although the proposed framework achieves strong results, there is no analysis explaining why the improvement occurs. For example, the key highlights of the framework are tool creation and reuse. It would be great to analyze whether the generated tools are actually better or more functional than the predefined ones across different tasks, and whether the model effectively reuses these generated tools. Such analyses should be included in the paper to substantiate the effectiveness of the proposed approach."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tT7L642aBI", "forum": "5rKsNFg1m9", "replyto": "5rKsNFg1m9", "signatures": ["ICLR.cc/2026/Conference/Submission15521/Reviewer_Bw6M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15521/Reviewer_Bw6M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936638699, "cdate": 1761936638699, "tmdate": 1762925803544, "mdate": 1762925803544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Alita, a generalist agent framework designed to overcome the scalability and adaptability limitations of agents that rely on extensive, manually predefined tools. Alita is built on two principles: \"Minimal Predefinition,\" where the agent starts with only a core set of components (like a web agent), and \"Maximal Self-Evolution,\" where it autonomously creates, refines, and reuses new capabilities as needed.\nAlita's workflow involves a Manager Agent that identifies a capability gap, brainstorms a new tool, and uses a Web Agent to find relevant open-source code. It then generates a script, tests it in a virtual environment, and encapsulates the new, successful tool as a \"Model Context Protocol\" (MCP) for storage and future reuse. The authors evaluate Alita on the GAIA, Mathvista, and PathVQA benchmarks, showing strong performance. They also demonstrate that the MCPs generated by Alita can be reused to improve the performance of other agents and smaller, less capable LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "S1. The core design principle of \"Minimal Predefinition and Maximal Self-Evolution\" is an elegant and original contribution that addresses the reliance on manually-defined tools in agent development.\n\nS2. Alita achieves nice performance, outperforming several baselines on the GAIA benchmark and also showing good results on Mathvista and PathVQA.\n\nS3. The auto-generated MCPs are not just useful for Alita but can be exported to improve other agents and distill reasoning capabilities from large LLMs to smaller ones is a significant finding. This is especially true for the dramatic improvement seen on difficult (GAIA Level 3) tasks.\n\nS4. The paper is well-written and easy to understand."}, "weaknesses": {"value": "W1. The framework highly relies on the coding and reasoning abilities of top-tier LLMs (Claude-3.7-Sonnet and GPT-4o). The results in Table 4 show that when a smaller model (GPT-4o-mini) is used to generate MCPs, the performance drops drastically. This suggests the \"minimal predefinition\" approach is not yet practical without access to the powerful models (especially powerful coding models).\n\nW2. The paper does not provide an analysis of the cost of MCP creation. How many tokens, how much wall-clock time, and how many self-correction attempts  are required, on average, to generate a new, functional MCP?\n\nW3. While the average GAIA score is high, the bar chart in Figure 1 shows Alita (46.15%) performing slightly worse than OpenAI DeepResearch (47.60%) on Level 3 tasks. This point is not discussed, but it might suggest that for the most complex problems, manually-engineered tools still have an edge.\n\nW4. There is a lack of newer baselines, such as the Tongyi Deep Research series (WebSailor, etc.)."}, "questions": {"value": "Q1. Could the authors clarify the discrepancy in the GAIA pass@1 score? The abstract states 75.15% , but Figure 1 and Table 1 show 72.73%. Which number is correct?\n\nQ2. What is the computational and token cost of the \"MCP Creation\" loop? Could the authors provide data on the average number of attempts, tokens, or time taken to successfully generate and validate a new MCP for different tasks?\n\nQ3. The workflow relies on the Web Agent finding useful open-source libraries. Does the timeliness of the searched documents affect the fairness of the experimental test? How effective is the \"open-source searching\" step in finding the necessary (and up-to-date) code or documentation?\n\nQ4. Alita’s performance across GAIA difficulty levels shows interesting non-dominance: it is outperformed on Level 1 (by OWL/A-World) and Level 3 (by OpenAI DeepResearch). Could the authors provide an analysis of how task difficulty and type influence the effectiveness of Alita's self-evolution mechanism. For instance, is the MCP creation overhead detrimental to solving simple Level 1 tasks, or do the complex requirements of Level 3 tasks simply exceed the current generation capability of the LLM?\n\nQ5. The limitations section correctly highlights the reliance on powerful LLMs for coding. To broaden the practical scope, could the authors provide a more detailed ablation or analysis exploring the impact of using different classes of models (specifically including open-source models like Llama 3 or Qwen 3) for the critical coding and reasoning steps within the MCP creation loop?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ObhKArSf4B", "forum": "5rKsNFg1m9", "replyto": "5rKsNFg1m9", "signatures": ["ICLR.cc/2026/Conference/Submission15521/Reviewer_WgV5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15521/Reviewer_WgV5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762175217900, "cdate": 1762175217900, "tmdate": 1762925803105, "mdate": 1762925803105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}