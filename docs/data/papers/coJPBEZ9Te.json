{"id": "coJPBEZ9Te", "number": 12179, "cdate": 1758206196592, "mdate": 1759897527023, "content": {"title": "Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions", "abstract": "A central goal of cognitive modeling is to develop models that not only predict human behavior but also provide insight into the underlying cognitive mechanisms. While neural network models trained on large-scale behavioral data often achieve strong predictive performance, they typically fall short in offering interpretable explanations of the cognitive processes they capture. In this work, we explore the potential of pretrained large language models (LLMs) to serve as dual-purpose cognitive models--capable of both accurate prediction and interpretable explanation in natural language. Specifically, we employ reinforcement learning with outcome-based rewards to guide LLMs toward generating explicit reasoning traces for explaining human risky choices. Our findings demonstrate that this approach produces high-quality explanations alongside strong quantitative predictions of human decisions.", "tldr": "We applied reinforcement learning with outcome-based rewards to post-train large language models to elicit explanations and predictions for human risky choice.", "keywords": ["cognitive modeling", "verbal theory", "risky choice", "group-relative policy optimization", "supervised fine-tuning", "large language model"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/854c51721407a30c9562e3a2fad5530c4b2f9a71.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the capabilities of LLMs in explaining human decision-making in risky choice scenarios. They compare three methods: a standard SFT, a variation of SFT used in the Centaur model (where only human data tokens affect the loss), and an RL-based approach where the GRPO algorithm is applied to model-generated candidates. The core idea is that CoT can be used to derive explanations for the predicted choices. The experimental results indicate that all three methods achieve comparable performance in terms of predictive power. The paper then analyzes the CoT streams of the RL-based method and utilizes them to characterize cognitive mechanisms in the risky-choice task. Finally, controlled experiments show that the RL-based approach can also learn explanations for synthetic expected-value-maximization choice data, and that the weaker backbone model leads to weaker performance on the RL method compared to the SFT alternatives."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and addresses an important and timely topic at the intersection of AI, behavioral economics, and cognitive sciences, which is likely to be of interest for a large share of the ML community.\n\n2. The use of CoT for explaining human choice is an elegant idea with potential applications that extend well beyond the scope of this paper, making it a promising direction to pursue.\n\n3. The experiments are comprehensive and rigorous, leading to important insights on the capabilities of LLMs not only in predicting but also in explaining human choice and behavior."}, "weaknesses": {"value": "While the paper offers a thorough examination of explaining human choices through various forms of LLM fine-tuning, it overlooks some natural alternative approaches to this problem, both in the literature review and in the experimental baselines.\n\nOne such approach would be to train a predictive ML model as a baseline predictor and then apply standard explainability techniques to identify which features drive its predictive performance. LLMs could still play a role in this framework - for example, in feature engineering or in interpreting feature importance measures. While I acknowledge that such methods may fall somewhat outside the paper’s primary scope, I would nonetheless expect some discussion of them: where might they fall short, and under what conditions might they provide sufficiently meaningful explanations?\n\nRelated work that could be referenced here includes Marantz & Plonsky (2025), who study the prediction of human choices in textually described lotteries, and Shapira et al. (2024, 2025), who explore data generation for improving human choice prediction in repeated persuasion games. Although these studies focus more on prediction than explanation, they nevertheless offer methodological insights into understanding decision-making in risky environments. For instance, comparing the performance of different predictive models can itself reveal patterns about how decisions are made. It would be valuable for the paper to discuss whether similar insights can be derived using the proposed approach, or whether the two approaches are fundamentally orthogonal. Such a discussion would significantly strengthen the paper’s contribution.\n\n**References**\n\nMarantz, E., & Plonsky, O. (2025). Predicting Human Choice Between Textually Described Lotteries.\n\nShapira, E., Madmon, O., Apel, R., Tennenholtz, M., & Reichart, R. (2025). Human choice prediction in language-based persuasion games: Simulation-based off-policy evaluation.\n\nShapira, E., Madmon, O., Reichart, R., & Tennenholtz, M. (2024). Can llms replace economic choice prediction labs? the case of language-based persuasion games."}, "questions": {"value": "The paper is concerned with explaining human choice in binary choice tasks at the population level. I wonder if you have any thoughts/ideas about how the RL-based approach could be extended beyond such settings, for instance:\n- predicted action is a high-dimensional object (e.g., free text) rather than a binary choice; and\n- predicting behavior at the individual level rather than in the population level.\n\nAdditionally, I encourage the authors to share their opinion regarding the points raised in the 'weaknesses' section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uDyrSyrSO3", "forum": "coJPBEZ9Te", "replyto": "coJPBEZ9Te", "signatures": ["ICLR.cc/2026/Conference/Submission12179/Reviewer_ABiG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12179/Reviewer_ABiG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760721499047, "cdate": 1760721499047, "tmdate": 1762923131897, "mdate": 1762923131897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to use RL to train language models to explain humans' cognitive behaviors. Compared to the previous approach that only focuses on the prediction accuracy, this paper prompts the language model to generate reasoning and prediction where the reasoning path can be used to explain human behaviors. The problem designs an outcome reward that checks whether the prediction is correct or not and uses GRPO to train the language model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The problem of explaining human decisions is interesting and worth exploring."}, "weaknesses": {"value": "I do not see the contribution of this paper. The outcome reward that compares prediction correctness is standard in RLVR, the GRPO is directly borrowed from literature, the \"step-by-step\" prompt is also directly borrowed from literature. In that sense, I do not see anything new or unique that is proposed by this paper. \n\nAdditionally, this paper focuses on the interpretability of human decisions, however, I do not see any special design for this purpose. Specifically, the paper proposes an outcome reward that checks prediction correctness, which is no different from the way where we only care about the prediction accuracy. One potential solution is that the paper can design some process reward to help supervise the reasoning/interpretability path."}, "questions": {"value": "Can the authors elaborate the unique design of this paper or special design for explaining human decisions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XbwolwhTTA", "forum": "coJPBEZ9Te", "replyto": "coJPBEZ9Te", "signatures": ["ICLR.cc/2026/Conference/Submission12179/Reviewer_Y1tw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12179/Reviewer_Y1tw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760970144203, "cdate": 1760970144203, "tmdate": 1762923131508, "mdate": 1762923131508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reinforcement learning (RL)–based method to model human behavior in risky decision-making scenarios. The authors adopt the GRPO framework to investigate the potential of large language models (LLMs) to both predict decisions and explain the underlying decision process."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strength:\n\n1 The topic is interesting, and to my knowledge, this is the first work that applies RL to analyze the risky decision of human behavior.\n\n2 The experiments are abundant.\n\n3 Figure 1 provides a clear and concrete example that effectively illustrates the task."}, "weaknesses": {"value": "Weakness:\n\n1 Although the paper integrates RL to explain human decision-making and defines a reward function in Formula (1), it lacks an in-depth analysis of the task. As a result, the contribution appears incremental, and the work reads more like an experimental report than a research paper.\n\n\n2  The paper leans more toward psychological or cognitive science research than computer science, as the main contributions involve cognitive interpretation rather than methodological innovation. Moreover, several cognitive science terms are used without sufficient explanation. For example, in the section “Cognitive mechanisms identified in CoT,” the term “cognitive mechanism” is not clearly defined."}, "questions": {"value": "Suggestions and Questions:\n\n1 I suggest that the authors provide clearer definitions and background information for the cognitive science terminology used in the paper, as noted in the weaknesses.\n\n2 Regarding Figure 5: how do you determine the function of each representation pattern identified by t-SNE clustering? Specifically, how do you validate that a subcluster corresponds to concepts such as “consider risk preference” or “final estimates when EV difference is large”?\n\n3 Could the authors provide a theoretical justification or formal analysis for the proposed reward decomposition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nAXa1CKwI3", "forum": "coJPBEZ9Te", "replyto": "coJPBEZ9Te", "signatures": ["ICLR.cc/2026/Conference/Submission12179/Reviewer_2MpK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12179/Reviewer_2MpK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762161064069, "cdate": 1762161064069, "tmdate": 1762923129326, "mdate": 1762923129326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel way in which LLMs can be used to aid cognitive science. They do RL fine-tuning on a Qwen model over a risky choices dataset and show that the fine-tuned model not only predicts choices better than cognitive models but also gives meaningful explanations for these choices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper has a clear goal, and that goal is well executed. I think this type of modeling and analysis efforts will be interesting for the cognitive science community. The evaluations and ablations are quite comprehensive. The Appendix, in particular, contains several insightful analyses. The two that are very important for the paper’s message are:\n\n- The ablation experiment in C.3, where the authors swap the CoT between the RL and the base models to show the importance of these traces. \n- The experiment in D.3., where the authors show doing RL-finetuning on different datasets, that either contain random choices or choices from other data generative processes, change the CoT dramatically.  \n\nI also appreciate that the authors share some failed attempts in the appendix, which will be valuable to the research community."}, "weaknesses": {"value": "I’m not convinced that RL is essential for this pipeline. The development of predictive and explanatory reasoning traces are attributed to RL by making comparisons to the base model. However, perhaps SFT (either Centaur style or full) is also sufficient to develop such traces. If this is the case, SFT may be preferred over RL fine-tuning given a) reduced computational costs during training and b) difficulties around getting RL to work, as the authors have also pointed out in section F.\n\n**If the authors make the following comparisons between SFT and RL and show the benefits of RL under these setups, I will increase my score to 8. Otherwise, I will keep it at 6**:\n\n- Generate reasoning traces from the SFT models. Plug these traces back into the base model. If RL is uniquely beneficial, the MSE of the SFT model here should be higher than 0.0212 (base model with RL reasoning traces).\n- Replicate Figure 6a with the SFT model. Again, if RL is uniquely beneficial, i would expect the ordering of the cognitive mechanisms to look different for the SFT model.\n\nMy doubts originate from the findings of [this paper](https://arxiv.org/abs/2501.11120), where the authors show that doing SFT over risky choices gives models awareness about these choices. Similarly, in your setup, doing SFT can already give the models insights about the data generative process.\n\nMinor:\n\n- Experiments described in Appendix D.3. are very interesting. However, the results are only described qualitatively in a few sentences. Can you share more structured evidence here? e.g. something like the t-SNE plot or Figure 6a?\n- At L 74, the text jumps too quickly into an example from the task. I’m familiar with the dataset, but for someone who is not, it would be helpful to have a sentence or two briefly describing what the task is. \n- L 160 has a typo: a outcome-based"}, "questions": {"value": "- I appreciate the discussion of the elicitation hypothesis, suggesting the RL approach may ultimately be bound by the knowledge of the base model. If this is the case, the model can only interpolate within the existing knowledge. If so, how useful is the approach you are suggesting beyond being a proof concept? You point out that RL can help bring theories from other disciplines into psychology to generate knowledge. Cognitive scientists have been doing this for decades, therefore this approach still runs into the same limitations you discuss. I'd be curious to get your thoughts on this.\n- In Figure 6a, the ordering and the proportions of different cognitive mechanisms seem to settle by the end of epoch 1. What else do you think is going on in the later parts of training that still allows the model to improve its predictions?\n- How are the summary sentences in Figure 5 generated? Did you use GPT-4.1 for this too?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k5zNvrjcDZ", "forum": "coJPBEZ9Te", "replyto": "coJPBEZ9Te", "signatures": ["ICLR.cc/2026/Conference/Submission12179/Reviewer_nJgt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12179/Reviewer_nJgt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762165582392, "cdate": 1762165582392, "tmdate": 1762923128695, "mdate": 1762923128695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}