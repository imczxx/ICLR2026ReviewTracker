{"id": "Cg9jgEq3lF", "number": 11142, "cdate": 1758190831190, "mdate": 1763465939980, "content": {"title": "Beyond Unified Directions: Context-adaptive Representation Steering for LLM Safety Alignment", "abstract": "Large language models (LLMs) face significant generative safety risks in deployment, and representation steering has emerged as a lightweight alternative to resource-intensive training-based safety alignment methods. However, existing representation steering approaches compute a unified steering direction, which fails to leverage context-specific information critical for precise safety alignment. To address this limitation, we propose \\textit{CA-Steer}, a context-adaptive representation steering method for LLM safety alignment. It computes a context-adaptive direction by retrieving contextually similar safe and unsafe representations as references. Besides, a sample-level steering gate is introduced to filter unnecessary operations, ensuring safety alignment without compromising LLM utility. Evaluations on three safety benchmarks and two utility benchmarks show that CA-Steer significantly outperforms existing baselines: it improves the vanilla LLM’s average safety score from 85.80\\% to 97.09\\% (surpassing the best baseline by 6.28 percentage points), and maintains nearly no utility loss. In-depth analyses further confirm the rationality of its design and its acceptable overhead.", "tldr": "", "keywords": ["Safety Alignment", "Representation Steering", "Context-adaptive"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac113230b6c94d8e0ca0fd94523fd6c5f1d0f60a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CA-Steer, a context-adaptive representation steering method for improving safety alignment in large language models (LLMs). Unlike prior steering approaches that apply a single global direction to all contexts, CA-Steer computes token-level context-specific steering vectors by retrieving contextually similar safe and unsafe representations from pre-built banks. It also introduces (1) a sample-level padding strategy to handle retrieval imbalance or emptiness and (2) a steering gate that conditionally applies steering only when the prompt is likely to be unsafe."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a clear limitation in current steering methods — the “one-size-fits-all” assumption — and introduces context-adaptive steering as a principled extension.\n2. Experiments are extensive and systematic, covering multiple open-source LLMs and diverse safety/utility benchmarks.\n3. Writing is clear, structured, and professional."}, "weaknesses": {"value": "1. Some experimental results are wrong. For example, in Table 1, ALERT benchmark for qwen with $\\text{prompt}_\\text{hand}$, the author reports the safety performance is 96.16%. However, the test dataset in ALERT has only 1,000 items. How can you get these results? I think the authors should check the results of your paper (Not only what I mentioned).\n\n2. The method is not novel.  The steering method has been widely used. The authors should cite these articles and add a statement of innovation."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6k0E7ufzrx", "forum": "Cg9jgEq3lF", "replyto": "Cg9jgEq3lF", "signatures": ["ICLR.cc/2026/Conference/Submission11142/Reviewer_XwJm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11142/Reviewer_XwJm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760837286895, "cdate": 1760837286895, "tmdate": 1762922311767, "mdate": 1762922311767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a context-adaptive representation steering framework (CA-Steer) for improving large language model (LLM) safety alignment. While prior methods typically apply a unified, dataset-level steering direction to all contexts, CA-Steer introduces a more fine-grained and dynamic approach. It retrieves token-level and sample-level contextually similar representations to compute adaptive steering vectors during inference, enabling more precise and effective safety control."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a lightweight method for steering LLM behaviour toward safer responses.\n2. The analysis showing that safe and unsafe representations form distinct clusters across different contextual settings is insightful."}, "weaknesses": {"value": "1. Limited generalizability. Since the steering vectors in CA-Steer are context-dependent, the method appears potentially data-intensive, requiring diverse and comprehensive representation banks that cover various contextual settings and risk types. \n- Could the authors discuss how CA-Steer scales when contextual coverage is limited, for instance, if certain risk categories or prompt styles are underrepresented in the safety dataset? \n2. Confusing design on computing the steering vector. The method computes a steering vector by retrieving contextually similar safe and unsafe representations for the same target token representation $h$. \n- How can $h$ be simultaneously similar to both safe and unsafe examples? Representation steering typically assumes that safe and unsafe representations are linearly separable, implying that a token should be close to either safe or unsafe examples but not both. \n- The authors report that, on ALERT, 55.1% of retrieved subsets are dominated by safe samples and 22.5% by unsafe ones. Does this suggest that most retrievals are inherently imbalanced and that balanced retrievals are relatively rare? If so, does this mean that computing a steering vector based on both safe and unsafe similar representations may not always be feasible or meaningful in practice?\n3. Concerns on the effectiveness of the padding strategy. In Section 4.2, the authors report that Global Padding outperforms Only Padding, even though Global Padding uses dataset-level mean representations while Only Padding uses context-specific samples. \n- Could the authors clarify why the global representation performs better than the local, context-dependent one?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tCZOIEZ75p", "forum": "Cg9jgEq3lF", "replyto": "Cg9jgEq3lF", "signatures": ["ICLR.cc/2026/Conference/Submission11142/Reviewer_oFFn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11142/Reviewer_oFFn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813244005, "cdate": 1761813244005, "tmdate": 1762922311431, "mdate": 1762922311431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method for LLM representation steering for inference-time safety. The key novelty behind the presented method is incorporating context information from the input prompt and partial response to compute the steering vector for each token. The paper also presents a simple gating mechanism that decides whether a token needs steering or not. Evaluation on safety and utility benchmarks shows improvements to the model safety and a minor degradation to the model utility (which is clearly demonstrated to be due to the introduced gating mechanism)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The presented method is simple and intuitive and is shown to introduce a relatively small compute overhead.\n\n2. The evaluation clearly demonstrates the effectiveness of the method compared to previous work."}, "weaknesses": {"value": "1. The safety evaluation is somehow limited: it is based on 3 benchmarks with all 3 models evaluated already are doing reasonably well on 2 of them. It would be interesting to demonstrate the effectiveness of the method against adversarial jailbreak attacks as well. That would be a stronger evidence on the robustness of the presented approach."}, "questions": {"value": "Would you please elaborate on the training data? its size, source, structure, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ROisHkc9vZ", "forum": "Cg9jgEq3lF", "replyto": "Cg9jgEq3lF", "signatures": ["ICLR.cc/2026/Conference/Submission11142/Reviewer_nZKu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11142/Reviewer_nZKu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762052719863, "cdate": 1762052719863, "tmdate": 1762922310982, "mdate": 1762922310982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Author Response to All Reviewers"}, "comment": {"value": "We thank all the reviewers for their insightful comments and helpful suggestions. We hope our responses and paper updates alleviate the concerns raised.\n\nFollowing the reviewers’ feedback, we have updated our manuscript mainly with the following contexts:\n\n- **Evaluation on adversarial jailbreak attacks** (Section 3.3-Line290): Following the suggestion from **Reviewer nZKu**, we supplemented experiments on adversarial jailbreak benchmarks to further enhance safety evaluation, and the consistent improvements also enhance the robustness of the method. \n- **Scalability under incomplete contextual coverage** (Section 3.4-Line307): Following the suggestion from **Reviewer oFFn**, we added a controlled experiment to explore CA-Steer's scalability under incomplete contextual coverage and validate its strong generalization.\n- **Typo correction and rechecking** (Table 1-Line231): Following the feedback from **Reviewer XwJm**, we corrected the manual typo (96.10 instead of 96.16), and implemented additional checks to prevent similar issues, which strengthens the manuscript’s accuracy.\n\nWe have highlighted the corresponding modifications in the manuscript with color blue for your convenience. Again, thank you for your hard work. We believe your input has already helped improve the paper and look forward to further engaging with you during the discussion. Please see our replies to each of you below."}}, "id": "341kzAhhyN", "forum": "Cg9jgEq3lF", "replyto": "Cg9jgEq3lF", "signatures": ["ICLR.cc/2026/Conference/Submission11142/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11142/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission11142/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763465917049, "cdate": 1763465917049, "tmdate": 1763466085873, "mdate": 1763466085873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}