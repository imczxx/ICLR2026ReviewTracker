{"id": "6mpv9kG81P", "number": 16272, "cdate": 1758262558003, "mdate": 1759897250829, "content": {"title": "Domain-Specific Data Synthesis for LLMs via Minimal Sufficient Representation Learning", "abstract": "Large Language Models have demonstrated remarkable progress in general-purpose capabilities and can achieve strong performance in specific domains through fine-tuning on domain-specific data. However, acquiring high-quality data for target domains remains a significant challenge. Existing data synthesis approaches follow a\ndeductive paradigm, heavily relying on explicit domain descriptions expressed in natural language and careful prompt engineering, limiting their applicability in real-world scenarios where domains are difficult to describe or formally articulate. In this work, we tackle the underexplored problem of domain-specific data synthesis through an inductive paradigm, where the target domain is defined only through a set of reference examples, particularly when domain characteristics are difficult to articulate in natural language. We propose a novel framework, DOMINO, that learns a minimal sufficient domain representation from reference samples and leverages it to guide the generation of domain-aligned synthetic data. DOMINO integrates prompt tuning with a contrastive disentanglement objective to separate domain-level patterns from sample-specific noise, mitigating overfitting while preserving core domain characteristics. Theoretically, we prove that DOMINO expands the support of the synthetic data distribution, ensuring greater diversity. Empirically, on challenging\ncoding benchmarks where domain definitions are implicit, fine-tuning on data synthesized by DOMINO improves Pass@1 accuracy by up to 4.63\\% over strong, instruction-tuned backbones, demonstrating its effectiveness and robustness. This work establishes a new paradigm for domain-specific data synthesis, enabling practical and scalable domain adaptation without manual prompt design or natural language domain specifications.", "tldr": "", "keywords": ["synthetic data", "representation learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d08e49a20a49559a87663e23ad52bca49be7fd0.pdf", "supplementary_material": "/attachment/41969f39aee3d2ef4514409fc648b252b26943e8.zip"}, "replies": [{"content": {"summary": {"value": "Domino is a synthetic data generation framework that learns a minimal sufficient representation of the generation task from a reference sample. Soft tokens representing both the domain and individual samples are learned, with a contrastive loss applied to the individual sample soft tokens to encourage the domain soft tokens to be minimal. For both coding and instruction-following domains, Domino generates training data that produces better models than baseline methods. Analysis on the coding domain also demonstrates that data generated via Domino better aligns with the full distribution of reference samples than baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clearly presented theory supports the formulation of the framework.\n- Strong results demonstrate training base models with Domino generated data outperforms training with baseline methods, and beats the model provider’s instruction-tuned version.\n- Extensive ablations and analyses to illustrate the workings and benefits of the framework."}, "weaknesses": {"value": "- There is a bit of a disconnect between the applications pitched at the start of the Experiments section and the actual evaluation domain. The pitch is that the generation domain would be a domain whose specifications would be hard to describe in words, such as “a typical LeetCode problem from late 2024” (line 268). Instead, the actual evaluation domain was just coding in general, with the only temporal aspect being that earlier problems are used as reference examples and later examples are used as evaluation examples. If there really were temporal idiosyncrasies that could not be captured via prompt engineering, this evaluation would not capture it as the reference set would not capture the specifics of the evaluation set.\n  - A potential way to demonstrate the ability of Domino to capture such specifications would be to demonstrate Domino-generated data for a particular timeframe can be used to train models that are better at problems from that timeframe than other timeframes, whereas baseline methods achieve more uniform performance across all timeframes.\n- There are also synthetic data generation methods that more directly leverage the reference samples that were not baselined. While MAGPIE is a good baseline, it does not inherently utilize information from the reference samples (though the few-shot MAGPIE setting does try to). Other methods like OSS-Instruct (Wei et al., 2024) and BARE (Zhu et al., 2025) utilize reference examples directly in the prompt to ground synthetic data generation and would be worth comparing against.\n- It is unclear whether the generation methods are compared holding training data size constant or compute constant. The authors mention that a filtering step was applied to the generated dataset for all methods, but I could not find discussion on how large the filtered dataset was for any method. If the filtering process was biased towards retaining synthetic data generated by Domino, this would mean more training on the Domino-generated dataset and thus better performance.\n  - Similarly, I feel including the reference SFT row in the results tables is unnecessary as much less training would have been performed on the much smaller reference set, unless you were to increase the number of training epochs to bring the number of training steps to parity with the other methods.\n- Confidence intervals are not provided for the results, which makes it difficult to contextualize comparisons against baseline methods. Given the size of the evaluation sets (≤ 200) and the small scale of the performance differences, it would be helpful to have an idea of the uncertainty in the metrics."}, "questions": {"value": "- Do you have insights into why not including sample soft tokens at generation time works well vs, e.g., including dummy sample soft tokens?\n- For the samples described in Table 2, was the sample presented representative of all samples from D? Line 407 mentions how the samples in the table from $\\mathbf{D}^*$ were selected specifically to be far from the “the dense cluster representing $\\mathbf{D}$,” but I don't think it is shown how dense that cluster is. Moreover, is the reference sample in the table the closest one to the sample from $\\mathbf{D}$? I would just like more clarification on whether Table 2 should be interpreted as a case study of a single sample or indicative of a general issue with data diversity when using Domino with only the $\\mathcal{L}_1$ loss.\n- The ablations on $\\lambda$ focus only on the code generation task. Do you have insights into whether the ablation recommendations generalize to other tasks, or would the choice of lambda be task-specific?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VjdePSg937", "forum": "6mpv9kG81P", "replyto": "6mpv9kG81P", "signatures": ["ICLR.cc/2026/Conference/Submission16272/Reviewer_3H1n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16272/Reviewer_3H1n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772973028, "cdate": 1761772973028, "tmdate": 1762926419309, "mdate": 1762926419309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes DOMINO, which generates domain-specific synthetic data using example samples, without requiring explicitly defined descriptions. It learns a minimal sufficient domain representation via prompt tuning and contrastive disentanglement to capture core domain patterns and avoid overfitting to sample-specific details. Experiments show that DOMINO improves performance on coding and instruction-following tasks compared to the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The motivation to generate synthetic data for domains that are difficult to explicitly describe is interesting. The proposed approach also shows good performance compared to baselines. It provides extensive visualizations and analysis results that illustrate the behavior of the proposed method."}, "weaknesses": {"value": "The method could benefit from comparisons with a broader range of state-of-the-art synthetic data generation baselines, and the evaluation metrics could be expanded to more comprehensively assess both data quality and downstream performance.\n\nIt is also not very clear how closely the synthesized data resemble the reference domain or how they compare to outputs generated by existing SOTA approaches. Providing direct qualitative comparisons would make the evaluation more convincing (e.g., showing samples from Figs. 19 and 20 alongside reference data and samples from other baselines).\n\nThe method appears highly dependent on the quality and representativeness of the reference samples. How reference examples are selected and how robust the approach is to noisy or imperfect input? \n\nThere are also minor presentation issues that affect readability, including inconsistent table font sizes, overlapping text in Fig. 5, and the yellow line in Fig. 3 being difficult to discern. Addressing these points would further enhance clarity."}, "questions": {"value": "It would be helpful to include a performance curve showing how model performance changes as the amount of generated synthetic data increases. Presenting this trend alongside other baselines would allow a clearer comparison of data efficiency.\n\nSimilarly, I would like to see how performance varies with respect to the number of reference samples. Understanding sensitivity to reference set size would provide more insight into practical applicability.\n\nWhat is the exact definition of the Pass@1 metric used in the experiments? A brief clarification in the main text would help ensure consistent interpretation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f25sqvkuiu", "forum": "6mpv9kG81P", "replyto": "6mpv9kG81P", "signatures": ["ICLR.cc/2026/Conference/Submission16272/Reviewer_NMuv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16272/Reviewer_NMuv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994166585, "cdate": 1761994166585, "tmdate": 1762926418843, "mdate": 1762926418843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DOMINO, a framework for learning minimal sufficient domain representations from reference samples and using them to generate domain-specific synthetic data. Current synthetic data generation methods depend heavily on human-defined domain descriptions expressed in natural language. DOMINO, on the other hand, learns a set of domain soft tokens directly from reference examples through an inductive approach and uses these tokens as input to guide LLMs in generating synthetic data.\nThis is achieved through prompt tuning using a combined loss function consisting of domain-level likelihood (L₁) and contrastive loss (L₂). The contrastive component leverages sample-specific representations to enforce disentanglement between shared domain patterns and individual sample details, preventing overfitting while preserving core domain characteristics. The authors provide theoretical proofs demonstrating that their approach learns representations with both sufficiency and minimal sufficiency properties, and prove that the method expands the support of the synthetic data distribution compared to standard prompt tuning.\nThe paper uses LiveCodeBench as the primary testbed, demonstrating performance improvements of up to 4.63% in Pass@1 accuracy when fine-tuning on DOMINO-synthesized data compared to strong instruction-tuned baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[1] The DOMINO framework addresses a critical limitation in current data synthesis methods by eliminating the need for manual prompt engineering and explicit domain descriptions. Instead, it learns minimal sufficient domain representations directly from reference examples through an inductive approach, making it applicable to domains that are difficult to articulate in natural language. [2] The paper provides rigorous mathematical foundations for domain-specific data synthesis through information-theoretic concepts of sufficiency and minimal sufficiency. The theoretical guarantees demonstrate that DOMINO learns representations that capture essential domain characteristics while discarding irrelevant sample-specific noise, with formal proofs showing expanded support of the synthetic data distribution. [3] The authors demonstrate DOMINO's generalization capability by extending beyond coding tasks to diverse instruction-following domains. Results show consistent improvements across these varied task types, validating the framework's robustness and broad applicability beyond its primary coding benchmark evaluation."}, "weaknesses": {"value": "1. Synthetic Data Quality Issues: The examples provided reveal significant correctness problems that undermine the method's reliability. In Figure 1, the synthetic sample incorrectly identifies subarrays [2,3] and [3] as valid when they sum to odd numbers, violating the problem's even-sum requirement. Similarly, Figure 9 shows fundamental logical errors where the explanation correctly identifies the need to count distinct subsequences but then incorrectly counts \"aba\" and \"baba\" multiple times instead of once. These are not minor formatting issues but quality failures that question whether DOMINO preserves problem constraints during synthesis.\n\n2. The paper's central claim of increased diversity lacks convincing quantitative support. Figure 3 shows DOMINO samples more tightly clustered around reference points for Live Code Generation, suggesting reduced rather than increased diversity. This aligns with the weaker performance gains in Table 1 for this domain compared to Live Code Execution where Figure 4 shows more dispersed samples. The authors need more rigorous diversity metrics beyond t-SNE visualizations and should address this apparent contradiction between claimed and observed diversity patterns.\n\n3. The uniform prior assumption p(D)∝1 is fundamentally violated when using code-specialized LLMs on coding benchmarks, as these models already possess substantial domain knowledge. The temporal cutoff in LiveCodeBench doesn't guarantee that coding paradigms and problem patterns haven't leaked from test to training data, making evaluation potentially inflated. The authors should evaluate on truly held-out domains or provide stronger guarantees about train-test separation to validate performance gains using DOMINO."}, "questions": {"value": "1. How can data quality be verified/addressed within the DONIMO framework ? As mentioned in the point [1] in weakness above the generated samples seem to have logical inconsistencies within them that will have negative impact when used for finetuning candidate models. \n2. For showing diversity comparison authors should include quantitative metrics other than just t-SNE plots which is more visual. Also, diversity is difficult to capture with just one metric and hence should be computed across a mirad of metrics such as compression ratio, Remote clique, chamfer distance, etc. Please refer - METASYNTH: Meta–Prompting–Driven Agentic Scaffolds for Diverse Synthetic Data Generation (https://arxiv.org/pdf/2504.12563) for the full list of diversity metrics which is a prompt driven agentic approach to synthetic data generation.  Additionally, how do these diversity measures correlate with downstream performance improvements to establish whether increased diversity actually translates to better model training? \n3. In my opinion a low resource domain is a must to demonstrate the effectiveness of the DOMINO framework as it will gaurantee no prior knowledge of the domain. My concern is emanating from the observation made in point 3 in weakness. Could the authors demonstrate DOMINO's effectiveness on truly low-resource domains where the backbone LLM has minimal prior knowledge? This would provide stronger evidence for the framework's ability to learn domain representations from scratch rather than refining existing knowledge.\n4. In Appendix D: Limitations you correctly point out that \" To be effective, this process relies on the assumption that the reference samples are representative of the target domain. \" Doesn't this shift the burden from crafting good natural language descriptions to curating high-quality exemplars that span the domain distribution? This again becomes non-trivial for low resource/obscure domains and one again has to take the path of collecting as many samples as possible to feed as reference to cast a wider net to capture the domain distribution, baring which you will only be able to generate limited samples similar to the reference samples (this seems to be the case in Figure 3)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r8LUuHMMUc", "forum": "6mpv9kG81P", "replyto": "6mpv9kG81P", "signatures": ["ICLR.cc/2026/Conference/Submission16272/Reviewer_tYar"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16272/Reviewer_tYar"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031326752, "cdate": 1762031326752, "tmdate": 1762926418245, "mdate": 1762926418245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}