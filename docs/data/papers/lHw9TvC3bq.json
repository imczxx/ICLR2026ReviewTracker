{"id": "lHw9TvC3bq", "number": 9061, "cdate": 1758109006327, "mdate": 1759897745797, "content": {"title": "Sliding Window Attention for Reinforced Reasoning", "abstract": "Large reasoning models such as DeepSeek-R1 employ reinforcement learning (RL) to incentivize the reasoning capability. As the context length growth, the quadratic complexity of self-attention (SA) prohibits scaling to longer contexts. Recently, hybrid, sparse and linear attention methods aim to reduce the cost of SA, yet suffer from costly retraining, high complexity or linear memory growth. To address it, we revisit sliding-window attention (SWA). It not only offers linear-time complexity and constant memory, enabling faster RL rollouts, but also facilitates cheap conversion from pretrained transformers. Notably, we prove that SWA can handle the reasoning tasks well due to the locality of thought. In this paper, we introduce Sliding Window Attention for Reinforced Reasoning (SWARR), a two-stage approach: (1) math-specific supervised fine-tuning to convert a pretrained SA model into a SWA as cold-start, and (2) RL optimization using DAPO to enhance reasoning capabilities. Under same settings, our SWARR outperforms SA by 1.78% on 1.5B, while delivering 6.2x higher throughput and 8x larger batch size, and 1.5x longer context under same memory budget. Our SWARR achieves the competitive performance among 1.5B and 7B models, surpassing the DeepSeek-R1-Distill-Qwen-1.5B and 7B by 1.9% and 3.4% respectively. To our knowledge, this is the first work to show that trained SWA is a competitive alternative to transformers, enabling efficient and scalable reasoning.", "tldr": "We are the first to show the potential of sliding window attention in Reinforced Reasoning, achieving better performance, longer context and higher throughput than self-attention.", "keywords": ["RNN", "Recurrent LLM", "Reason", "RL"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/45db49813598cf83b80c0368252aedddb57ef851.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents SWARR, a two-stage framework for improving long-context reasoning in LLMs through sliding-window attention combined with reinforcement learning. The proposed method consists of two stages. In the first stage, a pretrained LLM is converted into a sliding-window attention model by reducing the original attention window and then fine-tuned through supervised fine-tuning for adaptation and stability. In the second stage, the direct advantage policy optimization algorithm is applied to further enhance multi-step reasoning, encouraging the model to utilize local context efficiently while maintaining coherent long-range reasoning. Empirical results show that SWARR significantly improves reasoning accuracy and efficiency compared to standard long-context models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important and timely challenge. The proposed SWARR framework, which combines sliding-window attention with reinforcement learning, is simple yet effective. The method achieves strong empirical results across multiple reasoning benchmarks, demonstrating both improved performance and computational efficiency.\n\n- The paper is well written and easy to follow. The motivation, methodology, and experimental setup are clearly explained, with logical structure and smooth transitions between sections. The presentation makes the technical ideas accessible and highlights the key findings effectively."}, "weaknesses": {"value": "- The main SWA idea is not new and has been explored in several prior works (e.g., Longformer, Local Attention Transformers). The contribution mainly lies in applying this mechanism to reinforcement-learning-based reasoning models, rather than introducing a fundamentally new attention design.\n\n- The process of converting pretrained models to SWA by simply truncating the attention window and performing supervised fine-tuning is straightforward. While practical, it raises the question of whether such improvements could be achieved through simpler techniques (e.g., prompt-level constraints or context cropping) without requiring architectural modifications or additional fine-tuning. Have the authors explored or compared such simpler approaches? It would be helpful if the paper could provide experimental evidence or discussion on this comparison."}, "questions": {"value": "See the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GfR9CvjXpG", "forum": "lHw9TvC3bq", "replyto": "lHw9TvC3bq", "signatures": ["ICLR.cc/2026/Conference/Submission9061/Reviewer_yubA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9061/Reviewer_yubA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663325236, "cdate": 1761663325236, "tmdate": 1762920770352, "mdate": 1762920770352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an adaptation technique that replaces self-attention with sliding window attention to speed up computations during training and model inference for reasoning traces. The procedure consists of two steps: first, adapting the model to use SWA instead of SA on the SFT dataset, and then applying DAPO to the resulting model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method can be easily applied to pre-trained language models.\n\n- SWARR achieves the best performance across all baselines and provides a significant speedup compared to vanilla attention."}, "weaknesses": {"value": "- Baselines using SA + DAPO are not included in the main tables (Table 2 and Table 3), making it difficult to disentangle the performance improvements from DAPO and SWA.\n\n- While Figure 4 provides results for the iso-time setting, it would also be interesting to see how the models behave under an iso-token setting. I see captions with iteration counts, but it’s hard to align them. Moreover, the models seem undertrained, as there is no plateau at the end.\n\n- Another issue with Figure 4 is that you tried four different settings for SWA, while the SA baseline remained unchanged across experiments. Thus, four hyperparameter configurations were explored for SWA but only one for the baseline."}, "questions": {"value": "- In Figure 4, there is a performance drop between iterations 0 and 50 for the SA model. Is there any instability during the first iterations in the loss? Perhaps more rigorous optimization techniques, such as warmup or gradient clipping, could help?\n\n- Why could SWA outperform SA in the iso-token setting? Shouldn’t the remaining tail of the attention scores still be useful? Or is this a general issue with attention, as mentioned in [1]? Would a carefully tuned full-attention model outperform SWA in terms of iso-tokens or iterations?\n\n- Applying an attention sink to SWA seems very useful for post-training setups due to its minimal performance degradation. Did you try this? Is there any computational overhead associated with it?\n\n**General Thoughts**\n\nFrom my perspective, the positives of the paper outweigh the negatives—especially regarding speed improvements. However, there are still flaws in the analysis and SA baseline setup. My main concern is that the SA baseline was not carefully tuned; I would expect it to outperform SWA in terms of the number of iterations.\n\n[1] Differential Transformer. Tianzhu Ye, Li Dong et al."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M8hUluslOK", "forum": "lHw9TvC3bq", "replyto": "lHw9TvC3bq", "signatures": ["ICLR.cc/2026/Conference/Submission9061/Reviewer_w1FZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9061/Reviewer_w1FZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836789867, "cdate": 1761836789867, "tmdate": 1762920769970, "mdate": 1762920769970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a post-training time adapation method converting SA to SWA for math reasoning model. The propose consists of a warmup stage and RL stages. Results shows that the models get acceptable results on math reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The overall story is self-content. It starts from the efficiency and locality of math reasoning and finally leads to a system-wise solution of fine-tuning SA to SWA."}, "weaknesses": {"value": "1. If the authors truly believe in SWA, then what we need is only an SWA pretrain model. It should be efficient and accurate and provide better accuracy than this post-train/mid-train method. On the other hand, if SWA is not a good method (worse than SA in many other tasks), then people will not buy in your post-train/mid-train method neither. So I think this paper lies in a very awkward stage. This is the reason that I do not believe in changing architecture during post-train/mid-train stages. \n2. The results only focus on math domain, which might lead to over-fitting, meaning other domain performance will downgrade a lot."}, "questions": {"value": "1. Can the author also provide code/general benchmark results after doing SWA fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xmFmjb4gsg", "forum": "lHw9TvC3bq", "replyto": "lHw9TvC3bq", "signatures": ["ICLR.cc/2026/Conference/Submission9061/Reviewer_w2y5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9061/Reviewer_w2y5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920755534, "cdate": 1761920755534, "tmdate": 1762920769571, "mdate": 1762920769571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a recipe to use sliding window attention in post-training (SFT + RL) on top of a full self-attention (SA) base LLM. Their experiments that show that SWA can match SA performance while offering major efficiency gains which can allow scaling up post-training or cut costs in computationally constraint scenarios. However the novelty is limited, and the experimental setup is lacking to make any strong or conclusive claims."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Meaningful set of experiments and baselines as a first step to understand the tradeoffs between adapting to SWA and doing reasoning post-training with it vs using plain Self-Attention.\n- In compute constraint setting SWA allows to easily scale up RL rollout batch size by claimed 8x and for longer which helps RL performance.\n- SFT seems to be 1.23x faster without loss of accuracy compared to standard SA"}, "weaknesses": {"value": "- Tradeoffs are not adequately discussed. Experiments show that 20% of attention still depends on long-range tokens that SWA ignores. So some degradation on tasks which need very long dependencies is inevitable. No experiments on such benchmarks are presented.\n- Evaluation is also only done at 8k if i understand correctly which doesn’t quite capture the loss of reasoning from long-context.\n- All experiments are done on math only domain. For other domains/benchmarks it cant be said weather the loss of global context will lead to a hit or not.\n- Results are not strong and modest at best. Gains on these noisy benchmarks are not sufficient to claim that SWA beats SA in their setup\n- Limited novelty in terms of the recipe and mostly an analysis of architecture change."}, "questions": {"value": "- For SFT, doing distillation from a strong teacher on long traces is often beneficial. Here you have restricted to short context SFT if i understand correctly which might limit the full potential of SA compared to SWA. For eg some latest reasoning models like R1-0528 generate super long reasoning traces upto 64k tokens. Its possible SWA doesn’t do as good on SFT on those long traces. Can authors possibly do some experiments to test this hypothesis?\n- At what sequence lengths are the evaluations done at?\n- Deepseek-R1-qwen-1.5B seems on par with SWARR in Table2 even though its is just an SFT-ed model. For small models, SFT is a strong but needs a lot of training often with multiple epochs [1]. Maybe authors should scale up their SFT to get a proper comparison.\n- Authors should have a true apples-apple comparison between SA and SWA with equal number of optimization steps and training hyperparameters. Without that its unclear whether the gains are coming from just doing more steps or from their recipe. Also helps provide a sanity check because if their setup and baseline is well tuned then SA should outperform SWA with same number of optimization steps.\n\n[1] https://arxiv.org/abs/2505.00949"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "P9v7Jw6WkP", "forum": "lHw9TvC3bq", "replyto": "lHw9TvC3bq", "signatures": ["ICLR.cc/2026/Conference/Submission9061/Reviewer_uPnA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9061/Reviewer_uPnA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762158354244, "cdate": 1762158354244, "tmdate": 1762920769136, "mdate": 1762920769136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}