{"id": "UmxTIxHWkl", "number": 6818, "cdate": 1757996854925, "mdate": 1763607166914, "content": {"title": "Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals", "abstract": "Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent’s post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent’s capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula.", "tldr": "", "keywords": ["Reinforcement Learning", "Unsupervised Reinforcement Learning", "Meta-Reinforcement Learning", "Pre-training", "Curriculum Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f507269caa0d639df9e089c4b242aa24a34a5129.pdf", "supplementary_material": "/attachment/cf8baa384b0071edf62fceb202d198089d4fe6ef.zip"}, "replies": [{"content": {"summary": {"value": "The paper tackles meta‑reinforcement learning by combining in‑context RL with a curriculum for self‑generated goals. An ICRL agent is pre‑trained to reach goals proposed by a curriculum pipeline. This pipeline first optimizes a goal‑search policy to estimate the agent’s current capability, then a selector chooses goals within calibrated success bounds. On XLand‑MiniGrid benchmarks, the proposed method, ULEE, improves exploration and adaptation, generalizing to novel objectives, dynamics, and map structures, and achieves better zero‑shot and few‑shot performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- *Intuitive formulation:* ULEE frames meta-RL as in-context adaptation guided by self-imposed goals, yielding a conceptually clean pipeline (goal search → capability estimation → goal selection). This makes the approach easy to reason about and implement.\n- *Solid empirical evidence:* On XLand–MiniGrid, ULEE consistently improves zero-shot and few-shot performance over strong baselines (learning from scratch, DIAYN, alternative curricula), with generalization to new objectives, dynamics, and map structures.\n-  *High-quality writing and presentation:* The paper is clearly written, with strong contextualization relative to prior work and a well-structured narrative that explains the curriculum components (goal search, capability estimation, selector) and their interplay, which aids reproducibility and understanding."}, "weaknesses": {"value": "- Despite the intuitive design, the pipeline introduces multiple components and hyperparameters (e.g., difficulty/success bounds), raising concerns about training stability and sensitivity.\n- With adversarial goal-generation, does the search ever propose “degenerate” goals (e.g., trivially satisfiable or reward-hacking)? Although the goal selector may mitigate this issue, randomness in sampling can lead to training collapse."}, "questions": {"value": "- Why not integrate the selector’s criterion directly into the goal-search reward—for example, use a $|x−0.5|$-shaped reward over success probability or estimated difficulty to penalize goals that are too hard or too easy? What are the trade-offs?\n- Could classical exploration designs like UCB (or Thompson sampling) be used to guide goal sampling—treating difficulty bands or goal families as arms—and does this improve sample efficiency or stability compared to the current selector?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gBpfHbSZgI", "forum": "UmxTIxHWkl", "replyto": "UmxTIxHWkl", "signatures": ["ICLR.cc/2026/Conference/Submission6818/Reviewer_QuSP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6818/Reviewer_QuSP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760510032908, "cdate": 1760510032908, "tmdate": 1762919083199, "mdate": 1762919083199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ULEE (Unsupervised Learning of Efficient Exploration), an unsupervised meta-learning method designed to pre-train adaptive reinforcement learning policies. The core challenge it addresses is how to effectively generate and select self-imposed goals for an agent to learn from, particularly for broad distributions of downstream tasks. ULEE's main contribution is an automatic curriculum learning strategy guided by a novel post-adaptation task-difficulty metric. This approach optimizes for an agent's performance after a period of adaptation, rather than its immediate performance. The method combines an in-context learner with an adversarial goal-generation system that finds challenging yet achievable goals, effectively maintaining training at the \"frontier of the agent's capabilities\". On XLand-MiniGrid benchmarks, ULEE-pre-trained policies demonstrate improved exploration, adaptation, and generalization to novel tasks , resulting in better zero-shot and few-shot performance and providing a strong initialization for longer fine-tuning processes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The post-adaptation task-difficulty metric, for me, is novel. It significantly departs from prior works in automatic curricula, which typically evaluate goal difficulty based on the agent's immediate performance. By defining difficulty as the agent's expected success rate after an adaptation budget, the method directly optimizes for the agent's capacity to learn rather than just its current knowledge. This aligns the pre-training objective more closely.\n\n2. The paper originally combines three key components into a single system, ULEE. While concepts like meta-learning, adversarial goal generation, and in-context learners exist, ULEE integrates them synergically. It uses an adversarial \"Goal-search Policy\" to propose hard goals, a \"Difficulty Predictor\" network to estimate their post-adaptation difficulty, and an in-context learner (the \"Pre-trained Policy\") to meta-learn on a curriculum of goals selected for being at the \"frontier of the agent's capabilities\".\n\n3. The authors evaluate the pre-trained policy across a wide spectrum of downstream scenarios."}, "weaknesses": {"value": "1. The primary weakness of ULEE is its high methodological complexity. The system is not a single algorithm but a complex interplay of four distinct, learning-based components: the Pre-trained Policy ($\\pi$), the Goal-search Policy ($\\pi_{g, s}$), as well as the Difficulty Predictor. Are there practical bottlenecks (e.g., memory, wall-clock time) for this method?\n\n2. The overall system's success depends on these components learning in lockstep. Can this co-adaptive process be brittle? E.g., will a relatively poor Difficulty Predictor lead to a collapse? Adding a robustness analysis to each design ingredient will greatly strengthen the paper.\n\n3. The pre-training and evaluation tasks (4Rooms-Trivial, 4Rooms-Small, 6Rooms-Small) are all drawn from the same \"family\" of XLand-MiniGrid rules, which may not fully cover the claim “generalization to new goals, transition dynamics, and grid structures”.\n\n\nMinors:\n1. I recommend a figure to show the overall framework of the proposed, so that it can be more detailed and accessible.\n\n2. There are some grammatical and typographical errors, for example. In line 65, “more tasks become too…”,"}, "questions": {"value": "1. There are many hyper-parameters in ULEE, learning rates, network architectures, buffer sizes, sampling bounds LB/UB, number of goal-search episodes etc. How critical are the specific hyper-parameter values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NiMEfSdFJs", "forum": "UmxTIxHWkl", "replyto": "UmxTIxHWkl", "signatures": ["ICLR.cc/2026/Conference/Submission6818/Reviewer_ompC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6818/Reviewer_ompC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919236684, "cdate": 1761919236684, "tmdate": 1762919082637, "mdate": 1762919082637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Unsupervised Learning of Efficient Exploration (ULEE) as an unsupervised meta-learning approach that pre-trains a policy capable of rapid adaptation to new tasks. It achieves this by generating challenging-but-solvable goals based on the policy's estimated post-adaptation performance. The resulting policy is an in-context learner that adapts to new goals, dynamics and map structures using only its interaction history (observations, actions, rewards), requiring no explicit goal input. On the XLand-Minigrid benchmark, ULEE outperformed baselines in terms of fast adaptation, and provided a better initialization for both extended fine-tuning and supervised meta-learning. It also demonstrated generalization to novel environment structures. \n\n**Recommendation:**\\\nThis paper falls outside my area of expertise, but appears to have a well motivated and interesting problem setting and strong empirical results. However, I have some questions about the methodology regarding seeding and validation/test sets. Therefore, in its current state, I will recommend to reject. However, I will be open to change my score if my questions are answered satisfactorily."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Although this is not my area of expertise, the paper's motivation and positioning within existing literature appears strong. \n- The problem of pre-training for adaptation is very interesting.\n- The empirical results are very strong."}, "weaknesses": {"value": "- Section 4.3.1 does not sufficiently answer Q1. In this section the fraction of evaluation goals reached as a function of the number of evaluation episodes is shown in Figure 2. In my opinion this does not isolate exploration as the cause of evaluation goals reached, nor does it answer _\"what exploration capabilities\"_ the policy exhibits. For example, an increase in evaluation goals reached can also be due to zero-shot generalization, rather than improved exploration/adaptation. \n- Some parts of the experimental methodology is unclear. In particular when it comes to hyperparameter tuning and validation vs test split. \n- Certain details in the main text could be explained better."}, "questions": {"value": "- Are there better ways to isolate and analyse the exploration capabilities of the Pre-trained Policy? For example, subtract all evaluation goals reached in a single episode and only include the ones reached with more episodes? Or visualize the exploratory behaviour of the policies in some way? \n- There appears to be no explicit mention of a test versus validation set. Are the final results evaluated on an independent testing set of environments (that has not been used for validation, hyperparameter tuning, or generally for algorithm design)? Similarly, were the seeds used for the final evaluation (testing) different from the ones used for validation, tuning and design? \n- I could not find mention of the hyperparameter tuning approach for your method and the baselines. How is it ensured that your approach did not accidentally benefit from an advantageous hyperparameter combination or tuning budget? Did you use separate seeds for tuning and final evaluation? \n\n\n**Things to improve that did not impact decision:**\n- Some of the related work mentioned in Section 2 is missing an explicit comparison with the paper's approach.\n- Line 227: The variable $n$ is introduced there but not defined or mentioned in the text.\n- Line 294: I don't quite understand how $f_{counts}$ works. \n- Figure 3c: It is unclear to me what exactly Figure 3c is showing. Is it showing post-adaptation return on the evaluation set, evaluated at different points during pre-training?\n- Table 1: The bold highlight is very difficult to differentiate from the regular numbers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "awxbvOqjJI", "forum": "UmxTIxHWkl", "replyto": "UmxTIxHWkl", "signatures": ["ICLR.cc/2026/Conference/Submission6818/Reviewer_HPvB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6818/Reviewer_HPvB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921366819, "cdate": 1761921366819, "tmdate": 1762919082165, "mdate": 1762919082165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank reviewers SaiF, HPvB, ompC, and QuSP for their time, their constructive feedback, and for highlighting ULEE’s design, motivation, and empirical results. In this general comment, we summarize the main changes in the revised version of the paper.\n\n\n### **Ablation for use of post-adaptation instead of immediate difficulty**\n\nWe thank the reviewers for recognizing the novelty in using post-adaptation instead of immediate performance to guide the curriculum. We also agree with Reviewer SaiF that explicitly ablating this decision would strengthen the paper.\nTherefore we have added ULEE (SED) as an ablation (Section 4.2). This variant is identical to ULEE except that it estimates difficulty from the Pre-trained Policy’s immediate performance in single-episode interactions rather than from its performance over the last K episodes of a multi-episode interaction. The new results in Section 4.3.1 and 4.3.2 show that using a post-adaptation metric to guide the curriculum leads to solving more evaluation tasks and yields better zero-shot and few-shot performance, with the gap being larger on the more challenging benchmarks.\n\n\n### **Hyperparameter selection and stability**\n\nReviewers HPvB, ompC, and QuSP raised questions regarding hyperparameter tuning, sensitivity, and potential brittleness. In response, we added a new appendix section, *“Hyperparameter Selection and Stability”*, which details our exploration of hyperparameter configurations and the selection process. Across all configurations we tried, we observed no collapse in training and did not find ULEE unusually sensitive to hyperparameter choices. We thank the reviewers for these comments and expect this new section to be useful to future readers.\n\n\n**Updates to Section 4.3.3 (Fine-tuning on Fixed Tasks)**\n\nWe strengthen Section 4.3.3 by (i) adding RND as an additional baseline and (ii) updating the DIAYN results after tuning its fine-tuning hyperparameters. In the original submission, both ULEE (which still does) and DIAYN reused hyperparameters from pre-training. The new DIAYN curve is improved but still below PPO from scratch and ULEE."}}, "id": "3Oels6G2la", "forum": "UmxTIxHWkl", "replyto": "UmxTIxHWkl", "signatures": ["ICLR.cc/2026/Conference/Submission6818/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6818/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6818/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763607569970, "cdate": 1763607569970, "tmdate": 1763607569970, "mdate": 1763607569970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ULEE (Unsupervised Learning of Efficient Exploration), an unsupervised meta-RL pretraining framework that trains an in-context learner via an adversarial goal-generation strategy. Goal difficulty is defined by the post-adaptation success rate, yielding an intermediate-difficulty curriculum."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-motivated. The curriculum is based on performance after in-context adaptation, not immediate performance, which aligns with the intended meta-RL setting. Empirically, ULEE improves exploration, shows faster few-shot adaptation, and provides stronger initializations for finetuning."}, "weaknesses": {"value": "* The empirical impact of defining difficulty via post-adaptation performance, rather than immediate performance, remains unclear without an ablation. A direct ablation (e.g., a sensitivity study over $K$) would strengthen the paper.\n* The baselines do not include recent meta RL and unsupervised RL methods.\n* Experimental scope is limited to grid-world domains."}, "questions": {"value": "* Does the learned difficulty correlate with intuitive task hardness? A qualitative or heuristic-based comparison between high and low-difficulty goals would be helpful.\n* Does the goal-search policy reliably propose high-difficulty goals? What are the difficulty distributions of goals sampled by the goal-search policy (and a random policy)?\n* What other environment information $\\xi_M$ could be used, especially for environment domains other than grid-worlds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6fyy4hDqtx", "forum": "UmxTIxHWkl", "replyto": "UmxTIxHWkl", "signatures": ["ICLR.cc/2026/Conference/Submission6818/Reviewer_SaiF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6818/Reviewer_SaiF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971617162, "cdate": 1761971617162, "tmdate": 1762919081873, "mdate": 1762919081873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}