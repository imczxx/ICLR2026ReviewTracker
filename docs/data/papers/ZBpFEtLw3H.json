{"id": "ZBpFEtLw3H", "number": 15227, "cdate": 1758249119939, "mdate": 1759897320285, "content": {"title": "Adversarial Reinforcement Learning Framework for ESP Cheater Simulation", "abstract": "Extra-Sensory Perception (ESP) cheats, which reveal hidden in-game information such as enemy locations, are difficult to detect because their effects are not directly observable in player behavior. The lack of observable evidence makes it difficult to collect reliably labeled data, which is essential for training effective anti-cheat systems. Furthermore, cheaters often adapt their behavior by limiting or disguising their cheat usage, which further complicates detection and detector development. To address these challenges, we propose a simulation framework for controlled modeling of ESP cheaters, non-cheaters, and trajectory-based detectors. We model cheaters and non-cheaters as reinforcement learning agents with different levels of observability, while detectors classify their behavioral trajectories. Next, we formulate the interaction between the cheater and the detector as an adversarial game, allowing both players to co-adapt over time. To reflect realistic cheater strategies, we introduce a structured cheater model that dynamically switches between cheating and non-cheating behaviors based on detection risk. Experiments demonstrate that our framework successfully simulates adaptive cheater behaviors that strategically balance reward optimization and detection evasion. This work provides a controllable and extensible platform for studying adaptive cheating behaviors and developing effective cheat detectors.", "tldr": "We propose a simulation framework for modeling ESP cheaters, non-cheaters, and trajectory-based detectors, using reinforcement learning agents in an adversarial game setting.", "keywords": ["Game Hack Detection", "Reinforcement Learning", "Adversarial Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8020e6bc1896d65919dd3fa2511695841a4c3d37.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents an approach/framework for the study of adaptive cheaters and the development of efficient cheat detectors in games. A simulation approach is adopted, with cheaters and non-cheaters modelled as RL agents and the cheat detector as a classifier, with the interactions modelled as an adversarial game. Experiments with two simple environments are conducted to demonstrate the framework and the results analysed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a well written paper. The methodology is clearly laid out and well presented.\n\nThe experiments are comprehensive and thorough, and the analysis is considered and sound. The conclusion that the detector developed with adversarial training is a more robust classifier is a solid result, as well as the observation that the adversarially trained cheater can evade detection while still enjoying improved rewards over a non-cheater.\n\nThough not a core area for me, this seems to be a solid contribution, with some clearly elucidated directions for future research too."}, "weaknesses": {"value": "No weaknesses to note.\n\nMinor: not sure AUROC was defined."}, "questions": {"value": "Trivial point - for ease of reference, the baseline numbers from Table 1 might have been good to add as bars in Figure 4, so as to be able to get a better sense of the behaviour as lambda varies (and to more clearly see the excess AP performance for small lambda)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dMtNhA761r", "forum": "ZBpFEtLw3H", "replyto": "ZBpFEtLw3H", "signatures": ["ICLR.cc/2026/Conference/Submission15227/Reviewer_t5MH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15227/Reviewer_t5MH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760922016349, "cdate": 1760922016349, "tmdate": 1762925524190, "mdate": 1762925524190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an adversarial framework to model cheaters and anti-cheater detection in computer games. It fuses standard RL with a minmax objective to train both models at the same time. Experimental analysis is done on two toy environments and several configurations are studied, including sensible baselines based on reward and episode length."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "This paper proposes a relatively realistic model of cheating in games, which supports the cheater's dual objectives of maximizing reward while avoiding detection, and the mutual co-adaptation of cheater and detector. While it is not ground-breaking (being based on fairly well-established adversarial frameworks), it seems well-conceived.\n\nThe paper itself is clear, with only a few areas that could use improvement.\n\nThe experiments, while done in small settings, do focus on the main areas of interest: adaptive vs non-adaptive behaviour, comparing with proxy signals such as success and episode length, and sensitivity analysis of the coefficient trading off detection vs reward maximization."}, "weaknesses": {"value": "The main weakness of the paper is in the experimental validation. While the analysis is sound, the conclusions are limited because only two very simple settings were studied. An analysis of more realistic (if small) games would be more conclusive.\n\nThe lack of such a comparison is probably explained by another weakness of the methodology: it requires white-box access to RL policies of cheaters and non-cheaters. This means that it cannot be easily adapted to existing games, where game developers can collect datasets of player behaviour, and it is not obvious which are cheaters; but due to the complexity of modern games, it is not a trivial endeavour to train effective RL policies for them.\n\nAnother aspect that is understudied is variability in player skill. Both cheater and non-cheater are assumed to play as well as possible (given the limits of training). It is relevant to study the spectrum of non-cheater skill -- especially as high-skill players may be labelled as cheaters, depending on the game. Cheater skill level is also relevant, as a player with poor skills who uses cheating may be easier to detect than a skilled one.\n\nThe related work section is extremely short, making it seem like there are more works that were left out. As just one example, Franzmeyer et al. \"Illusory Attacks\" (ICLR 2024) also seems to study attackers that attempt to conceal their interference, but the frameworks may be different (maximize a player's reward vs minimize a victim's reward).\n\nFig. 1: The difference between cheater and non-cheater is unclear - would be better to have them interact with a single environment, and having an extra arrow from the environment that represents unfair (ESP) observations. The feedback going to both cheater and detector from itself is also a confusing way to present it."}, "questions": {"value": "I would like the authors to comment on the relevance of their contribution in the context of the difficulty of training good RL agents in realistic settings, and how it affects extensions to commercial games."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T9B6bGbNPV", "forum": "ZBpFEtLw3H", "replyto": "ZBpFEtLw3H", "signatures": ["ICLR.cc/2026/Conference/Submission15227/Reviewer_E6T1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15227/Reviewer_E6T1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851684556, "cdate": 1761851684556, "tmdate": 1762925523652, "mdate": 1762925523652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper constructs a simulation methodology that allows for reproducing ESP cheater behavior through adversarial reinforcement learning, including an adaptive cheat detector and adversarial training of cheaters.\nThe simulation combines existing techniques adapted to this problem setting, including environment formulation via POMDP/MDP, GAN/GAIL-style min-max training, reward shaping, PPO optimization, and trajectory classifier detection. Based on experiments, this study argues that detecting such adaptive cheaters requires a co-evolving detector."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This research targets a realistic and socially significant problem, ESP cheating, that has not been intorduced by conventional adversarial RL or security RL approaches.\n- The mathematical formulation of the proposed framework is well-organized."}, "weaknesses": {"value": "- The framework of ESP cheaters versus detectors appears to be a new application, but it is essentially a form of adaptive RL-based evasion and bears strong similarities to existing RL-based adversarial attacks against detectors/classifiers.\n- To the best of the reviewers' knowledge, the novelty in the design lies in:\n(1) the detector being based on trajectory-based cheetah detection, and (2) introducing a complementary structure for cheetahs similar to MoE or a gating network. Both represent straightforward applications of existing techniques to this problem setting."}, "questions": {"value": "The difference in observation where non-cheaters follow a POMDP while cheaters follow an MDP is critically important concerning the nature of the problem. How does this observational difference affect the difficulty of detection or convergence to equilibrium? To what extent does the information gap between partial observation and full observation bring about qualitative changes in the structure of the game? Can theoretical or empirical insights be provided on this point?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WnTihfMU7o", "forum": "ZBpFEtLw3H", "replyto": "ZBpFEtLw3H", "signatures": ["ICLR.cc/2026/Conference/Submission15227/Reviewer_ZEf6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15227/Reviewer_ZEf6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898290765, "cdate": 1761898290765, "tmdate": 1762925523181, "mdate": 1762925523181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a simulation framework to model the adversarial interaction between an ESP cheater and a cheat detector. The interaction between an adaptive cheater and a detector is then formulated as a minimax game. The framework is tested on custom Gridworld and Blackjack environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novel Architectural Choice: The Structured Cheater Model is a clever and novel contribution. By freezing the base policies and only learning the interpolation function $\\omega$, the authors simplify the adversarial learning problem.\n\nWell-Motivated Problem: The paper addresses a practical and difficult problem (detecting adaptive cheaters) where collecting labeled data is notoriously hard. The simulation-based approach is well-justified ."}, "weaknesses": {"value": "Disconnect Between Motivation and Experiments: This is the most significant weakness. The paper is motivated by complex, multiplayer First-Person Shooter (FPS) games where ESP cheats are a major issue. However, the experiments are conducted on simple, single-agent Gridworld and Blackjack environments. These environments fail to capture the strategic, interactive, and high-dimensional nature of the motivating problem. The authors acknowledge this as future work, but the gap is too large for the claims to be considered general.\n\nLimited Novelty of the Base Framework: While the structured model is novel, the underlying adversarial RL framework (a minimax game solved with GDA and a non-saturating generator loss ) is a standard formulation, similar in principle to GANs and Generative Adversarial Imitation Learning (GAIL). The paper's novelty rests almost entirely on the structured architecture rather than a new learning paradigm"}, "questions": {"value": "Please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OpSo6ft5ej", "forum": "ZBpFEtLw3H", "replyto": "ZBpFEtLw3H", "signatures": ["ICLR.cc/2026/Conference/Submission15227/Reviewer_tTX8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15227/Reviewer_tTX8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972663914, "cdate": 1761972663914, "tmdate": 1762925522829, "mdate": 1762925522829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}