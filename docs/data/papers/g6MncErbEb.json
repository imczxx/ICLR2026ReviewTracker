{"id": "g6MncErbEb", "number": 21903, "cdate": 1758323374175, "mdate": 1763667545996, "content": {"title": "Self-Improving Skill Learning for Robust Skill-based Meta-Reinforcement Learning", "abstract": "Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen tasks but faces challenges in long-horizon environments. Skill-based approaches tackle this by decomposing state-action sequences into reusable skills and employing hierarchical decision-making. However, these methods are highly susceptible to noisy offline demonstrations, leading to unstable skill learning and degraded performance. To address this, we propose Self-Improving Skill Learning (SISL), which performs self-guided skill refinement using decoupled high-level and skill improvement policies, while applying skill prioritization via maximum return relabeling to focus updates on task-relevant trajectories, resulting in robust and stable adaptation even under noisy and suboptimal data. By mitigating the effect of noise, SISL achieves reliable skill learning and consistently outperforms other skill-based meta-RL methods on diverse long-horizon tasks.", "tldr": "", "keywords": ["Reinforcement Learning", "Meta Reinforcement Learning", "Skill-based Reinforcement Learning", "Hierarchical", "Noisy Demonstration", "Skill Refinement", "Generalization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/33fff5972f1ef842e7f8747bfb63fcd0a92a00a4.pdf", "supplementary_material": "/attachment/44681ae5dd47a10e6b6030df8e5b87ad8aeba69a.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses a critical challenge in skill-based meta-RL: performance degradation when learning skills from noisy or suboptimal offline demonstration datasets.\n\nThe authors propose a new framework, SISL, designed to be robust to such data imperfections. SISL's novelty lies in two key contributions:\n- Decoupled Skill Self-Improvement\n- Skill Prioritization via Maximum Return Relabeling"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Significance**: The paper tackles a highly significant and practical problem: the sensitivity of skill-based (meta-)RL algorithms to noisy offline demonstration data. As the field moves toward using large, imperfect, real-world datasets, methods that can \"denoise\" and \"self-improve\" upon this data are critical. This work provides a strong solution to this problem.\n\n- **Clarity and Presentation**: The paper is a clear. It is well-written, logically structured, and supported by high-quality figures that provide both conceptual intuition (Fig. 1, 3) and compelling qualitative evidence (Fig. 7, F.2).\n\n- **Empirical Rigor and Performance**: The experimental evaluation is good. The authors demonstrate SISL's superiority across four challenging long-horizon environments against a comprehensive set of relevant baselines."}, "weaknesses": {"value": "- **Complexity and Computational Cost**: The SISL framework introduces several new components that add to the computational load: training the skill-improvement policy, the RND networks, and the reward model, plus periodically re-training the entire skill library.\n\n- **Periodic Re-initialization**: The design choice to re-train the skill library and re-initialize the high-level policy is a key part of the algorithm. The ablation in G.4 shows that not re-initializing is catastrophic, which makes sense due to non-stationarity. However, this periodic \"reset\" might be disruptive, and the paper does not explore the sensitivity to the frequency itself."}, "questions": {"value": "- **Sensitivity to $K_{iter}$**: The periodic re-training of the skill model and re-initialization of $\\pi_h$ every $K_{iter}$ steps is a critical design choice. The ablation in G.4 proves re-initialization is necessary, but how sensitive is the algorithm to the value of $K_{iter}$? What happens if re-training is too frequent (e.g., $K_{iter}=100$) or too infrequent (e.g., $K_{iter}=5000$)?\n\n- **Necessity of Decoupled Policies**: Why is the decoupling of $\\pi_h$ and $\\pi_{imp}$ strictly necessary? Could a single high-level policy not perform both roles?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "h2a64T6KJZ", "forum": "g6MncErbEb", "replyto": "g6MncErbEb", "signatures": ["ICLR.cc/2026/Conference/Submission21903/Reviewer_LUR5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21903/Reviewer_LUR5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760783387397, "cdate": 1760783387397, "tmdate": 1762941975780, "mdate": 1762941975780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their constructive feedback. Following your suggestions, we have substantially strengthened the manuscript with additional experiments and analyses to better demonstrate the computational cost and stability of our framework. A revised version, with all changes highlighted in blue, has been uploaded. The major updates are summarized below.\n\n**(i) Deeper analysis of computational cost (Section 5.1, 5.3, and Appendix E)**\n\nSince SISL introduces several components, we extended Appendix E beyond the original complexity discussion to include total training time comparisons and a more detailed breakdown of computation cost for SISL and its ablation variants. This analysis clarifies where the additional cost arises and shows that it remains moderate relative to the performance gains.\n\n**(ii) Reward model stability and additional visual comparisons (Section 5.4 and Appendix F.5)**\n\nWe now provide a clearer explanation of how the reward model used for relabeling is trained stably and avoids overfitting while performing maximum return relabeling. In addition, we include more fine-grained visualizations of skill trajectories for SISL and baseline methods, which highlight their behavioral differences and illustrate why SISL is more effective.\n\n**(iii) Additional ablations and baselines (Section 5.5, Appendix G.5, and G.6)**\n\nTo clarify the effect of the refinement schedule, we added a hyperparameter study on the skill refinement interval $K_{\\text{iter}}$ and analyzed its impact on performance. We also incorporated an additional comparison with goal-conditioned RL (GCRL) methods, as suggested by the reviewer, to further strengthen the empirical evidence and robustness of our conclusions.\n\nWe believe these revisions address the main concerns raised during review and improve the clarity and completeness of the paper. We are grateful for the reviewersâ€™ guidance, which materially enhanced the manuscript."}}, "id": "cKnGZ7w2ZG", "forum": "g6MncErbEb", "replyto": "g6MncErbEb", "signatures": ["ICLR.cc/2026/Conference/Submission21903/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21903/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21903/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763643977707, "cdate": 1763643977707, "tmdate": 1763643977707, "mdate": 1763643977707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the topic of skill-based meta-learning in reinforcement learning. It points out existing skill learning methods relying on offline demonstrations often suffer from suboptimal data quality. The proposed Self-Improving Skill Learning (SISL) framework shows robustness to noisy demonstrations by introducing a skill improvement policy and performing skill refinement through trajectory prioritization based on returns."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper adopts perturbations, a representative technique in meta-learning, where target tasks are unseen by the skill-based reinforcement learning agent. This approach constrains the learning process to remain close to the demonstration manifold, thereby facilitating effective skill acquisition."}, "weaknesses": {"value": "The SISL framework appears to be quite complex and computationally expensive. For example, it involves maintaining multiple buffers that serve similar functions. The proposed method also does not seem to be specifically tailored to address the meta-learning problem. Baselines such as SPiRL and SiMPL learn skills solely from offline demonstrations, whereas SISL additionally collects online data, making the comparison unfair.\n\n**Minor**\n\nAppendix B should be moved to the main body of the paper. The figures and their captions are positioned too close to the main text, which reduces readability. Figure captions are brief and could be improved to provide clearer explanations of the content."}, "questions": {"value": "(1) How does the method avoid overfitting to the source tasks when evaluating trajectories based on returns?\n\n(2) Please compare the skills extracted by SISL with those obtained by other baseline methods.\n\n(3) Consider including an additional experiment where Equation (3) is evaluated without the KLD term.\n\n(4) Please report the success rate on the maze environments.\n\n(5) Compare the computational cost across different ablation settings.\n\n(6) Could you clarify why performance increases as $\\sigma$ increases in some cases in Table 1?\n\n**Minor**\n\nIn Figure 2, which algorithm is being illustrated?\n\nHow is the KLD term in Equation (3) computed?\n\nHow many offline trajectories generated by the noisy behavior policy are used to solve each task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lhanaLncGr", "forum": "g6MncErbEb", "replyto": "g6MncErbEb", "signatures": ["ICLR.cc/2026/Conference/Submission21903/Reviewer_SPY1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21903/Reviewer_SPY1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761199726227, "cdate": 1761199726227, "tmdate": 1762941975531, "mdate": 1762941975531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the sensitivity of skill-based meta-RL methods to noisy offline demonstrations. The proposed method, Self-Improving Skill Learning (SISL), addresses this by dynamically refining the skill library during meta-training. Its core contributions are:\n\n- Decoupled Skill Self-Improvement: A dedicated skill-improvement policy $\\pi_{imp}$ is introduced alongside the standard high-level policy $\\pi_h$. $\\pi_{imp}$ explores near the offline data distribution to find higher-quality trajectories.\n- Skill Prioritization via Maximum Return Relabeling: To fuse the noisy $B_{off}$ with the clean $B_{on}$ for skill refinement, SISL trains a reward model using only online data. This model assigns a hypothetical maximum to each offline trajectory. The skill update then samples from $B_{off}$ using a softmax prioritization based on $\\hat{G}$, effectively filtering out low-quality data.\n\nExperiments on long-horizon tasks show that SISL substantially outperforms baselines in noisy data regimes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This work focuses on a key practical problem in meta-RL, where offline demonstrations may be noisy.\n- The paper provides strong empirical evidence demonstrating significant performance improvements over all baselines.\n- The paper is well-structured. The method is presented logically."}, "weaknesses": {"value": "- The paper claims only \"16% more time per iteration\", which seems surprisingly low. A SISL iteration appears to involve: (1) rollout with $\\pi_h+\\pi_l$, (2) rollout with $\\pi_{imp}$, (3) $\\pi_h$ update, (4) $\\pi_{imp}$ update, (5) $\\pi_l$ update, and (6) $\\hat{R}$ update. In contrast, the baseline presumably only includes steps (1) and (3). It is unclear how the 16% figure was calculated. A more detailed breakdown and a comparison of total training time, not just per-iteration cost, would be more informative.\n- The skill refinement interval, $K_{iter}$, is a critical new hyperparameter that balances the stability of the high-level policy $\\pi_h$ against the speed of skill refinement for $\\pi_l$. However, the paper provides no ablation study for $K_{iter}$, making it difficult to assess the algorithm's sensitivity to this important design choice.\n- The \"maximum return relabeling\" mechanism hinges on a reward model $\\hat{R}$ trained on online data. This implies a potential failure mode: in tasks with sparse or complex rewards, training $\\hat{R}$ could become unstable, compromising the data prioritization mechanism and overall performance.\n- Lack of Clarity in Pseudocode. For example:\n    - In Algorithm 1 (Meta-Train), line 14 updates the skill parameters $\\phi$ (including $q_{\\phi}$). However, the algorithm does not show where the skill encoder $q_{\\phi}$ is used for inference on trajectories.\n    - In Algorithm 2 (Meta-Test), line 7 updates parameters $\\theta$. It is ambiguous whether$\\theta$also includes the task encoder $q_{e,\\theta}$. If the task encoder is frozen during the meta-test phase (as is typical), it would be clearer to use different notation to distinguish its parameters from the policy/value function parameters being adapted."}, "questions": {"value": "- Figure F.1 shows that the mixing coefficient $\\beta$ quickly converges to high values, implying that the online-generated data quality significantly surpasses the offline data. This raises the question: could the offline data $B_{off}$ be discarded altogether? For instance, what if one first trained an expert policy (e.g.,$\\pi_{imp}$ trained without the KL-divergence term, acting as a standard RL agent) to collect a new, clean dataset, and then used this dataset for learning $\\pi_l$ and $\\pi_h$? Would this not be a simpler and potentially lower-cost alternative to the complex relabeling and mixing process?\n- Does the framework assume that only optimal trajectories are useful for training the low-level policy $\\pi_l$? It is possible that certain suboptimal trajectories, while not useful for the current set of training tasks, might contain skills that are highly beneficial for generalizing to unseen test tasks. The \"Maximum Return Relabeling\" mechanism prioritizes trajectories based on their estimated maximum return on *training* tasks. Does this design inadvertently suppress these \"suboptimal but generalizable\" trajectories, thereby weakening the utilization of data that could be crucial for meta-generalization?\n- See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "quEREY58zt", "forum": "g6MncErbEb", "replyto": "g6MncErbEb", "signatures": ["ICLR.cc/2026/Conference/Submission21903/Reviewer_jnLn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21903/Reviewer_jnLn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976121714, "cdate": 1761976121714, "tmdate": 1762941975265, "mdate": 1762941975265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose SISL (Self-Improving Skill Learning), a skill-based meta-RL framework designed to handle noisy offline demonstrations in long-horizon tasks. SISL introduces two key mechanisms: (1) decoupled skill self-improvement, where a dedicated improvement policy explores near the offline data distribution to discover higher-quality rollouts that progressively refine the skill library, and (2) skill prioritization via maximum return relabeling, which uses a learned reward model to assign estimated returns to offline trajectories and reweight samples through softmax prioritization."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-motivated and very clearly presented, with very useful illustrations.\n- Separating exploitation ($\\pi_h$) and skill improvement ($\\pi_{imp}$) with self-supervised guidance via prioritized buffers appears to be a novel and elegant contribution.\n- The evaluation is sound and detailed, with 4 diverse environments with multiple noise levels, thorough baseline comparisons, and extensive ablations."}, "weaknesses": {"value": "I believe the paper would benefit from comparing against a GCRL baseline with Hindsight Experience Replay (HER) or similar relabeling techniques. This would help demonstrate that SISL's approach to leveraging the offline dataset is superior to existing relabeling methods in both sample efficiency and final performance. \n\nMinor typo: \n- Line 279: \"addtion\""}, "questions": {"value": "Shouldn't $\\beta$ in Eq. 7 be task-dependent? I guess that sometimes, different tasks may benefit from different balances between $B_{\\text{off}}$ and $B_{\\text{on}}$. How sensitive is performance to using a global $\\beta$ versus task-specific $\\beta^i$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K3WwTlOBds", "forum": "g6MncErbEb", "replyto": "g6MncErbEb", "signatures": ["ICLR.cc/2026/Conference/Submission21903/Reviewer_brpU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21903/Reviewer_brpU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992643938, "cdate": 1761992643938, "tmdate": 1762941974996, "mdate": 1762941974996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}