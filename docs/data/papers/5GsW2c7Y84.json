{"id": "5GsW2c7Y84", "number": 3375, "cdate": 1757413021275, "mdate": 1759898093274, "content": {"title": "Informing Acquisition Functions  via Foundation Models for Molecular Discovery", "abstract": "Bayesian Optimization (BO) is a key methodology for accelerating molecular discovery by estimating the mapping from molecules to their properties while seeking the optimal candidate. Typically, BO iteratively updates a probabilistic surrogate model of this mapping and optimizes acquisition functions derived from the model to guide molecule selection. However, its performance is limited in low-data regimes with insufficient prior knowledge and vast candidate spaces. Large language models (LLMs) and chemistry foundation models offer rich priors to enhance BO, but high-dimensional features, costly in-context learning, and the computational burden of deep Bayesian surrogates hinder their full utilization. To address these challenges, we propose a likelihood-free BO method that bypasses explicit surrogate modeling and directly leverages priors from general LLMs and chemistry-specific foundation models to inform acquisition functions. Our method also learns a tree-structured partition of the molecular search space with local acquisition functions, enabling efficient candidate selection via Monte Carlo Tree Search. By further incorporating coarse-grained LLM-based clustering, it substantially improves scalability to large candidate sets by restricting acquisition function evaluations to clusters with statistically higher property values. We show through extensive experiments and ablations that the proposed method substantially improves scalability, robustness, and sample efficiency in LLM-guided BO for molecular discovery.", "tldr": "", "keywords": ["Molecular discovery; LLM for Bayes Optimization; Machine Learning;"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/70019b02a06577a895fe71fe19e39ef62cc3e724.pdf", "supplementary_material": "/attachment/b13fc68a277d7112055a6bfff17e019f338abbfb.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes LLMAT, a surrogate-free BO framework for molecular discovery that directly uses acquisition functions via binary classifiers and recursively partitions with a tree of local AFs selected via MCTS."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Abundant ablation studies."}, "weaknesses": {"value": "### Classifier vs. Surrogate model\n\nThe paper drop surrogate by training binary classifiers that are used to approximate AFs. In fact, this is still similar to using a surrogate model + UCB. UCB's computational cost is nearly zero, and the binary classifier corresponds to the surrogate model, so the computational cost remains the same as before. \n\n### Meta-learning \n\nThe method mentions meta-learning (Reptile) for initializing the shared classification head, but there is no explicit meta-task definition like task distribution, task splits, etc. Why is this meta-learning rather than multi-task pretraining? Please formalize the meta-objective that would not be achieved by simple joint training. \n\n### Fig. 3 readability\n\nThe current presentation was hard to parse on first read. Since Fig. 8 contains the complete comparisons, please separate by model or by feature backbone to avoid mixing too many lines in one panel.\n\n### Missing results\n\nThe paper targets Kristiadi et al. (ICML’24) as a main baseline, but LLAMA and multi-objective settings discussed there are absent here. \n\n### Typos\n\nExamples include “teee”, “reference[?]”, and minor formatting issues."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eek74ITsvN", "forum": "5GsW2c7Y84", "replyto": "5GsW2c7Y84", "signatures": ["ICLR.cc/2026/Conference/Submission3375/Reviewer_sYX2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3375/Reviewer_sYX2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653177452, "cdate": 1761653177452, "tmdate": 1762916694678, "mdate": 1762916694678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LLMAT is a novel framework that bypasses surrogate modeling and directly models acquisition functions (AF) by obtaining rich priors from foundation models. In addition, it develops a new approach that performs candidate selection based on LLM-based clustering to improve the efficiency of local AF prediction. As a result, unlike LLM-based approaches that rely on in-context learning (ICL) and prompt engineering, the proposed method presents an effective way to leverage LLMs for Bayesian Optimization (BO)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- LLMAT bypasses the explicit training of surrogate models such as GP and BNN and directly estimates AF. This approach has the advantage of reducing the high computational and memory costs that typically arise when data are scarce or when using high-dimensional LLM embeddings.\n- The LLM-based clustering in LLMAT presents an efficient way to utilize general LLMs, reducing the scope of local AF evaluation while maintaining strong performance."}, "weaknesses": {"value": "- LLMAT consists of several complex components, including LFBO, a meta-learning–based tree partition, LLM clustering, and statistical cluster filtering. This complexity makes the overall framework harder to understand and increases the number of hyperparameters that require tuning, which may make it difficult to ensure stability in practical applications.\n- The paper does not provide an analysis of how the clustering is affected by different prompts, and there may be potential reproducibility issues caused by hallucination.\n- The comparative analysis with LLM-based models (MOLLEO[1], LICO[2]) and BO methods[3,4] is somewhat limited\n\n\n[1] Wang, Haorui, et al. \"Efficient evolutionary search over chemical space with large language models.\" arXiv preprint arXiv:2406.16976 (2024).\n\n[2] Nguyen, Tung, and Aditya Grover. \"Lico: Large language models for in-context molecular optimization.\" arXiv preprint arXiv:2406.18851 (2024).\n\n[3] Song, Jiaming, et al. \"A general recipe for likelihood-free Bayesian optimization.\" International conference on machine learning. PMLR, 2022.\n\n[4] Tiao, Louis C., et al. \"BORE: Bayesian optimization by density-ratio estimation.\" International conference on machine learning. PMLR, 2021."}, "questions": {"value": "- When performing clustering guided by LLMs, is the consistency of the prompt maintained across different runs? Although in-context learning (ICL) may not be necessary, when molecules become longer or more complex, would it be useful to strengthen the prompt? Alternatively, if multiple LLMs were used together in an ensemble, could that lead to more robust results?\n- In related work, LAPLACE employed general LLMs such as T5, GPT-2, and Llama2. Why was Llama2 excluded in this paper? In addition, is there a specific reason the experiments were conducted only for single-objective optimization? (LAPLACE appears to include multi-objective experiments as well.)\n- Among the BO methods mentioned in the paper, why were only LAPLACE and GP used for comparison? Is there a reason why experiments with LFBO[3] and BORE[4] were not conducted?\n- Is the quantile γ used to determine the threshold fixed at 0.5? It would be helpful to include an ablation study on γ to examine how sensitively LLMAT’s performance and stability respond to changes in this parameter.\n- Would it be possible to compare LLMAT with other LLM-based methods for exploring chemical space, such as MOLLEO[1] and LICO[2]? Although a direct comparison may be difficult since MOLLEO and LICO are designed specifically for PMO tasks (single- or multi-objective optimization), including such comparisons could still provide useful insights into the limitations of existing models related to API calls and prompt engineering.\n- In Section 4.1, “Fixed Features and Foundation Models,” does the term general LLMs refer to models used for extracting prior features? The same term also appears in the context of clustering, which may cause some confusion. (Clarifying the terminology would be helpful)\n- The “Introduction” Section appears to be missing, which seems to have caused the section numbers throughout the paper to shift. Please check if similar formatting issues exist elsewhere. In Section 4.1 (“Setting”), the phrase “(4) maximizing fluorescence oscillator strength for laser materials (?)” is LaTeX mapping error. In addition, in the “Meta-learning Shared Binary Classifiers” section, the word “teee” seems to be a typo. Lastly, in the second paragraph of the Introduction, “AF prior” might be written as “Acquisition Function (AF)” since this is the first time the term appears in the text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JqDvRFFQji", "forum": "5GsW2c7Y84", "replyto": "5GsW2c7Y84", "signatures": ["ICLR.cc/2026/Conference/Submission3375/Reviewer_Z59Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3375/Reviewer_Z59Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800025829, "cdate": 1761800025829, "tmdate": 1762916694271, "mdate": 1762916694271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LLMAT (LLM-guided Acquisition Tree), which is a likelihood-free Bayesian Optimization method for molecular discovery that directly leverages priors from LLMs and chemistry foundation models to inform acquisition functions without explicit surrogate modeling. The method uses a decoder-only transformer with features extracted from foundation models (e.g., T5-Chem, MolFormer, GPT-2) or general LLMs to train binary classifiers that simultaneously partition the molecular candidate space into a tree structure via Monte Carlo Tree Search (MCTS) and estimate local acquisition functions at each tree node based on density ratio estimation from Equation 3. To improve stability in low-data regimes, the authors propose meta-learning shared LoRA weights and root node classifier initialization across tree nodes using a sequential Reptile-style update. Additionally, the method incorporates an LLM-based pre-clustering strategy that queries general LLMs (e.g., GPT-4o) once to assign coarse property-based cluster labels (e.g., \"low,\" \"medium,\" \"high\") to the entire candidate set, then uses statistical tests (Welch's ANOVA and Games-Howell post-hoc test) to filter out clusters with significantly lower property values, restricting acquisition function evaluations only to promising clusters to reduce computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed likelihood-free BO method bypasses explicit surrogate modeling by directly estimating acquisition functions through binary classifiers. This design choice may reduce training time and computational cost.\n\n2. The method leverages general-purpose LLMs to perform property-aware clustering with a single API call per dataset, which is orders of magnitude cheaper than in-context learning approaches that require accumulated API queries at each BO iteration."}, "weaknesses": {"value": "**1. Poor formatting and lack of professionalism**\n- The paper has a critical structural error: Section 1 is titled \"Related Work\" instead of \"Introduction,\" suggesting the Introduction section was accidentally deleted or improperly formatted during submission. While this may be unintentional, it significantly undermines the paper's professionalism and indicates insufficient proofreading before submission. This raises concerns about the thoroughness of the authors' quality control throughout the work.\n\n**2. Overly complex methodology with questionable necessity of components**\n- The proposed method combines an excessive number of disparate components: (1) MCTS-based hierarchical partitioning, (2) meta-learning for shared binary classifiers, (3) LLM-based clustering, and (4) statistical cluster filtering with Welch's ANOVA and Games-Howell tests. The necessity of each component is not convincingly justified.\n\n- No clear ablation study demonstrates that all these components are essential rather than incremental add-ons\n\n- The complexity makes the method difficult to reproduce and apply to new settings, as each component introduces additional hyperparameters \n\n- This complex design may be overly specialized for specific datasets, raising concerns about its generalizability to other molecular discovery tasks and domains.\n\n- Other researchers may face implementation challenges due to the intricate dependencies between components\n\n**3. Limited novelty in contributions**\n- The likelihood-free BO framework is already a well-established method, and the proposed approach does not appear to introduce substantial algorithmic innovation\n\n- The tree-based partitioning with binary classifiers is also adapted from existing work, and the core idea lacks novelty\n\n- The main contribution appears to be the engineering combination of existing techniques rather than algorithmic innovation\n\n- LLM-based clustering for coarse property grouping may be useful but remains a relatively simple application of prompting\n\n**4. Insufficient justification or explanation for using BO**\n- The authors do not adequately justify why BO is presented as the most appropriate framework for molecular discovery, especially in comparison with well-established alternative approaches\n\n- Genetic algorithms have been successfully applied to molecular optimization and often achieve strong performance with simpler implementation\n\n- Reinforcement learning–based methods have demonstrated strong performance in molecular discovery and can remain effective even in low-data regimes by leveraging existing offline data through offline model-based optimization or offline reinforcement learning [1] \n\n- Graph neural networks combined with grammar-based methods [2] can also perform well in low-data regimes—so it remains unclear why BO is presented as the preferred solution\n\n---\n[1] Shin et al., \"Offline Model-based Optimization for Real-World Molecular Discovery.\" ICML, 2025\n\n[2] Guo et al., \"Data-Efficient Graph Grammar Learning for Molecular Generation.\" ICLR, 2022"}, "questions": {"value": "**1. Why is there no comparison with pure LLM-based molecular discovery methods?**\n- The paper claims to leverage LLMs for molecular discovery but does not compare against recent pure LLM-based molecular optimization methods such as MOLLEO [3] or other LLM-driven approaches that directly generate or optimize molecules. Given that the authors argue LLMs provide valuable priors for molecular design, it is critical to demonstrate whether the proposed BO framework with LLM features actually outperforms direct LLM-based generation methods. Without this comparison, it is unclear whether the added complexity of the BO framework is necessary or if simpler LLM-based approaches would suffice.\n\n**2. Why does the paper only focus on single-objective optimization when multi-objective optimization is more relevant for molecular discovery?**\n- Other LLM-based methods for molecular discovery, such as MOLLEO, address not only single-objective but also multi-objective molecular optimization, reflecting the reality that drug discovery must simultaneously satisfy multiple molecular properties. However, this paper evaluates only single-objective tasks for each dataset, raising the question of why multi-objective scenarios were not considered.\n\n---\n[3] Wang et al., \"Efficient evolutionary search over chemical space with large language models.\" ICLR, 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VUVuUnypee", "forum": "5GsW2c7Y84", "replyto": "5GsW2c7Y84", "signatures": ["ICLR.cc/2026/Conference/Submission3375/Reviewer_Ui7f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3375/Reviewer_Ui7f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807216380, "cdate": 1761807216380, "tmdate": 1762916694050, "mdate": 1762916694050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a likelihood-free Bayesian optimization method called LLMAT (LLM-guided acquisition tree) applied to molecular discovery. The key selling point of this approach is in its use of chemistry foundation models to help bypass the costly training of surrogate models and to act as priors for molecular property prediction in low data settings. This method combines several innovations. It learns a tree-structured partition of the molecular space with local acquisition functions, enabling efficient candidate selection via MCTS. It uses a meta-learning approach to stabilize the acquisition functions in the low data setting. And finally, it incorporates an LLM directed clustering strategy to restrict acquisitions to statistically promising regions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors propose an original approach (as far as I can tell) for combining Bayesian optimization with LLMs that helps to avoid a key practical challenge, namely in training the surrogate model. This approach integrates knowledge from LLMs directly into the acquisition function, entirely bypassing costly surrogate model training. This appears to be a rather significant departure from prior work and has both speed/time complexity benefits and can potentially alleviate limitations in molecular discovery in low-data regimes. The paper is of good quality, with extensive experiments and detailed supplementary materials that includes many auxiliary analyses that are of interest. I found the paper's clarity and overall presentation to be moderate - I think it is a bit difficult for a reader who is not well versed in some of the background material."}, "weaknesses": {"value": "A weakness of this work is that it is lacking more comprehensive empirical validation of its central claims regarding scalability and real-world efficiency compared to non-LLM based approaches to molecular discovery, which are established. It would be helpful to have a synthetic setting in which the property landscape is known to be multimodal and continuous and assess the extent to which the proposed approach is able to effectively acquire the known high scoring designs. This would help in demonstrating the behavior of the localized search."}, "questions": {"value": "How well does the method perform when there is noise in the small labeled dataset?\n\nTo what extent is the final performance sensitive to slight variations in the prompt language?\n\nI may have missed it, but do you have an experiment that uses traditional BO where the features are derived from the fixed LLM embeddings so as to better understand the benefit of the proposed AF design?\n\nHave you compared against more traditional, non-LLM based approaches to these same problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lsnPsM5ZJb", "forum": "5GsW2c7Y84", "replyto": "5GsW2c7Y84", "signatures": ["ICLR.cc/2026/Conference/Submission3375/Reviewer_e9q6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3375/Reviewer_e9q6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964076050, "cdate": 1761964076050, "tmdate": 1762916693676, "mdate": 1762916693676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}