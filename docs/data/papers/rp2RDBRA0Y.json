{"id": "rp2RDBRA0Y", "number": 14178, "cdate": 1758229772870, "mdate": 1763612986084, "content": {"title": "The Value of Information in Human-AI Decision-making", "abstract": "Multiple agents are increasingly combined to make decisions with the expectation of achieving complementary performance, where the decisions they make\ntogether outperform those made individually. However, knowing how to improve\nthe performance of collaborating agents requires knowing what information and\nstrategies each agent employs. With a focus on human-AI pairings, we contribute a\ndecision-theoretic framework for characterizing the value of information. By defining complementary information, our approach identifies opportunities for agents to\nbetter exploit available information–in AI-assisted decision workflows. We present\na novel explanation technique (ILIV-SHAP) that adapts SHAP explanations to\nhighlight human-complementing information. We validate the effectiveness of\nthe framework on examples from chest X-ray diagnosis and deepfake detection\nand ILIV-SHAP through a study of human-AI decision-making. We also find that\npresenting ILIV-SHAP with AI predictions leads to reliably greater reductions in\nerror over non-AI assisted decisions more than vanilla SHAP.", "tldr": "We emphasize the value of complementary information in AI-assisted human decision making.", "keywords": ["Explanation", "Human-AI complementarity", "Decision theory"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f46be1b4317a18ecff8896dd301df7d092815fad.pdf", "supplementary_material": "/attachment/2465325569a4c5823d15f342580d4ec544a42e39.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a decision-theoretic framework to quantify the *value of information* in human-AI decision workflows. It formalizes (i) *Agent-Complementary Information Value (ACIV)* (the marginal payoff gain from adding a signal (V) beyond what is already encoded in an agent's decisions ($D_b$)) and (ii) an *instance-level* analogue (*ILIV*) to analyze specific realizations. Building on ILIV, the authors introduce *ILIV-SHAP*, an explanation method that attributes feature importance to *complementary* information rather than to predictive contribution alone. A preregistered Prolific study on *house-price prediction* ($n=421$) crosses two models with different ex-ante ACIV (AI1 high vs. AI2 low) and three explanation conditions (ILIV-SHAP+SHAP / SHAP / None). Results indicate that (a) the higher-ACIV model yields larger error reductions for human-AI teams, and (b) *ILIV-SHAP+SHAP* outperforms *SHAP* alone in reducing absolute percentage error (APE). The framework is further demonstrated on *chest X-ray diagnosis* and *deepfake detection* to show how ACIV/ILIV expose complementarity patterns between humans and models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* *Clear formalization of 'information value' in teaming.* The ACIV/ILIV definitions make the complementarity question precise, grounding it in Bayesian decision theory and proper scoring rules (incl. a robustness view). This provides a principled upper-bound benchmark and a way to diagnose under-use of available signals. \n* *Practical algorithms and instance-level lens.* Algorithms 1-2 give workable estimators for ACIV/ILIV, enabling empirical analysis without requiring full generative models of all signals.  \n* *Explanation innovation (ILIV-SHAP).* Reframing attribution around *decision-relevant payoff gains* (rather than prediction deltas) is original and well-motivated; the method preserves SHAP axioms while targeting complementarity. \n* *Empirical evidence for design claims.* In the crowdsourced study, the high-ACIV model (AI1) shows bigger APE reductions than the low-ACIV model (AI2), and ILIV-SHAP+SHAP yields larger gains than SHAP alone for AI1 (e.g., 7.11% vs. 5.20% for SHAP; CI reported). The analysis includes hierarchical modeling and multiple-comparison controls.  \n* *External demonstrations.* The chest-X-ray and deepfake case studies illustrate how the framework can compare agents and reveal which features/signals are under- or over-utilized by humans vs. models."}, "weaknesses": {"value": "1. *Human-AI complementarity definition*. In the related work section, the *human-AI complementarity* term is, in my opinion, not properly defined and there is also lack of related and important related work missing [1-5]. In this area, complementarity refers to team decisions that, in expectation, outperform both the human and the AI alone [Bansal et al 2021a(already cited),1,2], rather than just analyzing the *information value and use*, as author's state in the related work. However, imo, authors' use a better definition in the first paragraph of the introduction. \n2. *Complementarity by design related work*. While I understand the authors' analysis of the value of additional information from one of the agents, I believe they also make a claim in the paper that existing complementarity approaches \"do not account for the best achievable performance given the information available at the time of the decision\" (l. 41). I find this to be an overstatement and a shortcoming in the related work section. There are work in the literature proposing existing complementarity approaches **by design**. These methods exploit information asymmetry (where humans possess additional contextual knowledge, which the authors identify as a central aspect of human-AI complementarity) and offer principled methods with **provable guarantees** in decision-making about the better perfomance of the teaming. Specific applications of these methods include classification [3,4] or matching problems [5]. These methods effectively identify parameters that model collaboration, achieving what the authors define as the \"best achievable performance given the information available at the time of the decision\". I recommend revising the Related Work section to accurately position your contribution in relation to this body of research, highlighting the differences in goals, assumptions, and guarantees. Anticipating author's comments, I want to emphasize that I do not see these approaches as substitutes for the authors' contribution. Rather, *my concern is with how the introduction and related work is positioned*: some of the current statements underrepresent what prior work already achieves in this space. In fact, the authors' proposed framework (which focuses on quantifying and analyzing the value of additional information) could even complement and strengthen these existing methods. I appreciate that the authors are not proposing a new method per se but a general framework, and I believe clarifying this positioning would make the paper's contribution more precise and impactful.\n3. *Estimator dependence and potential leakage.* ACIV/ILIV are estimated via learned regressors ($\\hat a, \\hat a_b$). The main text does not detail safeguards (strict train/validation/test splits aligned with the team-vs-human comparisons, calibration of posteriors, or sensitivity to the choice of (A)). Since the conclusions rely on these regressors approximating posteriors used in the rational benchmark, a *sensitivity analysis* (alternate model classes, calibration checks, and ablations on (A)) would strengthen validity.\n4. *Flow of theory section.* The notation and explanation is dense and introduces granularly: mixing state, signals, decisions, and payoffs  are presented and used before an anchoring example. The exposition would benefit from a *running toy example* (e.g., the umbrella example) threaded through Defs. 3.1-3.3 and Algorithms 1-2, plus a *table of symbols* and an 'intuitions first' paragraph for ACIV vs. ILIV. \n5. *SHAP computation*. There is no optimization on the SHAP computation: shap is O(2^n) in the number of features, so for high-dimensional data this can be very slow. Some approximation methods exist (e.g., sampling-based), but they are not discussed. In addition, the paper does not address the potential for bias in the SHAP values (e.g. not stable to noise) [6], which could impact the fairness of the model.\n6. *Intervention realism in the main user study.* The two 'uninterpretable' features (Feature X/Y) are relabeled to be intentionally opaque, which creates complementarity by construction. This is a clean testbed but may *overstate* ILIV-SHAP benefits relative to realistic domains where human observables partially correlate with those features. I'd appreciate if authors discuss this and add a variant where 'semi-interpretable' proxies are available to humans.\n7. *Cognitive load and UI details for ILIV-SHAP.* The paper states that ILIV-SHAP 'ranked and highlighted' features exceeding a threshold, but there's limited information on *interface design*, thresholds, and how users processed both SHAP and ILIV-SHAP simultaneously. Add representative screenshots of the UI and an *attention/time* analysis to verify that added information did not induce confusion or anchoring.\n8. *Ethical nuance in DeepFake study.* In the deepfake study, the discussion of the 'dark skin' feature as informative should more explicitly acknowledge fairness and representation concerns and potential spurious correlations when translating these insights into deployed systems. Even if out of scope, a short cautionary note would be responsible. Authors could add a brief fairness statement in the deepfake section.\n\n*Minor*:\n\n* Some key p-values and sample sizes appear only in Appendix D/Table 2-3. Bring the primary contrasts (AI1 vs. AI2; ILIV-SHAP+SHAP vs. SHAP) with effect sizes and corrected p-values into the main text to improve readability and soundness of the results.\n* Fig 1 left and Fig 2 are a bit hard to read due to small fonts.\n* Bring key *CIs/p-values* and *N per condition* from Appendix D into the main text/figure caption.\n* ACIV abbreviation is mentioned in the abstract before being defined.\n* Missing space in line 135 \"...available to a DM, including...\"\n\nIn sum, the paper tackles an important and timely problem (measuring complementarity in human-AI teams) with a principled framework (ACIV/ILIV) and a neat explanatory tool (ILIV-SHAP). The conceptual contribution is solid and potentially influential. The empirical study is well-designed and aligns with the theory, and the demonstrations are informative. I'm slightly below the bar primarily due to (i) limited sensitivity analyses for the learned estimators behind ACIV/ILIV, (ii) the constructed complementarity in the user study, (iii) presentation density in the theory sections, and (iv) concerns in the related work and introduction. Addressing the listed weaknesses would likely move this into acceptance.\n\n> [1] Bayesian modeling of human-AI complementarity. PNAS 2022.\n\n> [2] Complementarity in human-AI collaboration: Concept, sources, and evidence. EJIS 2025.\n\n> [3] Improving Expert Predictions with Conformal Prediction. ICML 2023.\n\n> [4] Towards human-AI complementarity with prediction sets. NeurIPS 2024.\n\n> [5] Towards Human-AI Complementarity in Matching Tasks. HLDM @ ECML-PKDD 2025.\n\n> [6] On the failings of Shapley values for explainability. IJAR 2024."}, "questions": {"value": "1.  How sensitive are ACIV/ILIV estimates to the choice of model class and calibration? Could you report a *model-sweep* (e.g., LR vs. GBM vs. NN) and reliability diagrams for posterior quality used in Eq. (1)/(2)? \n2.  What exact *thresholding* produced the highlighted features, and how were ties/instability handled across permutations in the SHAP-style computation? Please include a *UI figure* and an ablation where only ILIV-driven highlights (without SHAP bars) are shown. \n3. Did you measure *time-on-task*, confidence, or reliance behaviors to ensure gains weren't due to simple anchoring on model outputs? Any heterogeneity across participants (e.g., numeracy)? \n4. Can you replicate the user-study result on a setting where human-visible proxies to X/Y exist (weaker complementarity), and test whether ILIV-SHAP still outperforms SHAP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q9JACDwPSx", "forum": "rp2RDBRA0Y", "replyto": "rp2RDBRA0Y", "signatures": ["ICLR.cc/2026/Conference/Submission14178/Reviewer_A4pM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14178/Reviewer_A4pM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761577576792, "cdate": 1761577576792, "tmdate": 1762924635258, "mdate": 1762924635258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a decision-theoretic framework to quantify the *value of information* in human-AI decision workflows. It formalizes (i) *Agent-Complementary Information Value (ACIV)* (the marginal payoff gain from adding a signal (V) beyond what is already encoded in an agent's decisions ($D_b$)) and (ii) an *instance-level* analogue (*ILIV*) to analyze specific realizations. Building on ILIV, the authors introduce *ILIV-SHAP*, an explanation method that attributes feature importance to *complementary* information rather than to predictive contribution alone. A preregistered Prolific study on *house-price prediction* ($n=421$) crosses two models with different ex-ante ACIV (AI1 high vs. AI2 low) and three explanation conditions (ILIV-SHAP+SHAP / SHAP / None). Results indicate that (a) the higher-ACIV model yields larger error reductions for human-AI teams, and (b) *ILIV-SHAP+SHAP* outperforms *SHAP* alone in reducing absolute percentage error (APE). The framework is further demonstrated on *chest X-ray diagnosis* and *deepfake detection* to show how ACIV/ILIV expose complementarity patterns between humans and models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* *Clear formalization of 'information value' in teaming.* The ACIV/ILIV definitions make the complementarity question precise, grounding it in Bayesian decision theory and proper scoring rules (incl. a robustness view). This provides a principled upper-bound benchmark and a way to diagnose under-use of available signals. \n* *Practical algorithms and instance-level lens.* Algorithms 1-2 give workable estimators for ACIV/ILIV, enabling empirical analysis without requiring full generative models of all signals.  \n* *Explanation innovation (ILIV-SHAP).* Reframing attribution around *decision-relevant payoff gains* (rather than prediction deltas) is original and well-motivated; the method preserves SHAP axioms while targeting complementarity. \n* *Empirical evidence for design claims.* In the crowdsourced study, the high-ACIV model (AI1) shows bigger APE reductions than the low-ACIV model (AI2), and ILIV-SHAP+SHAP yields larger gains than SHAP alone for AI1 (e.g., 7.11% vs. 5.20% for SHAP; CI reported). The analysis includes hierarchical modeling and multiple-comparison controls.  \n* *External demonstrations.* The chest-X-ray and deepfake case studies illustrate how the framework can compare agents and reveal which features/signals are under- or over-utilized by humans vs. models."}, "weaknesses": {"value": "1. *Human-AI complementarity definition*. In the related work section, the *human-AI complementarity* term is, in my opinion, not properly defined and there is also lack of related and important related work missing [1-5]. In this area, complementarity refers to team decisions that, in expectation, outperform both the human and the AI alone [Bansal et al 2021a(already cited),1,2], rather than just analyzing the *information value and use*, as author's state in the related work. However, imo, authors' use a better definition in the first paragraph of the introduction. \n2. *Complementarity by design related work*. While I understand the authors' analysis of the value of additional information from one of the agents, I believe they also make a claim in the paper that existing complementarity approaches \"do not account for the best achievable performance given the information available at the time of the decision\" (l. 41). I find this to be an overstatement and a shortcoming in the related work section. There are work in the literature proposing existing complementarity approaches **by design**. These methods exploit information asymmetry (where humans possess additional contextual knowledge, which the authors identify as a central aspect of human-AI complementarity) and offer principled methods with **provable guarantees** in decision-making about the better perfomance of the teaming. Specific applications of these methods include classification [3,4] or matching problems [5]. These methods effectively identify parameters that model collaboration, achieving what the authors define as the \"best achievable performance given the information available at the time of the decision\". I recommend revising the Related Work section to accurately position your contribution in relation to this body of research, highlighting the differences in goals, assumptions, and guarantees. Anticipating author's comments, I want to emphasize that I do not see these approaches as substitutes for the authors' contribution. Rather, *my concern is with how the introduction and related work is positioned*: some of the current statements underrepresent what prior work already achieves in this space. In fact, the authors' proposed framework (which focuses on quantifying and analyzing the value of additional information) could even complement and strengthen these existing methods. I appreciate that the authors are not proposing a new method per se but a general framework, and I believe clarifying this positioning would make the paper's contribution more precise and impactful.\n3. *Estimator dependence and potential leakage.* ACIV/ILIV are estimated via learned regressors ($\\hat a, \\hat a_b$). The main text does not detail safeguards (strict train/validation/test splits aligned with the team-vs-human comparisons, calibration of posteriors, or sensitivity to the choice of (A)). Since the conclusions rely on these regressors approximating posteriors used in the rational benchmark, a *sensitivity analysis* (alternate model classes, calibration checks, and ablations on (A)) would strengthen validity.\n4. *Flow of theory section.* The notation and explanation is dense and introduces granularly: mixing state, signals, decisions, and payoffs  are presented and used before an anchoring example. The exposition would benefit from a *running toy example* (e.g., the umbrella example) threaded through Defs. 3.1-3.3 and Algorithms 1-2, plus a *table of symbols* and an 'intuitions first' paragraph for ACIV vs. ILIV. \n5. *SHAP computation*. There is no optimization on the SHAP computation: shap is O(2^n) in the number of features, so for high-dimensional data this can be very slow. Some approximation methods exist (e.g., sampling-based), but they are not discussed. In addition, the paper does not address the potential for bias in the SHAP values (e.g. not stable to noise) [6], which could impact the fairness of the model.\n6. *Intervention realism in the main user study.* The two 'uninterpretable' features (Feature X/Y) are relabeled to be intentionally opaque, which creates complementarity by construction. This is a clean testbed but may *overstate* ILIV-SHAP benefits relative to realistic domains where human observables partially correlate with those features. I'd appreciate if authors discuss this and add a variant where 'semi-interpretable' proxies are available to humans.\n7. *Cognitive load and UI details for ILIV-SHAP.* The paper states that ILIV-SHAP 'ranked and highlighted' features exceeding a threshold, but there's limited information on *interface design*, thresholds, and how users processed both SHAP and ILIV-SHAP simultaneously. Add representative screenshots of the UI and an *attention/time* analysis to verify that added information did not induce confusion or anchoring.\n8. *Ethical nuance in DeepFake study.* In the deepfake study, the discussion of the 'dark skin' feature as informative should more explicitly acknowledge fairness and representation concerns and potential spurious correlations when translating these insights into deployed systems. Even if out of scope, a short cautionary note would be responsible. Authors could add a brief fairness statement in the deepfake section.\n\n*Minor*:\n\n* Some key p-values and sample sizes appear only in Appendix D/Table 2-3. Bring the primary contrasts (AI1 vs. AI2; ILIV-SHAP+SHAP vs. SHAP) with effect sizes and corrected p-values into the main text to improve readability and soundness of the results.\n* Fig 1 left and Fig 2 are a bit hard to read due to small fonts.\n* Bring key *CIs/p-values* and *N per condition* from Appendix D into the main text/figure caption.\n* ACIV abbreviation is mentioned in the abstract before being defined.\n* Missing space in line 135 \"...available to a DM, including...\"\n\nIn sum, the paper tackles an important and timely problem (measuring complementarity in human-AI teams) with a principled framework (ACIV/ILIV) and a neat explanatory tool (ILIV-SHAP). The conceptual contribution is solid and potentially influential. The empirical study is well-designed and aligns with the theory, and the demonstrations are informative. I'm slightly below the bar primarily due to (i) limited sensitivity analyses for the learned estimators behind ACIV/ILIV, (ii) the constructed complementarity in the user study, (iii) presentation density in the theory sections, and (iv) concerns in the related work and introduction. Addressing the listed weaknesses would likely move this into acceptance.\n\n> [1] Bayesian modeling of human-AI complementarity. PNAS 2022.\n\n> [2] Complementarity in human-AI collaboration: Concept, sources, and evidence. EJIS 2025.\n\n> [3] Improving Expert Predictions with Conformal Prediction. ICML 2023.\n\n> [4] Towards human-AI complementarity with prediction sets. NeurIPS 2024.\n\n> [5] Towards Human-AI Complementarity in Matching Tasks. HLDM @ ECML-PKDD 2025.\n\n> [6] On the failings of Shapley values for explainability. IJAR 2024."}, "questions": {"value": "1.  How sensitive are ACIV/ILIV estimates to the choice of model class and calibration? Could you report a *model-sweep* (e.g., LR vs. GBM vs. NN) and reliability diagrams for posterior quality used in Eq. (1)/(2)? \n2.  What exact *thresholding* produced the highlighted features, and how were ties/instability handled across permutations in the SHAP-style computation? Please include a *UI figure* and an ablation where only ILIV-driven highlights (without SHAP bars) are shown. \n3. Did you measure *time-on-task*, confidence, or reliance behaviors to ensure gains weren't due to simple anchoring on model outputs? Any heterogeneity across participants (e.g., numeracy)? \n4. Can you replicate the user-study result on a setting where human-visible proxies to X/Y exist (weaker complementarity), and test whether ILIV-SHAP still outperforms SHAP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q9JACDwPSx", "forum": "rp2RDBRA0Y", "replyto": "rp2RDBRA0Y", "signatures": ["ICLR.cc/2026/Conference/Submission14178/Reviewer_A4pM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14178/Reviewer_A4pM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761577576792, "cdate": 1761577576792, "tmdate": 1763551888282, "mdate": 1763551888282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper offers a decision-theoretic approach to analyze (and improve) human-AI decision-making by quantify the \"value of information\" employed by each (AI or human) agent. It introduces a Bayesian decision-theoretic framework for evaluating the performance of agents against a \"rational\" benchmark that represents the best-attainable payoff given a set of signals. It also proposes new metrics based on this framework: Agent-Complementary Information Value (ACIV), to quantify the global value an (AI) agent adds to (human's) existing information, and Instance-Level ACIV (ILIV), to quantify this value for a specific decision. ACIV is formalized as an upper bound using a learned surrogate for the unknown data-generating process, while ILIV is operationalized by a counterfactual payoff definition and an estimator. The authors further propose ILIV-SHAP, a variant of popular SHAP explanation method to highlight features that contribute most to the AI's \"complementary\" information value, rather than just its prediction. The authors validate this framework through a crowdsourced experiment on house price prediction, claiming that ACIV can identify the better AI teammate and that ILIV-SHAP improves team performance more than standard SHAP. Demonstrations on chest X-ray and deepfake detection are also provided."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **[S1] Novel and Important Problem:** The paper tackles a central problem in human-AI interaction: the failure to achieve complementarity.. The motivation to move beyond simple accuracy metrics and develop a formal theory of \"information value\" is both strong and timely.\n- **[S2] Crisp formalism:** The core idea of defining a Bayesian rational benchmark, $R(V)$, as the best-attainable performance is theoretically sound and provides an elegant upper bound to quantify agent sub-optimality, as noted in prior work (Guo et al., 2024). Algorithmic recipes make the framework implementable in principle, and the proposal of ILIV-SHAP that mirrors SHAP while targeting complementarity is conceptually interesting (offering a nice shift from \"why did the AI make this prediction?\" to \"why should the human listen to this prediction?\").\n- **[S3] Broader demonstrations.** The chest-X-ray and deepfake case studies help situate the framework beyond the main user study."}, "weaknesses": {"value": "-  **[W1] Weak and Confounded Experimental Validation of ILIV-SHAP:** The primary experiment (Section 5) designed to validate ILIV-SHAP (RQ2) suffers from multiple issues, making it impossible to attribute the observed performance lift to the proposed method.\n    * **Problematic Baseline:** The results for the main AI model (AI1) show that \"No explanation\" (6.50% error reduction) significantly *outperformed* \"SHAP\" alone (5.20%). This critical finding, which suggests standard SHAP was actively *harmful* to user performance, is reported in the plot but never discussed. The paper's headline claim that ILIV-SHAP + SHAP (7.11%) is superior to SHAP (5.20%) is misleading, as it uses a broken baseline. The more meaningful comparison to \"No Explanation\" (7.11% vs. 6.50%) shows a much smaller improvement, which is neither statistically nor practically significant.\n    * The experiment does not compare \"SHAP\" vs. \"ILIV-SHAP,\" and appears to run into **additive information confound** instead. It compares \"SHAP\" (control) vs. \"ILIV-SHAP *and* SHAP\" (treatment). The treatment group received strictly more information (two explanations) than the control (one). The reported improvement could simply be an effect of information dosage, not the specific content of ILIV-SHAP. The two conditions are not presented identically, leading to possible **presentational confound**. The SHAP condition is a standard table (Fig. 9). The ILIV-SHAP + SHAP condition includes *visual highlighting* of specific rows and a *direct textual hint* (Fig. 11). This simple, salient cue is a powerful intervention on its own and is not part of the core ILIV-SHAP contribution. The performance gain is likely attributable to this simple UI intervention, not a deep understanding of information value.\n\n- **[W2] The Practical Utility of ACIV is Overstated:** The data shows that while ACIV may predict the direction of improvement, it is a poor predictor of the *magnitude*. In the main experiment (Fig 1, left), AI1 has more than double the ACIV of AI2. This massive theoretical gap in complementarity translated to a *trivial difference* in actual team performance (6.27% vs 5.65% error reduction, collapsing across explanations). The observational study in Appendix (Table 1) shows the same weak link. The Art task has the highest ACIV (0.1772) and highest MSE reduction (0.0583). However, \"Sarcasm\" has a similarly high ACIV (0.1625) but its MSE reduction (0.037) is much smaller, and nearly identical to that of \"Cities\" (0.034), which had a far lower ACIV (0.0919). This weak correlation between the *metric's magnitude* and the *outcome's magnitude* severely limits its practical value for model selection.\n\n- **[W3] Weak \"Human-AI\" Framing and Reliance on an Idealistic Human Model:** The paper's \"human-AI\" framing is a significant weakness, as the core framework is general to any two agents (e.g., AI-AI) and fails to capture the unique and well-documented imperfections of human cognition.\n    * The framework's reliance on a \"rational Bayesian DM\" as the benchmark for optimal performance is an idealistic setup that is known to be a poor model for real human decision-makers, who are subject to cognitive biases and do not perform perfect Bayesian updates.\n    * Because the \"human\" component is modeled so abstractly, the paper fails to clearly position its contribution within specific, established human-AI collaboration paradigms, such as AI-assisted decision-making (where the human makes the final choice) versus learning-to-defer (where the system decides between agents) vs other \"human-in-the-loop\" options. Clear positioning would dictate which of the human-AI decision-making methods and baselines are relevant, and could require deeper engagement with additional literature (Noti et al., Zhang et al., Mahmood et al. etc.).\n    * A more rigorous evaluation in a controlled AI-AI setting might have been more effective at validating the core information-theoretic framework, as the current human experiment introduces significant, unmodeled \"human\" noise that obscures the results.\n\n- **[W4] Serious Practical and Computational Hurdles are Ignored:** The entire framework relies on a regression algorithm $\\mathcal{A}$ to proxy the \"rational Bayesian DM\" and estimate the posterior $Pr[\\omega | V]$.\n    * **Estimator Sensitivity:** The accuracy of ACIV and ILIV is critically dependent on the choice and quality of this surrogate model $\\mathcal{A}$. The paper provides no sensitivity analysis on this choice. If $\\mathcal{A}$ is a poor estimator, the entire framework produces meaningless numbers.\n    * **Intractability of ILIV-SHAP:** The ILIV-SHAP calculation (Definition 4.1) appears computationally intractable. It requires a combinatorial number of calls to the ILIV function. The ILIV function itself (Algorithm 2) requires conditioning on an exact signal match $v_j = v$. In any high-dimensional or continuous-feature domain, the probability of an exact match is zero. This means the set of matching instances $k$ will be 1 or 0, making the estimate undefined or infinitely high-variance. This is not a minor detail, and suggests the proposed method is unusable for most non-trivial problems.\n\n\n[Noti et al.] \"Learning When to Advise Human Decision Makers.\" In Proceedings of the 2023 International Joint Conference on Artificial Intelligence.\n\n[Zhang et al.]. \"Learning to Complement with Multiple Humans.\" arXiv preprint arXiv:2311.13172 (2023).\n\n[Mahmood et al.] \"Designing behavior-aware AI to improve the human-AI team performance in AI-assisted decision making.\" In Proceedings of the 2024 International Joint Conference on Artificial Intelligence."}, "questions": {"value": "- **Theoretical Inaccuracy in Appendix:** The appendix introduces a greedy algorithm (Algorithm 3) and claims that \"it orders the signals the same as the Shapley value\". This appears incorrect. A greedy algorithm finds a single ordering that approximates the maximum set value, whereas the Shapley value is a unique value attribution derived from *averaging* marginal contributions over *all possible* orderings. The claim indicates a conceptual misunderstanding and should be removed/corrected.\n\n- **Causal overreach in demonstrations.** In deepfake detection, the paper infers *why* humans underuse certain information (e.g., \"simply relying on AI predictions\"), but ACIV is *observational* and cannot identify causal cognitive mechanisms. Conclusions should be tempered."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QIfhMtYSM1", "forum": "rp2RDBRA0Y", "replyto": "rp2RDBRA0Y", "signatures": ["ICLR.cc/2026/Conference/Submission14178/Reviewer_r9GS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14178/Reviewer_r9GS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808349785, "cdate": 1761808349785, "tmdate": 1762924634382, "mdate": 1762924634382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper attempts to answer the question of whether complementing information improves human-AI collaboration. A preregistered between-subjects experiment with six conditions is performed. A novel instance-level explanation technique is proposed. Together with AI predictions, the new explanation technique appears to help reduce human-AI decision error."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The subject of the study is timely in the quick expansion of AI applications. \nHypotheses are tested with a wide choice of experimental conditions."}, "weaknesses": {"value": "The setup of the experiment needs significant improvement. \nThe claims presented in the paper are inconclusive."}, "questions": {"value": "1. Two fixed features, namely \"year remodeled\" and \"material and finish\", are selected as complementing information hidden from the human participants. To make the paper stronger, a varying subset of features of different sizes should be tested.   \n\n2. Noise is introduced to the selected complementing attributes to bring down the accuracy of AI1. The authors claim that  \"model accuracy differences would not confound our results\". Is this true?  Even if both models have similar accuracy, one model may consistently make higher predictions while the other produces random error. Humans may catch the consistency in AI1 and take advantage of it. To clarify, experiments should be set up so that AI1 is trained without noise, with a small amount of noise, and with a large amount of noise. This also helps to understand and quantify the impact of noise on the value of information. \n\n3. Were the participants aware that the highlighted features based on the ILIV-SHAP value are linked to human-complementary information? If so, this may incentivize the participants to always follow the AI as the AI knows more and consequently helps generate higher pay-off. \n\n4. For AI2, instead of using 0k as the SHAP value for features X and Y, a random value should be used. Otherwise, the participants would question the quality of AI and become more skeptical about AI2's predictions.  \n\n\nMinor comments:\n\nOn page 3, is there any difference between Pr[w] and pi(w)? Pi is referred to as a probability distribution (on page 3) and then an information model (on page 4). Are these two the same concepts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qs5l64lmZq", "forum": "rp2RDBRA0Y", "replyto": "rp2RDBRA0Y", "signatures": ["ICLR.cc/2026/Conference/Submission14178/Reviewer_C2Li"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14178/Reviewer_C2Li"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889472807, "cdate": 1761889472807, "tmdate": 1762924633866, "mdate": 1762924633866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers scenarios where multiple agents make a decision together, as in human-AI decision-making. The authors develop new metrics, ACIV and ILIV, which measure how much informational value the decision of an agent adds to the decision of another. Based on ILIV, they develop a new interpretability method called ILIV-SHAP, which highlights how much each feature contributes to the added informational value."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The goal of human-AI systems is to achieve complementary performance and the paper makes an important contribution to this effort, quantifying the complementary value of decisions made by different agents.\n\nThe proposed metrics are defined rigorously and presented very clearly. In particular, starting with the conceptually simple ACIV, and then extending it to the instance-wise case, ILIV, and finally incorporating it to SHAP is a good way of organizing the technical parts of the paper (with increasing complexity).\n\nPapers contributing metrics often struggle to provide concrete use cases. However, this paper uses ILIV with SHAP to create a new interpretability method, which is shown to improve human-AI accuracy with human-subject experiments.\n\nThe setup for the human-subject experiments is well thought out. The authors have created two versions of the AI agent with the same accuracy but varying ACIV and then observed ILIV-SHAP leads to greater reduction in errors when used with the higher ACIV model. This observation nicely ties all the ideas in the paper together."}, "weaknesses": {"value": "Motivation of the demonstrations in Section 6 is not clear. For instance, experiments for chest X-ray diagnosis and deepfake detection both vary different dimensions (AI models for chest X-rays and features for deepfakes) and report different metrics (ACIVs of combined decisions for chest X-rays and ACIVs of decisions combined with different features for deepfakes). It is not clear why the two settings are set up differently and what hypothesis drove the design of these setups.\n\nWhat I particularly found hard to understand with these setups: ACIV relative to an empty set is just an expected reward value that doesn’t consider complementarity. Although Figure 2a just reports ACIV relative to empty sets, I can look at the difference between green and orange or green and blue to reason about the complementary information each agent provides to the other. But I was not able to figure out what differences were relevant in Figure 2b."}, "questions": {"value": "ACIV measures how much one decision-maker (D^1) adds information to the decisions of another (D^2) by looking at how the optimal decisions made observing only D^2 differ from the optimal decisions made observing only D^1 and D^2. This seems like a sound measure, especially when the goal is to perform optimally, however isn’t it indirect in the sense that it involves a third decision-maker? What are the challenges associated with trying to quantify the impact of D^1 on D^2 directly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2s5qLAyaLZ", "forum": "rp2RDBRA0Y", "replyto": "rp2RDBRA0Y", "signatures": ["ICLR.cc/2026/Conference/Submission14178/Reviewer_3aDh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14178/Reviewer_3aDh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992589975, "cdate": 1761992589975, "tmdate": 1762924633267, "mdate": 1762924633267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for providing their comments and questions. We will reply by points below. We also submitted a revised version of the paper and highlighted the revised content in blue color."}}, "id": "cY2RrYaPfm", "forum": "rp2RDBRA0Y", "replyto": "rp2RDBRA0Y", "signatures": ["ICLR.cc/2026/Conference/Submission14178/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14178/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14178/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763491093673, "cdate": 1763491093673, "tmdate": 1763491093673, "mdate": 1763491093673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}