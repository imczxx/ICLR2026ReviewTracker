{"id": "EphTlUJ4XN", "number": 23805, "cdate": 1758348669943, "mdate": 1759896795995, "content": {"title": "PINFDiT: Energy-Based Physics-Informed Diffusion Transformers for General-purpose Time Series Tasks", "abstract": "Time series analysis underpins scientific advances. While specialized models have advanced various time series tasks, scientific domains face unique challenges: limited samples with complex physical dynamics, missing observations, multi-resolution sampling, and requirements for physical consistency. With the increasing demands on generative modeling capabilities, we introduce PINFDiT, a diffusion transformer-based model with physics injection during inference. Our approach combines a transformer backbone for capturing temporal dependencies with a comprehensive masking strategy that addresses imperfect data. The diffusion framework enables high-quality sample generation with inherent generative capability. In addition, our model-free physics-guided correction steers generated samples toward physically consistent solutions using calibrated Langevin dynamics, which balances distribution fidelity and physical law adherence without architectural modifications or retraining. Our evaluation demonstrates PINFDiT's effectiveness across multivariate forecasting with imperfect data, physics knowledge incorporation in data-limited scenarios, zero-shot and fine-tuning performance across diverse domains, establishing it as a proto-foundation model that bridges the gap between general-purpose and domain-specific models.", "tldr": "Physics-Guided Inference in Time Series Diffusion Transformers", "keywords": ["Diffusion; Transformer; Time Series; Physics Informed Machine Learning;Physics-Guided Inference in Time Series Diffusion Transformers"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f045d5cc6d5fb5697fa0ec74fed0b9e9e896e60.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes PINFDIT, a novel framework aiming to be a general-purpose model for time series analysis. It combines a Transformer architecture with a Diffusion framework to perform a variety of tasks, including forecasting, imputation, anomaly detection, and synthetic data generation. The core contributions of the paper are (1) a comprehensive masking strategy to handle imperfect time series data (missing values, multi-resolution) and (2) a novel \"energy-based physics-informed sampling\" technique that allows for the injection of physical laws at the inference stage without retraining or architectural modification.\n\nThis work tackles a very ambitious and important problem. The attempt to unify a pre-trained, general-purpose model with domain-specific physical knowledge is highly timely. The use of calibrated Langevin dynamics for correction at inference time is a practical and clever idea that could overcome the limitations of existing approaches like PINNs.\n\nHowever, there are several significant weaknesses regarding the paper's core claims, experimental fairness, and methodological clarity. Without addressing these, it is difficult to evaluate the generality and true contribution of the proposed framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Novel and Practical Physics-Injection Method:** Unlike traditional Physics-Informed Machine Learning (e.g., PINNs) which requires including PDE residuals in the training loss and retraining for specific problems, the proposed \"model-editing-free\" inference-time correction is highly practical. The ability to maintain the flexibility of a pre-trained general model while \"plugging in\" domain knowledge as needed is a major advantage.\n2.  **Comprehensive Problem Definition and Task Scope:** The paper has an ambitious goal of solving a wide range of time series tasks (forecasting, imputation, anomaly detection, generation) with a single model. It is also commendable that it directly addresses real-world data complexities such as missing values, multi-resolution sampling, and irregular intervals.\n3.  **Extensive Experimental Validation:** The authors have conducted a vast number of experiments to demonstrate their model's performance. This includes PDE simulator-based forecasting, practical forecasting on real-world data (climate, healthcare, finance), generative and imputation tasks, zero-shot performance, and detailed ablation studies. Achieving state-of-the-art (SOTA) performance on many benchmarks is impressive."}, "weaknesses": {"value": "1.  **Ambiguity in the Implementation of Physics Injection:**\n    * The concrete implementation of the physics-injection mechanism, one of the paper's core contributions, is critically unclear.\n    * Physical laws are quantified by an energy function $K(x^{tar};F)$ (the squared PDE residual).\n    * For correction during inference, the gradient of this energy function ($\\nabla K(x_{j}^{tar};F)$) is required (see Algorithm 1).\n    * **Critical Question:** $x^{tar}$ is discrete time series data. How are partial differential terms like $\\frac{\\partial x^{tar}}{\\partial t}$ and $\\frac{\\partial x^{tar}}{\\partial u_{i}}$ computed? If finite differences were used, there is no discussion of the associated discretization error, stability, or sensitivity to the choice of scheme. This information is essential for reproducing the methodology and verifying its validity.\n    * The authors claim they do not require a \"differentiable simulator\", but it appears they *do* require a \"differentiable (physical) residual function.\" The distinction and constraints are not adequately explained.\n\n2.  **Disconnect in \"General-Purpose\" and \"Foundation Model\" Claims:**\n    * The paper positions PINFDIT as a \"proto-foundation model\".\n    * However, the model's operation contradicts this claim. The model's pre-training (using the Chronos dataset) is entirely physics-agnostic and learns only statistical patterns.\n    * The physics injection is merely a post-hoc correction step (Algorithm 1, lines 6-8) that operates *outside* the pre-trained model during inference. This is less a \"Physics-Informed\" Transformer and more a \"General-Purpose Transformer\" combined with a \"Physics-Based Corrector Plugin.\" While this modularity is a strength, the title \"Physics-Informed\" is misleading as the model itself does not *learn* or *internalize* the physics.\n\n3.  **Unfair Experimental Comparisons and Lack of Consistency:**\n    * **Imputation (Tables 4 & 15):** The authors state, \"All baseline models are trained in a full-shot setting, while PINFDIT leverages a pre-trained foundation model, fine-tuning it on realistic datasets\". This is a **clearly unfair comparison**. PINFDIT benefits from pre-training on massive external data (Chronos, ~5B time points), while baselines like PatchTST and TimesNet are trained from scratch only on the task data (e.g., ETTh1). It is impossible to distinguish if PINFDIT's superiority comes from its architecture or simply from pre-training. A fair comparison would require reporting results for PINFDIT trained from scratch on the same data.\n    * **Anomaly Detection (Table 14):** The authors state they \"opted to bypass pretraining\" for this task and introduced an additional pre-processing step (Spectral Residue). This severely undermines the core narrative of a single foundation model for all tasks. If the pre-trained model is actually detrimental for anomaly detection (as it \"may inadvertently overfit by reconstructing anomalies\"), this clearly exposes a limitation of the proposed general-purpose model.\n\n4.  **Lack of Robustness Analysis for the Physics-Injection Plugin:**\n    * The paper only presents positive cases where physics injection improves performance (e.g., PDE simulations, ERA5 climate forecasting).\n    * There are no failure-case or sensitivity analyses. What happens if an incorrect physical law is injected (e.g., applying Burgers' equation to non-fluid data)? What happens if it's applied to data with no physical basis (e.g., the NASDAQ financial dataset)? Does performance degrade? Proving the validity of this modular approach requires such robustness checks."}, "questions": {"value": "1.  **(Re: W1)** Could you please provide a detailed explanation of how the partial differential terms and the gradient $\\nabla K$ in Eq. (5) and Eq. (8) are computed for discrete time series data $x^{tar}$?\n2.  **(Re: W3)** What was the specific reason for \"bypassing pretraining\" in the anomaly detection task? Does this imply that the pre-trained general-purpose model may be unsuitable for certain downstream tasks?\n3.  **(Re: W3)** For a fair comparison in the imputation experiments (Tables 4/15), can you provide results for PINFDIT trained from scratch (without pre-training)?\n4.  **(Re: W4)** Can you show what happens to performance when the physics-injection module (Alg. 1, lines 6-8) is applied with an intentionally incorrect physical law, or to a non-physical dataset like NASDAQ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pu33yhAQoA", "forum": "EphTlUJ4XN", "replyto": "EphTlUJ4XN", "signatures": ["ICLR.cc/2026/Conference/Submission23805/Reviewer_EgRK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23805/Reviewer_EgRK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714393522, "cdate": 1761714393522, "tmdate": 1762942813811, "mdate": 1762942813811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PINFDiT, a diffusion-transformer for time series that injects physics at inference by combining a learned conditonal model with an energy term that penalized PDE/physics residuals $K(x;\\mathcal{F})$. Sampling targets the Boltzmann-type density $\\propto \\exp{-K(x;\\mathcal{F})} - \\alpha \\log p_\\theta (x|x_\\text{con})$ via Langevin/gradient updates, so no retraining is required. Architecturally, the model uses temporal-wise attention and flexible masking to handle irregular and missing data. Experiments span PDE simulators, ERA5 temperature forecasting, long-horizon \"practical\" benchmarks with uncertain metrics, and imputation/anomaly detection and generation, where they report consistent gains over baselines and ablations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The most significant strength of this paper is its comprehensive evaluation. The authors conduct an extensive four-part experimental validation covering PDE simulations, real weather ERA5 data, multiple generative tasks (imputation, anomaly detection, etc) and zero shot generalization. The comparison agains a wide range of strong, modern baseline is thorough.\n- The paper tackles a highly relevant and challenging goal of creating a single, general-purpose model for scientific time series. This domain is critical, and the paper's explicit focus on imperfect data, physical constraints, and limited samples is well-motivated.\n- While individual components are not novel, making them work robustly across domains **at scale** is non-trivial. The idea of constrained generation without retraining is practically appealing. The generative and general-purpose capabilities are promising.\n- The foundation is principled, provides rigor and clarity. \n- The paper reports state-of-the-art or highly competitive results on the experiments."}, "weaknesses": {"value": "### Overstatement of Novelty; core \"theory\" is a standard energy composition\nThe paper presents its core \"physics informed sampling\" framework (Section 3.2) as a new development, complete with theoretical backing. \n- The claim is \"develop a model-editing-free physics knowledge injection framework\" and present \"Theorem 3.1\" as a closed-form solution for their optimization problem (Eq 6).\n- The reality: this entire \"model-editing-free physics knowledge injection framework\" is a well estimated, standard technique\n    -  Theorem 3.1 is a foundational principle of statistical mechanics and Energy-Based Models (EBMs). It simply states that the optimal distribution for a composed energy function is a Boltzmann distribution (a product of experts, PoE, established in 1999). The proof in Appendix A.3 is a standard Lagrange multiplier derivation yielding Bayes theorem. \n    - Algorithm 1 is standard Langevin MC applied to the composed energy function ($\\nabla\\log q^\\ast = \\nabla K + \\alpha \\nabla\\log p_\\theta$) . Theorem 3.2 restates standard LMC results under strong assumptions, which is quite general and provides no insight for this specific problem.\n\nBy presenting these foundational, decades-old principles of EBMs and its sampling algorithm as theorems derived for this paper with new names like \"model-free physics-guided correction\", \"calibrated Langevin dynamics\" and by failing to to cite or acknowledge the extensive prior art on Product of Experts, EBM composition, or the vast field of generative models for inverse problems that uses this exact mechanism, the authors overstate the novelty of their core contribution relative to well-established energy-guided/compositional sampling. It would be great that the authors can clarify the distinction and situate this work relative to relevant prior art. \n\n\n### Lack of Rigorous Statistical Validation\nThe paper fails to report any measure of experimental variance (e.g., standard deviations or confidence intervals over multiple runs) for nearly all of its main results. This is a critical omission, especially for a stochastic model. This is problematic when assessing the paper's central claims, where the reported differences are often small:\n- weakens the core contribution: In the crucial ablation study (Table 7), the performance difference between the full PINFDIT model (CRPSsum 0.424 on Solar) and the model \"w/o Phys\" (0.445) is marginal.  The paper compares the full PINFDIT (with Langevin correction) against PINFDIT DDPM (without it) to prove the value of the physics-injection. The differences are minuscule:\n    - Navier-Stokes (RMSE): 0.0030 (PINFDIT) vs. 0.0031 (DDPM)\n    - Advection (RMSE): 0.0035 (PINFDIT) vs. 0.0036 (DDPM) \n    - Diffusion-Sorption (RMSE): 0.0049 (PINFDIT) vs. 0.0050 (DDPM)\n\nWithout error bars, it's impossible to conclude that the Langevin correction (a key contribution) provides any significant benefit; This directly undermines the paper's entire narrative about the practical value of its main \"physics-informed\" contribution. \n- Obscures SOTA Comparisons:The paper claims SOTA performance, but many \"wins\" are tiny and could be statistical noise.\n\n    - Practical Forecasting (Table 3): On PhysioNet(a), PINFDIT achieves an MAE of 0.616 vs. CSDI's 0.620. On PhysioNet(c), it's 0.543 (CRPS) vs. CSDI's 0.548. These sub-1% differences are meaningless without a measure of variance.\n\n    - Anomaly Detection (Table 14): On the PSM dataset, PINFDIT scores an F1 of 97.57, while TimesNet gets 97.34 and TimeLLM gets 97.23 . A ~0.2% difference is well within any reasonable margin of error. It is misleading to claim superiority based on this.\n\nThe fact that the authors do report variance in Table 12 for the synthetic generation task (e.g., \"0.031(0.007)\") confirms they are aware of this analysis and capable of performing it. This makes its conspicuous absence from all other key results tables (Tables 1, 3, 6, 7, 9, 10, 11, 15, etc.) a more significant methodological flaw that prevents a proper assessment of the paper's claims.\n\n### Physics operator $K$ is under-specified for ERA5: mismatch to target variable.\nThis paper does state the output is 2-meter temperature prediction. And in appendix A.7, the paper lists the resolution and date splits. While the target as established, is to predicted 2 meter temperature, while in Appendix A.7, under \"Physical Constraints\", the only equation they provide is the Navier-Stokes momentum equation. This equation governs the evolution of wind velocity, not temperature. The physics of temperature near the surface is governed by a different, unmentioned equation (and in fact, t2m is a diagnostic quantity, obtained by Monin-Obukhov interpolation between skin temperature and the lowest model level, and is governed by surface energy-balance and boundary-layer physics). The paper explicitly states, \"The model's output time series is post-processed by penalizing the residuals... of the above equation\". This is physically meaningless. A temperature field does not and cannot satisfy the momentum equation. The authors have either applied a completely irrelevant physical constrained, or critically failed to describe their experiment. In either case, this is a major flaw. This gap makes their \"real-world\" physics-informed experiment unsound and irreproducible as described. \n\n### Limited applicability of the \"general-purpose\" physics module.\nThe proposed \"physics-injection\" model is far less \"general-purpose\" than the base model it is applied to, limiting its practical utility. The methods is entirely reliant on the existence of a known, explicit and differentiable residual function. The authors concede this point in their own limitation section.\n\n### On experiments\nThe paper's claims are undermined by some flaws in its experimental design.\n- Post-hoc refinement vs. no analogue in baselines. PINFDiT uses a second-stage Langevin refinement after the diffusion sampler. Table 1 then compares against NeuralODE/NeuralCDE and other models without an analogous inference-time correction. So it is comparing (PINFDiT + k steps physics correction) against (Neural ODE + no correction). It only proves that adding a post-hoc, physics-based gradient correction step improves the final results, which is trivial and obvious conclusion. A methodologically sound experiment  would have been to apply the same k-step gradient based physics correction to the outputs of all baseline models, and then compare the final errors. \n\n- Inappropriate baselines. In Table 1, the paper compares its forecasting model against SNPE and LFBC. Theses are Simulation-Based Inference (SBI) methods designed for parameter posterior estimation, not time-series forecasting. Their inclusion here is confusing and does not directly test the claimed inference-time energy guidance against the most comparable alternatives. \n- Mixing deterministic and probabilistic baselines on CRPS. In Table 1 and 3, the paper evaluates deterministic, point-estimate models (e.g., DLinear, PatchTST) using probabilistic metrics like CRPS. This is a nonsensical comparison. These models do not natively produce predictive distributions. Also. the paper does not explain how this was calculated."}, "questions": {"value": "### On inference cost reporting\nThe proposed \"physics-injection\" methods is an iterative Langevin MC loop that runs for $k$ stops. This is an additive computational cost applied at inference time for every sample generated, on top of the $T$-step diffusion sampling processes.\n- Critical omission. The paper never states the value of $k$ used in any of its experiments. This is an important hyper parameter that directly determines the inference cost. \n- In Table 19, the authors claim an inference time of 1 second per sample. Is this the time for the full PINFDIT, or for the base model?\n\nFor other questions/suggestion please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pBnz3iDCL5", "forum": "EphTlUJ4XN", "replyto": "EphTlUJ4XN", "signatures": ["ICLR.cc/2026/Conference/Submission23805/Reviewer_m5ac"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23805/Reviewer_m5ac"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867211707, "cdate": 1761867211707, "tmdate": 1762942813524, "mdate": 1762942813524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PINFDIT, a diffusion model for (conditional) time series generation, adhering to physical constraints via a physical injection post-sampling via Langevin dynamics. In their empirical evaluation, the model outperforms previous approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a physical injection during/post inference. The solution via Langevin dynamics is elegant and comes with a theoretical justification.\n- Experiments are conducted across different physical systems and demonstrate strong performance. \n- In general, the empirical evaluation is thorough and convincing. It also compares to recent foundation models."}, "weaknesses": {"value": "- A background section is missing. Certain things as the ELBO in Eq. 2 and the diffusion sampling in Eq. 1 should be moved to a background section and be explained accordingly.\n- Contributions and advancements over previous works are not entirely clear to me. E.g., L94-98, how is the model different from other diffusion-based models using a transformer backbone, e.g., CSDI [1]?\n- Self-supervised learning is mentioned in the methodology, but it is unclear how it differentiates from CSDI [1].\n- The notation is inconsistent. Sometimes the subscript for $x$ denotes the diffusion time (e.g., Eq. 1), sometimes it denotes the iteration (e.g., Eq. 8). The temporal time is also denoted with $t$ (e.g., Eq. 4 and 5).\n\nMinors:\n\n- L283: Sentence is broken\n- L284: likelihood -> the likelihood\n- Different styles to denote target *tar*, context *con*, and $\\mathbf{x}$ across the paper.\n\n[1] **Tashiro, Y., Song, J., Song, Y., & Ermon, S.** (2021). Csdi: Conditional score-based diffusion models for probabilistic time series imputation. Advances in neural information processing systems, 34, 24804-24816."}, "questions": {"value": "See weaknesses and:\n\n- What are the differences between the diffusion model (without the physical injection) and previous ones?\n- Can the physical injection also be done during generation via guidance rather than as a post-processing step?\n- Can the Langevin dynamics, i.e., the physical injection, also be applied to autoregressive models? For example, Chronos?\n\nI like the idea of the physical injection, but the current state of the paper has certain uncertainties, especially differences to previous approaches. If these are addressed, I am willing to increase my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "me4QaX9Gn3", "forum": "EphTlUJ4XN", "replyto": "EphTlUJ4XN", "signatures": ["ICLR.cc/2026/Conference/Submission23805/Reviewer_MmYJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23805/Reviewer_MmYJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946281125, "cdate": 1761946281125, "tmdate": 1762942813327, "mdate": 1762942813327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes using diffusion for time series modeling. The model is a specially designed transformer architecture that can deal with multivariate time series with missing values. Additionally, authors apply physical law adherence through the direct optimization-like procedure in the sampling. The experiments are extensive and show good results on many different setups."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The physics injections in the sampling of diffusion is a very natural extension and can guarantee the model behaves as intended. This is very powerful tool as virtually any constraint can be observed. In this work, authors apply PDE constraint and additionally provide theoretical results for this specific scenario. The architecture is sound and appears to outperform competitors. The paper is clearly written and easy to follow. There are many different experimental setups. In all of them authors demonstrate good results. The appendix is extensive and contains additional theoretical proofs and experiments."}, "weaknesses": {"value": "The main contribution of the paper seems to be the physics informed sampling. This part is under-explored and the authors only test a couple of datasets with non-physics models. More focus on this part would be appreciated given the title and general theme.\n\nThe rest of the experiments seem to not have any relation to the physics informed part. In my understanding there is no physics injection done here. The contribution is the architecture, which is interesting but not overly novel. Can the authors comment on what part of the architecture they believe is the most important in outperforming existing diffusion models like CSDI.\n\nMajor formatting issue: font size is reduced after eq 6."}, "questions": {"value": "- How do results in 4.1 compare to more conventional solvers? Is there any benefit to your method?\n- When should one use your method? To improve the accuracy or to learn the unknown dynamics?\n- Can you comment on the runtime, both for the training and inference? Especially the conventional diffusion sampling compared to the physics guided part.\n- Can Algorithm 1 be augmented to have guidance on the function K in the T diffusion steps instead of the second loop?\n- Do you have an explanation why the model performs better on irregular time series data compared to the some of the specialized models, which build around irregularity?\n- Can you speculate if the reason is the diffusion or the architecture? This can be an ablation study."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CoBvOJPyid", "forum": "EphTlUJ4XN", "replyto": "EphTlUJ4XN", "signatures": ["ICLR.cc/2026/Conference/Submission23805/Reviewer_7Sa8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23805/Reviewer_7Sa8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071227642, "cdate": 1762071227642, "tmdate": 1762942813073, "mdate": 1762942813073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}