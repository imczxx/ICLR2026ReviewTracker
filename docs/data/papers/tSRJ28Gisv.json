{"id": "tSRJ28Gisv", "number": 13286, "cdate": 1758216024827, "mdate": 1759897448648, "content": {"title": "MTRE: Multi-Token Reliability Estimation for Hallucination Detection in VLMs", "abstract": "Vision–language models (VLMs) now rival human performance on many multimodal tasks, yet they still hallucinate objects or generate unsafe text. Current hallucination detectors, e.g., single-token linear probing (LP) and $P(\\text{True})$, typically analyze only the logit of the first generated token—or just its highest-scoring component—overlooking richer signals embedded within earlier token distributions. We demonstrate that analyzing the complete sequence of early logits potentially provides substantially more diagnostic information.\nWe emphasize that hallucinations may only emerge after several tokens, as subtle inconsistencies accumulate over time. By analyzing the Kullback–Leibler (KL) divergence between logits corresponding to hallucinated and non-hallucinated tokens, we underscore the importance of incorporating later-token logits to more accurately capture the reliability dynamics of VLMs.\nIn response, we introduce Multi-Token Reliability Estimation (MTRE), a lightweight, white-box method that aggregates logits from the first ten tokens using multi-token log-likelihood ratios and self-attention. Despite the challenges posed by large vocabulary sizes and long logit sequences, MTRE remains efficient and tractable. Across MAD-Bench, MM-SafetyBench, MathVista, and four compositional-geometry benchmarks, MTRE achieves a 9.4% gain in Accuracy and a 14.8% gain in AUROC over standard detection methods, establishing a new state of the art in hallucination detection for open-source VLMs.", "tldr": "MTRE method improves reliability detection by aggregating multi-token logits, outperforming prior approaches efficiently despite large vocabularies and logit sequences.", "keywords": ["alignment", "fairness", "safety", "privacy"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b42c51cf631ebca9eac728d662a5368c49067ad4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces MTRE, a lightweight white-box method for detecting hallucinations in Vision-Language Models (VLMs). Unlike existing approaches that rely only on the first token's logits, MTRE aggregates information from the first ten tokens using multi-token log-likelihood ratios and self-attention. The method is shown to outperform baseline detectors across multiple benchmarks, including MAD-Bench, MM-SafetyBench, MathVista, and arithmetic tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a novel hallucination detection criterion, which captures reliability signals through the logits of multiple output tokens.\n2. Extensive observational experiments are provided, particularly the analysis and discussion of different types of detection tasks.\n3. The proposed method appears to be effective and exhibits promising performance."}, "weaknesses": {"value": "1. As described in the limitations section, the number of open-source VLMs validated by this method is limited. I suggest conducting illusion detection tests on larger-sized VLMs, such as those larger than 32B, to see if the experimental observations also hold true for larger models.\n2. Why start with 10 tokens instead of 20? What are the reasons and criteria for this choice?\n3. In practice, the proposed method relies to some extent on supervised information. However, in real-world scenarios, data from unseen domains is often more common. Does the proposed method possess any cross-domain capabilities?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "73quRhsz72", "forum": "tSRJ28Gisv", "replyto": "tSRJ28Gisv", "signatures": ["ICLR.cc/2026/Conference/Submission13286/Reviewer_woL3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13286/Reviewer_woL3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749985620, "cdate": 1761749985620, "tmdate": 1762923958593, "mdate": 1762923958593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets VLM hallucination detection and argues that single-token probes (LP, P(True)) miss reliability signals distributed across early token sequences. Empirically, KL divergences between hallucinated vs. truthful continuations grow over several tokens, motivating multi-token analysis. The authors propose MTRE, a lightweight white-box detector that aggregates the first 10 tokens’ logits via multi-token log-likelihood ratios and a small self-attention module; it remains tractable despite large vocabularies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic is interesting and tries to address an important problem.\n\n2. The paper is well written"}, "weaknesses": {"value": "1. The baseline model should add some new models, like LLaVA 1.5, LLaVA NeXT, Qwen 2.5 VL.\n\n2. Figure 1 seems unclear, I recommend the authors add more explanations.\n\n3. For the benchmark discussion, note that several recent studies [1, 2, 3] address both hallucination and maintain performance (even some improvement) on general scenario. I recommend the authors add some benchmarks like OCRBench, MMMU, MME etc.\n\n[1] Mitigating Object Hallucinations via Sentence-Level Early Intervention.\n\n[2] A topic-level self-correctional approach to mitigate hallucinations in mllms.\n\n[3] Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UcNSVHcvek", "forum": "tSRJ28Gisv", "replyto": "tSRJ28Gisv", "signatures": ["ICLR.cc/2026/Conference/Submission13286/Reviewer_N6aE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13286/Reviewer_N6aE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758654987, "cdate": 1761758654987, "tmdate": 1762923958110, "mdate": 1762923958110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the hallucination detection issue in Vision-Language Models (VLMs) by proposing a novel method called Multi-Token Reliability Estimation (MTRE) and its variant, MTRE-$\\tau$ . Departing from prior work of prioritizing the first generated token's logit, MTRE leverages the distributional shift across a sequence of early generated logits to extract richer diagnostic signals. The authors design a reliability classifier, $f_{\\theta}$, trained on token-level labels, and aggregate its log-likelihood ratios (LLRs) over a dynamically determined or fixed number of early tokens. Experiments demonstrate MTRE's competitive performance, especially in scenarios where hallucinations emerge late in the sequence (Type II setting)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear Motivation and Rationale: The fundamental premise—that the full sequence of early logits contains more diagnostic information than the single first token—is clearly articulated and well-supported by preliminary analysis (Section 3). \n\nSound Empirical Insights: The paper provides several valuable empirical observations (Section 3 and 5) that are useful for the VLM reliability community. For instance, the finding that hallucination divergence may emerge late in the sequence, and consequently, that single-token probing methods (e.g., Zhao et al., 2025) are suboptimal for Type II settings, is a significant takeaway.\n\nComprehensive Experimental Setup and Clarity: The authors provide a sufficiently detailed description of the experimental setup, particularly in the Appendix regarding model training and hyper-parameter choices. The experimental validation is thorough across various VLM architectures and hallucination settings (Type I and Type II).\n\nSolution to Core Hyper-parameter Identification: The introduction of the MTRE-$\\tau$ variant (Section 4.3), which employs cross-fitting for prior estimation (minimizing token-broadcast binary cross-entropy) and dynamic evidence length determination ($\\tau_{s_{i}}$), offers a practical approach to tackling the sensitivity of the aggregation step to hyper-parameters."}, "weaknesses": {"value": "Major Concerns\n\n1. Limited Efficacy in Type I Setting: While MTRE significantly outperforms baselines in the Type II setting, its performance edge over the competitive Linear Probing (Lin. Prb.) baseline on Type I tasks (more common ones) is often marginal. \n\n2.  Incremental Gain of MTRE-$\\tau$: MTRE-$\\tau$ fails to demonstrate a clear and significant performance improvement over MTRE. Given that MTRE-$\\tau$ introduces substantial additional complexity (cross-folding, parameter calibration, optimization for $C_{u}$ and $C_{b}$), the marginal utility of this variant is questionable. \n\n3. Extra Computational Cost Analysis: The MTRE method inherently requires training an auxiliary reliability classifier $f_{\\theta}$, which increases complexity relative to post-hoc methods. While some performance gain justifies this, the paper lacks a detailed comparative analysis of the computational overhead.  A clearer comparison of the inference time complexity of MTRE against Lin. Prb. needs to be provided.\n\nMinor Concerns\n\n1. The labels OE and OEH in Figure 2 are not immediately clear from the main body text or caption. It would be better to explicitly explain  these abbreviations in the figure caption or the accompanying paragraph.\n\n2. The definition of the $\\sigma(\\cdot)$ function used in the prior estimation loss (5) should be stated in Section 4.3 (though it is provided in the Appendix)"}, "questions": {"value": "Regarding the inference time reported in Table 3 (0.944 ms), please clarify whether this average inference time is measured per single generated token or per complete Q&A query (statement). Even if it is per complete query, the time consumption is highly significant, potentially negating the benefits for real-time deployment or subsequent hallucination mitigation steps (e.g., new inference, re-prompting, or post-hoc corrections)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SDDMEcHsHs", "forum": "tSRJ28Gisv", "replyto": "tSRJ28Gisv", "signatures": ["ICLR.cc/2026/Conference/Submission13286/Reviewer_nnVM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13286/Reviewer_nnVM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967753123, "cdate": 1761967753123, "tmdate": 1762923957701, "mdate": 1762923957701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MTRE (Multi-Token Reliability Estimation), a novel white-box hallucination detection framework for vision-language models (VLMs). Unlike existing single-token probing or P(True) methods that focus on the first generated token, MTRE aggregates information from the logits of multiple early tokens (typically the first 10) to estimate model reliability. By leveraging sequential log-likelihood ratios, attention-based classifiers, and calibration via cross-fitting, MTRE captures reliability dynamics that unfold across token generation. Experimental results on MAD-Bench, MM-SafetyBench, MathVista, and several arithmetic reasoning datasets show consistent gains over strong baselines like linear probing and TokenSAR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies a key limitation of previous approaches that rely solely on the first-token logit, showing through KL divergence analysis that hallucination-related divergence often arises in later tokens. This motivates the multi-token design in a theoretically grounded way.\n\n2. MTRE introduces a lightweight yet principled multi-token aggregation method, formulated as a calibrated sequential log-likelihood ratio test. The design effectively balances interpretability and computational efficiency.\n\n3. The authors conduct evaluations on four open-source VLMs (LLaVA-v1.5, mPLUG-Owl, LLaMA-Adapter V2, MiniGPT-4) and multiple benchmarks. The results demonstrate robust and consistent improvements, including on challenging self-evaluation (Type 2) settings.\n\n4. MTRE adds negligible overhead (<1% VRAM and inference time) and does not require retraining large models, making it feasible for deployment in real-world systems."}, "weaknesses": {"value": "1. The current comparison is restricted to models such as LLaVA-v1.5 (7B), mPLUG-Owl, LLaMA-Adapter V2, and MiniGPT-4, which may now be considered relatively early-generation VLMs. It remains unclear how MTRE performs on more recent and stronger models (such as LLaVA-Next, InternVL2 or Qwen2.5-VL), which exhibit lower hallucination rates. Incorporating these models would better demonstrate the generality and contributions of MTRE.\n\n2. All experiments use 7B-scale models. It would be informative to analyze whether MTRE scales effectively to larger architectures (e.g., 13B) or smaller lightweight models (3B in scale).\n\n3. The current datasets primarily involve English and relatively simple visual reasoning. Evaluating on multilingual or real-world datasets (e.g., OCR-heavy or video-based tasks) could further validate robustness."}, "questions": {"value": "1. Include Stronger Baselines: Evaluate MTRE on newer VLMs such as LLaVA-Next, Qwen2.5-VL, or InternVL3 to verify whether multi-token reliability estimation remains beneficial when hallucinations are rarer but more subtle.\n\n2. Ablation Across Model Sizes and Modalities: Analyze how performance scales with model size and whether similar gains appear for larger or multilingual models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q6ffGrNP7s", "forum": "tSRJ28Gisv", "replyto": "tSRJ28Gisv", "signatures": ["ICLR.cc/2026/Conference/Submission13286/Reviewer_fd2b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13286/Reviewer_fd2b"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990902239, "cdate": 1761990902239, "tmdate": 1762923957315, "mdate": 1762923957315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}