{"id": "rJwaRB4M6v", "number": 12242, "cdate": 1758206544964, "mdate": 1759897523093, "content": {"title": "Gradient-as-retrieval: Classification beyond Cross Entropy", "abstract": "Cross entropy (CE) is the loss of choice for classification tasks. However, computing the CE loss and gradient requires transcendental functions which may be expensive in emerging computational paradigms such as fully homomorphic encryption for privacy-preserving applications. The transcendental function-free familywise (FW) loss has been shown to enjoy strictly better statistical guarantees than the CE loss. In this work, we prove theoretical results that enable efficient computation of the gradient of the FW loss using ``retrieval-style'' algorithms. Based on our theory, we provide practical implementations. A challenge in designing new loss functions is that widely adopted optimizers and learning rate schedules are tuned to CE. Experimentally, we demonstrate that the FW loss outperforms cross entropy when we opt for parameter-free learning methods.", "tldr": "", "keywords": ["Loss function", "classification", "supervised learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ed5a90a52b916db4526e2771d95efddb3f46ece.pdf", "supplementary_material": "/attachment/69d87636ec7ff872166f86b3c3273177f9a0591f.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Familywise (FW) loss as a practical alternative to cross-entropy (CE) for multiclass classification, framing its computation as “gradient-as-retrieval.” The key observation is that a subgradient depends only on the true class and the top-$\\bar{\\kappa}$ impostor logits, yielding the simple form  \n\n$$ \\nabla_z L(z,y)= -\\mathrm{one\\text{-}hot}_y + \\tfrac{1}{\\bar{\\kappa}}\\ \\mathrm{many\\text{-}hot}_{{(1),\\ldots,(\\bar{\\kappa})}}, $$\n\nso the dominant operation is top-(k) selection rather than softmax/exponentials. The paper proves a bitonicity property for a sequence used to pick $\\bar{\\kappa}$ (enabling an $\\mathcal{O}(K)$ subgradient algorithm) and presents a Gaussian-logits approximation to estimate $\\bar{\\kappa}$ and explain emergent gradient sparsity during training. Implementations (multicore CPU, pure PyTorch GPU) and experiments on UCI, MNIST, and CIFAR-100 with parameter-free optimizers (Prodigy, GDTUO, DoG) indicate that FW is competitive and often favorable in those regimes (e.g., MNIST median 97.93\\% vs 97.66\\% for CE). The paper also clarifies that FW avoids transcendental functions and does not directly provide calibrated probabilities like CE/softmax."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Practical relevance.** Computation hinges on selection/top-(k), avoiding transcendental function, attractive for efficiency- or encryption-constrained settings.\n    \n- **Sparsity perspective.** The Gaussian-logits analysis gives an interpretable link between logit variance $\\bar{\\kappa}$, and gradient sparsity, with qualitative support across class counts.\n    \n- **Working implementations.** CPU and PyTorch GPU paths exploit the theory; the CPU shows speedups vs CE at large ($K$) (with GPU improving over a naïve FW baseline).\n    \n- **Empirical validation in parameter-free regimes.** UCI/MNIST/CIFAR-100 results with Prodigy/GDTUO/DoG support the claim that FW is particularly competitive when schedules are learned automatically; MNIST numbers are concretely reported."}, "weaknesses": {"value": "-  **Baseline strength and fairness.** The empirical focus is on parameter-free optimizers; this is a legitimate and interesting regime, but it leaves uncertainty about tuned CE (e.g., cosine decay, label smoothing, warmup). Without strong, standard CE baselines on MNIST/CIFAR-100, gains may conflate optimizer–loss interactions with the inherent merits of FW.\n    \n- **Statistical rigor and reporting.** Some results note non-significance for trends, but a consistent reporting of mean $\\pm$ std across seeds, paired significance tests, and effect sizes is not visible throughout. CIFAR-100 is described largely qualitatively.\n    \n- **Approximation vs exact computation.** The Gaussian estimator for $\\bar{\\kappa}$ is elegant, but the empirical accuracy/runtime trade-off compared to exact selection is not quantified across ($K$) (notably at moderate ($K=100$) vs very large ($K$)).\n    \n- **Calibration and applicability limits.** The paper highlights the absence of calibrated probabilities under FW; however, there is no experiment illustrating calibration error, uncertainty estimation, or downstream impact where probabilities matter (e.g., selective classification).\n        \n- **Breadth of evaluation.** There is no study of *class imbalance/long-tail* or larger-scale datasets; these are common stress tests for alternative losses and would increase external validity."}, "questions": {"value": "1. **Tuned CE baselines:** Can results on MNIST and CIFAR-100 include standard, tuned CE (e.g., cosine LR with warmup, label smoothing) to disentangle optimizer–loss coupling from the intrinsic effect of FW?\n    \n2. **Approximation ablation:** What is the accuracy vs runtime delta between exact $\\bar{\\kappa}$ selection and the Gaussian approximation for ($K=100$) and ($K\\ge 10^4$)? Plotting both within the same wall-clock budget would clarify practical trade-offs.\n    \n3. **Sparsity dynamics:** Can the paper report $\\bar{\\kappa}/K$ over training (with accuracy) to substantiate the claimed increasing gradient sparsity and its correlation with performance?\n    \n4. **Calibration assessment:** How does FW compare to CE in calibration error (ECE/mECE), and is there a practical post-hoc mapping to obtain usable probabilities when needed?\n        \n6. **Robustness to imbalance/long-tail:** How does FW behave on long-tailed datasets or with focal-style reweighting compared to CE variants?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "o3yG0LLEYH", "forum": "rJwaRB4M6v", "replyto": "rJwaRB4M6v", "signatures": ["ICLR.cc/2026/Conference/Submission12242/Reviewer_vUcr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12242/Reviewer_vUcr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761252349336, "cdate": 1761252349336, "tmdate": 1762923184450, "mdate": 1762923184450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission studies the properties of familywise (FW) loss as an alternative to cross-entropy loss learning classification tasks. FW loss is free of transcendental functions (log, exp, etc.) and mainly relies on the computation of the top-k largest (pre-softmax) logits for some k that maximizes a simple function. The authors prove that this k (and therefore the FW loss and its gradients) can be computed in O(K) time (where K is the total number of logits) using a unimodality property and “retrieval-style” algorithms, hence giving a better theoretical bound than the naive O(K logK) sorting. They also show k can be approximated with easy-to-compute quantities when the logits follow a Gaussian distribution. Lastly, they provide an efficent-in-practice implementation of computing FW loss (and its gradient) using the heap-of-heaps data structure. They experimentally demonstrate the FW can outperform cross-entropy (in terms of time/accuracy) under certain settings, in particular when using CPUs / the number of features is large, but is still defeated by cross-entropy when using GPUs / the number of features is small."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper combines sound theoretical claims with extensive experiments. While proofs of theoretical claims are deferred to the appendix, proof sketches are provided, which is very useful for building intuition. \n\n2) The authors do not shy away from discussing the limitations of their approach, such as underperforming with GPUs (and other methods that are largely optimized by cross-entropy loss) or the fact that FW does not estimate class conditional distributions. The discussion of such limitations is appreciated and makes it easier to comprehend the contributions of the paper.\n\n3) The paper is overall well-written and easy to read. The figures display clear trends and shine light on the performance of FW vs cross-entropy loss."}, "weaknesses": {"value": "1) As the authors admit, FW losses currently lack an efficient GPU implementation (the implementation introduced in this submission is optimized for CPUs) which inevitable limits the applicability of the paper considering how widely GPUs are adopted (e.g. see Fig. 4). While this indeed results from an unfair advantage to cross-entropy loss due wide adaptation (and therefore optimizers/architectures being tuned for it), an alternative loss (or another implementation of FW) that outperforms cross-entropy across different optimizers/architectures would be a much more attractive contender. While there is some discussion on the challenges of GPU implementation, it is not clear how much the results in this paper can be built on for that setting. \n\n2) While the mathematical properties of FW loss is rigorously discussed, those of cross-entropy are largely left out of the paper. In fact, cross entropy itself or the classification-calibration property satisfied by both losses is never formally defined in the paper. This makes it hard to appreciate how much stronger the universal equivalence to 01 loss satisfied only by FW loss actually is. In general, since so much of the paper's motivation centers around finding an alternative to cross-entropy loss, a more extensive discussion of the mathematical properties of it (along with which of them are satisfied by FW loss also) would be beneficial.\n\n3) Some statements/claims in the paper are anecdotal/unclear. For instance, Theorem 5 says \"as $K\\rightarrow \\infty$, $\\overline{\\kappa}$ can be approximated as XXX\", but it is not clear what \"can be approximated means\" without any approximation bounds. Similarly, the claim \"Since H_k is (...) typically maximized at $\\overline{\\kappa} << K$, the loop often terminates early\" is not justified by any results/citations.\n\nMinor:\n- l30 \"statistics, appeared\" -> \"statistics and appears\"\n- l35 \"FW loss\" -> familywise loss has not been mentioned in the introduction yet, so do not use an acronym. \n- l75 \"Theorem 5\" should be in parentheses.\n- l91 \"While less widely known\" -> than what?\n- l91 \"in (Bartlett\" -> use citet rather than citetp\n- l100 extra \")\" inside both $\\mathbb{E}_{\\mathbf{X},Y}$. Also, the second $\\mathbb{E}_{\\mathbf{X},Y}$ should have $f$ rather than $f_n$.\n- In general, the pages in which figures/tables are mentioned for the first time do not match the page they appear (and in fact do not follow the same order), which decreases readibility.\n- l235 environment should be labeled \"Proof sketch\" rather than \"Proof\"\n- l293 \"Instead of sorting all $K$ logits in $O(K \\log K)$ time...\" -> this sentence seems to ignore/omit the results from Section 3. While you have talked about efficiency in practice there, it would still be worth mentioning here for the coherence of the paper. \n- l375 \"optmized\" -> \"optimized\"\n- l427 \"set of 'hyper-hyperparameter' \" -> \"set of 'hyper-hyperparameters' \"\n- l438 \"This is supports\" -> \"This supports\"\n- l473 \"(...) context. familiywise loss requires only (...)\" -> capitilize f\n- l483 \"Williamson et al (...)\" -> use citep rather than citet."}, "questions": {"value": "Can you discuss other desirable properties of cross-entropy that are still satisfied by FW loss, beyond classification-calibration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EanhPUmF9f", "forum": "rJwaRB4M6v", "replyto": "rJwaRB4M6v", "signatures": ["ICLR.cc/2026/Conference/Submission12242/Reviewer_oiVK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12242/Reviewer_oiVK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947255095, "cdate": 1761947255095, "tmdate": 1762956447779, "mdate": 1762956447779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work is motivated by the expense of evaluating cross-entropy in fully homomorphic encryption due to transcendental functions, such as exponentials and logarithms. The familywise (FW) loss avoids transcendental operations, and this work develops new theoretical insights that enable efficient computation of FW gradients through retrieval-style algorithms. Because most existing optimizers and learning-rate schedules are tuned for CE, this work shows that FW surpasses CE in performance when used with parameter-free learning algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Theoretical grounding**: Extends known properties of FW loss (Duchi et al., 2018) and connects them with efficient computational methods. The bitonicity property and $O(K)$ computation result are elegant and novel from an algorithmic standpoint.\n2. **Computational innovation**: The heap-of-heaps and top-k retrieval implementations are nontrivial engineering contributions that make the loss practically usable for large-scale problems."}, "weaknesses": {"value": "1. **Characterize the failure mode**: In Section 5.3, it says that using FW loss on CIFAR-100 has numerical stability issues when using gradient descent, possibly due to the larger number of classes. I think this failure mode is worth investigation: Can this hypothesis be validated on synthetic data problems, showcasing the relations between numerical stability or performance with the number of classes? Also, does it indicate its inapplicability to language tasks due to the large vocabulary size? More thorough discussions on this would be helpful.\n2. **More non-linear architectures**: The introduction section claims that this work helps answer whether FW loss works in *deep learning* classifiers. Therefore, it would be helpful to provide experimental results with some small non-linear models in Sections 5.1 and 5.2. The models do not have to be large since the datasets are relatively small-scale, but it would be more complete if multiple widely used architectures are covered, including convolutional and transformer models. It would also be nice to cover architectures of different levels of complexity (in terms of depth). If FW loss works on different architectures of different complexities, the experiments would be convincing.\n3. **Efficiency in privacy-preserving applications**: The paper claims that FW can be beneficial in fully homomorphic encryption for privacy-preserving applications where cross-entropy (CE) is expensive. To support this, it helps to present experimental results in privacy-preserving applications that compare the efficiency of FW to CE."}, "questions": {"value": "There are some minor editing suggestions:\n1. The word \"transcendental\" is used from the beginning without a brief introduction or synonyms as an explanation. It would be nice to add one when it's used for the first time.\n2. It is mentioned that widely adopted optimizers and learning rate schedules are tuned to CE, which motivates the authors to use parameter-free learning methods. However, it still helps to report the FW results under widely adopted optimizers with their hyperparameters tuned for FW. If it underperforms, readers would be interested in at least seeing some discussions about this and having it left for future work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gU5e1iKHYT", "forum": "rJwaRB4M6v", "replyto": "rJwaRB4M6v", "signatures": ["ICLR.cc/2026/Conference/Submission12242/Reviewer_GTVe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12242/Reviewer_GTVe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956909107, "cdate": 1761956909107, "tmdate": 1762923183641, "mdate": 1762923183641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript explores the use of the familywise (FW) loss function as an alternative to cross-entropy. Previous studies have discussed FW loss, but they have not examined its performance in deep learning classifiers nor proposed efficient computational methods for it. In this work, the authors present an approach to efficiently compute the gradients of the FW loss function and demonstrate that the performance of deep learning models using FW loss is comparable to that achieved with cross-entropy loss."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The part of the study that examines how performance changes when replacing the conventional cross-entropy loss is indeed an aspect that could attract the interest of AI researchers. Since cross-entropy loss has become almost a de facto standard loss function for deep learning classifiers, any attempt to replace it would be considered a challenging and ambitious topic in the research community. Therefore, if the paper can present reasonable and convincing results, it could become a high-impact piece of work."}, "weaknesses": {"value": "However, I feel that this paper fails to convincingly argue why the FW loss function should replace cross-entropy.\n\nThe main justification given for using the FW loss instead of cross-entropy is that it facilitates the implementation of privacy-preserving AI under fully homomorphic encryption (FHE). The argument is that transcendental functions (e.g., logarithms) are difficult to compute exactly under FHE, so FW loss — which does not involve such functions — would be more suitable than cross-entropy.\n\nAs someone who has conducted continuous research in FHE-based privacy-preserving AI, I find this reasoning difficult to accept. This is because the proposed approach relies on max-heap operations for efficiency. In fact, comparison operations are among the most challenging computations in FHE — even more so than many transcendental functions. The max-heap data structure inherently requires frequent comparisons whenever data is inserted or removed, resulting in significant computational overhead when implemented homomorphically.\n\nIn contrast, transcendental functions can often be approximated quite effectively with polynomial approximations — as long as the interval is well chosen and the polynomial degree is sufficiently high — so the computational overhead is not as severe as one might expect. While replacing transcendental functions with polynomial-only loss functions is certainly desirable from the perspective of FHE-based AI research, I believe that using the current FW loss function in an FHE setting remains highly impractical.\n\nTo convincingly demonstrate the practicality of FW loss for FHE-based AI, the authors would need to actually implement the algorithm under FHE and show that the runtime is significantly reduced compared to cross-entropy. Without such an experiment, the claimed efficiency under FHE cannot be substantiated. From the current paper, it appears that there is no clear reason to use FW loss instead of cross-entropy outside the privacy-preserving AI context.\n\nMoreover, the dataset used to show that FW loss can replace cross-entropy seems **too small**. To make a compelling case for substitution, experiments on larger and more complex datasets would be necessary."}, "questions": {"value": "1. If the FW function is to be utilized under FHE, can you describe how the algorithm would need to be modified using FHE operations?\n2. Simply using transcendental functions is not inherently problematic in FHE as long as they can be reasonably approximated by polynomials of appropriate degree. Have you examined how high the polynomial degree must be when using cross-entropy? If so, it would be helpful to describe that in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h2xDPJVXXg", "forum": "rJwaRB4M6v", "replyto": "rJwaRB4M6v", "signatures": ["ICLR.cc/2026/Conference/Submission12242/Reviewer_VjWb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12242/Reviewer_VjWb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045738984, "cdate": 1762045738984, "tmdate": 1762923183236, "mdate": 1762923183236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}