{"id": "J2WeAMiSZh", "number": 9867, "cdate": 1758145026096, "mdate": 1759897690733, "content": {"title": "Utilizing LLM Robustness for LVLM Safety via Reducing the Pretraining Modality Gap", "abstract": "Large Vision-Language Models (LVLMs) suffer from significant safety degradation compared to their LLM backbones. Even blank or irrelevant images can trigger LVLMs to produce harmful responses to prompts that would otherwise be safely refused in text-only contexts.\nTo address this, existing methods focus on addressing safety vulnerabilities by modifying different components of LVLMs. This includes robustifying the vision encoder, inference-time purification and steering, and safety fine-tuning. However, these methods either substantially increase the training or inference time or significantly harm the model performance.\nIn this work, we address this problem from a different angle. We show that the amount of modality gap between text and image embeddings is strongly inversely correlated with LVLM safety. Crucially, this gap is introduced during pretraining and persists through fine-tuning. Inspired by this observation, we propose a novel regularization technique, \\alg, that explicitly reduces the modality gap during pretraining. Empirically, we show that \\alg\\ effectively enhances safety of various LVLMs, reducing unsafe rates by up to 16.3% while preserving their performance. Notably \\alg\\ is very light-weight and does not require any safety data. It can also easily stack with existing defense mechanisms to further improve safety by up to 18.2%.", "tldr": "LVLMs show safety issues due to a modality gap between image and text embeddings that arises in pretraining. We proposed a novel regularization method that reduces the gap and improves safety by up to 16.3% without hurting performance.", "keywords": ["Safety Alignment", "Jailbreak Attack", "Multimodal Safety", "AI security"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04bdfd7b8ebab1bf27d2ab47b3fc32db3ebd454a.pdf", "supplementary_material": "/attachment/f7c23cd798ac4bc877fbaa97e321d2db9a4cf891.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates the link between the modality gap in VLMs and safety degradation. It shows that a larger modality gap correlates with lower safety and that this gap mainly originates during pretraining. To address this, the authors propose REGAP, a pretraining regularizer that minimizes the distance between image and text token embeddings to reduce the modality gap. REGAP improves safety across multiple benchmarks while maintaining similar utility to the baseline. The method generalizes across architectures and can be combined with existing defenses for additional gains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-presented and logically structured, offering an enjoyable and smooth reading experience.\n\n- The idea of closing the modality gap without requiring any safety data, unlike most post-training safety methods, is interesting and represents a potential step toward inherent mitigation of cross-modality safety misalignment by pretraining VLMs this way from scratch. \n\n- The experimentation across multiple models and a wide range of safety benchmarks, combined with ablation studies examining layer placement, scaling factors, and interactions with other defenses, is comprehensive and commendable."}, "weaknesses": {"value": "Despite the strengths and comprehensive experiments, I have several serious concerns regarding the proposed method and its claims.\n\n- The authors repeatedly claim that existing safety post-training techniques are computationally expensive, yet their own approach requires VLM pretraining (i.e., retraining the projector with the regularization loss), which is equally if not more costly. The efficiency claims, therefore, seem questionable, as several post-training methods, such as safety fine-tuning or unlearning, also freeze the main architecture and train only a small LoRA adapter, making their cost comparable. \n\n- The proposed method requires redoing the pretraining stage, which is somewhat unrealistic since VLMs are typically already pretrained, and then fine-tuned and publicly available. In such cases, post-training interventions are far more practical, as it is generally infeasible to repeat the entire pretraining process for each model to ensure safety. Moreover, the authors themselves show that their method does not work when applied to fine-tuned models and is effective only during pretraining, further limiting its applicability as a post-training technique.\n\n- The authors show that the modality gap between text and image embeddings is negatively correlated with model safety. However, this finding is heavily emphasized throughout the paper and investigated a lot, despite already being established in prior work, particularly the \"CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration\" paper, which demonstrated the same relationship. \n\n- The effect of the proposed training on utility preservation and other model capabilities remains unclear. The paper quantitatively evaluates utility only for LLaVA-7B-LoRA in Table 1. For other models (MiniGPT-4 and ShareGPT4V), the claim of “preserved utility” is stated but not empirically supported with benchmark results or numerical comparisons, as these are missing from Table 2. It would be helpful to clarify the reason for this omission. Moreover, reporting benchmark-wise utility scores rather than only aggregate values would make the impact of the proposed training method on different capabilities more transparent.\n\n- According to Figure 2(f), the modality gap primarily exists up to layer 6 before applying REGAP. Could you provide an explanation for this behavior? How does it affect model safety, and what insights can be drawn from it? Is this the main reason you decided to minimize the modality gap only at the first layer? If so, why not extend it to multiple early layers? In Section 7, the ablation studies mention that regularizing the later layers worsens safety, but this observation is not clearly explained. It would be valuable to explore and justify why this occurs. Additionally, why does this regularization work only during the pretraining stage, yet corrupts the model when applied at the fine-tuning stage? These aspects remain unclear and would benefit from deeper investigation and discussion.\n\n- It would be helpful to report and visualize the MIR on additional datasets, such as FigStep, after training to verify whether the embeddings of images containing harmful text and those of the corresponding textual prompts are truly aligned, and whether the modality gap has been fully closed.\n\n- There is a minor issue with the template, which refers to ICLR 2025 instead of ICLR 2026.\nAdditionally, a few typos appear throughout the paper, such as:\n– “LLaVA1.57B, QWen2.5VL” -> (LLaVA1.5-7B, Qwen2.5-VL)\n– “and the unsafe rate for different LVLM after” -> (LVLMs)\n\n- Given the adversarial nature of this work and in line with ICLR’s guidelines, it would have been preferable to include a dedicated Ethics Statement as well as the Reproducibility section in the paper."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eVaBh2fHJo", "forum": "J2WeAMiSZh", "replyto": "J2WeAMiSZh", "signatures": ["ICLR.cc/2026/Conference/Submission9867/Reviewer_1y89"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9867/Reviewer_1y89"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761357800876, "cdate": 1761357800876, "tmdate": 1762921338122, "mdate": 1762921338122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing studies reveal that Large Vision-Language Models (LVLMs) suffer from significant safety degradation, blank or irrelevant images can trigger LVLMs to produce harmful responses to prompts that would otherwise be safely refused in text-only contexts. In this study, the authors reveal that the amount of modality gap between text and image embedding is strongly inversed correlated with LVLM safety. Based on this observation, they propose a regularization technique, RECAP, that explicitly reduces the modality gap during pretraining. The experiments showcase that RECAP helps improve the safety characteristics of the LVLM by upgrading a much higher model utility."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper proposes a novel idea about upgrading the LVLMs’ safety, which is not intuitive but makes their method distinct from other methods.\n\n2.\tThe generalization capability of the proposed method should be justified. In the experiments, only three LVLMs are tested. I suggest testing more LVLMs to verify the proposed method, like more models in Figure 1."}, "weaknesses": {"value": "1.\tSince the proposed method is not intuitive, it is crucial to verify the observation of the correlation between the modality integration rate and unsafe rate. The authors have drawn Figure 1 to verify their motivation, but there is a lack of discussion about the theory behind the observation. Another concern is that only several LVLMs are displayed, and it is not solid to obtain a clear conclusion.\n\n2.\tThe deep discussion on the existing safety alignment and the proposed methods is missing from the paper. Since it is a different solution to the LVLM's safety issues, the authors should justify their method carefully."}, "questions": {"value": "1.\tIs there any theory or larger-scale data to support the correlation between the modality integration rate and unsafe rate?\n\n2.\tWhat are the advantages and disadvantages of the proposed method and existing studies modifying different components of LVLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ejIDqej2do", "forum": "J2WeAMiSZh", "replyto": "J2WeAMiSZh", "signatures": ["ICLR.cc/2026/Conference/Submission9867/Reviewer_Y4x6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9867/Reviewer_Y4x6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807462341, "cdate": 1761807462341, "tmdate": 1762921337809, "mdate": 1762921337809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the important problem that integrating a vision module into a large language model (LLM) often leads to significant safety degradation. Existing approaches mainly focus on post-hoc safety finetuning or inference-time steering using safety directions. In contrast, this paper offers a fresh perspective by revealing a strong inverse correlation between the safety of vision-language models (VLMs) and the modality gap. The authors propose that this issue originates from what they call the pretraining modality gap. Specifically, since visual features differ substantially from the distribution of text embeddings that the LLM was originally trained on, the model’s inherent safety and robustness mechanisms can be disrupted or bypassed. This observation is closely related to prior works such as Wang et al. (2024c) and Liu et al. (2024a), which explore inference-time interventions and identify the safety direction between visual and textual modalities. The main contribution of this paper lies in tracing the root cause of safety degradation back to the pretraining stage and proposing a new REGAP method to mitigate this issue. Overall, the idea is clear, well-motivated, and convincingly presented."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper clearly defines the problem and provides empirical evidence that models with larger modality gaps tend to exhibit lower safety levels in VLMs.\n\n2.\tIt offers a novel finding that the modality gap originating from pretraining is a key factor driving safety degradation in VLMs, and that this gap persists even after fine-tuning.\n\n3.\tThe proposed REGAP method is simple, efficient, and does not require additional safety data or architectural modifications, making it easy to apply."}, "weaknesses": {"value": "1.\tThe second paragraph of the introduction could be clarified. Some of the discussed methods do not have the stated limitations (e.g., increased inference time), and others have demonstrated good generalization performance. Clarifying these points early on would strengthen the motivation for the proposed method.\n\n2.\tWhen reducing the distance between image and text token embeddings, a key concern is whether this might cause the two modalities to become overly mixed, potentially harming performance. Therefore, utility tests are particularly important in this work. Including more detailed utility analyses would greatly strengthen the paper’s empirical contribution.\n\n3.\tThe paper describes REGAP as a lightweight method, but this characterization is somewhat ambiguous. Since the approach involves additional pretraining, it could be more computationally expensive than inference-time intervention methods. It would be beneficial to refine or clarify what is meant by “lightweight” in this context."}, "questions": {"value": "Please check the weaknesses. More utility results will be appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bv4tdCFbze", "forum": "J2WeAMiSZh", "replyto": "J2WeAMiSZh", "signatures": ["ICLR.cc/2026/Conference/Submission9867/Reviewer_bKys"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9867/Reviewer_bKys"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966158523, "cdate": 1761966158523, "tmdate": 1762921337446, "mdate": 1762921337446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical problem: the significant safety degradation observed in Large Vision-Language Models (LVLMs) compared to their base Large Language Models (LLMs). The authors identify a \"modality gap\" between image and text embeddings and demonstrate a strong inverse correlation between this gap and the safety of LVLMs. They propose REGAP, a lightweight regularization method applied during the pretraining stage. REGAP explicitly reduces this modality gap using a computationally efficient surrogate loss without requiring additional safety data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. the paper makes a novel observation of how the amount of modality gap is related with LVLMs safety.\n2.The problem and the proposed method are clearly illustrated.\n3.The proposed REGAP requires no safety data and is complementary to other defense methods, making it practical."}, "weaknesses": {"value": "1. There is a certain risk of exaggeration in claims about data/model coverage and generality:  Experiments are validated on LLaVA / ShareGPT4V / MiniGPT-4, etc., but ShareGPT4V uses only one-quarter of the data and MiniGPT-4 uses a subset (Sec.6.1); moreover, if the effects under different vision encoders (other than CLIP-ViT-L) or larger LLMs (e.g., 13B/70B) have not been fully verified, the assertion of generality appears overly broad.\n2. Dependence on LLM-judge (Beaver-dam-7B): Automated judges can be biased or have failure modes. It is suggested to (a) report human validation on random subset; (b) evaluate sensitivity to different judge models/thresholds."}, "questions": {"value": "1. It is mentioned in Section 4 (lines 245–246) of sampling 100 image–prompt pairs to compute MIR which might not be sufficient considering the randomness. Could you report the diversity of the samples or the variance across different sample sizes (e.g., 50/100/500/1000) to justify that 100 samples are representative of the overall dataset.\n2. Some experimental details are lacked: The paper states that α is calculated and fixed during the warm-up period (Sec.5), but maybe it lacks key pretraining configurations such as warm-up duration, learning rate, batch size, sampling seed, and the sensitivity of REGAP to data of different scales.\n3. The generality regarding fusion architectures is in question: The experiments primarily focus on LLaVA-style architectures (MLP projector) and MiniGPT-4 (Q-Former) (lines 740-754). How would REGAP perform on models with different vision-language fusion mechanisms, such as those employing cross-attention (e.g., Flamingo, BLIP-2)? The paper generalizes across datasets (ShareGPT4V) and a different projector (MiniGPT-4), but its applicability to fundamentally different fusion architectures is not demonstrated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mkTLbgjU9Z", "forum": "J2WeAMiSZh", "replyto": "J2WeAMiSZh", "signatures": ["ICLR.cc/2026/Conference/Submission9867/Reviewer_tbDC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9867/Reviewer_tbDC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011394150, "cdate": 1762011394150, "tmdate": 1762921337087, "mdate": 1762921337087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}