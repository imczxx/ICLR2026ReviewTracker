{"id": "ZXr3Xx7Z1O", "number": 4049, "cdate": 1757591916049, "mdate": 1759898055832, "content": {"title": "Training Dynamics Impact Post-Training Quantization Robustness", "abstract": "While post-training quantization is widely adopted for efficient deployment of large language models, the mechanisms underlying quantization robustness remain unclear. We conduct a comprehensive analysis of quantization degradation across open-source language model training trajectories up to 32B parameters and 15T training tokens to accurately assess the relationship between training dynamics and quantization performance. Our key finding is that quantization errors in large-scale training runs are driven by a complex interplay between learning rate and other training hyperparameters. Specifically, once learning rates decay, validation loss and quantization error diverge, largely independent of training data scale. To investigate interventions on the training dynamics and identify specific configurations that can modulate quantization robustness favorably, we train our own models in controlled experiments up to 100B tokens. Our results challenge the assumption that increasing dataset scale inherently compromises quantization effectiveness, demonstrating instead that strategic training hyperparameter interventions can improve quantization quality at scale.", "tldr": "Post-training quantization robustness is tightly coupled to learning-rate dynamics rather than data scale. We suggest practical ways to modulate quantization performance outcomes.", "keywords": ["Efficiency", "quantization", "optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/afb8e1a7c843ad63fd6def222e7818577088ce17.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper is an empirical study which explores the relationship between training dynamics and quantization (PTQ) performance, providing interesting observations that quantization errors are related to training hyperparameters like learning rate and scheduler settings, challenging the previous assumption that quantization errors are inherently related to dataset scale. The authors also experiment to intervene the training dynamics to identify specific configurations that modulate quantization robustness favorably, providing practical insights for related studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The insights provided by this paper are very interesting and original. It discusses the relationship between quantization errors and training hyperparameters like learning rate and scheduler settings in detail, and shares some empirical results like the divergence of quantization error and validation loss with the decay of learning rates. I believe those insights could benefit future related studies.\n- The efforts of the authors trying to reduce quantization error by intervening training dynamics are more applicable for practical usage, compared with previous works' focuses on training data scale.\n- The paper is well presented with sufficient experiments and corresponding figures, making it easy to visualize the key points of the observations."}, "weaknesses": {"value": "- While the empirical results provided by this study are abundant and interesting, the paper fails to provide more in-depth explanations on the reasons that lead to such phenomena, as it's not very explicit to relate factors like learning rate with quantization errors. This might limit the interpretability of the provided results.\n- The selection of evaluated models lacks representativeness, as several of the most widely used and influential model families (e.g., LLaMA, Qwen) are not included. So it remains unclear whether the observed correlations between training dynamics and quantization robustness hold for mainstream architectures."}, "questions": {"value": "- Could the authors provide a theoretical or intuitive explanation for why learning rate decay leads to increased quantization error? A deeper understanding of the underlying mechanism would significantly enhance the paperâ€™s conceptual contribution and long-term impact.\n- The study focuses on the relationship between training dynamics and PTQ. However, given that PTQ is primarily applied when retraining is infeasible or training details are unavailable, what is the practical motivation for analyzing training-phase interventions on PTQ performance? If the authors have the capability to conduct large-scale pretraining & fine-tuning, why not extend the study to Quantization-Aware Training (QAT) and investigate how training dynamics influence QAT outcomes, which may offer more actionable insights for future model development?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tBKLMmETU1", "forum": "ZXr3Xx7Z1O", "replyto": "ZXr3Xx7Z1O", "signatures": ["ICLR.cc/2026/Conference/Submission4049/Reviewer_9xqL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4049/Reviewer_9xqL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761191193807, "cdate": 1761191193807, "tmdate": 1762917152953, "mdate": 1762917152953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the impact of training dynamics (learning rate schedule) on post-training quantization Error. Through the study of various open source model families, the authors find that the learning rate schedule is the primary driver of PTQ error contrary to the popular belief of over-training leading to higher PTQ error. Based on this insight, the authors propose ideas to mitigate this discrepancy."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This is an excellent paper. The authors analyse multiple different families to show that their insights hold.\n- The interventions (at a smaller scale) also demonstrate their findings.\n- PTQ degradation remaining near flat with WSD LR with higher tokens/params is a great observation.\n- The authors study both PTQ error and downstream performance degradation"}, "weaknesses": {"value": "- The authors claim the effect of training hyperparameters on quantization quality hasn't been well studied, yet the authors don't cite Intriguing Properties of Quantization at Scale [1] by Ahmadian et al. \n\n- The fact that smaller learning rates lead to larger PTQ errors hints at the manifold geometry (sharpness etc.) playing a key role in determining the degradation yet there is no discussion about this. It would be nice to relate PTQ errors to the geometry of the loss basin (albeit at a smaller scale). I think this would considerably strengthen the paper.\n\n[1] Intriguing Properties of Quantization at Scale : https://openreview.net/pdf?id=IYe8j7Gy8f"}, "questions": {"value": "- Do the observations hold for recent quantization techniques like QuaRot etc?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pzkEyLTUke", "forum": "ZXr3Xx7Z1O", "replyto": "ZXr3Xx7Z1O", "signatures": ["ICLR.cc/2026/Conference/Submission4049/Reviewer_2yxu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4049/Reviewer_2yxu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761246626869, "cdate": 1761246626869, "tmdate": 1762917152734, "mdate": 1762917152734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explains recent work on post-train quantization scaling finding that degradation increases with data. The authors attribute it instead of learning rate annealing effects, and propose two new ways to mitigate this degradation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly divided into sections, where each one has a clear claim and empirical evidence for it. \n- They replicate and explain past work on PTQ scaling, reassuring the reader that their baselines are well-tuned. \n- They propose interventions to mitigate the identified \"mechanism\" for degradation. \n- Fig5 is particularly strong evidence that the annealing itself is causal for degradation."}, "weaknesses": {"value": "- I'm not entirely sure this paper has enough \"meat\" to be a conference paper. There is a lot of redundancy in the plots, and the contributions/main claim can be summarized as \"higher LR and model averaging can partially mitigate PTQ degradations on long training runs, though we don't know why.\" It's not clear this is enough to comprise a conference paper? \n- Even the core claim that \"these data effects may actually be LR effects\" is actually not novel: [1] make a very similar claim in reference to the same literature, but in the related setting of finetuning instead of quantization. \n- The authors also do not posit any conceptual model explaining their findings, or interpret their findings. Even if there is no theory (which is fine), having a mental model with experimental ablations would be helpful. What exactly is going on here -- the fact that WSD and cosine both end up giving the same PTQ-induced loss but over timescales makes it feel like there is some \"degradation potential\" interpretation in the spirit or flavor of [2]. There is no actual scientific model presented with experiments. \n\n[1] Overtrained Language Models Are Harder to Fine-Tune. Springer et al, 2025. https://arxiv.org/pdf/2503.19206\n\n[2] Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape View. Wen et al., 2025. https://openreview.net/pdf?id=m51BgoqvbP"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q1BscNJO0G", "forum": "ZXr3Xx7Z1O", "replyto": "ZXr3Xx7Z1O", "signatures": ["ICLR.cc/2026/Conference/Submission4049/Reviewer_dMzB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4049/Reviewer_dMzB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761613833402, "cdate": 1761613833402, "tmdate": 1762917152478, "mdate": 1762917152478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines the interplay between post training quantization (PTQ) performance and variables related to training dynamics, specifically learning rate schedule and model averaging. The authors challenge the conclusion from prior work that PTQ error is primarily driven by training duration. The paper presents evidence that observations about training length duration are confounded by learning rate cooldown and that it's the cooldown period which primarily drives PTQ error growth. The authors demonstrated their claims against a large suite of open source models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The question that the paper studies is a very interesting one and disentangling learning rate cooldown from training duration is a subtle but important distinction that can inform practitioners in their pretraining choices.\n* The investigation into model souping provides practical information to guide practitioners in reducing PTQ error.\n* The effect observed is quite convincing in terms of home prominent of a phase transition there is\n* The empirical results are very thorough and replicate over a large suite of models and dataset sizes."}, "weaknesses": {"value": "* While the phenomenon observed is quite interesting, the paper is missing a predictive model of the effect of learning rate on PTQ. Similar to previous works (Kumar et al.), providing some scaling analysis that incorporates the relevant LR parameters would strengthen the paper greatly."}, "questions": {"value": "* While potentially out of scope for this work, it seems important to understand whether the same phenomenon occurs with quantization aware training (QAT). Did the authors run any experiments with QAT?\n* From looking at Figure 3, it seems like the slope of the quantization error decreases as a function of the model size examined. It would be good to disentangle whether lr cooldown impacts quantization error less at larger scales or whether this is the interplay between overtraining and lr cooldowns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FEoK5CE8dx", "forum": "ZXr3Xx7Z1O", "replyto": "ZXr3Xx7Z1O", "signatures": ["ICLR.cc/2026/Conference/Submission4049/Reviewer_Po58"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4049/Reviewer_Po58"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955855566, "cdate": 1761955855566, "tmdate": 1762917152039, "mdate": 1762917152039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}