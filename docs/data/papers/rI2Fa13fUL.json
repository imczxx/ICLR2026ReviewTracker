{"id": "rI2Fa13fUL", "number": 16030, "cdate": 1758258834714, "mdate": 1759897266465, "content": {"title": "Offline Reinforcement Learning with Generative Trajectory Policies", "abstract": "Generative models have emerged as a powerful class of policies for offline reinforcement learning (RL) due to their ability to capture complex, multi-modal behaviors. \nHowever, existing methods face a stark trade-off: slow, iterative models like diffusion policies are computationally expensive, while fast, single-step models like consistency policies often suffer from degraded performance. \nIn this paper, we demonstrate that it is possible to bridge this gap.\nThe key to moving beyond the limitations of individual methods, we argue, lies in a unifying perspective that views modern generative models—including diffusion, flow matching, and consistency models—as specific instances of learning a continuous-time generative trajectory governed by an Ordinary Differential Equation (ODE).\nThis principled foundation provides a clearer design space for generative policies in RL and allows us to propose *Generative Trajectory Policies* (GTPs), a new and more general policy paradigm that learns the entire solution map of the underlying ODE.\nTo make this paradigm practical for offline RL, we further introduce two key theoretically principled adaptations. \nEmpirical results demonstrate that GTP achieves state-of-the-art performance on D4RL benchmarks -- it significantly outperforms prior generative policies, achieving perfect scores on several notoriously hard AntMaze tasks.", "tldr": "", "keywords": ["offline RL", "genearative models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d471dbe0fc78b47e21a660bf0923d89caf78546.pdf", "supplementary_material": "/attachment/61ec39f2edd386315b2c7dd1ff7f1d0839f78d19.zip"}, "replies": [{"content": {"summary": {"value": "Although generative models like diffusion models and consistency models have made great progress in capturing complex behaviors.  However, the expensive computation cost of the multi-step sampling and the low performance of the single-step models are still a major challenge. To address this, this work proposes a unified perspective of these generative models in offline RL, including diffusion, flow matching, and consistency models. Based on this perspective, this work proposes Generative Trajectory Policies (GTPs), a new and more general policy paradigm that learns the entire solution map of the underlying ODE. Extensive experiments on both imitation learning and offline RL show that GTPs outperform prior generative policies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Computational efficiency and training stability are indeed core concerns in diffusion policies.\n\n- It is important to provide a unified view for diffusion policies, flow matching, and consistency models for handling offline RL.\n\n- Extensive experiments, including both imitation learning and offline RL, show the effectiveness of GTP, outperforming various baselines."}, "weaknesses": {"value": "- The Q learning in this work chooses standard double TD learning, which is widely used in online RL. However, in offline RL, several works have studied the overestimation of Q learning and proposed some conservative Q learning methods like CQL or IQL. As this work also considers offline RL, I'm curious about whether the TD learning here also faces the problem of Q overestimation? What about the performance of using CQL or IQL?\n\n- As the computational efficiency is one of the major claims of this work, it is better to add the time cost of each algorithm in table1-2 for better comparison. I only find the results of the time cost in Table 5 of the appendix."}, "questions": {"value": "See weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Tkq1zl8e2z", "forum": "rI2Fa13fUL", "replyto": "rI2Fa13fUL", "signatures": ["ICLR.cc/2026/Conference/Submission16030/Reviewer_3nZj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16030/Reviewer_3nZj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634842495, "cdate": 1761634842495, "tmdate": 1762926232964, "mdate": 1762926232964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response"}, "comment": {"value": "Dear reviewers,\n\nThank you for your valuable feedback and recognizing the following strengths of our paper:\n- **Theoretical contribution & unified framework.** It includes our continuous-time ODE solution-map formulation and the accompanying theoretical guarantees (e.g., Theorem 1). (all the reviewers)\n- **SOTA results** across both behavior cloning and offline RL benchmarks. (all the reviewers)\n\nHere, we addressee several possible concerns:\n1. What is the novelty of this paper?\n\t1. We introduce a **unified continuous-time ODE solution-map framework** showing that CM/CTM/Shortcut/Mean Flows are all special cases differing only in how they approximate the average velocity. This perspective has not appeared before and directly leads to a **new generative policy class** tailored for RL.\n\t2. We further propose an **approximate score**—together with Theorem 1—that provides a practical and theoretically justified supervision signal for RL. Without this approximation, the unified ODE framework would be impractical for RL because ODE-solver-based supervision (as in CTMs) is prohibitively expensive.  This approximate score is what makes the unified framework **actually usable** inside large-scale training.\n2. How is GTP different from CTM? \n\t1. GTP is **not** CTM adapted to RL. Our goal is to _identify the common structure_ across CM/CTM/Shortcut/MeanFlows and describe them in a unified way via average velocity. CTM, Shortcut Models, and Mean Flows can all be seen as reparameterizations of the same underlying ODE trajectory. We adopt a CTM-like parameterization because directly predicting $x_{s}$​ is efficient and aligns with the instantaneous-flow interpretation—not because we start from CTM. Our consistency loss is also **simpler** and arises naturally from the ODE solution-map formulation.\n\t2. CTM requires multi-step ODE supervision (20–40 steps) and DSM/GAN auxiliaries; GTP uses **one-step supervision** via Theorem 1. This is the fundamental difference: GTP is much closer to **flow matching** than to CTM, despite having a superficially similar form. (See Appendix B.4 for detailed discussion.)\n3. Is our value-guided loss just AWR?\n\t1. No. Our goal is to determine how to perform policy improvement for a generative trajectory model whose main supervision comes from behavior-cloning–style losses. Although adding a Q-term (as in Diffusion-QL or Consistency-AC) is possible, our model already uses two generative losses, and introducing a third term makes the weighting highly sensitive. This motivates using a cleaner alternative.\n\t2. Our solution is distribution-level reweighting, which keeps training in-distribution and naturally prioritizes high-value samples. This yields an advantage-weighted objective that improves sample efficiency without introducing extra loss balancing. While advantage weighting is not new, our use of it is driven by the structural needs of our unified ODE framework rather than by borrowing prior heuristics.\n4. What is the performance of GTP under $T=2$?\n\t1. Consistent with prior findings in Consistency-AC, GTP’s performance almost **saturates at** $T=2$. Increasing $T$ beyond 2 yields negligible additional gains but increases compute.\n\t2. The partial $T=2$ results we have completed show that GTP with $T=2$ achieves **slightly lower but very close performance** compared to $T=5$, while being substantially more efficient. We provide the available results below and will update the full table once the remaining runs finish.\n\n\n| Gym                          | GTP (T=2) | GTP (T=5)    | Diffusion-QL   | Consistency-AC |\n| ---------------------------- | ------------ | --------------- | -------------- | -------------- |\n| halfcheetah-medium-v2        | 53.1 ± 0.5   | 53.9 ± 0.1      | 51.1 ± 0.5     | 69.1 ± 0.7     |\n| hopper-medium-v2             | 87.8 ± 2.3   | 90.3 ± 2.7      | 90.5 ± 4.6     | 80.7 ± 10.5    |\n| walker2d-medium-v2           | 90.5 ± 0.5   | 89.5 ± 0.6      | 87.0 ± 0.9     | 83.1 ± 0.3     |\n| halfcheetah-medium-replay-v2 | 48.7 ± 0.2   | 50.8 ± 0.4      | 47.8 ± 0.3     | 58.7 ± 3.9     |\n| hopper-medium-replay-v2      | 101.6 ± 0.5  | 101.7 ± 0.3     | 95.5 ± 1.5 | 99.7 ± 0.5     |\n| walker2d-medium-replay-v2    |              | 94.2 ± 0.3      | 79.5 ± 3.6     | 79.5 ± 3.6     |\n| halfcheetah-medium-expert-v2 |              | 93.8 ± 0.8      | 84.3 ± 4.1     | 84.3 ± 4.1     |\n| hopper-medium-expert-v2      |              | 112.2 ± 0.6     | 96.8 ± 0.3 | 100.4 ± 3.5    |\n| walker2d-medium-expert-v2    |              | 114.2 ± 0.3 | 110.1 ± 0.3    | 110.4 ± 0.7    |\n| **Average**                  |              | 88.7        | 87.9           | 85.1           |"}}, "id": "6IY7gGgfWF", "forum": "rI2Fa13fUL", "replyto": "rI2Fa13fUL", "signatures": ["ICLR.cc/2026/Conference/Submission16030/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16030/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16030/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763707990691, "cdate": 1763707990691, "tmdate": 1763708444288, "mdate": 1763708444288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Generative Trajectory Policies (GTPs), a new framework for offline reinforcement learning (Offline RL) that unifies and extends existing generative policy approaches (e.g., diffusion, flow matching, and consistency models). The key insight is that all these generative models can be understood as instances of learning a continuous-time trajectory governed by an Ordinary Differential Equation (ODE).\n\nGTPs learn the entire solution map of the ODE, enabling efficient and expressive policy generation without the usual trade-off between sampling speed and representational power. The authors propose two main innovations to make this practical for offline RL:\n\nScore Approximation – replaces expensive ODE solvers with a closed-form surrogate to stabilize and accelerate training.\n\nValue-Driven Guidance – integrates advantage-weighted objectives to align generative modeling with RL policy improvement.\n\nExperiments on the D4RL benchmark show that GTPs achieve state-of-the-art performance, even obtaining perfect scores on challenging AntMaze tasks, surpassing prior diffusion and consistency policies in both expressiveness and efficiency.\n- An LLM was used to improve writing."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Conceptual Unification\n\nProvides a principled theoretical framework that connects diffusion, flow matching, and consistency models under a unified ODE-based formulation. Offers clear insight into the relationship between these methods.\n\n\n2. Strong Empirical Results\n\nConsistent SOTA performance on D4RL, with large gains in AntMaze tasks and solid results across Gym benchmarks, for both BC policy and value driven policy improvement.\n\n\n3. Clear Comparison & Ablations\n\nComprehensive experiments comparing to D-QL, C-AC, and other baselines.\n\nAblation studies verify that both score approximation and value guidance materially contribute to performance."}, "weaknesses": {"value": "1. Scope of Benchmarks\n\nAll experiments are on D4RL (a standard but limited suite). Testing on high-dimensional, complex robotics tasks would strengthen claims of generality."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OYLZlIAXL0", "forum": "rI2Fa13fUL", "replyto": "rI2Fa13fUL", "signatures": ["ICLR.cc/2026/Conference/Submission16030/Reviewer_eCrf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16030/Reviewer_eCrf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953898035, "cdate": 1761953898035, "tmdate": 1762926232594, "mdate": 1762926232594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Generative Trajectory Policy (GTP), which improves upon diffusion- and consistency-based policies by removing the slow, iterative sampling of diffusion models while avoiding the performance loss of prior consistency policies. Built on a unified continuous-time ODE framework, GTP learns the ODE‘s solution map via two complementary loss functions. For offline reinforcement learning, the authors introduce two key adaptations: a score approximation that replaces costly multi-step integrations with one-step surrogates for better efficiency and stability, and an advantage-weighted loss that guides the learned policy toward high-return regions. Experiments on D4RL tasks show that GTP achieves state-of-the-art performance in both behavior cloning and offline RL, with ablations confirming that these adaptations improve stability, generative quality, and training efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, clearly motivating the problem and guiding the reader from a high-level theoretical unification through to specific, well-justified algorithmic implementations.\n\n2. The paper successfully grounds its conceptual framework in a practical implementation by introducing two crucial adaptations specifically tailored for the offline RL setting: an efficient score approximation for stable training and a value-driven objective for policy improvement.\n\n3. The proposed GTP achieves state-of-the-art empirical results on D4RL benchmarks, validating its ability to model complex behaviors and effectively improve policies."}, "weaknesses": {"value": "1. The two loss functions derived in Section 3 both have corresponding counterparts in CTM [1], so the main contribution of the paper is essentially the application of CTM to the offline RL setting. Although the authors claim two innovations, the second one, using the advantage as a weighting factor for sample supervision, is a relatively straightforward and well-established idea that has been widely adopted in AWR [2] and later studies. In my view, the main novelty lies in replacing multi-step intermediate samples with a one-step surrogate term, but it remains questionable whether this alone can convincingly demonstrate the paper's novelty.\n\n2. According to Table 4, the authors tune several hyperparameters across different tasks, including the learning rate, $\\eta$, maximum Q backup, and gradient norm. However, they do not conduct a dedicated hyperparameter study, leaving the method’s sensitivity to these parameters unclear.\n\n3. The performance gain of GTP in offline RL tasks compared to that in BC is relatively modest, which may indicate that the proposed value-driven guidance is not quite effective.\n\n[1] Kim, Dongjun, et al. \"Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion.\" *The Twelfth International Conference on Learning Representations*\n\n[2] Peng, Xue Bin, et al. \"Advantage-weighted regression: Simple and scalable off-policy reinforcement learning.\" *arXiv preprint arXiv:1910.00177* (2019)."}, "questions": {"value": "How does the advantage-weighted policy optimization used in this paper compare with the classifier-free guidance approach in terms of effectiveness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s9t2Gr9LUC", "forum": "rI2Fa13fUL", "replyto": "rI2Fa13fUL", "signatures": ["ICLR.cc/2026/Conference/Submission16030/Reviewer_dRVt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16030/Reviewer_dRVt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967211168, "cdate": 1761967211168, "tmdate": 1762926232243, "mdate": 1762926232243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel policy class for offline reinforcement learning, GTP, which addresses the trade-off between slow, high-performance diffusion models and fast, lower-quality consistency models. GTPs are based on a unifying framework that views generative models as continuous-time ODEs, and the policy learns the entire solution map of this ODE. GTP consistently outperforms the baseline methods on the D4RL benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Theoretical groundness\n\nThe paper's theoretical grounding is a key strength. Theorem 1 presents an effective method to replace expensive ODE solvers, which appears to be a novel and impactful contribution. The paper's important claims are stated formally and supported with rigorous proofs.\n\n2. Effective performance over behavioral cloning baselines\n\nThe empirical results for behavior cloning are impressive. As demonstrated in Table 1, GTP-BC outperforms both standard offline RL baselines and existing diffusion-based behavior cloning methods. This is a strong result that suggests GTP may indeed be a more effective and expressive policy class compared to other generative model-based approaches."}, "weaknesses": {"value": "1. Limited novelty of value-driven guidance\n\nThe novelty of the proposed value-driven guidance appears limited. The paper's core policy improvement technique, advantage weighted regression, is a well-established baseline in offline reinforcement learning [1]. Furthermore, the insight that an advantage-weighted generative loss corresponds to a KL-regularized policy optimization problem is already well-known in the field [2].\n\n2. Are diffusion-based offline RL baselines really slow?\n\nThe motivation for introducing GTP, which is predicated on the computational burden of diffusion policies, appears to be contradicted by the authors' own empirical results. The introduction claims diffusion-based baselines are seriously slow, yet Table 5 shows the diffusion policy generates actions in just 0.00116 seconds. This aligns with my own experience in training diffusion policies for offline RL, where I have similarly observed that action generation is not a significant bottleneck. The 862Hz control frequency is fast enough and weakens the paper's core motivation.\n\n3. Efficiency concern: modest practical speed-up\n\nThe claim that GTP is effective could be strengthened with further clarification on two points. First, regarding efficiency, the practical significance of the speed-up appears modest. While Table 5 shows a 23% relative gain (0.94ms vs 1.16ms), the absolute difference is only 0.22ms. It is unclear if this small time-unit saving translates to a meaningful benefit in a practical setting, which relates back to the question of whether the baseline's speed was a significant bottleneck.\n\n4. Comparability concern about the SOTA claim\n\nI noted in Table 4 that GTP's hyperparameter search included \"gradient norm\", a parameter not reported in the original DQL paper [3]. To make the comparison more direct and robust, it would be beneficial to either include a sensitivity analysis on this parameter's effect or, ideally, to evaluate all baselines under the same, unified hyperparameter search space.\n\n5. Limited baselines and experimental scope\n\nThe experimental evaluation could be significantly strengthened by expanding the set of baselines. We noted the omission of FQL [4], a method that appears to share a similar motivation to this work. Consequently, its related methods, such as FAWAC and Flow-based Behavior Cloning policies, were also missing. Including these comparisons would be highly valuable. In particular, a direct comparison against FAWAC seems critical, as the primary difference appears to be the choice of generative backbone (Flow Matching vs. GTP). This would help isolate the specific benefits of the GTP architecture. Finally, to substantiate the claims of GTP's generality, we recommend evaluating the method on a larger-scale benchmark, such as OGBench [5].\n\n[1] Nair, Ashvin, et al. \"Awac: Accelerating online reinforcement learning with offline datasets.\" arXiv preprint arXiv:2006.09359 (2020).\n\n[2] Kang, Bingyi, et al. \"Efficient diffusion policies for offline reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 67195-67212.\n\n[3] Wang, Zhendong, Jonathan J. Hunt, and Mingyuan Zhou. \"Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning.\" The Eleventh International Conference on Learning Representations.\n\n[4] Park, Seohong, Qiyang Li, and Sergey Levine. \"Flow q-learning.\" arXiv preprint arXiv:2502.02538 (2025).\n\n[5] Park, Seohong, et al. \"OGBench: Benchmarking Offline Goal-Conditioned RL.\" The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "1. What is the performance difference between GTP with k=2 and C-AC [6] with k=2?\n\n2.  The value-driven guidance presented in Theorem 2 appears to be a direct application of Advantage-Weighted Regression (AWR), a well-established technique. Could the authors clarify the specific novelty of this component beyond the standard AWR formulation?\n\n3. Table 5 shows that GTP is 0.22ms faster than the diffusion baseline in absolute terms. In what practical, real-world robotics or control scenarios would this negligible 0.22ms speed-up provide a meaningful benefit?\n\n\n[6] Ding, Zihan, and Chi Jin. \"Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning.\" The Twelfth International Conference on Learning Representations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KISx0H95IN", "forum": "rI2Fa13fUL", "replyto": "rI2Fa13fUL", "signatures": ["ICLR.cc/2026/Conference/Submission16030/Reviewer_cuSk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16030/Reviewer_cuSk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978832164, "cdate": 1761978832164, "tmdate": 1762926231894, "mdate": 1762926231894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}