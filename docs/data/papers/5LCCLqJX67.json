{"id": "5LCCLqJX67", "number": 17674, "cdate": 1758279068925, "mdate": 1759897161412, "content": {"title": "Thinking Augmented Pre-training", "abstract": "This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to $100$B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of $3$. For a $3$B parameter model, it improves the post-training performance by over $10%$ on several challenging reasoning benchmarks.", "tldr": "This paper introduces a simple and scalable approach to improve the data efficiency of LLM training by augmenting existing text data with thinking trajectories.", "keywords": ["language model pre-training", "data efficiency", "language model reasoning", "data engineering"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c655bd7daa7f4d319ac6160781dad46013c9817.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a simple approach to improve the data efficiency of LLMs, namely Thinking-augmented Pre-Training (TPT). It augments raw text corpora with automatically generated *thinking trajectories*, which involves step-by-step reasoning rationales produced by LLMs. This augmentation effectively increases the volume of the training data, and makes high-quality tokens more learnable with the step-by-step reasoning trajectories. The paper applies TPT across diverse training regimes (from-scratch pre-training, data-constrained settings, and mid-training from strong open-source checkpoints) using up to 100B tokens. Experiments show gains across model sizes and benchmarks, with reported improvements of >10% on challenging reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It is valuable to improving data efficiency in an era of diminishing returns from scaling data. The paper directly acquires training data with explicit reasoning decomposition.\n2. The method is simple, and does not require human annotation. It can be easy to obtain the data and useful in multiple training phases. \n3. The paper reports consistent result improvements, which demonstrates the effectiveness of the proposed method."}, "weaknesses": {"value": "1. (Major) The novelty is somewhat limited. The core idea to train on self-generated rationales has precedents in several researches, such as distillation, synthetic data, and self-explanation. The paper ought to more clearly differentiate TPT from these approaches, especially in terms of training stage, data construction, and scalability.\n2. (Major) The quality and reliability of generated trajectories are not guaranteed. The method relies entirely on LLMs to generate reasoning traces. However, there is no analysis of error propagation (e.g., how often are the trajectories incorrect or unhelpful?), nor any filtering or verification mechanism. This raises concerns about potential data pollution.\n3. (Minor) It is not clear about the generalization beyond reasoning tasks, such as language modeling perplexity, commonsense QA, or stylistic generation. It would be valuable to assess whether TPT helps (or hurts) performance on non-reasoning tasks."}, "questions": {"value": "Please see strengths and weaknesses.\n\nBesides, how to ensure the quality of the automatically generated thinking trajectories? Did you perform human or automatic evaluation (e.g., faithfulness, coherence) on these trajectories? Besides, the teacher model capability may also affect TPT performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XldK8DHWWW", "forum": "5LCCLqJX67", "replyto": "5LCCLqJX67", "signatures": ["ICLR.cc/2026/Conference/Submission17674/Reviewer_cAFT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17674/Reviewer_cAFT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708533131, "cdate": 1761708533131, "tmdate": 1762927524089, "mdate": 1762927524089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces thinking augmented pretraining, a pretraining method which adds thinking tokens generated by a strong model to the original pretraining data. The motivation is that by using these traces, the model will learn how to think during the pretraining phase. The authors demonstrate that TPT improves performance on reasoning tasks and can increase data efficiency 3x shown on both pre-training and mid-training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Nice idea for \"re-writing\" the pretraining data to include thinking tokens\n- Method is simple, well explained, and can be easily implemented\n- Figure 2 (right) clearly shows that this method is better than vanilla pretraining\n- The abundant data results are nice."}, "weaknesses": {"value": "- My biggest concern is that it doesn't show it help for post-training. Since it's used to generate the thinking tokens, we can assume that we already have a post-trained model. It's not shown that this helps over that original model (i know that it's a bigger model, but it still needs to be shown that this method helps post-training in general). \n- The SFT dataset is R1 distilled, and the TPT thinking traces are distilled-R1). So it seems like this is primarily just R1 thinking distillation. This is ok in general because it shows that distilling into pretraining helps, but I think more experiments need to be conducted to show that this full pipeline with postraining works.\n- \"Midtraining\" experiments are done on models that are already post-trained (llama-instruct and qwen-instruct). Ideally this should be done on base models\n- The analysis and ablations don't really give insight to why this method is performing better than pretraining on the raw MegaMath tokens. It seems like a lot has to do with pure thinking distillation. I wonder if you can elicit this behavior with pure synthetic data from DS-Distill-Qwen-7B and not using MegaMath at all. Or re-writing the data with DS-Distill-Qwen-7B, instead of \"appending\"."}, "questions": {"value": "- Would lab that's launching a large-scale pretraining run actually do this? The experiments are done on small scale models, which is fine, but it's unclear what the benefits would be as you scale up\n- Is \"constrained data\" really a problem for pretraining? This is mostly done for large-scale runs\\\n- The models in Table 2 are unclear to me. Do you pretrain the baselines with the same pretraining data that you use for TPT (MegaMath-Web-Pro-Max), but without the thinking tokens? If not, I don't think this is a fair baseline.\n- How important is the thinking generator model (DS-Distill-Qwen-7B)? Can you easily swap it for other models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wI3gJuTMWo", "forum": "5LCCLqJX67", "replyto": "5LCCLqJX67", "signatures": ["ICLR.cc/2026/Conference/Submission17674/Reviewer_6v2k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17674/Reviewer_6v2k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886814501, "cdate": 1761886814501, "tmdate": 1762927523606, "mdate": 1762927523606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Thinking Augmented Pre-Training (TPT), a data engineering method designed to improve the data efficiency and reasoning capabilities of Large Language Models (LLMs). The method involves using an \"off-the-shelf\" teacher LLM to generate a \"thinking trajectory\" (e.g., a step-by-step rationale) for each document in an existing pre-training corpus. This trajectory is then concatenated with the original document to create an augmented dataset. A new LLM is then trained from scratch (or mid-trained) on this augmented data using a standard next-token prediction objective. The authors claim this \"simple and scalable\" method improves data efficiency by a factor of 3  and enables an 8B model to achieve reasoning performance superior to strong baselines (like LLaMA-3.1-8B-Instruct) trained on 150x more data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, and the method is described in an easy-to-understand manner. \n\n2. The core idea of TPT is conceptually simple and elegant.\n\n3. The experimental results are significant."}, "weaknesses": {"value": "1. The \"abundant data\" pre-training experiment, training an 8B parameter model from scratch on only 100B tokens , is severely undertrained by modern standards (where models are trained on 10T+ tokens ). This limited training budget makes it difficult to assess the true, long-term value of the TPT method. The \"3x data efficiency\"  claim may be an artifact of this specific, undertrained regime, where the model converges faster on the \"thinking\" data, rather than an indicator of a higher asymptotic performance.\n\n2. The mid-training experiments in Table 2  lack the most crucial control group. The paper compares its TPT mid-training models (which receive 100B tokens of TPT data plus SFT ) against base models that only receive SFT (e.g., \"OpenR1-LLAMA-3B*\"). This comparison is confounded. It is impossible to know if the observed gains come from the nature of TPT data or simply from the act of continual pre-training on 100B additional tokens. A necessary baseline—a model mid-trained on 100B tokens of the original, non-augmented dataset before SFT—is not included in this table, rendering the conclusions about TPT's mid-training efficacy unsubstantiated.\n\n3. The paper's key results table (Table 2)  contains highly anomalous scores that undermine the evaluation's credibility. \n    - The TPT-LLAMA-3B model scores a mere 0.3 on GSM8K, which is nonsensical for a model that simultaneously scores 75.5 on MATH-500 and 18.6 on AIME24. This score is lower than its non-thinking OpenR1-LLAMA-3B* counterpart (3.8) and suggests a catastrophic failure in either the SFT process or the evaluation pipeline.\n    - For stronger models, the scores are inconsistent. The DS-Distill-Qwen-7B reference model achieves a powerful 53.2 on AIME24 but only 90.7 on GSM8K. Models with such strong Olympiad-level math reasoning would be expected to be at or near saturation on a grade-school math benchmark like GSM8K. This disconnect suggests the SFT hyperparameters were not properly tuned, leading to unreliable results.\n\n4. The paper's premise is that TPT teaches models to reason. However, the method is functionally a simple distillation, where a student model is trained to reproduce the text generated by a teacher. It is a common finding that augmenting pre-training with such data improves performance on pre-train evaluations (i.e., the model gets good at mimicking the \"thinking\" style) but fails to generalize or show robust gains after post-training (SFT). The paper does not provide sufficient evidence (e.g., post-train evaluations on held-out reasoning tasks) to disprove this alternative explanation."}, "questions": {"value": "1. Can the authors justify the 100B token budget for an 8B model? Given this is far from convergence, how can the authors be confident that the TPT model isn't just learning faster on structured data, rather than learning better or reaching a higher final performance?\n\n2. Can the authors please explain the 0.3 GSM8K score for the TPT-LLAMA-3B model? \n\n3. Given the inconsistent results, what steps were taken to ensure optimal SFT hyperparameter tuning for all models being compared, including the baselines? The low GSM8K scores suggest the SFT process may have been flawed, potentially biasing the comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "njahDjgIQz", "forum": "5LCCLqJX67", "replyto": "5LCCLqJX67", "signatures": ["ICLR.cc/2026/Conference/Submission17674/Reviewer_7Wjm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17674/Reviewer_7Wjm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933036789, "cdate": 1761933036789, "tmdate": 1762927523220, "mdate": 1762927523220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a novel way to approach pretraining by augmenting existing data with LLM generated thoughts. Authors employ a strong, pre-trained reasoning model to add more data into existing corpus, and execute pretraining with that data. They show how such pretrained model is much better than counter parts trained on similarly sized data without augmented data inside, as well as better than models trained on much more human data. \nFurther, they show how fine-tuning with such data brings substantial improvements on the benchmarks related to reasoning and other LLM capabilities related to solving some tasks. Finally, they performed ablations with different model families and sizes, as well as different ways to generate trajectories."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Recently there have been a massive exodus of papers in this direction. All of them somehow generate more high quality data from a strong reasoning model to add into an existing pretraining corpus. Different publications tend to differentiate in a ways of adding this data, types of prompts they use, and how the resulting augmented data is used for pre or mid training. \n\nThe main strength of this work is the experimental results and ablations showing that their model is better than pretrained models that did not see such data. There been too many simultaneous work in this direction since it was an obvious move, but I believe that all of them did it with originality in mind."}, "weaknesses": {"value": "My main concern here is that we aim at improving the benchmark scores where the model we use to generate more data from already excel at those benchmarks. I might be wrong here, but this might be creating a vicious loop between the distributions of data we fit and \"seemingly unknown\" evaluation sets we test it on.\nWe may always find a strong teacher to augment such datasets and then show improvements on training much smaller models in modest data regimes. Then how would we improve the strongest teachers? To answer that questions we might want to experiment in the setup where the teacher is as strong as the mid training checkpoint we start from. Authors performed some ablation experiments in this direction (using smaller model for trajectory generation), but that smaller model is very mighty as its distilled from R1 model."}, "questions": {"value": "1. Do you think this data augmentation is a prominent approach to go with a frontier model development?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8UIDdNxVlY", "forum": "5LCCLqJX67", "replyto": "5LCCLqJX67", "signatures": ["ICLR.cc/2026/Conference/Submission17674/Reviewer_Kpxd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17674/Reviewer_Kpxd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762024878742, "cdate": 1762024878742, "tmdate": 1762927522716, "mdate": 1762927522716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}