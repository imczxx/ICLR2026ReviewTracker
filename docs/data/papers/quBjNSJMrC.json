{"id": "quBjNSJMrC", "number": 2260, "cdate": 1757042925597, "mdate": 1759898159768, "content": {"title": "Squeeze the Soaked Sponge: Efficient Off-policy RFT for Large Language Model", "abstract": "Reinforcement Learning (RL) has demonstrated its potential to improve the reasoning ability of Large Language Models (LLMs), yet most existing Reinforcement Finetuning (RFT) methods are inherently \\textit{on-policy} RL, failing to reuse historical data and thus preventing efficient scaling. In this work, we explore the potential of \\textit{off-policy} RL to leverage historical data for rollout-efficient RFT. Specifically, we propose \\textbf{Re}incarnating \\textbf{Mix}-policy Proximal Policy Gradient (\\textbf{ReMix}), which enables on-policy RFT methods to leverage off-policy data. ReMix consists of three major components: (1) Mix-policy proximal policy gradient with an increased Update-To-Data (UTD) ratio that utilizes the data from both current and past policies for efficient training; (2) KL-Convex policy constraint that combines the KL constraints on the base and precedent model to balance stability and flexibility; (3) Policy reincarnation that replaces the base model with the mix-policy RFT model in the mid way of training and restarts on-policy training, to achieve a seamless transition from early efficiency to steady convergence. In our experiments, we train a series of ReMix models based on PPO, GRPO from 1.5B, 7B base models. On five math reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and MATH500), ReMix achieves an average Pass@1 accuracy of \\textbf{52.10\\%} (with \\textbf{0.079M rollouts}) and \\textbf{64.39\\%} (with \\textbf{0.011M rollouts}) on 1.5B and 7B models, respectively. Compared with 15 recent advanced models, ReMix shows SOTA-level performance with an over \\textbf{30x to 450x reduction in training cost in terms of rollout data volume}, demonstrating superior training efficiency. Additionally, our multifaceted analysis reveals insightful findings, including the implicit preference for shorter responses of off-policy RFT, the collapse mode of self-reflection under severe off-policyness, etc.", "tldr": "", "keywords": ["Reinforcement Finetuning", "Large Language Model", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3914344474c393dabf301e5e72664a00b36eb981.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel method designed to improve the sample efficiency of Reinforcement Finetuning for LLMs. The core problem addressed is the inefficiency of on-policy RL algorithms like PPO, which are standard in RFT but discard historical data after each update. ReMix combines three main components to enable the use of off-policy data including mix-policy proximal policy gradient objective, KL-Convex policy constraint and policy reincarnation step. This paper demonstrates that ReMix achieves better performances on several math reasoning benchmarks than baseline approaches and models with a great reduction in the required volume of rollout data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a critical and expensive problem in the development of advanced LLMs: the prohibitive computational cost of reinforcement finetuning. The promise of achieving comparable or better performance with orders of magnitude less data is a highly significant contribution that could democratize the development of powerful reasoning models and enable further scaling.\n\n2. The empirical evaluation is extensive. The method is tested on two different model scales across several math reasoning benchmarks in comparison with a lot of recent models. Moreover, this paper includes a lot of abalation study such as UTD ratios, KL-convex and policy reincarnation, and experiments in different scenarios such as training with constrained max response length and with differnet prompts."}, "weaknesses": {"value": "1. Insufficient Comparison with SOTA Off-Policy Methods: The paper's primary weakness is its failure to adequately benchmark against other contemporary off-policy RFT methods. As a lot of papers discussed, GRPO is a biased estimation of KL penalty.[1, 2] Therefore, a lot of recent baselines should be included, such as DAPO [3]. Moreover, in Appendix F.3, the paper discusses the reason why it does not include other baseline algorithms including RePO, SRPO, SPO, AGRO and AsymRE with excuses like the lack of public checkpoints, different scales, or restrictive generation lengths.. However, such reasons are not sufficient to support the reason why the paper chooses only 2 baselines methods to re-implement. In addition, this paper chooses several baseline models which are trained with different datasets and settings. Therefore, it is unclear whether the quality of datasets and training hyper-parameters contributes to the improvement of the proposed method over these models.\n\n2. Limited Scope of Evaluation: All experiments are conducted exclusively on math reasoning tasks. While this is a challenging and important domain, the paper makes general claims about improving RFT for LLMs. It is unclear whether the method's effectiveness, particularly its implicit bias toward shorter responses, would be validated in other domains such as code generation. The findings may be domain-specific, a limitation that is not acknowledged.\n\nReferences: \n\n[1] Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., ... & Lin, M. (2025). Understanding r1-zero-like training: A critical perspective. arXiv preprint arXiv:2503.20783.\n\n[2] Zhang, Y., Liu, Y., Yuan, H., Yuan, Y., Gu, Q., & Yao, A. C. C. (2025). On the design of kl-regularized policy gradient algorithms for llm reasoning. arXiv preprint arXiv:2505.17508.\n\n[3] Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., ... & Wang, M. (2025). Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476."}, "questions": {"value": "How did you determine the optimal values for policy reincarnation step T in {50, 100}? Is there a risk that a poorly chosen T could erase the early-stage efficiency gains or lead to premature convergence before the model has fully benefited from off-policy exploration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uOVNtHtKVu", "forum": "quBjNSJMrC", "replyto": "quBjNSJMrC", "signatures": ["ICLR.cc/2026/Conference/Submission2260/Reviewer_1DV6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2260/Reviewer_1DV6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761163771537, "cdate": 1761163771537, "tmdate": 1762916165893, "mdate": 1762916165893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new RL workflow for reasoning models, composed of three key designs: 1) mixture of online and offline data 2) mixture of current policy and base policy in KL regularization 3) mixture of training algorithms. The experiments are conducted on 5 different benchmarks, showing advantages over vanilla PPO and GRPO, in performance and training cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- clear figures\n- extensive experiments on various benchmarks\n- prominent cost reduction compared with vanilla PPO and GRPO\n- the performance is generally comparable with online versions, even better sometimes"}, "weaknesses": {"value": "- only trained on Qwen model, which is known to be unreliable for reasoning training [1].\n- no theoretical explanation for the design of KLC loss and ReMix algorithm\n- no explanation for the choice of $\\lambda$ and $T$\n- figure 4 is too noisy for solid analysis\n- proof for eq.6 is missed\n\n[1] Shao et al. Spurious Rewards: Rethinking Training Signals in RLVR. https://arxiv.org/abs/2506.10947"}, "questions": {"value": "- why does ReMix-GRPO make the model even worse then base model for AIME in table 1?\n- what is the exact formulation of distribution $i\\sim \\nu$ used in practice, and is there any theoretical explanation?\n- is pass@1 enough? as [2] indicates that, the performance on pass@k might not be improved. \n- vanilla GRPO has a length normalization term, while some variants like Dr. GRPO [3] removed it. Did you apply length normalization in your experiments?\n\n[2] Yue et al. Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model? https://arxiv.org/abs/2504.13837\n\n[3] Liu et al. Understanding R1-Zero-Like Training: A Critical Perspective. https://arxiv.org/abs/2503.20783"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ROSru51Gkq", "forum": "quBjNSJMrC", "replyto": "quBjNSJMrC", "signatures": ["ICLR.cc/2026/Conference/Submission2260/Reviewer_h1SL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2260/Reviewer_h1SL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761292844125, "cdate": 1761292844125, "tmdate": 1762916165770, "mdate": 1762916165770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **ReMix** (*Reincarnating Mix-policy Proximal Policy Optimization*), a framework designed to make reinforcement fine-tuning (RFT) of large language models more sample-efficient by effectively leveraging off-policy data.  \n\nReMix extends standard on-policy approaches such as **PPO** and **GRPO** through three main components:  \n\n- **Mix-policy proximal gradient:** Combines on- and off-policy trajectories to achieve higher update-to-data (UTD) ratios and more efficient data reuse.  \n- **KL-convex constraint:** Interpolates between the base and current policies to ensure updates remain both stable and adaptive.  \n- **Policy reincarnation:** Periodically resets the base model to the current policy, enabling a smooth transition from rapid early learning to stable convergence.  \n\nAcross five math-reasoning benchmarks—**AIME24**, **AMC23**, **Minerva**, **OlympiadBench**, and **MATH500**—ReMix achieves state-of-the-art Pass@1 accuracy while using **30×–450× fewer rollouts** than leading baselines such as **DeepScaleR** and **AceReason**.  \nThe authors also analyze the behavioral effects of off-policy learning, observing patterns like shorter responses and reduced self-reflection when off-policyness becomes excessive."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Clear Motivation & Contribution** – Addresses the well-known issue of *sample inefficiency* in RFT with a principled and well-structured solution.  \n**Novel Integration of Off-Policy Concepts** – The combination of mix-policy updates, convex KL constraint, and policy reincarnation is original and intuitively justified.  \n**Strong Empirical Results** – Achieves SOTA reasoning accuracy while reducing training cost by 30×–450× across multiple benchmarks.  \n**Comprehensive Experiments** – Includes ablation studies, efficiency–accuracy trade-off curves, and behavioral analyses (e.g., response length, self-reflection rate).  \n**Readable and Well-Presented** – The paper is clearly written and easy to follow."}, "weaknesses": {"value": "Overall, there are no major weaknesses in this paper. A minor concern is that the authors did not compare **ReMix** with other *off-policy* algorithms, relying only on *on-policy* baselines. Including comparisons with off-policy methods in future work would provide a more comprehensive evaluation and strengthen the empirical claims."}, "questions": {"value": "1. In Eq. (3), why do the authors use \\(\\frac{\\pi_K}{\\pi_{K-i}}\\) to control the trust region? What is the motivation or benefit compared to using a fixed constant region? It would be helpful to see empirical evidence or ablation results demonstrating that this adaptive trust-region design improves performance.  \n2. In Fig. 3, ReMix-PPO appears to achieve roughly \\(6\\times\\) faster improvement than vanilla PPO in the early training stage. Does this imply that early trajectories are reused multiple times for updates? If so, could this repeated use of the same trajectories lead to training instability or bias accumulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5BytFFAEqk", "forum": "quBjNSJMrC", "replyto": "quBjNSJMrC", "signatures": ["ICLR.cc/2026/Conference/Submission2260/Reviewer_pHkv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2260/Reviewer_pHkv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683753719, "cdate": 1761683753719, "tmdate": 1762916165635, "mdate": 1762916165635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ReMix, a method for efficient off-policy Reinforcement Finetuning (RFT) of Large Language Models. ReMix integrates three key components: a Mix-Policy Proximal Policy Gradient to utilize historical data with an increased Update-to-Data ratio, a KL-Convex policy constraint that dynamically blends penalties from the base model and the previous policy, and a Policy Reincarnation mechanism that switches the base model and reverts to on-policy training mid-way. The authors demonstrate strong empirical results on mathematical reasoning benchmarks, showing that ReMix-based models can achieve state-of-the-art or competitive performance with a dramatic reduction (30x to 450x) in the amount of rollout data required compared to several strong baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-Designed and Synergistic Method: The three-component architecture is intuitive and addresses key challenges in off-policy RFT. The components work synergistically, as shown by the ablations, with Policy Reincarnation being a particularly clever and novel idea for bridging early-stage efficiency and late-stage stability.\n\n- Rigorous Ablation Studies: The ablation study in Table 3 is comprehensive and effectively validates the contribution of each proposed component, clearly showing that the full combination is necessary for optimal performance."}, "weaknesses": {"value": "- Overly Optimistic and Potentially Misleading Framing: The paper's central claim of \"30x to 450x\" reduction in rollout data is framed in a way that overstates the advantage. This is achieved by comparing ReMix's early-stage performance against baselines' final performance (e.g., ReMix-PPO at 100 steps vs. PPO at 900 steps). A comparison of final performances shows a much more modest efficiency gain, undermining the dramatic narrative.\n- Limited Scope of Evaluation: The empirical validation is constrained in two key dimensions: model scale (≤7B parameters) and task domain (mathematical reasoning only). This leaves the generality of ReMix an open question. Its effectiveness on larger models (e.g, 70B+) or on fundamentally different tasks (e.g., code generation) is not established. Furthermore, the absence of ReMix-GRPO results for the 7B model is a notable omission, as it prevents a consistent assessment of the method's performance across different underlying RL algorithms at a more capable scale."}, "questions": {"value": "- The KL-Convex constraint uses a dynamically decaying coefficient $\\lambda(t)$ to balance between the base and previous policy. The design of this decay schedule $\\lambda(t) = \\max\\left(1 − 0.1 \\cdot \\lceil \\max(t − 50, 0) / 10 \\rceil, 0.5\\right)$ appears to be heuristic. Could you provide an analysis that justifies this specific form?\n- The Policy Reincarnation mechanism is a critical component, yet its triggering is based on a predetermined step $T$. How was this specific hyperparameter chosen, and what is the sensitivity of the final performance to this choice?\n- The analysis reveals a strong \"preference for shorter responses.\" Do you think this bias could be harmful in tasks where comprehensive reasoning is crucial, and if so, how could ReMix be adapted to control for this effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jTX2B78AMG", "forum": "quBjNSJMrC", "replyto": "quBjNSJMrC", "signatures": ["ICLR.cc/2026/Conference/Submission2260/Reviewer_cESh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2260/Reviewer_cESh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930206196, "cdate": 1761930206196, "tmdate": 1762916165422, "mdate": 1762916165422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}