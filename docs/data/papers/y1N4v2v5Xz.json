{"id": "y1N4v2v5Xz", "number": 17876, "cdate": 1758281526821, "mdate": 1763275129693, "content": {"title": "Consistent Labeling Across Group Assignments: Variance Reduction in Conditional Average Treatment Effect Estimation", "abstract": "Numerous algorithms have been developed for Conditional Average Treatment Effect (CATE) estimation. In this paper, we first highlight an overlooked issue in CATE estimation: many algorithms exhibit inconsistent learning behavior for the same instance across different group assignments. We introduce a metric to quantify and visualize this inconsistency. Next, we present a theoretical analysis showing that this inconsistency indeed contributes to higher test errors and cannot be resolved through conventional machine learning techniques. To address this problem, we propose a general method called **Consistent Labeling Across Group Assignments** (CLAGA), which eliminates the inconsistency and is applicable to any existing CATE estimation algorithm. Experiments on both synthetic and real-world datasets demonstrate significant performance improvements with CLAGA.", "tldr": "We propose an enhancement applicable to any existing CATE estimation algorithms to reduce prediction variance and thus improve test errors with theoretical justification.", "keywords": ["conditional average treatment effect", "consistent labeling", "group assignments", "potential outcomes", "variance reduction"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3f604abdea7284260d1a234ae50b9f8702a620ea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a new framework built on top of existing CATE estimation methods to address what the authors describe as an inconsistency in how “vanilla” estimators perform across different treatment assignment groups. This inconsistency is quantified using a proposed metric called the Discrepancy Ratio (DR). The framework, termed Consistent Labeling Across Group Assignments (CLAGA), aims to mitigate this claimed inconsistency."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The authors clearly demonstrate a problem of cate estimation\n- The authors provide theoretical insight into the estimation error via decomposition, which may be a useful tool for discussion and analysis\n- The authors present a new framework intended to address the identified inconsistency in CATE estimation"}, "weaknesses": {"value": "- By developing their own metric on which they showcase a problem and own solution, the authors develop their own niche with no immediate comparison work available. Nevertheless other works concerned with overall CATE estimators performance exist and should be included. For example, algorithmic fairness research may be directly relevant here.\n- The presented issue and consequent solution are motivated by Figure 1, which is correctly caption as showcasing overfitting. Method for tackling overfitting are ubiquitous across machine learning disciplines. Discussion and comparison to e.g. methods with regularization or attempts at showing double descent seem like natural follow ups, but they are not discussed here\n- The DR learner and R learner have been shown to be asymptotically optimal. From that perspective nothing better can be done and only finite sample performance can be improved. Discussion of asymptotic point of view is missing.\n- The proposed method itself further splits the data on top of already present cross fitting necessary by existing methods such as the DR learning. This leads to further decrease of dataset size used to fit each stage. This disadvantage seems significant and is not discussed.\n- The proposed method introduces a new supervised learning step with synthetic labels on top of existing works. Such approach can be intuitively seen simply as a form of regularization for overfitting. Since the additional layer does not train wrt targets from the data but synthetic ones, no new information is being extracted from the dataset - only regularization occurs. In this sense, the method closely resembles stacking or self-distillation. This interpretation suggests that CLAGA’s gains may stem from known effects of cross-fitting and stacking, yet this crucial perspective is not discussed in the paper.\n- Viewing the method as a form of regularization to overfitting, the proposed approach seems excessively expensive.\n- The authors state that CLAGA “does not rely on unconfoundedness” (lines 113–114), yet CATE estimation fundamentally does. Since CLAGA is presented as a framework for CATE estimation, this statement is misleading.\n- Code is not released, restricting reproducibility."}, "questions": {"value": "- How does the proposed problem relate to work in fairness research which also concerns itself with balancing estimation error across groups? How is it different?\n- If the identified problem is overfitting, how do other regularization strategies that also reduce overfitting at much lower cost perform?\n- How do the authors view their solution wrt asymptotic vs finite sample statistics point of view?\n- Considering the added step regresses towards synthetic labels - how do the authors view their method from the information theoretic point of view which would imply no new information is being extracted from the data?\n- How does the framework perform on newer baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ezl3m27T6V", "forum": "y1N4v2v5Xz", "replyto": "y1N4v2v5Xz", "signatures": ["ICLR.cc/2026/Conference/Submission17876/Reviewer_Xq6g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17876/Reviewer_Xq6g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761427504402, "cdate": 1761427504402, "tmdate": 1762927702427, "mdate": 1762927702427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "gLUk2mqdLT", "forum": "y1N4v2v5Xz", "replyto": "y1N4v2v5Xz", "signatures": ["ICLR.cc/2026/Conference/Submission17876/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17876/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763275128784, "cdate": 1763275128784, "tmdate": 1763275128784, "mdate": 1763275128784, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a metric for evaluating CATE estimators.\n\nDespite many years of experience in developing CATE estimators, I couldn't make sense of the property the metric is attempting to evaluate (line 191) or the justification given in Section 5.1. Perhaps there's something fundamental that I'm missing. I'm very curious to hear what the other reviewers thought and am looking forward to engaging with the authors during the rebuttal period to see if there's something that I'm missing."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Its merits aside, the proposed metric seems to be novel."}, "weaknesses": {"value": "I don't understand what line 191 means. If you condition on $X=x$, then the value of $W$ doesn't matter. So I understand line 191 to mean that you're taking an expectation with respect to the training data, and only the training data.\n\nThe proposed discrepancy measure seems something about the overfitting of the estimators under a strong null hypothesis of both (i) treatment randomized independently of covariates and (ii) no distributional treatment effect. I don't understand why this setting would be of interest, as it (i) imposes conditions beyond those required of most CATE estimators and (ii) imposes conditions on higher-order moments of the counterfactual distributions that have nothing to do with the CATE."}, "questions": {"value": "Minor comment on references on line 47:\n* R-learner was first proposed in Corollary~9.1 of\nRobins, James M. \"Optimal structural nested models for optimal sequential decisions.\" Proceedings of the Second Seattle Symposium in Biostatistics: analysis of correlated data. New York, NY: Springer New York, 2004.\n* DR-learner was proposed in Section 3.1 of\nvan der Laan, Mark J. \"Targeted Learning of an Optimal Dynamic Treatment, and Statistical Inference for its Mean Outcome.\" (2013)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "erfla2kDcP", "forum": "y1N4v2v5Xz", "replyto": "y1N4v2v5Xz", "signatures": ["ICLR.cc/2026/Conference/Submission17876/Reviewer_Af5n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17876/Reviewer_Af5n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935041162, "cdate": 1761935041162, "tmdate": 1762927701804, "mdate": 1762927701804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers an issue in CATE estimation: many algorithms exhibit inconsistent learning behavior for the same instance across different group assignments. \nNext, the paper argues that the inconsistency indeed contributes to higher test errors and cannot be resolved through conventional machine learning techniques. To address this problem, the paper proposes a new algorithm."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper provides simulation and real-world studies."}, "weaknesses": {"value": "1. The paper's theoretical foundation is excessively simple, with key arguments' validity contingent upon vague definitions and assumptions. If the authors claim \"theoretical analysis\", they must rigorously align with established literature in the field. For instance, the paper's use of \"consistency\" appears misapplied.\n\n2. The manuscript seems preoccupied with the issue of unbiased estimators. It is overlooked that even biased estimators may converge to the true parameter as sample size increases. Given large samples prevalent in contemporary applications, large-sample theory would potentially justify such estimators.\n\n3. Extensive literature on causal inference using machine learning methods has demonstrated that naive applications invariably fail, whereas strategic modifications can yield suitable estimators. In that, the finding in the paper is not new. The existing studies have substantially gone beyond primitive mean-variance analysis, rendering the paper's simplistic derivations seemingly disconnected from these methodological advances."}, "questions": {"value": "Please find the items in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dr1oXXURyB", "forum": "y1N4v2v5Xz", "replyto": "y1N4v2v5Xz", "signatures": ["ICLR.cc/2026/Conference/Submission17876/Reviewer_PipY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17876/Reviewer_PipY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762171169665, "cdate": 1762171169665, "tmdate": 1762927701255, "mdate": 1762927701255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes new objective when estimating CATE to solve the problem of inconsistent learning behavior for different treatment groups. The paper proposes a decomposition of PEHE to motivate the method and proposes a new algorithm. The paper concludes with experiments showing the effectiveness of the algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Originality: I like the idea and I think the decomposition and the extra layer of thinking on estimating CATE with PEHE as loss is quite original. \n\nQuality: The paper has good theoretical justification, comprehensive experiments and clear algorithm descriptions. \n\nSignificance: I believe this consistency among group property is quite important in some applications."}, "weaknesses": {"value": "I like the paper but I think the main weakness is the clarity of the presentation and the experiments. Details below:\n\nClarity: \n1. line 191: I found the notation quite confusing. $\\hat{\\tau}(x)$ is usually a deterministic quantity with lower case $x$. \n2. Similar for line 237, the notation is quite confusing to me. \n3. Line 323: this is quite confusing, what is well-trained and what is well-designed. \n4. line 372 to 3745: still quite confusing. In particular I don't see how these different $\\tau$'s come up. I get what you are trying to convey here though.\n\n\nExperiments:\n1. The discrepancy ratio is cool but isn't that testing tons of hypothesis? Maybe you should use a lower p-value. \n2. How do you calculate PEHE in your experiments? This is quite trivial but I think at least should be mentioned. \n3. In table 1, why do the numbers in 2016 >> 2018?\n4. Also in table 1, I don't quite get why you use the ratio as a metric to compare before and after. \n5. Figure 3 does not have sd bars. \n6. In Table 2, why the first dataset is way worse?"}, "questions": {"value": "1. For figure 1(c): your method reduces errors unrelated to the training process, which basically says everything else the same, using CLAGA should be better than say using DR-learner. But why there is a much smaller almost horizontal line for CLAGA in figure 1, which from figure 3 is not necessarily true? I guess my confusion comes from how is this method solving the problem of figure 1(b) and (c)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cRRK2cqTNt", "forum": "y1N4v2v5Xz", "replyto": "y1N4v2v5Xz", "signatures": ["ICLR.cc/2026/Conference/Submission17876/Reviewer_2K6c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17876/Reviewer_2K6c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762234734325, "cdate": 1762234734325, "tmdate": 1762927700625, "mdate": 1762927700625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}