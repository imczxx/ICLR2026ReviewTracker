{"id": "VrZmga4cIG", "number": 20044, "cdate": 1758301823080, "mdate": 1759897004488, "content": {"title": "On the Capacity of Self-Attention", "abstract": "While self-attention is known to learn relations among tokens, we lack a formal understanding of its capacity: how many distinct relations can a single layer reliably recover for a given budget?\nTo formalize this, we introduce Relational Graph Recognition (RGR), where the key-query channel represents a graph on $m$ items with $m'$ directed edges, and, given a context of items, must recover the neighbors of each item. We measure resources by the \\emph{total key dimension} $D_K = h\\,d_k$. \n\nWithin this framework, we analytically derive a capacity scaling law and validate it empirically. We show that $D_K = \\Theta(m' \\log m' / d_{\\text{model}})$ is both necessary (information-theoretic lower bound) and sufficient (explicit construction) in a broad class of graphs to recover $m'$ relations. This scaling law directly leads to a new, capacity-based rationale for multi-head attention that applies even when each item only attends to a single target.  When embeddings are uncompressed ($m = d_{\\text{model}}$) and the graph is a permutation, a single head suffices.\nHowever, compression ($m > d_{\\text{model}}$) forces relations into overlapping subspaces, creating interference that a single large head cannot disentangle. Our analysis shows that allocating a fixed $D_K$ across many small heads mitigates this interference, increasing the number of recoverable relations.  Controlled single-layer experiments mirror the theory, revealing a sharp performance threshold that matches the predicted capacity scaling and confirms the benefit of distributing $D_K$ across multiple heads.\n\nAltogether, these results provide a concrete scaling law for self-attention capacity and a principled design rule for allocating key-query budget across heads.", "tldr": "We derive and empirically validate a scaling law for self-attention's capacity to recognize relationships, revealing a new rationale for using multiple attention heads, even when each item attends to only a single target.", "keywords": ["Self-Attention", "Transformers", "Capacity", "Scaling Laws", "Multi-Head Attention", "Information Theory", "Mechanistic Interpretability", "Deep Learning Theory", "Superposition."], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fb790eb4fbc62d9f51c3a92e0714c8fa2794f6d4.pdf", "supplementary_material": "/attachment/5b89ab747dd6ccb2715bd0eec6c642a10191c140.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies the capacity of self attention by considering the number of distinct relations a model can recover for a given budget (total query dimension). This is mathematically formalised by a ‘relational graph recognition’ (RGR) problem, where the task is to predict a node’s in-context neighbours (adjacent in the graph and with the same context label). For a given class of graphs, the authors ask: what is the minimum query dimension to get the RGR mapping for all graphs and all contexts? This is claimed to be a sensible abstraction for modelling how the modelling (c.f. memorisation, studied in previous work) capacity of attention varies with $d$."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. I broadly buy that finding ways to quantify the modelling capacity of attention is an interesting research direction, and the theoretical results seem plausible. \n2. The analysis of multi-head attention provides an interesting perspective on why introducing heads can help despite a fixed total head dimension."}, "weaknesses": {"value": "1. The paper is clearly in violation of the formatting rules – the margins are _way_ smaller than they ought to be. My understanding is that this technically ought to lead to desk rejection. Of course, I defer to the AC on this point. \n2. Whilst the theoretical results are plausible and Figs 1 and 2 verify that they hold in your idealised experimental setup, it would be helpful to see to what extent they hold/break down in more general settings – for more general graphs, using softmax, with unnormalised node embeddings. Even if the theory is intractable it would be good to get a sense of to what extent the empirical relationships still hold.\n3. As a broader qualm, it’s not totally clear how this construction relates to observable properties of transformers. I very much appreciate rigorous, theoretical work, but think the arguments would be more convincing if the results could be related (even in a toyish setting) to actual observable behaviour in trained models on real data."}, "questions": {"value": "See above. \n\n1. To what extent will your empirical findings hold when you relax the assumptions? (A qualitative answer is fine, but imo you need some kind of answer)\n2. Can you relate e.g. your scaling laws to any observable behaviour noted in transformers trained on real data -- either from your own experiments or from the literature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6dS4wnypsf", "forum": "VrZmga4cIG", "replyto": "VrZmga4cIG", "signatures": ["ICLR.cc/2026/Conference/Submission20044/Reviewer_TsWQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20044/Reviewer_TsWQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760711908458, "cdate": 1760711908458, "tmdate": 1762932940897, "mdate": 1762932940897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a fundamental question about self-attention mechanisms, given a computational budget, how many distinct relationships can a single attention layer reliable represent and recover? The authors formalize this through the Relational Graph Recognition framework, where attention must recover the edges of a directed graph among $m$ items $m'$ edges.\n\nThe key contributions are:\n\n* Theoretical Bounds: The paper derives both information-theoretic lower bounds and constructive upper bounds for the minimum total key dimension D_K = h·d_k (number of heads × per-head key dimension) required to solve RGR. For most graph families, these bounds match up to logarithmic factors: D_K = Θ(m'log(m)/d_model).\n\n* Multi-Head Advantage: The analysis reveals why multiple attention heads are beneficial even when each item only attends to a single target. When embeddings are compressed (d_model < m), relationships become superposed in overlapping subspaces. Multiple heads reduce interference by specializing on disjoint subsets of relationships.\n\n* Explicit Constructions: The paper provides concrete algorithms showing how to construct attention weights that achieve the theoretical bounds for various embedding types (one-hot, Gaussian unit-norm, sparse binary).\n\n* Empirical Validation: Experiments on permutation graphs show sharp phase transitions in performance as D_K increases, with the empirical threshold closely matching the theoretical prediction (R² = 0.944-0.992).\n\nThe work focuses on an idealized attention model that isolates the key-query computation while omitting components like softmax and the value pathway. This simplification enables precise mathematical analysis while preserving the essential computational constraints of attention."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper provides both information-theoretic lower bounds and constructive upper bounds for self-attention capacity, with the bounds matching up to logarithmic factors in many cases. This is a significant theoretical contribution.\n\n* The Relational Graph Recognition (RGR) framework is a clever abstraction that isolates the key-query mechanism while preserving the essential properties of self-attention. This allows for precise mathematical analysis.\n\n* The experimental results closely match the theoretical predictions (R² = 0.944-0.992), providing strong evidence for the theory. The sharp phase transitions in performance as a function of D_K are particularly compelling.\n\n* The paper provides a principled explanation for why multi-head attention is beneficial even when each item only attends to a single target - multiple heads reduce interference from superposition in compressed embeddings.\n\n* The paper covers multiple embedding types (one-hot, Gaussian unit-norm, sparse binary) and extends from permutations to general graphs."}, "weaknesses": {"value": "* The model omits several key components of real attention (softmax, value pathway). Furthermore, the OR of heads aggregation via max is not how standard multi-head attention works\n* The degree-skew condition limits the tightness of bounds for certain graph families"}, "questions": {"value": "* How well do you expect these results to transfer to actual transformer architectures with all their components (softmax, values, layer norm, etc.)? Have you conducted any preliminary experiments with more realistic attention mechanisms?\n* The analysis assumes fixed random embeddings. How would the capacit scaling change if embeddings could be learned to optimize for the graph structure? Could learned embeddings achieve better compression than random ones?\n* What about the computational complexity of actually learning these attention patterns? Are there fundamental optimization challenges when operating near capacity?\n* How can we inform ourselves based on the theory in the paper when designing real models containing attention? How should we pick the hyperparameters such as number of heads and head dimension etc.?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8Fa6zOdKfy", "forum": "VrZmga4cIG", "replyto": "VrZmga4cIG", "signatures": ["ICLR.cc/2026/Conference/Submission20044/Reviewer_d1fH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20044/Reviewer_d1fH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761295850958, "cdate": 1761295850958, "tmdate": 1762932940391, "mdate": 1762932940391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a formal framework, Relational Graph Recognition (RGR), to analyze the \"capacity\" of a single self-attention layer. Capacity is defined as the number of distinct relations ($m'$) that a layer can reliably recover. The paper's main theoretical contribution is a capacity scaling law, $D_K = \\Theta(m' \\log m' / d_{model})$, where $D_K = h \\cdot d_k$ is the total key dimension (number of heads times per-head key dimension). This analysis provides a new rationale for multi-head attention: when embeddings are compressed ($m > d_{model}$), distributing the $D_K$ budget across multiple heads mitigates interference and increases capacity. The paper provides an information-theoretic lower bound and a constructive upper bound to establish this scaling law, and validates it with experiments on synthetic data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Novel Formalization:** The paper provides a novel and interesting perspective on attention capacity. The RGR framework and the $D_K$ (total key dimension) metric are a clear and useful way to formalize the problem of relational storage in the QK-circuit.\n2.  **Rigorous Theoretical Analysis:** Within its stated assumptions, the theoretical analysis is comprehensive and sound. The authors provide both a lower bound (necessity) and an explicit construction (sufficiency) to derive their scaling law, which is a complete line of theoretical argument.\n3.  **New Rationale for Multi-Head Attention:** The explanation that multiple heads help mitigate interference from *embedding compression* (rather than just attending to different things) is an elegant and intuitive contribution to the understanding of MHA."}, "weaknesses": {"value": "1.  **Over-simplification (Omission of Softmax):** The most significant weakness is the decision to model attention using a global threshold on max-pooled scores, thereby completely omitting the softmax function. This simplification is not minor; it fundamentally changes the mechanism's behavior. Softmax introduces dynamic, context-dependent competition among tokens. The paper's static, non-competitive model is a very different mechanism.\n2.  **Lack of Relevance to LLMs:** Because of this simplification, the derived theoretical bounds are almost meaningless for practical LLM research. A large body of work on \"linear attention\" (softmax-free models) has repeatedly shown that while these models are efficient, they suffer from severe performance degradation precisely because they lack the competitive, non-linear dynamics of softmax. This paper's theory applies to a class of models that are known to be less powerful in practice.\n3.  **Purely \"Simulation Validation\":** The experimental setup does not test the theory's applicability; it merely validates the theory *within its own idealized assumptions*. The experiments are run on synthetic permutation graphs using an idealized model that perfectly matches the theoretical construction (no softmax, no OV circuit). The successful validation is therefore almost a foregone conclusion and feels more like a \"simulation\" of the theory than an empirical validation of its relevance.\n4.  **No Real-Data Experiments:** The paper lacks *any* experiments on real-world data, even a simple task. This is a critical omission for a deep learning conference. Without evidence that these findings hold (or even correlate) on a real dataset, the paper reads more like a mathematical analysis report on a simplified algorithm than a deep learning paper with practical implications. It is crucial to demonstrate that this theoretical capacity is the *relevant* capacity for tasks we care about."}, "questions": {"value": "1.  The core critique lies in the removal of softmax. How do the authors believe their $D_K$ bounds would change in the presence of softmax-induced competition? Would the \"interference\" from embedding compression still be the dominant factor, or would the context-dependent competition render these bounds inapplicable?\n2.  Given the well-documented performance drop of linear (softmax-free) attention models, it suggests their \"capacity\" is insufficient for complex tasks like language modeling. How does the paper's definition of capacity (recovering graph edges) relate to the functional capacity required for such tasks?\n3.  Why did the authors not attempt even a preliminary experiment on a real-world dataset (e.g., WikiText) to show that their $D_K$ metric correlates with downstream performance or that the multi-head advantage manifests as predicted? This would significantly strengthen the paper's claims of relevance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qkZ0hbuf29", "forum": "VrZmga4cIG", "replyto": "VrZmga4cIG", "signatures": ["ICLR.cc/2026/Conference/Submission20044/Reviewer_B9jt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20044/Reviewer_B9jt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649224419, "cdate": 1761649224419, "tmdate": 1762932939974, "mdate": 1762932939974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a formal theoretical analysis of the *capacity* of self-attention layers—the number of distinct token-to-token relations that a single attention layer can reliably represent, given a fixed parameter budget. To formalize this, the authors introduce the **Relational Graph Recognition (RGR)** framework, where the key–query channel is viewed as encoding a directed graph ( G = (V, E) ) among ( m ) items. The capacity problem is then cast as determining the minimal total key dimension required to recover all ( m' ) relations. The paper provides the first quantitative scaling law linking relational capacity, embedding dimension, and number of heads in self-attention."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The RGR formulation is novel and mathematically elegant, bridging mechanistic interpretability and information-theoretic perspectives. Unlike prior “memorization capacity” or “expressivity” analyses, RGR isolates *where* attention attends (the QK channel) rather than *what* it writes (the OV path), leading to fundamentally new capacity laws. The theoretical connection between *multi-head specialization* and *noise reduction under compression* is an insightful reinterpretation of the design rationale for multi-head attention.\n\n\n* The paper provides both **lower bounds** (information-theoretic) and **constructive upper bounds** (algorithmic proofs) that nearly match, supported by precise asymptotics and concentration arguments.\n* Empirical experiments are well-controlled and directly aligned with the theoretical model (frozen embeddings, synthetic graphs, fixed thresholds), validating the scaling laws quantitatively (( R^2 > 0.99 ))."}, "weaknesses": {"value": "* There is no equation number for all equations.\n\n* Experiments are restricted to synthetic permutation graphs (out-degree = 1) and omit evaluation on multi-edge graphs (( \\Delta > 1 )). Testing richer relational structures or real transformer layers would strengthen the empirical credibility.\n\n* The omission of softmax normalization and the value pathway isolates QK capacity but makes the model somewhat detached from actual transformer dynamics. A discussion of whether the same scaling persists under full attention (with softmax) would be valuable. Or experiments validate the conclusion with softmax."}, "questions": {"value": "* How is the directed graph built? Full attention allows tokens to attend to all other tokens so it should be a complete graph which is very dense. But this could add a gap of logm according to the paper. So how will this affect the application of the theory in real-world tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fUV9G93Ije", "forum": "VrZmga4cIG", "replyto": "VrZmga4cIG", "signatures": ["ICLR.cc/2026/Conference/Submission20044/Reviewer_EVn9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20044/Reviewer_EVn9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664141477, "cdate": 1761664141477, "tmdate": 1762932939396, "mdate": 1762932939396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}