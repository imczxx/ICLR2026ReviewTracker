{"id": "2McprQZ1Dt", "number": 20739, "cdate": 1758309535166, "mdate": 1763679932839, "content": {"title": "Adversarial Arena: Crowdsourcing Data Generation through Interactive Competition", "abstract": "Post-training Large Language Models requires diverse, high-quality data which is rare and costly to obtain, especially in low resource domains and for multi-turn conversations. Common solutions are crowdsourcing or synthetic generation, but both often yield low-quality or low-diversity data. We introduce Adversarial Arena for building high quality conversational datasets by framing data generation as an adversarial task: attackers create prompts, and defenders generate responses. This interactive competition between multiple teams naturally produces diverse and complex data. We validated this approach by conducting a competition with 10 academic teams from top US and European universities, each building attacker or defender bots. The competition, focused on safety alignment of LLMs in cybersecurity, generated 19,683 multi-turn conversations. Fine-tuning an open-source model on this dataset produced an 18.47\\% improvement in secure code generation on CyberSecEval-Instruct and 29.42\\% improvement on CyberSecEval-MITRE.", "tldr": "A framework for building high quality conversational datasets by framing data generation as an adversarial task: attackers create prompts, and defenders generate responses.", "keywords": ["synthetic data", "croudsourcing data", "adversarial arena", "data diversity"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b8e6de45d6467abeede97016c381a7526c2cf7a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces **Adversarial Arena**, a public platform for generating high-quality, diverse synthetic data through structured, multi-turn adversarial competitions between independent teams of attackers and defenders. The core idea is to frame data generation as a competitive task: attackers attempt to elicit failures (e.g., generating unsafe code), while defenders aim to produce robust, correct responses. This interactive process, orchestrated over multiple tournament rounds, naturally produces complex, multi-turn conversational data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The paper proposes **Adversarial Arena**, a public competition platform capable of generating high-quality data, and clearly elaborates on various details regarding the platform's construction, operation, and data generation process.\n* Based on this public competition platform, the paper constructs a dataset for cybersecurity alignment, and experiments demonstrate the promising fine-tuning performance achieved using this dataset."}, "weaknesses": {"value": "* The academic contribution of this paper is limited. The authors primarily emphasize the promising fine-tuning performance of their constructed dataset for safety alignment. However, they neither propose a new challenge (e.g., novel problems or paradigms within the safety alignment domain) nor deliberately construct data to address a specific, existing problem. While diversity is emphasized, the authors fail to provide a detailed explanation of what specific diversities were achieved. Furthermore, the authors act primarily as platform builders; the contributions of the participants and the details of their solutions lack systematic elaboration and summarization.\n*   The experiments in this paper are insufficient. For instance, the fine-tuning experiment in Table 2 lacks baseline comparisons. It would be crucial to compare against recent or classic datasets for cybersecurity alignment to understand how fine-tuning on those datasets impacts a model's resilience to attacks.\n*   The analysis of data diversity is somewhat superficial. Relying solely on t-SNE visualizations or embedding similarities to measure diversity and bias cannot effectively reveal novel problems or paradigms. (Even the same security issue, when phrased differently, can lead to significant shifts in vector representations). The authors should focus more on summarizing what new security issues or novel attack paradigms emerged from the competitive interactions among participants on their platform."}, "questions": {"value": "Consistent with the Weaknesses section, I do not recommend the authors to proceed with a rebuttal. If they choose to do so, please just provide targeted responses addressing the points raised in the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uiQx45VL9f", "forum": "2McprQZ1Dt", "replyto": "2McprQZ1Dt", "signatures": ["ICLR.cc/2026/Conference/Submission20739/Reviewer_q1j2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20739/Reviewer_q1j2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761457601909, "cdate": 1761457601909, "tmdate": 1762934159033, "mdate": 1762934159033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Adversarial Arena, a framework for crowdsourcing high-quality and diverse LLM training data through adversarial competitions between teams. The key contributions are:\n\n1. Adversarial Arena, a general framework where \"defenders\" aim to make models perform well on tasks of interest while \"attackers\" try to elicit failures, with multiple tournaments allowing iterative improvement\n2. An instantiation of this framework for cybersecurity tasks with 10 university teams (5 attackers, 5 defenders) competing over 4 rounds\n3. A resulting dataset of ~19k labeled multi-turn conversations that, when used for fine-tuning, improves secure code generation by 18% and cyberattack assistance refusal by 24% (on CyberSecEval benchmarks)"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Interesting approach to data collection**: The adversarial competition framework presents an intriguing method for obtaining training data. While similar competition-approaches have been used before, the systematic application to data collection beyond adversarial tasks is novel.\n2. **Good case-study design**: Given the complexity of organizing and orchestrating a multi-team competition, the case-study itself is a strong contribution. In particular, the authors also provide an orchestrator for such competitions with good documentation.\n3. **Successful empirical validation**: The case study demonstrates practical effectiveness, with the resulting dataset yielding meaningful improvements on the cybersecurity task of interests. The validation methodology through fine-tuning experiments is sound, and the approach to testing diversity via semantic alignment is reasonable.\n4. **Good presentation**: The paper is well-organized with helpful visualizations (Figure 3) and generally clear. The framework's instantiation for cybersecurity (Section 4), except for minor parts of the setup, is documented in depth."}, "weaknesses": {"value": "**Main points**\n\n1. **Lack of truly adaptive attacks**: The most significant limitation is that attackers and defenders interact simultaneously rather than sequentially. The paper mentions this limitation on L428; however, it only mentions either fully online settings or more frequent tournaments as a solution. In practice, defenders must commit first, allowing attackers to adapt. Hence, a more realistic approach would be turn-based, such that attackers get access to all defenses before submitting an attack strategy. The current framework hence artifically limits attackers (and thereby data quality), and it is not clear to me if the orchestrator can easily be adapted to such a turn-based setting.\n2. **Scalability concerns due to manual labor**: The approach requires substantial effort from many participants (e.g., 10 academic teams in the case-study). It's unclear whether this scales to the diverse set of tasks current LLMs have to support, thus limiting the practical impact of Adversarial Arena. For example, one big competition might see a lot of participation from a diverse set of teams, but multiple weekly competitions that span many tasks might see quickly diminishing interest.\n\n**Minor issues**\n\n3. **High-level framework with limited novelty**: The framework itself (Section 3) is relatively abstract; it could be helpful to make it more prescriptive. The paper's significant contribution, in my opinion, is more the instantiation of this framework on cybersecurity tasks and the resulting dataset/code, not necessarily the high-level approach. In particular, using an attacker-defender setting to obtain data or improvements has been explored before (e.g., [Bartolo et al., 2020](https://arxiv.org/abs/2002.00293), [Debenedetti et al., 2024](https://arxiv.org/abs/2406.07954), or the [Generative AI Red-Teaming Challenge](https://humane-intelligence.org/get-involved/events/defcon-2023-overview/)). The core idea of instantiating a general adversarial framework including *both* attackers and defenders is still novel and has, to the best of my knowledge, not been done before. Nevertheless, I think contextualization with existing approaches would be appropriate.\n4. **Diversity measurement limitations**: The diversity penalty for the case-study (L298-L307) captures only lexical diversity through BLEU scores, not semantic diversity of attack strategies. While Section 4.3 analyzes semantic diversity post-hoc, this isn't incorporated into tournament scoring; doing so would potentially better align incentives.\n5. **Mixed task evaluation**: The case study combines two distinct tasks (eliciting vulnerable code and cyberattack assistance), leading to attackers focusing on the easier target (see e.g., the last paragraph of Section 4.4). Separate tournaments or independent grading per task could avoid such exploitation.\n6. **Limited orthogonality testing**: Lines 182-184 emphasize testing utility on orthogonal tasks, but evaluations only examine related coding/cybersecurity tasks rather than truly orthogonal capabilities like general knowledge or reasoning. Even if the model should only work for coding tasks, I believe defenses should still be evaluated on general-purpose coding questions truly orthogonal to cybersecurity.\n7. **Missing details of case-study setup**: While the evaluations of the case-study are described in detail, some points about the setup were unclear to me; see questions."}, "questions": {"value": "1. Do the authors plan to publicly release the collected dataset and orchestration code? This would significantly enhance the contribution's impact.\n2. Can the framework be modified to support turn-based interactions where defenders release their defenses first, allowing attackers to adapt? This would better reflect real-world scenarios and potentially yield stronger attacks (and hence better data).\n3. Were defender teams allowed to fine-tune the ChallengeLLM, or were they restricted to prompting and auxiliary models?\n4. Is the 45-second latency budget (L244) per conversation, per completion, or aggregated differently? Does this constraint apply to both attackers and defenders?\n5. Does each tournament consist of single or multiple conversations per attacker-defender matchup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0HBtrhwyAf", "forum": "2McprQZ1Dt", "replyto": "2McprQZ1Dt", "signatures": ["ICLR.cc/2026/Conference/Submission20739/Reviewer_gzkS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20739/Reviewer_gzkS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575754694, "cdate": 1761575754694, "tmdate": 1762934157875, "mdate": 1762934157875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Adversarial Arena, a framework for crowdsourcing high-quality conversational datasets through structured adversarial competitions between “attacker” and “defender” teams. Each attacker attempts to elicit failures from defender models, while defenders aim to respond safely and effectively. The system reportedly produces diverse multi-turn datasets, demonstrated on a cybersecurity alignment task involving 10 university teams and ~19k conversations. Fine-tuning an open-weight model on the resulting data improves secure code generation and safety benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Creative Framework: The notion of gamifying data generation through an attacker-defender structure is intuitively appealing and could inspire future collaborative or competitive data collection paradigms.\n2. Scale and Engineering: The authors managed to coordinate 10 research teams and generate ~20k labeled dialogues, a notable engineering effort demonstrating feasibility at scale.\n3. Empirical Evidence: The fine-tuning results (18.47% and 29.42% gains on security benchmarks) empirically confirm that the generated dataset is at least useful for improving safety-aligned code generation."}, "weaknesses": {"value": "1. Domain Dependence.  The framework is tightly coupled to the cybersecurity task and depends heavily on fixed evaluation pipelines and manual annotation templates. The system’s success metrics rely on specific types of code vulnerabilities, which may not generalize to other domains like dialogue safety, factuality, or reasoning. The “attacker–defender” framing works mainly because cybersecurity naturally lends itself to adversarial setups; its general applicability remains unconvincing.\n3. Weak Signal and Limited Generalization. The feedback signal for guiding attackers and defenders is simplistic and weak. It does not ensure meaningful improvement across rounds beyond superficial variation. While the paper emphasizes “diversity” and “richness,” it lacks concrete evidence that such signals lead to systematically better or more representative data. Furthermore, improvements are shown only for one fine-tuning case without ablation or analysis of data quality vs. quantity effects, limiting confidence in broader generalization."}, "questions": {"value": "Can this idea apply to other domains for data collection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "940Sy1qk9y", "forum": "2McprQZ1Dt", "replyto": "2McprQZ1Dt", "signatures": ["ICLR.cc/2026/Conference/Submission20739/Reviewer_PTzc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20739/Reviewer_PTzc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964747304, "cdate": 1761964747304, "tmdate": 1762934156558, "mdate": 1762934156558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Adversarial Arena, a tournament-style framework that crowdsources multi-turn conversational data by pairing “attackers” (prompt generators) against “defenders” (LLMs/agentic systems) and uses an automated evaluator to label each conversation as attack/defense success. A real-world case study on cybersecurity alignment involved 10 university teams (5 attackers, 5 defenders) across multiple tournaments, yielding 19,683 labeled conversations. The authors define ranking incentives (e.g., normalized ASR with a diversity multiplier; utility-aware defense score), show semantic diversity across teams/rounds (t-SNE & cosine-based analyses), and report that fine-tuning Mistral-7B-Instruct on curated subsets improves secure code generation (+18.47% on CyberSecEval-Instruct) and refusal of malicious cyberactivity (+29.42% on CyberSecEval-MITRE). The framework is backed by a serverless orchestrator (AWS Lambda/SQS/DynamoDB) to run large asynchronous, multi-turn matches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Operational, repeatable framework for large-scale, multi-turn adversarial data creation with clear incentive design (normalized ASR, utility-aware defense scoring).\nDemonstrated scale & impact: ~20k labeled dialogs; measurable gains on CyberSecEval-Instruct/MITRE after SFT on curated subsets.\nDiversity evidence: attacker/defender/tournament-level separation via cosine distances and t-SNE plots; qualitative differences across teams and rounds.\nRobust engineering: serverless orchestrator (Lambda/SQS/DynamoDB) enabling asynchronous, fault-tolerant multi-turn matches and batching, with explicit guarantees/trade-offs.\nTransparency about limitations (imperfect evaluators; incentive timing; vulnerable-code vs malicious-intent skew) and concrete mitigations."}, "weaknesses": {"value": "Evaluator dependence / label noise: Reliance on a single static analyzer (CodeGuru) risks false positives/negatives; human labeling process lacks detailed inter-annotator agreement (IAA) stats and calibration analysis. A small dynamic analysis slice or cross-tool ensemble would strengthen claims.\nDiversity metric choice: BLEU captures lexical variety but can miss strategy-level novelty; the paper mentions considering embeddings, yet final ranking uses BLEU—an ablation comparing both (and their effect on attacker behavior) is missing.\nUtility normalization and trade-offs: Defenders’ scores are aggressively penalized by utility drops, but details on the utility test construction, coverage, and ceiling effects (capping at base model) could use deeper analysis and sensitivity checks.\nGeneralization beyond cybersecurity: While the Discussion argues generality, only one ToI is empirically validated; even a small second domain (e.g., hallucination reduction or refusal over-agreement) would bolster generality claims.\nOutcome attribution: The SFT improvements are compelling, but a breakdown by data slice (attacker team, tournament round, conversation length/turns, vulnerability type) would clarify which arena features drive the gains."}, "questions": {"value": "Evaluator robustness. Can you report IAA (e.g., Cohen’s κ) and disagreement resolution for the 3-annotator panel? Any spot-checks comparing CodeGuru with a second static analyzer or dynamic tests on a held-out subset?\nDiversity incentive. Why finalize on BLEU over embedding-based diversity for the ranking signal? Provide an ablation where attacker rankings and dataset properties are recomputed with an embedding-based metric.\nSFT details. Share fine-tuning hyperparameters, data filtering (e.g., max turns/code length), and a per-slice contribution analysis (by attacker/defender/tournament). Do longer multi-turn dialogs help more?\nUtility suites. Describe construction, difficulty, and overlap with tournament dialogs; include sensitivity of defender rankings to different utility weightings or removing the cap at base model.\nAttack coverage balance. You note skew toward vulnerable-code attacks; did you trial multi-objective scoring (e.g., harmonic mean across “malicious-intent” vs “vulnerable-code” successes) and observe behavior changes?\nGeneralization. Any preliminary runs on a second ToI (e.g., hallucination or sycophancy) to show the arena transfers with minimal changes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hmzEyX2agu", "forum": "2McprQZ1Dt", "replyto": "2McprQZ1Dt", "signatures": ["ICLR.cc/2026/Conference/Submission20739/Reviewer_Ndcg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20739/Reviewer_Ndcg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762126223021, "cdate": 1762126223021, "tmdate": 1762934155702, "mdate": 1762934155702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal Summary"}, "comment": {"value": "We are deeply grateful to the reviewers for their effort and time spent in reviewing our work and greatly appreciate all the feedback. We thank the reviewers for noting the following strengths of our paper:\n\n1. Novel and creative framework that frames data collection as an adversarial task between multiple participants (attackers and defenders).\n2. Robust and scalable framework with clear incentive design\n3. Empirical evidence by fine-tuning on collected data and diversity analysis on that data.\n4. Design of the case study on Cybersecurity Alignment.\n5. Presentation and transparency about limitations\n\n\nWe would like to re-emphasize that this is a general adversarial framework that can be instantiated to collect diverse multi-turn training data for different use cases. Pitting each attacker and defender against multiple other systems that conduct multi-turn conversations provides a feedback signal that is much closer to real-world feedback, unlike the usual approach of iterating over static benchmarks. This leads to models getting more robust over the course of the challenge and the generated data getting closer to real-world scenarios.\n\nWe demonstrate an instantiation of the framework in a case study for cybersecurity alignment, and our experiments show that it indeed produces data that's better for model alignment and more diverse than the data produced by individual groups.\n\nThe framework allows the use of any evaluation mechanism and hence can be used to incentivize objectives like generating novel attacks, balancing different tasks using harmonic mean, etc. Our case study provides a simple example of how such metrics can be designed using BLEU scores for diversity and score averaging to balance different tasks. Our intention behind providing this example is to demonstrate that even with relatively simple metrics, our framework can generate diverse, high quality, multi-turn data."}}, "id": "EEtwB7bVJY", "forum": "2McprQZ1Dt", "replyto": "2McprQZ1Dt", "signatures": ["ICLR.cc/2026/Conference/Submission20739/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20739/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission20739/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763679761235, "cdate": 1763679761235, "tmdate": 1763679761235, "mdate": 1763679761235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}