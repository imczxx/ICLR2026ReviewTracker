{"id": "aQLqieZC5r", "number": 3958, "cdate": 1757573621755, "mdate": 1759898060984, "content": {"title": "AME: ALIGNED MANIFOLD ENTROPY FOR ROBUST UNSUPERVISED VISION-LANGUAGE DISTILLATION", "abstract": "Unsupervised knowledge distillation is a long-established technique for knowledge transfer, and has regained attention in the emergence of pre-trained vision-language models (VLMs). However, its objectives, i.e, probability distributions, are inherently scalar and directionless, which represent a sharp contrast to the similarity-based objectives employed in vision–language model training. As a result, the unsupervised distillation paradigm fails to impose sufficient cross-modal alignment, such alignment is essential for generalization in vision–language knowledge distillation. To address this major challenge arising from the representation misalignment, we propose Aligned Manifold Entropy (AME) for robust unsupervised vision-language distillation (AME), aiming to achieve robust generalization in vision-language distillation tasks. Specifically, AME performs entropy compression over a restructured shared manifold (RSM), where multi-modal inputs (images and texts) are jointly embedded through projection functions. Here, we embed the features into a compact structure through representational compression, which in turn enforces directional alignment within the representation space. Note that AME keep the original backbone architecture without the need for additional modules. Thus, the proposed AME establishes a paradigm that effectively reinstates directional alignment and significantly improve representation convergence in low-data regimes. Extensive experiments and theoretical analysis across a wide range of settings demonstrate that AME is consistently conducive to robust unsupervised knowledge distillation, resulting in superior generalization across 11 datasets. Clearly, AME is a principled paradigm for unsupervised vision-language distillation, which advances it into a broader range of downstream tasks.", "tldr": "", "keywords": ["Vision Language Model", "Unsupervised", "Knowledge Distillation", "Information Entropy"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e79c5a921fd64f26ad0bdaf6980d655567ad968.pdf", "supplementary_material": "/attachment/ca7319c101a5c2ae4d88967fe4275b9ac71a972b.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces Aligned Manifold Entropy (AME), a framework for unsupervised vision-language distillation. The core idea is to apply entropy minimization over a Reconfigured Shared Manifold (RSM) where textual and visual embeddings are jointly projected using lightweight mappings (MLP for text, convolution for image). The authors argue that conventional KL-based distillation lacks directional alignment between modalities and propose AME to restore this property. A theoretical analysis -- based on mutual information and entropy regularization -- is presented to justify improved generalization. Empirically, AME is evaluated on 11 datasets and shows consistent but small gains compared to PromptKD and related baselines, especially under few-shot or low-data regimes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a persistent issue in multi-modal distillation -- directionless scalar objectives -- and connects it with cross-modal misalignment.\n2. The authors provide a high-level conceptual link between entropy regularization and representational alignment, making the method intuitively appealing.\n3. Empirical results cover multiple datasets and tasks, showing consistent if modest improvement in both base-to-new and cross-dataset generalization settings.\n4. The proposed RSM module is lightweight and easy to integrate into existing frameworks."}, "weaknesses": {"value": "1. The main novelty -- adding an entropy regularization term to a reconfigured feature manifold -- is conceptually modest.\nFormally, the proposed loss $L_{total}$=$L_{kd}+\\lambda H(M)$ resembles standard entropy minimization used in semi-supervised and domain adaptation literature. The “aligned manifold” terminology does not correspond to a fundamentally new algorithmic idea; the projection via simple linear/MLP layers followed by entropy compression is functionally equivalent to known low-entropy regularizers. The contribution thus appears incremental.\n\n2. The theoretical sections rephrase common results from information theory without adding substantive rigor. The assumption that minimizing entropy $H(M)$ increases conditional mutual information $I(W;V∣S)$ is asserted but not proven; it requires prerequisites like invertibility of projections that are neither justified nor realistic. The “generalization bound” uses undefined constants $\\delta$, $\\epsilon$ and postulates a reduction without formal derivation or empirical verification. Consequently, the theoretical framework lacks robustness and provides little provable insight beyond intuition.\n\n3. The practical RSM implementation -- concatenating feature projections and computing global entropy -- does not convincingly realize the theoretical narrative of manifold alignment. No quantitative measure (e.g., alignment scores, manifold curvature, inter/intra-class distance) supports the claim that this projection induces geometric consistency. The empirical behavior could simply result from regularization effects rather than true “directional alignment.”\n\n4. Experimental improvements are modest (≈1–3%) and sometimes within the variance of training noise. There is no demonstration that reduced entropy correlates with improved cross-modal consistency or generalization. Visualizations (t-SNE, feature heatmaps) are qualitative and not backed by statistical analysis. The experiments are limited to standard classification tasks; no evaluation on alignment-sensitive tasks (e.g., retrieval, captioning) is provided to substantiate the claimed advantages. Overall, empirical validation is too shallow to confirm the theoretical claims.\n\n5. The objective combines entropy minimization with distillation. This combination might suppress modality-specific variability and harm fine-grained discrimination. The paper does not explore this potential trade-off or provide ablations isolating the role of each component."}, "questions": {"value": "See weaknesses.\n\nAdditional Questions:\n1. The paper claims that minimizing the manifold entropy $H(M)$ increases the conditional mutual information $I(W;V∣S)$, but the proof appears qualitative. Could the authors provide a more formal derivation or an empirical estimation of $I(W;V∣S)$ to support this argument?\n2. The proposed “directional alignment” effect of entropy minimization is mostly illustrated through qualitative visualizations. Can the authors report a quantitative metric, such as cosine similarity alignment scores, inter/intra-class variance, or canonical correlation to substantiate this claim?\n3. The reconfigured shared manifold $M$ is created via simple projection functions before entropy minimization. How can the authors ensure that such a projection does not become a trivial bottleneck or induce over-compression that harms class diversity?\n4. The theoretical generalization bound in Corollary 2 defines $\\delta=H(M)$ and $\\epsilon=I(\\theta; S∣M)$. Have the authors measured or tracked these quantities during training to empirically validate the claimed link between entropy compression and generalization improvement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lIqSZPJM5C", "forum": "aQLqieZC5r", "replyto": "aQLqieZC5r", "signatures": ["ICLR.cc/2026/Conference/Submission3958/Reviewer_HdQh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3958/Reviewer_HdQh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727854299, "cdate": 1761727854299, "tmdate": 1762917113511, "mdate": 1762917113511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Aligned Manifold Entropy (AME) for unsupervised vision–language distillation. Given a pretrained teacher and a lightweight student, AME first projects text and image embeddings into a low‑dimensional Reconfigured Shared Manifold (RSM), then computes per‑row scalar scores ($s_t$) by feature‑wise averaging, converts them to probabilities ($p_t=\\mathrm{softmax}(s)_t$), and minimizes the entropy $(H(M)=-\\sum_t p_t\\log p_t\\)$. The total loss is\n\n$L_{\\text{Total}}=L_{\\text{KD}} + \\omega H(M)$,\n\nwhere $L_{\\text{KD}}=\\tau^2\\,D_{\\mathrm{KL}}\\big(\\sigma(z_t/\\tau)\\,\\Vert\\,\\sigma(z_s/\\tau)\\big)$.\n\nThe theoretical section argues that combining KD with entropy compression yields class‑wise prototype formation and inter‑class margins in RSM.\n\nEmpirically, the method reports modest improvements over PromptKD on Base→New"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- AME is a clean, lightweight add-on that merges information-theoretic entropy minimization with KD in a shared, low-dimensional manifold. The formalization of RSM—\"row-wise concatenation of projected text and image embeddings\", averaging to $s_t$, softmax to $p_t$, and entropy $H(M)$—is an explicit design that could be plugged into other UKD pipelines.\n- The paper motivates that KD’s logit-only supervision may lack directionality in multi-modal alignment and presents AME as an information-theoretic complement. The figures (Heatmaps, t-SNE) visually support the claimed structural effects.\n- AME is backbone-agnostic and \"plug-and-play,\" which is attractive for practitioners who want a minimal change to distillation pipelines."}, "weaknesses": {"value": "(W1) Equation (7) is incorrect / inconsistent.  \nThe paper wants the gradient \"with respect to $s_t$\", but the equation instead shows $\\partial H/\\partial p_t$ and includes a redundant summation:\n\n$\\text{In paper:}\\quad \\frac{\\partial H}{\\partial p_t} = -\\sum_t (1+\\log p_t)$\n\n$\n\\frac{\\partial H}{\\partial p_k}=-(1+\\log p_k),\\qquad \n\\frac{\\partial H}{\\partial s_t} = \\sum_i \\frac{\\partial H}{\\partial p_i}\\frac{\\partial p_i}{\\partial s_t}.\n$\n\nIt might be clearer to revise Eq. (7) and specify whether the derivative is taken with respect to $s_t$ or $p_t$, s the current form could make the subsequent discussion on gradient flow and collapse somewhat ambiguous.\n\n(W2) Theorem 1 margin term $\\zeta$ is used but not defined.  \nThe theorem concludes $\\|\\mathbf h_c-\\mathbf h_{c'}\\|\\ge \\zeta>0$, but there is no explicit definition of $\\zeta$ (norm, dependence on model/data, conditions under which $\\zeta$ exists).\n\n(W3) Limited empirical dominance; small average gains and mixed per-dataset results.  \n– Averages: 16-shot HM: 77.40 → 80.19 (+2.79); full-shot HM: 82.06 → 82.96 (+0.90). Gains are real but not large.  \n– Mixed results: e.g., ImageNet 16-shot HM drops 73.90 → 73.00; SUN397 16-shot HM roughly ties 81.55 ↔ 81.08; UCF101 full-shot HM drops 85.22 → 83.58.  \n\n(W4) No repeatability statistics in tables.  \nAuthors report 3 seeds averaged but give no standard deviations / confidence intervals.\n\n(W5) The weighting $\\omega$ is critical yet lacks ablation / sensitivity and rationale.  \n$\\omega$ controls the trade-off between KD and entropy compression. Authors fix $\\omega=50$, but provide no ablation (e.g., $\\omega\\in\\{0,1,5,10,25,50,100\\}$) nor a justification for 50. This is important because large $\\omega$ should push $p$ toward one-hot (collapse) while small $\\omega$ reduces AME’s effect."}, "questions": {"value": "Please provide responses to the points mentioned in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BRvq2ztEjC", "forum": "aQLqieZC5r", "replyto": "aQLqieZC5r", "signatures": ["ICLR.cc/2026/Conference/Submission3958/Reviewer_o3zJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3958/Reviewer_o3zJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964582377, "cdate": 1761964582377, "tmdate": 1762917112943, "mdate": 1762917112943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a problem in applying Unsupervised Knowledge Distillation (KD) to Vision-Language Models (VLMs) like CLIP. The authors note that CLIP's directional training losses conflict with standard, directionless KD objectives like KL divergence. To resolve this, the paper proposes Aligned Manifold Entropy (AME), which aims to make the student model's image and text representations more compact. This is achieved by minimizing entropy on a Reconfigured Shared Manifold (RSM)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies an interesting and fundamental discrepancy in typical Unsupervised KD applications for CLIP-style models."}, "weaknesses": {"value": "**Lack of Motivation:** The proposed AME loss formulation lacks motivation. The paper explains how the loss is computed (e.g., Eq. 4, Eq. 5) but lacks a clear justification for why this specific formulation is chosen.\n\n**Loss Formulation Concerns:**\n- The loss in Eq (4) does not appear to account for varying scales across different dimensions, which could disproportionately weight certain dimensions.\n- The intuition for minimizing entropy in Eq. (5) is unclear. This objective appears to favor a degenerate distribution (e.g., one highly positive row, with others negative before softmax), and it is not clear why this would improve representation alignment.\n\n**Experimental Details Missing:** The experimental details are insufficient. For example, the training dataset used in section 4.2.2 is not specified.\n\n**Missing Comparisons & Baselines:**\n- Key comparisons to recent and relevant CLIP distillation methods are missing from the discussion and results [1]-[5].\n- The evaluation is incomplete. Standard benchmarks, such as Image Classification in the Wild [6], are missing for Table 2.\n- There are no retrieval results on standard datasets like MS-COCO and Flickr-30k.\n\n**Results Analysis:** Table 2 suggests that the full-shot setting exhibits diminishing returns compared to the few-shot setting. Further analysis with fewer shots would be insightful.\n\n**Clarity:** The writing is generally difficult to follow,.\n\n[1] Yang, C., et al. (2024). Clip-kd: An empirical study of clip model distillation. In CVPR. \n\n[2] Vasu, P. K. A., et al. (2024). Mobileclip: Fast image-text models through multi-modal reinforced training. In CVPR. \n\n[3] Saidutta, Y. M., et al. (2024). CIFD: Controlled Information Flow to Enhance Knowledge Distillation. In NeurIPS. \n\n[4] Sameni, S., et al. (2024). Building vision-language models on solid foundations with masked distillation. In CVPR. \n\n[5] Wu, Kan, et al. \"Tinyclip: Clip distillation via affinity mimicking and weight inheritance.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[6] Li, C., et al. (2022). Elevater: A benchmark and toolkit for evaluating language-augmented visual models. In NeurIPS."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tSb6YE9Hcb", "forum": "aQLqieZC5r", "replyto": "aQLqieZC5r", "signatures": ["ICLR.cc/2026/Conference/Submission3958/Reviewer_s7ZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3958/Reviewer_s7ZV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974990112, "cdate": 1761974990112, "tmdate": 1762917112710, "mdate": 1762917112710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Aligned Manifold Entropy (AME), a framework for robust unsupervised vision–language distillation. Unlike traditional unsupervised knowledge distillation, which relies on scalar and directionless probability distributions, the authors address cross-modal misalignment from a distillation perspective by performing entropy compression on a reconfigured shared manifold (RSM), where image and text embeddings are jointly projected into a compact, directionally aligned space. Without modifying the original backbone, AME enforces geometric consistency between modalities, improving representation convergence and generalization in low-data regimes. Extensive experiments and theoretical analyses show that AME consistently enhances unsupervised VLM distillation, achieving superior performance across 11 datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed AME framework (via the RSM module) can be seamlessly integrated into various existing distillation backbones (e.g., PromptKD, SimpleKD) without altering the underlying architecture. This plug-and-play nature enhances its practicality and compatibility with prior knowledge distillation methods\n\n- Good finding on manifold misalignment within unsupervised vision–language knowledge distillation. The authors identify that conventional unsupervised KD prior works fail to preserve the directional geometry of multimodal embeddings. By introducing a shared manifold with entropy compression, the paper provides a principled solution to restore cross-modal alignment and improve generalization in unsupervised distillation settings."}, "weaknesses": {"value": "- The performance gains of AME are mainly observed under 16-shot or cross-dataset conditions, while the improvement diminishes to less than 1% in full-data settings. This indicates that as the number of training samples increases, the benefit of geometric alignment through the Reconfigured Shared Manifold (RSM) becomes less pronounced. The proposed entropy-based regularization seems primarily effective in stabilizing representation geometry under data-scarce regimes, but its impact weakens when the data manifold is sufficiently covered by abundant supervision\n\n- Insufficient benchmarking against recent unsupervised KD methods. The paper mainly compares AME with PromptKD but lacks broader evaluation against other recent unsupervised or label-free distillation approaches (e.g., KDPL 2024, COSMOS 2025, SILC 2024). Without such head-to-head baselines, it remains unclear whether the proposed manifold-based alignment truly provides stronger or more consistent benefits than other unsupervised objectives or prompt-based designs."}, "questions": {"value": "Could the authors include comparisons with more recent unsupervised or label-free knowledge distillation frameworks  to clarify whether the proposed manifold-based alignment truly offers superior or complementary benefits over other prompt-free or self-distillation objectives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "55X4VFUqQO", "forum": "aQLqieZC5r", "replyto": "aQLqieZC5r", "signatures": ["ICLR.cc/2026/Conference/Submission3958/Reviewer_3PZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3958/Reviewer_3PZV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018480684, "cdate": 1762018480684, "tmdate": 1762917112488, "mdate": 1762917112488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}