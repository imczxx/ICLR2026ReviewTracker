{"id": "G33Iemmj3Z", "number": 17402, "cdate": 1758275532772, "mdate": 1763665172430, "content": {"title": "Distractor-free Generalizable 3D Gaussian Splatting", "abstract": "We present DGGS, a novel framework that addresses the previously unexplored challenge: \\textbf{Distractor-free Generalizable 3D Gaussian Splatting} (3DGS). Previous generalizable 3DGS works are often limited to static scenes, struggling to mitigate distractor impacts in training and inference phases, which leads to training instability and inference artifacts. To address this new challenge, we propose a distractor-free generalizable training paradigm and corresponding inference framework, which can be directly integrated into existing Generalizable 3DGS frameworks. Specifically, in our training paradigm, DGGS proposes a feed-forward mask prediction and refinement module based on the 3D consistency of references and semantic prior, effectively eliminating the impact of distractor on training loss. Based on these masks, we combat distractor-induced artifacts and holes at inference time through a novel two-stage inference framework for reference scoring and re-selection, complemented by a distractor pruning mechanism that further removes residual distractor 3DGS-primitive influences. Extensive feed-forward experiments on the real and our synthetic data show DGGS's reconstruction capability when dealing with novel distractor scenes. Moreover, our feed-forward mask prediction even achieves an accuracy superior to scene-specific Distractor-free methods.", "tldr": "We present DGGS, a novel framework that addresses the previously unexplored challenge: Distractor-free Generalizable 3D Gaussian. Splatting", "keywords": ["Distractor-free", "Generalizable 3D Gaussian Splatting", "training stability"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d42b2e234233e35e2210fc541ed2f796e44bc7c1.pdf", "supplementary_material": "/attachment/3ca31d2508497d31693a3a1796a26a7b3e57c9ce.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose Distractor-free Generalizable 3D Gaussian Splatting, a method designed for removing transient objects in feedforward GS methods. The main goal is to find proper transient masks and remove transients from optimizing the feedforward network. Firstly, DGGS implemented a robust mask based on robustnerf. Such mask is improved by incorporating a mask_ref (from re-rendered photometric loss) from reference images and project it on the target pose. Such mask is further improved based on multi-view visibility and combines with robustnerf. At test time, DGGS introduces schemes to reduce the usage of images with strong transient objects, and perform additional optimization to remove artifacts in the scene."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors demonstrated superior performances in feedforward 3DGS compared to prior work based on PSNR/SSIM etc.\n3. The masks seem reasonable, and the resultant visual quality is also improved."}, "weaknesses": {"value": "In general, I think the writing requires a lot of polishing, in terms of readability, background knowledge, e.g., segmentation models which play a significant role, and overall importance/novelty of the method. \n\nAfter reading this work, the sense that I get is that, while performance is certainly good, the paper heavily leans on engineering and parameter tuning. This includes multiple important hyperparameters (threshold for masks), requirement for estimated depth to be reasonable for reprojection, integration of an external segmentation model that is minimally mentioned, test time hyperparameters (N views - BTW, N is defined multiple times with different meanings across the text). The overall insight is not very different from prior work, i.e., finding good transient masks such that rendering results can be improved by ignoring transients. I also have concerns about how this can be applied to larger scale scenes, as feedforward GS currently is relatively limited. Large scale scenes will lead to more noise in depth and multi-view inconsistencies, which this method seems to be very sensitive on. \n\nI lean borderline on this work based on novelty; since there is no borderline, I lean towards borderline reject as I believe the writing can be better."}, "questions": {"value": "1. The image resolution in the submitted PDF is really low. To the point where this is difficult to read. \n2. Given that NeRF-HUGS produces similar masks as DGGS, can the authors expand on why mvsplat + NeRF-HUGS is significantly worse than DGGS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FQxlL9ULHE", "forum": "G33Iemmj3Z", "replyto": "G33Iemmj3Z", "signatures": ["ICLR.cc/2026/Conference/Submission17402/Reviewer_ArTN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17402/Reviewer_ArTN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761271745609, "cdate": 1761271745609, "tmdate": 1762927305910, "mdate": 1762927305910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DGGS, a framework designed to enable generalizable 3D Gaussian Splatting under distractor-rich real-world scenarios. Unlike existing generalizable 3DGS methods that assume clean static scenes, this work introduces: 1) A Reference-based Mask Prediction mechanism leveraging multi-view consistency. 2) A Mask Refinement module using segmentation priors and occlusion-aware auxiliary supervision. 3) A Two-stage inference strategy, including reference scoring and 3D gaussian primitive pruning, to suppress inference-time artifacts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The use of multi-view geometric consistency to correct masks, reflects good insight rather than brute force.\n\n2. Comprehensive experiments, including synthetic distractor construction.\n\n3. Inference-time pruning is practical and effective."}, "weaknesses": {"value": "1. Heavy reliance on segmentation priors. The pipeline is not truly “feed-forward generalizable” if high-quality segmentation is required and pre-computed.\n\n2. Reference stability assumption unproven. The paper does not quantify how often reference re-rendering is accurate enough to serve as a stable supervisory source.\n\n3. Efficiency cost. Two-stage inference + segmentation noticeably sacrifices speed, which is a key appeal of 3DGS.\n\n4. Mask failure modes not fully analyzed. The limitations section mentions occlusions, but no systematic characterization is provided."}, "questions": {"value": "1. How robust is the reference mask filtering when references also contain distractors?\n\n2. What is the computational overhead of the full pipeline?\n\n3. Does the segmentation model need retraining or domain adaptation in unseen categories?\n\n4. Could the mask refinement be done without segmentation, e.g., self-supervised feature aggregation?\n\n5. How does performance degrade with increasing viewpoint disparity among references?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8B6sqVHaLG", "forum": "G33Iemmj3Z", "replyto": "G33Iemmj3Z", "signatures": ["ICLR.cc/2026/Conference/Submission17402/Reviewer_QPBd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17402/Reviewer_QPBd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653203317, "cdate": 1761653203317, "tmdate": 1762927305124, "mdate": 1762927305124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework called DGGS aimed at making generalizable 3D Gaussian Splatting (3DGS) robust against distractors (transient objects) in real-world scenes. Existing generalizable 3DGS models assume static environments and suffer from training instability and artifacts when transient objects appear. DGGS mitigates this by predicting distractor masks through multi-view geometric consistency and refining them with segmentation priors, then using these masks to exclude distractors during training. At inference, it selects cleaner reference views and prunes distractor-related Gaussian primitives. Experiments demostrate this method outperform the generalizable 3DGS baselines, even better than some scene-specific distractor removal techniques."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This is the first work addressing distractors in generalizable 3DGS, filling an important gap in real-world usage.\n- It significantly boosts robustness and reconstruction quality compared to both baseline 3DGS models and naively transferred scene-specific distractor-free methods.\n- The approach generalizes well to unseen scenes and improves inference quality via smart reference selection and pruning."}, "weaknesses": {"value": "- The method relies on several additional modules, but the sensitivity of the overall performance to the choice or quality of these modules is not discussed.\n- The quality of the generated masks depends on the accuracy of segmentation and depth estimation, which may lead to failure cases in scenes with heavy occlusions or imprecise geometry.\n- Since the approach depends on mask generation, it is unclear how well it would handle naturally dynamic environments, such as moving trees or water, where mask accuracy could be compromised."}, "questions": {"value": "Out of curiosity, can the two-stage inference be made more efficient for real-time use? Have you tested foundation segmentation models like SAM-2, and do they meaningfully improve results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rErWuM9ohr", "forum": "G33Iemmj3Z", "replyto": "G33Iemmj3Z", "signatures": ["ICLR.cc/2026/Conference/Submission17402/Reviewer_sapT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17402/Reviewer_sapT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878413893, "cdate": 1761878413893, "tmdate": 1762927304613, "mdate": 1762927304613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission introduces DGGS, a new framework for Distractor-free Generalizable 3D Gaussian Splatting. It tackles two overlooked issues in generalizable 3DGS: (1) training instability due to transient distractors in real-world data, and (2) feed-forward inference artifacts caused by distractors in references. The method proposes a reference-based mask prediction that leverages 3D multi-view consistency to filter robust residual-based masks, a mask refinement stage that decouples disparity-induced errors and uses entity segmentation plus an auxiliary loss, and a two-stage inference procedure with reference scoring and distractor pruning. Extensive experiments on real (On-the-go, RobustNeRF) and synthetic data show consistent improvements over retrained generalizable baselines and scene-specific distractor-free approaches adapted to the generalizable setting, with additional gains from the inference stage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clearly identifies and formulates a new, practically relevant problem: distractor-free generalizable 3DGS.\n\n- Elegant reference-based mask filtering that reduces over-suppression typical of residual-only masks.\n\n- Thoughtful mask refinement: decoupling disparity vs. distractor; auxiliary loss exploiting cross-view occlusion cues.\n\n- Practical two-stage inference: reference scoring and 3D primitive pruning demonstrably reduce artifacts/holes.\n\n- Strong empirical results with comprehensive comparisons, ablations, and both real and synthetic setups.\n\n- Method is modular and can plug into existing generalizable 3DGS pipelines."}, "weaknesses": {"value": "(1) Dependence on pre-trained entity segmentation during training and inference undermines full “feed-forward” purity and adds latency; domain robustness of the segmenter is not analyzed.\n\n(2) The mask fusion strategy uses intersection across references (conservative), which may lead to under-coverage in low-overlap or high-parallax settings; the trade-off is not deeply quantified.\n\n(3) Distractor pruning can introduce speckle/holes in commonly occluded areas; mitigation is heuristic and the failure modes are only briefly discussed.\n\n(4) Some reliance on depth/warping quality from inferred 3DGS; failure cases when depth is noisy or textures are repeated are not thoroughly dissected.\n\n(5) Fairness concerns: scene-specific methods are adapted into a generalizable training loop but may not reflect their best practices (e.g., stronger per-scene optimization), making cross-paradigm comparisons tricky.\n\n(6) Efficiency overhead from two-stage inference and segmentation is non-trivial; the paper reports times but not detailed profiling or memory usage under varied K, N, and resolution."}, "questions": {"value": "(1) How sensitive is performance to the quality of the pre-trained segmentation model and its domain shift (e.g., indoor vs. outdoor, low light)? Can lighter/zero-shot segmenters maintain most gains?\n\n(2) Why choose strict intersection for multi-view mask fusion? Have you tried soft/weighted fusion (e.g., confidence weighting by photometric residuals or view angle) to recover more static pixels without raising distractor leakage?\n\n(3) Can the auxiliary loss be extended with photometric/feature consistency terms to lessen dependence on segmentation?\n\n(4) How robust is the approach when the majority of references contain similar distractors (e.g., many frames with the same moving car)? Does reference scoring still find sufficiently clean views?\n\n(5) For pruning, did you evaluate per-primitive confidence aggregation across references (e.g., voting) instead of binary masking per view to reduce speckle?\n\n(6) Could you report memory/time breakdown across stages (feature projection, mask prediction/refinement, scoring, pruning) and how they scale with K and N?\n\n(7) Are there benefits or risks in training with the scoring mechanism online (curriculum-style selection of “cleaner” references) rather than only at inference?\n\n(8) How does DGGS perform when camera intrinsics vary or are noisy? Is U assumed known and consistent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HfNGyybWIH", "forum": "G33Iemmj3Z", "replyto": "G33Iemmj3Z", "signatures": ["ICLR.cc/2026/Conference/Submission17402/Reviewer_KDhL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17402/Reviewer_KDhL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977335746, "cdate": 1761977335746, "tmdate": 1762927303956, "mdate": 1762927303956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "## **General Response**\n\nWe thank all reviewers for their thoughtfull and helpfull feedback!\n\nWe are pleased that the reviewers (**KDhL**, **sapT**) recognize the practical significance of the new task of distractor-free generalizable 3DGS:\n\n- \"Clearly identifies and formulates a new, practically relevant problem: distractor-free generalizable 3DGS.\" **KDhL**\n- \"This is the first work addressing distractors in generalizable 3DGS, filling an important gap in real-world usage.\" **sapT**\n\nWe are also pleased that the reviewers (**KDhL**, **sapT**, **QPBd**) consider our proposed method elegant and having good insight:\n\n- \"Elegant reference-based mask filtering that reduces over-suppression typical of residual-only masks.\" **KDhL**\n- \"Thoughtful mask refinement: decoupling disparity vs. distractor; auxiliary loss exploiting cross-view occlusion cues.\" **KDhL**\n- \"The approach generalizes well to unseen scenes and improves inference quality via smart reference selection and pruning.\" **sapT**\n- \"The use of multi-view geometric consistency to correct masks, reflects good insight rather than brute force.\" **QPBd**\n\nWe are also glad that all reviewers find our proposed method reasonable, practical, and effective:\n\n- \"Strong empirical results with comprehensive comparisons, ablations, and both real and synthetic setups.\" **KDhL**\n- \"It significantly boosts robustness and reconstruction quality compared to both baseline 3DGS models and naively transferred scene-specific distractor-free methods.\"  **sapT**\n- \"Inference-time pruning is practical and effective.\"  **QPBd**\n- \"The authors demonstrated superior performances in feedforward 3DGS compared to prior work based on PSNR/SSIM etc.\"  **ArTN**\n- \"The masks seem reasonable, and the resultant visual quality is also improved.\"  **ArTN**\n\n## **Updates and new experiments**\n\nWe summarise the new experiments and discussions inspired by the reviewer's comments below. We plan to incorporate changes into the PDF shortly after discussion with reviewers.\n\n- To further clarify to readers the role of the segmentation model in DGGS, we additionally supplement **Sec. 5.3.3 and Tab. 5** for analysis. \n\n- We conduct additional ablation experiments and analysis in **Sec. B.3 and Tab. 9**. to further demonstrate the effectiveness of intersection fusion to readers.\n\n- To further analyze memory usage and time, we conduct additional ablation experiments and analysis under different *N*(*K*) and different resolutions in **Sec. B.2 and Tab. 10**.\n\n- We conduct additional ablation experiments (as mentioned above) and analysis in **Sec. B.4 and Tab. 11**. \n\n- To further demonstrate to readers the stability of reference re-rendering, we additionally supplement **Sec.5.2.2 and Tab. 6** for analysis.\n\n- We expand the description in the Limitation section and improve the image resolution.\n\n\nWe sincerely thank all reviewers again for their valuable suggestions, and we welcome further discussion at any time."}}, "id": "zy1Y4qEw24", "forum": "G33Iemmj3Z", "replyto": "G33Iemmj3Z", "signatures": ["ICLR.cc/2026/Conference/Submission17402/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17402/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission17402/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763663525836, "cdate": 1763663525836, "tmdate": 1763663632208, "mdate": 1763663632208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "## **General Response**\n\nWe thank all reviewers for their thoughtfull and helpfull feedback!\n\nWe are pleased that the reviewers (**KDhL**, **sapT**) recognize the practical significance of the new task of distractor-free generalizable 3DGS:\n\n- \"Clearly identifies and formulates a new, practically relevant problem: distractor-free generalizable 3DGS.\" **KDhL**\n- \"This is the first work addressing distractors in generalizable 3DGS, filling an important gap in real-world usage.\" **sapT**\n\nWe are also pleased that the reviewers (**KDhL**, **sapT**, **QPBd**) consider our proposed method elegant and having good insight:\n\n- \"Elegant reference-based mask filtering that reduces over-suppression typical of residual-only masks.\" **KDhL**\n- \"Thoughtful mask refinement: decoupling disparity vs. distractor; auxiliary loss exploiting cross-view occlusion cues.\" **KDhL**\n- \"The approach generalizes well to unseen scenes and improves inference quality via smart reference selection and pruning.\" **sapT**\n- \"The use of multi-view geometric consistency to correct masks, reflects good insight rather than brute force.\" **QPBd**\n\nWe are also glad that all reviewers find our proposed method reasonable, practical, and effective:\n\n- \"Strong empirical results with comprehensive comparisons, ablations, and both real and synthetic setups.\" **KDhL**\n- \"It significantly boosts robustness and reconstruction quality compared to both baseline 3DGS models and naively transferred scene-specific distractor-free methods.\"  **sapT**\n- \"Inference-time pruning is practical and effective.\"  **QPBd**\n- \"The authors demonstrated superior performances in feedforward 3DGS compared to prior work based on PSNR/SSIM etc.\"  **ArTN**\n- \"The masks seem reasonable, and the resultant visual quality is also improved.\"  **ArTN**\n\n## **Updates and new experiments**\n\nWe summarise the new experiments and discussions inspired by the reviewer's comments below. We plan to incorporate changes into the PDF shortly after discussion with reviewers.\n\n- To further clarify to readers the role of the segmentation model in DGGS, we additionally supplement **Sec. 5.3.3 and Tab. 5** for analysis. \n\n- We conduct additional ablation experiments and analysis in **Sec. B.3 and Tab. 9**. to further demonstrate the effectiveness of intersection fusion to readers.\n\n- To further analyze memory usage and time, we conduct additional ablation experiments and analysis under different *N*(*K*) and different resolutions in **Sec. B.2 and Tab. 10**.\n\n- We conduct additional ablation experiments and analysis in **Sec. B.4 and Tab. 11** for some alternatives to segmentation models. \n\n- To further demonstrate to readers the stability of reference re-rendering, we additionally supplement **Sec.5.2.2 and Tab. 6** for analysis.\n\n- We expand the description in the Limitation section and improve the image resolution.\n\n\nWe sincerely thank all reviewers again for their valuable suggestions, and we welcome further discussion at any time."}}, "id": "zy1Y4qEw24", "forum": "G33Iemmj3Z", "replyto": "G33Iemmj3Z", "signatures": ["ICLR.cc/2026/Conference/Submission17402/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17402/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission17402/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763663525836, "cdate": 1763663525836, "tmdate": 1763741189670, "mdate": 1763741189670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}