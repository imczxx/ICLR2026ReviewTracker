{"id": "bZAKJwyn1n", "number": 14338, "cdate": 1758233016044, "mdate": 1759897376396, "content": {"title": "Pursuing Minimal Sufficiency in Spatial Reasoning", "abstract": "Spatial reasoning, the ability to ground language in 3D understanding, remains a persistent challenge for Vision-Language Models (VLMs). We identify two fundamental bottlenecks: \\textit{inadequate} 3D understanding capabilities stemming from 2D-centric pre-training, and reasoning failures induced by \\textit{redundant} 3D information.\nTo address these, we first construct a Minimal Sufficient Set (MSS) of information before answering a given question: a \\textit{compact} selection of 3D perception results from \\textit{expert models}. We introduce \\textbf{MSSR} (Minimal Sufficient Spatial Reasoner), a dual-agent framework that implements this principle.  A \\textit{Perception Agent} programmatically queries 3D scenes using a versatile perception toolbox to extract sufficient information, including a novel \\textbf{SOG} (Situated Orientation Grounding) module that robustly extracts language-grounded directions. A \\textit{Reasoning Agent} then iteratively refines this information to pursue minimality, pruning redundant details and requesting missing ones in a closed loop until the MSS is curated. \nExtensive experiments demonstrate that our method, by explicitly pursuing both sufficiency and minimality, significantly improves accuracy and achieves state-of-the-art performance across two challenging benchmarks. Furthermore, our framework produces interpretable reasoning paths, offering a promising source of high-quality training data for future models. Source code will be made publicly available.", "tldr": "", "keywords": ["spatial reasoning", "agent", "VLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/af20d1aa36e8a0830ab69a47a1939ce04c12c33d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MSSR (Minimal Sufficient Spatial Reasoner), a dual-agent framework intended to improve spatial reasoning in vision-language models. The framework introduces a Perception Agent using a versatile perception toolbox (including a novel SOG (Situated Orientation Grounding) module) to extract 3D information, and a Reasoning Agent that iteratively prunes and augments this information to curate a minimal sufficient set (MSS) required for reasoning about a given question. Experimental results on two challenging benchmarks (MMSI-Bench and ViewSpatial-Bench) demonstrate state-of-the-art performance. The framework can also produce interpretable reasoning paths as training data for future models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The designed dual-agent framework is meaningful and effective.\n2. The proposed method is training-free, making it easy to integrate seamlessly with existing models.\n3. The method achieves state-of-the-art performance, outperforming all proprietary, open-source, specialist, and agentic models."}, "weaknesses": {"value": "1. The paper only selects a proprietary LLM (GPT-4o) as the backbone to demonstrate the effectiveness of the method. Additional experiments and analysis on open-source LLMs (e.g., Qwen2.5-VL) could further validate the efficacy and generalizability of the method.\n2. The paper only provides the inference time per iteration. A thorough efficiency comparison with previous methods and corresponding analysis could further enhance the soundness."}, "questions": {"value": "1. How is the quality of the constructed Minimal Sufficient Set ensured? Are there scenarios where the Reasoning Agent might prune critical information or request unnecessary information?\n2. What are the common failure modes of the model? Does it struggle (e.g., the Reasoning Agent never makes the final decision) when the question is very complex?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ojJsMRTgfB", "forum": "bZAKJwyn1n", "replyto": "bZAKJwyn1n", "signatures": ["ICLR.cc/2026/Conference/Submission14338/Reviewer_KaHi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14338/Reviewer_KaHi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909330293, "cdate": 1761909330293, "tmdate": 1762924760849, "mdate": 1762924760849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MSSR (Minimal Sufficient Spatial Reasoner), a dual-agent, zero-shot framework for improving 3D spatial reasoning in Vision-Language Models. The framework works by constructing a Minimal Sufficient Set (MSS), the smallest subset of 3D perceptual information sufficient to answer a query, by a dual agent iteratively. MSSR consists of a Perception Agent, which programmatically queries a scene using modular visual tools including a new Situated Orientation Grounding (SOG) module for robust direction grounding, and a Reasoning Agent, which prunes irrelevant information and iteratively requests only what is missing until the MSS is achieved. Experiments on MMSI-Bench and ViewSpatial-Bench show MSSR achieving state-of-the-art results, outperforming models like GPT-4o and Gemini 2.5 Pro."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and logically structured, making the methodology and motivation easy to follow.\n- The related work section provides a well-organized and comprehensive discussion covering VLMs for spatial reasoning, agentic framework, and visual programming paradigms.\n- The proposed MSSR framework is conceptually sound and technically well-motivated.\n- Extensive experiments demonstrate strong empirical performance, achieving state-of-the-art results on both MMSI-Bench and ViewSpatial-Bench while maintaining interpretability through reasoning traces."}, "weaknesses": {"value": "- The paper formatting does not fully comply with ICLR submission requirements.\n- The approach heavily depends on the accuracy of perception modules (e.g., reconstruction, localization, orientation grounding); sensitivity or robustness analysis is lacking.\n- The criterion for minimality of the MSS is conceptually emphasized but lacks clear quantitative verification - how to ensure the generated MSS is minimal?\n- Details on the prompting and coordination between the Reasoning Agent (RA) and Perception Agent (PA) are insufficient—specifically, how the RA is informed of the available tools or APIs.\n- Both benchmarks used (MMSI-Bench and ViewSpatial-Bench) are newly proposed and not yet peer-reviewed; the authors are suggested to conduct evaluations on established benchmarks such as SQA3D[1] or ScanQA[2].\n- The paper would benefit from qualitative examples of failure cases.\n\n\n[1] Ma, Xiaojian, et al. \"SQA3D: Situated Question Answering in 3D Scenes.\" The Eleventh International Conference on Learning Representations.\n\n[2] Azuma, Daichi, et al. \"Scanqa: 3d question answering for spatial scene understanding.\" *proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2022."}, "questions": {"value": "Please refer to the weakness section.\n\nMinor typo:\n- line 165, $S$ and $S^\\*$  should be $\\mathcal{S}$ and $\\mathcal{S}^\\*$"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HLG4ktnMEx", "forum": "bZAKJwyn1n", "replyto": "bZAKJwyn1n", "signatures": ["ICLR.cc/2026/Conference/Submission14338/Reviewer_QPFG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14338/Reviewer_QPFG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939965110, "cdate": 1761939965110, "tmdate": 1762924760257, "mdate": 1762924760257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MSSR (Minimal Sufficient Spatial Reasoner), a dual-agent framework aimed at improving 3D spatial reasoning in vision-language models through explicit pursuit of minimal sufficiency. The approach separates perception and reasoning into two collaborating agents: a Perception Agent and a Reasoning Agent. The framework achieves competitive results on MMSI-Bench and ViewSpatial-Bench."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is well-motivated, as MSSR presents a sufficient yet non-redundant agent framework that enables effective spatial reasoning.\n\n2. MSSR achieves strong performance on two challenging, vision-centric spatial reasoning benchmarks.\n\n3. The paper is well-organized and easy to follow."}, "weaknesses": {"value": "1. The state-of-the-art claim is unconvincing since Table 1 omits recent strong 3D-LLM baselines (e.g., Video-3D-LLM, VLM-3R) that are directly relevant to spatial reasoning. Including these comparisons is necessary for a fair evaluation.\n\n2. Although the paper includes ablations on the PA and RA, it would be beneficial to further analyze how individual components within the Perception Agent, such as the spatial reasoning modules, foundational scene reconstruction, and global calibration, contribute to overall performance.\n\n3. One potential concern is that MSSR appears to lack a mechanism to verify the correctness of the information provided by PA. Although the RA agent continuously queries PA and filters out irrelevant information, it remains unclear how the framework handles spurious or inaccurate inputs from PA. For future extensions, incorporating a verification module to ensure the reliability of PA’s outputs would be a valuable improvement."}, "questions": {"value": "1. Does the “only PA” setting in Table 2 refer to using PA to collect all information without any filtering, or does it mean collecting subset information for the first iteration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tGib5c1JRl", "forum": "bZAKJwyn1n", "replyto": "bZAKJwyn1n", "signatures": ["ICLR.cc/2026/Conference/Submission14338/Reviewer_gXuZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14338/Reviewer_gXuZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762168753987, "cdate": 1762168753987, "tmdate": 1762924759855, "mdate": 1762924759855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MSSR, a 3D spatial reasoning framework that constructs a minimal sufficient set of spatial facts before answering a question. The model comprises two agents: A perception agent that uses visual programming with tool calls to extract 3D information, and a reasoning agent that plans and iterative requests missing information until the set if sufficient. The model is evaluated on MMSI-Bench and ViewSpatial-Bench, claiming SOTA against prior works."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The model gives good motivation with Figure 1, introducing attention delusion. \n\nThe baselines compared are quite strong, including both closed and open-source models, as well as specialist and other visual programming frameworks like VADAR.\n\nThe design of the model allows plug-and-play of powerful models like VGGT, GroundingDINO, SAM2, etc, which can be easily swapped out with better models.\n\nThe ablations are quite thorough with the key components (Only PA and Only RA, without iterations, etc.)."}, "weaknesses": {"value": "This paper is generally quite strong in my opinion. My only issues are perhaps more datapoints for the ablation.\n\nFor Figure 5, I think the paper would be better benefitted by showing 1) more iterations to see whether more iterations would cause models to have errors in eliminating the number of sets needed for a correct answer and therefore leading to worse performance and 2) on more than just a subset of the MMSI-Bench.\n\nFor Table 2, it would be great if the authors can also provide information on ViewSpatial-Bench for a more comprehensive ablation.\n\nHowever, these are just minor points in general, and I would think the paper’s idea is in fact quite solid. I am therefore leaning towards accept."}, "questions": {"value": "Please see the above section for my questions regarding the ablations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3eJzp4bg2E", "forum": "bZAKJwyn1n", "replyto": "bZAKJwyn1n", "signatures": ["ICLR.cc/2026/Conference/Submission14338/Reviewer_9uKo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14338/Reviewer_9uKo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762243690906, "cdate": 1762243690906, "tmdate": 1762924759466, "mdate": 1762924759466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}