{"id": "SKGRMy3nXi", "number": 4266, "cdate": 1757651409490, "mdate": 1763027189171, "content": {"title": "Data-driven Staircase Activation Functions for Ordinal Classification", "abstract": "Ordinal labels are discrete and ordered but lack calibrated spacing, a structure that most deep networks ignore by treating them as nominal classes or real values. We introduce trainable staircase activations as a drop-in replacement, which partitions the output space into learnable, ordered intervals to align predictions with the ordinal labels. Direct parameterization reveals a degeneration–saturation dilemma in which gradients vanish and intervals collapse; we analyze its cause and propose three remedies: (i) stochastic noise injection to de-saturate plateaus, (ii) a monotonic ascending term to enforce order, and (iii) adaptive piecewise-linear functions that adjust thresholds end-to-end. Paired with a mutual information regularized absolute-error loss, our design stabilizes optimization and preserves ordinal structure. The modules are drop-in replacements for final layers and integrate with standard architectures without any architectural changes. Across diverse benchmarks, they consistently outperform softmax/logistic baselines and prior ordinal methods, demonstrating that staircase activations are an effective and principled building block for end-to-end learning with ordinal targets.", "tldr": "We propose trainable staircase-like activations that learn intervals preserving ordinal structure. With noise, monotonic terms, adaptive piecewise functions, and mutual-information regularized loss, it improves stability and outperforms baselines.", "keywords": ["Activation function", "Ordinal regression", "Ranking problem", "Parameterized activation", "Trainable activation", "Mutual information"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/44efb54ee99dcef8ad0c6c734dfe1de437527767.pdf", "supplementary_material": "/attachment/db2dd5d3a33e2ad8f60c342e01cea70edf0d0887.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces trainable staircase activation functions as a drop-in replacement for standard output layers. These activations partition the output space into learnable, ordered intervals, aligning predictions with ordinal labels.\nThe contribution is not so much in the core of the activation function (already proposed in the literature) but in the techniques to address the challenges that arise during training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Well-written, addressing an important problem.\n\nThe proposed technique allows a simple drop-in Replacement for Output Layers.\n\nThe work studies three complementary remedies for training stability."}, "weaknesses": {"value": "The citation format seems to be wrong, mixing textual with parenthetical citations (citet vc citep in natbib)\n\nPW is used before being defined.\n\nsection 2.2\n\"Then f should satisfy two properties: (i) monotonicity to preserve order,\n∀ xi < xj : f(xi) ≤ f(xj); and (ii) within-class closeness versus across-class separation\"\n\n1st Property: Since x_i is multidimensional, what order relationship is defined in R^d ?\n\n2nd property: does not seem specific to ordinal regression.\n\nSection 4.\n\"We evaluate it by replacing only the activation in public baselines while keeping the backbone, training protocol, and hyperparameters fixed, thus isolating its effect for fair comparison.\"\nBeing fair does not mean to do the same for everyone, but to do the best for each one. Keeping the hyperparameters fixed may favour one of the methods over the others. It would be advisable to optimize for each algorithm.\n\nIt seems that the mutual information version of the method is always the preferred one. That being the case, a bit more emphasis could have been given to its description in the main body of the document.\n \nThe methods selected in the empirical study vary from dataset to dataset, which makes the comparison harder and may give the impression that datasets/ methods were cherry-picked.\n\nAdditionally, the basic architectures trained with cross-entropy are often a strong contender, often beating the more complex alternatives; it should be included in the comparison."}, "questions": {"value": "- see weaknesses\n-How does the introduced dispersion regularizer solve the zero-gradient issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BzJSoakFwi", "forum": "SKGRMy3nXi", "replyto": "SKGRMy3nXi", "signatures": ["ICLR.cc/2026/Conference/Submission4266/Reviewer_deNA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4266/Reviewer_deNA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761341392898, "cdate": 1761341392898, "tmdate": 1762917265717, "mdate": 1762917265717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "YiDDwQ0Skz", "forum": "SKGRMy3nXi", "replyto": "SKGRMy3nXi", "signatures": ["ICLR.cc/2026/Conference/Submission4266/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4266/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763027188367, "cdate": 1763027188367, "tmdate": 1763027188367, "mdate": 1763027188367, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces *Staircase Activation Functions* designed to preserve ordinal relationships in ordinal classification tasks. The authors identify two critical challenges during training: Degeneration, Saturation. The paper provide a theoretical analysis of these issues and propose three complementary remedies: Noise-injected Staircase, Ascending Staircase, and Piecewise-Linear Staircase. Additionally, they incorporate a Mutual Information–regularized MAE loss (LMI) to further enlarge inter-class separation and enhance ordinal structure."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-motivated and easy-to-follow.\n- The paper introduces a novel approach that directly incorporates ordinal structure at the activation level.\n- The paper provides theoretical justification via saturation/degeneration analysis and MI-based objective formulation.\n- The paper demonstrates improvement across diverse modalities."}, "weaknesses": {"value": "- The theoretical section, while insightful, is not fully rigorous. Some derivations (e.g., Proposition 1 and Lemma 1) rely on informal assumptions and omit key intermediate steps, making parts of the argument more illustrative than formally proven. The intuition is sound, but it might help if the authors explicitly frame these as heuristic analyses rather than formal proofs.\n\n- The experiments, although showing consistent gains, do not always include the latest or strongest order-aware baselines for each domain. Including such comparisons would better contextualize the contribution and strengthen the empirical claims."}, "questions": {"value": "1. W1\n \n2. W2\n\n3. Does the proposed Staircase activation risk collapsing under highly imbalanced class distributions?\n\n4. How sensitive is performance to the threshold initialization strategy?\n\n5. How should the $λ_{Div}$ for the MI regularizer be selected, and how sensitive is tuning?\n\n6. Could the authors clarify the criteria used to select which Staircase variant is reported in the main tables?\nSince different variants show mixed performance in the appendix, a consistent reporting rule would help address potential reproducibility or fairness concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rH0vhUEZ4n", "forum": "SKGRMy3nXi", "replyto": "SKGRMy3nXi", "signatures": ["ICLR.cc/2026/Conference/Submission4266/Reviewer_U8Lf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4266/Reviewer_U8Lf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553543511, "cdate": 1761553543511, "tmdate": 1762917264761, "mdate": 1762917264761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "this work deals with ordinal classification (OC) task.\nauthors propose a learnable staircase activation that can be plugged into existing models.\nthey also showed its saturation issue and propose different ways to alleviate that.\nresults are reported on 5 applications and 8 datasets in comparison to some methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- the writing is good.\n- the work tackles an important task which is ordinal classification.\n- they proposed a learnable staircase activation that can be plugged in different models.\n- results are reported on different applications."}, "weaknesses": {"value": "- writing: introduction section is very short, does not highlight the important of application of ordinal classification. in addition, it does not cover well existing works, their limitations, and why staircase activation is relevant and why this specific approach is better. background section is very long. the proposal section is more like a list of things that authors consider to remedy the saturation problem.\n\n- poor alignment with existing works: this work is poorly aligned with existing works about OC. it is not clear why staircase activation - among all existing works- is relevant. what are existing gaps in previous works is not clear. the work seems an outlier picked - related to works mentioned in l61-, then authors attempted to reduce the saturation problem.\n\n- results: there is a clear lack of comparison to existing works. authors discussed several works in OC in related work section. but, there is a minimal comparison to them in terms of results. it is not clear how the proposed method performs compared to them, especially recent works / SOTA."}, "questions": {"value": "- please try to re-write the introduction, and position well the proposed method. try to justify why this approach is better than existing works - or why it worth pursuing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PQRjXDPBXn", "forum": "SKGRMy3nXi", "replyto": "SKGRMy3nXi", "signatures": ["ICLR.cc/2026/Conference/Submission4266/Reviewer_e6WF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4266/Reviewer_e6WF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858378621, "cdate": 1761858378621, "tmdate": 1762917264587, "mdate": 1762917264587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed data-driven staircase activation functions as drop-in loss replacement to improve deep learning for ordinal classification by correctly handling ordered categories like ratings or age groups. The main idea is a trainable staircase activation function that creates learnable, ordered output intervals in the final layer, directly matching the ordinal labels. \nDirect parameterization on the formulation faces the degeneration-saturation dilemma, where gradients vanish and the function stops learning. The authors then proposed three main fixes: (i) stochastic noise injection to desaturate plateaus, (ii) a monotonic ascending term to enforce order, and (iii) adaptive piecewise-linear functions that adjust thresholds end-to-end. The authors then demonstrate performance benefits across multiple tasks: time-series, age estimation, and sentiment analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "There are a few things I like about the paper:\n1. The paper addresses an interesting challenge in developing a proper training objective for ordinal classification problems.\n2. The proposed methods can serve as a drop-in replacement for training loss, so it is applicable to any deep learning model without architecture change.\n3. The proposed methods are well motivated from analyzing the degeneration-saturation dilemma.\n4. The authors provide extensive theoretical analysis of the property of the proposed method.\n5. The experiments are done in a range of datasets, where it shows consistent improvement."}, "weaknesses": {"value": "1. [Experiment Baselines]. The authors only compared the proposed method with a few baselines. Adding more baselines to the experiments is suggested.\n2. [Ablation]. The authors described 4 components of the proposed model. Based on the experimental results, it is not clear which components contribute the most to the performance improvement.\n3. [Size of dataset]. Many of the datasets used in the experiments are relatively small.\n4. [Experiment]. Some of the performance improvements in some dataset are relatively small.\n5. [Experiment run]. Many experiments have 0 standard deviation, where the experiment is only done in a single run. Running experiments multiple times is always suggested.\n6. [Presentation]. I suggest the author put the original metric number rather than percentage improvements. Putting percentage improvement may make it look bigger than what they are."}, "questions": {"value": "Please address the weaknesses mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dZ0zrS78TE", "forum": "SKGRMy3nXi", "replyto": "SKGRMy3nXi", "signatures": ["ICLR.cc/2026/Conference/Submission4266/Reviewer_HyXS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4266/Reviewer_HyXS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137008530, "cdate": 1762137008530, "tmdate": 1762917264146, "mdate": 1762917264146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}