{"id": "zbdhhlIy8o", "number": 14834, "cdate": 1758244504447, "mdate": 1759897346533, "content": {"title": "Accelerated Learning with Linear Temporal Logic using Differentiable Simulation", "abstract": "Ensuring that reinforcement learning (RL) controllers satisfy safety and reliability constraints in real-world settings remains challenging: state-avoidance and constrained Markov decision processes often fail to capture trajectory-level requirements or induce overly conservative behavior. Formal specification languages such as linear temporal logic (LTL) offer correct-by-construction objectives, yet their rewards are typically sparse, and heuristic shaping can undermine correctness. We introduce, to our knowledge, the first end-to-end framework that integrates LTL with differentiable simulators, enabling efficient gradient-based learning directly from formal specifications. Our method relaxes discrete automaton transitions via soft labeling of states, yielding differentiable rewards and state representations that mitigate the sparsity issue intrinsic to LTL while preserving objective soundness. We provide theoretical guarantees connecting Büchi acceptance to both discrete and differentiable LTL returns and derive a tunable bound on their discrepancy in deterministic and stochastic settings. Empirically, across complex, nonlinear, contact-rich continuous-control tasks, our approach substantially accelerates training and achieves up to twice the returns of discrete baselines. We further demonstrate compatibility with reward machines, thereby covering co-safe LTL and LTLf without modification. By rendering automaton-based rewards differentiable, our work bridges formal methods and deep RL, enabling safe, specification-driven learning in continuous domains.", "tldr": "We address challenges of scalable learning with correct objectives, using LTL as the formal specification language and differentiable simulation to accelerate learning.", "keywords": ["reinforcement learning", "temporal logic", "differentiable simulation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a529f2306fa39a706b76293d634fec35cf23a500.pdf", "supplementary_material": "/attachment/da7e3fddff89f9a275c2a3b10179bb604809e0cb.zip"}, "replies": [{"content": {"summary": {"value": "Logical formalisms enable the specification of agents’ behaviors and offer correct-by-construction objectives. In RL, reward design often struggles to capture the user’s intended task, resulting in misaligned or sparse signals. Linear temporal logic (LTL) provides an expressive language to specify such trajectory-level objectives but leads to discrete, sparse rewards that hinder policy optimization. This paper introduces an end-to-end differentiable framework that integrates LTL with differentiable simulators, allowing gradients to flow through both the environment and the logical specification. The approach replaces hard labeling with smooth probabilistic labeling, yielding differentiable transitions and rewards. The authors prove a theoretical bound linking discrete and differentiable LTL returns and empirically demonstrate faster learning and higher returns on continuous-control benchmarks. The authors also note the results hold in stochastic settings. The method also generalizes to reward machines, thus covering co-safe LTL and LTLf tasks. Overall, the framework bridges formal methods and deep RL through differentiable logic-based objectives."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- The proposed use of differentiable labeling functions and probabilistic automaton transitions provides a clear and elegant means of propagating gradients through LTL objectives.\n- Theoretical contributions include a bound relating discrete and differentiable LTL rewards, ensuring that the relaxation remains faithful to the underlying specification.\n- The algorithmic exposition is quite clear; gradients, $\\epsilon$-actions, and update steps are explicitly shown in the final algorithm, supported by a detailed parking example that illustrates the difference between discrete and differentiable LTL rewards.\n- The experimental evaluation is broad, covering easy-to-challenging continuous-control benchmarks (from 5-/1-D to 37-/8-D state-action spaces) and including ablation studies that isolate the role of differentiability.\n- Comparisons between differentiable and discrete LTL baselines are convincing, showing consistent improvement in convergence and policy quality.\n- The experiments on reward machines and their comparison to existing algorithms are particularly valuable, as they demonstrate compatibility across formal-specification frameworks and situate the contribution in a wider research context."}, "weaknesses": {"value": "- While Theorem 2 mentions the applicability of the approach to stochastic settings, the environments presented in the paper are solely deterministic.\n- The success of the approach largely relies on $\\beta$, which allows deriving the sole reward signal the agent gets and serves as a second discount factor. $\\beta$ is also critical in Theorem 2, providing a divergence between discrete and differentiable rewards. However, there is no discussion or intuition on how to choose $\\beta$ in practice to achieve the theoretical guarantees. \n\n*Remark*: I've had a hard time parsing the phrasing *\"Further, the rewards are discounted less in non-accepting states to reflect that the number of visitations to non-accepting states are not important.\"* Since $\\beta$ is a function of $\\gamma$ and the ratio in Theorem 1 should converge to zero, I understand that $\\beta$ should always be lower than $\\gamma$, but the phrasing (\"discounting less\") is confusing.\n- The signal functions $g_a$ underlying the soft labels remain heuristic and \"hard-coded\"; their design influence is not discussed.\n- The paper is visually extremely dense, relying heavily on negative `\\vspace` and compressed layout, which reduces readability."}, "questions": {"value": "- How should we tune $\\beta$ in practice? \n- Could the approach be extended to environments that are only partially differentiable or hybrid, where discrete transitions coexist with differentiable dynamics?\n- The proposed rewards depend on the frequency of visiting accepting states in the automaton. Could the authors clarify whether, by tweaking the discounts, agents might \"exploit\" repeated visits to accepting states to obtain high returns without maintaining long-term satisfaction of the LTL formula?\n- How sensitive is the method to the slope of the sigmoid activation used in the labeling functions, and how does this affect learning stability or correctness? Did you consider enriching the sigmoid function with a temperature parameter in practice? Would it be a good idea?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qw8A6Xsonm", "forum": "zbdhhlIy8o", "replyto": "zbdhhlIy8o", "signatures": ["ICLR.cc/2026/Conference/Submission14834/Reviewer_h7aD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14834/Reviewer_h7aD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575895726, "cdate": 1761575895726, "tmdate": 1762925186712, "mdate": 1762925186712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method to make logic-based specifications of rewards or constraints differentiable. Classical logic-specification frameworks rely on a formula that defines objectives through the truth values of Boolean predicates, which are then translated into an automaton and combined with an MDP to form a product MDP. The problem is that the resulting automaton produces non-differentiable states, as these depend on discrete Boolean propositions. This paper proposes a relaxation by introducing _soft labels_ that yield probabilities instead of Boolean values. Consequently, the automaton is represented by _probability vectors_ rather than one-hot encodings, enabling (i) differentiable reward signals and (ii) reduced reward sparsity, a well-known issue in the literature. The also formally prove the discrepancy between the discrete and differentiable returns. They provide empirical evaluation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed idea is interesting and their contribution naturally mitigates reward sparsity while maintaining differentiability, making it compatible with differentiable RL controllers.\n- The mathematical formulation is sound and providing both differentiability and normalization.\n- The proposed approach bridge the temporal abstraction provided by the LTL framework to the differentiable control settings."}, "weaknesses": {"value": "- I believe that limiting the labels to continuous functions of the state is a strong assumption, that restricts the benefits of logic specifications to consider only predicates that are expressible as continuous functions. For instance, $a$ = \"agent reached goal in $(x_g, y_g)$\" is captured by your assumptions, becasue it can be expressed as a distance, which is a continuous function of the state $x$. Instead, $a$ = \"the light is red\" is a boolean and condition. This assumption limits the expressive power of the logical specification framework to consider only continuous labels.\n- It would be nice to compare the performance of SHAC and AHAC with and without the LTL framework for a fair evaluation. Providing this comparison would clarify whether the observed improvements stem from the proposed LTL-based formulation or from the underlying algorithmic differences. Without this, is hard to tell what is the source of the improvement."}, "questions": {"value": "- Regarding the claim on line 253 that \"this computation can be efficiently done through differentiable matrix multiplication\" can you comment on this? The transition matrix has size $|Q| \\times |Q|$ do you assume that $|Q|$ is small in practice? Otherwise, the operation may not be computationally trivial.\n- Can you elaborate on the point raised in weakness 1?\n- Given that the framework can only express continuous functions of the state as labels, what are the advantages of relying on logical specifications rather than classical approaches such as Model Predictive Control, which naturally handles continuous constraints (aside from the temporal abstraction advantage)?\n\nTypos:\n- Line 122: \"...The state space S (is) the...\"\n- Line 132: possible repetition of \"the return.\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LmDexuXNdu", "forum": "zbdhhlIy8o", "replyto": "zbdhhlIy8o", "signatures": ["ICLR.cc/2026/Conference/Submission14834/Reviewer_QdnH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14834/Reviewer_QdnH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947626360, "cdate": 1761947626360, "tmdate": 1762925186126, "mdate": 1762925186126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an end-to-end framework that combines Linear Temporal Logic (LTL) with differentiable simulators, allowing agents to learn directly from formal task specifications using gradient-based optimization. The authors argue that when LTL is used in reinforcement learning, it typically produces discrete and sparse rewards, which can slow down training and hurt performance. To address this, they “soften” the discrete automaton transitions through a soft-labeling technique, creating smooth, differentiable rewards while still maintaining the original task semantics. They also provide theoretical guarantees showing that the gap between the discrete LTL reward and the relaxed differentiable version is bounded under certain assumptions. Experiments compare their method against PPO and SAC agents trained with standard sparse LTL rewards, demonstrating improved performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written and easy to understand. The authors do a nice job presenting their method.\n\nThe motivation is strong. LTL and other temporal-logic or formal-methods approaches often struggle with scalability and sparse rewards in RL, and this work tackles that important challenge.\n\nThe proposed approach appears sound. Using differentiable simulation creates a fully end-to-end training pipeline with gradient flow, which can speed up learning and potentially lead to better overall performance.\n\nThe authors include a theoretical analysis that bounds the difference between the original discrete reward and the relaxed differentiable version.\n\nThe experimental results are convincing and show promising improvements.\n\nThe related-work discussion is thorough and well-situated in the existing literature."}, "weaknesses": {"value": "1. It is not fully clear how this method compares to other model-based RL approaches that also incorporate formal guarantees during training. For example, recent work that uses world models together with barrier certificates or STL can also be viewed as an end-to-end differentiable pipeline, since the world model acts as a differentiable simulator and the formal constraints guide policy learning. It would be helpful for the authors to discuss these connections more clearly and highlight the differences, including works like \nReference: State-Wise Safe Reinforcement Learning with Pixel Observations.\n\n2. Unlike LTL, STL signals are already continuous rather than purely discrete. Why do the authors focus on making LTL differentiable instead of using STL, which may naturally fit differentiable learning?\n\n3. The experiments do not include comparisons with model-based or STL-based RL baselines, which would help clarify the advantages of the proposed method."}, "questions": {"value": "1. Page 3, the MDP formulation is more like a control formulation, generally we should have something like S x A x S -> Prob[0, 1] as transition dynamics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9hXupHJbq4", "forum": "zbdhhlIy8o", "replyto": "zbdhhlIy8o", "signatures": ["ICLR.cc/2026/Conference/Submission14834/Reviewer_kd6k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14834/Reviewer_kd6k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147651825, "cdate": 1762147651825, "tmdate": 1762925185620, "mdate": 1762925185620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a logic-based reinforcement learning (RL) framework that introduces differentiable LTL rewardsby employing probabilistic, or “soft,” atomic propositions within differentiable MDP environments. The key idea is to integrate logic-based specification methods into differentiable RL, thereby bridging symbolic reasoning and gradient-based optimization. The authors further extend the framework to differentiable generalizations of reward machines, providing a smooth relaxation of discrete logical rewards. Theoretical results show that by appropriately choosing the *softening parameter* $\\zeta$, the error between the discrete and differentiable logic-based RL formulations can be made arbitrarily small.\n\nEmpirical evaluations demonstrate that Short Horizon Actor-Critic (SHAC) and Adaptive Horizon Actor-Critic (AHAC) outperform discrete RL baselines such as PPO and SAC, confirming that the benefits of differentiable MDPs extend to logic-based rewards. The experiments also include a differentiable variant of reward machines, showing that differentiability continues to provide learning benefits even under structured quantitative objectives."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The integration of logical specifications into differentiable RL is conceptually elegant and addresses an important challenge in combining symbolic reasoning with continuous optimization.\n2. The derivation of error bounds demonstrating convergence between discrete and differentiable logic-based RL formulations adds mathematical rigor to the approach.\n3. Experiments with SHAC and AHAC validate the framework’s benefits over standard baselines, showing improvements consistent with expectations from differentiable modeling.\n4. The paper clearly situates itself within state-of-the-art logic-based RL research and provides a helpful summary of related work on differentiable MDPs and reward machines."}, "weaknesses": {"value": "1. It is unclear whether the proposed framework can be readily extended to discrete MDPs. Intuitively, a similar smooth approximation could be applied to the transition structure, but this is not discussed. Clarifying this would strengthen the paper’s generality.\n2. The paper does not explore whether inherently discrete methods such as reward shaping or counterfactual reasoning could complement, or interfere with, the differentiable automata framework. Including experiments or discussion in this direction would add practical depth.\n3. The paper focuses on LTL, but it would be helpful to discuss whether logics tailored to continuous systems, such as Metric Temporal Logic or Signal Temporal Logic, are more naturally suited to differentiable environments. A short comparison of their expressive and computational trade-offs would be helpful."}, "questions": {"value": "1. Can the proposed approach be extended to discrete MDPs, perhaps via smooth approximations of the transition probabilities?\n2. How would reward shaping or counterfactual experience replay interact with differentiable logical rewards?\n3. Are continuous-time temporal logics such as MTL or STL more suitable for differentiable RL, and how might they affect interpretability or computational cost?\n4. Discrete reward machines, with their inherently sparse reward structures, can sometimes perform competitively when combined with reward shaping or counterfactual experiences. Do the authors expect differentiable reward machines to consistently outperform discrete ones, or could sparsity occasionally confer advantages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7eUSFMHvS4", "forum": "zbdhhlIy8o", "replyto": "zbdhhlIy8o", "signatures": ["ICLR.cc/2026/Conference/Submission14834/Reviewer_Ay51"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14834/Reviewer_Ay51"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762187098227, "cdate": 1762187098227, "tmdate": 1762925184613, "mdate": 1762925184613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}