{"id": "5Rgn6x9jGn", "number": 404, "cdate": 1756738341486, "mdate": 1763120257363, "content": {"title": "Slimming the Giant: Efficient Structured Pruning for Adapter-Tuned SAM", "abstract": "Foundation models with parameter-efficient adapters enable strong segmentation but remain hard to deploy due to scale and cost. We propose Adapter-aware Structured Sparsification (ASSP), a structured pruning framework for adapter-tuned SAM. ASSP begins with a concise dependency analysis of backbone–adapter couplings and derives unified slicing rules for heads, channels, and kernels. It then scores structures via a Projected-Gradient Residual criterion that aligns upstream and downstream gradient subspaces, and restores accuracy with a dual-stream compensation scheme that alternates supervision on both data sources. The procedure runs in two stages: prune and recover adapters, then freeze adapters and prune and recover the backbone. Built on SAM-Med2D, ASSP uses only 20k images (0.4% of SA-Med2D-20M) yet reduces encoder parameters by over 75% and compute to about one quarter, while Dice typically stays within two points of the baseline. Under the same calibration budget it outperforms a transferred SlimSAM baseline and yields consistent latency and throughput gains on H20 GPUs. Although evaluated on medical data, the dependency modeling, PGR scoring, and dual-stream compensation are task-agnostic and broadly applicable to adapter-tuned models.", "tldr": "", "keywords": ["Structured Pruning", "Model Compression", "Vision Foundation Models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/edea921dccddec0194573c61f7630f1ea781c7e5.pdf", "supplementary_material": "/attachment/d800a04402e5029aae472ce662b9207bb0462a60.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a method to make large adapter-tuned SAM models more efficient by pruning unnecessary parts of the network. The proposed approach, ASSP, removes redundant parameters and at the same time try to keep accuracy (almost) unchanged. It works in two stages: First, simplify the adapters, then the backbone. Experiments on medical image segmentation show that the method cuts model size and computation by about 75% with only minor drops in performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses an important topic, namely improving the efficiency of large adapter-based segmentation models. The overall motivation is reasonable. The idea of pruning adapters and backbone in separate stages is intuitively sound, and the experiments demonstrate that some reduction in model size is possible without severe performance loss."}, "weaknesses": {"value": "The paper is poorly presented and difficult to interpret. Although it introduces many terms and claims to perform “structured pruning,” the underlying ideas are only loosely explained and seem largely heuristic. Key concepts, such as “dependency analysis” and “projected gradient residual,” are described in vague, abstract language without sufficient intuition or mathematical grounding. For example, statements like “We use dependency to mean the structured co-keep/co-prune constraint induced by operator dimension matching inside two-layer bottlenecks and by residual-add shape alignment across branches” are not clearly defined and make it hard to understand what is actually being done. Overall, the method appears ad hoc, with unclear motivation for its design choices. The experiments show modest gains, but given the lack of clarity and justification, it is difficult to assess the soundness or reproducibility of the approach.\n\nAnother weakness is that the framework is only applied to a particular model for medical images. Why not other models?"}, "questions": {"value": "Could the authors provide a clearer and more intuitive explanation of what is actually meant by “dependency analysis”? In particular, what does the “co-keep/co-prune constraint induced by operator dimension matching” practically imply for the pruning process?\n\nHow is the proposed “Projected Gradient Residual (PGR)” criterion fundamentally different from existing gradient- or magnitude-based pruning heuristics? A mathematical or empirical comparison would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ms5QADjICf", "forum": "5Rgn6x9jGn", "replyto": "5Rgn6x9jGn", "signatures": ["ICLR.cc/2026/Conference/Submission404/Reviewer_UGjS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission404/Reviewer_UGjS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662415552, "cdate": 1761662415552, "tmdate": 1762915512084, "mdate": 1762915512084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "qDW6q951H3", "forum": "5Rgn6x9jGn", "replyto": "5Rgn6x9jGn", "signatures": ["ICLR.cc/2026/Conference/Submission404/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission404/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763120255660, "cdate": 1763120255660, "tmdate": 1763120255660, "mdate": 1763120255660, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Adapter-aware Structured Sparsification (ASSP), to prune the adapter-tuned SAM. This paper proposes the Projected-Gradient Residual (PGR) score to measure the significance of each module in the adapter-tuned SAM. PGR guides the pruning strategy. This paper evaluated ASSP's performance on the SAM-Med2D test set, showing that ASSP's performance is comparable to the original SAM-Med2D's performance when 76% of the parameters are pruned."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposed the PGR score to measure the significance of each adapter-tuned SAM module. PGR computes the residual of the model's gradients projected on the upstream and downstream gradient plane. The larger residual, the more important that module. \n\nThe experimental results show the effectiveness of the proposed ASSP. By comparing with the original SAM-Med2D, when 76% of the parameters are pruned, ASSP's performance is still comparable to SAM-Med2D's performance. In addition, ASSP shows superior performance to the SlimSAM approach."}, "weaknesses": {"value": "Some parts of the papers are not clear or hard to understand. \n- The PGR scores are defined for the gradients. However, in each training iteration, the units' gradients could be different, and so are the computed PGR scores for the units. How to compute the final PGR scores for units? \n- In line 5 of Algorithm 1, what is the criterion to partition row indices into disjoint sets? \n- In Sec. 3.2, many concepts are hard to understand. For example, \"structured co-keep/co-prune constraint\", \"operator-dimension matching\", \"residual-add shape alignment\", \"Global dependencies\", \"global coupling\", and so on. \n\n\nIt's better to show some qualitative segmentation results."}, "questions": {"value": "In the experiments, does the proposed method ASSP start from the weights of the pretrained SAM-Med2D?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OBqxAhkP6N", "forum": "5Rgn6x9jGn", "replyto": "5Rgn6x9jGn", "signatures": ["ICLR.cc/2026/Conference/Submission404/Reviewer_4pHx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission404/Reviewer_4pHx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968767586, "cdate": 1761968767586, "tmdate": 1762915511919, "mdate": 1762915511919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce an adapter-aware structured pruning framework that incorporates a two-stage dual-stream compensation mechanism. In addition, they propose a Projected Gradient Residual criterion designed to align the gradient subspaces between upstream and downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors identify that the frozen backbone is more sensitive to pruning compared to the adapters.\n2. They propose an adapter-aware structured pruning framework tailored to this observation.\n3. They introduce a Projected Gradient Residual criterion to effectively fuse the upstream and downstream gradient subspaces.\n4. They design a two-stage dual-stream compensation scheme to enhance performance after pruning."}, "weaknesses": {"value": "1. The proposed method is restricted to adapter-tuned SAM models, while more widely adopted variants such as MedSAM are not supported. Since many SAM-based models do not employ adapters, this limitation raises concerns about the general applicability of the approach. The experiments are conducted solely on SAM-Med2D, further constraining the scope of validation.\n\n2. The authors are encouraged to evaluate their method on additional adapter-based SAM variants, such as 3DSAM-Adapter, MA-SAM, or MaskSAM, to demonstrate broader applicability. However, many adapter-tuned SAM models (e.g., 3DSAM-Adapter, MA-SAM, MaskSAM) already contain relatively few parameters, which may limit the potential benefits of additional pruning.\n\n3. Moreover, the experiments are limited to a single dataset, making it difficult to assess the method’s generalization across diverse data distributions. Finally, the absence of an ablation study for the proposed Projected Gradient Residual (PGR) criterion leaves its individual contribution to the overall performance unclear.\n\n[1] Gong, Shizhan, et al. \"3dsam-adapter: Holistic adaptation of sam from 2d to 3d for promptable medical image segmentation.\" arXiv e-prints (2023): arXiv-2306.\n[2] Chen, Cheng, et al. \"Ma-sam: Modality-agnostic sam adaptation for 3d medical image segmentation.\" Medical Image Analysis 98 (2024): 103310.\n[3] Xie, Bin, et al. \"MaskSAM: Towards Auto-prompt SAM with Mask Classification for Volumetric Medical Image Segmentation.\" arXiv preprint arXiv:2403.14103 (2024)."}, "questions": {"value": "1. Clarify whether the proposed method can be extended beyond adapter-tuned SAM models to more widely used variants such as MedSAM.\n2. Demonstrate how the approach would apply to SAM-based models that do not incorporate adapters.\n\n3. Evaluate the proposed method on additional adapter-based SAM variants (e.g., 3DSAM-Adapter, MA-SAM, MaskSAM) to validate its general applicability.\n\n4. Discuss the potential benefits of pruning in adapter-tuned SAM models that already contain few parameters.\n\n5. Extend the experimental evaluation to multiple datasets to assess generalization across diverse data distributions.\n\n6. Conduct an ablation study to isolate and quantify the contribution of the Projected Gradient Residual (PGR) criterion to the overall performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cqU1dmiYXj", "forum": "5Rgn6x9jGn", "replyto": "5Rgn6x9jGn", "signatures": ["ICLR.cc/2026/Conference/Submission404/Reviewer_B32k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission404/Reviewer_B32k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969108238, "cdate": 1761969108238, "tmdate": 1762915511826, "mdate": 1762915511826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Adapter-aware Structured Sparsification (ASSP), a structured pruning framework for adapter-tuned Segment Anything (SAM) models, i.e., SAM-Med2D. In detail, ASSP introduces a Projected Gradient Residual (PGR) scoring criterion that merges upstream (pretraining) and downstream (fine-tuning) gradient subspaces to identify redundant structures. The method performs pruning and compensation in two sequential stages: first the adapters, then the backbone, with a dual-stream training scheme that alternates supervision between pretraining and downstream datasets. Experimental results suggest that ASSP can significantly reduce compute while maintain Dice score performance, and outperform SlimSAM baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The dependency analysis is well-motivated and technically sound. \n\n2. The PGR criterion is a novel and theoretically justified improvement over traditional magnitude or Taylor-based pruning."}, "weaknesses": {"value": "1. The title of this paper seems to imply ASSP is a general method. However, this paper is limited to SAM-Med2D and medical imaging area. Thus, the effectiveness on other domains and architecture is unclear. \n\n\n2. This paper only compares to SlimSAM. If possible, other efficient SAM methods, not limited to medial area, should be included."}, "questions": {"value": "1. How does ASSP perform when applied to non-medical SAM variants (e.g., natural images or remote sensing)? Are the pruning dynamics consistent across domains?\n\n2. Can the authors quantify the additional computational cost of the PGR computation and dual-stream fine-tuning relative to standard pruning baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "I0k5MJ9WD8", "forum": "5Rgn6x9jGn", "replyto": "5Rgn6x9jGn", "signatures": ["ICLR.cc/2026/Conference/Submission404/Reviewer_xe5R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission404/Reviewer_xe5R"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150601840, "cdate": 1762150601840, "tmdate": 1762915511726, "mdate": 1762915511726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}