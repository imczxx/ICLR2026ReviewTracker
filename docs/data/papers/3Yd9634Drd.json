{"id": "3Yd9634Drd", "number": 20225, "cdate": 1758303874138, "mdate": 1759896989735, "content": {"title": "Noisy Quadratic Models of Scaling Dynamics", "abstract": "Pre-training scaling laws describe the best training decisions under resource constraints. The discovery of new laws is a demanding exercise, as each decision requires a separate law. An alternative is to model the scaling dynamics of LLMs directly, then use those models as surrogates for multiple decisions. Yet, most theoretical models of scaling dynamics cannot be fit to scaling data easily. In this paper, we introduce the Noisy Quadratic System (NQS), a fittable relative of the theoretical models that can generate new scaling laws. We also identify some key failure modes in the theoretical models, and further extend the NQS to correct for these deficiencies. In our experiments, our best model, fit on small-scale runs, closely predicted the performance of runs near critical points, which Chinchilla failed to do. Finally, the NQS is the first practical scaling model to include a variance term, which allows us to model the effect of batch size. Because of this, it may help practitioners configure training under many resource constraints, including compute, but also time and memory.", "tldr": "", "keywords": ["scaling laws", "compute-optimal pre-training", "noisy quadratic model", "Chinchilla", "batch size", "large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7483529e4b913b5f9fa0196b8223f6d9627f850d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates more sophisticated theory-inspired scaling law models for LLM training that can predict the loss over the batch size, model size and dataset size. The addition of the batch size is an improvement over prior scaling laws like those from Chinchilla. Their model, the Noisy Quadratic System (NQS) is based on modelling LLM training as stochastic gradient descent (SGD) on an infinite-dimensional quadratic loss function, projected onto its top N eigendirections. On top of this theoretically inspired model, the authors propose two more empirical adjustments, creating NQS++. The Effective Model Size adjust the parameter count to improve the overall fit, and Learning Rate Adaptation which simulates changing the learning rate which is needed to fit small batch sizes better. Experiments are performed on Pythia-style models and claim improvements over prior approaches like Chinchilla, for example when predicting test losses on out-of-sample compute budgets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "* Aims to bridge theory and practice, from purely empirical scaling laws to theoretical models that are hard to fit.\n* Incorporation of the batch size which prior scaling law approaches had a hard time with.\n* Strong empirical performance on their experiments.\n* The authors show how to make their scaling models computationally efficient which they are otherwise not when using naive approaches.\n* Upfront about limitations."}, "weaknesses": {"value": "* The paper feels poorly presented. The authors assume the reader is deeply familiar with related literature and frequently cite it without summarizing. There are also numerous references to the appendix without summarization. Overall the manuscript does not feel very self-contained and is likely relatively inaccessible to a large fraction of the community. I think the work could be significantly improved by better explaining the core concepts at a level that is accessible to a broader audience. Without this the impact of the work is significantly reduced despite presenting interesting and promising ideas.\n* Reliance on additional empirical fixes. The core theoretical model does not seem to be powerful enough to model the dynamics of LLM training accurately. The authors introduce additional tricks to deal with this but that deviates from the principled theoretical approach that inspired the paper.\n* The experimental approach has some key limitations. Making general claims about batch size optimality without tuning the learning rate for each batch size is not principled. The same goes for the beta2 and to a lesser degree the beta1 coefficients of Adam. The approach does not seem to generalize across learning rates or optimizers as the authors acknowledge in their limitations."}, "questions": {"value": "The claims about batch size optimality seem limited by the fixed LR experimental setup. How would you expect the NQS parameters to behave in a more realistic setting where the learning rate is properly co-tuned with the batch size? Would the LRA fix still be necessary in that case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "98BQ30QKPd", "forum": "3Yd9634Drd", "replyto": "3Yd9634Drd", "signatures": ["ICLR.cc/2026/Conference/Submission20225/Reviewer_GEC9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20225/Reviewer_GEC9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653802441, "cdate": 1761653802441, "tmdate": 1762933722845, "mdate": 1762933722845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Noisy Quadratic System (NQS), a parametric model for simulating the scaling dynamics of LLM losses during pre-training. By extending theoretical noisy quadratic models, NQS incorporates model size, batch size, and training steps, enabling predictions of optimal training configurations under various resource constraints like compute, memory, and time.\n\nTo improve predictive accuracy, they also introduce NQS++, incorporating Effective Model Size (EMS) to correct for loss curvature discrepancies and Learning Rate Adaptation (LRA) to adjust for overestimation at small batch sizes. Experiments on small-scale Pythia models show that NQS++ can extrapolate to compute budgets 64× larger than training data, outperform Chinchilla, explaining 90% of batch size variance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposal tries to bridge theoretical scaling models (Zhang et al., 2019; Bahri et al., 2021) with practical applicability, allowing a single model to handle multiple decisions like batch size optimization, which Chinchilla ignores. Novelty arises from making NQS fittable via simplifications (e.g., deterministic projections) and numerical approximations (e.g., Euler-Maclaurin for sums), plus extensions like Effective Model Size (EMS) and Learning Rate Adaptation (LRA) to fix observed mismatches in curvature and small-batch behavior. \n\nExperiments are rigorous, using IsoFLOPs and IsoTokens datasets for fitting, with clear metrics (e.g., additional variance explained ≥85-90% out-of-sample). The inclusion of a variance term for batch size effects is another key contribution for practitioners under time/memory constraints, and the model's extensibility (e.g., to batch schedules) adds impact."}, "weaknesses": {"value": "The core quadratic assumption, while simplifying inference, may not capture real LLM dynamics fully, as the paper also acknowledges. Experiments are limited to small models (<500M parameters) and a specific architecture, the contribution could be strengthened if discussions/experiments are conducted to address generalization to frontier-scale LLMs, multi-epoch training, or diverse optimizers/datasets. \n\nThe single-epoch noise independence might overlook data reuse effects seen in recent works (Lin et al., 2025a). Fitting relies on non-convex optimization with parallel initializations, but lacks sensitivity analysis to hyperparameter choices (LRA tolerance). While outperforming Chinchilla, baselines are limited; comparisons to more recent batch-aware scaling laws (Bi et al., 2024; Bergsma et al., 2025) could strengthen claims."}, "questions": {"value": "How does NQS generalize to multi-epoch training or data reuse, given the independence assumption in noise? Could you discuss extensions or limitations relative to works like Lin et al. (2025b)?\n\nFor LRA, the greedy approximation adds compute—how does its cost scale with K, and could you compare to full adaptation empirically?\n\nThe paper mentions failure modes; in what regimes (high compute) does NQS deviate most from LLMs, and how might this inform future extensions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x2c6nJwvXf", "forum": "3Yd9634Drd", "replyto": "3Yd9634Drd", "signatures": ["ICLR.cc/2026/Conference/Submission20225/Reviewer_maGC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20225/Reviewer_maGC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957329701, "cdate": 1761957329701, "tmdate": 1762933722304, "mdate": 1762933722304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose the Noisy Quadratic System (NQS), a fittable, 6-parameter model of scaling dynamics inspired by theoretical quadratic models, to model scaling laws for LLMs. The NQS models the final loss $L(N, B, K)$ by decomposing it into three terms: an approximation error $E_{app}(N)$, a bias term $E_{bias}(N,K)$, and a variance term $E_{var}(N,B,K)$, where $N,B,K$ are model size, batch size, and training steps. To make NQS work well, the authors extend it to NQS++ by adding two empirical components, \"Effective Model Size\" (EMS) and \"Learning Rate Adaptation\" (LRA), which correct for mismatches between the simple quadratic model and real LLM losses.\n\nThe central claim is that NQS++, when fit on small-scale experiments, can accurately extrapolate to larger compute budgets where the Chinchilla model fails. Because NQS++ models $B$ and $K$, it can be used to optimize for complex, compound constraints (e.g., compute+memory) and even rank the performance of different batch size schedules."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Originality: The paper's main contribution is essentially bringing together two directions: the practical, empirical scaling-law fitting (like Chinchilla) and the theoretical, mechanistic models of scaling. It bridges those two with noisy quadratic models, which were successful in the NN optimization literature before. By making a simplified theoretical model fittable, the authors create a \"semi-mechanistic\" surrogate that is more powerful and flexible than a simple power-law formula.\n\n* Quality and significance: The paper is generally done solid, with lots of details and derivations in the appendix. If the model's utility holds, it would allow better extrapolation, application to real-world constraints (like memory), or better understanding of scaling dynamics. The fitting results seem compelling. Moreover, the ability to evaluate batch size schedules (Fig 4) is a novel capability that, to my knowledge, is new.\n\n* Clarity: The main ideas are relatively clearly conveyed by the paper (though I have some comments below). The experimental design, built around IsoFLOPs and IsoTokens setups, provides a clear framework for evaluating the model's interpolation and extrapolation capabilities, with well done figures."}, "weaknesses": {"value": "* The \"++\" extension feels ad-hoc: The base NQS model, which is principled, basically fails (Table 2). The model's success is entirely dependent on the NQS++ extensions, which are essentially empirical patches. The EMS $N_{eff}=(AN)^r$ is just a new power law, and LRA is a greedy algorithm based on a hypothesis about normalization layers. This reliance on curve-fitting \"patches\" somewhat contradicts the \"mechanistic\" claim of the model.\n\n* Related to the above, applying the model (judging purely from the text) seems rather complex with the extensions. What made the empirical scaling laws (like Chinchilla) so attractive is also their simplicity. Perhaps this can be remedied if the authors release an easy to use package for future use.\n\n* Experimental scope: The authors admit this, but it's one of the obvious weaknesses. All claims are based on one model family on one dataset at a small scale ($<10^{19}$ FLOPs). The claim that Chinchilla \"overfits\" is hard to justify; how much effort was put into working on and improving the Chinchilla fits? Similar efforts to making the NQS and its extension work? It's also possible that the Chinchilla functional form is not a good fit at this scale or would fit differently with other setups, and the arguably more complex NQS++ model is just a better fit here.\n\n* Writing: It's clear upon reading the paper that the authors are knowledgeable and have worked a lot on this project. However, this may have also lead to too dense information and a lack of flow. For example, the introduction contains lots of paragraphs concatenated together, before any of the core concepts are introduced formally, with a sort of early conclusion. Then throughout, some key points emerge abruptly; providing more explicit framing would enhance readability. For example, the NQS extension or computational complexity seemingly come out of nowhere (e.g., since experiments only follow 2 pages after). In short, the writing would strongly benefit from transitional sentences, paragraphs, or overview descriptions."}, "questions": {"value": "Perhaps all questions are in the section above, but to potentially more direct: How much effort was put into making Chinchilla work better, i.e., how 'tuned' is the baseline? Similarly, how much trial and error was involved in the EMS and LRA extensions? \n\nMoreover, NQS++ must be tuned on a validation set. This validation set uses runs \"at least 4 times larger\" than the training runs. How sensitive is the final model's accuracy to the choice of this validation set? Does this requirement partially negate the \"train on small-scale\" promise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GaH2N3aHSF", "forum": "3Yd9634Drd", "replyto": "3Yd9634Drd", "signatures": ["ICLR.cc/2026/Conference/Submission20225/Reviewer_GytH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20225/Reviewer_GytH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117759580, "cdate": 1762117759580, "tmdate": 1762933721629, "mdate": 1762933721629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Noisy Quadratic System (NQS and NQS++), which are new functional forms for LLM scaling laws as a function of model size, batch size, and training steps, and can use dataset size, training flops, and peak memory as constraints. Their models act as an alternative to the Chinchilla scaling model (Approach 3) or other scaling laws based on high-dimensional linear regression / random features models. In particular, NQS includes a variance term intended to capture batch size effects. NQS++ extends the basic NQS by adding two empirically-motivated extensions to improve curvature modeling and small batch behavior.\n\nThey assume the LLM optimizes a quadratic loss function with power-law distributed eigenvalues and noise, giving an expression with three terms (approximation error, bias and variance) and six fitted parameters.\n\nThey evaluate on Pythia-style models up to 500M parameters trained on OpenWebText2. NQS++ outperformed Chinchilla on explaining the variance on test sets with a large (64x) compute gap from training data, and explained variance due to batch size changes (which Chinchilla can’t do at all). NQS++ was also predictive of close-to-optimal configurations under various resource constraints (time, memory, data) not just compute-optimal. Chinchilla overfit on their scaling dataset whereas NQS++ extrapolated well."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This is a worthwhile problem to tackle: the functional form of the Chinchilla scaling laws is almost certainly wrong (the asymptotics don’t make sense), and fails to capture batch size effects."}, "weaknesses": {"value": "I found the motivation in the paper for the particular functional form in NQS/NQS++ hard to follow. Models with additional terms and more fitted parameters are likely able to fit scaling data better than simpler models like Chinchilla, but the evidence is somewhat weak that this functional form is worth the additional complexity and likely to generalize well. The experiments were done with constant learning rates and on relatively small compute scales (<10^19 flops) which makes it hard to tell how practical the results are."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P9u12sLvlr", "forum": "3Yd9634Drd", "replyto": "3Yd9634Drd", "signatures": ["ICLR.cc/2026/Conference/Submission20225/Reviewer_8D3C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20225/Reviewer_8D3C"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762268039936, "cdate": 1762268039936, "tmdate": 1762933720876, "mdate": 1762933720876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}