{"id": "IAFwK6NyrP", "number": 20452, "cdate": 1758306303747, "mdate": 1763652313240, "content": {"title": "The Counting Power of Transformers", "abstract": "Counting properties (e.g. determining whether certain tokens occur more\n    than other tokens in a given input text) have played a significant role in \n    the study of expressiveness of transformers.\n    In this paper, we provide a formal \n    framework for investigating the counting power of transformers. We argue \n    that all existing results demonstrate transformers' expressivity only for \n    (semi-)linear counting properties, i.e., which are expressible as a \n    boolean combination of linear inequalities. \n    Our main result is that transformers can express counting properties that\n    are highly nonlinear. More precisely, we prove that transformers can\n    capture all semialgebraic counting properties, i.e., expressible as \n    a boolean combination of arbitrary multivariate polynomials (of any degree).\n    Among others, these generalize the counting properties that\n    can be captured by support vector machines via polynomial kernel in the \n    vector space model.\n    To complement this result, we exhibit a natural subclass of (softmax) \n    transformers that completely characterizes semialgebraic counting \n    properties. \n    Through connections with the\n    Hilbert's tenth problem, this expressivity of transformers also \n    yields a new undecidability result for analyzing an extremely simple \n    transformer model --- surprisingly with neither positional encodings \n    (i.e. NoPE-transformers) nor masking.\n    We also experimentally validate trainability of such counting\n    properties.", "tldr": "Transformers can express highly nonlinear counting properties", "keywords": ["FLaNN", "expressiveness", "attention", "formal languages"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0790eac9e326016a97210c75b0349e10f4647af0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work analyzes the ability of Transformer based models to performing higher order forms of counting (e.g. counting if the product of three variables is larger than some threshold). The authors analyze the counting abilities of Transformers under various assumptions about the self-attention layer and analyze how the counting abilities of models with different self-attention assumptions relate to one another in terms of expressivity.\n\nThe main theoretical result of the paper shows that softmax attention transformers can express any counting property which can be expressed as a boolean combination of multivariate polynomial inequalities. For AHAT, the authors provide an exact characterization showing that the set of counting languages accepted by AHAT is equivalent to the set of semi-algebraic languages.\n\nThe authors complement this with empirical results showing that models can easily learn to perform classification on the set of languages of the form $|w|_b \\leq |w|_a^k$ for k from 1 to 5.\n\nI think the claims made by the paper are very interesting and meaningfully advance the field of Transformer expressivity. Most of my feedback is minor details which I think could help improve the exposition of the theoretical results However, I quite like this paper and thus have given it an 8."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The main claims are interesting and novel. Showing that softmax attention transformers can recognize semi-algebraic counting languages and giving characterization of the subclass meaningfully advances our theoretical understanding of Transformer counting power\n- Paper is well written. Motivation of the theoretical problem considered is given. In the sections exposing the framework and the theoretical results, examples are given which is nice. \n- At a high level, the theoretical approach taken is clean and convincing. The authors also show an interesting inexpressibility result for PARITY as well as undecidability/universality Theorems (1.3 and 1.4). The argument for two layer networks is also appealing."}, "weaknesses": {"value": "* The main weakness of the paper for me is the exposition. I feel many things could be made clearer or more rigorous in the proofs/in the formalization of the problem. There are many occurrences of variables which are  referred to in text or in equations without properly being defined. I think this hinders the readability of the paper.\n* I also think the experiments are quite limited. It would be interested to see evaluation on languages that are not of the form of $L_k$.  I also wish the authors would have tested on languages with different vocabulary sizes to see how this affects performance. However, the theoretical contribution is, in my opinion, strong enough that this is not a major shortcoming to the paper's contribution."}, "questions": {"value": "**General Questions**\n\n- Do we know if, in terms of counting, there are languages in SMAT but **not** in SemiAlg?\n- Could the authors give somewhere a more formal/rigorous definition of the sets AHAT, AHAT[U] and SMAT in the context of semi-algebraic language recognition? No formal definition of these sets are given, but many of the main results hinge on inclusions between these and SemiAlg.\n- Could you give a more precise theorem statement for Thm 1.1? Which class of Transformers are concerned by this?\n- You do not use multi-head attention in your definition of the Transformer. Is there a reason for this?\n- I would like for asymptotics (in terms of number of layers/width/number of heads) to be clearly stated either in or around the definition of the theorems. Currently, the only place where number of layers is referred to is in the paragraph discussing the reduction to two layers argument. I found this to be late in the text to introduce these.\n\n**Minor Comments/Clarification Questions**\n\n- For the proof of Prop 3.1\"(each consisting of one uniform layer and several ReLU layers)\" How many RELU layers? At least state big O.\n- The argument for PARITY in the appendix should be introduced in a proof environment with an associated Proposition/Lemma/Theorem.\n- What is $d$ line~ 633?\n- What are PI and RE languages ~line 382?\n- The titles for the columns/lines in Figure 1 could have clearer names."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zayq0bDB8c", "forum": "IAFwK6NyrP", "replyto": "IAFwK6NyrP", "signatures": ["ICLR.cc/2026/Conference/Submission20452/Reviewer_Fyg9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20452/Reviewer_Fyg9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766229876, "cdate": 1761766229876, "tmdate": 1762933893577, "mdate": 1762933893577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper theoretically investigates the capabilities of encoder-only Transformers to recognize formal languages. Specifically, it focuses on languages characterized by polynomial inequalities based on occurrence counts. The authors leverage this analysis to present a related undecidability result for Transformers. The theoretical findings are empirically validated on the language class $L_k = \\{ w\\in \\{a,b\\}^+: |w|_b \\leq |w|_a^k \\}$."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary contribution is extending prior work on the counting abilities of Transformers. By generalizing the analysis from linear functions of occurrence counts to polynomial functions, the work advances our theoretical understanding of Transformer capabilities in this domain."}, "weaknesses": {"value": "1. Given the theoretical nature of the paper and the numerous formal language concepts, the presentation would be significantly clarified by adding a Venn diagram. This could visually situate the language classes considered in this work relative to each other and to prior art, making the precise scope of the contribution more apparent.\n\n2. The analysis is confined to encoder-only Transformers. While this is a valid methodological choice, the paper would have broader impact if it discussed the implications of these findings or extended them to autoregressive models, which are prevalent in modern applications.\n\n3. Theorem 1.1 establishes the expressiveness of Transformers for semialgebraic counting properties. However, this claim would be much stronger with a corresponding analysis of the required model size (e.g., depth, width, or number of heads) as a function of the complexity of the counting problem. For instance, do the required Transformers scale polynomially or exponentially with the degree of the polynomial inequalities?\n\n4. The empirical validation of this paper is limited to the single language class $L_k$. The paper's value could be enhanced by either broadening the experimental scope to other complex formal languages or by discussing the potential implications of these counting properties for more conventional NLP tasks, even at a high level."}, "questions": {"value": "1. Could the authors elaborate on the size requirements (e.g., depth, width) for the Transformers needed to express the counting properties discussed in Theorem 1.1 and other results? Specifically, how does the required model size scale with the parameters of the formal language (e.g., the degree of the polynomial)?\n\n2. The current experiments are focused on $L_k$. Are there other language classes the authors considered that would serve as interesting and challenging testbeds for this theory?\n\n3. To help bridge the gap between this theory and practice, could the authors speculate on any practical NLP tasks (e.g., in semantic parsing, logical reasoning, or program synthesis) where these specific polynomial counting abilities might be relevant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RtqzZw2r4f", "forum": "IAFwK6NyrP", "replyto": "IAFwK6NyrP", "signatures": ["ICLR.cc/2026/Conference/Submission20452/Reviewer_EZ5n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20452/Reviewer_EZ5n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772314168, "cdate": 1761772314168, "tmdate": 1762933892640, "mdate": 1762933892640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a theoretical analysis on transformers’ ability to perform counting operations. It is shown that transformers can capture all semialgebraic properties, meaning they can evaluate polynomials where variables are characterized by symbol counts (for instance, accepting strings s.t (#a)^2 + (#b)^3 > 0). The proof mainly consists of demonstrating that average-hard attention transformers (AHATs) with no positional embeddings exactly characterize semialgebraic sets. As a direct corollary, they are able to show that AHATs with no PEs cannot recognize Parity. Furthermore, by connecting this result with the MRDP theorem (which states that there is no algorithm that determines whether some given Diophantine has a solution), they provide an undecidability result for transformers: it is undecidable to determine whether their language is empty or not."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The perspective on semialgebraic sets rather than semilinear sets is novel and generalizes prior results on the counting power of transformers\n2. The corollary on inexpressibility of Parity is interesting and accompanies a rich body of work tackling this question\nNovel tools to me such as semialgebraic sets and Parikh images were introduced well enough for me to understand the technical parts of the paper\n3. Introduction is well written and puts in perspective previous work on semilinear counting with transformers"}, "weaknesses": {"value": "1. Importantly, this paper disregards the impact of precision. In the finite-precision regime (which describes transformers used in practice), it is impossible to store counts from uniform attention for any input string. Recently, the expressive power of fixed-precision transformers (SMATs and AHATs) has already been characterized by a subclass of regular languages [Li and Cotterell, 2025] (and therefore can not perform counts across all possible strings), undermining the relevance of the paper’s main claim on counting. I would recommend at least mentioning this inherent limitation.\n2. The experiments consist of training transformers on a single type of polynomial (comparing (#b) and (#a)^k) rather than a more diverse set of polynomials with different coefficients of different degrees. Experiments on polynomials with a larger alphabet, different coefficients with different degrees would consolidate the claims of the paper. Even better, training on randomly sampled polynomials would be a great contribution."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QxkOwmutuc", "forum": "IAFwK6NyrP", "replyto": "IAFwK6NyrP", "signatures": ["ICLR.cc/2026/Conference/Submission20452/Reviewer_2cZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20452/Reviewer_2cZ9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848821072, "cdate": 1761848821072, "tmdate": 1762933892041, "mdate": 1762933892041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the counting power of Transformers.\nWhile prior work on Transformer expressiveness has shown that Transformers possess semilinear counting properties, this paper demonstrates that Transformers can express all semialgebraic counting properties.\nIt also shows that Average Hard Attention Transformers (AHAT) without positional encodings (PEs), as well as their subset AHAT[U] that uses only uniform layers, precisely capture semialgebraic counting properties."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Understanding the expressive power of Transformers is an important research topic.  \n  This paper presents new results on counting capabilities that were not clarified in previous work.\n- The paper is concise and readable.\n- The theoretical results are supported by experiments.\n- Although I had some questions, the theoretical part appears mostly correct."}, "weaknesses": {"value": "- It is unclear how practical it is to apply Transformers to nonlinear counting problems as discussed in this paper.  \n  For sequences such as text, which Transformers typically handle, input order is important, and tasks are generally not permutation-invariant.  \n  Therefore, studying permutation-invariant input properties may have limited practical relevance.  \n  As the authors discuss in the paragraph beginning at Line 263, combining counting properties with other characteristics is interesting.  \n  However, it is not clear whether nonlinear counting is necessary in such cases.  \n  In fact, the use case shown at Line 264 can be realized with linear counting.\n\n- The proof of the main result, Proposition 3.1, seems somewhat trivial.  \n  The idea in Step I, computing the frequency via a Transformer layer, has already appeared in prior work such as Yang & Chiang (2024).  \n  Step II, which performs multiplication, also appears straightforward.\n\n\n## Minor comments\n\n- Line 22: properties.. => properties.\n- Line 161: (a.k.a. Parikh map => missing ')'\n- Line 180: I think the fourth vector should be (0,0,1, 1/4) if we use the one-hot embeddings.\n- Line 190: (x_1, \\ldots, x_n) => (x_1, \\ldots, x_m)? \n- Line 205: duplicated ':='.\n- Line 213: x_j \\tau => x_j / \\tau\n- Line 227: duplicated ','."}, "questions": {"value": "- Are there practical situations where nonlinear counting is required for tasks usually solved with Transformers?\n- Is it possible to define a new class of language, e.g., LTL with nonlinear counting?\n- I could not understand the paragraph beginning at Line 355.  \n  Regarding the arbitrary choices among the $2^{r+(m+1)^2a}$ options representable by AHAT, why is it “easy to do” to construct the polynomial expression of $u\\_{\\ell, i}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z8JaezWILK", "forum": "IAFwK6NyrP", "replyto": "IAFwK6NyrP", "signatures": ["ICLR.cc/2026/Conference/Submission20452/Reviewer_XAw7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20452/Reviewer_XAw7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983125123, "cdate": 1761983125123, "tmdate": 1762933891261, "mdate": 1762933891261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper formally studies the counting power of transformers, i.e., which counting properties of the input can transformer models represent. The authors substantially expand on prior understanding, showing that the answer goes well-beyond previously studied (Boolean combinations of) linear properties, to (Boolean combinations of) polynomial properties of the number of occurrences of various tokens in the input. Along the way, they also define and motivate counting properties, and provide a small empirical confirmation of the findings. Towards the end, they make connections to universality and undecidability of certain classes of transformers."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* Well-motivated and nicely written paper. While technically strong, its pitch and claims will also be accessible to broader audience not deep into theoretical research.\n\n* I liked the structure of the paper and how it (quickly) conveys the findings and uses many examples to explain concepts.\n\n* That the studied transformer model uses simple (or no) position encodings and the standard softmax attention (either directly or through AHAT[U] which is a special case of softmax) is a positive, especially compared to some efforts that have relied to unrealistically complex position embeddings and other design choices in their analysis.\n\n* The main result about transformers being able to capture counting properties well beyond (combinations of) linear properties is a strong technical advance. Having an empirical support for it is a plus.\n\n* The connections to undecidability are intriguing, though I must say the corresponding sections can use some more clarity of exposition."}, "weaknesses": {"value": "* While it's valuable to have an **empirical validation**, I felt that section 6 is not as well described and discussed as the rest of the paper. E.g., even the metrics mentioned in the caption of Fig 1 need clarification.\n\n* There are some places where it would be valuable to state which **design decisions / assumptions / choices** play an important role. E.g., it seems to me that Prop 4.1 (that whatever NoPE-AHAT can compute is expressible semi-algebraically) relies on the assumption of ReLU as the non-linearity; I suspect it will also work with any polynomial non-linearity, but not with sigmoid, inverse-tangent, or other choices that have been used in practice. (And this is fine, I just think it's better if the authors can call it out.)\n\n* The paper can also use a clearer treatment of the **datatype** assumed for the transformer model. Section 2.1 starts with *real* vectors, which clearly isn't realistic, at least for arbitrary precision. Later, the ReLU paragraph switches to *rationals*, leaving some lack of clarity. The choice of datatype -- and importantly the *precision* (how many bits are allocated, and whether they depend on the input length $n$ -- is an important consideration in transformer expressivity results, but seems to be lacking here. E.g., when, in the proof of Prop 3.1 (that semi-algebraic counts can be expressed by NoPE-AHAT[U]) the transformer is computing $u_p[j] \\in [0,1]$, this is presumably expressed as a rational where the numerator (and denominator) can grow very quickly. Their size should be bounded in terms of $\\ell$ and $n$, though. It would be helpful to get some clarity on this front.\n\n* Some of the proof ideas can use a short discussion of the **intuition**. E.g., in the proof of Prop 3.1, we are multiplying two integers (represented as rationals). A priori, it's unclear how a transformer might be able to do so! The key observation, I believe, is that one of the items being multiplied, namely $x_i$, is always a *count* of letters in the input word, and thus distributed across the input as captured by $u_p[i]$ being 0 or 1 at various positions $p$. Thus, one can get around the usual difficulty of multiplication by instead multiplying $y_i$ with a 0 or a 1 at each position, ensuring that there are exactly $x_i$ positions with a 1, and then using attention to aggregate. Is this the right intuition? In any case, adding some intuition would help."}, "questions": {"value": "Please see the comments above about the (implicit) assumptions on the datatype and precision, and on intuitions. Clarity on any of these fronts would be great to have!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yf1xZImhqm", "forum": "IAFwK6NyrP", "replyto": "IAFwK6NyrP", "signatures": ["ICLR.cc/2026/Conference/Submission20452/Reviewer_sqAV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20452/Reviewer_sqAV"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762386757404, "cdate": 1762386757404, "tmdate": 1762933890381, "mdate": 1762933890381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (1/2)"}, "comment": {"value": "We are grateful to all reviewers for raising these important questions, we believe they are important to improve the presentation. We will definitely add these clarifications to the revised version.\n\n\n> Question: Are there practical NLP tasks that might require nonlinear counting?\n\nThank you for this question. We have added our clarification below in Section 7.\n\nSince (due to our results) transformers are able to perform any polynomial counting, it follows that they can also approximate any continuous function of the number of occurrences of tokens (the set of polynomials is the universal approximator by the Weierstrass theorem). This might be useful in practical NLP tasks that require computation of nonlinear statistics in the word frequencies. In the paper, we have mentioned its relationship to the so-called *Vector Space Model (VSM)* for text classification and similarity analysis (a classic topic in information retrieval), where the standard method has been to employ Support Vector Machines (SVM), together with kernel analysis (e.g. using polynomial kernels). Our results imply that transformers are expressive enough to perform such tasks (in fact, we showed the added expressivity). We further clarify it here and added this to the new version. \n\nIn VSM (see Chapter 10 of the cited book by Shawe-Taylor and Cristianini (2004)), certain structures are dropped; most commonly, the word ordering - that is we obtain a bag of words. That is, a document $D$ is abstracted into a vector $v_D$ indexed by \"terms\" (could be words, sentences, etc.) that may occur in $D$. We assume that the set $T$ of terms is finite. That is, $v_D[t]$ is a count on the number of occurrences of $t$ in $D$. If we want to compare similarity between two documents $D,D'$, we may consider the Euclidean distance between $v_D$ and $v_{D'}$, which requires a polynomial. Another non-linear distance that is commonly used in this setting is the cosine distance, which can be well approximated by polynomials. In addition, there are often challenges including \"related terms\" (e.g. husband, wife, and spouse), which are missed when we only use the aforementioned metrics. For this reason, a similarity measure is often learned (see Section 10.2.2 in the cited book by Shawe-Taylor and Cristianini (2004), where SVM is used in combination with kernel analysis like polynomial kernels). Our results essentially show that transformers can solve such a task of identifying text similarity.\n\nA related task is the problem of determining proximity to a human written text, as dictated by the famous Zipf law stating that the frequency of the $k$-th most frequent word is proportional to $1/k$ in a natural language. Following the VSM approach above, we compare using Euclidean distance (or cosine distance) a document $D$ with a predetermined Zipf-vector. This is in general a polynomial expression, and our results show that this can also be captured by transformers.\n\n\n> Question: Usage of unbounded precision is unrealistic.\n\nOn inputs of length $n$, we require just $O(\\log n)$ bits of precision. Curiously, to see that, one does not even look at the construction because this is an inherent feature of the NoPE-AHAT model. Namely, numbers in every NoPE-AHAT transformer on inputs of length n can be represented as fractions of $O(\\log n)$-bit integers.\n\nThis is because every number can be computed in $O(1)$ additions, multiplications and divisions, starting from $x_1, …, x_m \\in \\\\{0, 1, …, n\\\\}$ – numbers of occurrences of the letters of our alphabet $\\Sigma = \\\\{a_1, ..., a_m\\\\}$ in a given $n$-length word. This follows from an observation, made in the proof of Proposition 4.1 – after any number of layers, positions with the same input letter have the same value. That is, as in the proof of Proposition 4.1, the values of the transformer after attention layers are given by $m + 1 = O(1)$ numbers $u_{l,0}, …, u_{l,m}$ (note that $m$ is the size of the alphabet and hence is constant). To compute numbers on the next attention layer, we need to compute averages of values for positions where the attention score is maximized. In any position on the next level, it can be maximized on some subset $S \\subseteq \\\\{0, 1, .., m\\\\}$. Then the corresponding average can be computed as the following expression, requiring just $O(1)$ arithmetic operations (for that, note again that we are summing over a set $S$ of size at most $m +1 = O(1)$):\n\n$(\\sum_{i \\in S} x_i \\cdot  u_{l,i}) / (\\sum_{i \\in S} x_i )$\n\nAfter that, to compute the final values in the next layer, in every position we need to perform just a constant number of arithmetic operations of our fixed ReLU network for that layer. Moreover, all expressions needed to evaluate attention scores are quadratic polynomials in the numbers of the respective layer; hence they also just require denominators with O(log n) bits."}}, "id": "MIEh1whIqs", "forum": "IAFwK6NyrP", "replyto": "IAFwK6NyrP", "signatures": ["ICLR.cc/2026/Conference/Submission20452/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20452/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission20452/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763652922856, "cdate": 1763652922856, "tmdate": 1763653117337, "mdate": 1763653117337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (2/2)"}, "comment": {"value": "> Question: What is the size of our model in Proposition 3.1?\n\nFirst, we want to note that all models we are studying in our paper have only one attention head. We added a remark to the definition in Section 2.1.\n\nThe embedding dimension and the number of layers of our transformer in Proposition 3.1 depends on the degree $d$ of the polynomial $p$ from the input and the number $M$ of monomials in $p$.\nFirst, it is easy to see that we need at most $O(d)$ layers – essentially, each layer to increase the degree of monomials that we have already computed by 1. In a fine-grained analysis in the appendix, we showed that polynomials of degree $d$ are accepted by NoPE-AHAT[U] with at most $d$ attention layers (see Proposition A.1).\n\nIn turn, the embedding dimension in our construction can be bounded by $O(dM)$, – we store the value of each monomial that we need to compute in a separate dimension, we need to compute $M$ monomials of $p$, but computing a monomial $x_{i_1}... x_{i_d}$ takes computing $d$ ``sub-monomial’’, so the dimension bound is multiplied by $d$.\n\nWe also added a remark at the end of Section 3 about the size of the constructed AHAT."}}, "id": "PuPe3ZJVaW", "forum": "IAFwK6NyrP", "replyto": "IAFwK6NyrP", "signatures": ["ICLR.cc/2026/Conference/Submission20452/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20452/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission20452/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763652970581, "cdate": 1763652970581, "tmdate": 1763652970581, "mdate": 1763652970581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}