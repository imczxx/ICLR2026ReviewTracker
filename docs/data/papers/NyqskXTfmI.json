{"id": "NyqskXTfmI", "number": 11925, "cdate": 1758204674344, "mdate": 1759897545757, "content": {"title": "ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking", "abstract": "Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation (RAG) systems by injecting adversarial content into knowledge bases, tricking Large Language Models (LLMs) into producing attacker-controlled outputs grounded in manipulated context. Prior work highlights LLMs' susceptibility to misleading or malicious retrieved content. However, real-world fact-checking scenarios are more challenging, as credible evidence typically dominates the retrieval pool.\nTo investigate this problem, we extend knowledge poisoning to the fact-checking setting, where retrieved context includes authentic supporting or refuting evidence. We propose \\textbf{ADMIT} (\\textbf{AD}versarial \\textbf{M}ulti-\\textbf{I}njection \\textbf{T}echnique), a few-shot, semantically aligned poisoning attack that flips fact-checking decisions and induces deceptive justifications, all without access to the target LLMs, retrievers, or token-level control.\nExtensive experiments show that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4 cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\\% at an extremely low poisoning rate of $0.93 \\times 10^{-6}$, and remaining robust even in the presence of strong counter-evidence. Compared with prior state-of-the-art attacks, ADMIT improves ASR by 11.2\\% across all settings, exposing significant vulnerabilities in real-world RAG-based fact-checking systems.", "tldr": "", "keywords": ["Knowledge Poisoning", "LLMs", "Fact-Checking", "Retrieval-Augmented Generation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e9e5298ae9ccce311d19e3b1f478b9a8c979a826.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Authors propose a pipeline for attacking automatic fact checking systems in the setting that the attacker sees the system as a black box, and is allowed to  inject a small set of false statements in the retrieved evidence. Given a claim, a set of relevant evidence is obtained (either from the web, or synthetically), then an LLM is used to generate a piece of false evidence based on the obtained relevant evidence. The the claim is added as a prefix to the false evidence and inserted into the corpus to be retrieved by the fact checking system.\n\nThe pipeline is evaluated in four datasets, and shows substantial successful attack rates. The experiments are carried out over multiple LLMs (as fact checkers) and the results are consistent."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-  The experiments are extensive.\n-  The performance in the proposed setting is very good.\n-  The explanations are very detailed."}, "weaknesses": {"value": "- Authors repeatedly claim that their setting is inspired by real world scenarios. But How realistic is it to have the exact claim beforehand? In their pipeline this is crucial as they use it to create the false evidence, and also to make it retrievable by pre-pending it to the false evidence. If it is not feasible, then what can be done? How would this change the experiments reported in the paper?\n- Authors claim as if no study has previously done on advarsarial attacks on fact checking systems!! This is not true, there are many studies on this topic [1]. Authors will probably come up with justification and say that \"well those studies have not looked at the problem from XXX aspect\". The point is this will question the author's claim about the topic, and also reduces the study to just a simple pipline which relies on a \"to be justifed\" assumption.\n- I appreciate the extensive experiments, but authors frequently refer to the appendix section inside the main body of the paper, which makes understanding the work (specifically for review purposes) very difficult. \n\n[1] Adversarial Attacks Against Automated Fact-Checking: A Survey, 2025."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cSsqDPJpUY", "forum": "NyqskXTfmI", "replyto": "NyqskXTfmI", "signatures": ["ICLR.cc/2026/Conference/Submission11925/Reviewer_jZ8Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11925/Reviewer_jZ8Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682628531, "cdate": 1761682628531, "tmdate": 1762922930557, "mdate": 1762922930557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies knowledge poisoning attacks on RAG, where malicious documents are injected to manipulate outputs. The authors find that attack success decreases when correct documents are retrieved alongside the malicious ones. To address this, they propose simulating such scenarios by collecting or generating realistic correct documents, enabling the creation of stronger malicious ones. This approach improves over naive document generation methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, clearly structured, and easy to follow.  \n2. The experiments are comprehensive and include comparisons with prior work.  \n3. The proposed method shows improvements over the naive document generation approach."}, "weaknesses": {"value": "1. The contribution is incremental. Knowledge poisoning in RAG has been well studied, including improving retrieval and injecting malicious documents to manipulate model behavior. This work focuses on a specific detail and provides only a modest extension of existing ideas.  \n2. The novelty is limited. The core idea is simple—providing the LLM with correct documents and prompting it to generate improved malicious ones. Moreover, some components overlap with prior work, such as making malicious documents more retrievable by appending the target query, a strategy already used in PoisonedRAG (Zou et al.).  \n3. While the method achieves some improvement, the gains are modest (e.g., an average ASR increase of 8% in 1-shot). Given the paper’s focus on this specific enhancement, I expected more substantial improvements. In some cases, such as on Climate-FEVER (Table 1), the improvement is marginal."}, "questions": {"value": "1. In the defense evaluation on LLM-based fake news detection, FakeWatch is tested only up to Llama-2-7B, which is not convincing evidence that LLM-based detection fails. Have the authors tried stronger models, such as GPT, for a quick check?  \n2. Why does the divide-and-vote defense fail? Does this imply that malicious documents dominate the retrieved results during the poisoning process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ww2XL16YMS", "forum": "NyqskXTfmI", "replyto": "NyqskXTfmI", "signatures": ["ICLR.cc/2026/Conference/Submission11925/Reviewer_8hnt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11925/Reviewer_8hnt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748995298, "cdate": 1761748995298, "tmdate": 1762922930105, "mdate": 1762922930105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a highly timely and critical problem: the vulnerability of retrieval-augmented generation (RAG) systems used for fact-checking, to “knowledge poisoning” attacks. The authors propose a novel method, ADMIT (Adversarial Multi-Injection Technique), which is notable in several respects. First, it systematically extends poisoning attacks into the more realistic fact-checking scenario — where retrieval pools include authentic supporting or refuting evidence, not merely benign or irrelevant context. This bridging of a practical deployment gap enhances the paper’s relevance for both the machine-learning and security communities. Second, the technical contribution is strong: ADMIT is a few-shot, semantically aligned poisoning approach that works without access to the target LLMs, retrievers or token-level control — thus modelling a realistic adversary. According to the abstract, it achieves an average attack success rate (ASR) of 86 % at an extremely low poisoning rate (~0.93×10⁻⁶) and shows transfer across multiple retrievers, LLMs, and cross-domain benchmarks. Such empirical strength and breadth of evaluation demonstrate both practical threat-model significance and robust experimental validation. Third, the work contributes to the broader “trustworthy AI / AI security” agenda by exposing an under-studied vulnerability of RAG-based fact-checking — a domain of clear interest to the ICLR audience given recent attention to large models, retrieval systems, and adversarial robustness. Taken together, the paper combines strong motivation, novel methodology, rigorous evaluation, and high relevance — making it well-suited for acceptance at ICLR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "See the summary above."}, "weaknesses": {"value": "While the paper presents an impressive and comprehensive study of few-shot knowledge poisoning in RAG-based fact-checking systems, several areas could be improved to further strengthen its contribution.\n\t1.\tScope of Defense Evaluation.\nThe paper focuses primarily on offensive analysis. While this is understandable given the novelty of the attack, the defense discussion could be expanded. For example, integrating a simple retrieval-level or embedding-level filtering baseline (e.g., detecting anomalous injected samples or measuring cosine distance drift) would provide more balanced insight and demonstrate the practical implications for defending real-world RAG systems.\n\t2.\tTheoretical Insight.\nAlthough the empirical results are strong, the paper could benefit from a more formal characterization of why ADMIT succeeds under few-shot conditions. For instance, an analysis of embedding-space perturbation alignment or retrieval sensitivity could help reveal deeper understanding of poisoning dynamics — making the work more theoretically grounded for the ICLR audience.\n\t3.\tHuman Evaluation or Case Study.\nSince fact-checking inherently involves human judgment and contextual reasoning, including a small-scale qualitative case study (e.g., analyzing misled outputs from a real-world RAG-based fact-checker) would make the findings more intuitive and impactful beyond numerical metrics.\n\t4.\tBroader Implications and Ethical Considerations.\nGiven the increasing deployment of RAG systems in sensitive domains (news verification, misinformation detection, etc.), it would be valuable to include a short discussion on responsible disclosure and mitigation guidelines — aligning with ICLR’s growing focus on AI safety and responsible deployment"}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethic review needed."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dz7Npd0lcO", "forum": "NyqskXTfmI", "replyto": "NyqskXTfmI", "signatures": ["ICLR.cc/2026/Conference/Submission11925/Reviewer_x6sL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11925/Reviewer_x6sL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773914610, "cdate": 1761773914610, "tmdate": 1762922929265, "mdate": 1762922929265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ADMIT, a few-shot knowledge poisoning attack specifically designed for RAG-based fact-checking systems. The method uses a multi-turn, proxy-based optimization process to generate semantically coherent adversarial passages that mimic real news or reports. The stated goal of the attack is to flip the system's fact-checking verdict (e.g.,"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well-written, clearly structured, and easy to follow. The authors' strongest contribution is the comprehensive empirical evaluation. The experiments are extensive, testing the proposed attack across 11 different LLMs (open-source, commercial, and reasoning-focused) and 4 retrievers, which demonstrates wide transferability.\n\n\nThe paper also does a thorough job of evaluating the attack against a suite of potential defenses, including statistical detection, fake news classifiers, and LLM-based knowledge consolidation . Demonstrating the failure of these defenses is a valuable, if concerning, result for the community."}, "weaknesses": {"value": "Despite the strong empirical results, the paper suffers from significant weaknesses in its framing, novelty, and threat model.\n\nLimited Novelty of the \"Threat\": The paper frames this as a new \"knowledge poisoning\" attack. However, it does not reveal a new vulnerability class. The core mechanism is the generation of highly plausible, semantically coherent misinformation (\"fake news\") that can be injected into a knowledge base. The techniques used to make this misinformation \"convincing\"—such as fabricating authoritative sources (e.g., \"CDC and WHO joint statement\" ), using recent timestamps (e.g., \"March 2024\" ), and providing plausible-sounding explanations (e.g., \"new genomic analysis\" )—are well-established strategies for creating effective disinformation. These are not new attack primitives, and similar concepts like \"temporal ordering\" and \"contextual explanation\" have been explored in prior work (e.g., GRAGPOISON[1] - 5.3.3. “Tricks” of Relation Injection.).\n\n\nMisleading Problem Formulation: The authors state that the attack works even when \"credible supporting or refuting evidence\" is present. This reframes the problem entirely. This is not \"poisoning\" in the sense of corrupting the only available source of truth. Instead, this is a failure of the LLM's contextual conflict resolution. The core question is simply: when an LLM is given two contradictory passages (one true, one a well-crafted lie), can it identify the truth? The paper's high ASR (86%) demonstrates that modern LLMs are poor at this, which is a known limitation, not a new threat.\n\n\n\n\n\nUnclear and Impractical Threat Model: The attack's evaluation relies on a \"per-claim\" generation process. This implies an attacker must identify a specific claim they want to flip and then expend significant computational resources (multi-turn optimization ) to craft a bespoke adversarial passage for that single claim. This is a high-effort, low-scalability attack model. The paper acknowledges this as a limitation and proposes fine-tuning as a fix , but the primary results and contributions are based on this impractical per-claim premise.\n\n\n\n\n\nConfounding Effect of LLM's Prior Knowledge: The paper struggles to isolate the attack's effectiveness from the LLM's own internal knowledge. In fact, the authors' own analysis in Appendix B  highlights this weakness. The attack is most effective on \"Gray\" (conflicting signals) and \"Black\" (no prior knowledge) claims, achieving 75% and 98% ASR respectively . However, it is significantly less effective on \"Gold\" claims (52% ASR), where the LLM has strong, correct prior knowledge. This suggests the attack's success is heavily confounded by the LLM's pre-existing uncertainty, rather than demonstrating an ability to universally override established facts.\n\n[1] Liang, J., Wang, Y., Li, C., Zhu, R., Jiang, T., Gong, N., & Wang, T. (2025). Graphrag under fire. arXiv preprint arXiv:2501.14050."}, "questions": {"value": "1. Could the authors please clarify the novelty of the attack's \"tricks\" (e.g., temporal ordering, fabricated authority ) compared to prior work on both misinformation generation and knowledge graph poisoning (e.g., GRAGPOISON), which use similar strategies?\n\n2. Given that the attack relies on the LLM failing to resolve a conflict between clean and poisoned text already in its context, why is this framed as a \"knowledge poisoning\" attack on RAG rather than a \"contextual conflict resolution failure\" of the LLM?\n\n\n3. The results in Appendix B  are critical. They seem to suggest the attack is primarily effective when the LLM is already uncertain (\"Gray\" or \"Black\" claims). Doesn't this undermine the central claim of the attack's power, as it largely fails (or is at least 50/50) when trying to override \"Gold\" (known) facts?\n\n\n4. Regarding the per-claim threat model: What is a realistic scenario where an attacker would expend the cost of multi-turn optimization for a single claim? If the proposed solution for scalability is fine-tuning , shouldn't this be the primary evaluation rather than an appendix?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zXnFbHe4hu", "forum": "NyqskXTfmI", "replyto": "NyqskXTfmI", "signatures": ["ICLR.cc/2026/Conference/Submission11925/Reviewer_njbn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11925/Reviewer_njbn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964207448, "cdate": 1761964207448, "tmdate": 1762922928673, "mdate": 1762922928673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}