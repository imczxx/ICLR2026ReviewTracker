{"id": "mWPQA3nNFj", "number": 14113, "cdate": 1758228643929, "mdate": 1759897389309, "content": {"title": "Learning KAN-based Implicit Neural Representations for Deformable Image Registration", "abstract": "Deformable image registration (DIR) is a cornerstone of medical image analysis, enabling spatial alignment for tasks like comparative studies and multi-modal fusion. While learning-based methods (e.g., CNNs, transformers) offer fast inference, they often require large training datasets and struggle to match the precision of classical iterative approaches on some organ types and imaging modalities. Implicit neural representations (INRs) have emerged as a promising alternative, parameterizing deformations as continuous mappings from coordinates to displacement vectors.\nHowever, this comes at the cost of requiring instance-specific optimization, making computational efficiency and seed-dependent learning stability critical factors for these methods.\nIn this work, we propose KAN-IDIR and RandKAN-IDIR, the first integration of Kolmogorov-Arnold Networks (KANs) into deformable image registration with implicit neural representations (INRs).  The learnable activation functions of KANs and their inherent suitability for approximating physical systems make them ideal for modeling deformation fields. Our proposed randomized basis sampling strategy reduces the required number of basis functions in KAN while maintaining registration quality, thereby significantly lowering computational costs. We evaluated our approach on three diverse datasets (lung CT, brain MRI, cardiac MRI) and compared it with competing instance-specific learning-based approaches, dataset-trained deep learning models, and classical registration approaches. KAN-IDIR and RandKAN-IDIR achieved the highest accuracy among INR-based methods across all evaluated modalities and anatomies, with minimal computational overhead and superior learning stability across multiple random seeds. Additionally, we discovered that our RandKAN-IDIR model with randomized basis sampling slightly outperforms the model with learnable basis function indices, while eliminating its additional training-time complexity. Source code is available in the supplementary material.", "tldr": "RandKAN-IDIR is the first model to incorporate Kolmogorov-Arnold Networks (KANs) into implicit neural representation-based deformable image registration. It improves registration accuracy and seed-dependent stability with minimal computational costs.", "keywords": ["medical image registration", "implicit neural representations", "Kolmogorov-Arnold networks"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/06d2abf93f69e7bb95473b3ccca497a61a12a2d8.pdf", "supplementary_material": "/attachment/2b09b65ea359bade765cf858c574fad3171b98d9.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a novel way of representing the transformation of deformable image registration in the context on INRs architectures, using Kolmogorov-Arnold-Networks (KANs). It proposes a new strategy of learning KANs that reduces the the number of learned basis function lowering the overall cost while maintaining competitive performance. The performance is evaluated on a 3 mono-modal datasets and compared against several INR-based methods and 2 cohort-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper utilizes a new architecture for pairwised INR-based registration that was not used before.\n2. It proposed a novel way of learning KAN activations that might be useful out of the context of deformable registration.\n3. The performance is evaluated against 3 mono-modal diverse datasets measuring the accuracy, regularity and speed.\n4. The language of the paper is clear."}, "weaknesses": {"value": "1. The introduction and related work are rather weak and unfocused failing to motivate the topic effectively. First of all, there is not a clear motivation on what makes KANs a better candidate as a network architecture compared to MLPs in the context of registration. The literature review both in the case of Registration and INRs is not thorough making it difficult to understand what is state-of-the-art in the field.\n2. The discussion about iterative optimization methods (classical methods) for registration is somehow missing from the discussion. In my opinion INRs are a sub-category of iterative optimization methods. As a result, I believe they should be mentioned in the introduction and also compared against during the evaluation.\n3. There are several inaccuracies in the text, namely:\n    1. the paper refers to INR methods as instance-specific optimization methods. However, since registration is happening in pairs I believe that the right term is pairwise optimization methods.\n    2. Line 130: *“Second, while segmentation labels enhance performance in brain MRI (Jena et al., 2024), these methods underperform on unlabeled data (e.g., DIR-Lab lung CT (Castillo et al., 2009)), trailing classical (Vishnevskiy et al., 2016) and INR-based (Harten et al., 2023) approaches.”*\n        \n        I believe that this is intuitive and that doesn’t diminish their utility. Moreover, I am not sure this is a fair claim since in the case of brains the deep learning methods are trained on segmentation losses and evaluated using dice score (optimization for the metric) while in the case of DIRLab even if they are trained using segmentation labels they are evaluated using TRE on landmarks (the network was not optimized for this metric).\n        \n    3. Line 127 - 129: “First, even when trained on large datasets, these image-to-image models remain vulnerable to domain shifts (Jena et al., 2024) - a problem often addressed through even larger datasets (Tian et al., 2024).”\n    \n    I am not sure why this is a limitation. If larger datasets are available they can be used to alleviate the domain shift issue mentioned by the authors.\n    4. Line 149: *“A fundamentally different (but still instance-specific) approach reformulated registration via neural ODEs in NODEO (Li et al., 2021).”*\n    \n    Why do the authors believe that this is a fundamentally different approach than the INRs?\n4. I appreciate that the authors experimented with different seeds, but I believe that any learning based method is suffering from the performance discrepancy when choosing different seeds. Is there any reason why this has not been applied to Vmorph and Tmorph?\n5. I am not sure I understand the motivation behind the analysis in figure 1 (e.g. What does 54 vs 64 for INR refer to?) This figure and its caption lack clarity.\n6. Although there are figures for the lung registration and the landmark errors there are no visualizations of the deformation fields and for the other modalities. Maybe the authors should consider adding them at least in the appendix since visual inspection is a very important way of evaluating whether the final result is accurate.\n7. The paper is using a combination of 2 regularization terms:\n    1. Where the other methods trained on the same objective? If this is not the case then the evaluation is not very fair and it makes sense why the proposed method is delivering lower folding compared to its counterparts.\n8. In the evaluation section I believe that the dice score that is reported is the mean over all structures. It would be usedul to see the mean and std and only demonstrate the seed stability as an ablation over only one application.\n9. For completeness I believe that VRAM and speed should also be included in the table for the cardiac data.\n10. There is not publicly available repository.\n\n**Minor points**\n\n1. Is there any drawback due to the choice of Chebychev polynomials instead of the original splines?\n2. Is there any intuition why the different KAN strategies behave differently?\n3. It is not clear whether the INR methods are fitted till convergence or over fixed steps and whether the learning rate and omega of siren was tuned to the application. Finally, the choice of the batch size affects both the memory requirements, the convergence and the performance. It would be nice to know how all these hyperparameters were chosen to understand whether the evaluation is fair."}, "questions": {"value": "Please see the weaknesses.\n\n**Preliminary Justification:**\n\nThe paper presents a conceptually interesting approach to a new way of representing the transformation of registration based on KANs.\n\nHowever, I believe that the paper quality is insufficient to be accepted in ICLR this year. I believe the authors have to put substantial effort into enhancing the motivation, the literature review, the storyline, experiments, and discussion, as well as to include more baselines (classical methods) and qualitative results. \n\nGiven these shortcomings, I recommend rejection in its current form. Nonetheless, the underlying idea is promising, and I encourage the authors to strengthen the paper and resubmit it to a future venue. Finally, given the structure of the proposed work, I believe that this would be a better fit in a computer vision conference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5Q6PGWyBrA", "forum": "mWPQA3nNFj", "replyto": "mWPQA3nNFj", "signatures": ["ICLR.cc/2026/Conference/Submission14113/Reviewer_NTqw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14113/Reviewer_NTqw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640785858, "cdate": 1761640785858, "tmdate": 1762924582417, "mdate": 1762924582417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a KAN-based Implicit Neural Representation (INR) framework for deformable image registration and compares its performance to SIREN-based INRs. Moreover, the authors ablate architectural variants of the KAN network with respect to its (learnable) basis function."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Using KANs over traditional, MLP-based architectures for INRs is a refreshing and novel perspective for deformable image registration.\n- Based on the authors' presentation, their proposed method outperforms all baseline models for all datasets."}, "weaknesses": {"value": "### Presentation of the method within the context of other works (Introduction/Related Work):\n\nThe structure of the introduction and related work does not effectively motivate the usage of KANs in the context of deformable image registration, and it lacks the placement of the proposed framework within this domain.\n\nTo be precise:\n* The authors provide a lengthy introduction to state-of-the-art cohort-based registration frameworks, such as VoxelMorph, yet they focus on iterative settings. They should instead focus on placing their proposed method within this context.\n* The motivation for choosing KAN over an MLP in the context of deformable registration is not clearly stated—what specific features do KANs offer that MLPs do not?\n* While seminal works like [1] have employed SIREN, a plethora of other INR architectures have been proposed since—which may be equally relevant (to KANs) for this task—such as those utilizing Gaussian activation functions, FINER, BACON, MFN, etc. [2-6]. Since the primary contribution of this paper is to use KANs instead of MLP-based INRs, the authors should discuss (and later ablate) these architectures in this context, and not focus only on SIREN. For instance, [2] remains of particular relevance since they group ReLU activations to be a spline, which may be adjacent to this work, even though the authors do not use spline-based KANs.\n\nMoreover, the KAN literature has also grown significantly over the past year. Since one of the authors' main contributions is the introduction of their RandKAN model, it would be beneficial to explain the elements of different KAN variants with more rigor and also contrast their performance in experiments. For instance, why did the authors choose the Chebyshev KAN architecture over the traditional KAN (Liu et al.) using B-splines, given that methods like FFD [7] typically use B-Splines? Is there any intuition for this? While the authors state that this increases the speed, did they ablate this decision?\n\n### Experimental Setup:\n\n* The authors use entirely different regularization than methods like IDIR [1]. The motivation for using other regularization (e.g., no bending energy) remains unjustified. A fair experimental setup would integrate all architectures into the same framework and evaluate all models for the same model footprint on the problem, requiring hyperparameter tuning for all architectures within the same framework. For instance, the KAN models are much smaller, which may cause them to overfit much less to noise and thus exhibit much less folding. How does folding behave once the model size is increased?\n* Missing classical, iterative registration pipelines for DIRLAB experiments.\n* The authors report that default parameters were used for, e.g., IDIR for all datasets, resulting in suboptimal performance since IDIR's hyperparameters were optimized for lung registration, and not for brain (OASIS) or cardiac datasets.\n* No qualitative results are provided for the OASIS and ACDC datasets.\n\nGiven these substantial weaknesses, I recommend a thorough revision of the Introduction/Related Work and the experimental setup for this manuscript.\n\n[1] Wolterink JM, Zwienenberg JC, Brune C. Implicit neural representations for deformable image registration. MIDL'22\n\n[2] Liu Z, Zhu H, Zhang Q, Fu J, Deng W, Ma Z, Guo Y, Cao X. Finer: Flexible spectral-bias tuning in implicit neural representation by variable-periodic activation functions. CVPR'24\n\n[3] Fathony R, Sahu AK, Willmott D, Kolter JZ. Multiplicative filter networks. ICLR2020\n\n[4] Lindell DB, Van Veen D, Park JJ, Wetzstein G. Bacon: Band-limited coordinate networks for multiscale scene representation. CVPR22\n\n[5] Ramasinghe S, Lucey S. Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps. ECCV'22\n\n[6] Shenouda J, Zhou Y, Nowak RD. ReLUs are sufficient for learning implicit neural representations. ICML'24\n\n[7] Rueckert D, Sonoda LI, Hayes C, Hill DL, Leach MO, Hawkes DJ. Nonrigid registration using free-form deformations: application to breast MR images. TMI'02"}, "questions": {"value": "1.  How is sparse approximation related to pruning literature, both within and outside the scope of KANs and neural networks?\n2.  How do spline-based KANs compare to the Chebyshev KANs in terms of performance (excluding speed)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MxdRFhpSUj", "forum": "mWPQA3nNFj", "replyto": "mWPQA3nNFj", "signatures": ["ICLR.cc/2026/Conference/Submission14113/Reviewer_NKPY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14113/Reviewer_NKPY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665812726, "cdate": 1761665812726, "tmdate": 1762924581860, "mdate": 1762924581860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes KAN-IDIR and RandKAN-IDIR, the first integration of Kolmogorov–Arnold Networks (KANs) into implicit neural representation (INR) based deformable image registration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Comprehensive evaluation: Evaluated on three modalities/organ types with multiple metrics (TRE/DSC/HD95/NJD), per-case tables, seed variability, and ablations on basis size and loss weights.\n\nSeed-stability analysis: The paper explicitly studies seed-dependent variability and demonstrates improved robustness, an important but often neglected INR property."}, "weaknesses": {"value": "1. In figure 1, the visual differences between the first two panels and the last two panels in Figure 1 are not explained; it is difficult to understand what each subfigure is intended to illustrate.\n2. The contribution reads largely as “apply Chebyshev KAN architecture to INR for registration” with no substantial methodological modifications targeted specifically to registration (beyond RandKAN sampling). This reduces perceived novelty.\n3. In figure 3, the regularization loss should be imposed to the learned displacement, not on the addition of the displacement and the input coordinate.\n4. In Table 2, the results of pTV method are not reported.  \n5. The claim of “best or best-in-class across all datasets” is overstated: classical pTV remains best on DIR-Lab TRE in Table 1, and dataset-trained methods outperform on some metrics (OASIS).\n6. It is unclear whether reported runtimes are end-to-end (training per instance) or inference only, and how they scale with image resolution.\n7. The paper adopts Chebyshev-based KANs but provides limited ablation comparing different KAN variants or other INR bases (e.g., SIREN, Fourier features, B-spline KAN). It is therefore unclear whether Chebyshev KAN is the best or merely sufficient."}, "questions": {"value": "How robust is the method to multimodal intensity differences (e.g., CT→MR registration)? Have you tried cross-modality experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZtlOhT8mkD", "forum": "mWPQA3nNFj", "replyto": "mWPQA3nNFj", "signatures": ["ICLR.cc/2026/Conference/Submission14113/Reviewer_bzhm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14113/Reviewer_bzhm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743571734, "cdate": 1761743571734, "tmdate": 1762924581274, "mdate": 1762924581274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider the task of deformable image registration using the well known method INRs to model the deformation/displacement field.\n\nContrary to previous work they use an other network architecture for the INR, in this case a Chebyshev KAN. They explored the use of fixed, randomized and adaptive selection of polynomial basis functions for their network. They evaluate their task on three datasets, including 4D lung CTs, brain MRIs and cardiac MRIs. They show that for the selected datasets their method slightly outperforms previous work with other network architectures, while consuming a little bit less of computational resources."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written in all sections and their architecture is clearly explained The authors report a performance improvement with a lower resource consumption. They shared their implementation and used publicly available datasets which is a plus for the reproducibility, and they promise to make their code publicly available. In the analysis the authors provide multiple ablations across various settings and parameter ranges and also show some qualitative examples of their method compared to others."}, "weaknesses": {"value": "The authors only investigate one type of networks architectures (the Chebyshev KANs) and compare them against other existing types from other publications. Since this was the only focus of this paper, it would have been nice to see some broader comparison across other network architectures that have been proposed for INRs. One of the previous work they base their work on (Wolternik, 2022) used SIREN, but since, there have many other architectures have been published (WIRE, FINER etc) that easily could have been explored in the same work, so the motivation for only focusing this particular architecture remains unclear."}, "questions": {"value": "- The authors should compare also compare their network with other architectures like WIRE, FINER,... How do they compare to their chosen approach? Justify your choice!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WAc85vyTKw", "forum": "mWPQA3nNFj", "replyto": "mWPQA3nNFj", "signatures": ["ICLR.cc/2026/Conference/Submission14113/Reviewer_Mfq6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14113/Reviewer_Mfq6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982041855, "cdate": 1761982041855, "tmdate": 1762924580662, "mdate": 1762924580662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}