{"id": "sMjESVShz3", "number": 10313, "cdate": 1758166728825, "mdate": 1762966856506, "content": {"title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning", "abstract": "While multi-modal large language models (MLLMs) have made significant progress in recent years, the issue of their hallucinations remains a major challenge. To mitigate this phenomenon, existing solutions either introduce additional data for further training or incorporate external or internal information during inference. However, these approaches inevitably introduce extra computational costs. In this paper, we observe that the occurrence of hallucinations in MLLMs is closely associated with low attention distribution on visual tokens. Moreover, due to the redundancy of visual tokens, the redundant visual tokens divert part of the model's attention, further causing important visual tokens to be neglected. Building on this observation, we propose \\textbf{PruneHal}, a training-free, simple yet effective method that leverages adaptive KV cache pruning to enhance the model’s focus on critical visual information, thereby mitigating hallucinations. To the best of our knowledge, we are the first to apply token pruning for hallucination mitigation in MLLMs. Notably, our method don't require additional training and incurs no extra inference cost, thereby introducing no computational overhead. Moreover, PruneHal is model-agnostic and can be seamlessly integrated with different decoding strategies, including these specifically designed for hallucination mitigation. We evaluate PruneHal on several widely used MLLMs and benchmarks, achieving robust and outstanding experimental results that highlight the potential of our method. Our code will be publicly available.", "tldr": "A training-free, adaptive pipeline for MLLMs' hallucination mitigation", "keywords": ["Multi-modal", "token pruning", "hallucination"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/90f29f47763246d5c149a0cc00e535df3d10242c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel approach: PruneHal. It reframes hallucination mitigation as attention concentration on informative visual evidence via adaptive, training-free KV-cache pruning, and reduces hallucinations across diverse MLLMs/decoders while maintaining descriptive detail and incurring minimal inference cost. Detailed information is as follows:\n\n## 1) Research Gap\nMLLMs still hallucinate (generate content inconsistent with the image) despite strong overall capability.\nExisting fixes usually need extra data/training or specialized decoding, increasing cost/latency.\nUnder-explored angle: long, redundant visual token sequences; low attention to visual tokens correlates with hallucination during generation.\n## 2) Motivation\nQualitative probe: boosting attention to visual tokens near an error can flip an incorrect word to a correct one.\nQuantitative probe: at steps producing hallucinations, average attention to visual tokens is typically below the caption’s baseline (mean across steps).\nGoal: concentrate attention on salient visual evidence during inference without retraining.\n## 3) Proposed Method (PruneHal)\nTraining-free, plug-and-play: adaptively prunes only the visual entries in the decoder’s KV cache; text KV is unchanged.\nSignal: use the most recent step’s attention over visual tokens (saliency).\nTrigger (layer-vote): for each layer $\\ell$, compare current average visual-token attention $A_\\ell^{(i)}$ to a running historical average $\\bar{A}\\ell$. If a majority of layers satisfy $A_\\ell^{(i)} < \\sqrt{r}\\bar{A}_\\ell$ , pruning is triggered.\nAction (fractional keep): retain the top-$r$ fraction of visual tokens (by attention from the previous step) and physically slice the visual KV to those indices; discarded tokens do not re-enter.\nSafeguards: cap the number of pruning operations at $t$; use $r\\in(0,1)$ (not a tiny fixed budget) to keep buffer tokens for potential late-use content.\nA fixed Top-$K$ variant exists; the adaptive version better balances truthfulness vs. descriptive richness.\n## 4) Experimental Results\nModels/Decoding: LLaVA-v1.5-7B/13B, InstructBLIP-7B, Qwen-VL-7B; greedy, nucleus, beam; also combined with DoLa, VCD, OPERA, DeCo.\nBenchmarks: CHAIR, AMBER facets, MM-Vet, plus LLM-assisted open-ended evaluation (correctness/detailedness).\nFindings:\n* Hallucination reduction: consistent CHAIR decreases across models/decoding strategies.\n* Compatibility: complementary to other decoding methods; gains add up.\n* Quality balance: correctness improves while detailedness is largely preserved.\n* Efficiency: negligible overhead; under beam search, pruning can reduce per-token latency by shrinking the visual attention set.\n## 5) Analysis\nToken selection: keeping top-attended visual tokens reduces hallucinations more than random or low-attention selections.\nAdaptive vs. fixed: aggressive one-shot pruning curbs hallucinations but can harm richness; conservative fixed pruning under-mitigates hallucinations. The adaptive trigger (layer vote + $\\sqrt{r}$ threshold) with cap $t$ strikes a better trade-off.\nHyperparameter robustness: reasonable ranges of $(r, t)$ yield consistent gains with light model-specific tuning.\nRisk & mitigation: pruning is irreversible; premature pruning could drop late-use evidence. Fractional keep, the layer-voted trigger, and prune cap mitigate this in practice."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "## 1. Training-free, plug-and-play, and compatible with other methods\n   The method requires no retraining, slots into standard decoding, and is explicitly positioned as compatible with hallucination-aware decoders. The conclusion also emphasizes “virtually no computational overhead.” \n\n\n## 2. Adaptive, principled trigger that tracks attention drift\n   The design of this paper reacts only when a majority of layers’ visual-token attention falls below a threshold (layer-vote with a ($\\sqrt{r}$) criterion), motivated by the observation that visual attention can continuously decline during long generations. The algorithm specifies the vote, threshold, and bounded number of prunes.  \n\n\n## 3. Consistent hallucination reduction across models and decoding strategies\n   On AMBER/CHAIR, PruneHal improves LLaVA-v1.5-7B/13B, InstructBLIP-7B, and Qwen-VL-7B over greedy, nucleus, and beam decoding; tables show the deltas for CHAIR, Hal, and Cog.  \n\n\n## 4. Quality balance: improves correctness while preserving detail\n   The paper reports GPT-4V-assisted Correctness gains while tracking Detailedness, and frames the adaptive module as balancing hallucination mitigation with output diversity/detail. (Table and accompanying discussion.)  \n\n\n## 5. Efficiency: negligible overhead; can even speed up beam search\n   The latency analysis shows near-zero overhead in general and faster per-token decoding under beam search because the pruned cache reduces compute, whereas alternatives like DoLa/DeCo/VCD/OPERA add extra passes or intermediate-layer work."}, "weaknesses": {"value": "## 1. **Correlation, not causation, in the empirical motivation**\n   The key quantitative probe links lower visual attention to hallucination using scatter plots and description, with details deferred to the appendix; This experiment is not considered as a full causal study, as 1) This experiment only shows the performance of llava-v1.5-7b. 2) Since visual-token attention tends to decay over later decoding steps, if hallucinations also occur later, lower attention could partially reflect step index rather than a direct cause.\n\n## 2. **Evaluation breadth is narrow for a strong general claim**\n   The main metrics center on CHAIR/AMBER for captioning and a GPT-4V judge; this focuses on object hallucination and open-ended descriptions, but leaves out other widely-used task families (e.g., VQA variants) within the main text/tables. \n   The authors explicitly note **POPE** is *not used* because yes/no outputs don’t change under their setup—so an entire line of existence-probing evaluation is omitted. \n\n## 3. **Heuristic trigger design with limited theoretical grounding**\n   The adaptive trigger uses a layer vote and a $\\sqrt{r}$ threshold. While intuitive, there’s no theoretical justification for $\\sqrt{r}$ beyond heuristic motivation, and the analysis focuses on empirical ablations rather than principled derivation.  \n\n## 4. **Hyperparameters tuned per model**\n   The paper sets different $(r, t)$ for each MLLM (e.g., LLaVA-7B/13B: $r{=}0.4,t{=}3$; InstructBLIP-7B: $r{=}0.7,t{=}2$; Qwen-VL-7B: $r{=}0.9,t{=}4$). This brings a concern that as a plug-and-play approach, when applied to a new model, does it need to conduct several experiments to allocate the best parameters before using?\n   Additionally, their ablation evidences robustness on the same 500-image slice, but still within the same dataset and models.  \n\n## 5. **Sample sizes for analyses are modest**\n   Several analyses use 500 images (motivation/GPT-4V) or 100 images (Top-K visualization curves), which are useful but limited for claims of breadth.  \n\n## 6. **Irreversibility and potential late-use information loss**\n   The method physically discards visual KV rows, and the paper itself notes that visual-token attention diminishes as decoding proceeds, motivating a dynamic mechanism. Still, once dropped, tokens can’t return; there is no recovery strategy beyond conservative $r$ and a cap $t$."}, "questions": {"value": "# Questions and Concerns\n\n## A. Empirical motivation: attention vs. hallucination (Layers 1, 16, 30)\n\n1. **Position confound (step index).** Visual-token attention typically decays over later decoding steps. If hallucinations also tend to occur later, lower attention at those steps may partially reflect position rather than a causal link. Therefore, the motivation experiment is not considered as a full causal study. Additional experiments and explanations is preferred.\n\n2. **Model generality.** The analysis is shown only for llava-v1.5-7b. The authors are suggested to replicate the same measurement on additional MLLMs to demonstrate robustness across architectures.\n\n3. **Caption selection bias.** Plots include only captions that already contain hallucinations (223/500). The authors may also report the distributions for non-hallucinating captions, and provide summary statistics/effect sizes and (ideally) significance tests comparing the two groups.\n\n\n## B. Method: applicability and length sensitivity\n\n1. **Trigger dependence on output length.** PruneHal’s pruning is event-triggered; very short generations may never trigger it. The authors may clarify the operating range: how often does pruning trigger vs. caption length, and report the distribution of prune counts (and time-to-first-prune) across lengths. And, if possible, a length-sensitivity plot (quality vs. output length) to show behavior on short, medium, and long captions is preferred.\n\n\n## C. Evaluation scope\n\n1. **Limited benchmarks.** Current results emphasize CHAIR/AMBER (captioning) and one GPT-judge setup. The authors are suggedted to add broader evaluations (e.g., LLaVA-Bench (In-the-Wild) and/or other open-ended sets).\n\n\n## D. Baseline discrepancy (MM-Vet)\n\n1. **Score gap for LLaVA-v1.5-7B.** The paper reports **28.3** on MM-Vet, while the LLaVA repository/paper commonly reports **31.1**. Please reconcile the difference by specifying exact evaluation settings (dataset/version, decoding parameters, resolution, conversation template), and judge model/version. If the authors follow settings that differ from the “official” configuration, consider reporting both numbers (yours and a reproduced one under their settings).\n\n\n*Thanks for considering these points. I would like raise my score once my concerns has been addressed.*"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No specific ethic concern."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6fxvaFr4mg", "forum": "sMjESVShz3", "replyto": "sMjESVShz3", "signatures": ["ICLR.cc/2026/Conference/Submission10313/Reviewer_HCd2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10313/Reviewer_HCd2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760588363592, "cdate": 1760588363592, "tmdate": 1762921655436, "mdate": 1762921655436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thanks for all reviewers’ feedbacks. Your comments help us improve our work a lot."}}, "id": "gFiaKGcDS0", "forum": "sMjESVShz3", "replyto": "sMjESVShz3", "signatures": ["ICLR.cc/2026/Conference/Submission10313/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10313/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762966855701, "cdate": 1762966855701, "tmdate": 1762966855701, "mdate": 1762966855701, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PruneHal, a framework for mitigating hallucinations in MLLMs. \n\nThe core idea is that hallucinations in MLLMs arise from low attention to visual tokens and redundancy in the visual input. \n\nPruneHal applies an adaptive key-value (KV) cache pruning strategy during decoding: it uses a cross-layer attention voting mechanism to identify visual tokens that receive low attention and prunes them from the cache, thereby forcing the model to attend better to remaining visual tokens. \n\nSeveral open‐source MLLMs (e.g., LLaVA-1.5, Qwen-VL) verified and reached ~25 % relative reduction in hallucination metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method is practical: no retraining, no architecture change, plug-in compatibility with existing MLLMs.\n\nGood empirical coverage across multiple models\n\nThe topic is significant: hallucination in MLLMs is a major deployment hurdle."}, "weaknesses": {"value": "Novelty is limited: prior works have identified the same underlying problem (neglect of visual tokens) and proposed decoding-time mitigations, such as OPERA\n\nNarrow experimental scope: the experiments rely mainly on the CHAIR-based benchmark. Other hallucination types (e.g., relation/attribute hallucination, VQA, multi-image or video inputs, it is easy to name many benchmarks such as pope/ hallusionbench/MMhal-bench, etc) are not explored. This limits claims of generality.\n\nInterpretability/analysis depth: While attention visualisation is provided, the paper could deepen the analysis to explain why pruning certain visual tokens can generalise to all domains. \n\nHyperparameter sensitivity: The pruning threshold and voting mechanism are somewhat heuristic; more systematic sensitivity analysis would strengthen confidence in robustness."}, "questions": {"value": "Please see weaknes part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d0HcpAJhS7", "forum": "sMjESVShz3", "replyto": "sMjESVShz3", "signatures": ["ICLR.cc/2026/Conference/Submission10313/Reviewer_wbKx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10313/Reviewer_wbKx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888988302, "cdate": 1761888988302, "tmdate": 1762921654948, "mdate": 1762921654948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a factor that may be closely associated with the hallucination phenomenon in multi-modal large language models (MLLMs), specifically that redundant visual tokens disperse the model’s attention, potentially leading to hallucinations.  Based on this insight, the authors propose PruneHal, a training-free approach that only leverages adaptive KV cache pruning to enhance the model’s focus on critical visual information, thereby reducing hallucinations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper identifies a noteworthy phenomenon: the allocation of attention influences the generation of hallucinations.\n* Building on this insight, PruneHal proposes a pruning-based approach to remove some redundant information, thereby reducing hallucinations.  It requires no additional training and operates at a relatively fast speed.\n* In the experiments, the paper not only tests the effectiveness of its own scheme but also verifies that combining this scheme with other existing schemes can further improve performance.  This indicates that the proposed scheme is relatively versatile."}, "weaknesses": {"value": "* The models tested in the paper are somewhat outdated. For instance, Qwen-VL, which is tested in the paper, has now been updated to Qwen3-VL, and LLaVA-v1.5 has also been updated to LLaVA-NeXT. It is suggested that more up-to-date models should be used to test the performance of PruneHal.\n* Why does Algorithm 1 not explain why pruning should also be triggered when m = 2? Is it hypothesized that the authors intend to intervene in visual attention at an early stage to prevent redundant visual tokens from diverting the model's attention in the initial phase?\n* The paper mentions that PruneHal accelerates inference, but only provides data for beam search. For different model scales (e.g., 7B vs. 13B) and decoding strategies (Greedy, Nucleus), what are the specific differences in inference speedup and computational cost reduction brought by PruneHal?"}, "questions": {"value": "* It seems that the argument that redundant attention allocation may exacerbate hallucinations is only supported by testing the effect of retaining tokens through pruning. In future work, supplementary verification could be added to determine whether \"proactively amplifying the attention scores of critical tokens\" can further optimize performance and improve the final results.\n* As observed in Figure 2, there are a small number of hallucinatory cases where the Average Attention value is relatively high. Could this phenomenon be explained? If processed according to the method proposed in the paper, these hallucinatory cases may also be retained, and it remains unclear whether this has an impact on the final generation results.\n* The paper applies different Top-K token numbers (e.g., Top-576/144/36 for LLaVA-v1.5-7B, Top-32/16/8 for InstructBLIP-7B, and Top-256/128/64 for Qwen-VL-7B) across models.  However, it does not explicitly explain the rationale behind these model-specific Top-K choices.  What is the specific design consideration or empirical basis for determining these distinct Top-K values for different MLLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8o8K0X4s5C", "forum": "sMjESVShz3", "replyto": "sMjESVShz3", "signatures": ["ICLR.cc/2026/Conference/Submission10313/Reviewer_QcpQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10313/Reviewer_QcpQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901533704, "cdate": 1761901533704, "tmdate": 1762921654551, "mdate": 1762921654551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies hallucinations in MLLMs as a major challenge and posits a link between these hallucinations and insufficient attention to critical visual tokens. The authors hypothesize that this problem is caused by a large number of redundant visual tokens, which disperse the model's attention.To address this, the paper proposes PruneHal, a training-free, adaptive KV cache pruning method. The method aims to remove redundant (low-attention) visual tokens, thereby forcing the model to concentrate its attention on the remaining, more informative ones. The proposed mechanism is adaptive, tracking the historical visual attention distribution and triggering a pruning step via a layer-wise voting mechanism when attention drops below a threshold . The authors claim this method adds \"nearly no extra inference cost\" , is model-agnostic , and effectively mitigates hallucinations on several benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method is training-free  and highly efficient. As shown in Figure 6, it adds negligible overhead and can even accelerate inference during beam search by reducing the KV cache size.\n- The idea of using adaptive pruning as a hallucination mitigation technique is novel. The layer-wise voting mechanism to decide when to prune is a smart approach to balance information loss and attention focusing."}, "weaknesses": {"value": "- The paper's most critical weakness is its admission that it cannot be evaluated on benchmarks like POPE because \"models' responses will keep unchanged\". This implies the pruning mechanism only activates after the first token is generated. This is a fundamental design flaw. A hallucination mitigation strategy that cannot influence the first generated token is of very limited use, as it cannot correct a model that is already on a hallucinatory path from the very first token (e.g., answering \"Yes\" to a \"is there a...\" question when the object is absent).\n- As a direct consequence of the first weakness, the paper's evaluation is incomplete. It is forced to rely only on long-form generation metrics (CHAIR, AMBER, GPT-4V). The complete and acknowledged absence of a standard object-presence benchmark like POPE is a major red flag that points to the method's limited applicability."}, "questions": {"value": "- Please clarify the exact timing of the pruning. Does the PruneHal intervention influence the generation of the very first output token?\n- If, as the text  implies, the method cannot change the first token, how can this be considered a robust hallucination mitigation strategy? Hallucinations often begin with the first word (e.g., \"Yes,\" or \"I see a...\").\n- The claim that POPE responses \"will keep unchanged\" is a critical admission. Does this mean the method is fundamentally incompatible with any task requiring a single, decisive token output (e.g., VQA)?\n- Algorithm 1  appears to initialize the historical attention A using the attention from the first decoding step. Does this not confirm that pruning can, by definition, only begin at or after decoding step 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wizuL1dlXq", "forum": "sMjESVShz3", "replyto": "sMjESVShz3", "signatures": ["ICLR.cc/2026/Conference/Submission10313/Reviewer_oCY1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10313/Reviewer_oCY1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971566809, "cdate": 1761971566809, "tmdate": 1762921654171, "mdate": 1762921654171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PruneHal, a novel framework aimed at reducing hallucinations in Multimodal Large Language Models (MLLMs) by leveraging adaptive KV cache pruning. Hallucinations, which occur when the model generates content that diverges from the visual input, have been a significant challenge in MLLMs. Existing methods often rely on additional training or computationally expensive inference techniques. The authors propose a training-free solution that improves the model's focus on critical visual information by selectively pruning redundant visual tokens during the inference phase. The method shows promising results, reducing hallucinations with minimal computational overhead and without the need for additional training. PruneHal is also model-agnostic, capable of enhancing the performance of various decoding strategies designed to mitigate hallucinations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. PruneHal is a training-free solution, making it accessible for deployment without additional training costs.\n2. PruneHal introduces minimal computational overhead compared to existing methods that require additional inference steps or training.\n3. The paper includes informative visual attention plots and latency analysis (Figures 3 and 6), which effectively demonstrate how pruning enhances model attention on critical visual tokens while mitigating hallucinations.\n4. The paper presents extensive experiments on a variety of MLLMs, including LLaVA, Qwen2-VL, InstructBLIP, and others, showing consistent improvements in hallucination reduction across different benchmarks (e.g., MME, MMMU, AI2Diagram)."}, "weaknesses": {"value": "1. The paper provides an overview of how adaptive pruning works, but does not fully explain the underlying mechanics that govern the pruning decisions.\n2. The method is evaluated on a variety of MLLMs, but there is little discussion on how **PruneHal** would perform on SOTA models like Qwen2.5-VL, Qwen3-VL, or InternVL3.5.\n3.  A more detailed analysis of failure cases or ablation studies on extreme pruning scenarios would be valuable.\n4. Some related and important works are not included in the discussion or experimental comparison, like HALC[1], MemVR[2], RLAIF-V[3], MINT[4].\n5. There are a few **minor clarity issues** and inconsistencies in notation. For example, some variables related to pruning (e.g., `r`, `t`, `Kvis`) are not fully defined or explained in detail. The presentation could benefit from tighter organization and clearer connections between figures and textual descriptions.\n\n[1] HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding, ICML 2024.\n\n[2] Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models, ICML 2025.\n\n[3] RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness, CVPR 2025.\n\n[4] Mitigating Hallucinations in Large Vision-Language Models via Token Reduction, 2025.02."}, "questions": {"value": "1.  Can you elaborate on the specific decision process for adaptive pruning? How is the threshold for pruning dynamically adjusted during inference?  \n2. Have you considered edge cases where pruning may accidentally discard crucial visual tokens?  \n3. Have you considered evaluating PruneHal on tasks that require multimodal reasoning across longer contexts or temporal reasoning, e.g., video analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yPv36QkgeG", "forum": "sMjESVShz3", "replyto": "sMjESVShz3", "signatures": ["ICLR.cc/2026/Conference/Submission10313/Reviewer_hbeo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10313/Reviewer_hbeo"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997503202, "cdate": 1761997503202, "tmdate": 1762921653767, "mdate": 1762921653767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}