{"id": "vKHCM9O25U", "number": 10779, "cdate": 1758181694069, "mdate": 1759897629314, "content": {"title": "INTENTION MATCHING STOPS JAILBREAKS", "abstract": "Large Language Models (LLMs) are vulnerable to jailbreak attacks even with safety\nalignments. Existing defenses typically lack precise localization of harmful intent,\nleading to ineffective defense when faced with complex jailbreak prompts. For\nprecise localization, we exploit ‘semantic-consistency’ between an input-output\npair: regardless of the jailbreak input complexity, the outputs always respond\naccording to the actual input intents. In this paper, we present SENTINEL, a\nplug and play module that can be fit into the auto-regressive generation process\nfor any model, systematically exploits ‘semantic-consistency’ to extract intent\nfor jailbreaks. Specifically, during generation process, we solve an optimization\nproblem to extract semantically aligned sub-sequences for an input-output pair, then\nwe efficiently quantify the harmfulness by using the refusal direction projection\nvalue, and determine should we halt the generation process or not as the defense.\nExperiments demonstrate that SENTINEL significantly reduces attack success\nrates mostly below 5% for on various jailbreaks across all evaluated LLMs, also\nwe explained the fundamental mechanism as re-distributing jailbreak features from\nalignment blind-spot to aligned regions.", "tldr": "a robust and effficient intention extraction method for LLM jailbreak defense", "keywords": ["LLM safety", "jailbreak defense"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5401cfeb49f4ba89e7b32cc2747d2bb500524f3c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SENTINEL, an online jailbreak detection and defense mechanism that leverages the semantic consistency between input–output pairs to detect jailbreak intent. The paper points out that existing token-level perturbation-based defenses scale poorly with long and complex jailbreak prompts, while context-level defenses can be bypassed through overwriting attacks. To address these limitations, the proposed method exploits semantic consistency to identify the most important harm-related tokens among all input tokens and performs jailbreak detection based on these selected tokens. Experiments demonstrate that the proposed method achieves incremental improvements across multiple models and datasets while maintaining acceptable computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem is interesting and important. Jailbreak attacks and defenses have been extensively explored over the past two years. Existing jailbreak defense methods can be categorized into input-based, output-based, and hidden-state-based approaches. This paper introduces a new perspective by considering the semantic consistency of input–output pairs. The idea is simple, intuitive, and likely correct, as evaluating input criticality based on output positioning has already been applied in other contexts, such as gradient heatmaps.  \n2. The evaluation is comprehensive, including thorough experiments on the effectiveness of the proposed methodology, the rationale behind each component, and interpretability analyses.  \n3. The paper has a clear structure and smooth flow of writing. Its problem-driven reasoning approach makes it easy to follow, while detailed methodological descriptions facilitate reproducibility."}, "weaknesses": {"value": "1. The literature review is not comprehensive. The paper claims that input-perturbation-based defenses and context-level defenses face different challenges. However, other types of defenses—such as those based on output filtering or hidden-layer guidance—are not discussed. While it is impractical to cover every single study in the rapidly growing field of LLM jailbreak research, acknowledging each category of defenses would provide a more complete overview.  \n2. Experimental results show that the improvements are incremental and not significant in many scenarios. As shown in Table 1, in most cases, the proposed method offers only marginal improvements over the second-best approach, which already demonstrates strong defensive capabilities. Under certain attack settings (e.g., RADICAL), the proposed method even exhibits greater vulnerability."}, "questions": {"value": "A question regarding the core approach: What is the difference between evaluating input tokens based on semantic coherence and using gradient-based methods to determine which tokens most significantly affect the output? How do their respective effects compare? Intuitively, could a Grad-CAM-like approach be used to more efficiently identify key input intents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I8ek6T61f4", "forum": "vKHCM9O25U", "replyto": "vKHCM9O25U", "signatures": ["ICLR.cc/2026/Conference/Submission10779/Reviewer_KQWy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10779/Reviewer_KQWy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761457257477, "cdate": 1761457257477, "tmdate": 1762921991489, "mdate": 1762921991489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SENTINEL, a plug-and-play jailbreak defense framework that leverages semantic consistency between input and output to extract intent-related subsequences and quantify their harmfulness via refusal direction projection. The method operates during autoregressive generation without model fine-tuning and demonstrates strong empirical performance across multiple LLMs and attack types."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) Strong empirical results: consistently reduces ASR to <5% across diverse LLMs and jailbreak methods.\n\n(2) Low over-refusal rate on boundary cases.\n\n(3) No model modification required—enables real-world deployment.\n\n(4) Interpretable intent extraction via context matching."}, "weaknesses": {"value": "(1) the defense is reactive (requires partial output generation), not input-only.\n\n(2) Computational overhead from sliding windows and optimization is not quantified.\n\n(3) Robustness against adaptive attackers who manipulate semantic consistency is not thoroughly evaluated.\n\n(4)Refusal direction is layer- and position-specific; its optimality and transferability need deeper analysis."}, "questions": {"value": "1.Can an adaptive attacker craft inputs that induce semantically inconsistent yet harmful outputs to evade matching?\n\n2.How does SENTINEL scale to very long prompts (e.g., >2K tokens) in terms of latency?\n\n3.Is the refusal direction stable across different model architectures (e.g., MoE models)?\n\n4.This method requires the LLM model to output a certain length of token before it can be used, and it also incurs other computational costs. Will this limit its practicality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N7d0Qzq4AB", "forum": "vKHCM9O25U", "replyto": "vKHCM9O25U", "signatures": ["ICLR.cc/2026/Conference/Submission10779/Reviewer_KRAJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10779/Reviewer_KRAJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720541476, "cdate": 1761720541476, "tmdate": 1762921990060, "mdate": 1762921990060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a defense against jailbreaking attacks based on the assumption that the outputs are always semantically aligned with the inputs regardless of the prompt complexity. The paper proposes Sentinel to defend against jailbreak prompts while an LLM is generating content autoregressively without the need for complete generation. The paper evaluates Sentinel on three datasets and compares it against several baselines by applying popular jailbreaking attacks. Results show that Sentinel achieves the lower attack success rate on average while also not having a high false positive rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The assumption that inputs and outputs must bear semantic consistency is intuitive and interesting.\n\n+ The paper seems to have good mathematical foundation to back its claims.\n\n+ The output of the approach is explainable since it gives importance scores to each token.\n\n+ Sentinel does not need to evaluate the complete output and can instead work on chunks as they are generated.\n\n+ Sentinel mostly outperforms other methods on several attacks.\n\n+ The paper presents results on adaptive attacks and the evaluation shows that adaptive Sentinel outperforms the other methods."}, "weaknesses": {"value": "- Sentinel is vulnerable to multi intent or intent mixing attacks such as Radical but the paper mentions this.\n\n- Several hyperparameters must be tuned correctly. For example, it is unclear how to choose the window size or thresholds.\n\n- A case study showing a failure of Sentinel might include insight on why it fails.\n\n- The adaptive attack is simple and does not consider adding irrelevant tokens or optimizing based on feedback."}, "questions": {"value": "- How does your approach perform if the underlying model is not aligned?\n\n- Can you provide a case study or failure examples to show why Sentinel might fail?\n\n- How are the hyperparameters such as window size and thresholds chosen?\n\n- How sensitive is Sentinel to the data used to compute the refusal direction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WAdN7ooCbC", "forum": "vKHCM9O25U", "replyto": "vKHCM9O25U", "signatures": ["ICLR.cc/2026/Conference/Submission10779/Reviewer_o6QR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10779/Reviewer_o6QR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943358438, "cdate": 1761943358438, "tmdate": 1762921989387, "mdate": 1762921989387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose SENTINEL, a jailbreak defense framwork based on semantic-consistency between the input and the output to detect the harmful intention, enabling more fine-grained test time denfense for LLM. SENTINEL identifies potential harmful intent by aligning semantic representations between the input and output contexts through an optimization that learns soft selection over context windows. The experimental results show that SENTINEL effectively reduces jailbreak success rates with minimal over-refusal and latency, offering a training-free and robust approch to LLM safety."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* A novel defense perspective based on intention dection: By modeling the semantic correspondence between input and output contexts, SENTINEL can uncover concealed harmful objectives even when the attack is obfuscated, enabling more robust and interpretable jailbreak defense.\n* SENTINEL can be integrated into existing LLM without additional fine-tuning, making it a practical defense solution for real-world deployment.\n* SENTINEL employs a probabilistic context\u0002matching mechanism that softly selects semantically aligned input–output segments to\nreveal the real intention. The design enhances robustness against disguised jailbreak prompts and provides clear interpretability."}, "weaknesses": {"value": "* Limitation in evaluating intent-encoding jailbreaks: SENTINEL’s malicious intention extraction is based on contiguous token windows, which may limit its robustness against intent-encoding jailbreaks like DRA[1], where harmful instructions are encoded into seperate representation. The paper does not include experiments evaluating its effectiveness under\nsuch attack types.\n* Limitation in general capability evaluation: The paper does not include experiments assessing the impact of SENTINEL on the model’s general capabilities, such as helpfulness or non\u0002harmful task performance.\n* The writing quality of this paper should be improved. I found multiple parts of this paper is hard to read (e.g., methods).\n* No comparison with papers that detecting jailbreaks through input-and-output analysis.\n\n\n\n[1]. Liu, Tong, et al. \"Making them ask and answer: Jailbreaking large language models in few queries via\ndisguise and reconstruction.\" 33rd USENIX Security Symposium (USENIX Security 24). 2024."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AybKnmPrGX", "forum": "vKHCM9O25U", "replyto": "vKHCM9O25U", "signatures": ["ICLR.cc/2026/Conference/Submission10779/Reviewer_Hj5F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10779/Reviewer_Hj5F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101758113, "cdate": 1762101758113, "tmdate": 1762921988995, "mdate": 1762921988995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}