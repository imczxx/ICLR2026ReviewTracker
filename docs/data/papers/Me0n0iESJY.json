{"id": "Me0n0iESJY", "number": 2761, "cdate": 1757241662070, "mdate": 1763553313666, "content": {"title": "OptMerge: Unifying Multimodal LLM Capabilities and Modalities via Model Merging", "abstract": "Foundation models update slowly due to resource-intensive training, whereas domain-specific models evolve rapidly between releases. Model merging seeks to combine multiple expert models into a single, more capable model, reducing storage and serving costs while supporting decentralized development. Despite its potential, previous studies have primarily focused on merging visual classification models or Large Language Models (LLMs) for code and math tasks. Recently, Multimodal LLMs (MLLMs) that extend LLMs through large-scale multimodal training have gained traction. However, no benchmark exists for model merging research that clearly divides the tasks of MLLM training and evaluation. In this paper, $(i)$ we introduce a model merging benchmark for MLLMs, which includes multiple tasks such as VQA, Geometry, Chart, OCR, and Grounding, studying both LoRA and full fine-tuning models. Moreover, we explore how model merging can combine different modalities (e.g., vision-language, audio-language, and video-language models), moving toward the Omni-language model. $(ii)$ We implement 10 model merging algorithms on the benchmark. Furthermore, we propose a novel method that removes noise from task vectors and robustly optimizes the merged vector based on a loss defined over task vector interactions, achieving an average performance gain of 2.48\\%. $(iii)$ We find that model merging offers a promising way for building improved MLLMs without requiring training data. Our results also demonstrate that the complementarity among multiple modalities outperforms individual modalities.", "tldr": "We introduce the first MLLM merging benchmark, along with a novel approach and theoretical insights.", "keywords": ["Model Merging", "Task Vector", "Data-Free Optimization"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37d69e3542cafd3b8573a1a1c70987aa3602912c.pdf", "supplementary_material": "/attachment/1257052310dd396bbf0b6c0d163f6ec414aad6d2.zip"}, "replies": [{"content": {"summary": {"value": "This paper explores data-free model merging as a scalable alternative to retraining multimodal large language models (MLLMs). Model merging offers a cost-effective path to consolidate multiple expert models into a single MLLM capable of broader multimodal understanding.\n\nThe paper makes three main contributions.\n(1) Benchmark: It introduces the first comprehensive benchmark for MLLM model merging, covering diverse capabilities such as visual question answering (VQA), geometry, chart reasoning, OCR, and grounding.\n(2) Methodology: The authors propose OptMerge, an optimization-based algorithm that denoises and regularizes task vectors (parameter differences between fine-tuned and base models) through singular value decomposition and robust loss minimization. This design stabilizes data-free optimization, mitigates parameter interference, and adapts to both full and low-rank (LoRA) fine-tuning regimes.\n(3) Experiments: OptMerge achieves the best or second-best results, improving average performance by 2.48% over prior methods and even surpassing mixture-training baselines. It effectively merges heterogeneous modality-specific models (vision, audio, video), confirming strong cross-modal complementarity. The approach reduces training time by over 90% and memory by more than 10× compared with data mixing, showing significant efficiency gains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work introduces a new benchmark for Multimodal LLM (MLLM) model merging, covering five capabilities (VQA, Geometry, Chart, OCR, Grounding), with carefully curated training and evaluation datasets. authors proposes OptMerge, a new merging method that improves robustness via task vector denoising and SVD-based low-rank approximation.  The proposed method unifies capability and modality merging within one framework, including vision, audio, and video encoders, which has not been systematically explored in prior literature.\n\nAs for technical contributions, this work\n\n- Implements and evaluates 10 merging baselines, categorizing them into linear interpolation, sparsification, SVD, and optimization-based methods.\n- Provides theoretical analysis (Theorem 3.1) on how parameter drift during fine-tuning affects mergeability, offering insights into fine-tuning strategy design.\n- Includes ablation studies and detailed comparisons on both synthetic and real-world Hugging Face checkpoints, demonstrating consistent improvements.\n\nGenerally, this work demonstrates that model merging can outperform mixture training without any data, offering a scalable, resource-efficient solution to building Omni-MLLMs."}, "weaknesses": {"value": "1. Unclear Motivation for a New Benchmark\nThe paper motivates the introduction of a new MLLM merging benchmark by arguing that no existing dataset separates model training and evaluation tasks. However, the necessity of building a new benchmark is not fully justified. Prior multimodal evaluation suites such as MMBench, MME, or VLMEvalKit already cover a broad range of MLLM capabilities (e.g., VQA, OCR, grounding). The authors could better clarify why existing benchmarks are insufficient—e.g., whether they lack fine-grained control of task specialization, incompatibility with merging-based evaluation, or insufficient modality coverage. Without this clarification, the benchmark may appear redundant rather than essential.\n\n2. Incomplete Description of Benchmark Construction and Data Distribution\nAlthough Table 1 summarizes datasets (e.g., GQA, GeoQA+, ChartQA, TextVQA, RefCOCO), the paper does not detail how these datasets are split, standardized, or balanced across tasks. The rationale for setting the minimum of 100k samples per task is not explained, nor is there any discussion on inter-task overlap or label consistency. Furthermore, the benchmark’s evaluation distribution (training–validation splits, number of samples per modality, or balance among English and Chinese data) remains vague. A more transparent benchmark construction pipeline—such as annotation filtering, normalization steps, or sampling policies—would greatly enhance reproducibility and fairness.\n\n3. Limited Novelty in Algorithmic Design\nWhile OptMerge introduces a low-rank denoising mechanism for task vectors, it conceptually extends prior optimization-based approaches such as WUDI Merging (Cheng et al., 2025) and TSV Merging (Gargiulo et al., 2025). The innovation mainly lies in the combination of SVD truncation and loss reformulation rather than a fundamentally new optimization paradigm. The paper could clarify theoretical differences or provide stronger justification for why its design uniquely improves over existing baselines. In addition, Can authors explain the meanin of O in Eq.2? It is hard for the reviewers such as me who are not familiar with related topics.\n\n4. Restricted Theoretical Depth\nTheoretical analysis (Theorem 3.1) only establishes an upper bound proportional to the fine-tuning learning rate and iteration count. However, this bound is loose and lacks quantitative verification or deeper connection to empirical results. A stronger theoretical framework—e.g., convergence analysis of the merged vector optimization—would improve credibility.\n\n5. Practical Deployment Considerations\nThe discussion overlooks issues like merging models with different tokenizer vocabularies, scaling to larger parameter sizes (e.g., 30B+), or stability under mixed fine-tuning frameworks. Addressing these limitations would make the contribution more applicable to real-world model integration scenarios."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "trYFGth4ZP", "forum": "Me0n0iESJY", "replyto": "Me0n0iESJY", "signatures": ["ICLR.cc/2026/Conference/Submission2761/Reviewer_4XK6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2761/Reviewer_4XK6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578929444, "cdate": 1761578929444, "tmdate": 1762916363881, "mdate": 1762916363881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OptMerge, a novel, data-free method for unifying multiple specialized Multimodal Large Language Models (MLLMs) into a single, more capable model. The authors present a new MLLM merging benchmark that is the first to evaluate unifying both task-specific capabilities (like VQA, OCR, and Geometry) and different modalities (vision, audio, and video). The proposed OptMerge algorithm achieves an average performance gain of 2.48% by robustly optimizing the model parameters, using techniques like low-rank approximation to remove noise and stabilize the merging process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Building benchmark for model merging in Multimodal LLMs (MLLMs). The paper introduces the first model merging benchmark specifically designed for MLLMs. It is a good contribution as the author claims that they are the first to evaluate the unification of both diverse task-specific capabilities (e.g., VQA, Geometry, Chart, OCR, and Grounding) and different modalities (vision, audio, and video).\n\n2. The proposed method OptMerge, is simple and effective. OptMerge method is data-free and exceptionally efficient compared to traditional \"mixture training\". It may be a cost-effective solution for developing powerful MLLMs.\n\n3. Experiments demonstrate the robustness and effectiveness of the method. The OptMerge algorithm demonstrates good empirical performance, outperforming 10 other merging methods and even matching or surpassing the results of full mixture training. Its strength lies in its tailored approach: it robustly handles full fine-tuned models by using low-rank approximation (SVD) to remove noise and effectively merges LoRA models by using techniques like SGD and mean initialization to prevent the common problem of norm explosion and performance collapse."}, "weaknesses": {"value": "1. Lack of discussion on model scale. This paper's experiments are based on InternVL2.51B-Instruct and Qwen2-VL-7B-Base. However, they are different model series. To observe the generalization of the model scale, it is more reasonable to conduct experiments on the same model series. For example, InternV2.5 has 1B, 2B, and 8B model scales.\n\n2. Lack of evaluating the merged models on general multimodal QA tasks. This paper merges the checkpoints of 5 abilities, including VQA, Geometry, Chart, OCR, and Grounding. Does the merged show emergent integrated capabilities of these 5 abilities? For example, if a question requires both capabilities of OCR and Grounding, it can not be solved by the model with only the capability of OCR or Grounding, but the merged model may have integrated capabilities to solve it."}, "questions": {"value": "Could you explain the relationships between model merging and federated learning? It seems they are very similar."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wer0Vze217", "forum": "Me0n0iESJY", "replyto": "Me0n0iESJY", "signatures": ["ICLR.cc/2026/Conference/Submission2761/Reviewer_iMYB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2761/Reviewer_iMYB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783570497, "cdate": 1761783570497, "tmdate": 1762916363697, "mdate": 1762916363697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on **low-cost integration of specialized MLLMs** (e.g., VQA, OCR, audio-language models) into a unified, high-performance model via model merging, addressing pain points like fragmented domain models and high retraining costs for new modalities. Its core contributions are as follows:  \n\n### 1. First MLLM Merging Benchmark  \nIt establishes the first standardized benchmark for MLLM merging, covering:  \n- 5 key MLLM capabilities (VQA, Geometry, Chart, OCR, Grounding) and cross-modal scenarios (vision/audio/video-language);  \n- Checkpoints of 2 mainstream models (InternVL2.5, Qwen2-VL) with 2 fine-tuning formats (full-tuning, LoRA);  \n- Public training/evaluation data and code for reproducibility.  \n\n\n### 2. Novel Merging Method (OptMerge)  \nOptMerge solves noise and instability in existing merging:  \n- For full-tuned models: Uses SVD to denoise task vectors, retaining core knowledge;  \n- For LoRA models: Adopts SGD optimization and truncated SVD to avoid unstable vector scaling;  \n- Outperforms baseline methods (e.g., WUDI, TIES) by 2.48% on average.  \n\n\n### 3. Validating Merging Feasibility  \nExtensive experiments show:  \n- Merged models outperform single specialists and approach mixed-data fine-tuning results;  \n- Cross-modal merging (e.g., vision+audio) boosts multimodal tasks (e.g., Audio-VQA accuracy);  \n- 8.6% less memory and 84% less time than mixed-data training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### 1. Well-Aligned and Impactful Research Motivation  \nThe paper addresses two long-standing, practical pain points in MLLM development that prior work has largely overlooked: (1) the fragmentation of domain-specialized MLLMs (e.g., VQA, OCR, geometry reasoning) in open-source communities, which incurs high storage/deployment costs and fails to leverage cross-task synergy; (2) the lack of standardized benchmarks for MLLM merging—existing frameworks focus on single-modal LLMs or vision classifiers, leaving MLLM-specific merging (e.g., cross-modality integration, LoRA compatibility) unevaluable. By directly targeting these gaps, the research motivation is highly relevant to both academic progress and industrial deployment, laying a clear foundation for its significance.  \n\n\n### 2. Strong Originality with Two Key Innovations  \nThe paper demonstrates notable originality in both benchmark design and merging methodology:  \n- **First MLLM-Specific Merging Benchmark**: Unlike generic MLLM evaluation benchmarks (e.g., MME, LLaVA-bench) that only measure single-model performance, this work constructs the first benchmark tailored for merging—covering 5 core MLLM capabilities (VQA, Geometry, etc.) and cross-modal scenarios (vision/audio/video-language), with publicly available checkpoints (full-tuning/LoRA) and datasets. This fills a critical void in standardizing MLLM merging research.  \n- **OptMerge: Fusion-Method Adaptation for MLLMs**: Instead of repurposing single-modal merging techniques (e.g., TIES, WUDI), OptMerge addresses MLLM-specific challenges: it uses SVD denoising for full-tuned models (to eliminate redundant task-vector noise) and SGD+truncated SVD for LoRA models (to avoid unstable vector scaling). This targeted design—rather than one-size-fits-all—removes key limitations of prior methods in MLLM scenarios.  \n\n\n### 3. Rigorous and Reproducible Experimental Design  \nThe experimental work exhibits high quality and clarity, supporting its conclusions convincingly:  \n- **Comprehensive Comparisons**: It evaluates 10 mainstream merging methods (e.g., Weight Average, Task Arithmetic) across 3 base models (InternVL2.5, Qwen2-VL, Vicuna-7B) and 2 fine-tuning formats, controlling variables (e.g., learning rate, hardware: 8×V100) to ensure fair comparisons.  \n- **Full Reproducibility**: All checkpoints, training/evaluation datasets, and code are made public. Key details (e.g., SVD top-k selection, optimizer parameters) are explicitly reported, eliminating ambiguity for follow-up work.  \n- **Theoretical + Empirical Support**: Beyond experimental results, the paper derives a theorem (Theorem 3.1) linking fine-tuning intensity (learning rate/epochs) to merging performance, providing a theoretical basis for model selection—avoiding purely empirical-driven conclusions. Results are also presented clearly via tables/figures (e.g., cost comparisons vs. mixed-data training), enhancing readability."}, "weaknesses": {"value": "## 1. The MLLM merging benchmark lacks coverage for practical scenarios  \nThe benchmark’s design is restricted in two key ways that limit its utility for real-world merging:  \n- **Model scale gap**: Experiments only use small-to-medium parameter models (1B InternVL2.5, 7B Qwen2-VL/Vicuna), while practical deployments rely on large-scale MLLMs (70B+; e.g., Qwen2-VL-72B). Large models have unique traits (higher parameter redundancy, sparser gradients) that may break OptMerge’s current logic (e.g., SVD denoising could become computationally prohibitive or fail to capture fine-grained task knowledge).  \n- **Training data bias**: The benchmark focuses heavily on vision-related tasks (VQA, geometry, OCR) but omits critical MLLM use cases—text-centric capabilities (multi-turn dialogue, summarization) and vertical domain data (medical imaging, industrial defect detection). This makes it hard to compare merging methods across the diverse scenarios MLLMs actually serve.  \n\n\n## 2. OptMerge’s generalization is unvalidated  \nThe benchmark’s limitations directly raise doubts about OptMerge’s practicality:  \n- The benchmark covers only 5 types of vision task, which fails to meet the comprehensive evaluation standards required for practical MLLM applications. \n- It lacks validation on non-vision tasks (text-centric/vertical domains), so it is unclear if OptMerge can retain task-specific knowledge outside the vision scope.  \n\n\nThese weaknesses are actionable, not fatal. The work’s core value—establishing a standardized MLLM merging benchmark and exploring data-free multimodal integration—remains highly impactful. Future improvements could expand the benchmark to 70B+ models, heterogeneous architectures, and more vision and non-vision tasks."}, "questions": {"value": "Same as the Section of **Weaknesses**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RWbKTZ9nvG", "forum": "Me0n0iESJY", "replyto": "Me0n0iESJY", "signatures": ["ICLR.cc/2026/Conference/Submission2761/Reviewer_3WMa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2761/Reviewer_3WMa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902142729, "cdate": 1761902142729, "tmdate": 1762916363480, "mdate": 1762916363480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the \"no data\" model fusion problem in multi-modal large language models (MLLMs), with main contributions including:\n- Establishing the first benchmark for model fusion aimed at MLLMs, covering five capability tasks: VQA, Geometry, Chart, OCR, and Grounding, as well as visual-audio-video tri-modal fusion scenarios, and publicly releasing the corresponding data and checkpoints;\n- Proposing the OptMerge method, which enhances fusion stability and performance based on the existing WUDI-Merging by low-rank denoising of task vectors, introducing SGD, and mean initialization strategies;\n- OptMerge outperforms 10 existing baselines in multi-task and multi-modal scenarios, exceeding mixed training methods in certain tasks while having significantly lower computational and storage costs. The paper also provides an O(ηT) theoretical analysis linking fine-tuning steps/learning rates to fusion upper bound errors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Sufficient motivation for the problem: The value of model fusion in terms of storage, inference costs, and community collaboration is prominent.\n2. Detailed benchmark construction: Adequate data volume, clear task decomposition, covering both LoRA and full parameter fine-tuning scenarios.\n3. The paper is generally easy to read."}, "weaknesses": {"value": "1. General algorithm innovation: The core ideas (SVD low-rank truncation, SGD implicit regularization, mean initialization) are relatively common and represent engineering improvements.\n2. The \"no data\" expression is somewhat exaggerated: λ relies on validation set grid search, and the choice of k also references test set statistics."}, "questions": {"value": "1. How sensitive is k (the SVD truncation rank)? It is suggested to provide ablation studies with different k values.\n2. λ only globally searches 6 points; has there been an attempt at hierarchical λ?\n3. The mathematical evaluation answer analysis relies on GPT-4o-mini; what is the complete prompt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NHoKhE97uV", "forum": "Me0n0iESJY", "replyto": "Me0n0iESJY", "signatures": ["ICLR.cc/2026/Conference/Submission2761/Reviewer_Kqrb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2761/Reviewer_Kqrb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762451135030, "cdate": 1762451135030, "tmdate": 1762916363327, "mdate": 1762916363327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}