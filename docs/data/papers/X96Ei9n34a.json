{"id": "X96Ei9n34a", "number": 2020, "cdate": 1756977158496, "mdate": 1763572427760, "content": {"title": "Stable Video Infinity: Infinite-Length Video Generation with Error Recycling", "abstract": "We propose **Stable Video Infinity (SVI)** that can generate non-looping, infinite-length videos with stable visual quality, while supporting per-clip prompt control and multi-modal conditioning. While existing long-video methods attempt to _**mitigate accumulated errors**_ via handcrafted anti-drifting (e.g., modified noise scheduler, frame anchoring), they remain limited to single-prompt extrapolation, producing homogeneous scenes with repetitive motions. We identify that the fundamental challenge extends beyond error accumulation to a critical discrepancy between the training assumption (seeing clean data) and the test-time autoregressive reality (conditioning on self-generated, error-prone outputs). To bridge this hypothesis gap, SVI incorporates **Error-Recycling Fine-Tuning**, a new type of efficient training that recycles the Diffusion Transformer (DiT)’s self-generated errors into supervisory prompts, thereby encouraging DiT to _**actively identify and correct its own errors**_. This is achieved by injecting, collecting, and banking errors through closed-loop recycling, autoregressively learning from error-injected feedback. Specifically, we (i) inject historical errors made by DiT to intervene on clean inputs, simulating error-accumulated trajectories in flow matching; (ii) efficiently approximate predictions with one-step bidirectional integration and calculate errors with residuals; (iii) dynamically bank errors into replay memory across discretized timesteps, which are resampled for new input. SVI is able to scale videos from seconds to infinite durations with no additional inference cost, while remaining compatible with diverse conditions (e.g., audio, skeleton, and text streams). We evaluate SVI on three benchmarks, including consistent, creative, and conditional settings, thoroughly verifying its versatility and state-of-the-art role. [Project page](https://anonymous.4open.science/w/Stable-Video-Infitity-51DE/)", "tldr": "", "keywords": ["Infinite-Length Video Generation", "Error Accumulation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79da08c3ed5eaacef5bc2b929412c56b0bf025c7.pdf", "supplementary_material": "/attachment/e2093491dd21fabef4091ca1785ff6937ba6ef86.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents Stable Video Infinity, which can generate infinite long video without quality degradation. It regards the long-video generation problem as a train-test hypothesis gap issue rather than a mere accumulation of generation error. During training, models are fed clean ground-truth latents and reference frames; at inference, to generate long video, they have to autoregressively condition on their own previously-generated, error-containing outputs. This mismatch brings two coupled error modes: single-clip predictive error and cross-clip conditional error. Instead of engineering stronger noise-schedulers or anchor frames, the proposed method recycle the model’s own error back into the training to avoid quality drop during generation. I generally like this work and only have minor questions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of considering the training and inference gap is interesting. Unlike previous methods that generally considering heuristic drift mitigation, SVI aims to loop the generated error into training to reduce the gap during testing.\n\n2. A Comprehensive analysis on the accumulated error in video generation is provided.\n\n3. Comprehensive experiments with various setting are conducted. The proposed method achieve higher results compared to other competitors."}, "weaknesses": {"value": "1. While injecting the error is interesting, in implementation it is not the online error injecting, which means the injected error is exactly the generated one, but instead an error sampled from the bank. This may cause some theoretical issues.\n\n2. The memory bank is capped at 500. However, the bank is filled by federated gathering across GPUs; if each GPU sees only a very few of videos, the effective diversity may be far smaller than 500, leading to the homogenization of errors."}, "questions": {"value": "Please see the questions in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KfrwRd7oFM", "forum": "X96Ei9n34a", "replyto": "X96Ei9n34a", "signatures": ["ICLR.cc/2026/Conference/Submission2020/Reviewer_rurm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2020/Reviewer_rurm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761305795298, "cdate": 1761305795298, "tmdate": 1762915992667, "mdate": 1762915992667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the fundamental challenge in generating very long videos: the discrepancy between error-free training with clean data and error-accumulating inference, which autoregressively operates on self-outputs with increasing error accumulation. They propose Error-Recycling Fine-tuning, which integrates the historical errors (degraded input) into training, to minimize the train-test gap. SVI deliberately injects historical errors into clean inputs and learns to predict an error-recycled velocity. THeir method achieves strong performance relative to baselines on their video generation benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper motivates well the core challenge for long video generation, i.e., that errors accumulate from self-outputs, and their proposed solution, i.e., put self-predicted errors into training.\n2. Their proposed method achieves strong performance relative to baselines on their video generation benchmark.\n3. The ablation study for each error term was helpful to empirically demonstrate that the image error term was the most important piece of their training strategy."}, "weaknesses": {"value": "1. I think the paper would benefit from hedging its claims a little more. For example, they say \"we propose Stable Video Infinity (SVI) that can generate infinite-length videos with temporally coherent and visually plausible context following a long storyline.\" This seems like a stretch considering that the evaluations are on the order of a few minutes, and it is challenging to evaluate the storyline.\n2. The paper would benefit from comparing how long it takes for training and inference for their method relative to baseline methods. Does the proposed method take longer to train? Does this method take longer for inference?\n3. The paper develops their own benchmark (which is well-motivated), but it would also be helpful to compare their proposed method against baselines on an existing benchmark (or a subset of one, if computationally expensive)."}, "questions": {"value": "1. There are other papers with a similar idea of addressing this fundamental train-test prediction error gap in long video generation by training models with their own outputs/errors. These papers are relatively recent, but it might still be beneficial to discuss: how is the proposed method (conceptually) similar to and different from these other works? Here are some examples:\n- Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion\n- Rolling Forcing: Autoregressive Long Video Diffusion in Real Time\n- others?\n2. In Figure 5, why do some metrics show a decrease then increase?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ehV6oLpcdF", "forum": "X96Ei9n34a", "replyto": "X96Ei9n34a", "signatures": ["ICLR.cc/2026/Conference/Submission2020/Reviewer_8N9r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2020/Reviewer_8N9r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761527736986, "cdate": 1761527736986, "tmdate": 1762915992519, "mdate": 1762915992519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Stable Video Infinity, a novel framework for infinite-length video generation using a Diffusion Transformer (DiT). The core contribution is an Error-Recycling Fine-Tuning (ERFT) strategy that addresses the train–test hypothesis gap in long video generation. Unlike previous anti-drifting or noise-rescheduling approaches that merely alleviate accumulated prediction errors, SVI explicitly recycles self-generated errors as supervisory prompts. The method injects historical errors into clean inputs, approximates error trajectories via one-step bidirectional integration, stores them in replay memory, and selectively resamples them for future timesteps. This closed-loop tuning enables DiTs to actively correct their own mistakes, scaling short-video models to arbitrarily long sequences. Extensive experiments across consistent, creative, and conditional benchmarks demonstrate superior temporal stability, scene diversity, and multimodal controllability over prior methods such as FramePack, StreamingT2V, and HistoryGuidance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The research problem of error accumulation in long video generation is crucial, and the proposed pipeline effectively addresses it.  \n2. The experiments cover various scenarios (long videos, creative storytelling, multimodal control), providing both quantitative and qualitative results with well-defined baselines.  \n3. The manuscript is well-organized, with clear figures (especially Fig. 1–4) illustrating the motivation and the training-test gap."}, "weaknesses": {"value": "1. The models are only fine-tuned on small datasets (6k short clips), raising concerns about robustness and scalability to large-scale or diverse domains.\n2. The full pipeline (error banking, resampling, bidirectional integration) may appear complex relative to the simplicity of the LoRA tuning objective.\n3. Compared with Self-Forcing[1], SVI operates on the data distribution rather than the objective level. It injects historical errors to simulate AR noise, whereas Self-Forcing constrains self-generated rollouts explicitly. This makes SVI more lightweight but potentially less principled in capturing temporal feedback dynamics.\n4. As discussed in 3, while SVI effectively stabilizes autoregressive generation, its bidirectional error curation only approximates local temporal errors. It remains unclear whether the model genuinely learns long-range dependencies or simply achieves local consistency via error smoothing.\n\n[1] Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion"}, "questions": {"value": "For creative generation, how does SVI balance between temporal coherence and semantic diversity—are there cases where error correction overly smooths scene transitions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S57ngcOuNa", "forum": "X96Ei9n34a", "replyto": "X96Ei9n34a", "signatures": ["ICLR.cc/2026/Conference/Submission2020/Reviewer_ci1H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2020/Reviewer_ci1H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911564032, "cdate": 1761911564032, "tmdate": 1762915992317, "mdate": 1762915992317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Error-Recycling Fine-Tuning (ERFT), a method designed to reduce the train–test hypothesis gap in autoregressive video diffusion models. During training, clean historical frames are typically assumed, whereas test-time generation depends on error-accumulating predictions. ERFT re-injects past model errors back into the conditioning frames, allowing the model to learn corrective behaviors. Experiments across diverse conditions and video models demonstrate strong gains in temporal stability and long-horizon video coherence."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper explains the train–test mismatch and derives ERFT step-by-step in a smooth narrative. Table 4 strongly supports the reasoning behind the method.\n- Figure 2 and Figure 4 present the mathematical formulation as intuitive diagrams, making the approach easy to understand even without flow matching background.\n- The method improves consistency in diverse scenarios (text-to-video, audio-conditioned video, skeleton-conditioned video) and demonstrates broad applicability.\n- Figure 5 and the qualitative samples show a large gap between baseline and finetuned models, especially as video length increases. The improvements in scene stability and reduction of drift are visually obvious and compelling.\n- The appendix provides thoughtful analysis of current boundaries (e.g., identity consistency, scene transitions) and potential directions, giving the work strong research depth."}, "weaknesses": {"value": "- As acknowledged by the authors (appendix B.3), the model can exhibit identity drift or swapping when the main character exits and re-enters the scene.\n- The paper claims “infinite” or arbitrarily long sequences, but experiments demonstrate up to ~250 seconds. It remains uncertain how the model performs over 10+ minutes, where identity consistency, looping, or mode collapse may appear."}, "questions": {"value": "- Appendix B.3 discusses identity limitations during scene transitions. Do you intend to develop explicit mechanisms to maintain subject consistency across transitions, and will this direction be included in the current work or future extensions?\n- How stable is ERFT for extremely long generations (e.g., 10–30 minutes)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IdWjwMAB52", "forum": "X96Ei9n34a", "replyto": "X96Ei9n34a", "signatures": ["ICLR.cc/2026/Conference/Submission2020/Reviewer_WpZa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2020/Reviewer_WpZa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997980689, "cdate": 1761997980689, "tmdate": 1762915992085, "mdate": 1762915992085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}