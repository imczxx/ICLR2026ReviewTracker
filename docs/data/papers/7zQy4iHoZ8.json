{"id": "7zQy4iHoZ8", "number": 18544, "cdate": 1758288969025, "mdate": 1759897096887, "content": {"title": "ContextKeeper: Head-Specific KV Cache Retention for Long-Context LLM Inference", "abstract": "Large Language Model (LLM) inference commonly requires caching all Key-Value (KV) states. This KV cache leads to substantial memory usage and increasing latency in long-context settings. Existing KV cache compression methods reduce cache size by keeping only tokens relevant to the current query, but discarding middle context tokens needed by queries in later turns - harming multi-turn fidelity. We observe head specialization: a minority of attention heads are Context-Anchored (CA), preferring middle context tokens, while most are locality heads, focusing on sink tokens and recent tokens. This motivates ContextKeeper, a training-free, head-specific KV retention policy that preserves all middle context tokens for CA heads and drops them for locality heads. Unlike prior head-splitting methods that require complex training procedures or deliver limited gains, our policy is derived by running inference on a small set of task samples and integrates as a plug-and-play inference strategy. ContextKeeper reduces KV cache size by up to 3.86× and lowers decoding latency by up to 1.25×, while introducing negligible accuracy loss compared to full attention across different models and 5-turn queries with up to 128K tokens. These results demonstrate a practical and scalable query-agnostic KV compression method that preserves multi-turn fidelity under tight memory budgets for long-context deployment.", "tldr": "A head-specific KV cache retention method that preserves context for the heads that need it, maintaining multi-turn fidelity while reducing memory.", "keywords": ["LLM Inference Acceleration; Long Context; KV Cache Compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aae5de3f9310f924e617d254a0fa27699ac29fbf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ContextKeeper, a training-free, head-specific KV cache retention method for long-context LLM inference. The key observation is that attention heads specialize differently: only a small subset consistently attends to middle-context tokens, while most focus on sink or recent tokens. ContextKeeper identifies these Context-Anchored (CA) heads through a lightweight inference-only profiling procedure that measures each head’s attention to context tokens across a few tasks. During inference, it retains full KV caches only for CA heads and keeps reduced caches for others, implemented efficiently via a FlashAttention-compatible design. Experiments on Llama-2-7B and Llama-3.1-8B across LongBench and SCBench show up to 3.86× memory reduction and 1.25× decoding speedup with near-full accuracy, preserving multi-turn fidelity better than prior methods like RazorAttention and DuoAttention. The method offers a simple, practical, and scalable approach to efficient long-context LLM deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. ContextKeeper identifies CA heads entirely through inference-time profiling, requiring only a few samples and no retraining, which makes it plug-and-play for existing models.\n2. Across multiple benchmarks (LongBench, SCBench) and two architectures (MHA, GQA), the method achieves substantial KV memory reduction (up to 3.86×) and decoding speedup (≈1.25×) with minimal accuracy degradation, outperforming prior methods such as RazorAttention and DuoAttention."}, "weaknesses": {"value": "1. The overall concept of ContextKeeper—splitting attention heads into those with full KV caches and those with truncated ones, is not brand new. The proposed framework is still similar to existing work such as DuoAttention.\n2. Despite large KV memory reduction, the reported decoding speedup (≈1.25×) is relatively small and may not translate into practical end-to-end efficiency gains in practical use cases, particularly the multi-batch or distributed inference settings."}, "questions": {"value": "Could you provide a latency comparison between ContextKeeper and other KV compression methods such as DuoAttention, SnapKV, and RazorAttention under the same setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x0g30tW3Xq", "forum": "7zQy4iHoZ8", "replyto": "7zQy4iHoZ8", "signatures": ["ICLR.cc/2026/Conference/Submission18544/Reviewer_kU1K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18544/Reviewer_kU1K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761184466707, "cdate": 1761184466707, "tmdate": 1762928246903, "mdate": 1762928246903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ContextKeeper, a KV cache eviction strategy for LLM inference with long multi-turn contexts. The key claim is that different attention heads specialize: a small subset of “Context-Anchored (CA) heads” consistently attends to the middle of the conversation history across tasks, whereas most other heads primarily attend either the global sink tokens at the very beginning or the most recent tokens.\n\nAt inference time, ContextKeeper keeps the full KV cache only for those CA heads, while aggressively truncating KV for the other “locality heads,” which only retain sink + a rolling recent window. \n\nThe paper reports: up to ~3.86× KV memory reduction and ~1.25× decoding speedup, while maintaining ~89% of full-KV quality on SCBench, and near-parity on long-context benchmarks such as LongBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper emphasizes that identifying CA heads is “training-free”; it only requires running inference on a profiling set and computing attention statistics + consistency voting across samples, unlike methods that require optimizing gates or doing lightweight finetuning to classify retrieval heads.\n2. The authors evaluate on SCBench (multi-turn / agent-style interactions) and ContextKeeper preserves ~90% of full-cache quality and often outperforms other KV-pruning baselines (including DuoAttention, Razor, etc.) in 7/9 categories, while still giving large KV savings."}, "weaknesses": {"value": "1. Novelty vs. DuoAttention feels incremental:\n- Conceptually, the method is extremely close to DuoAttention that (i) partitions attention heads into two groups, and (ii) keeps full KV history for the “important” group while aggressively pruning KV for the rest. DuoAttention refers to these as “retrieval heads” and “streaming heads,” and applies full-cache vs. sliding-window retention accordingly.\n- This paper replaces “retrieval heads” with “Context-Anchored heads,” and replaces DuoAttention’s identification procedure with an offline profiling procedure that measures which heads consistently allocate attention scores to the middle of the context across a few evaluation tasks. The profiling is indeed more efficient to run, which is an advantage. \n- However, at a high level, the policy is the same, while only the identification of heads with full KV cache changes. Thus, the contribution risks being viewed as an incremental variant of DuoAttention.\n2. A main claim of the paper is that, unlike prior KV eviction algorithms, the proposed method preserves critical mid-context for later turns in multi-round dialogue. While the reported results are competitive against other compression baselines, the performance is still far below full attention on SCBench. In other words, the method is not yet \"near-lossless\" in the multi-turn regime it claims to specialize in.\n3. DuoAttention explicitly reports gains not only in decode latency but also in prefill throughput, because it prunes the KV cache already during the prefilling. That matters a lot in long-context understanding scenarios where the prefill stage dominates end-to-end latency. By contrast, the proposed method mainly helps during decoding."}, "questions": {"value": "1. How often do CA heads and DuoAttention’s retrieval heads actually disagree? Specifically, for a given model, what fraction of heads are selected by one method but not the other? Could you show a Venn diagram + ablation (“remove only the heads in the symmetric difference”) to prove that CA heads are not just a renaming?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xHw5pxQlZD", "forum": "7zQy4iHoZ8", "replyto": "7zQy4iHoZ8", "signatures": ["ICLR.cc/2026/Conference/Submission18544/Reviewer_1ay8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18544/Reviewer_1ay8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619176054, "cdate": 1761619176054, "tmdate": 1762928246439, "mdate": 1762928246439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ContextKeeper, a training-free, head-specific KV cache retention strategy for long-context LLM inference. The method builds on the observation that a small subset of attention heads, termed Context-Anchored (CA) heads, consistently attend to middle-context tokens, while most focus on sink and recent tokens. Preserving full caches only for CA heads maintains multi-turn fidelity and yields substantial memory and latency savings with minimal accuracy loss on standard long-context benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Presents a lightweight, training-free identification procedure and a plug-and-play KV cache retention design that is compatible with FlashAttention.\n2. Offers clear organization and writing, with motivation, methodology, and experimental results presented in a manner that is easy to follow.\n3. Provides extensive experimental evidence showing that ContextKeeper consistently outperforms competitive baselines across diverse long-context benchmarks."}, "weaknesses": {"value": "1. Lack of novelty, as similar head specialization concepts have been explored in RazorAttention and DuoAttention.\n2. Insufficient discussion of the advantages of the proposed head identification method compared with DuoAttention."}, "questions": {"value": "1. ContextKeeper and DuoAttention adopt similar strategies, both utilizing a synthetic dataset to identify different head types, and the definition of CA heads is closely aligned with that of retrieval heads. The primary distinction lies in the identification method. Could the authors clarify why ContextKeeper’s identification approach is preferable to DuoAttention’s training-based method? In my view, offline training does not impact inference efficiency. If dataset choice is the critical factor, could the authors train DuoAttention using the profiling dataset constructed for ContextKeeper and provide a direct comparison?\n2. In identifying CA heads, you select the last $m$ tokens from the prefill stage and the first $k$ tokens from the decoding stage as the observation window. Could you elaborate on the rationale for this choice? Is there a risk that some tokens in this window attend heavily to context tokens while others do not, leading to skewed C-preference scores and potentially omitting relevant heads?\n3. Have you evaluated ContextKeeper across different model architectures and varying retention fractions to provide a more comprehensive picture of its performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uW8a2PILQa", "forum": "7zQy4iHoZ8", "replyto": "7zQy4iHoZ8", "signatures": ["ICLR.cc/2026/Conference/Submission18544/Reviewer_mVMG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18544/Reviewer_mVMG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970586247, "cdate": 1761970586247, "tmdate": 1762928246069, "mdate": 1762928246069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ContextKeeper, a training-free KV cache compression method. ContextKeeper identifies a subset of attention heads—Context-Anchored (CA) heads—that consistently attend to middle-context tokens. By retaining full KV cache for CA heads while dropping middle tokens for others,  ContextKeeper can reduce memory and latency while preserving performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Minimal Accuracy Drop: Experiments on LongBench show that ContextKeeper reduces KV cache size by up to 3.86× with negligible average accuracy loss, showing effective compression without sacrificing general capability.\n\n- ContextKeeper is training free with minimal cost. The CA head identification requires only ~10 minutes of inference, making it highly practical."}, "weaknesses": {"value": "- The experiments are unconvincing: Experiments are conducted only on Llama-2-7B-32K and Llama-3.1-8B-Instruct. Llama-2-7B-32K is outdated with a small context window and poor long-context ability compared to SOTA models. Testing on more architectures and newer models would strengthen generalizability. (e.g. Llama-3.1-8B-1024K, Phi-3-Mini-128K, Qwen2.5/3, etc).\n- No experiments on extremely long context settings: Currently, ContextKeeper lacks scalability to extremely long context settings. The RULER benchmark should be tested, since it is a more accepted dataset for long-context scenarios with context lengths up to 256k.\n- Keeping the KV cache for certain heads is not a novel idea. Similar head classification methods have been proposed in recent works like  DuoAttention. The author should discuss the novelty of ContextKeeper compared with these papers."}, "questions": {"value": "The appendix is empty. Why not include more experimental details in the appendix?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XpWvmpMLIv", "forum": "7zQy4iHoZ8", "replyto": "7zQy4iHoZ8", "signatures": ["ICLR.cc/2026/Conference/Submission18544/Reviewer_2Y2t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18544/Reviewer_2Y2t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977888252, "cdate": 1761977888252, "tmdate": 1762928243369, "mdate": 1762928243369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}