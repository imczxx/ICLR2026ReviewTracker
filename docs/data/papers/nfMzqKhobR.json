{"id": "nfMzqKhobR", "number": 17228, "cdate": 1758273682879, "mdate": 1763730707751, "content": {"title": "Fake Player: Imitating Real Player to Distill Data for LLM-based NPC Training", "abstract": "In the era of large language models (LLMs), games increasingly deploy LLM-based role-playing NPCs to replace traditional scripted NPCs, enabling more intelligent and dynamic interactions. \nTo ensure persona consistency and output stability, these NPCs require fine-tuning for alignment, utilizing training data with dual dimensions: persona-aligned assistant responses and diverse, authentic user inputs reflecting real player behaviors. \nHowever, existing research prioritizes persona consistency in NPC responses while neglecting the diversity and authenticity of user-side inputs. This critical gap leads to NPC responses that are misaligned with genuine player interactions, significantly impairing player immersion and experience. \nHuman annotation struggles to address this gap due to its inability to comprehensively cover the vast spectrum of player behaviors. Moreover, practical deployment constraints strongly favor small-parameter LLMs for NPCs, making data quality paramount.\nTo bridge this gap, we propose: 1) $\\textbf{Fake Player}$: A multi-agent LLM distillation framework where collaborative agents simulate expression-constrained human players to distill diverse, human-aligned dialogue data from large LLMs; 2) $\\textbf{Distill Bench}$: A standardized benchmark for quantitatively assessing distilled data quality, bypassing costly NPC retraining.\nExtensive experiments validate our method’s effectiveness in generating diverse player interactions and the benchmark’s reliability for data evaluation.", "tldr": "FakePlayer​​: Multi-Agent Simulation of Expression-Constrained Human Players for Automated High-Quality Data Distillation in LLM-based NPC Training.", "keywords": ["LLM", "NPC", "Data Distillation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2e26df077644bbbe3df13f485ea6433e09764b7b.pdf", "supplementary_material": "/attachment/cfb392c52cc1545616019140a4c67026a23c2169.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses a critical gap in the development of LLM-based NPCs: the lack of diverse and authentic player-side data for fine-tuning. The authors argue that existing methods focus too heavily on NPC persona-alignment while neglecting the quality of the player inputs, leading to models that fail in real-world interactions.\n\nTo solve this, they propose \"Fake Player,\" a novel multi-agent distillation framework. This framework simulates a human player by decomposing the interaction process into three stages: an Inner Monologue Agent (generating internal thoughts), an Intent Analysis Agent (extracting and stacking fine-grained intents), and a Typing Agent (converting the top intent into a concise and human-like utterance). This approach aims to generate data that is both diverse and behaviorally realistic.\n\nAdditionally, the paper introduces \"Distill Bench\", a benchmark for quantitatively assessing the quality of synthetic dialogue data before its use in training. Experiments show that Fake Player generates superior data compared to baseline methods (ICL and Role-Playing Agents) and that NPCs fine-tuned on this data significantly outperform others, even allowing smaller 7B models to compete with larger prompt-based ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies and tackles a practical, high-impact problem. The insight that the authenticity of player-side data is a bottleneck for fine-tuning robust NPCs is both non-obvious and significant for the field.\n\n2. The \"Fake Player\" architecture is a good solution. The conceptual model of a player having complex \"inner monologues\" but being limited by a \"typing constraint\" to express one intent at a time is an effective way to model human-NPC interaction in games. This principled approach is a clear improvement over more naive agent-based or few-shot generation methods.\n\n3. The authors prove the effectiveness of the synthetic data by fine-tuning models and measuring their performance. The proposal of a standardized benchmark to evaluate synthetic data before training is also a valuable secondary contribution. This provides a necessary tool for researchers and developers, saving significant time and computational resources that would otherwise be spent on iterative fine-tuning and evaluation cycles."}, "weaknesses": {"value": "1. The three-agent pipeline, while effective, is inherently more complex than the baselines. This paper would benefit from a brief discussion of the computational cost and latency of generating data using Fake Player versus the simpler ICL and RPA methods. It's important to understand the trade-offs between data quality and data generation efficiency.\n\n2. It is unclear how this \"typing agent\" work. Moreover, it is recommended to supplement the case study of \"typing agent\" (e.g., the prompting template and the synthetic human output) for a more comprehensive understanding. Moreover, The \"typing agent\" is noted to introduce \"stochastic imperfections\" like typos and grammatical errors. While this intuitively adds to \"human-likeness,\" the paper does not present an ablation study on how much this specific feature actually contributes to the downstream model's robustness or performance. It would be valuable to know if this is a critical component or a minor addition.\n\n3. For evaluation, some other NPC creation/style adaptation methods could also be considered besides fine-tuning, such as representation editing (e.g., Ma et al. 2025, DRESSing up LLMs, etc.). It could be more persuasive if the dataset is useful for broader types of methods.\n\n4. Regarding the Distill Bench evaluation (Table 1), the \"Human-Likeness\" score for Fake Player (e.g., 69.80) is a significant improvement over baselines but still notably lower than the score for real players (80.20, from Table 3). This gap is more significant compared to that between different methods. What do you believe accounts for this remaining gap, and what future work could help close it?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lNdCoja68o", "forum": "nfMzqKhobR", "replyto": "nfMzqKhobR", "signatures": ["ICLR.cc/2026/Conference/Submission17228/Reviewer_NmYs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17228/Reviewer_NmYs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728899257, "cdate": 1761728899257, "tmdate": 1762927187777, "mdate": 1762927187777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the critical gap in training data for LLM-based game NPCs: the lack of diverse, high-fidelity player-side inputs. The authors argue that existing data generation methods like In-Context Learning (ICL) or Role-Playing Agents (RPA) produce data that is either low-diversity or not human-like.\n\nTo solve this, the paper introduces two key contributions:\n1.  **`Fake Player`**: A novel multi-agent distillation framework designed to simulate a human player's cognitive and physical interaction constraints. It consists of an `Inner Monologue Agent` (to generate thoughts), an `Intent Analysis Agent` (to manage a stack of conversational intents), and a `Typing Agent` (to convert the top intent into a concise, device-constrained, human-like utterance).\n2.  **`Distill Bench`**: A new benchmark for quantitatively assessing the quality of the *distilled data itself* on dimensions of Topic Relevance, Human-Likeness, and Data Diversity, bypassing the need for costly downstream NPC retraining.\n\nExperiments demonstrate that `Fake Player` generates higher-quality data than baselines and that small 7B-parameter models fine-tuned on this data can achieve performance approaching that of large, prompt-based models like GPT-5."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strength lies in its novel and well-motivated `Fake Player` framework. Instead of just generating \"player\" text, it models the *underlying process* of human interaction, separating internal monologue from a device-constrained typed utterance via an intent stack. This is a significant and original idea. The second major strength is the `Distill Bench`, which provides a much-needed, low-cost method for evaluating the quality of synthetic training data. The experimental results are strong, particularly the downstream task analysis showing a fine-tuned 7B model approaching GPT-5's performance, and the industrial deployment confirms real-world significance."}, "weaknesses": {"value": "1.  **Missing Ablation Study:** The `Fake Player` framework is presented as a three-component system: `Inner Monologue Agent` (IMA), `Intent Analysis Agent` (IAA), and `Typing Agent` (TA). While the full system shows strong results, the paper lacks an ablation study to justify this specific architecture. It is unclear what the relative contribution of each agent is. For instance, how much of the \"Human-Likeness\" gain comes from the `Typing Agent`'s conciseness constraints versus the `Intent Analysis Agent`'s sophisticated intent stack? Would a simpler model (e.g., IMA -> TA) perform similarly? This is a key omission for understanding *why* the method works so well.\n\n2.  **Over-reliance on LLM-as-Judge:** The evaluation, both for `Distill Bench` and the downstream NPC task, heavily relies on \"LLM as a Judge\" (specifically, DeepSeek-V3). While this is a common practice, it introduces potential biases. The judge's high scores for `Fake Player`'s output might stem from a shared stylistic bias (as `Fake Player` also uses LLMs) rather than true human-likeness. The human evaluation in Table 3 is a good inclusion to mitigate this, but it is very small (only 35 dialogues) and only evaluates the *data*, not the final *NPC interaction*.\n\n3.  **Lack of Cost/Efficiency Analysis:** The `Fake Player` framework appears significantly more computationally expensive for data generation than the baselines. Algorithm 1 implies that each turn of generated dialogue requires multiple sequential LLM calls (IMA, IAA, TA). This is a 3x or more increase in generation cost compared to ICL (one call) or RPA (one call per agent, often parallelizable). The paper does not discuss this trade-off. For a method focused on distillation, the *cost of distillation* is a critical, practical factor that has been overlooked.\n\n4.  **Limited Scope of Evaluation:** The downstream evaluation is based on a single NPC persona (an \"estate manager\") in what appears to be a \"companionship\" context. It is not clear how well the `Fake Player`'s assumptions (e.g., concise, typo-prone inputs) would generalize to other game genres or interaction types, such as an \"antagonist\" NPC, a \"quest-giver\" NPC requiring complex informational queries, or a fast-paced action-game helper."}, "questions": {"value": "1.  **On Validating the Judge:** The reliance on \"LLM as a Judge\" is a potential weakness. While Table 3 provides a small human comparison for \"Human-Likeness\", could the authors provide more detail on the validation of the DeepSeek-V3 judge? For example, was any inter-rater reliability (e.g., Cohen's Kappa) measured between the LLM-judge and human evaluators on a larger subset of `Distill Bench` to ensure the judge's ratings for all dimensions (especially \"Topic Relevance\" and \"Improvisation\") are truly aligned with human preferences?\n\n2.  **On the Necessity of the Intent Stack:** Given the lack of an ablation study, could the authors provide insight into the specific contribution of the `Intent Analysis Agent`? What measurable drop in data quality (e.g., in \"Human-Likeness\" or \"Topic Relevance\" scores) would be expected if this agent were removed, and the `Typing Agent` was prompted to simply \"convert the inner monologue into a concise, human-like utterance\"?\n\n3.  **On Data Generation Costs:** Could the authors elaborate on the computational cost (e.g., API calls, tokens processed, or wall-clock time) of generating the training dataset using `Fake Player` versus the ICL and RPA baselines? How does this cost factor into the overall practical utility of the method, especially for smaller developers?\n\n4.  **On Generalizing Interaction Styles:** The `Typing Agent` simulates conciseness and \"stochastic imperfections\" typical of casual, \"expression-constrained\" interactions. How does this framework adapt to the \"knowledge-seeking\" domain, where a real player might realistically input a much longer, more complex, and grammatically correct query to get a specific answer? Does the `Typing Agent`'s behavior change based on the `Domain $D$` parameter?\n\n5.  **On Controlling Imperfections:** How were the \"stochastic imperfections\" (typos, grammatical errors) of the `Typing Agent` implemented? Was this achieved via prompting, or through a separate simulation/post-processing step? Furthermore, how sensitive was the final NPC performance to the *rate* of these imperfections? Is there a \"sweet spot,\" and does too much \"realism\" (i.e., too many typos) start to degrade the NPC's ability to understand the player?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dqTqkNJh1n", "forum": "nfMzqKhobR", "replyto": "nfMzqKhobR", "signatures": ["ICLR.cc/2026/Conference/Submission17228/Reviewer_PUtu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17228/Reviewer_PUtu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875068757, "cdate": 1761875068757, "tmdate": 1762927187420, "mdate": 1762927187420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical and often-overlooked problem in the training of LLM-driven game NPCs: the generation of high-quality, realistic \"player-side\" dialogue data for fine-tuning.\nThe authors argue that existing research focuses heavily on ensuring the NPC's \"persona consistency,\" while neglecting the diversity and authenticity of the user inputs in the training data. This leads to NPCs that perform poorly or break character (Out of Character, OOC) when faced with real-world player inputs, which are often concise and intent-driven.\nTo solve this, the paper presents two main contributions:\n1. Fake Player: A multi-agent LLM distillation framework. This framework uses three collaborating agents (Inner Monologue Agent to generate thoughts, Intent Analysis Agent to extract and maintain an intention stack, and Typing Agent to simulate physical constraints by converting intent to concise text) to mimic constrained, human-like players. This allows for the distillation of diverse and behaviorally-aligned dialogue data from large LLMs.\n2. Distill Bench: A standardized benchmark for quantitatively assessing the quality of distilled data. It aims to evaluate data across three dimensions—Topic Relevance, Human-Likeness, and Data Diversity—without requiring costly NPC retraining, thus reducing iterative development costs.\nExperimental results show that data generated by \"Fake Player\" scores higher on \"Distill Bench\" compared to two baselines (ICL and Role-Playing Agent, RPA). More importantly, in downstream tasks, small models (e.g., Qwen-7B) fine-tuned on \"Fake Player\" data significantly outperform those trained on baseline data and even approach or exceed the performance of large, prompt-based NPC models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear and Important Problem Definition: The paper accurately identifies a key bottleneck in the LLM-NPC domain—the quality of player-side training data. As shown in Figure 1, ensuring NPCs can handle realistic, varied player inputs is crucial for player immersion and the practical deployment of small-parameter LLMs. This is a practical and valuable research direction.\n2. Innovative Data Generation Framework: The \"Fake Player\" three-agent architecture is a novel design. It goes beyond simple \"role-playing\" and attempts to model the human \"cognition-to-expression\" pipeline. The use of an \"intention stack\" in the \"Intent Analysis Agent\" and the simulation of physical interface constraints (leading to concise input) by the \"Typing Agent\" are insightful and reasonable models for why LLM agent outputs often don't feel human.\n3. Practical Evaluation Benchmark (Distill Bench): Given the high cost of NPC training (especially SFT), the proposal of \"Distill Bench\" has significant engineering value. It provides an \"offline\" method for evaluating data quality, allowing researchers to rapidly iterate on and validate data generation strategies before committing to expensive model training. The three evaluation dimensions (relevance, human-likeness, diversity) are comprehensive and well-reasoned.\n4. Solid Downstream Task Validation: A key strength is that the paper does not stop at data-level evaluation (Table 1) but proceeds to validate the data's ultimate effectiveness through downstream NPC fine-tuning experiments (Table 2). The results strongly demonstrate that small models with high-quality data can approach the performance of large prompt-based NPCs, providing new evidence for the \"data quality over model scale\" argument, which is highly relevant to the industry."}, "weaknesses": {"value": "1. Questionable Strength and Fairness of Baselines: The main baselines, \"ICL\" and \"RPA,\" are \"adapted\" by the authors from SOTA methods. This raises a core question: are these baselines sufficiently strong?\n  - For the RPA (Role-Playing Agent) baseline, the paper claims it \"lacks behavioral constraints,\" leading to poor output. However, RPA methods themselves often rely on strong prompting. Did the authors attempt to strengthen the RPA baseline with more sophisticated prompt engineering (e.g., explicitly instructing the RPA's player agent to \"be concise,\" \"think and type like a real player\")?\n  - The current comparison gives the impression that the \"Fake Player\" architecture's advantage is overwhelming, but this might be contingent on weakly-implemented baselines.\n2. Lack of Reliability Validation for \"Distill Bench\": \"Distill Bench\" is a core contribution, but it relies heavily on \"LLM as a Judge\" (using DeepSeek-V3) for automated evaluation. It is well-known that LLM-as-a-Judge methods suffer from biases (e.g., position, verbosity) and consistency issues.\n  - The paper lacks a crucial validation experiment: What is the correlation between the \"Distill Bench\" automated scores and \"human evaluation\" scores?\n  - If the Judge LLM were replaced (e.g., with GPT-4, Llama 3, or Claude 3), would the ranking of methods in Table 1 (Fake Player > RPA > ICL) remain consistent? Without this validation, the reliability of \"Distill Bench\" is questionable.\n3. Missing Ablation Study for the \"Typing Agent\": The \"Typing Agent\" aims to simulate human typing behavior by introducing typos, grammatical errors, etc. This is an interesting detail, but what is its actual contribution to the downstream SFT task?\n  - Intuitively, injecting \"noise\" (like typos) into SFT data might harm model learning rather than enhance it.\n  - The paper lacks an ablation study for the \"Typing Agent.\" If this module were removed and the \"Intent Agent's\" top intent $C_i$ were directly converted into concise, natural language (without errors), how much would the downstream NPC performance (Table 2) drop? Is this module \"essential\" or just \"nice to have\"?\n4. Framework Complexity and Robustness: \"Fake Player\" uses a three-agent serial pipeline. This introduces complexity and a potential for \"compounding errors\"—a small deviation in the \"Inner Monologue\" could be amplified by the \"Intent Analysis\" and \"Typing Agent.\" The paper does not discuss the framework's robustness, e.g., its sensitivity to the initial (P, D, T) seed settings."}, "questions": {"value": "1. Can you elaborate on the implementation details of the RPA baseline? Specifically, did you attempt to strengthen the RPA's \"player agent\" prompt to simulate \"conciseness\" and \"human constraints\" to create a more competitive baseline?\n2. Regarding the reliability of \"Distill Bench.\" Have you conducted a correlation analysis between \"Distill Bench's\" automated scores and human expert ratings? Do the conclusions in Table 1 hold if you use a different LLM (e.g., GPT-4 or Claude 3) as the judge?\n3. What is the specific contribution of the \"Typing Agent\" module? Have you run an ablation study where the \"Typing Agent\" is replaced with a simple \"intent-to-text\" converter (i.e., preserving conciseness but removing noise like typos)? How does this affect the downstream NPC performance in Table 2? Is intentionally introducing errors into SFT data truly beneficial?\n4. How robust is the three-agent serial framework? For example, how sensitive is the iterative update in Equation (2) to the quality of the \"Inner Monologue\"? Is there a problem with error accumulation and amplification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "moMwkqvSbH", "forum": "nfMzqKhobR", "replyto": "nfMzqKhobR", "signatures": ["ICLR.cc/2026/Conference/Submission17228/Reviewer_CQsN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17228/Reviewer_CQsN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875492985, "cdate": 1761875492985, "tmdate": 1762927186919, "mdate": 1762927186919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new approach to modeling players in gaming settings that have them interact with NPCs. Players are modeled using a multistep process where players have their own longer inner monologue, which is turned into intents, and then these intents are turned into an utterance for an NPC. To test the setup, the NPCs are simulated with LLMs of various sizes, as well as smaller LMs that are trained on distilled interactions. Interactions are scored using LLM-as-judge with two criteria for human-likeness, conciseness and improvisation, and two for topical relevance, depth and breadth, as well as a third custom, data diversity score."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Interesting and novel design for how to model player mental states while effectively producing realistic interactions with NPCs.\n\n- Multiple evaluate metrics are considered to provide a more holistic assessment\n\n- I was very excited to see the details in Appendix F that this system has been deployed. However, details on this deployment are very minimal---likely due to the non-disclosure. I wish the authors could have highlighted this more though, as it would immensely strengthen the paper."}, "weaknesses": {"value": "- I found this paper surprisingly hard to follow. The paper does a good job of presenting high level information and narrative, but many specifics and details are hard to track down. One key missing piece is how these LLM based agents are actually designed and used. The prompts for the agents are not presented (only the evaluation prompts, as far as I can tell), so I am not sure how to evaluate the generality of the approach. Other examples include the presentation such as in the tables, the metrics are reported with two-letter abbreviations which, to me, were not easily identifiable without more searching. \n\n- Other experimental details are hard to assess. I appreciate the authors comparing with other baselines like ICL and RPA. However the paper says that these are adapted from their original approach but little details are provided. For example, it's not clear how many shorts are used in ICL. It's also not clear how much of the core player guidelines are shared between all three of these approaches (which would be necessary for modeling players), which prevents assessing whether the improvement is due to better prompts or the agentic system.\n\n- The paper makes extensive use of LLM as judge. I appreciate the authors using a different base model to evaluate to mitigate a model preferring its own outputs. However, these scores are never validated or compared against any kind of human output. The judge models are good, but some small part of the data needs to validated to calibrate how good they are at evaluating the qualities.\n\n- The language in Lines 418-422 is talking about the relative distances of clusters in the t-SNE embedding, but t-SNE doesn't preserve global distances so I don't think these kinds of comparisons are meaningful. I think you'd want to use umap or PCA to substantiate these claims. I do appreciate the figure though."}, "questions": {"value": "- The discussion of Bratman (1987) and player intent in lines 191-195 is very interesting for thinking about how to model players. Are there any citations to back up some of these claims?\n\n- I am confused by the use of a stack in the intent agent. As far as I can tell, the stack operations in (3) always push one or more item based on $S_{i-1}$, and then pop the top intent. However, I don't see any way to actually move deeper into the stack (e.g., by popping more items off or viewing more). There's a vague statement about operations in 209 but I'm not sure if these operations would do so. If not, why not just keep the most recent state around and discard all earlier states?\n\n- For topical relevance, how are topics defined? \n\n- The data diversity scoring why are the operations in the inner for loop needed? For example, why couldn't you measure diversity by the average pair-wise embedding similarity for utterances---i.e., why must a probability distribution be made and entropy be calculated? I realize there are multiple ways to measure what you're doing and I'm not saying the approach is wrong, but I don't yet understand the intuition for why this complexity is needed.\n\n- In line 319, the data is described as a \"real-sampled resources\". Could you say more about what is the \"real\" part?\n\n- How were the win-rates produced in Table 4? Are these from people or LLMs?\n\n- For the human likeness experiments in Appendix A, what are the settings for this human-NPC experiment? How many humans and how many unique NPCs? Are these players (real or simulated) all interacting with the same intents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0D73GA41Uz", "forum": "nfMzqKhobR", "replyto": "nfMzqKhobR", "signatures": ["ICLR.cc/2026/Conference/Submission17228/Reviewer_mgWL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17228/Reviewer_mgWL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970423406, "cdate": 1761970423406, "tmdate": 1762927186533, "mdate": 1762927186533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}