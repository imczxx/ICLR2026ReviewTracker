{"id": "8YQbVMWhRJ", "number": 19527, "cdate": 1758296972901, "mdate": 1763018756064, "content": {"title": "HUG3D: Human Group-Aware 3D Reconstruction from a Single Image with Physical Interaction", "abstract": "Reconstructing textured 3D human models from a single image is fundamental for AR/VR and digital human applications. However, existing methods predominantly focus on single individuals and thus fail in multi-human scenes, where naive composition of individual reconstructions often leads to artifacts such as unrealistic overlaps, missing geometry in occluded regions, and distorted interactions. These limitations highlight the need for approaches that incorporates group-level context and interaction priors. We introduce HUG3D, a holistic method that explicitly models both group- and instance-level information. To mitigate perspective-induced geometric distortions, we first transform the input into a canonical orthographic space. Our primary component, Human Group-aware Multi-View Diffusion (HUG-MVD), then generates complete multi-view normals and images by jointly modeling individuals and their group context to resolve occlusions and proximity. Subsequently, the Human Group-Aware Geometric Reconstruction (HUG-GR) module optimizes the geometry by leveraging explicit, physics-based interaction priors to enforce physical plausibility and accurately model inter-human contact. Finally, the multi-view images are fused into a high-fidelity texture. \nExtensive experiments show that HUG3D significantly outperforms both single-human and existing multi-human methods, producing physically plausible, high-fidelity 3D reconstructions of interacting groups from a single image.", "tldr": "", "keywords": ["3D reconstruction", "multi-view diffusion model", "physical interaction", "digital human"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6284802a33ba3ea123ca40e7f02e81eec2b143d2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a multi-human reconstruction framework that first performs instant segmentation and SMPL-X fitting to obtain initial human conditions, followed by a multi-instance diffusion model to generate multi-view images for each human instance. Finally, the geometry of each person is reconstructed from these generated multi-view images."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposed pipeline is technically plausible, and the qualitative results demonstrate that the method can produce reasonable multi-human reconstructions. The idea of combining multi-instance diffusion with multi-view reconstruction is conceptually sound and has potential for further development."}, "weaknesses": {"value": "The paper claims to address the reconstruction of groups of people, but all presented results involve only two individuals. If the method is limited to reconstructing pairs of people, the authors should moderate their claims accordingly and revise the title and presentation to reflect the true scope.\n\nThe proposed framework is complex and involves several sequential stages, including segmentation, SMPL-X fitting, and multi-view diffusion. Errors in intermediate stages may propagate and significantly affect the final reconstruction. The paper does not discuss how robustness is maintained against variations or inaccuracies in intermediate results.\n\nMulti-view diffusion models are known to struggle with maintaining cross-view consistency. Since the proposed approach relies on partial observations as input, this challenge may be exacerbated. The paper should provide analysis or evidence demonstrating how view consistency is ensured.\n\nThe qualitative results in the paper are presented at very small image sizes, making it difficult to visually assess the reconstruction quality and consistency. Higher-resolution visualizations or quantitative evaluations would strengthen the paper."}, "questions": {"value": "Please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6ni5LrczJv", "forum": "8YQbVMWhRJ", "replyto": "8YQbVMWhRJ", "signatures": ["ICLR.cc/2026/Conference/Submission19527/Reviewer_MyME"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19527/Reviewer_MyME"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761671286058, "cdate": 1761671286058, "tmdate": 1762931416463, "mdate": 1762931416463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "XL3NHxBq4t", "forum": "8YQbVMWhRJ", "replyto": "8YQbVMWhRJ", "signatures": ["ICLR.cc/2026/Conference/Submission19527/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19527/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763018754226, "cdate": 1763018754226, "tmdate": 1763018754226, "mdate": 1763018754226, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors tackled a challenging problem of 3D reconstructing multi-persons from single images. The proposed pipeline has three main stages: (1) Pers2Ortho that converts the single perspective RGBs into a canonical multi-view orthographic scene, (2) HUG-MVD that predicts missing geometry and texture for multi-persons, (3) HUG-GR that optimizes meshes using physical priors. Experiments are conducted on MultiHuman dataset and the method is compared to competitive single human reconstruction baselines such as ECON, SIFU, PSHuman. HUG3D outperforms such baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel idea: Involving Pers2Ortho and making partial 3D information as the condition for multiview diffusion seems like an interesting and sound idea which can help multi-view reasoning from a single RGB image.\n\nGood performance: Even under a single-image setting, the method outperforms multi-viewed or video-based human reconstruction baselines (ie. DeepMultiCap, Multiply). This seems impressive."}, "weaknesses": {"value": "The method is composed of multiple stages and it is hard to analyze what happens when some modules fail.  The paper should quantify how sensitive the final result varied. Especially, Pers2Ortho still depends on the initial SMPL-X mesh fitting. If the initial fit is bad, how does the overall performance change? There is no such analysis.\n\nThe overall pipeline looks too heavy. In appendix E.5, it is reported that the overall components take more than a few minutes. \n\nThe term “Physics-aware” looks fancy, while it only considers the interpenetration loss, which is similar to penetration or contact losses frequently observed in the related literature. This is bit disappointing."}, "questions": {"value": "Mostly, the results are shown for two-person interaction cases. How does the method work for more than 3 persons interactions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Al4oRPWODn", "forum": "8YQbVMWhRJ", "replyto": "8YQbVMWhRJ", "signatures": ["ICLR.cc/2026/Conference/Submission19527/Reviewer_ExtP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19527/Reviewer_ExtP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933748893, "cdate": 1761933748893, "tmdate": 1762931416095, "mdate": 1762931416095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HUG3D to reconstruct textured 3d human groups  from a single image. Actually, a few works have existed, but the task has not been well explored until now. This work is technically well-structured, including a per2ortho module, a group-aware MVD, and a mesh refinement stage. The experiments demonstrate consistent improvement across geometry, texture and realism metric."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper includes comprehensive experiments to evaluate the effectiveness of each design, showing significant improvement against baselines.\n2. This work introduces a principal way  to handle group-level context through both diffusion and optimization stages, rather than simply extending single-person pipelines."}, "weaknesses": {"value": "1. How to optimize grouped SMPLX with only the input view, normal and depth. I think the optimization works for single human, but not trivial for multiple individuals due to severe occlusion. The visualization looks good, but is there any insight and discussion?\n2. At line 174, how to initialize multiple SMPLX to prevent overlap? Although with the following refinement, but i think the initialization is also critical.   There should be more details and include some visualizations.\n3. Equation 7 is confusing.  From my understanding, this loss is underspecified. It measures only the absolute distance between point pairs without considering the inside-outside relationship. Its gradient provides no corrective directions even when two meshes overlap. Moreover, when |d| < tol, loss is tol; when |d| > tol, loss is |d|. Which will encourage the distance to tol.  I do not think it makes sense.\n4. The presentation could be improved, like including a demo video for 3d visualization. And it seems like the submission only shows two-person cases, without general \"multiple\" human generalization performance. \n5. The caption in Fig. 3 should be fixed. It's very blurry."}, "questions": {"value": "as listed in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7L65k4u0tO", "forum": "8YQbVMWhRJ", "replyto": "8YQbVMWhRJ", "signatures": ["ICLR.cc/2026/Conference/Submission19527/Reviewer_RAzp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19527/Reviewer_RAzp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949110714, "cdate": 1761949110714, "tmdate": 1762931415702, "mdate": 1762931415702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HUG3D, a method to generate 3D humans from single images. At the core of the method is a Human Group-aware Multi-View Diffusion (HUGMVD) net, which generates complete multi-view normals and images from single input images. An Human Group-Aware Geometric Reconstruction (HUG-GR) module is proposed to model inter-human contact, and generate 3D meshes. Finally, a 3D reconstruction module is proposed to generate 3D textured human. The authors conducted experiments on DeepMulticap dataset to verify the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is easy to follow.\n\nSynthesizing 3D humans from single images is an interesting task with practical applications.\n\nThe method is technically sound by leveraging a multiview diffusion network and a group-instance reconstruction module to model both instance level and group level geometry for multihuman reconstruction."}, "weaknesses": {"value": "The goal is to address multi-human reconstruction, but the experiments only involve two people, with relatively simple occlusions. It remains to be demonstrated through experiments whether HUG-MVD and HUG-GR can handle group generation for three or more humans.\n\nMultiHuman/DeepMultiCap dataset only captured three human interactions. How about the performance on the three human data?\n\nFor evaluations, how does the SMPL/SMPLX estimation accuracy affect the performance? For multihuman scene with occlusions, the SMPLX estimation may not be accurate. This issue should be discussed, especially for multihuman reconstruction.\n\nDeepMultiCap is the first method to reconstruct multiple humans, and qualitative comparisons against DeepMultiCap are required\n\nDemo video is required to visualize more views of the generated geometry."}, "questions": {"value": "The experiments are conducted on human subjects with relatively tight clothing. How to handle loose clothing since the method utilizes SMPLX, while SMPL cannot model loose clothing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HArPMipGG4", "forum": "8YQbVMWhRJ", "replyto": "8YQbVMWhRJ", "signatures": ["ICLR.cc/2026/Conference/Submission19527/Reviewer_pxJK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19527/Reviewer_pxJK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995981411, "cdate": 1761995981411, "tmdate": 1762931415249, "mdate": 1762931415249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}