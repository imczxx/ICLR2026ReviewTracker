{"id": "03u504EDJp", "number": 525, "cdate": 1756744193214, "mdate": 1759898255701, "content": {"title": "Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs", "abstract": "This paper identifies a critical yet underexplored challenge in distilling from multi-modal large language models (MLLMs): the reasoning trajectories generated by multiple drifting teachers exhibit concept drift, whereby their reasoning distributions evolve unpredictably and transmit biases to the student model, ultimately compromising its performance. To tackle this issue, we pioneer a theoretical connection between concept drift and knowledge distillation, casting the non-stationary reasoning dynamics from multiple MLLM teachers as next-token prediction of multi-stream reasoning trajectories. Guided by concept drift, we introduce the “learn–compare–critique” paradigm, culminating in autonomous preference optimization (APO). Under the active guidance of the teachers, the student model first learns and self-distils preferred thinking by comparing multiple teachers. It then engages in critical reflection over the drifting inference from teachers, performing concept alignment through APO, ultimately yielding a robust, consistent, and generalizable model. Extensive experiments demonstrate our superior performance of consistency, robustness and generalization within knowledge distillation. Besides, we also contributed a large-scale dataset CXR-MAX (Multi-teachers Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public at: https://anonymous.4open.science/r/Autonomous-Distillation/.", "tldr": "", "keywords": ["concept drift", "transfer learning", "multi view", "knowledge distillation", "multi modal large language model"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe4866ea94ed809fb98d3d8b49b15b242306766f.pdf", "supplementary_material": "/attachment/d3f2bf191b959b040fec6edae75de60b04403059.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces APO, a new framework for distilling reasoning capabilities from multiple MLLMs that exhibit conceptual drift, defined as variability in their reasoning behaviors or conclusions. The core idea is that APO aggregates all available reasoning trajectories and learns to prefer the self-distillation as positive signals against all negative trajectories. This approach treats distillation as a preference optimization problem, aligning the student model’s reasoning trajectory with the highest-quality outputs among drifting teachers, in a “learn-compare-critique” paradigm. The method is tested on a newly constructed dataset, CXR-MAX, based on chest X-ray interpretation, and shows improvements in accuracy."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Conceptually interesting framework that reframes multiple teacher KD as a PO problem over reasoning trajectories and models a *drifting* behavior, although this drifting is not well elaborated.\n- The paper provides a clear experimental setup, with the introduction of CSR-MAX dataset to evaluate alignment in medical reasoning."}, "weaknesses": {"value": "There are several weaknesses regarding the lack of clarity in methods, the justification on the problem setting, and the rationale about the limited application of the proposed method.\n\n- The paper is using these kind of terms: “drifting teachers”, “drifting MLLMs”, “inter-model drift”, “concept drift”, but the central concept of “drift” is not well-defined or formalized. A formal mathematical definition of drifting or empirical characterization of what and how constitutes “drift” would help.\n- Example in Figure 1 (b) just looks like a standard error or hallucination commonly observed in LLMs. It fails to provide evidence of concept drift, or that this error came from the drifting teachers.\n- The paper assumes a multi-teacher distillation setup which is a very specific framework. It is unclear why this setup is necessary when multiple MLLM teachers exhibit inter-model drift. In practice, single-teacher KD is used in general, and the proposed scenario of drifting across many MLLMs is not well justified.\n- The authors claim that distilled students exhibit concept drift after learning from misaligned teachers, yet APO selects the student’s own trajectory as the preferred one during the optimization. If the student is already drifting, why is its reasoning preferred?\n\nExperimental demonstration is only limited to chest X-ray image domain, which is very specific and narrow. \n\n- Since APO is presented as a general framework for MLLM distillation, experiments on broader tasks (e.g., general image classification, natural language reasoning, or multi-step math and coding problems) would provide more compelling evidence of its utility. I cannot find the reason why the authors have applied it exclusively on the medical domain.\n- No quantitative evaluation is provided on the degree of concept drift or how much APO reduces this drift. Some metric to evaluate teacher-teacher drifting and teacher-student alignment over time or across tasks would strengthen the claims.\n\n\n### Writing and Attribution Concerns\n\nThere are notable similarities in structure between this paper and prior work, particularly Yang et al. (2025b), which has been cited in this paper. [Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning, Yang et al., 2025]\n\nFor example, the first contribution point in Introduction says:\n\n> We establish a novel theoretical framework that casts the autoregressive generation of reasoning trajectories in MLLMs within the perspective of concept drift theory, providing a principled foundation for understanding and analyzing knowledge distillation from multiple drifting teachers.\n\n**Yang et al. (2025b)**:\n\n> we establish a novel theoretical framework that formalizes autoregressive token generation in MLLMs through the lens of concept drift theory, enabling systematic identification and causal analysis of detrimental reasoning divergence during non-stationary reinforced custom-tuning.\n\nThe last contribution point in Introduction says:\n\n> As a pioneer contribution to the community, we introduce CXR-MAX (Multi-teahers Alignment X-rays), a large-scale dataset comprising 170,982 distillation results of reasoning trajectories derived from publicly accessible MLLMs based on MIMIC-CXR.\n\n**Yang et al. (2025b)**:\n\n> As a pioneer contribution to the community, we introduce CXR-CounterFact (CCF), a large-scale dataset comprising 320,416 meticulously curated counterfactual reasoning trajectories derived from MIMIC-CXR.\n\nWhile the details of methodology differ, the overall framing, structure, and phrasing appear heavily relied on the previous work, raising concerns about originality in presentation. Many parts of Abstract, Introduction, and main figures look too similar with a few technical modifications."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ogYTGtXZT6", "forum": "03u504EDJp", "replyto": "03u504EDJp", "signatures": ["ICLR.cc/2026/Conference/Submission525/Reviewer_XwXM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission525/Reviewer_XwXM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761194835318, "cdate": 1761194835318, "tmdate": 1762915539179, "mdate": 1762915539179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses the concept drift problem in knowledge distillation of multimodal large language models (MLLM). Through the analysis of the connection between concept drift and knowledge distillation, the authors introduce the “learn–compare–critique” paradigm to tackle the issue. The resulting method, autonomous preference optimization (APO), trains the student with self relection over the drifting inference for concept alignment. Experiments demonstrate the effectiveness of APO on knowledge distillation tasks. The authors also contribute to a large-scale dataset called CXR-MAX."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well structured\n2. Extensive experiments demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. **Unclear notation**: Definition 2.1 and 2.2 are not clear to me. In particular, the definition of $P_{i, j}$ and $P_i$ are not explained anywhere, do they refer to the joint probability distribution across teachers? More broadly, the jump from Eq. (1) to Definition 2.1 and then to multi-stream drift in Definition 2.2 is not intuitive.\n\n2. **Unclear connection between concept drift and MLLM**: the problem of concept drift might occur naturally in multi-agent distillation. And it seems that there is not special connection to \"multimodality\"\n\n3. The proposed algorithm in Section 2.3 is a natural extension of DPO, which seems not relevant to the definition in section 2.1\n\n4. The figures in the paper contain a lot of annotations, but a more clear pipeline would help readers' understanding.\n\n5. The experiments are entirely focused on chest X-rays."}, "questions": {"value": "1. How does $t^{+}$ generated exactly in Eq (7)? Does it aggregate the results of $\\mathcal{T}$? If it's the response from $\\pi_{st} $, given input image $v$ and prompt $l$, then it should be $t^+ \\sim \\pi_{st} (\\cdot|v,l)$. \n\n2. Experiments are mainly done on the domain of Chest X-ray. What would the performance of APO be on other general domains of visual reasoning?\n\n3. Is APO sensitive to the choice of teacher models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tQNQ14YJUU", "forum": "03u504EDJp", "replyto": "03u504EDJp", "signatures": ["ICLR.cc/2026/Conference/Submission525/Reviewer_GPmW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission525/Reviewer_GPmW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847924982, "cdate": 1761847924982, "tmdate": 1762915538784, "mdate": 1762915538784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of knowledge distillation from multiple multimodal large language models (MLLMs). The authors observe that the reasoning trajectories of different teacher models can change inconsistently across models or over time, and that such concept drift can propagate to student models during distillation. To address this issue, the paper proposes a “learn–compare–critique” pipeline. The student model first learns from multiple MLLM teachers; then it performs self-distillation to align and identify inconsistent teacher outputs. Finally, through a preference optimization step, the student reinforces alignment with stable reasoning outputs while down-weighting drifted or biased outputs.\n\nFor experiments, the authors construct the CXR-MAX dataset, which is an extension of the MIMIC-CXR dataset by adding reasoning trajectories about clinical chest X-ray interpretation from multiple MLLM teachers. Results show that the proposed method outperforms other existing distillation methods, while achieving performance comparable to or exceeding that of the original teacher models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper studies an interesting and practical problem of distilling knowledge from multiple MLLMs, and the identified challenge of concept drift across models or over time is reasonable and well motivated.\n\n2. The proposed “learn–compare–critique” pipeline is conceptually sound and clearly presented.\n\n3. The experimental results are promising, showing that the proposed approach outperforms both previous distillation methods and the original teacher MLLMs. The ablation study provides useful evidence on the contribution of each component and helps analyze the effectiveness of the proposed framework."}, "weaknesses": {"value": "1. All experiments are conducted in the medical imaging domain, specifically chest X-ray reasoning. The paper does not provide evidence that the proposed approach generalizes to other modalities (e.g., vision–language reasoning, VQA) or non-medical tasks, which limits its demonstrated scope.\n\n2. The study uses only one base model (Qwen2.5-VL, 7B) as the student backbone throughout experiments. It remains uncertain whether the proposed framework would behave similarly to other models or model scales.\n\n3. The paper’s citation style does not follow standard formatting, and some theoretical sections (e.g., formal definitions of concept drift) are expressed in dense, overly complex language. Simplifying these formulations and improving readability would make the paper more accessible.\n\n4. The paper could be strengthened by discussing the method's limitations."}, "questions": {"value": "How sensitive is APO to the number or quality of teacher models? Would it still help if one teacher is significantly weaker?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Eu39zKqsIm", "forum": "03u504EDJp", "replyto": "03u504EDJp", "signatures": ["ICLR.cc/2026/Conference/Submission525/Reviewer_L18q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission525/Reviewer_L18q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959085574, "cdate": 1761959085574, "tmdate": 1762915538605, "mdate": 1762915538605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper aims to address the challenge of knowledge distillation from multiple, heterogeneous MLLMs. The main challenge is the concept drift problem, where the teacher models provide conflicting information that can confuse the student model.\n- To tackle this, this work proposes a novel three-stage \"learn-compare-critique\" paradigm called Autonomous Preference Optimization (APO). The student model first learn a broad knowledge via standard supervised distillation from all teachers. Second, it compares and aggregates the teachers' outputs and performs self-distillation to generate a unified reasoning trajectory. Finally, it critiques the initial knowledge by using the consensus trajectory as a preferred sample and the individual teacher outputs as negative samples using a simple contrastive learning loss."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to understand. The concept drift problem is clear and easy to follow.\n- The core idea is straightforward: using the student policy model to rephrase the distilled multiple teacher model responses and then learning from it, whether using imitation learning (SFT) or using offline RL (DPO), which is used in this work."}, "weaknesses": {"value": "- How the model actually creates the consensus answer from all the different teacher outputs. Is the text from all teachers combined?  If so, how to do this when the multiple teacher outputs are too long for the student model? \n- The self-distillation process is a critical step in this work. However, the student model does not undergo specific training on the aggregation task of multiple teacher outputs; how can it generate positive samples for subsequent DPO training?\n- The APO framework treats the outputs from all strong teacher models (e.g., GPT-5) as negative samples, which assumes that the pre-distilled small student model can provide better responses based on the aggregation outputs. This claim should have a solid experiment to verify (e.g., using a rule/model-based score to justify this). \n- This work lacks a lot of baselines to be compared with in the main results: (1) single teacher distillation (KL-based / response-based); (2) naive multi-teacher distillation (KL-based / response-based); (3) distill-then-merge methods."}, "questions": {"value": "Same as weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uJUjaPDF61", "forum": "03u504EDJp", "replyto": "03u504EDJp", "signatures": ["ICLR.cc/2026/Conference/Submission525/Reviewer_ppCm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission525/Reviewer_ppCm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762074525510, "cdate": 1762074525510, "tmdate": 1762915538461, "mdate": 1762915538461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the underexplored problem of knowledge distillation from multiple drifting MLLMs, where inconsistent reasoning trajectories across teachers cause concept drift and bias propagation. The authors propose APO, a “learn–compare–critique” paradigm that enables the student model to self-distill and align reasoning concepts autonomously. Experiments show that this method has certain effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper establishes a clear theoretical link between concept drift and knowledge distillation, extending the KD paradigm to multi-teacher and non-stationary environments.\n2. The introduction of the CXR-MAX dataset provides a new and valuable benchmark for the medical imaging community."}, "weaknesses": {"value": "1. The paper’s motivation is unclear. Although the authors claim to study the distillation of MLLMs, they do not compare the method with other mainstream methods[1,2,3,4,5,6] in the distillation community. In my view, this work reads more like a medical image understanding paper rather than a genuine distillation study.\n2. All experiments are conducted solely on the CXR-MAX dataset, making it difficult to demonstrate the general effectiveness of the proposed method.\n3. The assumptions are overly strong. In Equation (4), the authors assume that the inference trajectories of teacher models are independent. This assumption is unrealistic in practice since different MLLMs often originate from similar pretraining distributions.\n4. The paper claims theoretical contributions, but the proposed APO objective is mainly a reformulation of DPO and lacks a theoretical analysis of convergence and stability under multi-teacher drift scenarios.\n5. The notation in the theoretical section is confusing, with many symbols left undefined or unexplained, making the derivations hard to follow.\n6. The font size in Figure 2 is too small, making it difficult to read.\n\n[1] f-Divergence Minimization for Sequence-Level Knowledge Distillation. ACL 2023.\n\n[2] DistiLLM: Towards Streamlined Distillation for Large Language Models. ICML 2024.\n\n[3] MiniLLM: Knowledge Distillation of Large Language Models. ICLR 2024.\n\n[4] Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models. COLING 2025.\n\n[5] ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via alpha-beta-Divergence. ICML 2025.\n\n[6] DA-KD: Difficulty-Aware Knowledge Distillation for Efficient Large Language Models. ICML 2025."}, "questions": {"value": "In addition to the issues mentioned in the Weaknesses, I have a few more concerns:\n\n1. Although the authors mention high computational cost, the paper lacks concrete efficiency analysis or runtime comparison experiments.\n2. Is it always better to use more and stronger teacher models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YpLGCI4tXU", "forum": "03u504EDJp", "replyto": "03u504EDJp", "signatures": ["ICLR.cc/2026/Conference/Submission525/Reviewer_5ziE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission525/Reviewer_5ziE"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762157446982, "cdate": 1762157446982, "tmdate": 1762915538277, "mdate": 1762915538277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}