{"id": "pFTfTJitMb", "number": 20318, "cdate": 1758304706196, "mdate": 1759896984173, "content": {"title": "Embedding Interpretability Score: A Domain-Agnostic Representation Quality Assessment", "abstract": "Interpretability has long influenced the selection of machine learning models, yet the role of data representations remains relatively underexplored. Although model choice is known to influence performance, the interpretability of embedding models can also be equally critical. In this study, we present a comparative analysis of various black-box and interpretable embedding models within multiple domains, including natural language processing and computer vision. We introduce a domain-agnostic quantitative score called Embedding Interpretability Score (EIS) to measure the interpretability of embedding models based on three fundamental properties: dimensionality, which reflects representational compactness; sparsity, which highlights feature selectivity; and clusterability, which measures semantic organization. Our results indicate that, in general, the choice of the embedding technique exerts a significant influence on downstream performance in comparison to classifier selection. Interestingly, the relationship between interpretability and performance differs across modalities: in NLP tasks, higher-performing embeddings tend to have lower interpretability, whereas in CV tasks, embeddings with higher interpretability often achieve better downstream performance.", "tldr": "", "keywords": ["Embedding Interpretability Score", "Interpretable Models", "Black-Box Models", "Trade-Off Analysis", "Accuracy vs. Interpretability", "Dimensionality", "Sparsity", "Clusterability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa6b3b73e4ee735d4e2e90726272617810ebf439.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper offers an equation that computes an embedding interpretability score for architecture. The Score consists of 3 components, loading, roughly on the extent to which the embeddings form clusters (whose size is preset), the dimensionality of the embeddings studied (extracted from a layer) and the sparsity of the embeddings. The score is computed for different NLP and CV architectures treated as feature extractors, and evaluated in relation to performance of classifiers added to those architectures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The score is simple, model agnostic to allow interpretation across models; does not require supervision. It can be a good starting to ground human-facing interpretability in a single score, but currently should be treated more as a complexity metric."}, "weaknesses": {"value": "**  Major weaknesses\n\n* It is difficult to understand the specific contribution: The review of related work is quite vague (esp. lines 84–87), generic, and doesn’t let the reader understand the potential of this method to provide information that differs and goes beyond from prior work. There is not tight contrast or competitive comparison in the work.\n* No alternatives are evaluated for the specific equation of the main composite score (geometric mean of three parts). Why these components, under equal weighting, and this aggregation?\n\n* Most generally, it seems that the main claims have to do with the relation between embedding interpretability (complexity) and architecture: while the score is computed on embeddings from specific datasets, the claims, and intro, refer repeatedly to architectures and their complexity. If the focus is inherent complexity, why not use weight-space or function-space complexity (e.g., effective dimensionality, spectral complexity) that are computed from weight matrices and are dataset-agnostic? At minimum, showing the relation between the two families of measures is essential to understand relative contribution of current work.\n\n* (line 159). The claim that high silhouette implies human-aligned organization isn’t supported by data or literature; it can just reflect internal organization.\n\n* The argument that sparse embeddings are “more interpretable” isn’t established. At the limit, sparsity could just reflect a codebook-like representation in a layer, over feature combinations per category, not meaningful axes.\n\n* Evaluating the silhouette by Fixing k=5 clusters is unjustified. Clustering in embedding space may not reflect category structure; that’s what a linear readout decides.\n\n** Minor weaknesses\n\n* Repetition (lines 170–172 vs 161). Redundant discussion around the clustering choice.\n\n* Figure 2 legend. Missing a color scale mapping colors to accuracy.\n\n* some LLM-like phrasing in multiple places.  Terms like \"striking\" trend (line 61) and “underscoring” (line 60) read like a promotion."}, "questions": {"value": "None at this point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KKLhmlHd0Z", "forum": "pFTfTJitMb", "replyto": "pFTfTJitMb", "signatures": ["ICLR.cc/2026/Conference/Submission20318/Reviewer_Pw7x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20318/Reviewer_Pw7x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906306944, "cdate": 1761906306944, "tmdate": 1762933780231, "mdate": 1762933780231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an “Embedding Interpretability Score” (EIS) that geometrically averages three components: Dimensionality, Sparsity, and Clusterability. The authors applied their score to CV and NLP tasks and concluded that, for NLP tasks, embeddings with higher predictive accuracy tend to be less interpretable, while in CV tasks, the more interpretable embeddings are also more performant."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear writing. the paper is straightforward to follow."}, "weaknesses": {"value": "The first important issue is the claims and the resulting outcome of this paper. Since the proposed score seems to be largely heuristic and based on intuition, any change in the formulation of dimensionality, sparsity, or clusterability can alter the results. Below, I outline some of the major weaknesses of the paper\n\n- **Novelty and the Claims**: Most ingredients already appear in prior work: sparsity as interpretability, topic/category coherence or silhouette-style cluster quality, and compositing simple metrics. What’s “new” here is mostly *the specific normalization choices and the geometric mean*, not the underlying principles which are not justified theoretically or empirically. the paper should reduce the novelty claims and position EIS as a lightweight heuristic rather than a principled metric.\n- **Evaluation Problem** comparing Word2Vec vs. BERT vs. MPNet vs. RoBERTa, trained on different data with different objectives and dimensionalities, cannot support causal statements about “interpretability vs. accuracy.” The reported (anti)correlations may be entirely driven by model family and training recipe rather than the three properties. The paper itself interprets these as meaningful “domain-specific” trends, which is too strong.\n- A proper way to evaluate their score and compare performance would be to control for these confounding variables and measure interpretability only based on dimensionality, sparsity, and clusterability. For example, one simple experiment would be to train Word2Vec on the same dataset with different embedding dimensions. In this setup, everything else remains constant, and only the dimensionality changes. According to the intuition of the paper, this should result in lower interpretability but higher performance.\n- **Arbitrary normalization & floors.** Dimensionality is normalized using a fixed reference range (e.g., 64–2048), and all three components are clipped to  0.05 , 1. This means models above 2048 dimensions (or below 64) saturate, and the floor artificially props up scores. There’s no sensitivity study to the reference range or the 0.05 floor. This can materially reorder models.\n\n\nOverall, I believe that the paper’s current experimental setup where embeddings from different models, training paradigms, and even datasets are compared is not appropriate for evaluating the proposed scoring method. Therefore, I am inclined to reject this paper."}, "questions": {"value": "What is the justification for using the geometric mean to combine the components, and why were other aggregation methods (e.g., arithmetic or weighted means) not considered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ITnLQao5PK", "forum": "pFTfTJitMb", "replyto": "pFTfTJitMb", "signatures": ["ICLR.cc/2026/Conference/Submission20318/Reviewer_5oAF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20318/Reviewer_5oAF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955464480, "cdate": 1761955464480, "tmdate": 1762933779958, "mdate": 1762933779958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduced the Embedding Interpretability Score (EIS), a domain-independent framework designed to evaluate how interpretable embedding models are.\nEIS unifies three quantitative factors--dimensionality, sparsity, and clusterability--to characterize the compactness, selectivity, and semantic organization of an embedding space.\nThese components are individually normalized and aggregated through a geometric mean, ensuring balanced weighting while discouraging weak performance in any single aspect.\nThrough systematic comparisons, the proposed provides a consistent and quantitative basis for ranking embeddings across domains, such as natural language processing and computer vision, regardless of the classifier used."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- **S1. Straightforward Metric Design and Comprehensive Cross-Domain Evaluation**\n\nThe Embedding Interpretability Score (EIS) is formulated in a clear and straightforward way, making it easy to understand and implement. By combining three intuitive and widely recognized properties--dimensionality, sparsity, and clusterability--the metric provides a simple yet systematic approach to quantifying interpretability across embedding models. \n\nIn addition, the paper conducts a diverse and balanced comparison across both NLP and CV domains, evaluating a broad spectrum of embedding models ranging from traditional word embeddings to modern transformer- and vision-based architectures. \nThis experimental design demonstrates the generalizability of the proposed metric and helps illustrate how interpretability behaves differently across modalities."}, "weaknesses": {"value": "- **W1. Overly Simplistic Metric Design**\n\nAlthough the EIS offers a clear, domain-agnostic framework for assessing interpretability, its formulation is overly simplistic and fails to reflect the sophistication of modern interpretable modeling.\nThe three underlying components--dimensionality, sparsity, and clusterability--are generic geometric descriptors long used as regularization or auxiliary objectives in representation learning and explainable AI (XAI). This minimal design overlooks the richer structural principles that define current explainable models.\n\nIn computer vision, for instance, interpretability is increasingly achieved through explicit concept or prototype reasoning rather than post-hoc statistical properties. Works such as [1] introduce object-centric concept learning and cross-attention for human-aligned explanations, while [2] and [3] anchor interpretability in prototype-based reasoning. Likewise, Concept Bottleneck Models (CBMs) like [4] and [5] enable controllable reasoning via explicit concept supervision.\nCompared with these intrinsically interpretable frameworks--also mentioned by the authors in their introduction--EIS merely aggregates low-level statistics and provides little insight into how concepts, prototypes, or attention mechanisms contribute to interpretability. Its simplicity ensures broad applicability but limits its ability to capture the causally grounded and semantically structured interpretability emerging in recent concept-based research.\n\n- **W2. Limited Actionability and Overreliance on Quantitative Interpretability**\n\nDespite presenting detailed performance-interpretability trade-offs across NLP and CV tasks, the practical value of EIS for end-users remains unclear. The metric treats interpretability as a single numerical construct, neglecting its multidimensional and context-dependent nature [6–8]. As a result, it is uncertain how users should act on these scores—whether a low EIS should discourage using an embedding, or how to balance interpretability against accuracy in real applications.\n\nIn practice, interpretability depends on human understanding, contextual relevance, and explanatory faithfulness, as emphasized in human-centered studies [9–11], none of which are captured by a purely statistical index. The trade-off results in the paper are diagnostic rather than prescriptive: they describe correlations but offer no actionable guidance, thresholds, or decision criteria. Consequently, despite its analytical clarity, EIS risks functioning only as a reporting benchmark rather than a decision-support framework for interpretability-driven model selection.\n\n\n- **W3. Insufficient Empirical Validation and Weak Baseline Comparison**\n\nAlthough the paper positions EIS as a domain-agnostic metric, its empirical validation relies on a limited set of clean benchmark datasets-Amazon Reviews for NLP and CIFAR-10, STL-10, and Fashion-MNIST for CV.\nThese datasets are convenient but fail to represent the noise, heterogeneity, and domain complexity of real-world scenarios such as multimodal or clinical data. Hence, the results do not fully substantiate the claim of cross-domain generality or robustness.\n\nFurthermore, EIS is not compared against existing quantitative interpretability measures, such as [12] or [13]. Without these baselines, it remains unclear whether EIS introduces genuinely new insights or simply re-expresses known embedding properties. This weak comparative validation limits the external credibility and novelty of the proposed metric.\n\n\n- **References**\n- [1] Hong, J., Park, K.H., & Pavlic, T.P. (2024). Concept-Centric Transformers: Enhancing Model Interpretability through Object-Centric Concept Learning within a Shared Global Workspace. IEEE/CVF WACV 2024, 4880–4891.\n- [2] Ma, C., Donnelly, J., Liu, W., Vosoughi, S., Rudin, C., & Chen, C. (2024). Interpretable Image Classification with Adaptive Prototype-Based Vision Transformers. NeurIPS 2024, 37, 41447–41493.\n- [3] Davoodi, O., Mohammadizadehsamakosh, S., & Komeili, M. (2023). On the Interpretability of Part-Prototype Based Classifiers: A Human-Centric Analysis. Scientific Reports, 13(1), 23088.\n- [4] Lai, S., Hu, L., Wang, J., Berti-Equille, L., & Wang, D. (2023). Faithful Vision-Language Interpretation via Concept Bottleneck Models. ICLR 2023.\n- [5] Wang, Z., Popel, A., & Sulam, J. (2025). Concept Bottleneck Model with Zero Performance Loss. Proceedings of the 2nd Conference on Parsimony and Learning (Proceedings Track).\n- [6] Lipton, Z. C. (2018). The Mythos of Model Interpretability. Queue, 16(3), 31–57.\n- [7] Miller, T. (2019). Explanation in Artificial Intelligence: Insights from the Social Sciences. Artificial Intelligence, 267, 1–38.\n- [8] Doshi-Velez, F., & Kim, B. (2017). Towards a Rigorous Science of Interpretable Machine Learning. arXiv:1702.08608.\n- [9] Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., & Kagal, L. (2018). Explaining Explanations: An Overview of Interpretability of Machine Learning. IEEE DSAA 2018, 80–89.\n- [10] Mohseni, S., Zarei, N., & Ragan, E. D. (2021). A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems. ACM TiiS, 11(3–4), 1–45.\n- [11] Poursabzi-Sangdeh, F., Goldstein, D. G., Hofman, J. M., Wortman Vaughan, J. W., & Wallach, H. (2021). Manipulating and Measuring Model Interpretability. CHI 2021, 1–52.\n- [12] Trifonov, V., Ganea, O.-E., Potapenko, A., & Hofmann, T. (2018). Learning and Evaluating Sparse Interpretable Sentence Embeddings. EMNLP BlackboxNLP Workshop 2018, 200–210.\n- [13] Şenel, L. K., Utlu, İ., Yücesoy, V., Koc, A., & Cukur, T. (2018). Semantic Structure and Interpretability of Word Embeddings. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26(10), 1769–1779."}, "questions": {"value": "Most of my main concerns or questions have been outlined in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T8pazzt5FW", "forum": "pFTfTJitMb", "replyto": "pFTfTJitMb", "signatures": ["ICLR.cc/2026/Conference/Submission20318/Reviewer_HgBM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20318/Reviewer_HgBM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762041361127, "cdate": 1762041361127, "tmdate": 1762933779505, "mdate": 1762933779505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}