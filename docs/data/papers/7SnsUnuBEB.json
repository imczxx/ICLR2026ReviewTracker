{"id": "7SnsUnuBEB", "number": 15344, "cdate": 1758250418403, "mdate": 1759897312645, "content": {"title": "Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted Annealing", "abstract": "Large language models (LLMs) fine-tuning shows excellent implications. However, vanilla fine-tuning methods often require intricate data mixture and repeated experiments for optimal generalization. To address these challenges and streamline the training process, we propose an efficient and universal solution, Dynamic Boosted Annealing (DBA). We obtain a global gradient through zero-learning-rate training on general data, which is subsequently employed for gradient boosting and dynamic training step correction during domain training. In conjunction with annealing learning, we end up establishing a fine-tuning pipeline that relies solely on domain data without collapse. By evaluating both general and domain-specific performance across multiple tasks on several popular base models, DBA achieves an average improvement of 5.8\\% in joint performance over vanilla fine-tuning. Furthermore, since general data is no longer involved in annealing, repeated experiments led by data mixture are also eliminated. According to our tests, the DBA method can reduce GPU hours by 91.0\\% compared to the vanilla method.", "tldr": "We propose Dynamic Boosted Annealing (DBA), an efficient fine-tuning method that avoids catastrophic forgetting without data mixture by leveraging a pre-computed global gradient.", "keywords": ["LLM", "Domain-Specific Training", "LLM Finetuning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d8801b896fcc6406f2fc6ecbf474232da1af090.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework called Finetune-Once that aims to improve multi-domain fine-tuning efficiency of LLMs by decoupling the optimization of the model’s backbone and task-specific heads. The key idea is to first perform a shared backbone update that captures domain-general knowledge, followed by lightweight per-domain head updates to preserve domain-specific information. Experimennts on several benchmarks covering language understanding, reasoning, and coding tasks show that Finetune-Once achieves comparable or better performance than standard sequential or joint fine-tuning approaches, with reduced computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem of efficient multi-domain fine-tuning for LLMs is timely and practically relevant.\n2. The proposed decoupling idea offers an intuitive way to separate shared and domain-specific optimization, potentially reducing interference.\n3. The method is relatively simple to implement and could be useful in applied fine-tuning scenarios."}, "weaknesses": {"value": "I have the following concerns. *If the authors could properly address them during the rebuttal phase, I am willing to raise my score.*\n\n1. The idea of decoupling backbone and head optimization has been explored in several prior works, such as adapter-based learning, LoRA variants, and modular continual fine-tuning. The paper does not clearly distinguish how Finetune-Once differs conceptually or algorithmically from these existing methods.\n2. This paper provides little analysis to justify why decoupling should prevent gradient interference or why the proposed optimization order (backbone first, head later) leads to better generalization.\n3. Although the experiments show modest improvements, the gains are small and may fall within typical variance. No significance testing is provided.\n4. Some important recent baselines [1,2,3] are missing. The paper would benefit from either a comparison with these methods or a discussion of their relevance.\n5. The proposed Finetune-Once may underperform when domains are highly imbalanced or when tasks require cross-domain transfer, but the authors do not analyze such scenarios.\n\n[1] Mixture-of-Skills: Learning to Optimize Data Usage for Fine-Tuning Large Language Models. EMNLP 2024.\n\n[2] How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. ACL 2024.\n\n[3] Boosting Multi-Domain Fine-Tuning of Large Language Models through Evolving Interactions between Samples. ICML 2025."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dgII0XpTDT", "forum": "7SnsUnuBEB", "replyto": "7SnsUnuBEB", "signatures": ["ICLR.cc/2026/Conference/Submission15344/Reviewer_T8Kj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15344/Reviewer_T8Kj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761481214101, "cdate": 1761481214101, "tmdate": 1762925634104, "mdate": 1762925634104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a core, pervasive problem in large language model (LLM) fine-tuning: catastrophic forgetting of general capabilities when adapting to a specific domain, and the high computational cost of data mixture strategies used to mitigate it.\nExperimental results show that DBA significantly outperforms baselines (including direct fine-tuning, vanilla fine-tuning with data mixture, and PEFT methods like LoRA) across four different domains (Finance, Medicine, Law, News QA) and three base models (Llama3.1, Phi4, Qwen3). DBA achieves the best balance between domain performance (SD) and general performance (SG), as measured by the harmonic mean (S). Most critically, because DBA completely eliminates the need for data mixture and its associated tuning, it is claimed to reduce GPU hours by up to 91.0% compared to the vanilla fine-tuning method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a very practical and costly problem. In practice, finding the optimal data mixture ratio for each domain is a compute-intensive, empirical process. DBA offers a principled solution that claims to eliminate this process entirely, which is highly significant for the practical deployment and iteration of LLMs. The claimed 91.0% reduction in computational cost is a massive advantage.\nThe paper is exceptionally well-written. The problem statement (Table 1, Fig 1), methodology (Fig 3, Sec 3), and experimental results (Table 3) are all presented with great clarity."}, "weaknesses": {"value": "The term \"Gradient Boosted\" is highly misleading. As the authors acknowledge in Section 3.1, this is not the standard meaning of sequentially fitting models to residuals, as in Gradient Boosting Machines (e.g., XGBoost). Here, it functions more as \"gradient anchoring\" or \"gradient augmentation.\" Using a term with a well-established, different meaning in machine learning is likely to cause unnecessary confusion."}, "questions": {"value": "DBA is presented as a full-parameter tuning technique. Have the authors investigated combining it with PEFT methods like LoRA? For example, applying the GGB and DC gradient modifications to the low-rank adapter instead of the full weights? This seems like a very promising direction to achieve both computational (no data-mixture) and memory (parameter-efficient) efficiency.\n\nIn Section 3.1, the derivation for GGB (Eq. 4) suggests that using a fixed $g_G$ reduces the variance of the combined gradient. This is intuitive. How does this connect to the motivation in Section 2.2 that the general domain (Fig 2a) has \"many local optima\"? Does GGB help the optimizer \"bypass\" these poor local optima by providing a smoother, more stable gradient (derived from $g_G$)? More intuition on how GGB addresses the observed loss landscape issue would be appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r4tnBfujx8", "forum": "7SnsUnuBEB", "replyto": "7SnsUnuBEB", "signatures": ["ICLR.cc/2026/Conference/Submission15344/Reviewer_aLnJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15344/Reviewer_aLnJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603675347, "cdate": 1761603675347, "tmdate": 1762925633591, "mdate": 1762925633591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper looks at a big challenge people face when fine-tuning large language models: these models can easily forget their general abilities when you train them for a specific topic. The usual fix—mixing lots of different data in the training process—can be very expensive and slow.\nTests show that DBA does much better than other common approaches—like regular fine-tuning, mixing in lots of different data, or using methods like LoRA—across areas like Finance, Medicine, Law, and News QA, and on several popular models. DBA finds a great balance between doing well on the new topic and not forgetting what the model already knows. The best part: since DBA doesn’t need all that extra mixed data or complicated setup, it can cut the time spent training on GPUs by up to 91% compared to the usual fine-tuning methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper takes on a real-world problem that costs a lot of time and money. Usually, figuring out the best way to mix different types of training data for each domain is a slow and tricky process that eats up a lot of computing resources. DBA promises a much simpler approach that sidesteps this hassle completely, making it much easier to use and update large language models in practice. If it really can cut computing costs by 91%, that’s a huge deal.\nThe paper itself is also very clear and easy to follow. The authors do a great job explaining the problem, how their method works, and the results, using helpful tables and figures along the way."}, "weaknesses": {"value": "Calling this method “Gradient Boosted” is pretty confusing. As the authors mention in Section 3.1, it doesn’t actually mean what most people expect—like training a series of models to fix errors, as with Gradient Boosting Machines (like XGBoost). Instead, it’s more about anchoring or adding to the gradients. Using a familiar term in a totally new way will probably just make things unclear for readers."}, "questions": {"value": "DBA is presented as a full-parameter tuning technique. Have the authors investigated combining it with PEFT methods like LoRA? For example, applying the GGB and DC gradient modifications to the low-rank adapter instead of the full weights? This seems like a very promising direction to achieve both computational (no data-mixture) and memory (parameter-efficient) efficiency.\n\nIn Section 3.1, the derivation for GGB (Eq. 4) suggests that using a fixed $g_G$ reduces the variance of the combined gradient. This is intuitive. How does this connect to the motivation in Section 2.2 that the general domain (Fig 2a) has \"many local optima\"? Does GGB help the optimizer \"bypass\" these poor local optima by providing a smoother, more stable gradient (derived from $g_G$)? More intuition on how GGB addresses the observed loss landscape issue would be appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r4tnBfujx8", "forum": "7SnsUnuBEB", "replyto": "7SnsUnuBEB", "signatures": ["ICLR.cc/2026/Conference/Submission15344/Reviewer_aLnJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15344/Reviewer_aLnJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603675347, "cdate": 1761603675347, "tmdate": 1763354935715, "mdate": 1763354935715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dynamic Boosted Annealing (DBA), a novel fine-tuning framework designed to address catastrophic forgetting of general capabilities when adapting Large Language Models (LLMs) to specific domains. The authors aim to overcome the inefficiency of traditional data mixture strategies, which require costly repeated experiments to find optimal mixing ratios. DBA proposes a two-stage approach: first, it pre-computes a \"global gradient\" representing the general domain optimization direction using zero-learning-rate training; second, during fine-tuning on only domain-specific data, it uses this global gradient to \"boost\" the domain gradient (Global Gradient Boosted Learning - GGB), dynamically adjusts the update magnitude based on the similarity between global and domain gradients (Dynamic Correction - DC), and employs a very low, decaying learning rate (Annealing Learning - AL). The authors claim this \"finetune once\" approach significantly reduces computational cost while effectively balancing domain performance and general capability retention, presenting experiments across several models and domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework offers a highly practical and efficient alternative to data mixing for domain adaptation. By pre-calculating a reusable global gradient and enabling fine-tuning solely on domain data, DBA directly tackles the major cost bottleneck of repeated mixture experiments .\n\n\n\n2. The proposed combination of techniques (GGB, DC, AL) represents an innovative attempt to explicitly decouple and balance general knowledge retention and domain-specific learning within the optimization process itself .\n\n\n\n\n\n3. The empirical results demonstrate that DBA achieves a strong balance between domain performance and general capability preservation across multiple base models and domains, often outperforming baselines in harmonic mean score while drastically reducing GPU hours compared to vanilla fine-tuning ."}, "weaknesses": {"value": "1. The theoretical justification for Global Gradient Boosting (GGB) appears insufficient. While motivated by reducing variance, adding a fixed, pre-computed global gradient (potentially outdated as the model parameters change) to the domain gradient throughout training lacks rigorous analysis on why this acts as an effective regularizer, especially compared to established techniques. The connection drawn to SVRG seems tenuous given the fixed nature of $\\hat{g}_G$.\n\n2. The Dynamic Correction (DC) mechanism lacks clear intuition and adequate ablation. The paper adjusts update steps based on gradient similarity , potentially increasing step size when gradients differ  (based on Eq 9), but the rationale for why divergence warrants larger steps isn't fully explained. Furthermore, the ablation study (Table 4) only shows combined effects, not the isolated contribution or potential negative impacts of DC itself.\n\n3. The performance comparison might overstate practical gains relative to optimally tuned baselines. While DBA significantly reduces the cost of finding the best data mixture, its final domain performance ($S_D$) may still be slightly below that achieved by vanilla fine-tuning at its optimal (but expensive to find) mixing ratio. The claimed 91% cost saving compares DBA to the entire process of searching for the best vanilla ratio, not a single run with the optimal ratio."}, "questions": {"value": "1. Figure 2 is cited as Figure 3 in the text. Please correct the figure numbering or citation.\n\n2. The calculation of the global gradient $\\hat{g}_G$ uses $\\beta_1 \\to 1$. Does this require a full pass over the general data? How sensitive is $\\hat{g}_G$ (and thus DBA's performance) to the size and quality of the general data used for this pre-computation?\n\n3. The choice of $k_0 = 200/T$ for the GGB boost magnitude seems empirically derived. Is there any theoretical intuition for why this inverse relationship with total steps (T) is appropriate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Cwl33LooBj", "forum": "7SnsUnuBEB", "replyto": "7SnsUnuBEB", "signatures": ["ICLR.cc/2026/Conference/Submission15344/Reviewer_PtTU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15344/Reviewer_PtTU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724531069, "cdate": 1761724531069, "tmdate": 1762925633250, "mdate": 1762925633250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **Dynamic Boosted Annealing (DBA)**, a two-stage *full-parameter fine-tuning* framework intended to mitigate catastrophic forgetting without mixing in-domain and general data during training.\n\n**Stage 1 (Zero-LR Global Gradient Estimation):** On a “general” dataset, the model is run with learning rate ≈ 0 and large β₁ so that Adam’s first-moment accumulator approximates the **expected general gradient**; this “anchor” gradient is then **SVD-compressed** (low rank) and stored.\n\n**Stage 2 (Domain FT with Anchoring):** During domain fine-tuning, each step linearly combines the current domain gradient with the stored **global anchor gradient** (with a decaying weight), applies a **similarity-based dynamic correction**, and uses **very small initial LR + annealing**.\n\nEvaluation uses a **balance metric** $S = \\mathrm{HM}(S_D, S_G)$ (harmonic mean of domain and general scores). The paper reports higher $S$ than direct full FT and several PEFT baselines on multiple base models, plus experiments on a **temporal OOD** News-QA benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Mitigation of Catastrophic Forgetting:** Compared to standard full fine-tuning (Direct FT), the core contribution of DBA is its ability to mitigate catastrophic forgetting. By **leveraging global gradient information** ($\\hat{g}_{G}$) as a regularizing anchor during domain training , the method can specialize in a new domain while **maintaining a certain level of performance on general tasks** ($S_G$).\n\n**Achieving a Performance Balance (S-Score):** The method is designed to optimize for a **balance** between domain-specific performance ($S_D$) and general capability retention ($S_G$). By preventing the collapse of $S_G$ that plagues Direct FT, DBA consistently achieves a superior harmonic mean ($S$) score, demonstrating its effectiveness in producing a more robust and practical model.\n\n**Reusable global signal:** The **single** global anchor (after low-rank compression) can be reused across domains for the *same* base model, which is practically appealing for organizations that repeatedly fine-tune on related domains."}, "weaknesses": {"value": "- **Cost fairness is not established.** The paper primarily compares **GPU-hours** (time) against **PEFT** baselines (LoRA/DoRA/etc.). That’s **not an apples-to-apples comparison** because **PEFT methods are designed mainly to be memory-efficient and to train with few tunable parameters**.\n    - DBA is full fine-tuning **plus** an **extra memory/IO footprint** for storing and repeatedly using the low-rank **anchor gradient**.\n    - Without reporting **peak VRAM**, **number of trainable parameters**, and **on-disk/host memory** for the anchor, the “comparable GPU-time” claim can be misleading.\n    - A fair comparison should include **(i) peak VRAM, (ii) wall-clock time, (iii) GPU-hours, (iv) $S/S_D/S_G$** on **identical hardware**, and should also compare to **memory-efficient full-FT** methods (e.g., GaLore/Natural-GaLore, MeZO/Sparse-MeZO) rather than only PEFT.\n- **Anchor-gradient justification lacks direct baselines.** The paper motivates anchoring via variance reduction and stability, but **does not directly compare** against established *gradient conflict/scale control* methods such as **PCGrad** or **GradNorm** under the same setups. Such comparisons are needed to validate that the *anchor-mixing* specifically is the right mechanism (vs. gradient surgery or reweighting).\n- **“Finetune once” vs. hyperparameter sensitivity.** The reported sensitivity peak around a specific **k₀** (e.g., ~200/T) undercuts the “no retuning” narrative unless the authors provide a **robust heuristic** that generalizes across models/domains and include **mean±std** over multiple seeds.\n- **Metric opacity risk.** Because $S$ is a harmonic mean over **normalized** scores, it can hide **absolute** degradations. Raw (unnormalized) scores and **Δ from the base model** should be reported alongside $S$.\n- **Dependency on “general data.”** The approach **requires** access to a sufficiently representative “general” dataset to estimate the anchor. Reproducibility, licensing, and **sensitivity to the composition/size** of this dataset are not fully characterized."}, "questions": {"value": "1. **Fair-cost table.** For Direct/Vanilla FT, LoRA/DoRA, GaLore (and variants), and DBA, please report on **the same hardware**: **peak VRAM, wall-clock time, GPU-hours, number of trainable parameters, on-disk/host memory for the anchor**, and final **$S, S_D, S_G$**.\n2. **Anchor vs. gradient-surgery baselines.** Under the same experimental setups (including the News-QA OOD split), how does DBA compare against **PCGrad** and **GradNorm** in terms of $S/S_D/S_G$, early-stage stability (loss/grad-norm curves), and convergence speed?\n3. **k₀ robustness.** Does the **k₀** optimum generalize across domains/models/seeds? Please provide **multi-seed sweeps** (mean±std) and, if applicable, a **default heuristic** that is demonstrably robust.\n4. **General-data sensitivity.** How does performance vary when the **general dataset** used to estimate the anchor changes in **composition** or **size**? Please report variance across at least **three** distinct general datasets.\n5. **Raw metrics.** Alongside $S$, please provide **raw, unnormalized** scores (and **Δ from the base model**) for general benchmarks (e.g., MMLU, MMLU-Pro, GSM8K, MATH, M3Exam) and domain metrics to ensure the harmonic mean is not masking regressions.\n6. **Anchor overhead.** What is the **memory and IO overhead per step** from storing/restoring the rank-truncated anchor gradient during Stage-2 training, and how does it affect **throughput**?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4MAvylRAF8", "forum": "7SnsUnuBEB", "replyto": "7SnsUnuBEB", "signatures": ["ICLR.cc/2026/Conference/Submission15344/Reviewer_UAVW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15344/Reviewer_UAVW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963181609, "cdate": 1761963181609, "tmdate": 1762925632764, "mdate": 1762925632764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}