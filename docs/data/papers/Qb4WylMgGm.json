{"id": "Qb4WylMgGm", "number": 11365, "cdate": 1758197524040, "mdate": 1759897579673, "content": {"title": "CASE: Coupled Adaptive Feature–Target Smoothing with Density-Gated Mixture-of-Experts for Robust Imbalanced Tabular Regression", "abstract": "Although tabular data are central to many real-world applications, their target distributions are often imbalanced, as the majority of samples correspond to a narrow range of values. This imbalance severely degrades performance in sparse, few-shot regions. Prior work on imbalanced regression has typically relied on coarse binning of the target space, discarding fine-grained information, or on spatial-locality assumptions inherited from image domains. We introduce CASE (Coupled Adaptive Feature–Target Smoothing with Density-Gated Mixture-of-Experts), a framework tailored to deep imbalanced tabular regression. CASE combines two complementary mechanisms. (i) Coupled Adaptive Smoothing first identifies “true” neighbors by jointly considering similarities in both the feature and target spaces. Based on these neighbors, it then calibrates representations in sparse regions by scaling the smoothing strength according to each sample’s continuous density. (ii) A Density-Gated Mixture-of-Experts (MoE) weights the contributions of specialized experts via a gate that predicts a density range from the original features. During training, experts take the calibrated features as input; at inference, the learned experts operate on the original features, yielding both higher accuracy and faster inference. Across 40 tabular benchmarks, CASE establishes state-of-the-art performance on balanced test sets by attaining the top average rank. Notably, it demonstrates exceptionally robust by minimizing performance loss on the original imbalanced test sets, consistently delivering balanced predictions that enhance few-shot accuracy without significantly sacrificing many-shot performance.", "tldr": "We introduce CASE, a framework for imbalanced tabular regression that adaptively smooths sparse data representations and weights specialized experts to achieve state-of-the-art balanced performance across all densities.", "keywords": ["Imbalanced Regression", "Tabular Data", "Mixture of Experts", "Adaptive Smoothing", "Representation Learning", "Density-Aware Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bba28d3d8191e4cc6765a97de8e27adb5a71ee0a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the challenge of imbalanced tabular regression. The authors propose CASE, a novel framework with two main components. First, Coupled Adaptive Smoothing. Instead of just finding neighbors based on features, CAS identifies \"true\" neighbors by jointly considering similarity in both the feature and target spaces. It then smooths samples from sparse regions by blending them with these neighbors. Second, Density-Gated Mixture-of-Experts. A \"gating network\" learns to identify the data's density region (e.g., few-shot, medium-shot, or many-shot) from the original features. It then routes the data to specialized \"expert\" models. The model uses an asymmetric training process: the experts are trained on the \"smoothed\" features from CAS to help them learn, but at inference time, they run on the original features."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper tackles the critical and relevant challenge of imbalanced regression, a problem that is far less explored than its classification counterpart \n2.\tThe authors validate their method on a diverse benchmark of 40 public tabular datasets."}, "weaknesses": {"value": "1.\tA notable concern is the inconsistency regarding inference speed. The abstract and main text describe the model as achieving \"faster inference\" and being \"fast and efficient.\" However, the empirical results in Appendix A.4 (Table 13) show an inference latency is 4 times higher than the single MLP baselines, as ($K+1$) forward passes are needed due to MoE.\n2.\tThe model's asymmetric architecture introduces a significant theoretical gap. The expert networks are trained on CAS-calibrated (smoothed) features, but are evaluated on the original, uncalibrated features during inference. The paper does not provide a theoretical justification or empirical about this.\n3.\tThe CASE framework is highly complex, combining several novel mechanisms (Coupled Smoothing, a Learnable Metric, Adaptive Density, and an Asymmetric MoE). The provided ablation study is insufficient to disentangle the individual contributions of these sub-components.\n- It is unclear if the performance gain originates from the complex density-gating or simply from ensembling three models. A baseline comparing CASE to a simple, non-gated ensemble of three MLPs or other baseline is critically missing.\n- It is not possible to determine which sub-component of the CAS module (e.g., the learnable metric vs. the coupled smoothing) is responsible for its benefit.\n4.\tThe paper's clarity could be improved.\n- The justification for some design choices relies on qualitative descriptions rather than empirical proof. For instance, the claim that the smoothing strength $s_i$ \"allows the model to intelligently navigate the bias-variance trade-off based on data uncertainty\" is presented without supporting evidence. The proof does have any impact of the way the model is used during inference.\n- Core components of the methodology, such as the auxiliary and load-balancing losses, are relegated to the appendix. I would suggest to the authors to put it in the main paper. \n- Figure 1 explicitly labels the experts as “Many,” “Medium,” and “Few” shots, suggesting a strict specialization of each expert to a particular density region. However, the training mechanism does not appear to enforce or guarantee this one-to-one correspondence. Moreover, the paper does not provide any empirical evidence that such specialization actually emerges in practice. This raises a key question: if the “few-shot” expert is indeed exposed to only a limited number of samples, how can it learn a meaningful and generalizable representation?\n5.\tThe related work section does not reflect the current state of the field. The majority of the cited works on imbalanced regression appear to be from before 2023. \n6.\tI would like to see newer baselines but also CatBoost results. \n7.\tI would suggest that authors reconsider the name of their paper as ‘Case’ will be hard to find online."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dj8DFMrMw0", "forum": "Qb4WylMgGm", "replyto": "Qb4WylMgGm", "signatures": ["ICLR.cc/2026/Conference/Submission11365/Reviewer_7c5u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11365/Reviewer_7c5u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858541742, "cdate": 1761858541742, "tmdate": 1762922494939, "mdate": 1762922494939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the problem of \"deep imbalanced tabular regression\" and proposes a framework named CASE to mitigate it. The method consists of two main components: (1) a Coupled Adaptive Smoothing (CAS) module that performs input-space smoothing, and (2) a Mixture-of-Experts (MoE) module. The authors evaluate the method on 40 public tabular regression datasets from UCI, OpenML, Scikit-Learn, and REBAGG benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provide a good and technically sound integration of many existing ideas, and form a practical design for real-world problems. \n- The paper is clear and easy to follow"}, "weaknesses": {"value": "### 1. Problem setup\nI hesitate to frame “deep imbalanced tabular regression” as a brand-new research direction. (1) Imbalanced regression focuses on label distribution, where prior work aims to mitigate the mismatch between imbalanced training data and balanced test performance; (2) Tabular data introduces input heterogeneity, and improving it with deep networks still remains an open problem. As the authors' experiments also suggest, the main challenge still lies in the tree-based vs. deep-based performance gap. These two aspects (deep imbalanced regression vs. deep tabular regression) are inherently orthogonal, and the methods addressing them are largely independent (so is the proposed CAS module). Combining them as a single research direction feels more like overlapping two separate challenges than defining a genuinely new problem.\n\n### 2. Method\nThe proposed framework contains a variety of components. Even though the integration sounds reasonable, none of them (weighted metric, input smoothing, or MoE) are technically novel, and the paper lacks sufficient ablation to clarify individual effect of each module or how they interact. Overall, the method feels more like a good engineering combination of existing ideas rather than novel contribution typically expected at a top-tier deep learning venue.\n\n- It is unclear why smoothing the input in CAS is effective for imbalanced tabular regression. The mechanism is similar to MixUp [1], where mixing neighboring samples improves generalization by encouraging local smoothness. The paper does not convincingly explain why input-space smoothing would specifically benefit imbalance + tabular scenarios beyond the general regularization effect already provided by MixUp-style interpolation.\n- The design of the auxiliary and load-balancing losses is conceptually unclear. My understanding is that the auxiliary loss uses the ground-truth frequency region as supervision, but the load-balancing loss, as defined in the manuscript, does not reflect frequency or density at all. It only enforces equal utilization across the three experts. If the authors believe that the load-balancing loss can automatically route low-frequency samples to a “low-shot” expert, this should be discussed and empirically verified, since the auxiliary loss is inherently biased toward majority regions. \n\n### 3. Experiment\nThe experimental comparison does not feel entirely fair. The authors mainly compare against deep imbalanced regression baselines, while the key challenge in deep imbalanced tabular regression still remains the handling of input heterogeneity. As a result, tree-based methods still show highly competitive performance. Including comparisons with recent deep tabular networks would make the experimental results more solid.\n\n[1] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. MixUp: Beyond Empirical Risk Minimization. ICLR, 2018."}, "questions": {"value": "- I find Eq. (1) and Eq. (2) interesting, and the authors could strengthen the paper by providing more ablation studies to evaluate and showcase the effect of $\\omega$. This would help solidify the technical contribution.\n- The authors could also show how accurately the MoE router assigns samples to the correct regions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ekoRbG2OFq", "forum": "Qb4WylMgGm", "replyto": "Qb4WylMgGm", "signatures": ["ICLR.cc/2026/Conference/Submission11365/Reviewer_4r9g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11365/Reviewer_4r9g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876100358, "cdate": 1761876100358, "tmdate": 1762922494379, "mdate": 1762922494379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers (Part 1)"}, "comment": {"value": "We greatly appreciate the reviewers’ thoughtful evaluations and valuable suggestions that helped us improve the paper. To help the reviewers better understand our contributions and alleviate your concerns, we provide our rebuttal as follows.\n\nThe content below constitutes our General Response addressing common concerns shared by the reviewers. Detailed, point-by-point responses to each reviewer's specific comments will be provided separately in their respective threads.\n\n# 1. Ablation Study and Contribution Analysis\n\nWe present additional Ablation Study results designed to clearly decouple the complexity of the CASE framework and isolate the contribution of each individual component. Addressing the common feedback received regarding this aspect, we have summarized these additional results to facilitate a better understanding of the paper's contributions.\n\nTo address the concern that it was unclear which internal elements of the CASE module contributed to performance, we introduce an ablation study that finely separates the sub-components.\n\n- **MoE_Only** : A baseline model that removes the CAS module and uses only raw features as input, measuring the independent performance of the MoE structure itself.\n\n- **MixUp_MoE** : A comparative model trained by applying standard MixUp data augmentation instead of CAS, used to demonstrate that CAS is more effective for imbalanced regression than simple augmentation techniques.\n\n- **CAS_Only** : A model that uses a single MLP instead of the MoE structure but applies the complete CAS module, verifying that expert specialization via MoE is essential for performance improvement.\n\n- **CAS_Feature_Only** : A model that analyzes the importance of the feature-target 'Coupled' mechanism by excluding Target Similarity from the CAS module and selecting neighbors based solely on feature similarity.\n\n- **CAS_No_Learnable_Metric** : A model that validates the utility of a learned metric for handling feature heterogeneity in tabular data by using standard Euclidean distance (1) instead of learnable weights ($w$) for distance calculation.\n\n- **CASE (Fixed Smooth_Strength_0)** : A setting where smoothing strength ($s_i$) is fixed to 0 (no smoothing performed), used to check for increased variance and overfitting in sparse regions.\n\n- **CASE (Fixed Smooth_Strength_1)** : A setting where smoothing strength ($s_i$) is fixed to 1 (using only neighbor information without raw features), used to check for increased bias and underfitting in many-shot regions.\n\n- **CASE** : The final model integrating all proposed components (Coupled Adaptive Smoothing, Density-Gated MoE, Asymmetric Training, etc.), achieving the best performance by optimizing the bias-variance tradeoff."}}, "id": "6XL9XzOLQh", "forum": "Qb4WylMgGm", "replyto": "Qb4WylMgGm", "signatures": ["ICLR.cc/2026/Conference/Submission11365/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11365/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11365/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763732881014, "cdate": 1763732881014, "tmdate": 1763732881014, "mdate": 1763732881014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on tabular data, and introduces the Coupled Adaptive Feature–Target Smoothing with Density-Gated Mixture-of-Experts (CASE) method for imbalanced tabular regression. It integrates Coupled Adaptive Smoothing (CAS) and Density-Gated Mixture-of-Experts (MoE) modules. CAS adaptively calibrates features by coupling similarity in both feature and target spaces, while MoE promotes expert specialization over regions. During training, experts take the calibrated features as input; at inference, the learned experts operate on the original features for fast and accurate inference. The architecture uses an asymmetric dual-path design, where the gating network processes raw features and the experts operate on density-calibrated representations. Experiments across 40 tabular benchmarks show that CASE can provide the best average rank on balanced and imbalanced test sets, and improve few-shot accuracy without sacrificing many-shot performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ Overall, the paper is clearly written, well organized, and easy to follow.  \n+ This paper focuses on a less explored problem of imbalanced regression on tabular data. CASE is a method specialized for this problem that combines a novel adaptive smoothing that considers feature and target spaces with a density-gated MoE that weights the contributions of specialized experts.   \n+ The CAS module is a novel smoothing mechanism that adaptively calibrates the feature space according to data density to stabilize feature representations in sparse, low-density regions. Calibrated features from CAS are utilized by expert networks within the density-​ated MoE.  The authors provide a connection to Vicinal Risk Minimization, where CAS is interpreted as an adaptive, density-aware extension. \n+The empirical evaluation involves 40 tabular benchmarks datasets, multiple baselines, ablations, and efficiency analyses. CASE can outperform SOTA methods on balanced test sets by attaining the top average rank. It can provide balanced predictions that enhance few-shot accuracy without significantly sacrificing many-shot performance. \n+ The Appendix has additional information on datasets, hyper parameter settings, ablation studies, analysis of computational complexity, feature visualizations, description of CASE components, analysis of novelty, and experimental results that help support the paper."}, "weaknesses": {"value": "- CASE is based on known approaches, like VRM, MoE, density smoothing. The paper combines adaptive feature-target smoothing and a density-gated MOE. The motivation for certain architectural choices (e.g., fixed three experts, specific gating losses) could be justified more rigorously. The math formulation in Section 3 could benefit from an improved explanation of the intuition.\n- In the experimental validation, the authors should provide a better interpretation of results on cross-dataset generalization and diversity.\n- The CASE architecture can significantly increase computational cost. For instance, the training overhead is about 4 times that of baselines.This paper should contain a more detailed analysis and comparison  to SOTA methods of time and memory complexity (at training and test times).\n- Their code is not made available, so there is a concern that the results in this paper would be difficult for a reader to reproduce."}, "questions": {"value": "See my comments in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G4pgGPHLAe", "forum": "Qb4WylMgGm", "replyto": "Qb4WylMgGm", "signatures": ["ICLR.cc/2026/Conference/Submission11365/Reviewer_pEnV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11365/Reviewer_pEnV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978747498, "cdate": 1761978747498, "tmdate": 1762922493910, "mdate": 1762922493910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}