{"id": "Ml2bDkzNeq", "number": 9011, "cdate": 1758107124745, "mdate": 1759897748164, "content": {"title": "AdaptSR: Rank-Aware Low-Rank Adaptation for Real-World Super-Resolution", "abstract": "Recovering high-frequency details from low-resolution images remains a central challenge in super-resolution (SR), particularly under complex and unknown real-world degradations. While GAN-based methods improve perceptual sharpness, they are unstable and introduce artifacts, and diffusion models achieve strong fidelity but demand excessive computation, even in few-step variants. We present AdaptSR, a rank-aware low-rank adaptation framework that efficiently repurposes bicubic-trained CNN and Transformer SR backbones for real-world tasks. Unlike full fine-tuning, AdaptSR inserts lightweight LoRA modules into convolution, attention, and MLP layers, updates them under a rank-aware allocation strategy guided by layer importance, and merges them back after training—ensuring no additional inference cost. This design reduces trainable parameters by up to 92% and shortens adaptation time from days to just 1–4 hours on a single GPU, aligning with the goals of sustainable and budget-friendly AI. Extensive experiments across diverse SR backbones and datasets show that AdaptSR consistently matches or surpasses full fine-tuning, outperforms recent GAN- and diffusion-based methods in distortion metrics, and delivers competitive perceptual quality. Comparisons with other parameter-efficient fine-tuning (PEFT) baselines further confirm the advantages of our rank-aware allocation. By unifying efficiency, scalability, and practical deployment, AdaptSR establishes a sustainable path for adapting SR models to real-world degradations. The code will be made publicly available.", "tldr": "AdaptSR, a rank-aware low-rank adaptation framework that enables sustainable, budget-friendly domain adaptation of super-resolution models, achieving state-of-the-art real-world performance with minimal training cost and no inference overhead.", "keywords": ["real-world super-resolution", "domain adaptation", "rank-aware allocation", "low-cost training", "parameter-efficient finetuning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61d1b02159454d749376c1cb28c236db5b06485d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AdaptSR, a framework for the rapid and parameter-efficient adaptation of bicubic-trained super-resolution (SR) models to real-world degradations. Based on Low-Rank Adaptation (LoRA), its key innovation is a rank-aware allocation strategy that distributes adapter capacity across layers based on gradient importance. The method is highly efficient, reducing adaptation time to 1-4 hours on a single GPU and trainable parameters by up to 92%, with no additional inference cost after merging. Extensive experiments demonstrate that AdaptSR matches or surpasses full fine-tuning and outperforms recent GAN- and diffusion-based methods on standard distortion metrics, while offering competitive perceptual quality. The work presents a compelling solution for practical and sustainable SR deployment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper successfully extends the LoRA paradigm beyond its common application in NLP and generative vision tasks to the domain of image restoration. The proposed rank-aware allocation mechanism is a novel and justified contribution for SR, providing a principled way to allocate limited parameters.\n2. The work directly addresses critical real-world constraints: training time, parameter efficiency, and inference overhead. The results—hours instead of days for adaptation, sub-10% parameter updates, and zero inference cost—are highly significant for sustainable AI and deployment on resource-constrained devices.\n3.  The paper provides thorough experiments across multiple SR backbones (EDSR, SwinIR, etc.) and datasets (RealSR, DRealSR), convincingly showing that the method consistently outperforms heavy GAN/diffusion baselines in fidelity and is competitive in perceptual quality."}, "weaknesses": {"value": "1. The literature review fails to engage with several recent and highly relevant PEFT methods that also explore dynamic or non-uniform rank allocation. Works such as \"RaSA,\" \"Sparse High Rank Adapters,\" \"RankAdaptor,\" or \"RA-SpaRC\" propose alternative strategies (e.g., rank-sharing, hierarchical allocation, sparse-plus-low-rank) for optimizing adapter capacity. Their absence raises questions about the novelty of the \"rank-aware\" concept in the broader PEFT landscape. A discussion or comparison with these methods is necessary to properly contextualize the contribution.\n2. As evidenced by the results in Table 2, the proposed rank-aware allocation offers only a marginal improvement over a simple uniform rank (r=8) under the same parameter budget (~0.07 dB PSNR, ~0.0025 LPIPS). This calls into question the practical necessity and complexity-to-benefit ratio of the gradient-based scoring and allocation algorithm. The authors should better justify why this added complexity is warranted over a well-tuned uniform baseline.\n3. The evaluation, while comprehensive on standard metrics, lacks a deeper analysis of semantic correctness, especially for structured content like faces and text. The visual examples focus on textures and patterns. Including dedicated tests on face super-resolution (using metrics like FID or identity preservation) and text-heavy images would more thoroughly demonstrate the method's ability to avoid the semantic hallucinations common in GAN/diffusion models.\n4. The term 'Baseline' in Table 1 is ambiguous. Please explicitly define the specific model configuration it refers to for each backbone (e.g., the pre-trained bicubic model, or the fully fine-tuned model) to allow for a clear interpretation of the performance gains."}, "questions": {"value": "1. How does your rank-aware allocation strategy conceptually and empirically compare to other recent non-uniform PEFT schemes like RaSA or Sparse High Rank Adapters? Could your gradient-based method be complementary to these approaches?\n2. Given the small performance delta between your rank-aware method and a uniform rank of 8 (Table 2), what is the definitive argument for adopting the more complex allocation strategy in practice? Are there specific architectures or degradation types where the advantage is more pronounced?\n3. Could you provide more rigorous evaluation on semantic faithfulness, for example, by reporting results on a face super-resolution benchmark (e.g., CelebA) or by quantifying text recognition accuracy on reconstructed text images?\n4. Please clarify the \"Baseline\" model used in Table 1. Is it the pre-trained model without any fine-tuning, or is it the fully fine-tuned model? This is critical for assessing the reported improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JzpUwoyhID", "forum": "Ml2bDkzNeq", "replyto": "Ml2bDkzNeq", "signatures": ["ICLR.cc/2026/Conference/Submission9011/Reviewer_isbx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9011/Reviewer_isbx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761208093318, "cdate": 1761208093318, "tmdate": 1762920738092, "mdate": 1762920738092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Parameter-Efficient Fine-Tuning (PEFT) framework named AdaptSR. The core innovation of the method lies in its \"Rank-Aware\" Low-Rank Adaptation (LoRA) strategy, which aims to efficiently adapt CNN and Transformer Super-Resolution (SR) models pre-trained on bicubic data to real-world scenarios with complex degradations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The primary advantage of this work lies in its practical value and training efficiency. AdaptSR reduces the domain adaptation process for SR models from several days to just a few hours on a single GPU, while concurrently decreasing the number of trainable parameters by over 90%. Crucially, through its weight-merging mechanism, it incurs no additional computational or storage overhead during inference. This facilitates the deployment and rapid iteration of SR models on resource-constrained devices and aligns with the current pursuits in the field.\n2. Although LoRA itself is not a new technique, the \"rank-aware allocation strategy\" proposed in this paper is an effective approach. By guiding rank allocation using gradient norms, the method provides a simple solution for a more intelligent application of PEFT in SR models."}, "weaknesses": {"value": "1. Using the gradient norm as a metric for layer importance is a common and intuitive heuristic that has been widely applied in fields such as model pruning and neural architecture search. While the paper successfully applies this to LoRA rank allocation, the theoretical novelty of the strategy itself is relatively limited.\n2. Based on the quantitative results in Table 1 on real-world SR datasets, while AdaptSR demonstrates outstanding performance on distortion metrics like PSNR and SSIM, it is generally outperformed by other GAN- and diffusion-based methods on perceptual metrics such as LPIPS and DISTS. The authors claim \"competitive perceptual quality,\" but the results suggest a clear compromise. For real-world SR tasks, where the pursuit of visual realism is a primary objective, this represents a non-trivial shortcoming.\n3. The experiments are primarily focused on existing real-world SR datasets. Although the degradations in these datasets are complex, they still follow specific distributions. The method's generalization capability to other unseen and more extreme degradations has not been systematically evaluated.\n4. Lack of Sensitivity Analysis for the Mini-batch in Rank Allocation. The layer importance ranking in Algorithm 1, which is central to the rank-aware strategy, is derived from gradients computed on a single mini-batch. In theory, the estimate of this gradient is dependent on the size and composition of the mini-batch. \n5. The rank allocation is determined based on a one-time gradient calculation before training and remains static throughout the process. This static strategy may not be optimal. A potential direction for improvement is to adopt a dynamic allocation strategy, for instance, by periodically re-evaluating layer importance and adjusting the ranks during training, which could lead to further performance gains."}, "questions": {"value": "1. The proposed method seems to be universal and can be applied to many tasks. Why are LoAR and the proposed rank-aware allocation scheme particularly suitable for real-world super-resolution tasks?\n2. The authors chose the gradient norm as the metric for measuring the importance of layers. Why did they choose this indicator instead of other potential indicators (e.g., Fisher information, activation amplitude)?\n3. Does the low-rank approximation of LoAR have limitations when restoring complex, stochastic textures or dealing with complicated degradations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zNfUdZ8Q86", "forum": "Ml2bDkzNeq", "replyto": "Ml2bDkzNeq", "signatures": ["ICLR.cc/2026/Conference/Submission9011/Reviewer_98pz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9011/Reviewer_98pz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558076566, "cdate": 1761558076566, "tmdate": 1762920737550, "mdate": 1762920737550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposed AdaptSR to efficiently adapt bicubic trained super-resolution models to realworld super-resolution tasks. AdaptSR utilizes LoRA (e.g., Conv-LoRA, Linear-LoRA, MSA-LoRA) for finetuning and further advances it by Rank-Aware allocation. Accordingly, AdaptSR shows faster and efficient adaptations, while matching or outperforming GAN-based and Diffusion-based realworld super-resolution baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The overall method is simple and straightforward, yet outperforms other GAN-based and Diffusion-based SR networks.\n- The overall writing is clear and easy to follow.\n- The authors conducted experiments across multiple baselines and provides extensive experimental results (both quantitatively and qualitatively).\n- The authors provide thorough comparison against other LoRA based methods."}, "weaknesses": {"value": "**Weakness1: Limited Contribution**\n\nThe authors propose to adapt LoRA to realworld super resolution tasks, with further improvement based on rank-aware insertion. \n\nHowever, using LoRA for domain adaptation is already a common strategy. While the reviewer agrees with the guideline that not every work requires an entirely new method; these type of works require thorough analysis and strong intuitions on why and how a simple adaptation of  prior methods helps for the specific task. \n\nIn this work, the authors claim that using LoRA outperforms in both performance and training efficiency compared to GAN/Diffusion based methods (which I believe is not a good comparison; see **Weakness2**), however, sufficient explanations or insights for the reason lacks.  \n\nSpecifically, 1) the reason for LoRA outperforming full-fine tuning is missing (please counterargue if I have missed this part) and 2) benefits as simply faster convergence is not sufficient since it is trivial when using LoRA. \n\nSimilarly, Algorithm 1 (it has a typo in line222) is naive (or in positive terms, simple yet effective), but lacks sufficient explanation on why this should outperform other improved-LoRA techniques, specifically in terms of real-world SR tasks. Also, the quantitative comparison against naive-LoRA and other improved-LoRA vs AdapSR seems to have issues (see **Weakness3**).\n\nThe reviewer strongly suggests to provide thorough discussion on 1) super-resolution task specific advantages on adapting LoRA (apart from simply faster and efficient training) and 2) why LoRA outperforms full-fine tuning and 2) why Algorithm 1 outperforms other LoRA variants. I believe that faster convergence and training efficiency by utilizing LoRA cannot be seen as a contribution of this paper without these discussion.\n\n---\n\n**Weakness2. The main quantitative comparisons against baselines is fundamentally wrong.**\n\n**2.1 Fidelity oriented SR vs. Perceptual quality oriented SR**\n\nIn the experimental details, the authors noted that AdaptSR is trained with the L1 loss solely. This indicates that the network primarily aims for fidelity-oriented realworld SR (mainly aiming for the highest PSNR scores). However, all other baselines in Table 1 are perceptual-quality oriented realworld SR method. \n\nDue to the perception-distortion (PD) trade-off, AdaptSR (w/ only L1 loss) outperforming others in terms of PSNR is quite trivial. Meanwhile the perceptual-quality oriented version of AdaptSR can be found in the Appendix Table 10, where only limited test sets are reported (I suggest reporting all test sets, since the appendix does not have any page limits, and only performing evaluation is not costly). A proper comparison would be comparing each fidelity-oriented and perceptual-oriented baseline method (with the official training settings), and comparing it AdaptSR version counterpart. \n\nGiven this misaligned comparison, I have strong concerns about this table being propagated in the field of SR. This is since this table may give wrong intuitions as LoRA significantly outperforming full finetuned versions with large margins (e.g., +3 PSNR); which is wrong. This performance gap is mainly due to the the PD trade-off.\n\n**2.1.1 Comparison in terms of Perceptual SR**\n\nAccordingly, I have compared the perceptual SR version of AdaptSR in Table 10 with perceptual SR baselines in Table 1, where AdaptSR simply isn’t the best model (e.g., SinSR outperforms in both PSNR and DISTS).  Additionally, I highly suggest comparing in the following setting (using official weights for baselines if possible). \n\n- Other GAN-based vs GAN-based + AdaptSR\n- Other Diffusion-based vs Diffusion-based + AdaptSR\n\n**2.1.2 Comparison in terms of Fidelity SR**\n\nWhen comparing the fidelity SR version of AdaptSR, the baselines should be also fidelity oriented SR methods. Accordingly, I highly suggest comparing similarly as the following examples (using official weights for baselines if possible).\n\n- RealESRNet vs RealESRNet + AdaptSR\n- SwinIR (PSNR-oriented Realworld ver.) vs SwinIR + AdaptSR\n- EDSR (Realworld ver.) + AdaptSR\n\nAdditionally, I see several experiments already in Figure 1, but the AdaptSR version does not seem to outperform the full fine-tuned version.\n\n**2.2. Backbone and training configuration of baselines**\n\nSimilarly as discussed above, the main comparison should be performed against the full-fine tuning counterpart (with sufficient training budget and proper training configurations). However, backbones in Table 1 do not properly align with AdaptSR. (e.g., RealESRGAN uses RRDB while AdaptSR seems to use SwinIR). \n\nThe reviewer strongly suggests to counterargue about the concerns above if it is wrong; or to provide sufficient quantitative evaluation under fair settings (e.g., align fidelity/perceptual-oriented, align backbone, use sufficient training budget for full-finetuning).\n\n---\n\n\n**Weakness3 Requires final performance for Table 2 and Table 3.**\n\n**3.1 Comparison against other PEFT methods**\n\nAs the authours have claimed, training AdaptSR should be very efficient and lightweighted. Accordingly, I believe that reporting final scores (or at least for 100K iterations since it already reaches peak performance regarding Figure 6) should be within a plausible computational cost range. \n\nThe reviewer suggests reporting these scores in order to verify if Algorithm 1 is not only effective in the very early training stages (as 10K iter as the authors provided). \n\n**3.2 Comparison against full fine-tuning.**\n\nI could not find the training iterations for the full fine-tuned counterpart (denoted either as FT or +FineTuning) for Table 2 and Table 3. \n\nThe reviewer suggests to specify the configuration, and also report both the fully finetuned version and 10K finetuned version. This is since 10K finetuned version performing worse than LoRA variants (including AdaptSR) is trivial, and it is necessary to compare AdaptSR with the fully finetuned version.\n\n---\n\n**Weakness4 Wrong numbers for quantitative scores, and missing experimental settings.**\n\nMinor typos within the text are fine. However, values for quantitative scores should be carefully double-checked. I found several wrong numbers in the table values, which leads other reported scores less convincing. Examples are as below.\n\n- Params in Table 10 and Table 1 do not match.\n- Scores for RealESRGAN do not match in Table 10 and Table 1.\n- Scores for Baseline in Table 3 do not match with Table 2.\n- Which backbone is used for LDL in Table 1? (RRDB or SwinIR?).\n\n---\n\n**Weakness5. Experimental settings and details**\n\n**5.0 (Minor) Experimental specifications**\n\nComparison against full-finetuned model, under fair configuration is important. However, it is very hard (or missing) to find the experimental specifications for each Tables and Figures. I suggest noting important configurations in the caption.\n\n**5.1 Training budget**\n\nThe full fined-tuned version should use sufficient training budget. However, the overall training budget (e.g., batchsize) is reduced compared to the official training settings. It is quite trivial that LoRA-based methods (as AdaptSR) outperforms full finetuning under limited data and training budget. \n\nSince checkpoints for real-world versions of most baseline methods are officially provided, I recommend comparing with these.\n\n**5.2 Training configuration**\n\nThe learning rate is specified as 1e^-3 for both AdaptSR and full finetuned. According to muP theorems, it is likely that optimal learning rates for smaller networks (especially in terms of channel, as LoRA) is greater compared to larger networks (as full finetuning). The currently specified learning rate is too large for most full finetuning models (e.g., most use values near 1e^-4). I believe that this may be a potential reason for fluctuating scores for full finetuned model in Figure 6.\n\n---\n\nThe reviewer sincerely appreciate the efforts the authors have made. Accordingly, I am looking forward for further discussion and willing to update my scores if my concerns are sufficiently addressed. Please counterargue my claims and provide according experimental results."}, "questions": {"value": "Please see the **Weaknesses**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethic concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KZK8uJyqRX", "forum": "Ml2bDkzNeq", "replyto": "Ml2bDkzNeq", "signatures": ["ICLR.cc/2026/Conference/Submission9011/Reviewer_sD2X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9011/Reviewer_sD2X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723843272, "cdate": 1761723843272, "tmdate": 1762920737037, "mdate": 1762920737037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AdaptSR, which applies LoRA to adapt bicubic-trained SR models for real-world degradations. The key idea is to insert low-rank adapters into CNN and Transformer layers with a rank-aware allocation strategy based on gradient importance. The authors claim significant parameter reduction (up to 92%) and faster training (1-4 GPU hours vs days) while matching or exceeding full fine-tuning performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a genuinely practical problem - adapting bicubic-trained SR models to real-world degradations efficiently. The engineering execution is solid, with experiments spanning multiple architectures (EDSR, SAFMN, SwinIR, DRCT) and showing consistent improvements. The training efficiency gains are impressive and well-documented: reducing adaptation time from days to hours while using only ~8% of parameters is meaningful for practitioners working with limited compute budgets.\n\n2. The experimental scope is reasonably comprehensive, covering different dataset types (RealSR, DRealSR, DSLR) and providing extensive visual comparisons. I appreciate that the authors test their approach across both CNN and Transformer architectures, demonstrating some generality. The DSLR iPhone experiment, while limited, at least attempts to show rapid adaptation to new device-specific degradations.\n\n3.The paper is clearly written overall, with good figure quality and reasonable contextualization of related work. The supplementary material is thorough, providing additional ablations and visual results that support the main claims."}, "weaknesses": {"value": "1. My primary concern is the lack of theoretical depth. The rank-aware allocation strategy uses gradient norm computed on a single mini-batch to determine layer importance, but there's no justification for why this particular metric should be optimal. Have the authors considered that gradient magnitude might be noisy or biased by factors like layer depth, initialization, or batch statistics? For a conference emphasizing principled approaches, I'd expect either theoretical analysis of why gradient-based scoring works or systematic comparison with alternative importance measures (Fisher information, Hessian trace, activation-based metrics, etc.). Algorithm 1 feels like a heuristic that happens to work rather than a principled contribution.\n\n2. The Conv-LoRA decomposition (Equation 2) also lacks motivation. Why use 1×1 followed by k×k specifically? The paper doesn't explore alternative factorizations or explain what makes this choice superior. These architectural decisions seem arbitrary without proper ablation studies showing they matter.\n\n3. Looking at the experimental results more carefully, I'm puzzled by the inconsistency between distortion and perceptual metrics. Table 1 shows AdaptSR achieves the best PSNR/SSIM but noticeably worse LPIPS/DISTS on DIV2K (0.5047 vs most methods around 0.29-0.35). This is a huge gap that the paper doesn't adequately address. For real-world SR applications, perceptual quality often matters more than PSNR, yet the paper frames these results as uniformly superior. The abstract and introduction oversell the results by claiming to \"outperform\" methods when the picture is actually quite mixed depending on which metrics you prioritize.\n\n4. The novelty is limited. Applying LoRA to vision tasks is well-established, and gradient-based importance scoring for resource allocation is standard practice. The paper reads more as a competent application study than a fundamental contribution. What SR-specific insights does this work provide? Why is the bicubic-to-real adaptation problem particularly amenable to low-rank updates? These deeper questions remain unanswered.\n\n5. The comparison with diffusion models feels somewhat unfair. Methods like PASD and SeeSR are designed to leverage generative priors for hallucinating plausible details, which naturally leads to different metric trade-offs. Comparing them primarily on PSNR/SSIM misses their intended use case. The paper should more carefully discuss when AdaptSR's approach (fidelity-focused) is preferable versus when generative approaches (diversity-focused) might be better.\n\n6. Table 2's ablation is concerning for the paper's central claim. The rank-aware allocation achieves 28.47 PSNR while uniform rank-8 gets 28.40 - a difference of only 0.07 dB. Is this margin statistically significant? Does it justify the added complexity of gradient-based scoring? The heatmap in Figure 5 is interesting but doesn't prove that this allocation strategy is superior to simpler alternatives.\n\n7. The generalization experiments are limited. RealSR, DRealSR, and DSLR all involve camera-based degradations with similar characteristics. What about other real-world degradation types: compression artifacts at various quality levels, different noise distributions, atmospheric effects, motion blur? The paper claims broad applicability but only tests a narrow slice of the real-world degradation space."}, "questions": {"value": "1. The gradient-based importance scoring happens once at the start - have you experimented with updating layer importance during training? Intuitively, the most important layers for adaptation might shift as the model learns. Dynamic allocation strategies could potentially improve results.\n\n2. In Table 3, you compare against multiple ARC configurations but always use the same setup for AdaptSR. Why not tune your method more carefully? For instance, have you tried different rank distributions beyond what the gradient-based scoring suggests? It would strengthen your claims to show you've explored the design space thoroughly.\n\n3. Can you provide statistical significance testing for the performance differences? When you report that rank-aware beats uniform rank by 0.07 dB PSNR, is this within the noise of different random seeds and batch sampling?\n\n4. The LPIPS/DISTS degradation on DIV2K in Table 1 really stands out - what's causing this? Is there something about the DIV2K distribution that makes your approach struggle with perceptual quality? Understanding this failure mode would be valuable.\n\n5. You emphasize \"no inference overhead\" repeatedly, but isn't this true of any mergeable PEFT method? What makes this a distinguishing feature of your specific approach versus a general property of the technique you're using?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6JEAJfov6I", "forum": "Ml2bDkzNeq", "replyto": "Ml2bDkzNeq", "signatures": ["ICLR.cc/2026/Conference/Submission9011/Reviewer_74or"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9011/Reviewer_74or"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854641560, "cdate": 1761854641560, "tmdate": 1762920736395, "mdate": 1762920736395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}