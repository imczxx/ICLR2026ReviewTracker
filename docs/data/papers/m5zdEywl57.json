{"id": "m5zdEywl57", "number": 13001, "cdate": 1758212627371, "mdate": 1759897471472, "content": {"title": "Persuasive Prediction via Decision Calibration", "abstract": "Bayesian persuasion, a central model in information design, studies how a sender, who privately observes a state drawn from a prior distribution, strategically sends a signal to influence a receiver's action. A key assumption is that both sender and receiver share the precise knowledge of the prior. Although this prior can be estimated from past data, such assumptions break down in high-dimensional or infinite state spaces, where learning an accurate prior may require a prohibitive amount of data.\nIn this paper, we study a learning-based variant of persuasion, which we term *persuasive prediction*. This setting mirrors Bayesian persuasion with large state spaces, but crucially does not assume a common prior: the sender observes covariates $X$, learns to predict a payoff-relevant outcome $Y$ from past data, and releases a prediction to influence a population of receivers. To model rational receiver behavior without a common prior, we adopt a learnable proxy: *decision calibration*, which requires the prediction to be unbiased conditioned on the receiver's best response to the prediction. This condition guarantees that myopically responding to the prediction yields no swap regret.  Assuming the receivers best respond to decision-calibrated predictors, we design a provably efficient algorithm that learns a decision-calibrated predictor within a randomized predictor class that optimizes the sender's utility. In the commonly studied single-receiver case, our method matches the utility of a Bayesian sender who has full knowledge of the underlying prior distribution. Finally, we extend our algorithmic result to a setting where receivers respond stochastically to predictions and the sender may randomize over an infinite predictor class.", "tldr": "", "keywords": ["Calibration", "Decision Calibration", "Persuasion", "Information Design"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/490324235e44a213f6a2c8a4969edacb9055ae9b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper considers the sequential Information Design model, popularized by the Bayesian Persuasion model, but relaxes the assumption of a common prior. They motivate this from a prediction standpoint. The principal observes a feature vector x, with E[y|x] representing the \"world state\" a la persuasion. It is natural that without a rich amount of data, the sender/receiver would not have a joint prior over this. The principal chooses a classifier h(x), which serves as the \"signal\", and the follower best responds by essentially choosing the optimal action corresponding to the classification. The work focuses on decision-calibrated classifiers, under which the receiver optimal is to follow the recommendation. They then optimize for the optimal decision-calibrated mixture of classifiers (for the sender) by using a lagrangian/minimax approach. They relate this sender optima to that under the standard BP model. \n\nI generally found the work to be quite interesting and well-motivated (once I understood the model). The common prior assumption in BP is generally quite restrictive and the authors make the right trade-offs to address this in a classification inspired setting. The use of decision calibrated classifiers, rigorous results on optimizing this, and their relation to the standard BP benchmark were well done.\n\nThe work could be improved by better explaining the model and clarifying a few things. The following questions came to mind for me:\n- I understand the theoretical results consider an online setting where the principal chooses a decision, receiver best responds and the principal updates. It would help to make this online model explicit, especially when introducing theorem 3.1\n- Can you spell out clearly the benefit of randomizing over the classifiers in this setting? In standard BP, randomizing induces different posteriors and the outcome can be interpreted from a concave closure perspective. Can you give some intuitions on randomization here?\n- I see often the phrase \"following the action recommendation\". This is a bit imprecise here right? The signals sent by the principal is a real value h(x). This gets converted to an action using the br function. If it suffices to just recommend the action (and hide the h(x)) then why not explicitly mention the revelation principal. In general, the lack of revelation principle here is a bit confusing to me. \n\nLastly, and a deeper point, I found the fact that the principal choosing a single classifier to face n receivers quite interesting. The classical analogue of this in BP is public persuasion (On the Tractability of Public Persuasion with No Externalities, Haifeng Xu), which is known to be NP-Hard. Could you comment on the connection here, especially as it relates to section 4.\n\nHappy to engage on this!"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "See above"}, "weaknesses": {"value": "See above"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MelJkfskUi", "forum": "m5zdEywl57", "replyto": "m5zdEywl57", "signatures": ["ICLR.cc/2026/Conference/Submission13001/Reviewer_rkYE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13001/Reviewer_rkYE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761434871963, "cdate": 1761434871963, "tmdate": 1762923747478, "mdate": 1762923747478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces persuasive prediction, a learning-based analogue of Bayesian persuasion that eliminates the need for a common-prior assumption. In this framework, a sender observes covariates, uses them to predict an outcome, and releases the prediction to influence receivers’ actions. The paper proposes decision calibration as the key behavioral proxy, which requires predictions to be unbiased conditional on the receiver’s best response and ensures that receivers incur no swap regret when reacting myopically. The authors present three main theoretical results. First, they prove the equivalence between decision calibration and the absence of swap regret. Second, they show the existence of an optimal decision-calibrated predictor whose utility equals that of a Bayesian sender with full prior knowledge in the single-receiver case. Third, they develop a statistically and computationally efficient algorithm that learns an \n$\\epsilon$-optimal decision-calibrated predictor with polynomial sample complexity independent of the covariate space."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Please find the strengths below:\n1. The paper identifies an important limitation of Bayesian persuasion when the common prior is high-dimensional or difficult to observe. The proposed concept of decision calibration links calibration in machine learning with rational decision-making in game theory.\n2. The authors prove that, in the single-receiver case, the optimal decision-calibrated predictor achieves the same utility as a Bayesian sender with full knowledge of the prior, demonstrating an equivalence to the Bayesian-optimal benchmark.\n3. The proposed no-regret with ERM-oracle algorithm is computationally efficient, with sample complexity independent of the covariate dimension. Moreover, the framework flexibly incorporates stochastic receiver responses and randomized predictor classes."}, "weaknesses": {"value": "Please find the weaknesses below: \n1. The model assumes the existence of a true joint distribution $D(X,Y)$ that can be accurately learned. In practice, the predictor $f$ may be biased or only approximately estimated, making exact decision calibration difficult to satisfy.\n2. The calibration constraint is an infinite family of conditional expectation constraints, which cannot be perfectly verified with finite samples. The paper does not analyze robustness under distribution shift or small-sample regimes.\n3. The theoretical equivalence to Bayesian persuasion relies on perfect calibration and simplified outcome structures. Its validity under nonlinear dependencies between $X$ and $Y$ or in high-dimensional deep learning settings remains untested, and no empirical results are provided."}, "questions": {"value": "The questions are related to the weaknesses: \n1. Can similar optimality or equivalence results be derived for approximately calibrated predictors, where $f$ only satisfies calibration up to a bounded error?\n2. Does a global equilibrium exist under decision calibration, and can convergence or stability be established beyond the no-swap-regret condition?\n3. How robust is the proposed algorithm to distribution shift or model misspecification, and can finite-sample error bounds be extended to practical high-dimensional predictors such as deep networks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "k23obs5A2L", "forum": "m5zdEywl57", "replyto": "m5zdEywl57", "signatures": ["ICLR.cc/2026/Conference/Submission13001/Reviewer_gCE4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13001/Reviewer_gCE4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761591750053, "cdate": 1761591750053, "tmdate": 1762923747124, "mdate": 1762923747124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the framework of persuasive calibration (introduced in [Feng & Tang 2025], which aims to model situations where a sender predicts a label that a group of receivers will then use to choose actions by best-responding. The authors focus on the setting where the sender seeks to incentivize certain actions but faces the challenge of not having a prior over the label y.\nThe authors study what happens when the learner selects from the set of calibrated predictors. In particular, they propose a sample-efficient algorithm to learn a randomized predictor f that maximizes the sender’s utility over the space of calibrated predictors.\nThey further show that, in the case of a single receiver, the utility achieved by their algorithm matches that of a fully informed Bayesian sender. Finally, they extend their framework by providing an approximately optimal algorithm for settings where receivers choose actions according to a softmax distribution rather than by strict best responses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors put effort to motivate the problem and give intuitive explanations for their assumptions\n- The paper includes strong theoretical foundations. I appreciated the effort to gradually introduce the algorithm by guiding the reader to the working principles behind it using background on game theory and optimization. \n- They make the connection of their algorithm perfomance on the 1-receiver case with the Bayesian case\n- They introduce the smoothed version of best-responding so that they can study the challenging setting of infinite hypothesis class."}, "weaknesses": {"value": "- In step 5 of both algorithm 1 and 2 the word \"Auditor\" is used but this is never explained. Is the auditor different from the learner and why?\n- Absence of any empirical evaluation."}, "questions": {"value": "- Can the writers think of a real-world dataset and define an experimental setup to evaluate their technical results. How does one choose tolerance gamma in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vcASdcRq2m", "forum": "m5zdEywl57", "replyto": "m5zdEywl57", "signatures": ["ICLR.cc/2026/Conference/Submission13001/Reviewer_r3eE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13001/Reviewer_r3eE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873120271, "cdate": 1761873120271, "tmdate": 1762923746795, "mdate": 1762923746795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies prior-free persuasion in a batch (iid) setting: a sender observes covariates $X$ and predicts $h(X)$ where $h\\in \\mathcal{H}$ is a class of predictors.   Receivers best respond, and the sender wants to maximize the utility subject to decision calibration: the predictions are calibrated conditional on the receiver's best response $b_i$ under some Lipschitz utility function.  The core algorithm casts the problem as a zero-sum game through Lagrangian if $\\mathcal{H}$ is finite.  For infinite $\\mathcal{H}$, they adopt the quantal response and the smoothed calibration notion."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This is an active line of research relaxing the common prior in the mechanism design problem.  \n- The connection between calibrated prediction and signaling schemes is a nice way to understand persuasion."}, "weaknesses": {"value": "- The algorithm results seem standard.  The reader expects more discussion on the optimality of the algorithm.\n- Given there is a lot of work in the space, the reader feels the related work should be improved.  For instance, how to position the algorithmic contribution in multi-objective learning, e.g., Garg, Sumegha, et al.   In particular, can we view the best response function $b_i$ as some checking function, and decision calibration becomes multi-calibration to those functions?  Additionally, how the sample complexity compares to other work is not discussed.\n- Instead of quantal responses, can the result be generalized to other smooth response functions, e.g., Lipschitz approximate best-response by Foster and Hart?\n\nGarg, Sumegha, et al. \"Oracle efficient online multicalibration and omniprediction.\" _Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_. Society for Industrial and Applied Mathematics, 2024.\n\nFoster, Dean P., and Sergiu Hart. \"Smooth calibration, leaky forecasts, finite recall, and nash dynamics.\" _Games and Economic Behavior_ 109 (2018): 271-293."}, "questions": {"value": "Can we view the problem as multi-calibration problem in Garg et al.?  \n\nIs the algorithm has better (computational/sample) complexity than existing works?\n\nCan the result be generalized to other smooth response functions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dmBIvr7hEC", "forum": "m5zdEywl57", "replyto": "m5zdEywl57", "signatures": ["ICLR.cc/2026/Conference/Submission13001/Reviewer_Argb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13001/Reviewer_Argb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762295092157, "cdate": 1762295092157, "tmdate": 1762923746441, "mdate": 1762923746441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}