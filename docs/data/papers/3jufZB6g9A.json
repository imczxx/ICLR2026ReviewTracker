{"id": "3jufZB6g9A", "number": 1034, "cdate": 1756829426165, "mdate": 1759898231672, "content": {"title": "Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage Laws", "abstract": "Dataset distillation (DD) aims to construct compact synthetic datasets that allow models to achieve comparable performance to full-data training while substantially reducing storage and computation. Despite rapid empirical progress, its theoretical foundations remain limited: existing methods (gradient, distribution, trajectory matching) are built on heterogeneous surrogate objectives and optimization assumptions, which makes it difficult to analyze their common principles or provide general guarantees. Moreover, it is still unclear under what conditions distilled data can retain the effectiveness of full datasets when the training configuration, such as optimizer, architecture, or augmentation, changes. To answer these questions, we propose a unified theoretical framework, termed configuration–dynamics–error analysis, which reformulates major DD approaches under a common generalization-error perspective and provides two main results: (i) a scaling law that provides a single-configuration upper bound, characterizing how the error decreases as the distilled sample size increases and explaining the commonly observed performance saturation effect; and (ii) a coverage law showing that the required distilled sample size scales linearly with configuration diversity, with provably matching upper and lower bounds. In addition, our unified analysis reveals that various matching methods are interchangeable surrogates, reducing the same generalization error, clarifying why they can all achieve dataset distillation and providing guidance on how surrogate choices affect sample efficiency and robustness. Experiments across diverse methods and configurations empirically confirm the derived laws, advancing a theoretical foundation for DD and enabling theory-driven design of compact, configuration-robust dataset distillation.", "tldr": "We establish a unified configuration–dynamics–error framework for dataset distillation, proving scaling and coverage laws that explain IPC saturation and quantify configuration diversity.", "keywords": ["Dataset Distillation", "Generalization theory"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/57f0566df86f5f3be35214a9dee5d76fa31613fe.pdf", "supplementary_material": "/attachment/50dccbb46b91c7254b7353c007054ee6493a4380.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a unified theoretical framework for dataset distillation (DD), a field focused on synthesizing compact datasets that preserve the performance of the original, large-scale data. The authors make significant contributions by proposing a \"configuration-dynamics-error\" framework that unifies the three dominant DD paradigms—Gradient Matching (GM), Distribution Matching (DM), and Trajectory Matching (TM)—and derives two fundamental laws governing the process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper fills a critical gap in the DD literature by providing the first unified theoretical foundation. The coverage law, in particular, is a novel and powerful concept that moves beyond the typical single-configuration analysis and directly addresses the practical requirement of robustness in distilled datasets.\n\n2. The experimental section is well-designed, validating both the scaling and coverage laws across multiple datasets (MNIST, CIFAR-10/100, ImageNette) and several canonical DD methods."}, "weaknesses": {"value": "1. The theoretical analysis relies on several strong assumptions, most notably the Polyak-Łojasiewicz (PL) condition for the inner-loop optimization dynamics.\n\n2. The definition of \"configuration\" is focused on optimization hyperparameters and architecture. It does not explicitly model more fundamental distribution shifts, such as semantic or domain shifts, which are also critical for robustness.\n\n3. While the experiments cover standard benchmarks, validating the coverage law on larger, more complex datasets like full ImageNet or in cross-domain transfer settings would further strengthen the paper's impact."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "G2IQE7ezpA", "forum": "3jufZB6g9A", "replyto": "3jufZB6g9A", "signatures": ["ICLR.cc/2026/Conference/Submission1034/Reviewer_gdRs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1034/Reviewer_gdRs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761373067664, "cdate": 1761373067664, "tmdate": 1762915659292, "mdate": 1762915659292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified configuration–dynamics–error framework that integrates gradient, distribution, and trajectory matching within a generalization-error analysis. It establishes the scaling law and coverage law linking distilled sample size to performance and configuration diversity, theoretically and empirically unifying major dataset distillation methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper presents a unified bi-level generalization error framework that connects gradient, trajectory, and distribution-based DD, providing an important step toward a unified theoretical foundation for DD.\n2.\tThe proposed scaling and coverage laws formalize intuitive empirical observations into mathematically grounded relationships."}, "weaknesses": {"value": "1.\tThe framework relies on PL conditions and Lipschitz continuity. While these assumptions are standard in convergence analysis, they may not strictly hold for modern deep networks with non-smooth activations, normalization layers, and stochastic training components. The practical relevance of the theoretical results could be further clarified by discussing their validity under relaxed or empirically realistic assumptions.\n2.\tThe validation of the proposed laws relies mainly on curve-fitting without statistical significance tests, variance analysis, or sensitivity checks. While the observed trends are consistent with the theory, the slopes vary across datasets and architectures, and further analysis could help clarify these differences and strengthen the empirical support for the proposed laws.\n3.\tWhile the paper introduces the configuration–dynamics–error decomposition, its underlying structure closely follows standard generalization-error decompositions and stability analyses. The framework builds on established theoretical tools and primarily reinterprets them within the context of dataset distillation, rather than introducing fundamentally new analytical techniques or tighter bounds. As such, the contribution may be viewed more as a conceptual consolidation than a substantial theoretical advance.\n4.\tThe coverage diversity $H(A,r)$ is defined abstractly through a covering number on configuration space. While this formulation is theoretically elegant, it may be challenging to compute or approximate in practical training scenarios.\n5.\tThe theoretical insights could be further translated into clear, actionable guidelines (e.g., for selecting IPC or surrogate objectives), which would help enhance the practical relevance of the work."}, "questions": {"value": "1.\tCould the authors clarify the practicality of the PL and Lipschitz assumptions for modern non-smooth architectures, and whether the results hold under weaker conditions?\n2. It would be helpful if the authors could clarify whether any statistical tests or variance analyses were conducted to validate the fitted slopes, and how sensitive the results are to random initialization or dataset variations.\n3. Please clarify the main theoretical novelty of the configuration–dynamics–error decomposition beyond its conceptual unification of existing generalization and stability analyses.\n4. The coverage diversity term $H(A,r)$ is defined via an abstract covering number in configuration space. Is there a practical way to estimate or approximate this quantity in real-world training scenarios?\n5. Could the theoretical insights, such as the scaling and coverage laws, provide clearer practical implications for choosing IPC values, surrogate objectives, or data selection strategies in dataset distillation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Uyv5uIWoc6", "forum": "3jufZB6g9A", "replyto": "3jufZB6g9A", "signatures": ["ICLR.cc/2026/Conference/Submission1034/Reviewer_zpQM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1034/Reviewer_zpQM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815146295, "cdate": 1761815146295, "tmdate": 1762915658838, "mdate": 1762915658838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified theoretical framework to study dataset distillation (DD) from a generalization-error perspective. The authors derive two key theoretical results: (i) a *scaling law* that characterizes how test error decreases with distilled dataset size under a fixed training configuration, and (ii) a *coverage law* that quantifies how the required distilled sample size must scale with the diversity of training configurations (e.g., architectures, optimizers, augmentations) to maintain performance. The framework unifies three major DD paradigms: gradient matching (GM), distribution matching (DM), and trajectory matching (TM), as different surrogates minimizing the same underlying alignment discrepancy. Empirical validation is provided across standard benchmarks (MNIST, CIFAR-10/100, ImageNette) and representative DD methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n1. **Theoretical novelty and unification**: The paper offers the first generalization-error-based–based framework that unifies disparate DD approaches under a common lens. This is a significant conceptual advance over prior paradigm-specific analyses.  \n2. **Clear and impactful theoretical results**: The scaling law explains the widely observed IPC saturation phenomenon, while the coverage law formally defines the “utility boundary” of distilled data across configuration shifts, a practically relevant and previously unaddressed question.  \n3. **Tight bounds with matching upper/lower results**: The coverage law includes both upper and lower bounds that match up to constants, establishing near-optimality of the √H/k rate.  \n4. **Empirical validation aligns with theory**: Experiments across multiple DD methods and datasets consistently show the predicted 1/√k scaling and √H/k coverage behavior, lending strong support to the theoretical claims."}, "weaknesses": {"value": "---\n\n**Weaknesses:**  \n1. **Limited experimental scale**: All experiments are conducted on relatively small-scale vision datasets (MNIST, CIFAR, ImageNette). The absence of evaluation on larger, more realistic benchmarks (e.g., ImageNet-1K or language datasets) raises concerns about the practical relevance and scalability of the derived laws in modern settings.  \n2. **Proxy for configuration diversity**: The paper approximates coverage complexity Hcov(A, r) by log M (M = number of configurations), which may oversimplify the true geometric structure of the configuration space. A more refined empirical estimation of dA (e.g., via gradient-based distances) would strengthen the experimental validation.  \n3. **Assumption dependence**: The theoretical analysis relies on Polyak–Łojasiewicz (PL)-type contraction and Lipschitz assumptions, which may not hold in highly non-convex or adaptive optimization settings (e.g., large-batch AdamW). The robustness of the laws under such conditions remains unverified.  \n4. **Lack of comparison to recent large-scale DD methods**: While the paper includes MGD3 (a diffusion-based method), it omits comparison to state-of-the-art scalable DD approaches like SRe2L or TESLA, which are designed specifically for large datasets and may challenge or refine the proposed utility boundary.\n5. \"Missing some SOTA baselines\": I have listed them below. Please show the experiments about them. \n\n\n\nTowards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching. ICLR 2024\n\nDataset Distillation with Neural Characteristic Function: A Minmax Perspective. CVPR 2025."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nYs6tnDM16", "forum": "3jufZB6g9A", "replyto": "3jufZB6g9A", "signatures": ["ICLR.cc/2026/Conference/Submission1034/Reviewer_jTCw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1034/Reviewer_jTCw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993519719, "cdate": 1761993519719, "tmdate": 1762915658686, "mdate": 1762915658686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified framework, the configuration-dynamics-error framework, to better understand the scaling and generalizability of existing dataset distillation algorithm. This frameworks places existing dataset distillation algorithm such as gradient matching, distribution matching, and trajectory matching into a single framework for analysis. The framework provides a scaling law, which captures the relationship between distilled dataset size and test error. The framework also provides coverage law, which show how distilled sample size should scale with configuration diversity."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. How to better analyze existing dataset distillation algorithms into a unified theoretical framework is a very important problem to solve as many of the existing dataset distillation algorithms lack strong theoretical foundations.\n2. The paper offers a large amount of theoretical derivations (44 out of the 51 pages of the paper)"}, "weaknesses": {"value": "1. The key formulation, the unified form of stability summarized by equation 6, is not well justified. Why does the absolute difference in the expected risk is approximately upper bounded by optimization residual + statistical fluctuations + matching term. The paper to motivate them from stability and information-theoretical approaches to generalization but none of the three cited work are relevant. The notion of mutual information is not mentioned anywhere in the prior text.\n2. The quality and quantity of the experimental justification to the proposed theoretical framework is very weak. Only 1 of the 51 pages are spent on experiments. The paper attempts to justify its scaling laws by fitting linear regression curves on data that is clearly not linear.\n3. The presentation of the paper can considerably be improved. Figure 3, for instance, contains a total 15 plots. Some of the plots are very text that is completely unreadable. The captions also needs to be more descriptive on what the figure is displaying. The x axis labels are also covering the x-ticks which needs to be fixed."}, "questions": {"value": "1. Table 1 suggest robustness of trajectory matching is low compared to distribution matching. What are the implications of this since trajectory matching is among the most popular dataset distillation algorithms that can work reliably across many different contexts. \n2. How does dataset distillation with Backpropogation through time (BPTT) fit in this unified framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WmRxKlL6CJ", "forum": "3jufZB6g9A", "replyto": "3jufZB6g9A", "signatures": ["ICLR.cc/2026/Conference/Submission1034/Reviewer_XLKG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1034/Reviewer_XLKG"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159851772, "cdate": 1762159851772, "tmdate": 1762915658324, "mdate": 1762915658324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}