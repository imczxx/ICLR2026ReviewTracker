{"id": "1khmNRuIf9", "number": 887, "cdate": 1756821929770, "mdate": 1759898237380, "content": {"title": "MASpi: A Unified Environment for Evaluating Prompt Injection Robustness in LLM-Based Multi-Agent Systems", "abstract": "LLM-based Multi-Agent Systems (LLM-MAS) leverage inter-agent collaboration to tackle complex tasks, yet the dense interactions among agents also make them vulnerable to prompt injection attacks. Such attacks often originate from a few compromised agents and rapidly propagate across the system, posing significant security threats. Existing studies mainly focus on a limited set of attack strategies and rely on researcher-specific implementations of LLM-MAS, which makes it difficult to adapt attacks across different systems and hinders comprehensive evaluation. To bridge this gap, we introduce MASpi, a unified environment for evaluating the prompt injection robustness of LLM-MAS. MASpi offers systematic evaluation suites spanning multiple attack surfaces (i.e., external inputs, agent profiles, inter-agent messages) and attack objectives (i.e., instruction hijacking, task disruption, information disclosure). Specifically, MASpi provides interfaces for executing 23 prompt injection attacks tailored to LLM-MAS. Its modular design enables researchers to easily integrate new LLM-MAS approaches and develop novel attack strategies on top of it. Our benchmarking results reveal that increasing the topological complexity of LLM-MAS does not guarantee security. Instead, the risks are distributed across agents, with the most harmful agent varying depending on the specific attack objective. Moreover, defenses designed for single-agent prompt injection do not reliably transfer to LLM-MAS; in fact, narrowly scoped defenses may inadvertently increase vulnerabilities to other types of attacks. MASpi aims to provide a solid foundation for the community to advance deeper exploration of security design principles in LLM-MAS.", "tldr": "", "keywords": ["Large Language Models", "Multi-Agent Systems", "Prompt Injection", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d2a0bcfb8eca9452bb5a94fe211fd9cb930b8bf5.pdf", "supplementary_material": "/attachment/84be7d1cf44c52ef14ae6d722da1bc75f1316c5c.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes MASPI, a unified and extensible environment for evaluating prompt injection robustness in multi-agent systems (MAS). It enables comparable evaluations by providing a modular framework for integrating diverse agents and attacks, a threat taxonomy covering three attack surfaces and three objectives, and a benchmark featuring 23 attacks and 966 cases in math reasoning and code generation. The paper evaluates seven popular systems with three LLMs. The results show that topology does not ensure security, risks are dispersed across agents by objective, and single-agent defenses do not reliably transfer to multi-agent settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Timely Problem Focus: The paper addresses the critical problem of prompt injection by extending it from single-agent systems to the MAS context. It correctly identifies that inter-agent collaboration introduces novel, propagation-based vulnerabilities.\n\n2. Systematic Threat Taxonomy: The work provides a clear taxonomy of MAS vulnerabilities, structured by attack surfaces (inputs, profiles, messages) and objectives (hijacking, disruption, exfiltration). This categorization, illustrated with 23 attacks, aids standardized evaluation."}, "weaknesses": {"value": "Major Weaknesses:\n\n1. Limited Framework Novelty: The core contribution of this work is a unified environment for MAS robustness evaluation. However, the paper fails to articulate what makes this framework fundamentally different from other existing unified MAS frameworks (e.g., MASLab [1]). Specifically, the attack modules are essentially programmatic modifications to an MAS configuration (e.g., changing an agent's profile, intercepting a message, or modifying user input). It is not clear why a new environment is required for this, as these attack simulations could seemingly be implemented as an evaluation harness on top of existing unified MAS frameworks.\n\n2. Contradictory Threat Model: The paper's threat model is contradictory. It claims a \"black-box\" setting but grants attackers powerful, white-box capabilities. The \"Malicious Agent\" and \"Message Poison\" surfaces require privileged access to internal agent profiles and the ability to intercept inter-agent communication, respectively. These assumptions are unrealistic for a black-box attacker.\n\n3. Simple MAS Attack: The paper's conclusions on robustness are based on 23 manually crafted, static prompt templates. The evaluation omits more sophisticated, automated attacks, such as those using gradient-based or adaptive search methods. A system's robustness to these simple attacks does not guarantee its resilience against more advanced variants.\n\nMinor Weaknesses:\n\n1. Narrow Evaluation Scope: The evaluation focuses only on mathematical reasoning and code generation, which limits the generalizability of its conclusions. It is unclear if these findings apply to other common MAS domains, such as creative writing, social simulation, or web navigation, where attack propagation and impact might differ significantly.\n\n2. Defense Analysis: The paper overlooks more robust, MAS-specific defenses, such as topology-aware security policies (e.g., restricting agent capabilities based on role) or anomaly detection based on communication patterns.\n\n[1] Ye R, Huang K, Wu Q, et al. MASLab: A unified and comprehensive codebase for LLM-based multi-agent systems [J]. arXiv preprint arXiv:2505.16988, 2025."}, "questions": {"value": "1. Could you differentiate MASPI from existing unified MAS frameworks?\n\n2. Could you strengthen the threat model?\n\n3. Could you add automated or adaptive attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HJupLk7KgB", "forum": "1khmNRuIf9", "replyto": "1khmNRuIf9", "signatures": ["ICLR.cc/2026/Conference/Submission887/Reviewer_e2tU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission887/Reviewer_e2tU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916780771, "cdate": 1761916780771, "tmdate": 1762915636261, "mdate": 1762915636261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. This paper proposes a unified environment for evaluating the prompt injection robustness of LLM-based multi-agent systems (LLM-MAS).\n2. Benchmarking results reveal that existing LLM-MAS remain highly vulnerable to prompt injection attacks, highlighting the urgent need for stronger defenses in multi-agent collaboration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n\n2. The problem addressed—evaluating the prompt injection robustness of LLM-based multi-agent systems (LLM-MAS)—is important.\n\n3. The proposed benchmark comprehensively covers multiple attack surfaces and objectives, encompassing diverse and realistic threat scenarios.\n\n4. The unified and modular environment allows for reproducible evaluation, easy integration of new attacks or systems, and standardized comparisons across LLM-MAS.\n\n5. The experiments involve seven multi-agent systems and three large models, providing broad evidence that current LLM-MAS remain highly vulnerable, which motivates further research."}, "weaknesses": {"value": "1. The defense analysis (e.g., BERT detector, Delimiter, Sandwich) is relatively shallow and does not propose new mitigation methods.\n\n2. While empirical coverage is strong, the paper does not deeply analyze why certain systems or topologies are more vulnerable.\n\n3. MASPI mainly functions as an evaluation framework and contributes less on the conceptual or algorithmic aspects of enhancing robustness. Moreover, while the benchmark reports BU, ASR, and UA metrics, it does not introduce new or more insightful evaluation metrics."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dZGadVZoDh", "forum": "1khmNRuIf9", "replyto": "1khmNRuIf9", "signatures": ["ICLR.cc/2026/Conference/Submission887/Reviewer_jtxB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission887/Reviewer_jtxB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963351294, "cdate": 1761963351294, "tmdate": 1762915636147, "mdate": 1762915636147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MASPI, a novel and unified benchmarking environment for systematically evaluating the robustness of LLM-based Multi-Agent Systems (LLM-MAS) against prompt injection attacks. The authors address a critical fragmentation in existing research by providing a standardized framework, which includes a 3x3 threat taxonomy (organizing attacks by 3 surfaces and 3 objectives), a modular codebase integrating 7 popular LLM-MAS frameworks, and a large-scale test suite of 966 test cases. Using this benchmark, the authors conduct extensive experiments, revealing that current systems are highly vulnerable (90-100% ASR in some cases), that single-agent defenses are often ineffective or even counter-productive in a multi-agent context, and that vulnerability is dispersed across agents rather than being tied purely to system topology."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper's primary contribution, namely a unified, extensible, and standardized benchmar, is both timely and highly valuable. The authors correctly identify that the field's progress on LLM-MAS security is hampered by ad-hoc implementations and fragmented evaluations. By providing a single environment that allows for \"apples-to-apples\" comparisons of different systems (AutoGen, MetaGPT, etc.) and attacks, MASpi provides a strong foundation for reproducible and cumulative research."}, "weaknesses": {"value": "1. Limited Diversity of Task Domains: The benchmark's current focus is exclusively on mathematical reasoning and code generation (Section 4.1). While these are excellent domains for testing structured, collaborative problem-solving, they are not representative of all common MAS applications. It remains unclear if these findings would generalize to other tasks. \n2. Limited Evaluation of Defenses. The paper stops short of assessing more advanced defenses. Notably, many existing guardrails [1, 2] for LLM agents are themselves implemented as agents; when embedded into an agentic framework they effectively create a multi-agent configuration that fit into the paper's scope. These agent-as-guardrail settings should also be evaluated.\n3. No Stealthiness Constraints on Attacks. The proposed benchmark reports ASR/UA/UDR but does not bound or measure the stealthiness of injected prompts (e.g., no embedding space distance/token-budget limits, similarity checks, or detectability metrics). Because payloads can substantially alter inputs or messages, high ASR may partly reflect “loud” interventions rather than genuinely covert prompt injections. This may lead to cases where the LLMs are essentially following the \"instruction\", as the prompt may has been modified a lot. Adding a perturbation budget and a stealth score (semantic similarity or judge-based detectability) will be much more reasonable.\n\n[1] Chen, Zhaorun, Mintong Kang, and Bo Li. \"Shieldagent: Shielding agents via verifiable safety policy reasoning.\" arXiv preprint arXiv:2503.22738 (2025). \\\n[2] Luo, Weidi, et al. \"Agrail: A lifelong agent guardrail with effective and adaptive safety detection.\" arXiv preprint arXiv:2502.11448 (2025)."}, "questions": {"value": "For rebuttal, please refer to the weaknesses. In addtion, on the \"Sandwich\" Defense Backfiring One of your most interesting findings is in Table 3, where the \"Sandwich\" defense not only failed but actively increased the ASR for Exfiltration attacks. This is a significant result which also need further investigation. Do you have a concrete insight for this mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zDh2MRuUZo", "forum": "1khmNRuIf9", "replyto": "1khmNRuIf9", "signatures": ["ICLR.cc/2026/Conference/Submission887/Reviewer_ikCb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission887/Reviewer_ikCb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969845910, "cdate": 1761969845910, "tmdate": 1762915635955, "mdate": 1762915635955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark MASpi to standardize the measurement of the robustness of LLM-MAS against prompt injection. This paper establishes some initial evaluation protocols and results on existing LLM-MAS and LLMs. The experiment results suggest that current LLM-MAS systems are highly vulnerable to prompt attacks, and the safety does not increase as the complexity of the communication grows."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is novel in that it first tries to establish a method for evaluating the LLM-MAS robustness against prompt injection\n2. The paper presents its results in a logical way"}, "weaknesses": {"value": "1. No confidence interval is reported with the evaluation numbers\n2. The analysis of the experiments is a bit shallow. For example, one can group the compared baselines and analyze which components in that method are more critical against attacks."}, "questions": {"value": "1. Are the tasks meaningfully set up? My concern is that some tasks in prompt injection are not even defensible and can only be addressed by security layers (e.g., proper access authorization) on top of the LLM-MAS system. \n2. All numbers should have their statistical confidence intervals to make the work a serious research paper. If you have the original evaluation numbers, please do include them in the appendix or in the main paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HNBJUyOrro", "forum": "1khmNRuIf9", "replyto": "1khmNRuIf9", "signatures": ["ICLR.cc/2026/Conference/Submission887/Reviewer_9bHV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission887/Reviewer_9bHV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042821761, "cdate": 1762042821761, "tmdate": 1762915635790, "mdate": 1762915635790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}