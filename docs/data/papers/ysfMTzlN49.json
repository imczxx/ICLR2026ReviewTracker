{"id": "ysfMTzlN49", "number": 9792, "cdate": 1758140777567, "mdate": 1759897695381, "content": {"title": "OML: A Primitive for Reconciling Open Access with Owner Control in AI Model Distribution", "abstract": "The current paradigm of AI model distribution presents a fundamental dichotomy: models are either closed and API-gated, sacrificing transparency and local execution, or openly distributed, sacrificing monetization and control. We introduce OML(Open-access, Monetizable, and Loyal AI Model Serving), a primitive that enables a new distribution paradigm where models can be freely distributed for local execution while maintaining cryptographically enforced usage authorization. We are the first to introduce and formalize this problem, introducing rigorous security definitions tailored to the unique challenge of white-box model protection: model extraction resistance and permission forgery resistance. We prove fundamental bounds on the achievability of OML properties and characterize the complete design space of potential constructions, from obfuscation-based approaches to cryptographic solutions. To demonstrate practical feasibility, we present OML 1.0, a novel OML construction leveraging AI-native model fingerprinting coupled with crypto-economic enforcement mechanisms. Through extensive theoretical analysis and empirical evaluation, we establish OML as a foundational primitive necessary for sustainable AI ecosystems. This work opens a new research direction at the intersection of cryptography, machine learning, and mechanism design, with critical implications for the future of AI distribution and governance.", "tldr": "We introduce, rigorously formulate, and depict roadmap to Open-access, Monetizable, and Loyal (OML) primitive: a foundational shift to securely distribute AI models by synthesizing transparency with granular monetization and critical safety controls.", "keywords": ["AI Governance", "Anti-monopolization", "AI Deployment", "Data Poisoning", "Model Fingerprinting", "Trusted Execution Environments", "Obfuscation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f239056dd2ca46061e6b3e0d4347715abacc9d1e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes OML, a new paradigm for AI model distribution that aims to balance open access with owner control. The key idea is to allow models to be distributed freely while enforcing usage permissions cryptographically. The authors formalize OML  by defining rigorous security properties – notably Model-Extraction Resistance and Permission-Forgery Resistance – to capture the challenges of protecting an open model under white-box access. They prove fundamental results that delineate the design space. Importantly, they introduce OML 1.0, a practical instantiation using AI-native model fingerprinting. In this scheme, the owner fine-tunes the model on a set of secret (key, response) pairs so that only authorized queries produce the correct outputs. Usage by clients is tracked via a blockchain-based \"Sentient\" protocol: model users must request authorization (and post collateral), and external provers can query the live model with secret keys to detect unauthorized usage and trigger penalties (collateral slashing). The authors implement OML 1.0 on a 7B language model and show that one can embed on the order of 10^3 fingerprints with negligible accuracy loss, and that many fingerprints survive realistic fine-tuning. Overall, the paper delivers a comprehensive theoretical framework and a working prototype, situating the work at the intersection of machine learning, cryptography, and blockchain. The ambition and scope are impressive: to our knowledge this is the first formal treatment of this open-vs-closed distribution problem, and it lays a strong foundation for future research."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies a timely and important dilemma: how to break the monopoly of API-gated models while still enabling creators to monetize and govern their models. The OML primitive is defined clearly with three core properties (open access, monetizability, loyalty), grounding a broad vision for democratized AI development. This framing is novel and could spur much follow-up work.\n2. The authors provide strong theoretical analysis. They prove that perfect white-box protection is impossible but also show that, under powerful cryptographic assumptions (e.g., indistinguishability obfuscation), OML is achievable, and they derive a learning-theoretic query–security tradeoff. These results rigorously delineate what is and isn’t possible, giving confidence in the soundness of the approach.\n3. By combining ML, cryptography, and blockchain, the paper offers a bold vision for aligning AI incentives. The \"Sentient protocol\" for transparent model governance is especially compelling: it creates economic incentives for contributors and accountability for usage. This high-level perspective underscores the significance and potential impact of the work on the AI ecosystem."}, "weaknesses": {"value": "1. As the authors themselves acknowledge, absolute protection is impossible under white-box access. Consequently, OML 1.0 only provides post-hoc enforcement (\"next-day security\") rather than real-time prevention. Unauthorized usage can occur and be detected only later by probing with secret keys. This economic-deterrence approach is inherently weaker than cryptographic enforcement; a highly-motivated adversary might still abuse a model before penalties apply. The reviewer is left uncertain about scenarios where users might collude or operate outside the blockchain protocol, or simply absorb the collateral cost.\n2. The empirical results, while encouraging, are somewhat narrow. They focus on a single 7B LLM and \"standard language tasks.\" It is unclear how the approach scales to much larger models (e.g. 100B) or to other domains (vision, RL, etc.). Also, there are no comparison baselines (though there are no direct competitors), and it is hard to judge the practical overhead in deployment. For instance, what is the inference latency/throughput penalty of the fingerprinted model? Some quantitative cost analysis would be useful.\n3. Some system aspects are described only at a high level. For example, how exactly the blockchain-based access control is implemented (gas costs, latency, privacy of queries on-chain) is not detailed."}, "questions": {"value": "1. Have you evaluated targeted attempts to remove fingerprints? For example, training with adversarial fine-tuning that specifically tries to eliminate the secret (query, response) mapping, rather than benign fine-tuning. What defense strategies might mitigate this?\n2. The protocol requires users to request permission for each query. How is user privacy preserved on the blockchain? Can queries be batched or encrypted? What are the practical costs (e.g. gas fees) of using the Sentient protocol per query?\n3. The experiments use a 7B model. How do you expect the approach to scale to, say, 70B or 175B models? Would the fingerprint capacity scale roughly linearly with size, or are there diminishing returns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mfJNOv9uWw", "forum": "ysfMTzlN49", "replyto": "ysfMTzlN49", "signatures": ["ICLR.cc/2026/Conference/Submission9792/Reviewer_TUNM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9792/Reviewer_TUNM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875420933, "cdate": 1761875420933, "tmdate": 1762921278987, "mdate": 1762921278987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Both black-box and white-box modes of distributing AI services have endemic limitations, namely: (i) in the black-box setting, the model owner has an outsized amount of control which leads to conflicts of interest concerning privacy of client inputs and fidelity of model outputs, and lack of transparency poses numerous practical problems for model governance; (ii) in the white-box setting, the model owner has extremely limited control -- they cannot effectively monetize model usage, and cannot enforce safety and ethical protections on model behavior. This work attempts to address the limitations of these distribution modes by (a) characterizing and (b) proposing a proof of concept for, a compromise between the two. The main idea is a primitive that enables *local* execution of AI services, while still protecting the interests of model owners."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The goal of the paper is an important one. Black-box and white-box modes of distributing AI services both have critical limitations, which indeed should be addressed. \n- The theorems in section 2.3, while relatively straightforward results, have some originality in their usage towards sketching the design space of this goal."}, "weaknesses": {"value": "- **Core issue: the proposed 'feasibility demonstration' does not address the problems raised in the motivation.** Model extraction resistance (as it would need to be interpreted for OML 1.0 to be a secure OML) does not prevent catastrophic undesirable behavior. For example, in OML 1.0 it appears that the model host can simply leak the weights of the OMLized model to any other party, which can then run any query they want locally without having to worry about an auditing mechanism. In this case, it does not matter that the model host cannot remove the fingerprint or forge signatures -- the presence of the fingerprint and auditing mechanism does not actually prevent the downsides of the white-box distribution mode. **Thus while the goals of the paper are laudable, it remains totally unclear that there exists a feasible solution to the problem the authors characterize.** \n\n- **The 'design space characterization' over-claims novelty.** The paper is rather self-congratulatory in how it characterizes the issues with applying existing work in computer security to OMLization, despite the fact that the problems encountered here (unreliability of software obfuscation, performance limitations and side channels in TEEs, high computational overhead in FHE) are all well-known problems in computer security. \n\n- **OML 1.0 is described very vaguely in the main text.** The feasibility demonstration is much more important to whether this work is viable than the design space characterization. Section 3.1 should be relegated to a paragraph with links to the appendix, and Section 3.2 should be much more thoroughly explained in the main text. Algorithm 5 is way too vague, it does not provide the reader enough information to understand the approach.\n\n- **OML 1.0 does not conform to 'open-access' criterion in introduction.** The users whose queries are being answered in Figure 7 do not obtain \"local execution, analogous to compiled binaries.\" From the perspective of the user, there is essentially no difference between OML 1.0 and a black-box AI service. The party that *does* get local execution, the model host, can run as many queries as they want locally without the proposed auditing mechanism detecting it, so for this party the 'monetizable' criterion also fails. \n\n\nTo summarize and return to the core issue: **the design criteria of OML are applied to the 'feasibility demonstration' much more weakly than they are characterized in the introduction and problem formulation.** The authors might argue that establishing OML as a primitive is their primary contribution, and thus the weaknesses of the feasibility demonstration are auxiliary to their work. But given the problems with the feasibility demonstration, that argument breaks down: the lack of a feasible way to address these computer security problems in the context of AI service distribution is precisely the *reason* that previous work has not formulated such a primitive. So it really subtracts from the novelty and impact when this context is understood."}, "questions": {"value": "For OML 1.0, what stops the model host from simply leaking the model parameters to another party? In this case, couldn't the party with the leaked model answer arbitrary queries without having to verify the signatures? Am I misunderstanding something about OML 1.0 which prevents this attack?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hWbTjVebBe", "forum": "ysfMTzlN49", "replyto": "ysfMTzlN49", "signatures": ["ICLR.cc/2026/Conference/Submission9792/Reviewer_ydmk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9792/Reviewer_ydmk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944096982, "cdate": 1761944096982, "tmdate": 1762921278494, "mdate": 1762921278494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tries to solve this tension: let people download and run a model locally (open access) but still let the owner control and charge for each use. They call the target trio OML. It outline sseveral ways to build this (secure hardware/TEEs, pure crypto like FHE, hybrids, etc.). \n\nBut the only thing the paper could run today is OML 1.0, which is post-hoc: secretly “fingerprint” the model with hidden Q→A pairs, issue per-input permissions, send periodic secret probes, and punish violators (e.g., slash a deposit) if answers don’t match. Detection is modeled with a simple probability. The paper also states hard limits and practical blockers: with unlimited authorized queries, perfect protection is impossible; security depends on strictly limiting issued answers. And the pre-hoc paths aren’t ready at LLM scale (no commercial GPU TEEs yet, and FHE adds roughly 10^3 to 10^5  * runtime overhead)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper defines what counts as Open-access, Monetizable, and crucially Loyal; “Loyal” is set as a pre-hoc target (full-quality outputs only after a cryptographically bound permission check), while the current instantiation is explicitly scoped as post-hoc. It also provides a concrete, operable accountability workflow (OML 1.0: hidden fingerprints, permission issuance, periodic probes, slashing) together with a closed-form detection probability \\Pr[\\text{caught}]=1-(1-\\alpha)^n, making the audit lever measurable. \n\nThe paper is transparent about near-term limits for pre-hoc routes (no commercial GPU TEEs; FHE overheads), preventing over-interpretation about immediate LLM-scale deployability. Finally, it sets theoretical guardrails (e.g., impossibility under unbounded authorized queries), which bound expectations and clarify the assumptions any practical OML system must satisfy."}, "weaknesses": {"value": "a) The paper defines *Loyal* as producing high-utility outputs only when valid, cryptographically-bound permissions are presented (pre-hoc verification). Yet Table 2 labels OML 1.0 as “Post-hoc,” relying on economic deterrence and detection rather than pre-execution denial of utility. This contradicts the claimed realisation of the OML primitive: a post-hoc mechanism cannot satisfy the pre-hoc loyalty property as defined. For this, I am expecting either (a) upgrade OML 1.0 with a concrete pre-hoc verifier entanglement that gates the high-utility pathway at inference (consistent with Algorithm 1’s intent to entangle authorisation in “critical paths”) and prove fidelity/robustness bounds under that construction, or (b) explicitly re-scope OML 1.0 as Monetizable-only and remove any claim that it currently satisfies Loyal. Cite an evaluation that shows d(M_oml(x,p_valid), M(x)) ≤ \\epsilon_utility and d(M_oml(x,p_invalid), M(x)) > \\epsilon_robust under pre-hoc enforcement, as required by the formal promises of an ideal OML.\n\n--- \n\nb) The paper proves that preventing model replication is impossible under unbounded authorised queries, and replaces this with a bounded-query setting; extraction resistance then scales with the number of authorised answers an adversary can obtain (fingerprint dimension/sample complexity). The security claim becomes a rate-limit argument, not a model-intrinsic hardness result. Without a concrete bound linking authorised-answer budget N to an explicit failure probability \\epsilon_ME(t,N) (in the sense of Requirement 2.1) under realistic attacker strategies, the result does not guarantee protection once N grows (slow leakage, collusion). The author(s) should at least provide a finite-sample lower bound on N (or query cost) required for \\epsilon_ME ≤ \\delta against a specified adversary class, or incorporate an authorisation-coupled perturbation that preserves utility on authorised inputs while provably destroys identifiability on unauthorised replay/mixture distributions within the stated metric d(·,·). State the concrete adversary and show how the bound composes over multiple attackers.\n\n---\n\nc) The paper claims an economic Nash equilibrium where rational users buy authorisation rather than circumventing controls, but gives no game-theoretic model (players, utilities, monitoring, penalties, detection function) to prove the existence or stability of such an equilibrium. The same monetizability text appears again in Sec. 1.2 without a formalisation.  Without a specified game (including detection probabilities and expected penalties), “users pay rather than cheat” is an unsupported assumption, not a result.\n\n---\n\nd) The paper argues TEEs are “production-ready” but then notes current support is CPU-only, with no GPU-TEE availability; it also states FHE adds 10^3–10^5 * overhead, making full-scale deployment infeasible today. The “practical pathway” depends on components that are either not deployable for modern LLM inference (GPU-TEE) or prohibitively slow at the stated security level (FHE). This undermines the paper’s feasibility narrative absent measured end-to-end overhead \\epsilon_overhead on realistic models/hardware, as required by the OML promises  For this, I expect at least a measured end-to-end latency/throughput overheads (or hard upper bounds) for the proposed OML 1.0 pipeline on concrete models and accelerators; if relying on TEE/FHE in future variants, constrain claims to a clearly labeled roadmap or include scaling experiments that demonstrate target-task feasibility within the stated \\epsilon_overhead budget."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "g13VfkOk9z", "forum": "ysfMTzlN49", "replyto": "ysfMTzlN49", "signatures": ["ICLR.cc/2026/Conference/Submission9792/Reviewer_k8UG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9792/Reviewer_k8UG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972517325, "cdate": 1761972517325, "tmdate": 1762921277937, "mdate": 1762921277937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "The Value of Foundational Work, Our Stance, and Proposed Revisions to our Manuscript"}, "comment": {"value": "We sincerely thank all three reviewers, as well as the AC, SAC, and PC, for investing significant time and thought into our submission. We deeply appreciate the careful consideration given to our work, particularly as it addresses a fundamental, long-horizon problem rather than a narrowly scoped empirical improvement.\n\n**Our Core Contribution: Establishing OML as a Research Primitive**\n\nAll reviewers recognize the genuine tension between open weight models and owner control/monetization. Our primary contribution is formalizing this challenge as the OML (Open access, Monetizable, Loyal) primitive: transforming an informal industry concern into a rigorous research problem with clear mathematical foundations.\n\nWe do not claim to have solved OML. Instead, we:\n\n(1) Define the primitive with mathematical precision;\n\n(2) Prove fundamental impossibility results under unrestricted white box access;\n\n(3) Demonstrate feasibility at two extremes (perfect security with overhead via TEE/FHE, perfect efficiency with weaker guarantees via OML 1.0) ;\n\n(4) Map the vast unexplored space between these extremes;\n\nThis follows the tradition of papers establishing primitives like MPC (multi-party computation) or FHE (fully homomorphic encryption). The value lies in problem formulation and theoretical foundations, not immediate practical solutions.\n\n**Why Problem Formulation Matters**\n\nWe believe that identifying and formalizing important problems represents fundamental scientific contribution. The absence of prior OML formulation reflects that this challenge has only recently become critical as models scale. By establishing rigorous foundations today, we enable systematic progress rather than ad hoc attempts.\n\nOur framework bridges previously disparate areas, including learning theory, cryptography, model fingerprinting, and game theory, creating opportunities for cross pollination of techniques. This synthesis itself represents contribution beyond the individual components.\n\n**Our Stance**\n\nWe maintain that establishing important research directions through rigorous problem formulation is as valuable as achieving marginal improvements on existing benchmarks. The field benefits from both incremental progress and identification of new frontiers. Our work deliberately chooses the latter: not because it is easier (convincing a community to study new problems is harder than optimizing known metrics), but because we believe it addresses a critical emerging challenge.\n\n**Addressing Reviewer Concerns**\n\nOn Feasibility: Reviewers correctly note that perfect OML appears extraordinarily difficult. We agree—this difficulty validates its importance as a research direction. Our impossibility results and extreme point constructions (OML 1.0 and TEE/FHE approaches) bound the problem space and provide concrete starting points for exploration.\n\nOn OML 1.0 Limitations: We acknowledge that OML 1.0 cannot prevent private model theft. It targets public unauthorized deployment through economic incentives and detection. Most end users cannot self host large models, making detection of public misuse meaningful protection against commercialization of stolen models. We will clarify this scope explicitly in our revision.\n\nOn Theoretical Gaps: We acknowledge that establishing tight bounds on model extraction under limited access remains an open challenge. Existing learning theory addresses provable learning, while we need frameworks for provable inability to learn. We have explored SQ lower bounds extensively but recognize they do not fully capture our needs. We view identifying this theoretical gap as contribution that invites further theoretical development.\n\n**Revisions We Will Make**\n\nBased on valuable reviewer feedback, in the revision, we will:\n\n(1) Emphasize problem formulation as the primary contribution in the introduction;\n\n(2) Clarify OML 1.0's scope (post hoc enforcement for public deployment, not private use prevention);\n\n(3) Expand protocol descriptions in main text despite space constraints;\n\n(4) Add explicit discussion of theoretical gaps and open problems;\n\n(5) Include game theoretic formalization in appendices;\n\n**Our Final Thoughts**\n\nWe appreciate the reviewers' engagement and look forward to continued discussion. Whether through acceptance, future workshops, or informal exchanges, we hope this work contributes to addressing one of the most pressing challenges in modern AI deployment. We remain open to all perspectives and eager to learn from the community's insights."}}, "id": "73sEk4rpZG", "forum": "ysfMTzlN49", "replyto": "ysfMTzlN49", "signatures": ["ICLR.cc/2026/Conference/Submission9792/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9792/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission9792/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763193533564, "cdate": 1763193533564, "tmdate": 1763193533564, "mdate": 1763193533564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "An Open Invitation for Discussion"}, "comment": {"value": "We view this work as beginning a conversation, not ending one with definitive answers. The OML challenge spans multiple disciplines, including learning theory, cryptography, systems, and model fingerprinting, and we recognize that our initial formulation represents just one perspective. The OML challenge requires diverse expertise: no single group has all necessary skills. We welcome alternative formalizations, tighter bounds, better constructions, and completely different approaches.\n\nWe welcome: \n\n(1) Alternative formalizations that capture different aspects of the problem;\n\n(2) Challenges to our assumptions and theoretical bounds;\n\n(3) Completely different approaches we have not considered;\n\n(4) Identification of gaps or weaknesses in our framework;\n\n(5) Extensions and refinements from any research community;\n\nThe measure of success for foundational work is not being right about everything, but catalyzing important research. If our formulation is eventually superseded by better frameworks, or if solutions emerge through approaches we never imagined, we would consider that success. Every perspective enriches our collective understanding. We see critique not as opposition but as collaboration toward solving a shared challenge.\n\nWe sincerely look forward to more discussion around the OML primitive, and would like to, once again, extend our deepest gratitude to all the reviewers for engaging in the fruitful discussion with us."}}, "id": "O1oLWJMCTr", "forum": "ysfMTzlN49", "replyto": "ysfMTzlN49", "signatures": ["ICLR.cc/2026/Conference/Submission9792/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9792/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission9792/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763193641552, "cdate": 1763193641552, "tmdate": 1763193641552, "mdate": 1763193641552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}