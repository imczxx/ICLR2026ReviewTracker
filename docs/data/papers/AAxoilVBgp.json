{"id": "AAxoilVBgp", "number": 18971, "cdate": 1758292466549, "mdate": 1759897069896, "content": {"title": "Graph Neural Dynamics via Learned Energy and Tangential Flows", "abstract": "We introduce TANGO -  a dynamical systems inspired framework for graph representation learning that governs node feature evolution through a learned energy landscape and its associated descent dynamics. At the core of our approach is a learnable Lyapunov function over node embeddings, whose gradient defines an energy-reducing direction that guarantees convergence and stability. To enhance flexibility while preserving the benefits of energy-based dynamics, we incorporate a novel tangential component, learned via message passing, that evolves features while maintaining the energy value.\nThis decomposition into orthogonal flows of energy gradient descent and tangential evolution yields a flexible form of graph dynamics, and enables effective signal propagation even in flat or ill-conditioned energy regions, that often appear in graph learning. Our method mitigates oversquashing and is compatible with different graph neural network backbones. Empirically, TANGO achieves strong performance across a diverse set of node and graph classification and regression benchmarks, demonstrating the effectiveness of jointly learned energy functions and tangential flows for graph neural networks.", "tldr": "TANGO learns a Lyapunov energy over node features and updates via two orthogonal flows: energy descent for stability and a learned tangential, energy-preserving flow, improving propagation, reducing oversquashing, and boosting GNNs.", "keywords": ["Graph Neural Networks", "Graph Neural Dynamics"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/df80344ed225fac9130f02c42e9fc8d27ebf1963.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the use of Lyapunov stability concept from control theory to develop a GNN model with desirable properties."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a GNN model based on control theory concepts. It is principled with theoretical support for Lyapunov stability."}, "weaknesses": {"value": "1. The literature survey seems to be focused on GNN works but ignore the wider neural network literature. The concept of Lyapunov stability has been utilized in general neural networks in the literature. It is unclear what additional novelty this work brings, except to apply to GNNs specifically.\n\n1. While having Lyapunov stability is a desirable property, the work does not go one step deeper to discuss why this is a good thing to have. In particular, there are no empirical experiments on adversariabl robustness. \n\n1. The claim that the proposed model can \"actively mitigate oversquashing effects\" is not theoretically supported (see questions below). The authors should not over claim the contributions. The wording in the abstract and introduction needs to be revised to reflect this."}, "questions": {"value": "1. The proposed model seems to be similar or a special case of [R1], which does not focus solely on GNNs. The main difference seems to be that the form of the neural function (RHS of (2)) is fixed in this work to guarantee Lyapunov stability instead of being a general network that is learned to achieve that. The authors need to clarify the novelty, differences, and improvements made over the literature.\n\n    [R1] Stable neural ODE with Lyapunov-stable equilibrium points for defending against adversarial attacks, NeurIPS 2021\n\n1. The model performance compared to several of the baselines is marginal. It is unclear what advantages the Lyapunov stability confer on the model performance. [R1] indicates that Lyapunov stability can lead to adversarial robustness, but this paper does not mention that or perform any robustness studies.\n\n1. Prop. 4 states that \"it is possible to learn\" a desired tangent direction, but this does not mean that the proposed model (using the typical training procedure) actually achieves this! It is unclear under what conditions this learning will happen. The oversquashing claim is thus only supported via empirical observations, not theoretical guarantees. The claim wording needs to be adjusted carefully to reflect this.\n\n1. The datasets used for homophily case are limited. Although the authors are using benchmark datasets recommended by Dwivedi et al. (2023), it is noted that MNIST and CIFAR10 are more suited for computer vision tasks while the experiments do not include graph datasets with diverse underlying geometric properties.\n\n1. There are no ablation studies.\n\n1. There are no sensitivity studies on how the choice of the Lyapunov function $V_\\mathcal{G}$ impacts the model performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MRcB4GJbQC", "forum": "AAxoilVBgp", "replyto": "AAxoilVBgp", "signatures": ["ICLR.cc/2026/Conference/Submission18971/Reviewer_sudZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18971/Reviewer_sudZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760514648724, "cdate": 1760514648724, "tmdate": 1762931022744, "mdate": 1762931022744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for graph representation learning that combines a learned energy function with a tangential flow component to govern node feature evolution. The method is grounded in Lyapunov stability theory and offers both theoretical guarantees and empirical improvements over existing GNN baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written, theoretically rigorous, and experimentally validated across a range of graph learning benchmarks. The use of Lyapunov stability theory and energy-based dynamics provides a principled and interpretable framework."}, "weaknesses": {"value": "Theoretical Concerns:\n1. [Forward Euler discretization] The continuous-time dynamics are discretized using the forward Euler method. However, this discretization typically cannot preserve the energy property guaranteed in the continuous setting. The authors should provide more evidence that the discrete version still satisfies the Lyapunov stability condition.\n2. [Energy function design] The energy function is constructed as a non-negative scalar via a sum of squares. Still, there is no guarantee that its minima correspond to the optimal solutions of the downstream tasks. \n\nExperimental Concerns:\n\n3. [Stability verification] The paper theoretically claims stability via Lyapunov analysis, but does not validate this empirically under adversarial attacks or noisy inputs. It is recommended to compare Tango with baselines (e.g., GCN, GAT) under adversarial settings or input perturbations to assess whether the model’s stability holds in practice.\n4. [Energy landscape analysis] The energy landscape is essential to the proposed method, but the paper lacks visualization or qualitative analysis of the learned energy function.\n5. [Comparison with other energy-based GNNs] The paper lacks the discussion and comparison with the existing energy-based GNNs, such as [1].\n\nMinor Issues: \n6. The content and caption of Figure 2 might not provide enough insights regarding the propagation behaviors. \n\n\n[1] Zhao et al., Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach, NeurIPS 2023."}, "questions": {"value": "See the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RqGuEIbiUA", "forum": "AAxoilVBgp", "replyto": "AAxoilVBgp", "signatures": ["ICLR.cc/2026/Conference/Submission18971/Reviewer_h5ap"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18971/Reviewer_h5ap"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806783480, "cdate": 1761806783480, "tmdate": 1762931022218, "mdate": 1762931022218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TANGO, a framework that decomposes GNN feature evolution into two orthogonal components: (1) energy gradient descent that minimizes a learned task-specific energy function, and (2) tangential flows that evolve features while preserving energy. The authors claim this addresses oversquashing and provides stable dynamics with theoretical guarantees."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Conceptual Innovation\n  - Novel decomposition: The orthogonal decomposition into energy descent + tangential flows is mathematically elegant and provides a principled way to combine stability (energy minimization) with flexibility (tangential evolution)\n  - Task-driven energy learning: Unlike prior work using fixed energy functions (e.g., Dirichlet energy), learning task-specific energy functions is a meaningful advance\n  - Theoretical grounding: Connection to Lyapunov stability theory provides formal guarantees for convergence and stability\n\n2. Mathematical Rigor\n  - Formal propositions: Four theoretical results with proofs covering energy dissipation, flat landscape evolution, and potential for quadratic convergence\n  - Proper mathematical framework: Clear continuous-time formulation with principled Euler discretization\n  - Orthogonality guarantee: The projection method mathematically ensures ⟨T_VG, ∇_HV_G⟩ = 0\n\n3. Experimental Breadth\n  - Diverse benchmarks: Covers synthetic (graph property prediction), molecular (LRGB), standard GNN benchmarks, and heterophilic node classification\n  - Multiple backbone compatibility: Demonstrates the approach works with different GNN architectures (GatedGCN, GPS)\n  - Consistent improvements: Shows gains across most tested scenarios, suggesting general applicability\n\n4. Implementation Completeness\n  - Detailed algorithmic description: Clear specification of energy function computation, tangential projection, and coefficient learning\n  - Hyperparameter documentation: Comprehensive tables of search spaces and experimental settings"}, "weaknesses": {"value": "1. Theoretical Gaps\n  - Gap between theory and claims: While they cite oversquashing theory appropriately, the connection to their specific solution needs stronger theoretical foundation\n  - Proposition 4 limitations: Should acknowledge this shows theoretical possibility rather than practical guarantee\n  - Missing empirical analysis: No investigation of whether learned tangential flows actually approximate Newton directions in practice\n\n2. Experimental Limitations\n  - Small improvements: 1-3% gains often within potential noise, especially given missing variance analysis\n  - Missing key comparisons: Lacks comparison with recent energy-based GNNs (BLEND variants, PDE-GCN extensions)\n  - No runtime complexity: Missing analysis of O(L·L_gnn·(n+m)·d) complexity impact\n  - Overhead quantification: Tables show 40-60% runtime increase but no analysis of performance/cost trade-offs\n  - Memory requirements: No discussion of additional memory for dual networks and gradient computation\n\n3. Methodological Concerns\n  - Many parameters: Requires tuning α, β, ε, L, L_gnn, d - makes method potentially brittle\n  - No principled selection: Lack of guidelines for choosing energy function architecture or step sizes\n  - Architecture dependence: Performance may be sensitive to specific ENERGYGNN/TANGENTGNN designs\n  - Dual network requirement: Need separate networks for energy and tangential components increases complexity\n  - Automatic differentiation dependency: Requires computing gradients of energy function w.r.t. input features\n  - Projection stability: Numerical stability of orthogonal projection not analyzed\n\n4. Questionable Necessity\n  - Building on existing work: Core dynamical systems approach well-established (GRAND, GraphCON, A-DGN)\n  - Main novelty limited: Addition of tangential component, while interesting, may not justify complexity\n  - Simpler alternatives unexplored: No comparison with residual connections, attention mechanisms, or skip connections that might achieve similar benefits\n  - When to use unclear: No guidance on when TANGO is preferable to simpler methods\n  - Modest gains: Improvements don't clearly justify the added theoretical and computational complexity\n  - Scalability questions: Dual networks + projection may not scale to very large graphs\n\n5. Experimental Design Issues\n  - Synthetic bias: Heavy reliance on synthetic tasks (graph property prediction) that may not reflect real-world challenges\n  - Limited diversity: Missing important domains like social networks, biological graphs, knowledge graphs\n  - Size constraints: Most experiments on relatively small graphs (≤50K nodes)\n  - Limited ablations: Tables 10-11 provide some ablation but miss key components like energy function architecture, projection variants\n  - No analysis of learned components: Missing visualization or analysis of what the energy and tangential networks actually learn\n  - Hyperparameter sensitivity: No systematic study of sensitivity to α, β, ε values"}, "questions": {"value": "Q1: Newton Direction Approximation in Practice\nCan you provide empirical evidence that the learned tangential flows actually approximate Newton directions? For example, measuring the angle between your update direction D and the true Newton direction N = (∇²V_G)^(-1)∇V_G on some tasks?\n\nQ2: Energy Landscape Analysis\nWhat do the learned energy functions actually look like? Can you visualize energy landscapes for simple cases or analyze whether the learned V_G corresponds to meaningful graph properties (e.g., connectivity, clustering)?\n\nQ3: Hyperparameter Sensitivity\nHow sensitive is the method to hyperparameters α, β, ε? Can you provide ablation studies showing performance as these vary? Are there principled ways to select these values?\n\nQ4: Comparison with Simpler Alternatives\nHow does TANGO compare to simpler approaches that might achieve similar benefits, such as:\n  - Residual connections with learned gating\n  - Multi-scale message passing\n  - Attention mechanisms for long-range dependencies\n\nQ5: Alternative Architectures\nHow does performance change with different choices for ENERGYGNN and TANGENTGNN architectures? Must they be the same depth/type?\n\nQ6: Scalability\nHow does the method scale to very large graphs (>100K nodes)? Have you tested on larger real-world networks?\n\nQ7: Broader Baseline Comparison\nCan you compare against recent energy-based GNN methods (e.g., BLEND variants, recent PDE-GCN extensions) and second-order optimization approaches for GNNs?\n\nQ8: Training Dynamics\nHow do the energy and tangential components evolve during training? Do you observe the energy function actually being minimized, and how does the tangential flow contribution change?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XmJocGamn7", "forum": "AAxoilVBgp", "replyto": "AAxoilVBgp", "signatures": ["ICLR.cc/2026/Conference/Submission18971/Reviewer_LWCT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18971/Reviewer_LWCT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832943573, "cdate": 1761832943573, "tmdate": 1762931021500, "mdate": 1762931021500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TANGO, a GNN framework that learns (i) a task-driven energy (Lyapunov) function over node embeddings and (ii) two coupled dynamics per layer: a gradient-descent component that strictly dissipates the learned energy and a tangential component that is orthogonal to the energy gradient. The authors give Lyapunov-style stability arguments, show that the energy is non-increasing, and report strong results on long-range and standard graph benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clear dynamical decomposition. The “descent + tangential” split is well-motivated: it guarantees energy non-increase while adding expressive movement along level sets.\n- Stability & optimization insights. Proposition 1 (dissipation) and the “flat-landscape evolution” (Prop. 2) make the benefits precise; the argument that the tangential component can realize a Newton-like direction (Prop. 4) is appealing for ill-conditioned, oversquashed regimes."}, "weaknesses": {"value": "- Forward-Euler stability. The theory is continuous-time; the model uses explicit Euler with step \\epsilon. Stability can be fragile for stiff/ill-conditioned flows. The paper bounds \\alpha(G) but does not analyze discretization stability or step-size sensitivity. Ablation study on \\epsilon will help.\n- The paper claims the proposed method can mitigate oversquashing, yet some recent anti-oversquashing competitors (e.g., curvature/rewiring-based) are not directly compared in real-world data.\n- Labeling Proposition 4 as a ‘theoretical guarantee’ (in contribution) for oversquashing mitigation is misleading: the result is existential. It shows that such a case can occur, not that TANGO will mitigate oversquashing in general\n- TANGO updates the embeddings for a fixed number of iterations, so it is unclear whether a minimum of the energy is actually attained. Showing the norm of energy gradient would help."}, "questions": {"value": "- TANGO updates embeddings layer by layer. Does TANGO suffer from vanishing gradient (due to stacking layers)? If not, can you explain why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N15SIiG0HO", "forum": "AAxoilVBgp", "replyto": "AAxoilVBgp", "signatures": ["ICLR.cc/2026/Conference/Submission18971/Reviewer_YfoY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18971/Reviewer_YfoY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971528627, "cdate": 1761971528627, "tmdate": 1762931020991, "mdate": 1762931020991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}