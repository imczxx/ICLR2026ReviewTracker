{"id": "SD8Z231C45", "number": 6741, "cdate": 1757994164380, "mdate": 1763651504016, "content": {"title": "DuPO: Enabling Reliable Self-Verification via Dual Preference Optimization", "abstract": "We present DuPO, a dual learning-based preference optimization framework that generates annotation-free feedback via the generalized duality. DuPO addresses two key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)’s reliance on costly labels and applicability restricted to verifiable tasks, and traditional dual learning’s restriction to strictly dual task pairs (e.g., translation and back-translation). Specifically, DuPO decomposes a primal task’s input into known and unknown components, then constructs its dual task to reconstruct the unknown part using the primal output and known information (e.g., reversing math solutions to recover hidden variables), broadening applicability to non-invertible tasks. The quality of this reconstruction serves as a self-supervised reward to optimize the primal task, synergizing with LLMs’ ability to instantiate both tasks via a single model. Empirically, DuPO achieves substantial gains across diverse tasks: it enhances the average translation quality by 2.1 COMET over 756 directions, boosts the mathematical reasoning accuracy by an average of 6.2 points on three challenge benchmarks, and enhances performance by 9.3 points as an inference-time reranker (trading computation for accuracy). These results position DuPO as a scalable, general, and annotation-free paradigm for LLM optimization.", "tldr": "", "keywords": ["Self-Verification", "Dual Learning", "Preference Optimization", "Large Language Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bcbe513d6fe2913138686efa0a9e9d1946cc2fad.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a self-verification reward idea for reinforcement learning in LLMs. The general setting is given a prompt $x$, for the LLM to provide an answer $y$, as $\\pi(y|x)$. The core idea of dual learning is to use the prediction of the prompt from the answer as a dual self-supervised training signal. The authors observe that it is not sufficient or possible for many tasks, identifying non-unique reconstruction or failure to reconstruct as core problems. To tackle this, they propose to decompose the prompt $x$ into  a known and an unknown parts, $x = (x_k, x_u)$, to use a model to predict the unknown part from both the answer and the known part, $\\hat{x}_u = \\mathcal{T}(y,x_k)$, and to use this to define a (self-verification) reward signal $r(x,y) \\propto \\exp(-\\lambda d(x_u, \\hat{x}_u))$, with $d$ being a task-dependent distance function. This reward is then optimized using RL. The proposed approach is then quite thoroughly experimented."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is very well written and structured, a pleasure to read\n* The idea of decomposing the prompt for extending applicability of dual learning, and using it as self-verified reward signal is simple yet elegant and very interesting \n* The experimental section is thorough and the results seem to be quite strong"}, "weaknesses": {"value": "* The approach is presented as being very general (with mentions to code generation, dialogue systems, open-ended and creative domains), while it is strongly dependent on the prompt decomposition which has to be defined manually for every task. Math reasoning and translation are the considered tasks, and only math reasoning calls for such a decomposition. This is fine, but this is a limitation that should be acknowledged and discussed. \n* If the paper does a very good job at providing the high-level intuition, it somehow fails to provide a precise enough description of the proposed approach, harming understanding and reproducibility. See questions for more details.\n* There are missing details harming reproducibility. The authors promise for open-sourcing the code, which is very nice, but some additional details could be provided in the appendix. See questions for more details."}, "questions": {"value": "I'm supportive of accepting this paper, which is really great, provided that the questions below are answered, especially those regarding clarifying the proposed approach (which is probably a rather minor modification)\n\n### More details on the proposed approach \n\n* The paper currently doesn’t say what model is used for generating $x_u$ from $y$ and $x_k$. From the context, it is likely the LLM being trained, but is it the initial frozen LLM, the LLM being trained (as training progresses), or even another LLM?\n* In all the above cases, the reward being optimized depends heavily on the capacity of the LLM to provide correct answer to the dual problem (predict $x_u$). If it’s not capable of making that prediction correctly enough, there is not learning signal. Can you comment on this? Notably, for the Llama experiment (table 3), the initial score is pretty low, suggesting that probably predicting $x_u$ may not be good enough, which makes the huge improvement quite surprising. Or is it that predicting $x_u$ is much easier than predicting $y$?\n* Is there a mechanism for improving the prediction of $x_u$ (if the current LLM is used for building the reward function)? Or is it a byproduct of improving the prediction of $y$? This would be an interesting experiment to add, measuring the initial reward quality (given the ground truth $y$), and how it evolves along learning, once the precise setup has been clarified.\n\n### Reproducibility details\n\nSome parameters are not provided, such as the optimizer, learning rate and other related parameters, but more importantly the proposed reward function relies on hyper parameters that are not provided, and they should be provided but also ideally the sensitivity of the approach to their value should be studied. Notably, the choice of $\\lambda$ and of the distance $d$.\n\n### Other comments and questions \n\n* What is called preference optimization (eg title of sec 3.3) is not preference optimization, it’s RL. It is semantics, but also misleading. \n* Everywhere but in Eq (1), the reward does not depend on $x$, which is hopefully a typo\n* A paper that would be interesting to discuss (no experiment needed) is “Reinforcement Learning Teachers of Test Time Scaling” by Sakana. They address a different problem, but there is some interesting similarity in the idea of decomposing the prompt and predicting part of it.\n* Sec. 4.5.2 (scaling reasoning during inference without training) is very interesting. Please discuss the additional cost. Also, an experiment that would be quite interesting would be to run this on huge close-source strong models like the latest R1, through the API (as there is no learning), to see the amount of possible improvement there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HwmYfdE5uI", "forum": "SD8Z231C45", "replyto": "SD8Z231C45", "signatures": ["ICLR.cc/2026/Conference/Submission6741/Reviewer_P36R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6741/Reviewer_P36R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760719692047, "cdate": 1760719692047, "tmdate": 1762919026883, "mdate": 1762919026883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DuPO, a self-supervised preference-optimization framework that replaces external reward signals (human or verifiable answers) with a generalized duality signal. The idea is to decompose each input into known and unknown parts $(x_k, x_u)$ and define a dual task that reconstructs only the unknown parts from the model’s output and the known context. The reward is the distance between the true $x_u$ and its reconstruction. DuPO then optimizes the promal policy $\\pi_{\\theta}(y|x)$ with standard RL updates (GRPO in experiments). The authors conduct the experiments on translation, math reasoning, and inference reranking tasks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The idea of integrating dual learning into self-supervised LLM fine-tuning is relatively new and underexplored.\n* The authors conduct experiments across several tasks, including both LLM fine-tuning and reranking.\n* The paper includes a nice ablation study on component selection."}, "weaknesses": {"value": "* Although the idea of dual learning is relatively new, the motivation for why it improves performance remains unclear. The notion of generalized duality is defined axiomatically, but there are no guarantees showing that optimizing the DuPO reward leads to better true task utility, nor theoretical bounds relating to the choice of decomposition. For tasks like machine translation, the intuition makes sense as the forward and backward tasks are naturally aligned, but for other tasks, it is unclear why DuPO would outperform existing methods such as RLVR or other self-play approaches.\n* Several claims in the paper are problematic. For instance, the authors state that RLVR and RLHF rely on costly labels. However, constructing an appropriate dual task for DuPO may be even more challenging. Moreover, this may not even be possible for tasks such as creative writing. While the authors briefly mention the creative writing task, they do not discuss how it could be resolved.\n* Reward design: It is unclear how the reward is defined when there are multiple valid outputs for the reconstructed unknown. For example, in a math problem with a single correct answer but many possible solution paths, how is the reward computed consistently?\n* Baselines: The paper lacks RLVR baselines on the math reasoning task, as well as self-play LLM baselines such as [1-4].\n\n[1] Chen, Zixiang, et al. \"Self-play fine-tuning converts weak language models to strong language models.\" ICML 2024.\n\n[2] Pang, Richard Yuanzhe, et al. \"Iterative reasoning preference optimization.\" NeurIPS 2024.\n\n[3] Dong, Qingxiu, et al. \"Self-boosting large language models with synthetic preference data.\" ICLR 2025.\n\n[4] Wu, Yue, et al. \"Self-play preference optimization for language model alignment.\" ICLR 2025."}, "questions": {"value": "* How should the inputs be decomposed into known and unknown parts in tasks such as creative writing? \n* How does the choice of known and unknown parts affect the reward? For example, if the unknown parts are too simple, can the model predict them from the known parts alone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6xYeKVcukD", "forum": "SD8Z231C45", "replyto": "SD8Z231C45", "signatures": ["ICLR.cc/2026/Conference/Submission6741/Reviewer_tLFR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6741/Reviewer_tLFR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881804878, "cdate": 1761881804878, "tmdate": 1762919026533, "mdate": 1762919026533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DuPO (Dual Learning-based Preference Optimization), a novel framework for optimizing Large Language Models (LLMs) without external supervision. DuPO addresses the limitations of traditional dual learning—which requires strictly invertible task pairs—and Reinforcement Learning with Verifiable Rewards (RLVR)—which relies on costly ground-truth labels. The core innovation is a generalized duality framework: it decomposes the input of a primal task into known and unknown components, and then defines a dual task that reconstructs only the unknown component using the primal output and the known input. The quality of this reconstruction serves as a self-supervised reward signal to optimize the primal task. The authors demonstrate DuPO's effectiveness on both mathematical reasoning and multilingual translation, showing significant performance gains during training and as a training-free inference-time reranker."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Fully Self-Supervised: The method requires no human labels, reference answers, or external reward models, which is a major advantage for scalability and cost-efficiency.\n2. Practical Impact: The ability to use DuPO as a simple, training-free reranker that boosts performance by +9.3 points is a highly practical contribution."}, "weaknesses": {"value": "1. Insufficient experiments: In most experiments, DuPO is compared to untrained models, which is unfair. For translation tasks, RLVR may be difficult, but it is still worth using as a baseline. For mathematical tasks, standard RLVR should be added as a baseline. In addition, SFT can be added to each benchmark as a baseline.\n2. Task Construction: While the paper provides principles for selecting the unknown component (Appendix A), the process still seems somewhat heuristic and task-specific. I still have concerns about the generality of this method on other tasks."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "77vZk4cWNp", "forum": "SD8Z231C45", "replyto": "SD8Z231C45", "signatures": ["ICLR.cc/2026/Conference/Submission6741/Reviewer_EbeA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6741/Reviewer_EbeA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913704296, "cdate": 1761913704296, "tmdate": 1762919026135, "mdate": 1762919026135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce DuPO, a consistency-based preference optimization framework which exploits task duality to increase post-training robustness. The authors achieve success over existing baselines on math datasets, and extend their framework to work at inference-time as well."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written, easy to follow, and the figures are detailed with the appendices including relevant information and concrete examples\n- The idea of using dual problems in RLHF is interesting and (to my knowledge) novel\n- The experimental results appear rather promising"}, "weaknesses": {"value": "- My primary concern with DuPO is regarding the difficulty of constructing the dual problems, verifying the correctness of those duals problems, and how generalizable it is to different domains. \n- In addition, it seems rather expensive to have to run DuPO at inference-time since the dual problems would also need to be generated on the fly?"}, "questions": {"value": "- Please address weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OXpuW5CjTC", "forum": "SD8Z231C45", "replyto": "SD8Z231C45", "signatures": ["ICLR.cc/2026/Conference/Submission6741/Reviewer_S26D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6741/Reviewer_S26D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762190238277, "cdate": 1762190238277, "tmdate": 1762919025484, "mdate": 1762919025484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}