{"id": "cd5bhCHbMe", "number": 25592, "cdate": 1758369345845, "mdate": 1762943997928, "content": {"title": "hDRIVE: HDR Image Visual Evaluation Metric for SDR to HDR Upconversion Quality Assessment", "abstract": "HDR displays are becoming increasingly common on both TVs and mobile devices, that requires to adapt existing legacy SDR to HDR screens. Several algorithms have been developed for SDR-to-HDR upconversion, also known as Inverse Tone Mapping (ITM). However, there is still a lack of reliable metrics for assessing the quality of these algorithms. This is due in part to the ill-posed nature of the ITM task, where the most visually pleasing result can significantly differ from the original image. In this work, we propose a novel state-of-the-art no-reference video quality metric for evaluating upconverted HDR content. To support our approach, we collect a large-scale dataset of human visual preferences, capturing both the perceived visual appearance and quality of HDR videos. The HDR ITM video quality metric might be very helpful and drive the rapid advancement of SDR-to-HDR algorithms development and benchmarking. Both the metric and the training dataset are publicly available for download via the provided link.", "tldr": "", "keywords": ["HDR", "SDR", "inverse tone mapping", "video quality assessment"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ca12c73c3f295e0ef0523c2e529d93d673d1aa75.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a new no-reference metric for evaluating SDR-to-HDR inverse tone mapping (ITM) results, claiming to achieve state-of-the-art correlation with human judgments. The authors also introduce a new human-labeled dataset focusing on aesthetic preference and technical artifact perception for HDR content. The metric is a dual-branch network combining a CLIP-based aesthetic branch and a CNN-based artifact detection branch."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a timely and valuable problem and proposes a reasonable dual-branch design for perceptual HDR quality assessment.\nHowever, each claimed contribution suffers from missing or inconsistent details: the image/video scope is unclear, the dataset is not accessible or reproducibly described, and HDR preprocessing and evaluation protocols are insufficiently specified."}, "weaknesses": {"value": "1. The title and abstract conflict: the title claims an HDR image metric, while the abstract repeatedly calls it a video quality metric.\n2. The method itself is image-based, yet the evaluation uses video datasets (Voronin et al., 2024). No temporal modeling, motion handling, or frame aggregation method is described.\n3. The abstract states that “the metric and dataset are available for download,” but no link or preview is provided in either the paper or the supplementary materials.\n4. No visualization of the dataset is shown (no tone/LUT examples, no patches, no aesthetic pair examples).\n5. The annotation procedure uses mobile HDR screens without standardized luminance, tone mapping, or environmental light control, which introduces uncontrolled bias.\n6. The HDR-to-CLIP preprocessing pipeline is not described. CLIP was trained on SDR sRGB data—how were HDR values mapped?\n7. References include placeholders like “[ref]” and incomplete “et al.” entries."}, "questions": {"value": "As mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fCUI0mMvFO", "forum": "cd5bhCHbMe", "replyto": "cd5bhCHbMe", "signatures": ["ICLR.cc/2026/Conference/Submission25592/Reviewer_qovQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25592/Reviewer_qovQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760931659992, "cdate": 1760931659992, "tmdate": 1762943487613, "mdate": 1762943487613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "engMaNDz8J", "forum": "cd5bhCHbMe", "replyto": "cd5bhCHbMe", "signatures": ["ICLR.cc/2026/Conference/Submission25592/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25592/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762943997188, "cdate": 1762943997188, "tmdate": 1762943997188, "mdate": 1762943997188, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this submission, the authors try to address the critical need for a reliable quality metric for SDR-to-HDR upconversion (ITM). The authors propose HDRIVE, a no-reference, dual-branch metric trained on a new, large-scale dataset. \n\nThe architecture separates the problem into: 1) an Aesthetic Branch (a frozen CLIP encoder on a downscaled image) to predict global visual preference, and 2) an Artifact Branch (a patch-based CNN) to detect local distortions.\n\nThe dataset itself is also a major contribution, collected via pairwise comparisons on a mobile app that verified the user's display was HDR-capable. The authors promised they will make this dataset public."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The scale and diversity of the dataset. In terms of scale, the subjective evaluation involved gathering responses from over 3,000 crowdsourcing participants, resulting in a robust preference dataset, and the technical artifact labeling extracted over 100,000 image patches. The diversity in stimulus was achieved by generating rich and perceptually varied distortions through methods like parameterized tone mapping operators, the incorporation of over 200 3D LUTs, and automated color grading via tone curve manipulation.\n\n2. The choice of evaluation platform and verification. I agree that evaluation on smartphones makes sense since they grow really fast for HDR content consumption. The verification step is critical to ensure the capacity of display of participants.\n\n3. The design of dual-branch network is interesting, and the performance and efficiency of the proposed method shows a significant advantage among compared methods."}, "weaknesses": {"value": "1. Too many unknown details. \n- The specific origin or identity of the base content is unknown, such as the initial 1,000 FullHD images or the videos used for subjective preference collection.\n- The sources of LUTs and color grading algorithms are unknown.\n- The reference of an adopted pretrained model for artifact detection is a placeholder citation (L351).\n\n2. The evaluation condition. Although the authors verify the participants' smartphones have an HDR display, but HDR displays differ to each other significantly. For example, an iPhone 15 Pro with a 2000-nit peak XDR display is completely different from a 3-year-old mid-range Android phone that technically supports the HDR10 standard with a 600-nit panel. Furthermore, the view condition is totally uncontrolled, and the brightness of the screen may changing according to ambient lights. Finally, there are possibly on-device tone-mapping algorithms to adapt the displayed image with the screen capacity, which are again unspecified in the dataset.\n\n3. The validation of the method. It is unclear that how the other methods are prepared for the comparison. For trainable methods, it would make more sense to train them on the collected dataset. It seems to me the validation is mixing two perspectives of contribution together. Further, from the ablation study,  the model's performance is overwhelmingly dominated by the aesthetic branch. It would be better to try on other benchmarks to separate the evaluation of aesthetics and quality."}, "questions": {"value": "Is it a video metric or an image metric? The entire methodology describes a 2D, static image metric (CLIP image encoder, image patch CNN) applied frame-by-frame, which is blind to temporal artifacts. However, the collected dataset and the validation one seem video datasets."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The sources of the HDR videos, images, and LUTs are unknown."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MI2lObeQRS", "forum": "cd5bhCHbMe", "replyto": "cd5bhCHbMe", "signatures": ["ICLR.cc/2026/Conference/Submission25592/Reviewer_ou3w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25592/Reviewer_ou3w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709926083, "cdate": 1761709926083, "tmdate": 1762943487383, "mdate": 1762943487383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript identifies a critical gap in the current evaluation of ITM algorithms, where existing full-reference and no-reference metrics fail to align well with human perception due to the ill-posed nature of the task. It constructed a large-scale human-labeled HDR images dataset generated from SDR-to-HDR upconversion and proposes a novel no-reference quality assessment metric specifically designed for evaluating HDR images.x"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A primary contribution of this work is the creation and public release of a large-scale human-annotated dataset tailored for ITM quality assessment. This directly addresses a critical bottleneck in the field—the lack of subjectively labeled data for training and benchmarking.\n\n2. The proposed metric is a novel, dual-branch architecture specifically engineered for the ill-posed problem of ITM quality assessment."}, "weaknesses": {"value": "1. There are many conflicting descriptions in the manuscript. For example, the manuscript aims to design a model for video evaluation, yet the dataset construction and the computational model design appear to be for images. There is no design seen that targets video data.\n\n2. The fusion mechanism of the proposed metric between the aesthetic and artifact branches is not described in sufficient detail. \n\n3. While the dataset is a strength, more information on its size, diversity, and potential biases would be helpful.\n\n4. The manuscript does not deeply discuss limitations or failure cases of the proposed metric."}, "questions": {"value": "1.\tCould you provide more details on how the outputs of the aesthetic and artifact branches are fused? Is the fusion module trained end-to-end after branch pre-training?\n\n2.\tHow does the model perform on HDR content that was not generated via ITM (e.g., native HDR)? Is the metric generalizable to other HDR quality assessment tasks?\n\n3.\tWere any strategies used to ensure the dataset covers a diverse set of content and distortion types beyond the synthetic transformations?\n\n4.\tThe benchmark for evaluation is limited. Could you provide more evaluation results on more HDR datasets?\n\n5.\tIt should be further clarified whether there is a significant gap between the synthetic distortion techniques introduced in this manuscript for creating distorted images and the distortions caused by existing inverse tone mapping (ITM) algorithms, and whether these techniques are sufficiently capable of simulating the distortions produced by ITM algorithms.\n\n6.\tIn line 225, it states, \"Each participant evaluated 50 image pairs.\" However, the previous descriptions in this manuscript were all about video evaluation. What is the reason for this contradiction with the earlier description?\n\n7.\tSections 3.1 and 3.3 seem to describe the construction of image datasets, while Section 3.2 describes the evaluation of video data. The author should provide a clearer explanation for this.\n\n8.\tWhy construct an aesthetic dataset in Section 3.1 and an artifact dataset in Section 3.3 separately, thus requiring two large-scale subjective annotation processes? Why not construct a single dataset and annotate these several dimensions simultaneously in one annotation process?\n\n9.\tThe author used an application to verify participants' devices for HDR support, but these details are not clearly described. What is the maximum brightness (luminance) of the HDR images used? What criteria must a user's display meet to be considered a \"ready\" HDR display? Will inconsistencies among different users' mobile phone displays cause additional color differences or brightness discrepancies? When such discrepancies exist, is calibration necessary to ensure that the same data source appears consistent to different users?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hDLDHRHDge", "forum": "cd5bhCHbMe", "replyto": "cd5bhCHbMe", "signatures": ["ICLR.cc/2026/Conference/Submission25592/Reviewer_TUpE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25592/Reviewer_TUpE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882001370, "cdate": 1761882001370, "tmdate": 1762943487029, "mdate": 1762943487029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}