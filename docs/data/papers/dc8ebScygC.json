{"id": "dc8ebScygC", "number": 16336, "cdate": 1758263313925, "mdate": 1759897246775, "content": {"title": "Structured Uncertainty guided Clarification for LLM Agents", "abstract": "LLM agents extend large language models with the ability to perform real-world actions through tool calls, but ambiguous or incomplete user instructions often lead to incorrect invocations, failed tasks, and degraded user experience. We introduce a \\textbf{principled formulation of structured uncertainty} over tool-call parameters, modeling joint tool--argument clarification as a POMDP. By optimizing an Expected Value of Perfect Information objective, our approach selects clarification questions that maximize expected task success while an aspect-based cost function prevents redundant questioning. Building on this formulation, \\textbf{SAGE-Agent} leverages structured uncertainty to improve interaction efficiency and task coverage, increasing coverage on ambiguous tasks by 7--39\\% and reducing the number of clarification questions by 1.5--2.7$\\times$ over strong prompting- and uncertainty-based baselines. To support evaluation, we present \\textit{ClarifyBench}, the first benchmark for multi-turn, tool-augmented disambiguation, equipped with an LLM-based user simulator that enables realistic conversational progression across diverse domains including document editing, vehicle control, stock trading, travel booking, and file system manipulation. Finally, we show that structured uncertainty serves as an effective reward model for reinforcement learning; on the When2Call dataset, uncertainty-weighted training boosts accuracy from 36.5\\% to 65.2\\% for the 3B model and 36.7\\% to 62.9\\% for the 7B model. These results demonstrate that structured uncertainty provides a principled, efficient approach for tool-augmented LLM agents, improving both task success and interaction efficiency in multi-turn, real-world scenarios.", "tldr": "", "keywords": ["ambiguity", "llm agents"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/874d317b1a388ce1e2d4bcaab99c1b4f333b31c0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper’s general topic are tool-using agents that have to ask clarification questions about ambiguous queries. It is split in three parts: 1) A handcrafted inference strategy to decide which clarification question to ask based on how complete the best possible function call would be, 2) An evaluation dataset with ambiguous scenarios, and 3) an RL-trained agent. The authors find that their two agents work better than their respective baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The general topic is highly important, and the dataset is useful for future research\n2. I appreciate that the authors always report spans of results and show confidence intervals and noise, even when broad.\n3. Both proposed methods look promising, but it is hard to say how robust their performance is due to the limited evaluation (see below)"}, "weaknesses": {"value": "Weaknesses, in order of magnitude: \n1. The paper seems to contain three parts that are only loosely connected by their general topic. For example, there is both the hand-engineered SAGE agent and then the GRPO trained agent in Section 6. They are similar in that they both use pi_i, but are never compared to one another or executed on the same benchmark dataset. \n2. I think the paper would greatly benefit from being split into multiple papers, because:\n    1. As a dataset paper, the dataset construction would need to be more critically assessed, and should have a larger size than n=716. The construction of the ClarifyBench dataset is also just very high-level and not detailed, neither in Section 4.2 (“we design handwritten rules based on common API errors to create tool calls that would generate failures, followed by a similar LLM-based query augmentation process”), nor in the appendix.\n    2. As a paper that proposes a new method (SAGE-Agent), the new method is only evaluated on one dataset (the dataset introduced above), and there is a second method intoduced (GRPO training on When2Call, and evaluated on a different dataset), plus the methods failure modes and when it does / does not work are not analyzed.\n3. The probabilities pi_i, upon which both the SAGE-Agent and the GRPO training rest, are heavily hand-crafted and do not take into account LLM’s token uncertainties.\n4. The expectation over a maximum in the EVPI objective creates a heavy inference-time compute burden, especially in less restricted domains, because we have to search over all questions, take an expectation over all follow-up answers, and then search the maximum pi_i. \n5. There are multiple readability issues. Normally I would note this as not influencing my score, but here it poses real difficulties to comprehend the paper. The paper could have a much greater impact if it benefitted from more time spent to make it readable.\n    1. abbreviations are not introduced before usage (POMDP), \n    2. the contributions section in the introduction is hard to understand due to the non-motivated and non-explained terms (Bayesian Value of Information Objective, expected value of perfect information), \n    3. What are coverage rate, tool match rate, and parameter match rate (benchmark metrics in Table 3)\n    4. observations_t in line 153 is undefined (and probably the same as obs_t)\n    5. I suppose that line 117 is an indicator function?\n    6. Cost(q) in line 177 is not defined until line 215\n    7. There are Latex missing citations errors (line 181)\n    8. Likewise, Algorithm 1 (called SAGE (final corrected version)) has multiple reference errors in the equations\n    9. The reward function in line 177 is not used. A second reward is defined in line 400\n    10. Citations are not in brackets and not hyperlinked, \n    11. Section 4.2: Data Augmentation has multiple grammar errors\n    12. vspace around figures is sometimes very small and sometimes very large (Figure 2 vs Figure 4), \n6. I have doubts about the independence of the human annotators that ensure quality and naturalness of the dataset, given that it is “two graduate student annotators that were compensated following their respective graduate school policies” (Appendix B.2). To make the evaluation more rigid, I suggest to use independent raters (Mechanical turks and the likes), use multiple annotators per sample, and measure annotator disagreement.\n7. It would be great to evaluate on more than one LLM (GPT-4o for SAGE-Agent and Qwen 2.5 3B/7B for the RL experiments). Especially in user simulation and interactive settings, different LLMs behave wildly differently. \n\n\nSmaller weaknesses that do not influence my score and do not need to be rebuttled, but I suggest fixing them for the revised version:\n\n* It would increase readability if you could hyperlink your references (e.g., use the cleveref package)\n* The caption of Table 2 is not a full sentence\n* Algorithm 1 has multiple fonts\n* Spelling error in Table 3: LLm → LLM\n\n## Justification for the overall score\n\nI believe that this paper addresses an important gap in the current research field, clarification questions for tool-calling agents. However, the paper in its current form is not clear and reproducible, and strechted too thinly across three subtopics to discuss either of them in detail. It reads more like three workshop short-papers in the current form. I recommend to reject the paper at this point, but believe that the authors are working on something that is promising. I encourage the authors to disentangle their three subcomponents, and focus on one of them fully, in order to have the capacity to analyze and discuss it in detail."}, "questions": {"value": "1. Could you explain the argument you make in lines 131-136? Why do other approaches _have to_ take this route to model ambiguity, and why is your approach not influenced by model (/epistemic) uncertainty? Your proposed uncertainties can easily break if the model makes a wrong prediction, this is assumed away if I understand correctly?\n2. Can you discuss the difference between the Expected Value of Perfect Information and the more commonly used Expected Information Gain objective, and why the former corresponds to “optimal” (line 182) question selection in your opinion? Can you benchmark against Expected Information Gain? \n3. By “aspect” a_i,j, you just mean the identifier (i, j) of some theta_i,j, is that correct? \n4. Why is your reward self-calibrating? (line 405) How does this relate to calibration, is it a proper scoring rule?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YwgjD6uJ4d", "forum": "dc8ebScygC", "replyto": "dc8ebScygC", "signatures": ["ICLR.cc/2026/Conference/Submission16336/Reviewer_e7rp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16336/Reviewer_e7rp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16336/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760453558823, "cdate": 1760453558823, "tmdate": 1762926470949, "mdate": 1762926470949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SAGE-Agent, a framework that uses structured uncertainty and Expected Value of Perfect Information to help LLM agents ask optimal clarification questions before taking actions.\nIt also presents ClarifyBench, a new benchmark for evaluating tool-augmented agents under ambiguous or infeasible user requests across multiple domains. \nExperiments show that SAGE-Agent significantly improves task success rates and reduces unnecessary clarifications compared to strong LLM baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- ClarifyBench fills a gap in existing evaluations by supporting dynamic user simulation, multi-turn requests, and infeasible queries across diverse domains (documents, vehicle control, stocks, travel, and file systems).\n- The structured uncertainty approach also serves as an effective reward signal, improving sample efficiency and performance on unrelated tasks\n- The paper is thorough, with detailed algorithmic design, theoretical proofs, and practical implementation notes."}, "weaknesses": {"value": "- While ClarifyBench is valuable, the user simulator relies on LLM-generated interactions, which may not fully capture human ambiguity or pragmatic nuances.\n- EVPI computation scales with candidate size and domain dimensionality. Though approximations are discussed, concrete runtime or complexity comparisons with simpler heuristics are missing."}, "questions": {"value": "- The paper relies on an LLM-based user simulator to model realistic conversational progression. How do the authors ensure the accuracy and reliability of this simulator’s responses compared to real human interactions?\n- It would be helpful to include an ablation study on key hyperparameters to better understand their influence on model performance and stability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FiOWYz2ewq", "forum": "dc8ebScygC", "replyto": "dc8ebScygC", "signatures": ["ICLR.cc/2026/Conference/Submission16336/Reviewer_udb7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16336/Reviewer_udb7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16336/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853689714, "cdate": 1761853689714, "tmdate": 1762926470562, "mdate": 1762926470562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "They propose SAGE: keep a belief over structured tool-call candidates and choose clarifying questions by Expected Value of Perfect Information (EVPI), with a cost for redundancy. They also introduce ClarifyBench. (Their own text sets up the belief factorization and EVPI definition.They use RL with GRPO to train QWen model and show improvements over baselines on their own benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Separation of uncertainties. They argue against using model generated response for uncertainty quantification which is a reasonable structural move.\n2. A potentially useful benchmark, ClarifyBench which covers several tool domains with explicit/ambiguous/infeasible splits and reports basic stats."}, "weaknesses": {"value": "1. You make strong assumptions which are not validated in the work. The viability score assumes 1) a uniform prior over tools 2) naive independence across paramters and 3) an arbitrary $\\epsilon$  for continuous domains. Additionally, there is no sensitivity analysis.\n2. While they compare against a few agent baselines, there’s no demonstration on widely used external tool-use suites (tau-bench, etc). Moreover, the dataset is llm augmented using GPT-4ofor  query generation/obfuscation.\n3. They introduce a certainty-weighted reward, which by construction favors their belief-based approach. The paper doesn't isolate how much of the gain comes from reward shaping vs the EVPI policy itself.\n4. There is no minimal, prompting baseline where you have something like \"ask only for missing required parameters, no repeats\" and no ablation showing the incremental value of EVPI vs simple heuristics.\n5. Works/tools like GenieWorksheets [1], and MCP automatically take care of these aspects. If the LLM partially fills an API, they generate an `ask_question()` like function which asks for unfilled parameters. \n6. The presentation of this manuscript, needs a lot of work. The mathematical notations are not clear and for some scenarios are seem forced.\n7. Beyond simulator metrics (TMR/PMR/CR), can you report a small human study on question helpfulness and over-questioning, especially in ambiguous cases?\n8. Seems like SAGE only adds a scaffold around llm proposed candidates and questions, you’re still depending on the LLM to decide the potential candidates. You have just made the use of those proposals safer and more auditable.\n\n[1] Controllable and Reliable Knowledge-Intensive Task-Oriented Conversational Agents with Declarative Genie Worksheets (Joshi et al., ACL 2025)"}, "questions": {"value": "1. It is unclear to me why do you want domains:   Di = {Di,1, Di,2, . . . , Di,mi } where Di,j is the domain of parameter θi,j -- why do you need domain of parameter? what does domain even mean?\n2. Please define what is $u$ before Line 131.\n3. In line 166, State Space: S = {(Ti, θi) : Ti ∈ T , θi ∈ Di} represents true user intent -- why is theta_i \\in D_i?\n4. GPT generated equations and notations. eg line 177, gpt generates this for classification. \n5. Missing citation on line 181\n6. Please fix citation formatting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YAS305CqJ3", "forum": "dc8ebScygC", "replyto": "dc8ebScygC", "signatures": ["ICLR.cc/2026/Conference/Submission16336/Reviewer_Vi8S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16336/Reviewer_Vi8S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16336/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970129902, "cdate": 1761970129902, "tmdate": 1762926470139, "mdate": 1762926470139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a dataset and method for developing LLMs that can ask clarifying questions to disambiguate user requests in tool-calling dialogues. Their dataset consists of automatically perturbed examples sourced from another dataset, DocPilot, using GPT to introduce ambiguity into the example. Examples are then verified by a human annotator. The authors then propose a method for training an LLM to engage in such dialogues with users, which is based on estimating the the benefits of clarifying the user query and weighing this against the cost. The authors then compare their proposed method on their proposed benchmark against several prompt-based baselines, demonstrating gains."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This work provides a dataset for studying the intersection between ambiguity and tool use settings, which as of yet is underexplored and presents many novel challenges."}, "weaknesses": {"value": "1. Presentation is poor. The task itself from the constructed dataset, while it is sourced from an existing work, is not actually explained. Variables and acronyms are frequently not defined, or are defined in unintuitive places like figure captions. Citations are incorrectly formatted.\n\n2. The authors compare exclusively against prompt-based baselines. Several more comparable and competitive methods should be compared against or discussed in the least, even if it's just a simple SFT baseline. Additional related methods to be discussed or compared against include may include others that utilize GRPO/PPO/DPO training for similar clarify or execute decisions in user-llm dialogues.\n\nCollabLLM: From Passive Responders to Active Collaborators\nShirley Wu, Michel Galley, Baolin Peng, Hao Cheng, Gavin Li, Yao Dou, Weixin Cai, James Zou, Jure Leskovec, Jianfeng Gao\nICML 2025\n\nModeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions\nMichael J.Q. Zhang, W. Bradley Knox, and Eunsol Choi.\nICLR 2025\n\nLearning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training\nM. Chen, R. Sun, T. Pfister, S.O. Arik\nICLR 2025\n\n3. The validity of the dataset is unclear. While the authors say that examples are validated by a human annotator, it's unclear how reliable this is or what agreement on validation would be. Almost all of the results are based on this dataset as well, so looking at other tasks/settings would also help substantiate the method."}, "questions": {"value": "1. Could you elaborate on the PII from the source dataset that is being filtered out? Is the PII potentially harmful?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "This work relies on LLMs with a human validator to remove PII from examples. Unclear how reliable this is or whether the PII would be potentially harmful."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H3Shc0y9Zk", "forum": "dc8ebScygC", "replyto": "dc8ebScygC", "signatures": ["ICLR.cc/2026/Conference/Submission16336/Reviewer_y8WD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16336/Reviewer_y8WD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16336/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762531680128, "cdate": 1762531680128, "tmdate": 1762926469697, "mdate": 1762926469697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}