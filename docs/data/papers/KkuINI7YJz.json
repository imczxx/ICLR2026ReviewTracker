{"id": "KkuINI7YJz", "number": 18098, "cdate": 1758283798260, "mdate": 1762939188988, "content": {"title": "EmoDialogCN: A Multimodal Mandarin Dyadic Dialogue Dataset of Emotions", "abstract": "Face-to-face audiovisual interaction is fundamental to human communication, conveying rich and spontaneous emotional expressions. However, existing multimodal dialogue datasets suffer from irregular framing, insufficient coverage of upper-body dynamics, limited emotional diversity, small scale, and a lack of genuine spontaneity. We introduce EmoDialogCN, a large-scale auditory–visual–emotion multimodal dataset specifically designed to capture the richness and spontaneity of real-world face-to-face dialogues. The dataset comprises 21,880 dialogue sessions performed by 119 professional actors across 20 realistic scenarios and 18 emotion categories, totaling 400 hours of recordings—the largest and most comprehensive of its kind. A novel data collection framework minimizes equipment interference and ensures authentic multimodal signals. Actors were encouraged to improvise based on their understanding of the context, allowing spontaneous emotions to emerge naturally. EmoDialogCN achieves superior quality metrics, including natural and clear emotional expressions confirmed by subjective evaluations (average inter-rater std = 0.12), lower emotion distribution deviation (0.64 vs. 5.65), consistent subject framing (52–59\\% occupancy), and comprehensive coverage of facial and upper-body expressions. Models trained on this dataset generate contextually appropriate facial expressions, natural body movements, and realistic speaker–listener dynamics, underscoring the value of authentic spontaneous emotional data. The dataset is publicly available at: https://github.com/EmoDialogCN/EmoDialog", "tldr": "", "keywords": ["Multimodal Dialogue Dataset", "Mandarin Emotional Dialogue", "Auditory-Visual Emotion Modeling"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/cfe8786df74e116172d917e2b727bf36bfe64a0c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces EmoDialogCN, a new large-scale, multimodal dataset for Mandarin dyadic (two-person) dialogues. The primary motivation is to address the shortcomings of existing datasets, which often suffer from poor visual quality due to webcam use, limited emotional diversity, lack of spontaneity, and insufficient coverage of upper-body dynamics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-Motivated and Thoughtful Data Collection: The paper does an excellent job of identifying the key limitations of existing datasets and proposes a meticulous data collection methodology to overcome them. \n\n- Impressive Scale: The dataset is substantial in scale, with 400 hours of recordings, 119 actors, and over 21,000 dialogue sessions. \n\n- Clarity and Presentation: The paper is well-written, clearly structured, and easy to follow."}, "weaknesses": {"value": "- Lack of Methodological Detail in Post-Processing and Application: The paper lacks crucial details that are necessary for reproducibility and scientific rigor. For example:\n   - ASR Model: The paper states that Automatic Speech Recognition (ASR) was used to transcribe audio and generate timestamps for segmentation, but it fails to mention or cite the specific ASR model used. The performance of this model directly impacts the quality of the temporal alignments.\n   - Quality Filtering: The authors mention that segments with \"poor audio or video quality are filtered out,\" but the criteria for this filtering are not defined. This process is subjective without explicit metrics.\n   - Data Application Model: The description of the video generation model is superficial, listing a series of components (WhisperVQ, FLUX-VAE, Qwen2.5-3B, flow-matching diffusion) without explaining the architecture, training methodology, or rationale for these choices.\n\n- Superficial Experimental Evaluation: The \"Data Application\" section (4.4) feels more like an appendix than a core part of the results. There is no quantitative evaluation of the generated video. Metrics for lip-sync accuracy, expression appropriateness, or image quality are completely absent.\n\n- No baselines are provided. A stronger evaluation would have involved training the same model architecture on other datasets and comparing the results to demonstrate the superiority of EmoDialogCN."}, "questions": {"value": "Could you please specify which ASR model was used for segmentation and provide its accuracy (e.g., Character Error Rate) on a sample of your data? What were the specific, objective criteria used to filter out \"poor audio or video quality\" segments during post-processing?\n\nCould you provide a more detailed diagram and description of the generative model architecture used in Section 4.4? Why was this specific combination of components chosen?\n\nThe paper explicitly mentions supplementary materials for viewing the video results. Can these be provided? The claims about the quality of the generated speaker-listener dynamics are difficult to assess from a few static images alone.\n\nBeyond confirming the dataset's quality, have you performed any exploratory analysis that reveals unique insights? For example, given the face-to-face improvised setting, did you observe different turn-taking dynamics, gesture patterns, or listener feedback cues (e.g., nodding, smiling) compared to datasets recorded via video calls?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qqKhlsKL3p", "forum": "KkuINI7YJz", "replyto": "KkuINI7YJz", "signatures": ["ICLR.cc/2026/Conference/Submission18098/Reviewer_FGcg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18098/Reviewer_FGcg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760631772960, "cdate": 1760631772960, "tmdate": 1762927867616, "mdate": 1762927867616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes EmoDialogCN, a large-scale audiovisual dataset of Mandarin dyadic dialogues, collected with an in-studio, face-to-face setup intended to minimize webcam distortions and capture upper-body dynamics. The dataset is comprised of ~400 hours, with ~21.8k dialogues, 119 actors, 20 scenarios and 18 emotions. The authors emphasize careful capture (4K vertical cameras, studio mics), semi-improvised scripts seeded by LLM prompts, segmentation with ASR, and several quality analyses (emotion distribution vs. sociological priors, inter-rater variability, body/framing heatmaps)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Collection framework to capture high quality dialogues. The paper presents a well-designed data collection framework that enables the recording of natural, face-to-face Mandarin dialogues with high audiovisual fidelity. The authors use 4K vertical cameras, studio microphones, and a sound-treated environment to minimize visual and acoustic distortion. The setup captures upper-body gestures, maintains consistent framing, and ensures good synchrony between audio and video\n\n2. Multiple scenarios with varied emotional expressions. The dataset covers 20 everyday conversational scenarios designed to elicit 18 different emotions through semi-improvised performances"}, "weaknesses": {"value": "1. Unbalanced set of emotions to represent real-world emotion frequencies. The dataset is designed to mirror natural emotional frequency distributions but this leads to a large imbalance across classes. While realistic, this imbalance may limit the dataset’s usefulness for model training.\n2. The dataset consists of Mandarin-speaking actors from China, which limits both cultural and linguistic diversity. Although the paper acknowledges this limitation, the absence of multilingual or cross-cultural data reduces the dataset’s generalizability.\n3. Some information is missing, making parts of the paper less understandable or accurate in their descriptions (see \"Questions\" section e.g. Q2, Q3, Q4). For example, the paper does not specify the ASR model used for segmentation or its accuracy on Mandarin, nor does it clearly explain the construction of the “emotion valence vector space” or how inter-rater consistency was computed."}, "questions": {"value": "1. Why did you construct your dataset to represent real-world frequencies instead of creating a balanced set, so that derived models could be more generalizable?\n2. Sec. 3.4: Which ASR model did you use? How accurate is the model on Mandarin? \n3. Sec. 4.2: You mention that the annotations were mapped into an “emotion valence vector space.” How exactly did you do this and why did you use only valence? Did you rely solely on facial expressions? If so, how did you account for paralinguistic information in speech? \n4. Sec. 4.2: You state that “results show strong inter-rater consistency,” but you do not provide metric values. \n5. Sec. 4.3: What are the “empirical measurements” you refer to in the text? \n6. Sec. 4.4: This section is interesting and demonstrates the applicability of the dataset, but you do not include baselines, metrics, or user studies. \n7. What instructions were given to the subjects and annotators? It would be interesting to include cross-dataset evaluations (train on EmoDialogCN, test on MultiDialog/RealTalk, and the reverse) to test generalizability.\n\nMinor Comment\n\n1. Dialog count appears as 21880 in abstract vs 21800 in contributions. Please correct it"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper involves a large dataset of 4K audiovisual recordings of human subjects. The authors state that participants signed consent forms and that the dataset follows GDPR-aligned practices, with privacy-preserving measures such as anonymization, tiered release, and watermarking. However, some details remain unclear: how anonymization is implemented for facial and vocal data, and how consent revocation and data storage are managed."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BhRqYceCgf", "forum": "KkuINI7YJz", "replyto": "KkuINI7YJz", "signatures": ["ICLR.cc/2026/Conference/Submission18098/Reviewer_xyHx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18098/Reviewer_xyHx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945190270, "cdate": 1761945190270, "tmdate": 1762927866997, "mdate": 1762927866997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Ff8eEilykk", "forum": "KkuINI7YJz", "replyto": "KkuINI7YJz", "signatures": ["ICLR.cc/2026/Conference/Submission18098/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18098/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762936515849, "cdate": 1762936515849, "tmdate": 1762936515849, "mdate": 1762936515849, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents EmoDialogCN, a large-scale Mandarin dyadic audio-visual-emotion dialogue dataset: 21,880 dialogue sessions with 119 professional actors across 20 scenarios and 18 emotions, totaling ~400 hours of recordings. It introduces a purpose-built face-to-face recording setup where actors were encouraged to improvise based on their understanding of the context, allowing spontaneous emotions to emerge naturally."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Coverage and scope of the work is very resonable: 400h, 21.9k dialogues, 18 emotions, 20 scenarios, substantially larger and richer than prior dyadic dataset.\n- Though improvised, however, recorded by professional speakers, with eye-contact preservation; professional cameras at human-eye focal length; and with studio acoustics\n- Emotion distribution close to sociological priors (deviation 0.64 vs 5.65), inter-rater std 0.12; framing consistency (~52–59% occupancy)\n- The methodological approach to dialogue generation is also reasonable -- integrating human + LLM (GPT-4) pipeline for dialogue generation. \n- ASR-based turn alignment ensures that recording are properly segmented into dialogue-level and turn-level clips"}, "weaknesses": {"value": "Only a prototype application is presented; no systematic model comparisons or baseline results are provided. To demonstrate the dataset’s utility, quantitative evaluations are needed. Given the availability of both open- and closed-source multimodal models, baseline experiments could be conducted."}, "questions": {"value": "- Is there any analysis how emotional tone reflected in the recorded speech? How did you ensure that? I understand that it has been done by professional speaker and there were supervision, however, how they have a reflection in the acoustics might be important to see, which will also ensure the quality of the dataset. Same goes for the facial expressions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OfidP4Td5o", "forum": "KkuINI7YJz", "replyto": "KkuINI7YJz", "signatures": ["ICLR.cc/2026/Conference/Submission18098/Reviewer_ajbN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18098/Reviewer_ajbN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762081003864, "cdate": 1762081003864, "tmdate": 1762927866540, "mdate": 1762927866540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}