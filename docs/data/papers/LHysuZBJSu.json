{"id": "LHysuZBJSu", "number": 18695, "cdate": 1758290219131, "mdate": 1759897087106, "content": {"title": "Mac-Tiger: Multi-Agent Cooperation for Enhanced Text-to-Image Generation", "abstract": "Recent advancements in text-to-image (T2I) generation have significantly improved image fidelity and alignment with textual prompts, yet challenges remain in addressing complex compositional requirements, such as attribute binding, spatial relationships, and numerical precision. To tackle these issues, this paper introduces Mac-Tiger, a novel multi-agent cooperative framework that leverages multimodal large language models (MLLMs) to optimize T2I generation through iterative refinement. Unlike traditional single-agent approaches, Mac-Tiger employs a tri-agent system—comprising Reviewer, Challenger, and Refiner roles—that collaboratively evaluates and refines prompts based on dynamically generated feedback and multimodal analysis. Key innovations include integrating advanced modules for perception, memory, and cooperative planning to facilitate adaptive prompt optimization. Experiments on benchmarks like T2I-CompBench and MagicBrush demonstrate Mac-Tiger’s superior performance in generating semantically consistent and visually coherent images, particularly in scenarios involving intricate object interactions and detailed edits. This work underscores the potential of multi-agent systems to address long-standing limitations in T2I generation, paving the way for more robust and context-aware generative models.", "tldr": "We propose Mac-Tiger, a multi-agent cooperative framework leveraging MLLMs to iteratively refine text-to-image generation, improving semantic consistency and visual coherence in complex compositional tasks.", "keywords": ["Text-to-image generation", "Mulit-agent", "Image editing", "Multimodality"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de634036fad9921cc6cbf03c877ca8f7551cba11.pdf", "supplementary_material": "/attachment/13a763324c7ff2cd981e85260f93ea6765a3b5a1.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Mac-Tiger for complex compositional T2I generation task. Mac-Tiger employs a tri-agent system, including Reviewer,\nChallenger, and Refiner roles, that evaluates and refines prompts via dynamic feedback loops and multimodal analysis. The agents first generate initial solutions by exploring diverse perspectives and evaluating other's outputs. Then, the agents iteratively refine the prompt by addressing gaps and inconsistencies identified during the review and challenge phases. Experiments show effectiveness on T2I-CompBench and MagicBrush."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. Experiments are extensive for two benchmarks (T2I-CompBench, MagicBrush)."}, "weaknesses": {"value": "1. My major concern is about the lack of novelty: There are many other multi-agent works for T2I/T2V generation that share similar ideas (GenArtist, MovieAgent, MAViS). \n[A] Wang Z, Li A, Li Z, et al. Genartist: Multimodal llm as an agent for unified image generation and editing[J]. Advances in Neural Information Processing Systems, 2024, 37: 128374-128395.\n[B] Wu W, Zhu Z, Shou M Z. Automated movie generation via multi-agent cot planning[J]. arXiv preprint arXiv:2503.07314, 2025.\n[C] Wang Q, Huang Z, Jia R, et al. MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling[J]. arXiv preprint arXiv:2508.08487, 2025.\n\n2. The method relies heavily on the SoTA LLM, which will not generalize to less capable LLMs. \n\n3. The refinement process will introduce additional time cost."}, "questions": {"value": "1. What are the differences among the multi-agent works (GenArtist, MovieAgent, MAViS)? \n2. What are the performances of open-source models? \n3. What are the time/money costs for each agent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HKX5ns4Xt3", "forum": "LHysuZBJSu", "replyto": "LHysuZBJSu", "signatures": ["ICLR.cc/2026/Conference/Submission18695/Reviewer_Gzjq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18695/Reviewer_Gzjq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926360621, "cdate": 1761926360621, "tmdate": 1762928397516, "mdate": 1762928397516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MAC-TIGER, a multi-agent prompt refinement framework for text-to-image (T2I) generation and editing. The system consists of three roles: Reviewer, Challenger, and Refiner, which iteratively critique and improve prompts before sending them to a diffusion model. The method is training-free and model-agnostic. Experiments on T2I-CompBench and MagicBrush show improved compositional correctness and editing fidelity over standard prompting and simple self-refinement loops. Ablations demonstrate the importance of each agent's role and iteration count."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear and modular tri-agent design that maps to real T2I failure patterns (missing entities, incorrect relations, vague prompts).\n\nTraining-free and model-agnostic pipeline, compatible with various diffusion and editing models.\n\nDemonstrated improvements on compositional benchmarks and instruction-guided editing tasks."}, "weaknesses": {"value": "The paper does not contrast well with other multi-agent T2I frameworks. I'm not clear what the main contribution of this paper compared to them. \n\nMissing comparison to recent multi-agent T2I systems, which substantially weakens novelty claims. Relevant works include:\n\n1. GenArtist (NeurIPS 2024) - MLLM-driven multi-stage generation & editing\n\n2. Proactive Agents (ICML 2025) - interactive multi-turn T2I refinement\n\n3. Anywhere (AAAI 2025) - multi-agent foreground-conditioned generation\n\n4. MCCD (CVPR 2025) - multi-agent compositional diffusion\n\nLimited discussion of computational overhead, prompt token budget, inference latency, and API cost.\n\nMostly automatic metrics; lacks rigorous human preference study."}, "questions": {"value": "Please add a direct comparison with other multi-agent T2I baselines (e.g., GenArtist or MCCD).\n\nProvide scaling analysis: performance vs. number of iterations and prompt length.\n\nReport cost/latency per iteration and total token usage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gkcKTKMTQB", "forum": "LHysuZBJSu", "replyto": "LHysuZBJSu", "signatures": ["ICLR.cc/2026/Conference/Submission18695/Reviewer_qT1F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18695/Reviewer_qT1F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983242310, "cdate": 1761983242310, "tmdate": 1762928396416, "mdate": 1762928396416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a multi-agent framework for text-to-image generation. Particularly, it designs three agents (based on GPT-4o) that refine the generated results iteratively: Given an initial synthesized image, a Reviewer is used to rate the image. After that, a Challenger receives the feedback from the Reviewer and finds unsatisfied objectives. Subsequently, the outputs of the reviewer and the challenger are fed into a refiner to generate more specific prompts. The proposed method is evaluated through T2I generation and editing on two public benchmarks, which improves the performance of SDXL and DALL-E 3."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Three agents are designed to cooperate and facilitate image editing. All of them employ off-the-shelf models directly; hence, the proposed method can be implemented easily. \n\n2. Curated prompts are designed to analyze and refine user instructions.\n\n3. The proposed method supports both T2I generation and editing."}, "weaknesses": {"value": "1. The technical contributions of the proposed framework are limited. Essentially, there is no learning algorithm or optimizable module in the proposed framework to ensure that the final results can be improved. In fact, mainstream methods for this topic are exploring reinforcement learning (like GRPO) and unified models for multimodal understanding and generation.\n\n2. From the quantitative comparison and the ablation study, the performance gain of the proposed method is marginal.\n\n3. The proposed method relies on multiple large models (e.g., GPT-4o), causing substantial computational cost."}, "questions": {"value": "1. How does the base model of the agents affect the performance? Has any open-source model (e.g., QwenVL) been tested?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mkqYgp3zAS", "forum": "LHysuZBJSu", "replyto": "LHysuZBJSu", "signatures": ["ICLR.cc/2026/Conference/Submission18695/Reviewer_YkWB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18695/Reviewer_YkWB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992388246, "cdate": 1761992388246, "tmdate": 1762928394678, "mdate": 1762928394678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Mac-Tiger, a multi-agent cooperation framework designed to enhance text-to-image (T2I) generation, particularly for complex prompts that challenge existing models with attribute binding, spatial relationships, and numerical precision. The framework employs a tri-agent system—comprising a Reviewer, a Challenger, and a Refiner—powered by Multimodal Large Language Models (MLLMs). These agents collaborate through iterative feedback loops to evaluate and optimize text prompts, thereby improving the final generated image's quality and alignment with the initial request. The authors demonstrate the framework's effectiveness by applying it to state-of-the-art models like SDXL and DALL-E 3 and evaluating its performance on the T2I-CompBench and MagicBrush benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-Defined Problem: The paper addresses a significant and well-recognized limitation in the T2I field: the difficulty of current models in handling compositional complexity. The motivation is clear and targets a critical area for advancement.\n\n2. Novel Conceptual Framework: The application of a structured, multi-agent system to the problem of T2I prompt optimization is a novel idea. The division of labor among the Reviewer, Challenger, and Refiner roles provides a systematic and interpretable approach to iterative prompt refinement.\n\n3. Strong Empirical Results: The quantitative results are a key strength. The framework consistently improves the performance of powerful foundation models (SDXL, DALL-E 3) across all categories of the challenging T2I-CompBench benchmark. This demonstrates the practical effectiveness of the proposed method.\n\n4. Methodological Generality: The framework is shown to be effective not only for T2I generation but also for text-guided image editing, improving the performance of models like InstructPix2Pix and HIVE on the MagicBrush benchmark. This suggests the approach is versatile and not limited to a single task."}, "weaknesses": {"value": "1. While the framework is novel, its core is essentially a sophisticated prompt engineering pipeline that orchestrates existing, off-the-shelf models (GPT-4V as the agent brain, mPLUG-large as the VQA tool). The paper does not address the deeper technical challenges of how to guarantee diverse, non-redundant, and insightful feedback from the agents. The justification for a three-agent system over a simpler two-agent (e.g., critic-refiner) or single-agent self-correction loop is not rigorously established. The ablation study shows each component is helpful but does not prove this specific architecture is optimal.\n\n2. The entire iterative optimization process is guided by a quantitative score from the Reviewer's VQA module. However, this introduces a major point of failure. VQA models themselves are known to be fallible, especially on the same complex compositional queries (e.g., counting, nuanced spatial relations) that the T2I system is trying to solve. The paper treats the VQA score as ground truth without providing any analysis of the VQA model's own accuracy on this task or discussing how the system would handle incorrect feedback. This is a critical technical oversight.\n\n3. Figure 4 shows that performance peaks at 3 iterations and then begins to decline. The authors' explanation—blaming LLM limitations and coordination inefficiencies—is superficial and insufficient. This phenomenon points to a core technical problem, possibly related to prompt overfitting or the feedback loop becoming counter-productive. The paper fails to analyze the root cause of this degradation or propose a mechanism to mitigate it, which is a significant weakness.\n\n4. Unfair Comparison and Lack of Cost Analysis: The experiments compare a baseline model's single inference pass (e.g., \"SDXL\") with the full Mac-Tiger system (\"Mac-Tiger + SDXL\"), which involves multiple rounds of expensive GPT-4V calls, VQA model inferences, and T2I generations. This is not a fair comparison. The paper completely omits any discussion of the substantial overhead in terms of latency, computational cost, and API fees. This makes it impossible to assess the method's practical utility. A more rigorous baseline would involve a single-agent iterative method with a comparable computational budget.\n\n5. For a generative task where the output is visual and subjective, relying exclusively on automated metrics is insufficient. A critical missing piece is a human evaluation study. Do human evaluators agree that the images produced by Mac-Tiger are of higher quality and better follow the prompt's instructions compared to the baseline? Without this, the claims of \"higher-quality\" and \"more context-aware\" generation are not fully substantiated.\n\n6. The ablation study is too limited. It confirms that removing the Reviewer or Challenger hurts performance but fails to answer more critical questions. An ablation comparing the tri-agent setup to a dual-agent (combining Reviewer and Challenger) or a single-agent self-critique loop is necessary to justify the architectural complexity. Additionally, the sensitivity of the system to the choice of the VQA model or the specific prompting templates used for the agents is not explored."}, "questions": {"value": "1. Could you provide a deeper analysis of why performance declines after three iterations? Have you analyzed the prompts from later iterations to identify if they become overly constrained, verbose, or contradictory? Is there a principled mechanism to automatically terminate the process to avoid this \"over-optimization\"?\n\n2. How do you address the fact that your core evaluation signal from the mPLUG-large VQA model may be inaccurate? Have you measured the VQA model's accuracy on the T2I-CompBench evaluation set itself? How does the system ensure it is not optimizing towards an incorrect objective set by a fallible VQA model?\n\n3. Can you provide stronger evidence to justify the necessity of having separate Reviewer and Challenger agents, as opposed to a single, more capable \"Evaluator-Critic\" agent? What specific failure modes does the Challenger address that the Reviewer consistently misses?\n\n4. Can you quantify the computational overhead of your framework? For a typical run of 3 iterations, what is the increase in cost (in terms of LLM/VQA API calls, GPU hours, and wall-clock time) compared to a single inference from the baseline T2I model?\n\n5. Have you considered conducting a human preference study to validate your results? How can you be certain that the improvements in automated metrics translate into a meaningfully better experience and output quality for a human user?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1BBR9tisDI", "forum": "LHysuZBJSu", "replyto": "LHysuZBJSu", "signatures": ["ICLR.cc/2026/Conference/Submission18695/Reviewer_6QU6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18695/Reviewer_6QU6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997028019, "cdate": 1761997028019, "tmdate": 1762928394117, "mdate": 1762928394117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}