{"id": "T7jNygNj82", "number": 7066, "cdate": 1758006506860, "mdate": 1759897874463, "content": {"title": "Distribution-Aware Synergistic Evolution for Few-shot Discrimination and Generation", "abstract": "Discrimination and generation are two distinct yet complementary paradigms in machine learning.\nGenerally, discriminative models are better at estimating the $\\textit{class center}$, while generative models are better at modeling the $\\textit{data variance}$.\nTo harness the strengths of both paradigms, we propose a synergistic evolution framework that allows discriminative and generative methodologies to cooperate in estimating feature distributions.\nFor one thing, the discriminative model incorporates synthetic samples from the generative model to improve the estimation of feature covariance, especially when the available data is limited.\nFor another, the generative model leverages calibrated class centers from the discriminative pathway as anchors to improve the semantic accuracy of the generated samples.\nIn summary, our framework enables the discriminative model and the generative model to jointly develop and collaborate within a few-shot learning scenario, thereby enhancing both of their individual capabilities.\nAdditionally, our design improves open-set learning by enhancing out-of-distribution detection through better covariance modeling in the discriminative space. \nExtensive experiments on the CUB-200 and miniImageNet datasets demonstrate performance gains in few-shot class-incremental learning (FSCIL), few-shot incremental generation (FSIG), and open-set recognition tasks.", "tldr": "", "keywords": ["Few-Shot Class Incremental Learning", "Few-shot Image Generation", "Open-set Learning", "Stable Diffusion"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/854912ac07d590bd91200d8941e5675674211f8a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "1.  Uses generative samples to refine covariance estimation, improving few-shot class-incremental learning (FSCIL) and open-set recognition (OSR).\n\n2. Leverages calibrated class prototypes from the discriminator to enhance semantic information in few-shot image generation\n\n3. Enables mutual reinforcement between discrimination and generation, outperforming baselines on CUB200 and miniImageNet"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel bidirectional interaction between discriminative and generative models\n\n2. Introduces calibrated prototypes for generation and covariance refinement via synthetic samples\n\n3. Rigorous experiments on FSCIL, OSR, and FSIG, with ablation studies validating each component\n\n4. Unifies traditionally disjoint paradigms, enabling applications like continual learning with generative feedback"}, "weaknesses": {"value": "1. Experiments limited to Stable Diffusion 1.5 + LoRA. Larger diffusion models or non-CLIP backbones are untested.\n\n2. Diagonal covariance ignores cross-feature correlations. A low-rank or sparse approximation could better capture structure.\n\n3. No analysis of adversarial robustness such as distribution shifts."}, "questions": {"value": "1. How does DASE perform with larger diffusion models or non-CLIP backbones?\n\n2. Could structured (e.g., block-diagonal) covariance improve discrimination without overfitting?\n\n3. How does OSR performance degrade under domain shifts (e.g., CUB200 → iNaturalist)?\n\n4. Is there any theoretical support for the arguments presented in the abstract? \"Generally, discriminative models are better at estimating the class center, while generative models are better at modeling the data variance.\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h1EZZ9h6C0", "forum": "T7jNygNj82", "replyto": "T7jNygNj82", "signatures": ["ICLR.cc/2026/Conference/Submission7066/Reviewer_7Td4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7066/Reviewer_7Td4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761471929875, "cdate": 1761471929875, "tmdate": 1762919256331, "mdate": 1762919256331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified framework called Distribution-Aware Synergistic Evolution (DASE) to integrate discriminative and generative learning for few-shot class-incremental learning (FSCIL), open-set recognition (OSR), and few-shot image generation (FSIG). It models each class as a Gaussian distribution in CLIP feature space and uses calibrated means and variances for both classification and generation. The approach consists of two phases: an initialization phase where visual prototypes are calibrated using base class statistics and text guidance, and a synergy phase where classifier and generator iteratively refine each other. Experiments on CUB-200 and miniImageNet demonstrate improvements across classification, out-of-distribution detection, and generative quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Presents a unified framework that leverages distribution-aware Gaussian modeling in CLIP feature space to support few-shot classification, open-set recognition, and image generation within a shared probabilistic structure."}, "weaknesses": {"value": "1. The reliance on diagonal Gaussian assumptions, while practical for few-shot settings, may oversimplify class feature distributions.\n2. The method depends heavily on heuristic filtering of synthetic data and text-guided prototype calibration, yet lacks a detailed ablation or sensitivity analysis to understand the robustness of these design choices."}, "questions": {"value": "1. How sensitive is the overall performance to the quality of the synthetic samples selected during Phase II? Would the model degrade significantly if lower-quality generations were mistakenly included?\n2. Have the authors considered or tested more expressive distribution models beyond diagonal Gaussians, such as full covariances, low-rank approximations, or mixture models? If so, what were the tradeoffs?\n3. Could the text-guided calibration process introduce bias if semantic similarity does not align with visual similarity? How robust is the calibration step when text embeddings are noisy or ambiguous?\n4. In the synergy phase, how often is feedback exchanged between the generator and classifier, and how does this frequency affect convergence and performance?\n5. Does the method generalize well to non-fine-grained datasets or other domains (e.g., medical, synthetic imagery), or is it limited by assumptions baked into CLIP and Stable Diffusion’s pretraining?\n6. Could the authors provide a more comprehensive ablation study that isolates the contributions of the synergy phase—specifically the generator-to-discriminator feedback via synthetic data and the discriminator-to-generator guidance via calibrated prototypes—across all three tasks (FSCIL, OSR, FSIG), as these components appear central to the claimed performance gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eQfMx9HaZq", "forum": "T7jNygNj82", "replyto": "T7jNygNj82", "signatures": ["ICLR.cc/2026/Conference/Submission7066/Reviewer_nbJQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7066/Reviewer_nbJQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525705899, "cdate": 1761525705899, "tmdate": 1762919255943, "mdate": 1762919255943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified framework called Distribution-Aware Synergistic Evolution (DASE) that integrates discriminative and generative paradigms for few-shot learning. The key idea is to enable mutual reinforcement between a CLIP-based discriminator and a  Diffusion-based generator. The discriminative pathway refines class mean and covariance estimation by incorporating generated samples, while the generative pathway uses calibrated class prototypes from the discriminator as semantic anchors to enhance the fidelity of synthesized images. The framework is applied to three tasks—few-shot class-incremental learning (FSCIL), open-set recognition (OSR), and few-shot image generation (FSIG)—demonstrating moderate improvements in accuracy, AUROC, and FID on CUB200 and miniImageNet. Overall, the paper aims to bridge the gap between discrimination and generation in few-shot scenarios through iterative distribution calibration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper’s structure and writing are clear, making the method easy to follow. Figures and tables are properly formatted, and the overall presentation is neat.  \n2. The research problem itself is meaningful and relevant. Exploring how discriminative and generative models can mutually benefit each other under few-shot conditions addresses a long-standing challenge in vision research, and the proposed bidirectional feedback idea has potential value for future work on hybrid learning frameworks."}, "weaknesses": {"value": "1. The literature review is incomplete. Many recent works in few-shot image generation are not discussed, such as ADAM [1], RICK [2], and GenDA [3], as well as several key few-shot class-incremental learning methods from the past two years.\n2. The experimental design is limited. Most comparisons are only comparing with the variants of the proposed method. For example, in Table 3, the method is evaluated on few-shot image generation but without comparing to any existing FSIG methods. Furthermore, well-known diffusion-based personalization methods like DreamBooth and Textual Inversion are mentioned in the related work but not included in experiments. This is important, since both CLIP and Stable Diffusion are pre-trained on massive paired datasets, and their inherent generalization ability could overshadow the claimed contribution of the proposed method.  \n3. The analysis of the experimental results is not convincing. In Figure 2, the visual differences between the proposed method and the baseline are not significant; some images appear almost identical, and only a few qualitative examples are shown, making the analysis not statistically supported.  \n4. There are minor typographical errors. For instance, the abstract ends with “few-shot incremental generation (FSIG),” but based on the paper’s context, it should be “few-shot image generation.”\n\n[1] Few-shot Image Generation via Adaptation-Aware Kernel Modulation\n\n[2] Exploring Incompatible Knowledge Transfer in Few-shot Image Generation\n\n[3] FEW-SHOT CROSS-DOMAIN IMAGE GENERATION VIA INFERENCE-TIME LATENT-CODE LEARNING"}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Na8sAaT47a", "forum": "T7jNygNj82", "replyto": "T7jNygNj82", "signatures": ["ICLR.cc/2026/Conference/Submission7066/Reviewer_iTVM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7066/Reviewer_iTVM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943117022, "cdate": 1761943117022, "tmdate": 1762919255329, "mdate": 1762919255329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Distribution-Aware Synergistic Evolution (DASE), an approach designed to unify few-shot discrimination and generation tasks. The core idea is to establish a bidirectional, mutually beneficial relationship between a discriminative and a generative pathway. The framework operates in two phases: (I) The discriminative model calibrates the prototype for new few-shot classes, which then serves as a semantic anchor to guide the generative model in producing higher-fidelity images. (II) The discriminative model, in turn, leverages the synthetic samples from the generator to estimate a more robust feature covariance matrix, addressing the unreliability of covariance estimation from scarce data. Experiments demonstrate that this synergistic loop enhances performance across Few-Shot Class-Incremental Learning, Open-Set Recognition, and Few-Shot Image Generation on the CUB-200 and miniImageNet datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed framework is technically plausible and is supported by empirical validation."}, "weaknesses": {"value": "1. The abstract makes a strong assertion that “discriminative models are better at estimating the class center, while generative models are better at modeling the data variance”. However, this claim lacks sufficient theoretical justification or citations from prior literature.\n2. The method section does not present any explicit loss or objective function; the algorithm is only described procedurally, which weakens its mathematical rigor.\n3. Experiments are conducted only on miniImageNet and CUB-200, lacking more challenging benchmarks, which limits the assessment of generalization.\n4. The paper omits comparisons with recent state-of-the-art methods in open-set recognition and few-shot image generation, making it difficult to evaluate the competitiveness of the proposed approach.\n5. The method introduces several critical hyperparameters, yet the paper provides no discussion or sensitivity analysis regarding their selection, which reduces reproducibility."}, "questions": {"value": "1. The paper states that the calibrated visual prototype $\\mu_c$ is injected into Stable Diffusion by replacing the $t_{EOS}$ and $t_{PAD}$ embeddings in the CLIP text encoder. This is a rather unusual and insufficiently justified design choice. The authors should provide ablation studies or theoretical reasoning to support this key component.\n2. In Section 5.1, the authors mention using a “class-specific prior preservation loss” during LoRA training on the CUB-200 dataset, but not on miniImageNet. Why is this regularization applied to only one dataset? This inconsistency makes the comparison between datasets unfair and undermines the claimed generality of the method.\n3. In Table 1, the $A_{last}$ score of DASE (Phase I) on CUB-200 is reported as 78.51%, whereas in Table 4, the corresponding method (“Calibrated Class Distribution”) achieves 78.15%. These two entries appear to represent the same experiment (the discriminative Phase I only), yet the results are inconsistent. Please clarify this discrepancy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YDxly904qC", "forum": "T7jNygNj82", "replyto": "T7jNygNj82", "signatures": ["ICLR.cc/2026/Conference/Submission7066/Reviewer_rDSf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7066/Reviewer_rDSf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016013356, "cdate": 1762016013356, "tmdate": 1762919254332, "mdate": 1762919254332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}