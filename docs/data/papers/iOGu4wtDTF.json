{"id": "iOGu4wtDTF", "number": 4258, "cdate": 1757648388882, "mdate": 1763625276277, "content": {"title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs", "abstract": "Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by on the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework, with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to cater to diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting that speeds up the computation by 20×, quantization-aware singular value decomposition (SVD) decompositions to minimize the quantization errors, state-aware weight sorting for SSMs, and a fused rotary embedding (RoPE) kernel for the pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a one-shot fashion, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models offer a memory reduction of 4×–5.7× and a token throughput improvement of 2.7×–3.4×, maintaining accuracy within 5% of the original models at 15% pruning rates across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models will be released at: https://github.com/.", "tldr": "UniQL is designed as a general framework that integrates quantization and pruning for Transformers, State Space Models (SSMs), and hybrid models to cater to diverse edge applications.", "keywords": ["large language model", "model quantization", "model compression"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2eae429a7bf9b0bbb8a22312341cf7c9a128a6a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper targets dynamic, resource-constrained edge deployment of LLMs. It proposes a one-shot, post-training pipeline that produces a single INT4 model which can be adaptively pruned on-device at different rates depending on runtime constraints. Overall, the paper is a well-executed systems integration that addresses a timely deployment problem and demonstrates tangible engineering wins. However, for ICLR, the lack of clear algorithmic novelty and the absence of a rigorous case for “why combine pruning with quantization instead of simply going lower-bit” (especially W3) are significant gaps. Clarifying the scope of Hadamard/rotation, aligning it with pruning, and adding stronger/fairer baselines and ablations would strengthen the submission."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Practicality and scope: A unified, one-shot post-training pipeline that supports Transformers, SSMs, and hybrids, and enables on-device adaptive pruning without retraining is very relevant for edge deployment.\n2. Solid engineering: the fused RoPE kernel, quantization-aware SVD, and avoidance of pseudo-inverses show careful engineering; the export path quantizes even embeddings/LM head to 4-bit, reducing footprint beyond common W4A16 libraries."}, "weaknesses": {"value": "1. Most core techniques (importance-driven structured pruning, Hadamard/rotation-based quantization smoothing, W4 PTQ) are known. The main novelty is system integration and some engineering refinements (QSVD, fused RoPE, SSM-aware sorting). This level of originality feels marginal for ICLR unless paired with stronger conceptual/theoretical advances or broader evidence.\n\n2. No justification for “quantization + pruning” vs “lower-bit quantization alone”: The paper does not compare W4 + k% pruning to a lower-bit alternative (e.g., W3) under matched memory/latency. Prior results (e.g., QuaRot with 3-bit) suggest that W3 may match or exceed the accuracy of W4 + 25% pruning at similar or lower budgets. Without this key comparison, the necessity of combining pruning with quantization remains unproven.\n\n3. Weak PTQ baselines : PTQ comparisons are largely against framework built-ins (TRT-AWQ, TAO-HQQ) and a basic GPTQ variant; missing stronger recent baselines such as AWQ/QServe, Quarot/SpinQuant, etc.\n\n4. The scope of rotations is not clearly specified: If rotations are applied on input channel axes of layers whose channels will later be pruned (e.g., O_proj or MLP down-proj input channels), this mismatch can harm pruning efficacy. There is no ablation on restricting rotations to non-pruned axes or aligning pruning boundaries with quantization/rotation groups."}, "questions": {"value": "1. Please provide matched-budget comparisons of W4 + {25}% pruning versus W3 (with Hadamard/rotation, e.g., QuaRot-style) on the same models, reporting accuracy–memory Pareto curves. Under what budgets does “W4 + pruning” strictly dominate “W3 alone”?\n\n2. Which layers/axes are rotated (Q/K/V/O proj in attention; MLP up/gate/down; SSM B/C/Z/X/O)? Are rotations applied to channel dimensions that will be pruned on-device?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YMg6SXhxzA", "forum": "iOGu4wtDTF", "replyto": "iOGu4wtDTF", "signatures": ["ICLR.cc/2026/Conference/Submission4258/Reviewer_ZDeG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4258/Reviewer_ZDeG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761206458899, "cdate": 1761206458899, "tmdate": 1762917259340, "mdate": 1762917259340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniQL, a unified post-training quantization and low-rank compression framework designed to support adaptive deployment of large language models on resource-constrained edge devices. UniQL achieves integrated compression for Transformer, SSM, and hybrid models through structured weight ranking, quantization-aware SVD decomposition, state-aware SSM compression, and the integration of RoPE kernels. The framework performs one-time compression in the cloud and supports dynamic on-device pruning (up to 35%) based on current load. Experiments demonstrate that UniQL achieves significant memory reduction and inference acceleration across multiple models and hardware platforms, with manageable accuracy loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Wide Model Architecture Coverage: This approach systematically supports post-training quantization and structured pruning for Transformer, SSM, and hybrid models for the first time.\n\nStrong On-Device Adaptability: This approach dynamically adjusts model size based on memory and compute resources after deployment to adapt to the dynamic load of edge devices.\n\nHigh Compression Efficiency: This approach significantly accelerates the compression process (up to 22x faster than MoDeGPT) by avoiding pseudo-inverses and optimizing SVD decomposition.\n\nSystem-Level Optimization: This approach integrates RoPE kernels and quantization-aware decomposition to significantly improve inference speed and reduce quantization error.\n\nExtensive Experimentation: This approach demonstrates its effectiveness and versatility across a variety of models (e.g., Llama, Qwen, Mamba) and hardware (e.g., A6000, Nano 8G)."}, "weaknesses": {"value": "Accuracy-compression tradeoff not yet optimal: At high pruning rates (e.g., 35%), the accuracy of some models (e.g., Mamba2) drops significantly (down to 57.7%).\n\nSensitive to calibration data: Structured ordering and quantization depend on the calibration set; their robustness to different data distributions is not analyzed.\n\nLack of comparison with unstructured methods: No comparison with popular unstructured pruning methods (e.g., SparseGPT) or hybrid sparse methods is provided.\n\nDevice-side pruning overhead not quantified: While device-side pruning is supported, its runtime overhead (e.g., memory rearrangement, index lookup) is not analyzed.\n\nLimited interpretability: No visualization or interpretable analysis is provided for the \"state-aware\" or \"quantization-aware\" mechanisms."}, "questions": {"value": "Calibration Data Sensitivity:\nHow sensitive is UniQL to the choice of calibration data? Have you tested its robustness across domains (e.g., code, math, dialogue) or with out-of-distribution samples?\n\nComparison with Unstructured Pruning:\nHow does UniQL compare with unstructured pruning methods like SparseGPT, especially in terms of accuracy-efficiency trade-offs and hardware friendliness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z0gPddUpja", "forum": "iOGu4wtDTF", "replyto": "iOGu4wtDTF", "signatures": ["ICLR.cc/2026/Conference/Submission4258/Reviewer_8Vsv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4258/Reviewer_8Vsv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635212099, "cdate": 1761635212099, "tmdate": 1762917259067, "mdate": 1762917259067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified quantization and on-device structural pruning method for edge LLMs, including Transformers, State Space Models (SSMs), and hybrid models. To enable the on-device structural pruning, weight sorting is designed for different model blocks, and a LoRA-based recovery fine-tuning (FT) is conducted on the sorted model. 4-bit quantization and pruning with different rates are applied to the model. SVD decomposition is used to reduce quantization error. The evaluation presents different pruning rates that offer a memory reduction of 4x–5.7×."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The adaptive LLM memory problem discussed in this paper is important and interesting.\n2. The proposed methods are evaluated on different model structures."}, "weaknesses": {"value": "1. The contribution is limited, and the proposed methods are very incremental.\na) Methods applied to different model structures appear more as a systematic engineering effort than a novel algorithmic advancement.\nb) The quantization, pruning combination has been explored.\n\n2. Insufficient Empirical Evaluation.\na) The paper claims adaptive deployment of LLMs on the edge, but there is no real deployment with different workloads. Crucially, the paper does not address the core systems challenge: how the memory footprint can be dynamically managed at runtime without first loading the entire unpruned model.\nb) Unignored performance drop. The reported performance drop is substantial, especially at lower pruning rates (e.g., a 6% drop for Llama2-7B at only 35% pruning).\nc) Lack of comparison with related works: Quantization or pruning methods for SSM and hybrid models. Structure pruning methods, such as SliceGPT [1] .\n\n3. The paper writing should be improved.\na) Explain why you do it before the detailed introduction. For example, the reason for weight sorting should be given in the introduction. \nb) Too many mathematical symbols make the methods part hard to follow. The algorithms in the Appendix are better for understanding.\nc) All of the components should be included in the overview Figure.\n\n[1] Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari Do Nascimento, Torsten Hoefler, James Hensman. SliceGPT: Compress Large Language Models by Deleting Rows and Columns. ICLR 2024"}, "questions": {"value": "1. The finetune is applied after the weight sorter. Why does it apply after quantization? Have you experimented with different orders of the applied methods?\n2. The pruning method is similar to SliceGPT [1]. What’s the key difference between SliceGPT and the proposed sorting method?\n\n[1] Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari Do Nascimento, Torsten Hoefler, James Hensman. SliceGPT: Compress Large Language Models by Deleting Rows and Columns. ICLR 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0NDuIKxpYS", "forum": "iOGu4wtDTF", "replyto": "iOGu4wtDTF", "signatures": ["ICLR.cc/2026/Conference/Submission4258/Reviewer_oUGV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4258/Reviewer_oUGV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937077374, "cdate": 1761937077374, "tmdate": 1762917258800, "mdate": 1762917258800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces UniQL, a unified post-training compression framework designed to efficiently deploy large language models (LLMs) on edge devices. It integrates structured pruning and quantization in a single-shot pipeline that supports adaptive on-device compression based on real-time resource availability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- Unified Framework: UniQL supports Transformers, State Space Models (SSMs), and hybrid architectures, addressing a wide range of LLM structures.\n\n- On-device Adaptive Pruning: Enables users to prune the model at inference time based on the current device memory state."}, "weaknesses": {"value": "- While the results are generally strong, some inconsistencies exist in the evaluation setup:\nIn Tables 1 and 2, the latency results for different models and methods are evaluated on different hardware platforms (Llama-3.1-8B and Nemotron-H-8B on A6000; Qwen-2.5-7B and Mamba2-8B on Nano 8G). Additionally, Table 2 lacks baseline comparisons such as TRT-AWQ for some models. This raises concerns about the consistency and comparability of latency evaluations across models and methods. Can the authors clarify why all models and methods are not evaluated uniformly across both platforms, and whether such comparisons are fair and meaningful under these mixed settings?\n\n- UniQL is compared to SVD-LLM both with and without fine-tuning, which is helpful. However, the comparison with MoDeGPT is conducted only without fine-tuning, despite UniQL including fine-tuning in its best-performing configuration. Furthermore, all comparisons are conducted at only one sparsity level (15%), which limits the ability to assess how robust each method is across different compression regimes (e.g., 25%, 35%).\n\n- In Table 7, UniQL is evaluated under single-pass adaptive pruning across multiple pruning rates and compared only with SVD-LLM. However, MoDeGPT, another key baseline used in Table 5 and throughout the paper, is not included."}, "questions": {"value": "- With masked fine-tuning (FT), UniQL remains faster (6h 59 m) than both MoDeGPT (7h 03 m) and SVD-LLM (15h 57 m). I do not understand why masked fine-tuning would be faster — could the authors clarify the reason behind this behavior?\n\nI am open to discussing this further during the rebuttal and will be happy to increase my score if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5QHEGPcmXZ", "forum": "iOGu4wtDTF", "replyto": "iOGu4wtDTF", "signatures": ["ICLR.cc/2026/Conference/Submission4258/Reviewer_Duby"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4258/Reviewer_Duby"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951990606, "cdate": 1761951990606, "tmdate": 1762917258561, "mdate": 1762917258561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response"}, "comment": {"value": "We thank all reviewers for their time and effort in evaluating our manuscript. We appreciate the thoughtful feedback, which has helped us further refine and strengthen the work, and we have revised the manuscript accordingly. Reviewer comments are highlighted in yellow, and newly added content is marked in blue. Below, we summarize the key contributions of our work.\n\n## Novelty and contributions (Reviewer oUGV and ZDeG)\n\nTo the best of our knowledge, our work is the first to: (1) address flexible model compression under a novel unified-memory setting for edge devices, (2) systematically combine quantization and structured pruning with several new algorithms, and (3) provide a unified framework applicable to most popular model architectures. \n\nWe present a novel compression framework that enables flexible n-bit (i.e., 3, 4-bit) models for edge devices with unified memory, applicable to Transformers, State Space Models (SSMs), and hybrid architectures. We introduce several new algorithmic components: a pseudo-inverse-free, quantization-aware SVD, state-aware weight sorting, and a fused rotary embedding kernel, all specifically designed for the proposed unified framework. \n\nTo clarify our contributions, we present a comparison in Tables R7 and R19 to help reviewers better understand our contributions below.\n\n## Baselines, latency and ablation study (Reviewer Duby, oUGV, 8Vsv and ZDeG)\nDue to the page limit at submission time, we included only the most representative experiments and strongest baselines to demonstrate our core idea. In response to the reviewers’ feedback, we have added and updated all requested experiments and comparisons, which substantially strengthen the work. All additional experimental tables introduced in the rebuttal are labeled with the prefix “R” (e.g., Table R1, Table R2) as enumerated below, while tables without the “R” prefix refer to those already included in the main manuscript.\n\n## List of table used in the rebuttal\n- Table R1: Full latency profiling on A6000 in ms (1k+1k)\n- Table R2: Full latency on Nano 8G in ms (256+256)\n- Table R3: Full evaluation results without finetuning\n- Table R4: Full evaluation results with finetuning\n- Table R5: Full evaluation results with finetuning and 4-bit quantization\n- Table R6: Pseudo-inverse latency\n- Table R7: UniQL compared to prior pruning methods\n- Table R8: Latency evaluated at batch size = 1 with 512 prefill and 512 decode tokens in millisecond (ms)\n- Table R9: Latency evaluated at batch size = 1 with 128 prefill and 512 decode tokens in millisecond (ms)\n- Table R10: Latency evaluated at batch size = 1 with 512 prefill and 128 decode tokens in millisecond (ms)\n- Table R11: Comparing accuracy at 35% pruning rate\n- Table R12: Comparison accuracy with SliceGPT\n- Table R13: Ablation study on calibration sets\n- Table R14: Comparison latency with SliceGPT\n- Table R15: Ablation study on calibration sets\n- Table R16: Evaluation results on the MBPP+ coding tasks\n- Table R17: Perplexity comparison with semi-structured pruning\n- Table R18: Speedup comparison with semi-structured pruning\n- Table R19: UniQL compared to prior quantization methods\n- Table R20: Experimental results of 3-bit UniQL\n- Table R21: Comparison between weight-only and weight-activation quantization\n- Table R22: Generation throughput comparison for Llama-2-7B on A100\n- Table R23: Accuracy comparison for Llama-2-7B across quantization methods\n- Table R24: Hadamard matrix fusion for Transformer blocks\n- Table R25: Hadamard matrix fusion for Mamba blocks\n\n## List of modification of our manuscript\n- Add pseudo-inverse latency in Table 1 (reviewer Duby)\n- Highlight line 158-159 and add brief intuitions (reviewer oUGV)\n- Highlight line 197-203 (reviewer Duby)\n- Highlight line 248-249 and 253-254 (reviewer 8Vsv)\n- Highlight line 278-281 (reviewer 8Vsv)\n- Append 25% pruning rate to Table 2 (reviewer Duby)\n- Add “weight-only” in line 416 and Table 3 caption (reviewer ZDeG)\n- Add 4-bit MoDeGPT with 15% and 25% pruning rates, and SVD-LLM with 25% pruning rate in Table 4 (reviewer Duby) \n- Add “Additional ablation studies can be found at Appendix C.” (reviewer Duby, oUGV, 8Vsv)\n- Reorganize our experiment sections for better readability (reviewer oUGV)\n- Add B.1 COMPARISON WITH ADDITIONAL BASELINES (reviewer oUGV)\n- Add B.3 EVALUATION ON CODING TASKS (reviewer 8Vsv)\n- Add C.1 ABLATION STUDY ON CALIBRATION SETS (reviewer 8Vsv)\n- Add C.2 ABLATION STUDY ON 3-BIT UNIQL (reviewer ZDeG)\n- Add F.3 HADAMARD TRANSFORM FUSION (reviewer ZDeG)\n- Add G ENERGY PROFILING"}}, "id": "qyyupgq2Rj", "forum": "iOGu4wtDTF", "replyto": "iOGu4wtDTF", "signatures": ["ICLR.cc/2026/Conference/Submission4258/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4258/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission4258/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763625161620, "cdate": 1763625161620, "tmdate": 1763625161620, "mdate": 1763625161620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}