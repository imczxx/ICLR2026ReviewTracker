{"id": "0cmuYj3WeG", "number": 23970, "cdate": 1758351174908, "mdate": 1763634057152, "content": {"title": "PALC: Preference Alignment via Logit Calibration", "abstract": "Aligning Large Language Models with human preferences typically requires computationally intensive training or complex reward architectures. We introduce PALC (Preference Alignment via Logit Calibration), a parameter-efficient framework that achieves test-time alignment through a novel intervention strategy: direct calibration in vocabulary space. Unlike existing methods that manipulate entangled hidden representations or rely on external reward models, PALC operates at the logit layer where each dimension corresponds to a distinct token, providing interpretable and efficient control. Our approach employs a bottleneck architecture that learns to compress the base model's hidden states and generate position-dependent calibration vectors, requiring only a fraction of the base model's parameters. Through this design, PALC sidesteps the superposition problem inherent in representation engineering while eliminating the computational overhead of guided decoding methods. A single scaling factor enables runtime adjustment of alignment strength without retraining, allowing practitioners to balance between preserving model capabilities and enforcing preferences. Experiments demonstrate that PALC outperforms most test-time alignment methods while maintaining near-baseline inference speed. Our ablations reveal that human preferences concentrate on surprisingly low-dimensional manifolds, validating our architectural choices. By establishing vocabulary-space intervention as an effective alignment paradigm, PALC makes preference alignment accessible for resource-constrained deployments where traditional methods are infeasible, opening new avenues for scalable and adaptive AI alignment.", "tldr": "PALC: preference alignment via logit calibration. Learns compact calibrations for frozen LLMs, achieving strong alignment without external rewards or fine-tuning. Outperforms most test-time methods with minimal latency.", "keywords": ["AI alignment", "Representation Editing"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/403372d34832ef570fa54be71e50642d0125268f.pdf", "supplementary_material": "/attachment/ab3c197dccb1547791823cc8e2c69ee61ab15f66.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents PALC (Preference Alignment via Logit Calibration), a novel and lightweight method for preference alignment during LLM inference time. By employing a bottleneck architecture between the final hidden states and output logits, PALC learns to generate position-dependent calibration vectors from hidden states to adjust logits accordingly. Experiments on the HH-RLHF dataset demonstrate that PALC achieves competitive or superior performance compared to several baseline methods while requiring minimal training resources and introducing negligible inference latency. Furthermore, theoretical analysis and empirical ablation studies validate that the bottleneck architecture has sufficient capacity to capture the principal dimensions of human preferences, with the learned manifold concentrating on an effective dimension substantially smaller than the bottleneck size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is intuitive and straightforward to implement, making it accessible for practical adoption.\n- The paper provides thorough optimization and experimental analysis for each component of the method, offering strong empirical support for the design choices."}, "weaknesses": {"value": "- Modest Performance Gains: While PALC demonstrates computational efficiency, the experimental results show limited advantages compared to the base model and several baseline methods. Moreover, the paper does not discuss whether the compared baselines were optimally tuned, raising questions about comparison fairness. That said, given PALC's focus on providing a new methodological direction, some performance trade-offs may be acceptable.\n- Limited Evaluation Scope: Following from the first point, it remains unclear whether this alignment method performs better in some domains while underperforming in others. The evaluation is limited to a single dataset, and the paper lacks qualitative analysis such as case studies, making it difficult to characterize when and where PALC is most effective."}, "questions": {"value": "- Regarding Equation 12 (the power-law assumption for singular values), does this assumption have theoretical justification? Since subsequent theoretical analysis heavily relies on this assumption, why not directly perform singular value decomposition analysis on the trained bottleneck structure to empirically validate it?\n- Other concerns are included in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G4VrxdG51r", "forum": "0cmuYj3WeG", "replyto": "0cmuYj3WeG", "signatures": ["ICLR.cc/2026/Conference/Submission23970/Reviewer_5uCg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23970/Reviewer_5uCg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657493480, "cdate": 1761657493480, "tmdate": 1762942878150, "mdate": 1762942878150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses preference alignment, identifying limitations of both existing training-time and test-time methods. To address this, the paper proposes a method called PALC (Preference Alignment via Logit calibration), which leverages a lightweight, trainable calibration module to intervene directly in the vocabulary logits at inference time. The calibration module uses a low-rank design to reduce parameter size and is trained with DPO loss on a frozen base model, which reduces computational cost and avoids unnecessary drifts. Experiments on the HH-RLHF dataset with the LLaMA 7B SFT model show that PALC performs on par or better than most test-time alignment baselines with lower latency. However, it still underperforms DPO and GenARM, while GenARM requires ~3x latency. Overall, PALC offers a resource-efficient approach for preference alignment, but evaluation on a single dataset and model limits conclusions about its generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly articulates the limitations of existing work, and the proposed logit-space intervention is well-motivated and interesting. \n2. The method is clearly presented, with theoretical claims well supported.\n3. The calibration module is lightweight due to low-rank design, with low training resource required and low inference latency.\n4. Experiments cover a wide range of baselines (training-time and test-time methods), which clearly illustrate the trade-offs between quality and latency."}, "weaknesses": {"value": "1. The experiments are limited to a single model and a single dataset, as noted by the authors. Testing on additional datasets and models would help demonstrate its generalization. For example, how would the method perform on more complicated preference criteria; how would the optimal bottleneck dimension B vary with criteria complexity.\n2. While PALC achieves low latency than GenARM, it underperforms than GenARM by a non-neglible margin. This quality-latency trade-off may not always be acceptable in practice."}, "questions": {"value": "1. How would the model behavior change when \\gamma becomes negative?\n\nnit: line 288 should be \\citet"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nZt6exCo2G", "forum": "0cmuYj3WeG", "replyto": "0cmuYj3WeG", "signatures": ["ICLR.cc/2026/Conference/Submission23970/Reviewer_gcjg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23970/Reviewer_gcjg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795155667, "cdate": 1761795155667, "tmdate": 1762942877947, "mdate": 1762942877947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PALC (Preference Alignment via Logit Calibration), a test-time alignment method that adds a small “calibration module” to generate position-dependent vectors in vocabulary/logit space (rather than hidden states). A single inference-time scalar γ controls alignment strength. On HH-RLHF with a 7B SFT base model, PALC reports higher win rates than several test-time baselines with less latency overhead, while trailing training-time DPO and reward-model methods. The work argues that acting in vocabulary space is interpretable, and is parameter-efficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear intervention point & simplicity. Acting directly on logits is conceptually clean and operationally simple. The $\\gamma$ knob is practical for deployment, enabling a tunable alignment/utility trade-off.\n\n2. Theoretical framing. Provides analysis of SoftMax sensitivity, low-rank structure, and (bounded) KL divergence to the base model; these help justify stability and interpretability claims. \n\n3. Positioning vs. prior work. The paper clearly distinguishes PALC from activation steering and reward-model guided decoding, highlighting an underexplored “vocabulary-space” control point."}, "weaknesses": {"value": "Results are limited to a single dataset (HH-RLHF), a single base model family/size (7B SFT), and 300-prompt evaluations. This is a slim basis for generality, especially for safety-critical alignment claims."}, "questions": {"value": "My only question is that do the authors plan on more robust evaluations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wKzVHP9mCE", "forum": "0cmuYj3WeG", "replyto": "0cmuYj3WeG", "signatures": ["ICLR.cc/2026/Conference/Submission23970/Reviewer_gfFq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23970/Reviewer_gfFq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982008909, "cdate": 1761982008909, "tmdate": 1762942877727, "mdate": 1762942877727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new alignment method that trains a calibration network to modify the $\\text{LLM}$'s output logits directly. The goal is to achieve alignment comparable to full fine-tuning while avoiding the high computational cost and the \"entanglement\" problem inherent in manipulating dense hidden representations (Representation Engineering). The Calibration Network is trained using the DPO objective and operates by reading a compressed signal from the LLM's hidden state, which is then applied as a position-dependent scaling vector to the final logits. Empirical results show superior performance to certain calibration methods but is behind state-of-the-art in terms of quality, which is a deliberate tradeoff for improving training efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method proposes a new apprach that solves the entanglement problem of hidden representations by intervening in the disentangled logit space.\n- The system is highly efficient, as it only trains a small, auxiliary calibration network, which is inherently cheaper than full model backpropagation.\n- The theoretical section demonstrates that alignment is fundamentally a low-rank problem, providing mathematical justification for the lightweight architecture.\n- The method adds only a small overhead during inference and outperforms certain previous approaches in terms of quality such as CAA, Re-control, ARGS."}, "weaknesses": {"value": "- W1. The positioning and motivation of this work does not include several training approaches that achieve competitive quality-efficiency tradeoff in the context of inference-time alignment (e.g. DPO trained tiny auxiliary LLMs or original LLM trained with lightweight, selective PEFT). \n- W2. The claim related to \"avoiding entangled hidden representations\" was not qualified theoretically or empirically. The calibration network still relies on the hidden representations of the model. \n- W3. The evaluation is incomplete, as it fails to compare against other simple, efficient alternatives (e.g., small, distilled LLMs) that compete on the same axis of low parameter count and simplicity.\n- W4. Despite its complexity, the method still performs worse than DPO and GenARM, so the efficiency gain during training comes with a significant quality degradation."}, "questions": {"value": "- Why were methods that compete on efficiency and simplicity, such as small distilled $\\text{LLMs}$ finetuned with $\\text{DPO}$ + LoRA with very low rank or selective layer application, omitted from the comparison?\n- Could the authors clarify how the calibration approach avoids the hidden state entanglement issue? The phrasing on this makes it sound that there is no dependence on the hidden state which is not the case."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VQ9gBiBvfE", "forum": "0cmuYj3WeG", "replyto": "0cmuYj3WeG", "signatures": ["ICLR.cc/2026/Conference/Submission23970/Reviewer_243J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23970/Reviewer_243J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066172944, "cdate": 1762066172944, "tmdate": 1762942877512, "mdate": 1762942877512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}