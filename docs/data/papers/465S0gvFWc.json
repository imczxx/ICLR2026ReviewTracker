{"id": "465S0gvFWc", "number": 3860, "cdate": 1757557405313, "mdate": 1759898065924, "content": {"title": "MAGNET: Multi-granular Adaptive Gradient-guided Knowledge Distillation for Pareto-Efficient Tuning", "abstract": "Despite the remarkable advances achieved by large pretrained models (LPMs), their practical utility remains significantly constrained due to prohibitive computational and memory demands. While knowledge distillation (KD) partially alleviates this challenge, existing KD techniques rely predominantly on heuristically selected student architectures, resulting in suboptimal teacher-student pairings that frequently fail to achieve Pareto-optimal trade-offs between efficiency and performance. To overcome this limitation, we introduce MAGNET, a multi-granular adaptive gradient-guided knowledge distillation framework designed to analytically derive Pareto-efficient student architectures without heuristic intervention. Specifically, MAGNET initiates a concise gradient-profiling step on a small validation set, computing mean absolute gradients to rank layers according to their saliency. Based on this gradient-informed hierarchy, MAGNET selectively inherits only the most informative blocks, forming a highly compact student model. Within each selected block, MAGNET further masks parameters exhibiting minimal gradient magnitudes and executes a unified, single-stage training procedure integrating direct supervision, logit matching, and feature alignment. Comprehensive experiments across various vision and language benchmarks demonstrate that MAGNET consistently achieves superior accuracy with significantly fewer parameters and reduced computational overhead compared to state-of-the-art (SOTA) KD approaches.", "tldr": "", "keywords": ["Large Pretrained Models; Knowledge Distillation; Gradient-Guided Distillation; Pareto Efficiency"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c9e195313bafaacd7f0d1124cf4501fc7c7ea46e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MAGNET, a novel knowledge distillation (KD) framework that systematically derives compact and efficient student models from large pretrained teachers via gradient-guided saliency profiling, eschewing heuristic or NAS-based architecture selection in favor of a two-level adaptive strategy: coarse-grained inheritance, which selects the most salient teacher layers based on mean absolute gradient magnitude, and fine-grained masking, which prunes less informative parameters within those layers; a unified optimization integrates cross-entropy, KL-divergence, and feature alignment losses to drive the process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novel idea: Gradient-based saliency profiling provides a principled, data-driven mechanism for student architecture selection, replacing heuristic or costly NAS approaches.\n\nMethodological clarity: The paper is well structured and presents equations, algorithms, and ablations clearly.\n\nStrong empirical evidence: Comprehensive experiments across multiple modalities (vision + NLP) and datasets.\n\nPareto-efficiency: The method demonstrates real performance–efficiency trade-offs, maintaining accuracy with reduced parameters.\n\nGeneral applicability: Framework is generic and can be applied to various teacher architectures."}, "weaknesses": {"value": "Computational overhead of gradient profiling: Although lighter than NAS, the need for full gradient computation on all layers may still be costly for large teachers.\n\nGeneralisation beyond classification: Evaluation is restricted to classification tasks.\n\nReproducibility: Some implementation details (e.g., layer selection ratio, validation subset size) are not fully specified."}, "questions": {"value": "How sensitive is MAGNET to the choice of validation set used for gradient profiling?\n\nCould the gradient computation step be approximated (e.g., via low-rank or Fisher information) to further reduce cost?\n\nHow stable is the method when teacher and student domains differ (cross-domain distillation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RmaSLY9Ve0", "forum": "465S0gvFWc", "replyto": "465S0gvFWc", "signatures": ["ICLR.cc/2026/Conference/Submission3860/Reviewer_ZhXD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3860/Reviewer_ZhXD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827860027, "cdate": 1761827860027, "tmdate": 1762917070188, "mdate": 1762917070188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MAGNET is a gradient-guided, multi-granular KD framework that first runs a brief gradient-profiling pass on a validation set to compute mean-absolute gradient saliency per layer, then builds a compact student by inheriting only the top-ranked teacher layers; inside each inherited layer it further masks parameters with log gradient saliency, and finally trains the student in a single stage combining CE, KL, and cosine feature alignment while freezing masked weights. Evaluated on several classification tasks, MAGNET reports higher average accuracy than other KD baselines, and ablations indicate both coarse-grained inheritance and fine-grained masking materially contribute to the gains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is simple and intuitively understandable, and the paper presents it with clear exposition. Moreover, because of its straightforward nature, the approach can be readily applied to a wide range of architectures.\n2. It incurs minimal computational cost as it relies on gradient values computed over a relatively small validation set.\n3. The paper includes a well-designed ablation study that effectively analyzes the contribution and importance of each element in the proposed method."}, "weaknesses": {"value": "1. In traditional NAS, the gradient magnitude itself has been used as a heuristic proxy [1]. Therefore, I am not entirely convinced by the statement *“This gradient-guided approach eliminates reliance on heuristic methods,”* since gradient-based heuristics have already been a common practice in NAS.\n\n2. Because using gradient magnitude is already quite prevalent among zero-cost proxies in traditional NAS, the novelty of the proposed approach feels somewhat limited. Clarifying how this work differs from or improves upon those prior gradient-based heuristics would make the paper substantially stronger.\n\n3. The datasets used to validate the proposed method appear to be relatively easy and are all limited to classification problems. To strengthen the methodological contribution, it would be important to test whether this approach generalizes to non-classification tasks and remains effective in more challenging or diverse settings.\n\n\n#### References \n[1] Abdelfattah, Mohamed S., et al. \"Zero-cost proxies for lightweight NAS.\" arXiv preprint arXiv:2101.08134 (2021)."}, "questions": {"value": "1. How is the masking ratio determined?\n2. In my view, applying the same masking ratio to all layers may not be optimal. It might be more intuitive and beneficial to adapt the masking ratio based on each layer’s relative importance. Would it be feasible to explore this adaptive strategy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LKcLA8bL0P", "forum": "465S0gvFWc", "replyto": "465S0gvFWc", "signatures": ["ICLR.cc/2026/Conference/Submission3860/Reviewer_hWNp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3860/Reviewer_hWNp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969294416, "cdate": 1761969294416, "tmdate": 1762917069853, "mdate": 1762917069853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MAGNET, a gradient-guided framework for KD that adaptively selects and prunes model components to construct efficient student architectures. The method replaces heuristic or NAS-based student model design by using mean absolute gradients to rank and select the most informative layers of a teacher model (coarse-grained inheritance), and further applies parameter-level masking within these layers (fine-grained masking) to remove redundancy. A unified training objective incorporates classification, logit, and feature-level losses. MAGNET is evaluated across vision and language tasks, demonstrating improved performance and compactness over various KD baselines. The paper includes gradient analysis, ablation studies, and scaling evaluations to support its claims."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a structured, interpretable framework using gradient-based selection. The distinction between coarse- and fine-grained selection is logical and grounded in the principle of gradient saliency, which is a reasonable proxy for task-relevant information.\n\n2. The ablation experiments isolate the contributions of each key module (inheritance, masking), which helps clarify what aspects of the method are responsible for performance gains. For example, varying the masking ratio and comparing different inheritance strategies (uniform, random) is informative and shows empirical grounding in design choices."}, "weaknesses": {"value": "1. **Assumption of gradient saliency as a universal importance signal**: The central assumption that *average gradient magnitude reliably indicates component importance* may not always hold across domains, training regimes, or loss landscapes. Some layers with low gradients might still be essential for generalization or robustness, particularly in overparameterized settings. The paper could benefit from more critical discussion or empirical testing of this assumption’s limitations.\n\n2. **Lack of robustness evaluation**: While accuracy and compression metrics are reported, there is no evaluation under distribution shift, adversarial robustness, or low-data regimes. Since MAGNET uses fewer layers and fewer parameters, its behavior under these conditions is important, especially if the pruned layers encode rare but essential signals.\n\n3. **Limited analysis of computational cost and efficiency trade-offs**: Although the method is proposed as more efficient than NAS-based approaches, the actual overhead of gradient profiling (e.g., extra forward-backward passes) is not fully quantified. Additionally, comparisons focus on accuracy rather than end-to-end wall-clock time or energy cost—metrics that are crucial for deployment."}, "questions": {"value": "N.A. But I suggest the following exps to strengthen the paper:\n\n1. Robustness tests under domain shift or adversarial settings (e.g., corrupted CIFAR or adversarial SNLI) to assess whether the pruned models retain robustness.\n\n2. Ablation on the reliability of gradient saliency: Analyze whether high-gradient layers always contribute more to performance. For example, shuffle gradient rankings and compare results.\n\n3. Profiling overhead analysis: Include wall-clock time and energy consumption comparisons between MAGNET and NAS/KD methods, to back the claim of practical efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OeX10Plxe1", "forum": "465S0gvFWc", "replyto": "465S0gvFWc", "signatures": ["ICLR.cc/2026/Conference/Submission3860/Reviewer_Pu7c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3860/Reviewer_Pu7c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050375757, "cdate": 1762050375757, "tmdate": 1762917069546, "mdate": 1762917069546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}