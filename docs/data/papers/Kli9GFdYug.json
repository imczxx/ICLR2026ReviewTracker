{"id": "Kli9GFdYug", "number": 6953, "cdate": 1758003231863, "mdate": 1759897882038, "content": {"title": "INTERPRETING QUANTUM CIRCUIT LEARNING WITH QPERT: A STEP TOWARD TRUSTWORTHY QUANTUM AI", "abstract": "Quantum Circuit Learning (QCL) presents a promising hybrid computational framework that combines the representational capacity of parameterized quantum circuits (PQCs) with classical optimization techniques for solving machine learning problems. However, the opaque nature of QCL models limits their adoption in domains requiring transparency and accountability. In this work, we introduce quantum perturbation (QPERT), a novel perturbation-based explainability approach tailored for QCL. QPERT generates a saliency mask by quantifying the importance of input features for a given instance while preserving key quantum properties such as entanglement and superposition. We evaluate QPERT in explaining a hybrid quantum-classical architecture trained on the Iris dataset. Comparative analysis against established explainability techniques, including SHAP and LIME, highlights QPERT's effectiveness in delivering interpretable insights into quantum model behavior. Our results demonstrate the feasibility of interpretable quantum learning and offer practical guidance for integrating explainability into quantum-classical pipelines.", "tldr": "", "keywords": ["Explainable Quantum AI", "XAI", "Quantum computing", "Quantum circuit learning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf76a028ae5ab2482606a3364c3fddf833b86a47.pdf", "supplementary_material": "/attachment/07c5b6fd5232482d64260a8d8d3f151155aea46d.zip"}, "replies": [{"content": {"summary": {"value": "In this work, the author's introduce QPERT, a perturbation-based explainability method for quantum circuit learning (QCL), a subclass of quantum machine learning. QPERT fills a gap in the XAI for QML literature as the first perturbation-based XAI method for QCL models. The author's compare QPERT to LIME and SHAP, two classical explainability methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Significance: QPERT is the first perturbation-based explanability method for QCL, a popular subclass of QML methods. \n\nClarity: The paper's prose is lively and engaging. As a result, the paper is easy to follow and understand."}, "weaknesses": {"value": "Too niche: The paper develops a specific kind of explainable AI (XAI) method for a subclass of QML methods. Other XAI methods already exist for QCL, as covered in the related work section. The paper fails to convincingly argue why QPERT is superior to these methods or significantly advances the field. As a result, the paper is likely too niche for ICLR. \n\nFailure to support major claims: The data presented in the paper fails to support multiple claims including, most importantly, the utility of QPERT. At the highest level, the ablation study suggests that SHAP does a better job at explaining the underlying QCL model than QPERT, thus undermining the claim that QPERT is superior because it respects the \"quantumness\" of the model. At lower levels, the paper makes claims like \"[QPERT] capture[s] meaningful local gradients, while respecting the underlying structure of the model (Line 311-312).\" This claim comes after a short presentation about how QPERT ranks the relative importance of different features. As far as I can tell, no argument is made as to why this relative ranking demonstrates how QPERT respects the underlying structure of the model. At an even lower level, numerous quantitative values are cited which are then contradicted by the figures (e.g., stating that petal width has a mean importance of 0.8 for the Virginicus class even though the reference figure has no bars that reach 0.8).  \n\nContradictory internal logic: The results of the ablation study suggest that LIME is no better than a baseline method for interpreting QCL models, yet the paper relies on LIME to support claims about the relevance of different input features. \n\nMissing experimental details: It is not possible to reproduce the paper's main results based on the information provided in the main text. For instance, under what conditions where the demonstrations and ablation studies performed? Were they run on experimental hardware? A cloud-accessed simulator? With or without noise?"}, "questions": {"value": "Why did you not compare QPERT to qSHAP and qLIME?\n\nHow robust is this method to the encoding scheme? What happens if I encode my data in a way that breaks a one-to-one mapping between individual qubits and data features?\n\nWhy are we using different colors in the subfigures of Figure 4? Honestly, the plots look very similar. Its hard to tell what I'm supposed to get out of these plots. It looks like QPERT is telling me that all features are treated roughly equally when classifying Setosa and Versicolor.\n\nFigure 11b / Figure 10 has no bar that reaches .8. Are you sure the data is presented correctly? \n\n\"For Setosa, petal length was the most influential, while Versicolor exhibited a more balanced dependence on both the petal and sepal characteristics.\" How am I supposed to see this? Neither of these claims appear to be supported by Figure 4. \n\nHow does any of the presented data support QPERT's ability to \"capture meaningful local gradients, while respecting the underlying quantum structure of the model?\" You sort of claimed that preserving fidelity, minimizing entanglement loss, and preserving superposition would imply respecting the underlying quantum structure of the model…but you haven't talked about that at all in this section.\n\nHow am I supposed to interpret Figure 6? It is just a big grey box with a very very mild downward trend. Did QPERT actually learn much over the course of its training that it didn't essentially immediately learn?\n\nWhat's the point of Figure 5? It says \"Top Features (Ranked)\" and it just…lists the four features.\n\nYou make a lot of claims about the behavior of the different loss components, but then you don't present any quantitative evidence that the claims are true. For example, you state that \"Entanglement loss converges rapidly,\" without ever actually plotting entanglement loss!\n\nWhy should I care about QPERT if SHAP outperformed it in your ablation study?\n\n\"The Random Baseline, as expected, produced a near-zero AUC-Difference, validating its role as a control.\" But the Random Baseline is further from 0 than QPERT's AUC-Diff. So wouldn't that mean that QPERT is a good control too (and therefore a bad method)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FwckaKpMXw", "forum": "Kli9GFdYug", "replyto": "Kli9GFdYug", "signatures": ["ICLR.cc/2026/Conference/Submission6953/Reviewer_LLnC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6953/Reviewer_LLnC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761677530480, "cdate": 1761677530480, "tmdate": 1762919179700, "mdate": 1762919179700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider the problem of explainability of a particular class of QML models they call Quantum Circuit Learning. They introduce a new quantum version of the PERT method, called QPERTH which involves a \"saliency based optimization architecture\" which perturbs instances using a learnable mask. The protocol measures the effect of perturbation. The  authors also introducde a regularized loss which also penalizes for loss of \"quantum features\" incl. fidelity, entanglement loss and superposition loss.\nThe method is implemented in a 4 qubit example and evaluated on the iris dataset.\nThe numerical part of the paper includes comparation to other local methods such as LIME, global suchas SHARP, investigates loss trends, includes a hyperparameter study, an AUC anaysis and and ablation study."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I find this contribution original in incorporating entanglement etc in the loss (although I don't know why this is done). \nIn my view the storngest part of the paper are the extensive numerics which are done very well and I believe are informative, although I cannot associate an actual quantitative statement  to them. It opens a line of resaerch where \"quantum properties\" are preserved, and puts importance on explainability. I liked the numerics performed (albeit being somewhat restricted in terms of model and dataset)."}, "weaknesses": {"value": "Clarity of presentation: from my perspective, the model is not sufficiently clearly defined: it is not clear to me if this t is really a \"single shot\" model in which the model output is the actual measurement outcome, and not an expectation value. If this is true then it is highly unconventional and in my view not very promising (due to intrainability and extreme demand on expressivity as we now need to generate quantum functions which give very close approximations of computational basis states).\nA second issue I see is that the paper adapts a classical method and does what seem like ad-hoc modification to \"incorporate quantum\" but I found this poorly motivated. Why should we desire these new additional loss terms?\nRelated to clarity issues: the measrue of entanglement is completely unclear to me. Why is this measuring entanglement?\nThe paper deals with explanations so I understand that demanding purely quantitative statements may be out of the question but some more quantitative goals would be much more convincing. \nThe experiments are limited to just one model, and just one dataset.\nScalabilty of the approach is fully unclear to me.\nMore, generally, I fully agree with the authors that finding new, better explanation methods is important in ML.\nHowever, I feel this can only be as important as the model we are talking about is accepted to be useful. \nThe QCL model discussed (and I am not 100% sure how it is defined, what is the output?) is praised in the intro as offering exponential advantages.\nHowever these models are only known to be non-simulable and none of them have actually been proven to be useful for any task of relevance. Furthermore it is well know in general they suffer from very serious trainability issues.\nConsequently I feel this contribution is addressing the no 3 to no 4 most important problem in the field out of ... 3 or 4."}, "questions": {"value": "(1) can you motivate why the loss you describe makes sense for explainabilty\n(2) can you please explain the quantum model precisely (see prior comments)\n(3) can you discuss scalability incl. training costs\n(4) what is the computational cost of the evaluation of the loss on a QC and on a CC?\n(5) how would you make comparative statements between other methods and yours quantitiative.\n(6) (not very important) do you consider \"explainability\" and 'interpretability\" as the same thing? I always thought explainability is a post-hoc approach, whereas interpretability is an inherent property of a model. \n(7) Do you really consider lack of interpretabiliity as \"the key limitation\" of QCL applications in critical domains? You do not say it explicitly but the introduction is quite suggestive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b1ylKk9mJQ", "forum": "Kli9GFdYug", "replyto": "Kli9GFdYug", "signatures": ["ICLR.cc/2026/Conference/Submission6953/Reviewer_Jb5u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6953/Reviewer_Jb5u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731566556, "cdate": 1761731566556, "tmdate": 1762919179389, "mdate": 1762919179389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce QPERT, a perturbation-based explainability framework tailored for QCL. The core is a composite loss function with multiple components designed to explicitly preserve key quantum properties, such as fidelity, entanglement, and superposition, during the generation of a saliency mask."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is written with clarity. The motivation, the limitations of prior work, the architecture of QPERT, the design of its loss functions, the experimental setup, and the results are all clearly articulated. \n- The core idea of designing a quantum-aware loss function to guide the explanation generation is interesting. Formulating concepts like fidelity, entanglement, and superposition as optimizable loss terms is an innovative attempt to bridge quantum mechanics with explainable AI."}, "weaknesses": {"value": "- While QPERT is designed to be a more \"faithful\" explainer for QCL, it is outperformed by the classical, non-quantum-aware SHAP method on the authors' own chosen metric, AUC-Difference (SHAP 0.074 vs. QPERT 0.035).\n- The evaluation is conducted exclusively on the Iris dataset. This is a classic but overly simplistic \"toy\" dataset with low feature dimensions and a small sample size. To truly validate QPERT's value, experiments on more complex datasets (perhaps synthetic, or from fields like quantum chemistry) where quantum advantages are more pronounced are necessary.\n- The loss functions used to quantify quantum properties like \"entanglement\" appear to be classical proxies in their implementation. For example, the entanglement loss is defined by calculating the Pearson correlation matrix of the perturbation mask elements. The paper does not provide a rigorous theoretical argument for why minimizing the correlation of a classical mask vector directly and reliably corresponds to preserving the entanglement of the quantum state itself."}, "questions": {"value": "- Why was the Iris dataset chosen as the sole benchmark for this study? On such a simple dataset, how can we be confident that the quantum properties QPERT is designed to protect are actually critical factors in the model's decision-making process?\n- Could you please elaborate on the design of the entanglement loss? Specifically, what is the theoretical link that justifies minimizing the Pearson correlation between elements of a classical perturbation mask as an effective strategy for preserving quantum entanglement within the QCL model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HzSNLMdW1B", "forum": "Kli9GFdYug", "replyto": "Kli9GFdYug", "signatures": ["ICLR.cc/2026/Conference/Submission6953/Reviewer_oGzi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6953/Reviewer_oGzi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754313487, "cdate": 1761754313487, "tmdate": 1762919179011, "mdate": 1762919179011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the interpretability gap in Quantum Circuit Learning (QCL), where classical post-hoc explainers such as SHAP and LIME, built on deterministic outputs and largely independent feature perturbations, clash with stochastic measurement, entanglement, and inherently probabilistic outputs, often yielding unstable or misleading attributions. The authors propose QPERT, a model-agnostic, perturbation-based framework that learns a continuous saliency mask to blend each feature with in-distribution background samples, re-encodes the perturbed instance into a quantum state, and optimizes a composite loss combining target-suppression, sparsity (L1), and quantum-aware regularizers (fidelity, entanglement, superposition) to preserve quantum properties during explanation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Studying interpretability explicitly through fidelity/entanglement/superposition is conceptually interesting and goes beyond naïve classical perturbations.\n2. Diverse and well-organized visualizations (e.g., local masks, global summaries, convergence plots) aid interpretation analysis.\n3. The insertion/deletion protocol with AUC-Difference, plus sparsity, convergence, and multi-run stability analyses, is appropriate for probing explanation faithfulness and robustness."}, "weaknesses": {"value": "1. The framework identifies influential input features via a learned mask, but the manuscript does not articulate a theoretical advantage over straightforward L1-regularized feature selection (e.g., conditions under which QPERT yields strictly better identifiability or faithfulness).\n2. Multiple losses (fidelity, entanglement, superposition, sparsity) are introduced, yet potential negative interactions or trade-offs are not analyzed; no ablations isolate their individual and joint effects.\n3. Evidence is restricted to Iris on a noise-free simulator with a small 4-qubit circuit, leaving behavior under deeper circuits, harder datasets, or realistic noise unverified.\n4. QPERT learns a mask per instance with staged loss activation (e.g., L1 at 250 iters, fidelity at 500, entanglement/superposition at 750), which may be costly relative to simpler post-hoc probes; runtime/complexity and shot-budget sensitivity are not reported.\n5. Several captions in Figures are not fully self-consistent or sufficiently descriptive to stand alone."}, "questions": {"value": "1. In ablations, QPERT and other explainers perform very closely, and SHAP attains the best AUC-Difference. Where is QPERT’s clear advantage? Please clarify what properties (e.g., stability to shot noise, quantum-consistency, sparsity) QPERT demonstrably improves, and analyze why SHAP still leads on partial metrics.\n2. You mention grid search, but the explored range seems narrow. Did you try broader searches or alternative strategies (random search, Bayesian optimization, population-based training)? Please report the search space, budgets, and sensitivity results.\n3. How were SHAP and LIME configured (background/reference choices, sample sizes, kernel/neighborhood parameters) and tuned? Please ensure comparable tuning budgets to QPERT, and report fairness controls (e.g., same number of model evaluations/shots).\n4. Can you provide preliminary results on a larger dataset or deeper circuits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7KFVWMZuMr", "forum": "Kli9GFdYug", "replyto": "Kli9GFdYug", "signatures": ["ICLR.cc/2026/Conference/Submission6953/Reviewer_QFDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6953/Reviewer_QFDF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900826688, "cdate": 1761900826688, "tmdate": 1762919178470, "mdate": 1762919178470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}