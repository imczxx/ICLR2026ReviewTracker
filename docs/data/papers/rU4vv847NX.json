{"id": "rU4vv847NX", "number": 12825, "cdate": 1758210605512, "mdate": 1759897482646, "content": {"title": "$\\texttt{M-Attack-V2}$: Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting", "abstract": "Black-box adversarial attacks on Large Vision‚ÄìLanguage Models (LVLMs) present unique challenges due to the absence of gradient access and complex multimodal decision boundaries. While prior $\\texttt{M-Attack}$ demonstrated notable success in exceeding 90% attack success rate on GPT‚Äë4o/o1/4.5 by leveraging local crop-level matching between source and target data, we show this strategy introduces high-variance gradient estimates. Specifically, we empirically find that gradients computed over randomly sampled local crops are nearly orthogonal, violating the implicit assumption of coherent local alignment and leading to unstable optimization.\nTo address this, we propose a theoretically grounded **gradient denoising** framework that redefines the adversarial objective as an expectation over local transformations. Our first component, *Multi-Crop Alignment (MCA)*, estimates the expected gradient by averaging gradients across diverse, independently sampled local transformations. This significantly reduces gradient variance, enhancing convergence stability. Recognizing an asymmetry in the roles of source and target transformations, we introduce *Auxiliary Target Alignment (ATA)*. ATA regularizes optimization by aligning the adversarial example not only with the primary target image but also with auxiliary samples from a semantically correlated distribution. This forms a smooth semantic trajectory in the embedding space, acting as a low-variance regularizer.\nFinally, we reinterpret prior momentum replay within local matching as variance-minimizing estimators under the crop-transformed objective landscape. Momentum replay stabilizes and amplifies transferable perturbations by preserving gradient directionality across local perturbation manifolds. Together, MCA, ATA, momentum replay, and a delicately selected ensemble constitute $\\texttt{M-Attack-V2}$, a principled framework for robust black-box LVLM attack.\nEmpirical results show our framework significantly improves the attack success rate on Claude-4.0 from **8% ‚Üí 30%**, on Gemini-2.5-Pro from **83% ‚Üí 97%**, and on GPT‚Äë5 from **98% ‚Üí 100%**, surpassing all existing black-box LVLM attacking methods.", "tldr": "", "keywords": ["Large Vision-Language Model", "Adversarial Attack", "Muti-modality Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e22c9cead67394b4c8c29124c1f9bbc15e7a197c.pdf", "supplementary_material": "/attachment/f2bee7044bbd489ab43252276fad411d7ebe5fd0.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces¬†M-Attack-V2, a new framework for¬†black-box adversarial attacks on Large Vision-Language Models (LVLMs). The work builds upon the previously successful¬†M-Attack¬†method, which relied on local crop-level feature matching between source and target images but suffered from unstable gradients. M-Attack-V2 mitigates this instability through three innovations: (1)¬†Multi-Crop Alignment (MCA), a variance-reducing gradient aggregation strategy; (2)¬†Auxiliary Target Alignment (ATA), a semantic regularizer that stabilizes optimization via auxiliary samples; and (3)¬†Patch Momentum (PM), a replay-based stabilization mechanism. These components yield improved transferability and attack success rates."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a well-defined empirical diagnosis of M-Attack instability, showing near-orthogonal consecutive gradients and quantifying the issue via mIoU. The proposed method effectively addresses these problems through principled variance reduction and patch-level alignment mechanisms.\n2. The work re-casts local matching as an expectation over transformations and targets, deriving a Monte-Carlo averaging estimator is a technically interesting and novel formulation that improves gradient stability.\n3. Extensive evaluations show consistent ASR/KMR improvements across commercial LVLMs (GPT-5, Claude-4.0-extended, Gemini-2.5-Pro). The component ablations (Table 3) isolate MCA, ATA, and PM contributions, confirming the method‚Äôs improvement."}, "weaknesses": {"value": "1. While gradient overlap, improved sampling quality, and patch momentum are analyzed theoretically, their explicit connection to adversarial transferability remains unclear beyond empirical evidence. The theoretical evidence and the observed transfer performance appear disconnected.\n\n2.  The results rely heavily on proprietary models, which are subject to continuous updates and thus limit reproducibility and fair comparison. It would strengthen the work to include experiments on open-source models in the main text. The appendix currently reports results on older variants; incorporating evaluations on more recent VLMs such as GLM-4.1, LLaVA 1.6, and Llama-3.2-Vision would help to strength the findings.\n\n3. The proposed method is compared against only a few baselines on VLM classification tasks, without considering a broader line of adversarial attack studies [1-5] on CLIP that explore more diverse tasks such as retrieval, VQA, and image captioning. The use of CLIP as surrogate models is appropriate and fair; however, including evaluations across these additional task types would provide a more comprehensive assessment of transferability and generalization.\n\n4. The ASR evaluation relies on GPTScore and keyword matching (KMRa/b/c), which may introduce prompt-specific bias. \n\n5. The performance improvements for individual modules (Table 3) are small and may not justify the added complexity.\n\n6. Several presentation issues: table captions appear below rather than above (refer to ICLR style files), and one reference is duplicated (lines 540‚Äì546).\n\n7. Different hyperparameters (e.g., $\\alpha$ = 0.75 for Claude vs 1.0 for others) raise potential cherry-picking concerns.\n\n\n\n[1] Zhang, J., Yi, Q., & Sang, J. (2022, October). Towards adversarial attack on vision-language pre-training models. In Proceedings of the 30th ACM International Conference on Multimedia (pp. 5005-5013).\n\n[2] Lu, D., Wang, Z., Wang, T., Guan, W., Gao, H., & Zheng, F. (2023). Set-level guidance attack: Boosting adversarial transferability of vision-language pre-training models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 102-111).\n\n[3] Zhou, Z., Hu, S., Li, M., Zhang, H., Zhang, Y., & Jin, H. (2023, October). Advclip: Downstream-agnostic adversarial examples in multimodal contrastive learning. In Proceedings of the 31st ACM International Conference on Multimedia (pp. 6311-6320).\n\n[4] Huang, H., Erfani, S. M., Li, Y., Ma, X., & Bailey, J. (2025). X-Transfer attacks: Towards super transferable adversarial attacks on CLIP. In Proceedings of the 42nd International Conference on Machine Learning (Vol. 267, pp. 25204‚Äì25234). \n\n[5] Liu, C., Chen, H., Zhang, Y., Dong, Y., & Zhu, J. (2024). Scaling laws for black box adversarial attacks. arXiv preprint arXiv:2411.16782."}, "questions": {"value": "1. Why are the hyperparameters different for different models ($\\alpha$ = 0.75 for Claude and $\\alpha$ = 1.0 for all others)? \n2. Why are DINO models included in the ensemble?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hJf56n2Ayt", "forum": "rU4vv847NX", "replyto": "rU4vv847NX", "signatures": ["ICLR.cc/2026/Conference/Submission12825/Reviewer_9cWL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12825/Reviewer_9cWL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559549964, "cdate": 1761559549964, "tmdate": 1762923630306, "mdate": 1762923630306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes M-Attack-V2, a theoretically grounded framework for black-box adversarial attacks on Large Vision‚ÄìLanguage Models (LVLMs). It addresses the high gradient variance in local-level matching by introducing three key components: Multi-Crop Alignment (MCA) for gradient denoising, Auxiliary Target Alignment (ATA) for stabilizing target embeddings, and Patch Momentum for maintaining consistent optimization across local regions. The method achieves substantial gains in attack success rates on models such as GPT-5, Gemini-2.5-Pro, and Claude-4.0."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated, clearly identifying gradient mismatch as a key limitation of local-level matching.\n2. The proposed method is theoretically supported.\n3. The approach achieves strong empirical results, outperforming baselines across multiple models."}, "weaknesses": {"value": "Overall the paper writing is very unclear and confusing. \n\n1. The introduction to the original M-Attack is very limited, making it difficult to follow the motivation and context of the proposed improvements. The discussion on gradient mismatch is especially confusing. There is no formal definition or equation specifying which gradient is being analyzed. It is unclear whether the gradient refers to that of the source crop or the target crop, and whether the mismatch occurs between similar source crops with the same target crop or across different target crops. The description of how cropping operates on the source and target sides is also vague; the important distinction that the source crop acts in pixel space while the target crop acts in feature space only becomes apparent much later (around line 177). These key details should be clearly articulated in the background section.\n\n2. The method section also suffers from a lack of precision. The notion of a ‚Äútarget embedding distribution‚Äù is introduced without a clear definition or explanation of how it is constructed. Similarly, the process for selecting auxiliary images is not described. Even in Algorithm 1, the definition of the transformation distribution ùê∑ is missing."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dkGNATAlOY", "forum": "rU4vv847NX", "replyto": "rU4vv847NX", "signatures": ["ICLR.cc/2026/Conference/Submission12825/Reviewer_pwjM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12825/Reviewer_pwjM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714139084, "cdate": 1761714139084, "tmdate": 1762923629665, "mdate": 1762923629665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes M-Attack-V2, an improved black-box attack on large vision-language models (LVLMs) that focuses on fine-grained detail targeting.\nBy selectively perturbing semantically important visual regions, the method aims to increase attack success with fewer queries and minimal perceptual change.\nExperiments on several LVLMs show moderate improvements in success rate and query efficiency compared to prior M-Attack and other black-box baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles the important and practical challenge of improving black-box attacks for LVLMs, a domain with limited existing methods.\n\n- The proposed fine-grained detail targeting effectively balances perceptual imperceptibility and attack strength, showing improvements in attack success rate and query efficiency.\n\n- Experiments cover multiple LVLMs and include comparisons with standard baselines such as Square Attack and Bandit-based methods, providing a reasonably comprehensive empirical evaluation."}, "weaknesses": {"value": "- The conceptual novelty is limited. The method largely extends prior M-Attack with refined region selection, but does not introduce a new theoretical framework or attack family.\n\n- The fine-grained targeting module is described at a high level; there is no detailed mathematical formulation or clear rule for selecting salient regions beyond heuristic attention maps.\n\n- The ablation studies are incomplete. For instance, removing the fine-grained targeting or using random region selection could help isolate its contribution, but such baselines are missing.\n\n- The experiments lack diversity in tasks. The evaluation is limited to visual question answering and captioning; other LVLM tasks (reasoning, OCR, or instruction following) are not tested.\n\n- The transferability and generalization of the attack are unclear. It is not shown whether perturbations generated for one model are effective on others.\n\n- The real-world feasibility is uncertain. Query limits, response filtering, and adaptive throttling in deployed APIs could heavily constrain the attack‚Äôs practicality."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lkJ4NsjZkb", "forum": "rU4vv847NX", "replyto": "rU4vv847NX", "signatures": ["ICLR.cc/2026/Conference/Submission12825/Reviewer_YJew"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12825/Reviewer_YJew"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738009338, "cdate": 1761738009338, "tmdate": 1762923629087, "mdate": 1762923629087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes M-Attack-V2, a novel black-box adversarial attack method targeting Large Vision-Language Models (LVLMs). Building upon the M-Attack framework, the authors aim to address a critical issue in local patch matching‚Äîhigh gradient variance‚Äîwhich hinders optimization stability. To this end, the paper introduces three key components: Multi-Crop Alignment (MCA) to reduce gradient variance via averaging across multiple local views, Auxiliary Target Alignment (ATA) to enhance semantic consistency using auxiliary target embeddings, and Patch Momentum (PM) to improve gradient direction consistency over iterations. Extensive experiments on leading commercial LVLMs (e.g., GPT-5, Claude-4.0, Gemini-2.5) demonstrate that M-Attack-V2 significantly outperforms prior state-of-the-art methods in attack success rate (ASR) and transferability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tTheoretical analysis is rigorous: Theorems 1 and 2 clearly explain how MCA and ATA reduce gradient variance and improve semantic alignment.\n\n2.\tThe experimental design is thorough and convincingly validates the method‚Äôs effectiveness and practicality."}, "weaknesses": {"value": "1.\tWhile the method achieves notable improvements in both performance and theory, it remains an enhancement of the existing M-Attack framework. Its originality and novelty are somewhat limited by this dependency.\n2.\tAlthough the attack success rate improves significantly, the method shows weaker imperceptibility (i.e., higher l2 and l1 norms) compared to prior work, indicating a trade-off between success and stealth. It would be helpful for the authors to further analyze this trade-off and explore potential ways to balance both.\n3.\tThe introduction of multiple crops (MCA) and auxiliary targets (ATA) increases the per-iteration computational cost. Training efficiency should be more explicitly quantified and discussed."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KLFlnoUu2U", "forum": "rU4vv847NX", "replyto": "rU4vv847NX", "signatures": ["ICLR.cc/2026/Conference/Submission12825/Reviewer_7P9g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12825/Reviewer_7P9g"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931726934, "cdate": 1761931726934, "tmdate": 1762923628677, "mdate": 1762923628677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}