{"id": "AmczI1k3Yk", "number": 4025, "cdate": 1757585768226, "mdate": 1759898057329, "content": {"title": "Capturing Visual Environment Structure Correlates with Control Performance", "abstract": "The choice of visual representation is key to scaling generalist robot policies. However, direct evaluation via policy rollouts is expensive, even in simulation. Existing proxy metrics focus on the representation's capacity to capture narrow aspects of the visual world, like object shape, limiting generalization across environments. In this paper, we take an analytical perspective: we probe pretrained visual encoders by measuring how well they support decoding of environment state—including geometry, object structure, and physical attributes—from images. Leveraging simulation environments with access to ground-truth state, we show that this probing accuracy strongly correlates with downstream policy performance across diverse environments and learning settings, significantly outperforming prior metrics. Our study provides insight into the representational properties that support generalizable manipulation, suggesting that learning to encode full environment state is a promising objective for visual representations for control.", "tldr": "We tackle the problem of visual representation selection for policy learning by proposing a practical and scalable proxy task: decoding the full environment states from visual observations.", "keywords": ["Robot Learning", "Computer Vision", "Diffusion Policy"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abf28b4595463f5a1dc07c68da9786d64c94e337.pdf", "supplementary_material": "/attachment/c54be5a9ce75eb3983e75effea33aa2057a321f1.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a new proxy for manipulation downstream policy success by measuring how well visual encoder pretrained backbones can predict the state of the environment (lighting conditions, robot arm joints and end effector pose, objects' positions, orientations, shapes, materials). Training such regressors is possible in simulation where ground truth state is available. For a plethora of backbone encoders, the proxy is shown to provide significantly better correlation to task performance than other methods both in simulation and on real world deployment. In addition, jointly fine-tuning visual backbones (5 different variants) for policy learning as well as state prediction is shown to  consistently offer clear improvements to policy performance on the MetaWorld benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well presented and thorough in situating the work within the current literature.\nThe manuscript is clear and enjoyable to read with a logical and consistent progression.\n\nThe solution presented to the proxy measure learning problem is simple and intuitive which makes it convincing. Its implementation in terms of encoding, multi-object handling, loss design and metric definition are all clear and logical.\n\nThe experimental protocol is thorough, at least for the base experiments looking into correlation quality, with many relevant benchmarks and visual encoder backbones evaluated both in simulation and in the real world. \n\nThe results appear to back the claims of the paper very elegantly."}, "weaknesses": {"value": "**On the evaluation protocol:**\n- The authors state at line 302 that the protocol follows that of the SimplerEnv benchmark without any additional details or what part of the evaluation this entails, one suspects it is the averaged success rate measurements but that is not clear\n- Furthermore, the authors select the Mean Maximum Rank Violation (MMRV) metric as well as the Pearson correlation between performance and state prediction capacity as the statistical measures of correlation without any introduction, explanation or presentation of the what these metrics represent and why those and not others are utilized in the work. This leaves room for doubt as to whether the metric selection is cherry picked to amplify the paper results or objectively standard. This warrants more clarity as the choice is not explained in the main text, nor is it clarified in the appendix.\n\n**On the results:**\n- The success rates vary at most by about 15% percent between the best and worst models, with 3 out of four benchmarks having all models perform at low success rates. Each datapoint is obtained with 100 rollouts. Looking only at the MMRV and r numbers provided (and not explained as mentioned above) and average policy success rates with no standard deviations with different x axis scales, it is difficult to appreciate the statistical significance of the results.\n\n**On the analysis of individual state dimensions:**\n\nI thought this was a very nice ablation to have, yet it is not easy to decipher how it was conducted and what is interesting about the results. After some scrolling back and forth my understanding is that from the full state regressor you extract the specific predicted state entries and compute the MMRV score wrt performance. The idea being that the ones with the lowest would correlate the best with performance should they have been used on their own. In many ways this feels incomplete and perhaps the wrong question to ask:\n- The state dimensions are hand designed, and although they seem reasonable one could argue that the possibilities/variants might extend to other attributes. If the authors assume that training a regressor on the full state is the best approach, they should make a case that shows that each attribute or dimension in the state is actually contributing to correlation (at least for some backbone/benchmark combination)\n- Otherwise the authors should be looking for the best mix of attributes to consider for correlation and considering the couplings that arise both in the training of regressors as well as in establishing the correlation scores.\n- Indeed, with numbers varying quite aggressively across benchmarks for a single state dimension on its own, the only conclusion is that (l 458) \"that different environments indeed present different demands for visual representations\". But what about doing a leaving one out experiment for example to try to understand not what each brings on its own, but which are not that useful in the mix.\n- In summary, it is unclear whether all the attributes in the full state regressor are needed and how this affects the state regression quality should they be reduced/modified which can then couple back into the correlation quality.\n\n**Application demonstration asymmetry :**\n- First and foremost, though well written, the paper fails to my taste to clearly explain what are the uses of such proxies and why they are important tools. They might be cheaper to evaluate during architecture design or backbone selection for a policy but the case could be. made clearer as to their exact purpose.\n- The most interesting application beyond ranking seems to be joint fine-tuning which the authors show provides substantial performance gains across backbone models. This is a very interesting aspect of the work that is largely neglected compared to the potential interest for practitioners. This part of the work warrants more results and details.\n\nThe authors have omitted to include the use of LLMs statement."}, "questions": {"value": "- Are all the tasks considered on parallel gripper pick and place tasks? \n\n- This circles back to points about what state attributes to consider, for tasks with end effector camera where the arm body is not visible how would you say things would differ, one can only predict general scene and object attributes in this situation. Would similar correlations hold in your opinion?\n\n- The other proxies seem very generic, are there no intermediate metrics or any works that provide proxies that are closer to capturing the state of the scene? \n\n- Along these lines, are there ways to combine the pre-existing metrics to see if some combination of them can challenge your approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MQUOVjaRJi", "forum": "AmczI1k3Yk", "replyto": "AmczI1k3Yk", "signatures": ["ICLR.cc/2026/Conference/Submission4025/Reviewer_oHEz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4025/Reviewer_oHEz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760647545232, "cdate": 1760647545232, "tmdate": 1762917139779, "mdate": 1762917139779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Training robot policies, especially in the real world, are expensive.  These policies depend upon visual inputs, which generally need to be encoded before they can be fed as input to a policy network.  There are a wide variety of possible ways to encode pixels to inputs that would be useful for a policy network, and the authors aim to identify which visual encoders are most useful without having to train the expensive robot policy.\n\nThe authors posit that learning to predict underlying state representation of a robot scene from a visual representation of that scene is a good predictor how useful the resulting visual encoding is for learning control policies.  This visual encoding comes from a variety of pre-trained visual encoders like masked auto encoders, etc. As ground state information about a scene is hard to attain in the real world, the focus is on measuring this predictive effect in simulation.  \n\nThe author carefully formulate the learning problem of predicting the state from the visual simulator, with appropriate normalization and discretization applied, and details such as disambiguating which objects are targeted in the state vector by providing 2D bounding boxes for each object are handled in a systematic manner. \n\nThe process of learning to predict the state from a visual encoding is applied to a variety of different pre-trained visual learnt encoders including self-supervised, manipulation specific, and generative models. This state prediction metric is compared to a variety of alternative proxies like segmentation accuracy, etc.  The ranking via this proxy target is more accurate (Mean Maximum Rank Violation) in ranking how well the trained robotics policy will perform.  The state prediction proxy is  also more computationally efficientcompared to other proxies as learning to predict the state is a relatively low dimensional regression problem.\n\nThey then demonstrated that this ranking also transferred across the real to sim gap, and that the best visual representation in simulation also predicted best on robot policy learning performance.  They further explored which aspects of the state prediction best predicted which visualization would be best for a task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "## Originality\nThe proposed metric is to my knowledge novel and useful.  It can also be a useful proxy when designing new visual encoders, as well as for quality control of the final result (and intermediate checkpoints during training), and potentially as additional auxillary loss during training.\n\n## Quality\nThe authors cast a wide net, and systematically explore the effects of many different visual representations in many robotics tasks.  They empirically demonstrate that their proxy is both more efficient and predictive than alternatives in the literature, and they demonstrate transfer to real robot tasks.\n\n## Clarity\nThe paper is clearly written, easy to follow with clear reasoning, and would be reproducible from the provided descriptions.\n\n## Significance\nBetter visual encoders are likely to help robotics, and approaches like those outlines in this paper may useful in developing that research.  It can be expanded to include other predictors like contact surfaces/other information from the simulation."}, "weaknesses": {"value": "In general, I would expect that L2 losses on poses would have problems with wrapping causing large error.  I didn't notice in the paper where they tackled this potential problem.  E.g., a scene where the objects are aligned in a specific manner (lying on a table?) could have problems where close poses end up with very difference values per axis of the pose, throwing off the pose for in a set of tasks.  \n\nThe representation you can get out of a simulator will be a bit limited - it is unlikely to be useful in learning to fold a jumper as it is unlikely there will be a good \"symbolic\" target to score the outputs of the model against.  That to some degree limits the information you can get out of this proxy target, but it might still be sufficient for ranking the different representations."}, "questions": {"value": "Have you considered mixtures of visual representations?  E.g., is it more powerful to just concatenate subsets of the representations?  Can you use your method to tell in advance which representations are likely to complement each other or make up for each others shortcomings?\n\nHow strong are the effects of initialization on how good the regression solution is?  Does the learning converge to the same point, and if not can you include error bars?\n\nPose might be highly ambiguous for many objects, e.g., cube have symmetry, vases have rotational symmetry.  Did you encounter such problems in the simulators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VuJ5OAgbMR", "forum": "AmczI1k3Yk", "replyto": "AmczI1k3Yk", "signatures": ["ICLR.cc/2026/Conference/Submission4025/Reviewer_vCpr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4025/Reviewer_vCpr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935357779, "cdate": 1761935357779, "tmdate": 1762917139523, "mdate": 1762917139523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes state prediction from visual inputs as a fast, reliable proxy for evaluating visual representations in robotics. Instead of expensive policy rollouts, the authors measure how well a frozen encoder supports decoding of ground-truth environment state (geometry, object structure, and physical attributes). They show that state-prediction accuracy correlates strongly with downstream policy performance across multiple manipulation suites (MetaWorld, RoboCasa, SimplerEnv), is orders of magnitude faster than rollouts, and exhibits non-trivial sim-to-real correlation. The work is a simple, practical idea with useful empirical insights for representation selection and benchmarking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strong empirical validation: Correlation holds across MetaWorld, RoboCasa, SimplerEnv with multiple seeds and error bars.\n\n- Computational efficiency: Good speedups over policy rollouts; actionable for everyday / larger-scale benchmarking.\n\n- Unified evaluation setup: Works across environments with a consistent state target.\n\n- Actionable insights: Per-dimension/attribute analyses give interpretability; sim-to-real correlation is encouraging.\n\n- Clarity: Problem framing and metrics are easy to implement and reproduce."}, "weaknesses": {"value": "-Task diversity and modality coverage: While the results across manipulation benchmarks are compelling, the current evaluation is limited to manipulation-centric settings. Prior work (e.g., VC-1) has shown a form of multi-modality, where visual representations that excel in certain domains (e.g., R3M on MetaWorld) can perform poorly in others (e.g., navigation tasks in Habitat). This raises the question of whether the proposed proxy generalizes across task families with different perceptual and temporal demands (e.g., navigation, long-horizon multi-stage tasks, language-conditioned control). Including experiments or discussion around such cross-domain generalization—or clarifying the scope of applicability—would strengthen the claims and situate the method relative to known modality gaps in representation learning for robotics.\n\n- Simulator dependence: Ground-truth state access limits direct real-world deployment of the proxy; a concrete real-world surrogate (e.g., 6D pose from VLM/trackers) would elevate practicality.\n\n- Failure cases underexplored: Provide root-cause analysis for outliers (e.g., WidowX MAE)—is it state granularity, visuals, or embodiment mismatch?\n\n- Ablations on proxy design: How sensitive are correlations to the decoder capacity, training budget, or subset of state dimensions?"}, "questions": {"value": "0. Could you discuss/explore more in-detail the first weakness I raised in the previous point?\n\n1. Real-world without GT state: What concrete proxy(s) would you propose—e.g., learned keypoints/pose tracking, depth recon, object state estimators—and have you piloted any?\n\n2.  Can you report correlations when decoding only task-critical subsets of state (e.g., end-effector pose + key object states) rather than the full state? This would help clarify whether the proxy is capturing the information actually needed for control, as opposed to benefiting from full-state regression capacity.\n\n3.  How does correlation change when using smaller decoders or shorter training? Understanding this would help confirm that the proxy reflects encoder quality rather than decoder strength, and that the metric remains reliable under lower-compute settings.\n\n4.  Sim-to-real outlier analysis: What explains the WidowX MAE anomaly? Can you isolate the culprit (visual domain, state scaling, or dynamics mismatch)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pEnl7kcMOv", "forum": "AmczI1k3Yk", "replyto": "AmczI1k3Yk", "signatures": ["ICLR.cc/2026/Conference/Submission4025/Reviewer_ATcT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4025/Reviewer_ATcT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977761991, "cdate": 1761977761991, "tmdate": 1762917139342, "mdate": 1762917139342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training/evaluation-free proxy for selecting visual backbones for robot manipulation. It includes regressing a unified simulator state from images using the frozen backbone + MLP, then reranking backbones by this state prediction score. They show the proxy correlates strongly with policy success across 3 simulation benchmarks, and provide a real-world validation. The method is computationally cheaper than baselines, and improves policy success when used as an auxiliary objective."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The method is simple and efficient. The predicted state covers information about object and scene-level variables, finally providing a single score to policy performance.\n\n* The paper evaluates across a breadth of envs, including 3 simulation envs and real-world evaluation on two tasks.\n\n* The paper uses a strong set of baselines, including few-shot, action MSE, Depth, etc."}, "weaknesses": {"value": "1) The method relies on privileged information from the simulator (state + 2D object boxes) which can not be made available in the real world. It is not clear to me from the current set of experiments if the rankings would correlated if the real world env is substantially different from the simulated environment where the data is collected.\n\n2) As tasks get more complicated, the number of state variables to track will keep on increasing. For example, if the task requires picking up objects of different categories, it would require more variables to track.\n\n3) The authors should compare the real world performance of their approach with other visual backbone selection proxies. Some of these proxies (eg: Action MSE, Depth (can be obtained from the RealSense camera)) do not need privileged information from the simulator and therefore I suggest that the authors also report results when using the data from the real world.\n\n4) I would have liked to see more robotics specific visual encoders like VC-1, MVP [1], LIV [2], etc. They would have also added some diversity in terms of the dataset used in training of the vision encoder.\n\n5) The paper appears to use a baseline that closely matches the method proposed in prior work (SCR: Stable Control Representations [3]), but this connection is not acknowledged or cited; the authors should clarify this and include the appropriate reference.\n\n[1] Xiao, Tete, et al. \"Masked visual pre-training for motor control.\" arXiv preprint arXiv:2203.06173 (2022).\n\n[2] Ma, Yecheng Jason, et al. \"Liv: Language-image representations and rewards for robotic control.\" International Conference on Machine Learning. PMLR, 2023.\n\n[3] Gupta, Gunshi, et al. \"Pre-trained text-to-image diffusion models are versatile representation learners for control.\" Advances in Neural Information Processing Systems 37 (2024): 74182-74210."}, "questions": {"value": "See point 3, 4 and 5 above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ljn0ZKXcQq", "forum": "AmczI1k3Yk", "replyto": "AmczI1k3Yk", "signatures": ["ICLR.cc/2026/Conference/Submission4025/Reviewer_XhXW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4025/Reviewer_XhXW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051991915, "cdate": 1762051991915, "tmdate": 1762917139172, "mdate": 1762917139172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}