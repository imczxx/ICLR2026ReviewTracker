{"id": "tBCaP9hkfh", "number": 17595, "cdate": 1758277981976, "mdate": 1758806400532, "content": {"title": "Learning Well-Separated Neural Representations with the Bhattacharyya Regularization", "abstract": "Deep neural networks demonstrate remarkable classification performance, yet traditional cross-entropy training provides limited control over the geometric structure of learned embeddings. We present a moment-based geometric regularization framework that shapes class-conditional feature distributions through distributional separation measures. Our approach estimates empirical class statistics and optimizes a differentiable objective derived from the Bhattacharyya distance to encourage inter-class separation while controlling intra-class scatter. We develop three computational variants tied covariance, diagonal approximation, and projected subspace methods that provide explicit complexity-accuracy trade-offs from $\\mathcal{O}(D^3)$ to $\\mathcal{O}(D)$ per training step. Comprehensive empirical validation across image and text classification benchmarks demonstrates consistent improvements in embedding quality metrics while maintaining competitive classification accuracy. Statistical analysis reveals that while learned embeddings deviate from perfect Gaussianity, the moment-based approximation provides an effective geometric regularization mechanism. The framework's modular design enables straightforward integration into existing architectures with minimal computational overhead.", "tldr": "", "keywords": ["Regularization", "Deep Neural Networks", "Bhattacharyya distance", "Loss function", "Optimization"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "", "supplementary_material": ""}, "replies": [], "withdrawn": true}