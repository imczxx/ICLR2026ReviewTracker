{"id": "fGe0izJHai", "number": 12673, "cdate": 1758209411435, "mdate": 1759897494568, "content": {"title": "Are Large Language Models Good Temporal Graph Learners?", "abstract": "Large Language Models (LLMs) have recently driven significant advancements in Natural Language Processing and various other applications. While a broad range of literature has explored the graph-reasoning capabilities of LLMs, including their use as predictors on graphs, the application of LLMs to dynamic graphs—real-world evolving networks—remains relatively unexplored. Recent work studies synthetic temporal graphs generated by random graph models, but applying LLMs to real-world temporal graphs remains an open question. To address this gap, we introduce Temporal Graph Talker (TGTalker), a novel temporal graph learning framework designed for LLMs. TGTalker utilizes the recency bias in temporal graphs to extract relevant structural information, converted to natural language for LLMs, while leveraging temporal neighbors as additional information for prediction. TGTalker demonstrates competitive link prediction capabilities compared to existing Temporal Graph Neural Network (TGNN) models. Across five real-world networks, TGTalker performs competitively with state-of-the-art temporal graph methods while consistently outperforming popular models such as TGN and HTGN. Furthermore, TGTalker generates textual explanations for each prediction, thus opening up exciting new directions in explainability and interpretability for temporal link prediction.", "tldr": "We propose TGtalker, a novel temporal graph learning framework designed for LLMs. We show that TGTalker achieves competitive performance to TGNNs across five real-world datasets.", "keywords": ["LLMs", "Temporal Graph Learning", "Graph Representation Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/beed218e6f879a58e56cf5cfaf3f35b4c25457e6.pdf", "supplementary_material": "/attachment/eab286623e1647a59a434505852cc180f7836ce3.zip"}, "replies": [{"content": {"summary": {"value": "The authors have introduced TGTalker, a framework that use LLMs to perform link prediction on temporal graphs and provide natural language explanations. The experiments are carried out on five dataset on temporal-graphs and while the performances reached are comparable to other methods, the fact that have been reached without fine-tuning or training the models is very important."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main strengths of the work are the following:\n\n- Strength 1: the paper is well-written, easy to follow and logically structured. Figures effectively illustrate the framework. As a very side note, numbers on Figure 3 may be a bit too small; \n\n- Strength 2: the experiments span multiple LLMs and multiple datasets providing a solid empirical grounding;\n\n- Strength 3: the work applies LLMs to TG and this is relevant to current trends in combining structured reasoning with language models."}, "weaknesses": {"value": "The main weaknesses of this paper are the following ones:\n\nWeaknesses 1: while the paper describes the context for TGTalker, there is not a proper discussion on why this specific design (e.g., background set, example set, …) works or how sensitive the results are to prompt formulation. There is also a limited discussion on computational constraints (e.g., token limits, context truncation strategies,...) and related impact \n\nWeaknesses 2: concerning the selected baselines, it is not clear whether the same temporal splits and negative samplings are identical for all models. While it is clear for LLMs, are the baselines trained with the same level of data accessibility? \n\nWeaknesses 3: it was interesting that the agreement scores in Table 3 are around the 70%. Any chance to have a deeper interpretation in the discussion of where the disagreements arise? any specific category? Also, some of the categories in the taxonomy presented in Table 2 show some overlapping (for example “Pattern Continuation” and “Repeated Interaction Patterns”). \n\nWeaknesses 4: concerning the explanations and the human annotation, it would be important to understand on which models the alignments are computed. Are the values reported averages? If so, a standard deviation is missing. Otherwise, on which model’s explanation are the alignments computed? Is there an LLM that is better than others? which are the minimum and maximum alignments across the models? A proper evaluation and discussion is missing and, as the explanation is a key contribution of the paper, it would be important to produce a clearer table and more specific results and a deeper discussion."}, "questions": {"value": "Some questions are the following ones:\n\nSee Table 3, where the disagreements arise? any specific category?\n\nConcerning the explanations, on which model’s explanation are the alignments computed? Is there an LLM that is better than others? which are the minimum and maximum alignments across the models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9kL5YoYKJe", "forum": "fGe0izJHai", "replyto": "fGe0izJHai", "signatures": ["ICLR.cc/2026/Conference/Submission12673/Reviewer_uJSF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12673/Reviewer_uJSF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806439429, "cdate": 1761806439429, "tmdate": 1762923511865, "mdate": 1762923511865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the LLM4DyG (KDD'24) and utilizes LLMs to perform explainable temporal link prediction. The authors introduce a series of prompt engineering for the in-context learning of the LLM, including constructing the background (for showing the recent temporal graph interactions), examples (for few-shot learning), queries (for asking questions), and temporal neighbor sampling (for pointing out the recent neighborhoods for a given node). They also conduct experiments to validate the effectiveness of the proposed methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The authors introduce prompt engineering of the LLMs for the temporal link prediction tasks in a temporal graph, which is easy to follow.\n2. The paper is well-written."}, "weaknesses": {"value": "1. This paper seems to lack novelty. TGTalker employs a similar prompting paradigm to LLM4DyG, and no learning process has been introduced during the whole pipeline. Moreover, utilizing LLMs cannot be considered a novelty within the temporal graph learning community, as there already exist related works that integrate LLMs into temporal graphs [1][2].\n2. The baselines used in this paper seem to be insufficient. The authors only compare against the recent model TNCN, while the other baselines are quite outdated. Compared to AP, MRR can be unstable when evaluating different models. Therefore, it is necessary to include more recent baselines, such as DyGFormer and NAT, to make the comparison more comprehensive and convincing.\n3. The scalability of TGTalker may be a concern. The scales of datasets used in this paper are relatively small by the standards of the temporal graph learning community. For each given edge, the LLM should perform reasoning, and incur such computational costs are not affordable in the large datasets.\n4. The design of the \"background set\" seems ineffective. As shown in Table 5, removing this module does not lead to any noticeable performance degradation; in fact, the performance even slightly improves. \n5. MRR is a rank-based metric. How do you compute the ranking from the LLM’s generated responses? \n6. Line 268 mentioned that TGTalker predicts test edges in fixed-size batches (under 200). However, Figure 4 only shows a single prompt example. How do you handle the prompt becoming excessively long, especially with such a batch size?\n\n[1] LLM-driven Knowledge Distillation for Dynamic Text-attributed Graphs, AAAI 2025.  \n[2] Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models, NeurIPS 2025."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ALXCy0iyQb", "forum": "fGe0izJHai", "replyto": "fGe0izJHai", "signatures": ["ICLR.cc/2026/Conference/Submission12673/Reviewer_tTdT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12673/Reviewer_tTdT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836001928, "cdate": 1761836001928, "tmdate": 1762923511622, "mdate": 1762923511622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of learning on temporal graphs using LLM as backend model.  Authors highlight the recent success of LLMs in in-context learning, reasoning over broad category of tasks. Authors proposes a novel method  TGTalker to adapt LLMs on temporal graphs by converting target link into a LLM prompt by extracting relevant temporal graph like recent history using background set, extract question-answer pairs as few shot examples, query set that formulate link prediction task as natural language and temporal Neighbour sampling mechanism for relevant context. TGTalker also generates natural language explanation.\n\nThe authors evaluate their proposed framework against Temporal Graph Neural Networks (TGNN) like TGN, TCNN, GraphMixer and Edge-bank. Authors also explore multiple LLMs like Qwen, Llama, Mistral and GPT4.1 mini."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. Novel perspective on using pre-trained LLMs to temporal graph domain. This paper explores their capabilities on this domain and answer fundamental questions. \nS2. Extensive Evaluation on multiple LLMs\nS3. Clear paper writing"}, "weaknesses": {"value": "**W1.** A key weakness in the paper is the unclear positioning of its main contribution. There are 2 parallel goals: to show that LLMs can perform temporal link prediction, and that LLMs can also generate textual explanations of those predictions.  The explanations component is largely a by-product of prompting rather than a distinct methodological contribution.  The link-prediction contribution itself requires further clarification. A more focused paper at this early stage of exploring LLMs for temporal graphs would have chosen either to rigorously benchmark LLMs for temporal link prediction or to build and evaluate an LLM-based framework specifically designed for explaining temporal predictions.\n**Other weaknesses:**\nW2. Lack of competitive TGNN baselines: PINT, TPNet, DyGFormer and CAWN\nW3. More clarity is needed on node and time representation: Current framework lacks representation node attributes and it’s unclear how LLMs can relate different nodes with similar neighborhood or attributes. Can it support millions or large-sized graphs? What kind of temporal granularity can be supported?\nW3. As shown in the results, high surprise leads to low performance. The key cause seems to be over-dependency on recent temporal neighbours in the prompt context. \nW4. Evaluated datasets are rather small in size, with fewer than 10k nodes. This raises questions about the method's utility in real-world large temporal graphs. \nW5. Evaluation approach for proposed framework is also not clear. The proposed framework directly outputs the destination node ID. How do you get ranks for other/negative target nodes?\nW6. Inference time for TGNN baselines is missing.\n\nThe overall claim that LLMs are temporal graph learners is weakly supported; it seems that heuristics encoded in prompts are deriving the majority of the gains."}, "questions": {"value": "Please answer the issues raised in the weakness section above. \n\nClarification questions:\nQ1. Do background sets and example set change with target links?\nQ2. Is token vocabulary of backend LLM fixed, or it depends on node ids?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8fQLrwZDwW", "forum": "fGe0izJHai", "replyto": "fGe0izJHai", "signatures": ["ICLR.cc/2026/Conference/Submission12673/Reviewer_wyy6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12673/Reviewer_wyy6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973363690, "cdate": 1761973363690, "tmdate": 1762923511349, "mdate": 1762923511349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents TGTalker, a framework that leverages pre-trained LLMs for learning on temporal graphs. The method reformulates temporal link prediction as a text-based learning problem: the evolving graph is serialized into natural language and provided to an LLM together with recent structural context and examples. Through this prompting strategy, TGTalker enables LLMs to predict future links. Later, the article claims that  TGTalker generates human-readable explanations for its predictions, without any fine-tuning or task-specific training. Experiments across real-world temporal networks show that the approach achieves reasonable performance."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Although it does not involve any task-specific training, TGTalker achieves performance comparable to state-of-the-art temporal graph neural networks. \n\nThe paper is clearly written and well organized, making it easy to read and understand."}, "weaknesses": {"value": "The paper mainly demonstrates that LLMs can be applied to temporal graphs but offers little theoretical understanding of why this works.\nIt lacks a principled analysis of temporal reasoning, structure encoding, or generalization, so the contribution feels more empirical than conceptual.\n\nThe approach treats LLM prompting as a black box, missing opportunities to relate findings to established principles like message passing, memory models, or temporal inductive bias.\n\nThe evaluation lacks rigorous interpretation of what the model learns or how explanations relate to reasoning. The “explainability” component is superficial, as the model’s justifications are unverifiable.\n\nThe approach ignores node features, which are often high-dimensional embeddings (e.g., bag-of-words). Directly adding them would further shorten the already limited context window, making scalability worse. This is a major limitation, especially since node features can be essential and may also evolve over time."}, "questions": {"value": "1) How would your framework handle node or edge features, especially when they are high-dimensional or time-varying?\n2) Given the token-length limitation of LLMs, how does TGTalker scale to large temporal graphs or long histories? This is particularly important in social domains, where periodicity of interactions arises.\n3) How far into the future can your model reasonably predict before recency bias dominates? Did you test for long-range temporal dependencies?\n4) How sensitive are the results to prompt phrasing, number of examples (k-shot), or ordering of edges in the background set?\n5) Understanding vs. Pattern Matching – How can you be sure that the LLM is performing genuine temporal reasoning rather than exploiting short-term recency or frequency heuristics?\n6) Since explanations are generated and classified by the same LLM, how do you avoid circular validation? Have you tested explanation consistency across models or prompts?\n7) Have you evaluated TGTalker on datasets with different temporal dynamics (e.g., periodic, bursty, or irregular) to assess robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KK1D6DGCAM", "forum": "fGe0izJHai", "replyto": "fGe0izJHai", "signatures": ["ICLR.cc/2026/Conference/Submission12673/Reviewer_p4VJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12673/Reviewer_p4VJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991583554, "cdate": 1761991583554, "tmdate": 1762923511106, "mdate": 1762923511106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}