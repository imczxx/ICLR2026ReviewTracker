{"id": "i6PCa45gBh", "number": 9205, "cdate": 1758115178662, "mdate": 1759897737787, "content": {"title": "TopoWeaver-R1: Reinforcing Difficulty-Aware Topology Evolution in Multi-Agent Competition-Level Code Generation", "abstract": "Recent studies have shown that large language model (LLM)-driven multi-agent systems (MAS) are promising for addressing complex problems, with competition-level code generation as a representative domain. By emulating the collaboration among human programmers,  these systems leverage predefined interaction topologies to achieve notable gains. However, such fixed structures introduce interaction redundancy and excessive token costs as task difficulty drops. While graph pruning and generation methods can produce sparser topologies, they remain static during inference, unable to adapt to execution feedback, and often converge to limited density ranges. To overcome these issues, we propose TopoWeaver-R1, a reinforcement learning–optimized MAS centered on an LLM orchestrator agent, which supports end-to-end evolutionary dynamic interaction topology generation. For each query, it infers agent roles and task difficulty, then constructs a task-adapted, density-aware layered directed acyclic graph (DAG) topology. The topology evolves via execution feedback and history, thereby improving the task-solving performance of the generated code. On three competition-level and two basic code datasets, TopoWeaver-R1 achieves state-of-the-art accuracy, with up to 14.6\\% higher accuracy, 13\\% lower density and 68\\% lower token cost than the strongest baseline. Our approach transitions multi-agent topologies from static designs to dynamic, feedback-driven evolutionary designs with fine-grained, difficulty-aware density control.", "tldr": "", "keywords": ["Multi-Agent System", "Competition-Level Code Generation", "Dynamic Topology Generation", "Reinforcement Learning Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3d7cc7253620c5b2ea5831c8edb1beee27a5ab1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces TopoWeaver-R1, a framework designed to optimize MAS for competition-level code generation. It addresses the inefficiency of fixed communication topologies, which lead to redundancy and excessive token costs, by introducing a difficulty-aware mechanism. The framework utilizes RL to dynamically generate efficient interaction graphs for agent communications."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method explores addressing the problem of dynamic topology optimization in LLM-based MAS, moving beyond static, handcrafted structures to improve efficiency and reduce token costs. The optimized MAS achieves a better balance of performance and computational expense."}, "weaknesses": {"value": "1. The paper lacks discussion on the design principles, heuristic choices, and specific numerical weighting applied to the reward functions. This critical omission prevents the rigorous assessment of the sensitivity of the final performance to these crucial hyperparameters.\n\n2. The proposed method is actually a general paradigm, but the experimental validation is limited to the specific domain of \"competition-level code generation\". The general applicability to more representative MAS problems, such as multi-step arithmetic reasoning, complex planning, or scientific document summarization, where communication requirements, agent roles, and reward structures differ significantly, remains unproven. \n\n3. The quantitative results are presented without standard deviation across multiple experimental runs, which should be added for more solid validation.\n\n4. The paper lacks crucial details regarding the SFT data, including the data source, filtering process, and quality assurance protocols. This missing information makes it hard to assess the potential bias or limitations of the dataset."}, "questions": {"value": "1. The current document's font, layout appear inconsistent with the official ICLR submission template？\n\n2. Why does the method only optimize the topology of MAS without considering other MAS factors such as agent role, prompts or functions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "psj0724BJK", "forum": "i6PCa45gBh", "replyto": "i6PCa45gBh", "signatures": ["ICLR.cc/2026/Conference/Submission9205/Reviewer_PF5v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9205/Reviewer_PF5v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806416574, "cdate": 1761806416574, "tmdate": 1762920872042, "mdate": 1762920872042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the inefficiency and rigidity of fixed-topology multi-agent systems (MAS) in complex domains like competition-level code generation. The authors propose TopoWeaver-R1, an MAS framework centered on an LLM \"orchestrator agent\" trained with reinforcement learning. This orchestrator dynamically generates a layered, directed acyclic graph (DAG) topology, represented in YAML, based on the task's inferred difficulty. The topology evolves across multiple turns in response to execution feedback. The orchestrator is trained using a multi-objective reward function that balances code accuracy, structural correctness, and topology density, with density targets explicitly conditioned on task difficulty. Experimental results on five code benchmarks show that TopoWeaver-R1 achieves state-of-the-art pass@1 accuracy while simultaneously reducing token costs and utilizing sparser, more efficient topologies compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and uses language that is easy to understand.\n\n2. The proposed TopoWeaver-R1 framework is a complete and sensible solution, employing a three-stage pipeline (SFT, RL) to train a dedicated orchestrator agent.\n\n3. The design choice of using YAML to represent the layered DAG topology is strong, as it is both human-readable and easily generated by an LLM."}, "weaknesses": {"value": "1. Limited Novelty of Dynamic Topologies: The paper's central claim of addressing static agent structures appears overstated.  The core idea of dynamic or optimized interaction patterns is not entirely novel;  similar motivations to move beyond fixed topologies have been previously explored in works such as MaAS[1] and FlowReasoner[2].\n\n2. Inaccurate Characterization of FlowReasoner: The paper's distinction from FlowReasoner seems to be based on an inaccurate premise.  The authors classify FlowReasoner as a method that \"focuses on optimizing sequential workflows.\"  However, to my understanding, FlowReasoner operates on a search space defined by ADAS[3], which is not limited to sequential structures and likely encompasses the parallel, graph-based topologies defined in this work.\n\n3. Limited Empirical Scope: The paper's evaluation is confined solely to the domain of code generation.  This narrow focus makes it difficult to assess the generalizability of the TopoWeaver-R1 framework.  This contrasts with other significant works in agent architecture optimization (e.g., ADAS[3], Aflow[4], AgentSquare[5], MaAS[1]), which have demonstrated the generality of their approaches by evaluating on a more diverse set of benchmarks across different domains.\n\n4. The experimental setup appears problematic. Limiting the multi-agent interaction to 2 turns suggests the model may be receiving and using feedback from the test set to correct its initial solution. This protocol, which allows for test-time adaptation, differs from Aflow's setup (search on validation, single pass on test) and thus seems to constitute an unfair comparison.\n\nReference\n\n[1]Zhang G, Niu L, Fang J, et al. Multi-agent Architecture Search via Agentic Supernet[C]//Forty-second International Conference on Machine Learning.\n\n[2]Gao, Hongcheng, et al. \"Flowreasoner: Reinforcing query-level meta-agents.\" arXiv preprint arXiv:2504.15257 (2025).\n\n[3]Hu S, Lu C, Clune J. Automated Design of Agentic Systems[C]//The Thirteenth International Conference on Learning Representations.\n\n[4]Zhang J, Xiang J, Yu Z, et al. AFlow: Automating Agentic Workflow Generation[C]//The Thirteenth International Conference on Learning Representations.\n\n[5]Shang Y, Li Y, Zhao K, et al. AgentSquare: Automatic LLM Agent Search in Modular Design Space[C]//The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "1. To further substantiate the claims within the code domain, could the authors conduct experiments on other significant benchmarks, such as SWE-bench[6]?\n\n2. To demonstrate the generalizability of the framework, could the authors provide experimental results on benchmarks from more diverse domains, for example, GAIA[7] or HLE[8]?\n\n3. Could the authors please clarify the aforementioned issue regarding the experimental setup?\n\nReference\n\n[6]Jimenez C E, Yang J, Wettig A, et al. SWE-bench: Can Language Models Resolve Real-world Github Issues?[C]//The Twelfth International Conference on Learning Representations.\n\n[7]Mialon G, Fourrier C, Wolf T, et al. Gaia: a benchmark for general ai assistants[C]//The Twelfth International Conference on Learning Representations. 2023.\n\n[8]Phan L, Gatti A, Han Z, et al. Humanity's last exam[J]. arXiv preprint arXiv:2501.14249, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "K2v6YSQG43", "forum": "i6PCa45gBh", "replyto": "i6PCa45gBh", "signatures": ["ICLR.cc/2026/Conference/Submission9205/Reviewer_x3tB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9205/Reviewer_x3tB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882407181, "cdate": 1761882407181, "tmdate": 1762920871630, "mdate": 1762920871630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TopoWeaver-R1, a multi-agent system (MAS) for competition-level code generation. It tackles the inefficiency of fixed-topology systems by using a reinforcement learning (RL) optimized orchestrator agent. This orchestrator dynamically generates an interaction topology (a layered DAG) in YAML format, adapting the graph's density based on the problem's inferred difficulty and execution feedback. The system is trained via Supervised Fine-Tuning (SFT) followed by RL (GRPO) using a multi-objective reward function."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The core idea of an RL-trained orchestrator that generates *difficulty-aware* and *evolutionary* (feedback-driven) topologies is highly original.\n- The method achieves SOTA accuracy while being significantly more cost-effective than all baselines, a rare and important result.\n- The multi-objective reward function, especially the $\\mathcal{S}_{complex}$ density metric (Eq. 7) that is tied to task difficulty (Eq. 13), is a clever and effective design."}, "weaknesses": {"value": "- The SFT stage, which is shown to be crucial, relies on data generated by a powerful proprietary model (GPT-40).\n- The agent roles (planner, coder, etc.) are predefined. The system optimizes the *interaction graph* but not the *composition* of the team itself.\n- It remains unclear how well the orchestrator transfers to unseen problem types or agent role definitions, or whether it overfits to the YAML schema used in training."}, "questions": {"value": "1. How sensitive is the final performance to the weights ($\\lambda_1, \\lambda_2, \\lambda_3$) chosen for the $\\mathcal{S}_{complex}$ reward in Eq. 7? Were these tuned per dataset or fixed globally?\n\n2. How does the system perform if the initial difficulty-level inference (which sets $N_{max}(l)$ in Eq. 13) is incorrect? Can the RL policy recover through execution feedback, or does it remain constrained by the wrong density cap?\n\n3. Have the authors analyzed whether the RL-trained orchestrator produces diverse graph patterns across problem types, or does it converge to a small family of template structures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ax1ZSDPHW1", "forum": "i6PCa45gBh", "replyto": "i6PCa45gBh", "signatures": ["ICLR.cc/2026/Conference/Submission9205/Reviewer_1SLm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9205/Reviewer_1SLm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927911956, "cdate": 1761927911956, "tmdate": 1762920871207, "mdate": 1762920871207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TopoWeaver-R1, a reinforcement-learning–based framework for multi-agent code generation that introduces dynamic, difficulty-aware topology evolution. An LLM-based orchestrator outputs layered YAML-DAG structures describing agent collaboration workflows, which adapt across multiple turns according to execution feedback and inferred task difficulty. A composite reward integrating code execution results, topology complexity, and YAML validity guides this evolution through Group Relative Policy Optimization (GRPO). Experiments on APPS, LiveCodeBench, CodeContests, HumanEval, and MBPP demonstrate consistent gains compared to strong multi-agent baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.\tNovel conceptual framing: Recasting multi-agent orchestration as a dynamic topology evolution task is original and impactful.\n2.\tStructured and interpretable representation: The YAML-based layered DAG allows both fine-grained control and interpretability.\n3.\tComprehensive experimentation: Solid benchmarking and ablations convincingly show the benefit of dynamic evolution.\n4.\tReward engineering: The integration of task difficulty, code execution, and structural complexity into a single reward is elegant and domain-specific.\n5.\tSystematic pipeline: The SFT + RL training workflow is clearly delineated and reproducible."}, "weaknesses": {"value": "1.\tAlgorithmic innovation remains limited.\nThe use of SFT and GRPO is largely standard; while well-executed, the RL stage mainly adds domain-specific reward shaping rather than introducing new optimization techniques.\n2.\tEmpirical boundary conditions not fully explored.\nThe dynamic process is capped at two evolution turns, yet the paper provides no analysis of whether additional turns would further improve or destabilize performance. Similarly, there is no study on how sensitive the model is to reward weightings or the choice of density evaluation function.\n3.\tDifficulty-awareness partially hand-designed.\nThe mechanism for controlling topology density by preset thresholds (4/7/10) is heuristic. It would be stronger if this control were learned automatically rather than manually fixed.\n4.\tData quality concerns in the SFT stage.\nThe SFT dataset used to pretrain the orchestrator is synthetically generated, but the paper does not clarify how the ground truth YAML topologies are validated or filtered. Given that topology correctness directly affects downstream RL stability, more evidence of data verification or human curation would strengthen confidence in the results."}, "questions": {"value": "How is the quality of the SFT training data ensured? If YAML topologies are generated automatically, what validation mechanisms or filtering thresholds are used to ensure they align with valid ground-truth agent workflows?\n\nHow sensitive is performance to the task difficulty classification? Have you tested robustness against misclassified or noisy difficulty labels?\n\nThe dynamic process is limited to two turns — is there an empirical reason or trade-off behind this bound?\n\nHow does the orchestrator handle inconsistent or cyclic YAML outputs? Are such cases frequent, and how are they penalized in training?\n\nCan the reward weighting parameters (α,β,γ) be tuned automatically, or do they require manual adjustment per dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yCHSANOBZf", "forum": "i6PCa45gBh", "replyto": "i6PCa45gBh", "signatures": ["ICLR.cc/2026/Conference/Submission9205/Reviewer_P5e8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9205/Reviewer_P5e8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956386008, "cdate": 1761956386008, "tmdate": 1762920870781, "mdate": 1762920870781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}