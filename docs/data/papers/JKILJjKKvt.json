{"id": "JKILJjKKvt", "number": 13531, "cdate": 1758218952098, "mdate": 1759897430642, "content": {"title": "Conjecturing: An Overlooked Step in Formal Mathematical Reasoning", "abstract": "Autoformalisation, the task of expressing informal mathematical statements in formal language, is often viewed as a direct translation process. This, however, disregards a critical preceding step: conjecturing. Many mathematical problems cannot be formalised directly without first conjecturing a conclusion such as an explicit answer, or a specific bound. Since Large Language Models (LLMs) already struggle with autoformalisation, and the evaluation of their conjecturing ability is limited and often entangled within autoformalisation or proof, it is particularly challenging to understand its effect. To address this gap, we augment existing datasets to create ConjectureBench, and redesign the evaluation framework and metric specifically to measure the conjecturing capabilities of LLMs both as a distinct task and within the autoformalisation pipeline. Our evaluation of foundational models, including GPT-4.1 and DeepSeek-V3.1, reveals that their autoformalisation performance is substantially overestimated when the conjecture is accounted for during evaluation. However, the conjecture should not be assumed to be provided. We design an inference-time method, Lean-FIRe to improve conjecturing and autoformalisation, which, to the best of our knowledge, achieves the first successful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1 and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisite knowledge to generate accurate conjectures, improving autoformalisation performance requires treating conjecturing as an independent task, and investigating further how to correctly integrate it within autoformalisation. Finally, we provide forward-looking guidance to steer future research toward improving conjecturing, an overlooked step of formal mathematical reasoning.", "tldr": "Conjecture generation is a key bottleneck in autoformalisation, so we measure it with a new dataset and evaluation framework, demonstrate both the limits of current LLMs and the first successful conjecturing and autoformalisation of Putnam problems.", "keywords": ["formal reasoning", "Lean4", "autoformalisation", "conjecturing"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b8d799b60e5463a5a5586fbb5762e5fe61ba745.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of autoformalization by being the first to explicitly identify and decouple conjecturing as a critical, overlooked step. It defines conjecturing as the process of generating a concrete solution before formalization. The authors introduce a new benchmark, ConjectureBench, and two metrics (ConJudge, equiv rfl) to measure this specific capability. Finally, they propose LEAN-FIRE, a few-shot, inference-time reasoning method combing informal and formal chains of thought, which demonstrates improved performance on some no-answer problems in PutnamBench compared to a baseline that directly calls the LLM."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The primary strength of this paper is its novel formulation of the problem. Identifying conjecturing as a separate bottleneck from formalization is a key insight that is valuable to the community and the experiments are effective. This conceptual reframing, supported by the intuitive \"seen\" vs. \"unseen\" experimental design, provides a novel lens for analyzing failures in end-to-end reasoning."}, "weaknesses": {"value": "1. The authors' own ablation study (Table 3 and Table 5, 'Baseline' vs. 'LEAN-FIRE w/o FS') shows that the hybrid reasoning structure ('w/o FS') provides almost no benefit, and the gains come entirely from the few-shot examples, which suggests LEAN-FIRE is not an effective reasoning framework, but rather a form of highly-tuned prompt engineering. This finding undermines the core claim that the framework can \"distill the LLM's latent parametric mathematical knowledge\".\n2. The paper’s empirical claims rely on evaluation metrics with major methodological issues, undermining the validity of its conclusions. Specifically, ConJudge shows clear inaccuracy, while BEq+ is overly strict and prone to false negatives. In addition, the LLM Grader’s back-translation approach has not been empirically verified. Additionally, The equiv_rfl task is claimed to assess the model’s conjecturing ability independently of autoformalization, yet the methodology still appears to involve a rigid formal-language check, which blurs this distinction. Furthermore, the conjectures illustrated in Figure 2 seem easily verifiable through informal LLM reasoning. As the authors themselves note on Page 7, “(models) more often generate auxiliary constructs such as definitions or lemmata instead of the conjecture itself,” suggesting that the reported performance on this task may not reliably reflect the models’ true conjecturing ability.\n3. The paper's methodological contribution to the conjecturing step (pre-proof solution-finding) appears limited. The paper frames this step as a key bottleneck, but the proposed solution does not seem to offer a novel or structured technique for solving it. Instead, the methodology appears to rely on the Large Language Model's (LLM's) inherent, general-purpose reasoning capabilities to inference these closed-form problems. If the core of the conjecturing method is, in essence, to prompt the LLM for the solution, then the contribution is less of a new conjecturing scheme and more an application of an existing LLM.\n4. The integrity of the ConjectureBench dataset is questionable due to its construction via manual \"rephrasing\" of existing problems. This method is a weak defense against data contamination, which is a problem the authors themselves admit to observing in their results. It cannot guarantee that the \"unseen\" setting is truly new to the models. Consequently, the reported performance likely underestimates the actual difficulty of the conjecturing task."}, "questions": {"value": "1. The equiv_rfl scores in Table 4 are notably low. Could you elaborate on the reason for this? Specifically, could you elaborate on your analysis of potential false negatives where logically correct conjectures were rejected due to syntactic differences from the gold standard?\n2. The proposed metrics present clear trade-offs: ConJudge has a notable disagreement rate with human annotators, while equiv_rfl is strict. How do you propose combining these complementary signals into a single, reliable evaluation? For instance, how could the formal certainty from equiv_rfl be used to calibrate the potential inaccuracies in ConJudge's semantic judgments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rFzeLncRGX", "forum": "JKILJjKKvt", "replyto": "JKILJjKKvt", "signatures": ["ICLR.cc/2026/Conference/Submission13531/Reviewer_BRQu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13531/Reviewer_BRQu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713048869, "cdate": 1761713048869, "tmdate": 1762924134279, "mdate": 1762924134279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that autoformalisation, commonly treated as a translation task from natural language to formal statements, misses a key step: conjecturing the answer. The authors introduce ConjectureBench, a benchmark of 457 mathematical problems adapted from PutnamBench and CombiBench, which separates the conjecture from the problem to explicitly test this capability. They also introduce ConJudge and equiv_rfl, two metrics to evaluate conjecture quality, and propose LEAN-FIRE, a hybrid inference-time prompting method that interleaves Chain-of-Thought and formal Lean code (Lean-of-Thought). Experiments show a sharp drop in performance when conjectures are not provided and demonstrate that LEAN-FIRE improves conjecture generation in both isolated and end-to-end settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and well-structured, with good motivation for evaluating conjecturing as a distinct subproblem in autoformalisation. The methodology is systematic and the empirical evaluation is thorough, including ablations, multiple models (GPT-4.1, DeepSeek-V3.1), and three different evaluation metrics (typechecking, semantic equivalence, and LLM-based grading). The paper presents the first successful autoformalisation of 13 PutnamBench problems without the answer given. These compelling results demonstrate that current models over-rely on having the solution embedded and that conjecturing is a key bottleneck that demands focused study.\n\nThe paper demonstrates a significant performance drop when conjectures are withheld and shows that LEAN-FIRE with few-shot guidance can partially recover this gap. The discussion of the strictness of BEq+ and the difficulty of assembling formally correct proofs reflects solid empirical awareness."}, "weaknesses": {"value": "The central claim—that conjecturing is an overlooked step—is somewhat overstated. Several recent works (e.g., Enumerate-Conjecture-Prove, STP, LeanConjecturer) have explicitly addressed conjecture generation, even if not with the same benchmarking focus. The paper underplays this context and would benefit from acknowledging that the step is not novel, but under-evaluated.\n\nThe novelty of ConjectureBench is modest. It is derived entirely from two existing datasets and primarily involves rewording and filtering. While useful for this task, the dataset is small (457 items), focused only on competition math, and does not explore general mathematical reasoning domains or longer proofs. Similarly, LEAN-FIRE is an inference-time prompting strategy using structured CoT+LoT, but its gains are heavily dependent on a few-shot setup. Without those examples, performance collapses, indicating limited generalisation. The method is effective, but not especially innovative in design.\n\nI am also concerned about the significant variation in success rates among the different evaluation metrics reported in Table 5. The success rates vary dramatically between type-checking, BEq+, and ConJudge, which raises questions about metric reliability and how well these measures align with actual correctness as judged by humans. Clarifying these differences, or explaining which metric should be considered the main indicator, would strengthen the empirical analysis.\n\nFurthermore, The paper does not explore transferability: all evaluations are confined to ConjectureBench, and it is unclear how well LEAN-FIRE or the proposed metrics generalize to more diverse or real-world formalisation problems.\n\nMinor issues:\n1.\tThe table number and title should appear before the table.\n2.\tConJudge’s agreement with human annotations (~70%) could be improved or discussed more in terms of trustworthiness.\n3.\tRelated work could more thoroughly discuss prior formal theorem proving pipelines that integrate conjecture synthesis."}, "questions": {"value": "1.\tHow sensitive is LEAN-FIRE to the style and domain of the few-shot examples?\n2.\tWould the model generalise to different math domains (e.g., analysis, geometry)?\n3.\tWhat is the performance of a baseline LLM using CoT but without LEAN-FIRE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uNaiYZ4RZr", "forum": "JKILJjKKvt", "replyto": "JKILJjKKvt", "signatures": ["ICLR.cc/2026/Conference/Submission13531/Reviewer_h4kR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13531/Reviewer_h4kR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789368841, "cdate": 1761789368841, "tmdate": 1762924133910, "mdate": 1762924133910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work examines the problem of formal conjecturing - the bridge between an answerless formal problem and a formal proof. The authors first present `ConjectureBench`, an adaptation of a mix of `PutnamBench` and `CombiBench` problems, that ask for a conjecture to a statement, instead of a proof to an already provided conjecture. To facilitate the benchmark, the authors introduce 2 evaluation methods -- `ConJudge`, an LLM-as-a-judge system for evaluating conjecture equivalence to a ground truth one, and `equiv_rfl`, an automated checker for definitional equivalence checking between the conjecture and the ground truth Lean conjecture. This work shows that under both metrics, when the conjecture is not explicitly provided, the end formalization suffers in quality. To this end, it introduces `Lean-FIRe`, an inference method, interleaving Chain-of-Thought reasoning with Lean formalization for more accurate Lean conjecturing. They show that, with few-shot examples, `Lean-FIRe` outperforms simple baselines in conjecturing using few-shot examples (while not doing so without them), hypothesizing this is due to lack of exposure to the relevant data. For end-to-end autoformalization, they show that `Lean-FIRe` does not provide significant performance improvements."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper makes a valuable contribution by identifying and isolating conjecturing as a critical, distinct, and challenging sub-problem within the broader autoformalization and theorem-proving.\n\n2. The authors introduce `ConjectureBench`, a well-motivated benchmark for this new task. The proposed `Lean-FIRe` framework, which combines informal and formal reasoning, is an intuitive and novel inference-time approach.\n\n3. The distinction between \"seen\" (conjecture provided) and \"unseen\" (conjecture must be inferred) settings provides a clean experimental design that effectively quantifies the difficulty of the formal conjecturing step."}, "weaknesses": {"value": "1. Despite the authors' claim in the reproducibility statement, they have not included neither the code, nor the dataset, nor the scripts in the supplementary material. The lack of these assets, especially the benchmark itself, prevents verification of the paper's results and hinders future research.\n\n2. The central claim that `Lean-FIRe`'s architecture improves conjecturing is not sufficiently substantiated. The authors show that `Lean-FIRe` without few-shot examples performs poorly, but they fail to test whether standard baselines also improve when given the same few-shot examples? Without this experiment, it is impossible to disentangle the effect of the few-shot examples from the `Lean-FIRe` framework.\n\n3. The choice of models to evaluate is unorthodox. The authors focus on generalist proprietary models which are not specifically trained for reasoning. In particular, GPT-4.1 has no \"thinking mode\", which has been shown to improve mathematical and coding reasoning significantly. Further, it is unclear whether DeepSeek-V3.1 was run with or without thinking mode on, which is likely to significantly influence the results.\n\n4. The authors hypothesize that lack of specific training data is the core issue in conjecturing performance, this hypothesis can be at least partially verified through the use of a specific autoformalization model, such as `Kimina-Autoformalizer-7B` [1].\n\n5. It has been shown that a mixture of a strong natural language model, combined with a specialized prover model, can achieve significant improvements in formal mathematics [2]. In particular, most frontier models have already saturated final-answer competitions, showing great capabilities for usage as natural language conjecturer. These conjectures can then be easily formalized using a specialized model, which has better exposure to Lean data. Therefore, in contrast to the `Lean-FIRe` framework, which uses a single, generalist, non-reasoning model, it seems more sensible to use a strong reasoning model as a CoT conjecturer (cheaper examples include the GPT-OSS models, `GPT-5-mini`, `Grok 4 Fast`, or the thinking mode of `DeepSeek-v3.1/2`), which can then be directly formalized by a good autoformalization model, such as `Kimina-Autoformalizer-7B`. Given that the authors' approach is more naive, it remains unclear whether existing systems may not already be good enough to solve this challenge.\n\n6. The two primary conjecturing metrics have significant limitations that call their reliability into question.\n    - `ConJudge`'s error rate of 17% compared to human validation is nevertheless significant. The authors should either provide confidence intervals accounting for the error, or try to improve the judging mechanism through using better models, or majority voting.\n    - `equiv_rfl`, which uses the `rfl` tactic verifies only for **definitional equivalence**. This type of equivalence, while formally verifiable, covers a very narrow range of equivalent expressions, making it **too strict**. For example, any differences in notation, or even some small syntactic differences (e.g. $0 + x = x$ is not definitionally true) will result in the tactic failing, even though. A more robust and reliable metric for equivalence checking is necessary, as one should not expect any conjecturer to have to guess the format of the gold conjecture.\n\n7. The claim that `Lean-FIRe` enables the \"first successful end-to-end autoformalisation\" of certain PutnamBench problems is overstated, as the baseline models also achieve non-zero success rates on the \"unseen\" task. This makes it more of a case of a lack of such an evaluation (as explained, there exist several open autoformalization models, and many more capable proprietary models that may also be able to correctly apply autoformalization on Putnam problems).\n\n8. The autoformalization metrics scores for `Lean-FIRe` present a neglibigble, or non-existent improvement (especially for `pass@10` scores), casting further doubts about the reliability of the framework.\n\n9. The claim of observing data contamination is interesting but undersubstantiated, supported only by a single referenced example (`IsMagicSquare`) without sufficient context or analysis of its prevalence."}, "questions": {"value": "1. Can the authors elaborate better in their methodology how the proposed metrics address the \"faithfullness\" and \"conjecture completenss\" issues, outlined in **Section 1**?\n\n2. Could the authors provide results for the baseline models when given the same few-shot examples as `Lean-FIRe` as pointed in **W2**?\n\n3. If the conjecturing framework uses a setup, similar to what is outlined in **W6**, how does that perform in the \"unseen\" setting?\n\n4. How was the ground truth Lean conjecture created for each problem in `ConjectureBench`?\n\n5. Can the authors present the end-to-end formalisation results for when a sample passes all 3 metric checks? This would give a better signal for the validity of the sample.\n\n6. Please clarify any remaining concerns from the **Weaknesses** section, treating each point not specifically elaborated in this section as a separate inquiry.\n\n## Current recommendation\n\nI am assigning this paper a score of **2: Reject**. While the problem of conjecturing is important, the work is undermined by critical flaws in the experimental design, the use of potentially unreliable evaluation metrics, and suboptimal model selections that weaken the central claims. The performance improvement from `Lean-FIRe` is not clearly attributable to the method itself, and its downstream impact on autoformalization is minimal. I would be willing to reconsider my score if the authors can convincingly address the major concerns during the discussion period.\n\n\n### References\n\n[1] Wang, Haiming, et al. \"Kimina-prover preview: Towards large formal reasoning models with reinforcement learning.\" arXiv preprint arXiv:2504.11354 (2025).\n\n[2] Varambally, Sumanth, et al. \"Hilbert: Recursively Building Formal Proofs with Informal Reasoning.\" arXiv preprint arXiv:2509.22819 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sM6EDyCeml", "forum": "JKILJjKKvt", "replyto": "JKILJjKKvt", "signatures": ["ICLR.cc/2026/Conference/Submission13531/Reviewer_Qo3U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13531/Reviewer_Qo3U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831666134, "cdate": 1761831666134, "tmdate": 1762924133584, "mdate": 1762924133584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a current practice in autoformalization of mathematics where formalizers are typically given the final answer directly; the authors argue that the final answer should instead be conjectured by the autoformalization model. They augment existing automated theorem-proving datasets CombiBench and PutnamBench to create ConjectureBench, a benchmark designed to evaluate conjecturing ability. The authors design two metrics to assess automated formalization: (1) ConJudge, an LLM-as-a-judge method, and (2) an automatic check based on the Lean tactic equiv_rfl. They introduce Lean-Fire, which guides an LLM to alternate between chain-of-thought and Lean-of-thought reasoning. The approach reportedly achieves the first end-to-end autoformalization and offers analysis and discussion on future directions for conjecturing."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper focuses on a novel task — conjecturing — which differs from the usual objectives of autoformalization and is highly original.\n2. The proposed dataset, ConjectureBench, is potentially valuable. By requiring models to produce conjectures, it raises the difficulty of autoformalization and could drive development of more capable autoformalizers.\n3. The figures and diagrams are clear and easy to follow; they help the reader quickly grasp the pipeline."}, "weaknesses": {"value": "1. The improvement of Lean-FIRe over the baseline appears marginal, especially in the Unseen setting in Table 5 where metric gains are small and in some cases performance at @10 even seems worse. This suggests that the combination of CoT and LoT does not clearly improve performance.\n\n2. The model in the \"standalone conjecture generation\" setting performs substantially worse than \"autoformalisation\" setting, which is counterintuitive. One would expect conjecturing to be a subtask of autoformalization (autoformalization + conjecturing). This result may indicate a problem in the standalone conjecture generation design: when generating conjectures no formal statement is provided, so the LLM cannot infer the type/format of conjectures expected by Lean 4, which may harm performance.\n\n3. The equiv_rfl metric is unnassarilly stringent: Lean’s notion of definitional equivalence is very strict, so many equivalences that are intuitively acceptable to human mathematicians are not considered equivalent by Lean. This likely contributes to the very low equiv_rfl scores reported in Table 4. It would be better to extend the equivalence check with additional tactics (analogous to how BEq includes more tactics) to allow more flexible equivalence transformations.\n\n4. Minor writing and correctness issues:\n   - Line 053 mixes up formalizing the solution of a problem with formalizing the problem statement. Formalizing a IMO level problem statement typically takes on the order of tens of minutes rather than hours. [1, 2]\n   - Line 155: the three examples given are incorrect. Only the last conjecture is verifiable in Lean as stated. For set equality one must prove both inclusions; the authors appear to have confused \"necessary and sufficien\" and \"sufficient\" conditions.\n\n[1] Zheng, K., Han, J. M., & Polu, S. (2022). *MiniF2F: A cross-system benchmark for formal Olympiad-level mathematics* (No. arXiv:2109.00110). arXiv. https://doi.org/10.48550/arXiv.2109.00110\n\n[2] Tsoukalas, G., Lee, J., Jennings, J., Xin, J., Ding, M., Jennings, M., Thakur, A., & Chaudhuri, S. (2024). *PutnamBench: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition* (No. arXiv:2407.11214). arXiv. https://doi.org/10.48550/arXiv.2407.11214"}, "questions": {"value": "Regarding Weakness 2: Figure 1 indicates that the conjecturer is required to produce conjectures that conform to Lean syntax without access to Lean types or formal statements. This design may explain the poor performance in the \"standalone conjecture generation\" setting. Would providing Lean types as prompts be a more practical design choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "83JKxc1MXs", "forum": "JKILJjKKvt", "replyto": "JKILJjKKvt", "signatures": ["ICLR.cc/2026/Conference/Submission13531/Reviewer_WkBK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13531/Reviewer_WkBK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993426074, "cdate": 1761993426074, "tmdate": 1762924133269, "mdate": 1762924133269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}