{"id": "MKM8iEaowV", "number": 18881, "cdate": 1758291703417, "mdate": 1759897075802, "content": {"title": "Generative Trajectory Planning in Dynamic Environments: A Joint Diffusion and Reinforcement Learning Framework", "abstract": "Real-time trajectory optimization requires planners that can simultaneously ensure safety and energy efficiency in environments containing both static and dynamic obstacles. This paper introduces a generalized framework that combines diffusion-based trajectory generation with deep reinforcement learning (DRL). The diffusion component generates diverse candidate trajectories by modeling feasible sub-paths, where a sub-path denotes a short-horizon segment aligned with receding-horizon execution. In this formulation, the entire trajectory is decomposed into consecutive sub-paths, enabling the diffusion model to learn local collision avoidance and smoothness while maintaining consistency across the fully identified path (e.g., global path and whole trajectory). The DRL component then evaluates these candidates online, selecting actions that improve safety while adapting to dynamic obstacles and maintaining energy-efficient behavior. The joint design leverages the generative diversity of diffusion and the adaptive decision-making of DRL, producing a planner that is both responsive and reliable. To assess effectiveness, the method is evaluated in unmanned aerial vehicle (UAV) path optimization scenarios with dynamic obstacles. The results demonstrate that sub-path training enhances the generalization of diffusion-based planners by linking local feasibility to global performance, and that the approach offers a practical solution for real-time UAV trajectory optimization with improved safety and efficiency.", "tldr": "", "keywords": ["Diffusion model", "Reinforcement learning", "trajectory optimization"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec3002dc3f640e04292f42f7f191c73545fe0b0b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors proposes a hybrid framework for real-time trajectory planning in dynamic 3D environments. The method decomposes the planning problem into generating and selecting short-horizon sub-paths. A diffusion model is then trained to generate a diverse set of feasible candidate sub-paths. Subsequently, a DRL agent selects the optimal sub-path from this candidate set based on a multi-objective reward function that considers safety, efficiency, and goal progress. The proposed framework is evaluated in a simulated UAV environment with static and dynamic obstacles, demonstrating superior performance in success rate and safety compared to heuristic, classical, and pure diffusion-based planning methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper tackles the challenging and highly relevant problem of real-time motion planning in dynamic environments.\n2. The proposed method is well-excuted in simulated UAV environment."}, "weaknesses": {"value": "1. My main concern is the insufficient comparison with relevant baselines. While the paper compares against classical and diffusion planners, it overlooks several relevant works such as MPD [1], MMD [2], and SMD [3]. Without comparisons to these methods, it is difficult to fully assess the contribution of the proposed approach.\n2. The experiments are conducted in a simulated environment with geometrically simple obstacles. It is unclear how the proposed method scales to environments with more complex, non-convex obstacle geometries, such as those derived from real-world sensor data (e.g., LiDAR point clouds). \n3. The reward function in Equation is a weighted sum of seven distinct terms. Such complex reward functions can be sensitive to the choice of weights, which often require extensive tuning. \n\n[1] Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models, 2023\n\n[2] Simultaneous Multi-Robot Motion Planning with Projected Diffusion Models, 2025\n\n[3] Multi-robot motion planning with diffusion models, 2025"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KchIlfHjPS", "forum": "MKM8iEaowV", "replyto": "MKM8iEaowV", "signatures": ["ICLR.cc/2026/Conference/Submission18881/Reviewer_AQ23"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18881/Reviewer_AQ23"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760870534593, "cdate": 1760870534593, "tmdate": 1762930853257, "mdate": 1762930853257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an motion planning algorithm with online replanning which, at each replanning step, uses a diffusion model to generate a small set of short-horizon candidate trajectories, from which an RL policy subsequently selects the best candidate."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Motion planning via multiple candidate trajectory generation followed by candidate selection is a popular and sensible approach. This paper’s idea to combine diffusion, which excels in parameterizing rich distributions over trajectories, with an RL policy, which learns to maximize a user-defined reward by choosing a candidate from a set of samples drawn from the diffusion model, is therefore a potentially impactful direction to explore."}, "weaknesses": {"value": "This paper appears to be unfinished. There are a lot of missing details, especially on the diffusion model training and candidate generation (I could not find any information on what data the diffusion model is trained on, how trajectories are parameterized, etc.). The missing information hinders understanding of the method and makes it impossible to understand the reported results. The experimental evaluation is very limited, consisting of a single experiment whose setup is not clearly explained (How many different environments were evaluated? What constitutes a timeout?), and with questionable baselines (see Q3/Q4 below).\n\nThe paper also fails to reference or compare against relevant prior work, such as [1], which also uses DRL to select among candidates from a “high level action space” (although this high level action space is much larger than the 12 candidates used here, and not produced by sampling from a generative model, the overall structure of the approach is very similar). It would be important to position your contribution in the context of such high-level-action-selection-type work.\n\n[1] K. R. Williams et al., \"Trajectory Planning With Deep Reinforcement Learning in High-Level Action Spaces,\" in IEEE Transactions on Aerospace and Electronic Systems, vol. 59, no. 3, pp. 2513-2529, June 2023, doi: 10.1109/TAES.2022.3218496. https://ieeexplore.ieee.org/document/9940484 / https://arxiv.org/abs/2110.00044"}, "questions": {"value": "**Q1.** Can you provide details on the diffusion model training? What dataset do you use, or do you behavior-clone some kind of expert planner? How are trajectories parameterized? How is uncertainty evaluated? **How do you ensure that your candidate set is diverse enough?** In general, there are almost no details or analysis regarding the diffusion model, and it would be crucial to include this.\n\n**Q2.** Why are all the trajectory segments straight lines, and why are they so long (Figure 3)? Is your diffusion model only sampling very coarse straight line segments?\n\n**Q3.** What is happening in Figure 3 (d) and (e) near to the goal/arrival point? It looks like these baselines are “jumping around” the goal, rather than terminating, potentially due to a bug in the implementation (in the text, you mention obstacles in close proximity to the goal, but I do not see any in the visualization). This brings into question the results reported in Table 1.  Furthermore, why is there a big visual offset between the yellow goal point and the end of the trajectories in all the other panels (except d and e)?\n\n**Q4.** Can you compare against stronger heuristic/classical baselines, like A* on a grid, or A*/RRT followed by nonlinear optimization? Do you allow your classical baselines to compute an updated plan at each timestep, or do you only compute it once and then execute?\n\n**Q5.** What is the performance of your method as a function of the number of candidate samples? In particular, it would be important to evaluate replanning without the policy at all (i.e. setting the number of candidates to 1).\n\n**Q6.** Can you show the output of the diffusion model? It would be interesting and insightful to have a visualization of multiple samples from the diffusion model in order to qualitatively evaluate the diversity and quality of the candidate set.\n\n**Q7.** Does it run in real time? What is the runtime/performance?\n\n**Q8.** Regarding Algorithm 1 in the appendix: why do you interleave diffusion and policy training? In my understanding, there are no gradients flowing between the two modules, so it would be equivalent and more efficient to train the diffusion model first (independently) and then later train the candidate selection policy using the pretrained diffusion model. Is my assessment correct, or did I misunderstand?\n\n---\n\nMinor comments:\n* The intro is inconsistent with the rest of the paper. For example, the explanation of the selection approach is inconsistent: In the intro (L051), it is stated that the final trajectory is a convex combination of candidates, but in Section 4, the trajectory selection process is described as being a “hard” assignment. The intro also references a “Shield” which is never elaborated on.\n* In the appendix, “Proof of diffusion and RL algorithm”: this is just a collection of background information, so it should not be called a “proof”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9zxF14cBQe", "forum": "MKM8iEaowV", "replyto": "MKM8iEaowV", "signatures": ["ICLR.cc/2026/Conference/Submission18881/Reviewer_dFYo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18881/Reviewer_dFYo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761162344914, "cdate": 1761162344914, "tmdate": 1762930852382, "mdate": 1762930852382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper integrates diffusion-based trajectory generation with deep reinforcement learning (DRL). The diffusion model generates candidate sub-paths, from which the DRL agent selects the optimal one to produce the final action output. Experimental results demonstrate that the proposed method performs well in unmanned aerial vehicle (UAV) path optimization scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces diffusion-based planning into deep reinforcement learning (DRL) and demonstrates strong performance on the unmanned aerial vehicle (UAV) path planning task."}, "weaknesses": {"value": "1. The paper applies diffusion-based planning to the Unmanned Aerial Vehicle (UAV) path planning problem. However, the approach is highly problem-specific, which limits its generality and novelty. Moreover, the techniques used in the work are largely standard and not conceptually new.\n\n2. The components shown in the model figure are not clearly explained in the text, making it difficult for readers to understand their roles and design motivations.\n\n3. Several ideas presented in the paper lack novelty and do not appear promising. For example, the sub-path generation process is essentially identical to what the original Diffuser [1] framework performs in practice. Similarly, multi-horizon trajectory handling has been studied extensively in prior works such as MCTD [2] and trajectory stitching [3].\n\n4. The paper lacks experimental comparisons with state-of-the-art diffusion-based reinforcement learning methods, which weakens the empirical evaluation.\n\n5. The equations in Section A of the Appendix merely list the formulas used in the method rather than providing any theoretical justification or proof of effectiveness.\n\n6. The paper claims that the proposed method ensures policy safety; however, no theoretical analysis or empirical evidence is provided to support this claim.\n\n[1] Janner, Michael, et al. \"Planning with diffusion for flexible behavior synthesis.\" arXiv preprint arXiv:2205.09991 (2022).\n\n[2] Yoon, Jaesik, et al. \"Monte carlo tree diffusion for system 2 planning.\" arXiv preprint arXiv:2502.07202 (2025).\n\n[3] Luo, Yunhao, et al. \"Generative trajectory stitching through diffusion composition.\" arXiv preprint arXiv:2503.05153 (2025)."}, "questions": {"value": "1. The authors claim that existing selection methods provide only short-term improvements and fail to capture long-horizon trade-offs. However, I am not convinced by this statement, as current diffusion-based planning methods are generally designed to optimize long-term performance. Could the authors clarify why they believe existing selection methods are limited to short-term improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JPdODmPVhQ", "forum": "MKM8iEaowV", "replyto": "MKM8iEaowV", "signatures": ["ICLR.cc/2026/Conference/Submission18881/Reviewer_gdmG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18881/Reviewer_gdmG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975890492, "cdate": 1761975890492, "tmdate": 1762930851208, "mdate": 1762930851208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a generative trajectory planning framework that integrates diffusion models and deep reinforcement learning (DRL) to address the \"safety-energy efficiency-real-time performance\" trade-off in real-time trajectory optimization under dynamic environments (with static and dynamic obstacles), and validates its effectiveness through UAV 3D simulation experiments, outperforming heuristic, classical sampling, and pure diffusion baselines. The key contributions are:\n1. Decompose long trajectories into short-horizon sub-paths, reducing the dimensionality of the DRL action space and solving the problem of unstable training in high-dimensional trajectory generation with pure DRL.\n2. Integrate the generative diversity of diffusion models (for generating safe and diverse candidate sub-paths) with the adaptive decision-making of DRL (for online optimal sub-path selection), balancing real-time responsiveness and environmental robustness.\n3. Design a safety-aware state representation (incorporating sub-path uncertainty and collision probability) and a multi-objective reward function (covering goal achievement, obstacle avoidance, and energy consumption), providing a transferable framework for trajectory planning in dynamic environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality is strong. It targets core limitations of existing methods: diffusion models often suffer from full-trajectory generation inefficiency, while DRL struggles with high-dimensional waypoint training. Its hierarchical \"sub-path generation-selection\" design cuts diffusion computational complexity and integrates diffusion-derived safety attributes (e.g., collision probability) into DRL state representation—an innovation rare in existing diffusion-RL frameworks.\n\nQuality is good. Theoretical foundations are solid: detailed derivations for diffusion’s forward/backward processes, classifier-free guidance, and maximum entropy RL’s soft Q-functions, with unified mathematical notation (e.g., α_t, β_t in diffusion). Experiments are rigorous: three baseline types, comprehensive metrics (success rate, collision rate, etc.), and quantitative (Table 1) + qualitative (Figure 3) validation (e.g., pure diffusion’s 72% timeout vs. segmented diffusion-RL’s 0%-8%).\n\nClarity is good. Structure is logical: \"problem → related work → preliminaries → method → experiments → conclusion\". Figures (e.g., Figure 1’s framework flow, Figure 2’s training reward comparison) aid understanding, and key terms (e.g., sub-path diffusion) are explained on first mention.\n\nSignificance is high. It addresses core needs in UAV navigation/autonomous driving (real-time, collision-free, low-energy trajectories). The UAV-specific design (considering kinematics/energy models) and transferable framework make it practically valuable for engineering applications."}, "weaknesses": {"value": "1. Experiments are simulation-only (no real-world validation) and lack dynamic obstacle scalability analysis.\n\nSuggestion: Add hardware-in-the-loop/real-platform tests (or state simulation limitations); test 20/50 obstacles and plot scalability curves.\n\n2. Shield mechanism, sub-path length selection lack details/validation; no reward weight sensitivity analysis.\n\nSuggestion: Append Shield pseudocode/performance comparisons; test 8/64-step sub-paths; analyze reward weight impact (e.g., adjusting w_c).\n\n3. Incomplete related work (misses 2024–2026 studies) and no core component ablation experiments.\n\nSuggestion: Include latest hierarchical RL-diffusion studies; add ablations (e.g., removing sub-path decomposition).\n\n4. No statistical significance analysis to verify method differences.\n\nSuggestion: Conduct 3+ repeated experiments, add error bars, and use t-tests for significance."}, "questions": {"value": "1. Do you plan to test the algorithm on real UAVs? If yes, how to handle simulation-real discrepancies (e.g., sensor noise)?\n\n2. What is the basis for choosing 16/32-step sub-paths? How does the Shield detect \"grazes\" and calculate lateral offsets?\n\n3. Why fix dynamic obstacles at 10? Why omit statistical significance analysis?\n\n4. Are reward weights experience-based or hyperparameter-searched? How do weight adjustments affect multi-objective trade-offs (e.g., w_c vs. energy consumption)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k0Fhcv0wK3", "forum": "MKM8iEaowV", "replyto": "MKM8iEaowV", "signatures": ["ICLR.cc/2026/Conference/Submission18881/Reviewer_8EaP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18881/Reviewer_8EaP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240702922, "cdate": 1762240702922, "tmdate": 1762930847395, "mdate": 1762930847395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}