{"id": "jLItllJ5xm", "number": 16401, "cdate": 1758264236366, "mdate": 1759897242782, "content": {"title": "Is Graph Mixup Beneficial? Investigating Interpolation And Empirical Performance of Graph Mixup Methods", "abstract": "Mixup is a widely used data augmentation technique that constructs new training\n  examples by interpolating between existing ones. While effective in domains like\n  vision and language, applying mixup to graph data is challenging. In this paper, we\n  analyze and empirically explore state-of-the-art graph mixup methods. We conducted an\n  independent evaluation following established evaluation protocols for graph\n  classification and found that none of the mixup methods yielded statistically\n  significant improvements over the no-mixup baseline. To obtain further insights, we\n  analyzed the graphs generated from existing mixup methods from an interpolation\n  perspective using the graph edit distance. We found that (i) many mixup methods failed\n  to interpolate well, (ii) that mixup methods that interpolated well often outperform\n  methods that did not, (iii) even optimal interpolation did not lead to performance\n  improvements. Our findings highlight the need for a more rigorous exploration and\n  evaluation of mixup for graphs.", "tldr": "We investigate graph mixup and analyze prediction performance as well as interpolation properties of prior methods empirically.", "keywords": ["Graph Neural Networks", "Data Augmentation", "Mixup", "Graph Classification", "Graph Edit Distance", "Representation Learning", "Evaluation"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be6e51b3296c56dd4e1cb6477b8ed07ce18c4f99.pdf", "supplementary_material": "/attachment/e2770b0eb7191f25af67783516cd70488886253a.zip"}, "replies": [{"content": {"summary": {"value": "The work focuses on several interesting questions regarding graph mixup problems and methods: Does the interpolation align with the graph edit distance (GED)? Does misalignment lead to poor empirical performance? Does GED-aligned interpolation improve performance compared to the baseline? The paper first reviews different graph mixup methods, ranging from structure-based to embedding-based approaches, and then conducts empirical analyses to examine the benefits of mixup. Next, it analyzes the cause of performance differences by comparing interpolation errors with GED. The results are interesting. In summary, this is an engaging direction, and the paper provides valuable insights related to GED. However, it deserves clearer presentation to highlight its contributions—such as whether GED has been used before to evaluate or analyze mixup, whether GED-based mixup is newly proposed in this work, and what the generalizability and limitations of GED and the paper’s conclusions are in graph tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work focuses on several interesting questions regarding graph mixup problems and methods and is organized to address them step by step, making the paper relatively easy to follow.\n\n2. The idea of GED-aligned mixup is interesting."}, "weaknesses": {"value": "## 1. Evaluation tasks should be extended\nOne important application of mixup is data augmentation to address the issue of limited training data. This is a common scenario for molecular tasks, which can naturally be modeled as graphs. However, the current work only includes four datasets, with only one molecular dataset, and it is unclear what the data distributions look like—whether the labeled data are imbalanced and whether data augmentation is truly needed.\n\nConclusions such as “even optimal interpolation did not lead to performance improvement” and “graph mixup provided no significant improvement over the no-mixup baseline” are too arbitrary and dataset-specific. It is important to ensure that these conclusions are generalizable across different types of graph learning problems.\n\nTo make the conclusions more robust and convincing, the study should be extended to broader molecular benchmarks, such as those from OGB, MoleculeNet, or Polaris. Many molecular tasks are regression problems, which is another missing aspect in the paper’s current analysis. The tasks should also explicitly consider issues related to data imbalance and small-data regimes.\n\n## 2. Insufficient coverage of existing mixup algorithms\n\nThe set of reviewed and evaluated mixup algorithms appears limited. Given the strong statement that “graph mixup provided no significant improvement over the no-mixup baseline, which questions the practical benefits of graph mixup,” several aspects remain unclear: What exactly is the no-mixup baseline? Which mixup variants are currently evaluated, and do they represent all major categories of mixup methods? Currently, only one embedding-based mixup approach is included. For a more systematic and fair evaluation, additional baselines should be considered. For example, there exists a line of research on graph rationale-based methods, which share conceptual similarities with mixup (e.g., selectively preserving interpretable parts of graphs). Including such methods would make the empirical comparison more comprehensive.\n\n\n## 3. Limitations and generalizability of the mixup algorithms\n\nDifferent mixup methods may vary in efficiency. How efficient is the GED-based mixup? An analysis of the training time and a comparison with other baselines should be included.\n\nAnother question concerns how different mixup methods generalize to molecular and other graph-structured data, since different graphs may have different node and edge features. When performing structural mixup, do the authors also mix the feature dimensions of nodes and edges? How are these aspects handled, and to what extent do these design choices affect the model’s performance?"}, "questions": {"value": "1. Is the GED-Mixup method newly proposed in this paper? How does it differ from or simplify the EPIC method?\n\n2. How much computation time does the GED-Mixup process require? Is the runtime related to the dataset size or the graph size? How long does each training epoch take with this method?\n\n3. Can GED-Mixup be applied to molecular graphs? Specifically, can it mix nodes and edges that contain multiple features, including both discrete and continuous values?\n\n4. What types of atom and bond features are used in the MUTAG dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "21sdf3yHaQ", "forum": "jLItllJ5xm", "replyto": "jLItllJ5xm", "signatures": ["ICLR.cc/2026/Conference/Submission16401/Reviewer_z4K1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16401/Reviewer_z4K1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682382080, "cdate": 1761682382080, "tmdate": 1762926522774, "mdate": 1762926522774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Frequently Asked Questions (FAQs): FAQ-1 & FAQ-2"}, "comment": {"value": "We'd like to thank all reviewers for their insightful, constructive comments and\ntheir support of our work.\n\nWe'll address some of the most prominent thoughts being raised below, and we\nplan to include these discussions into the revised version. Once we performed\nthe revision, we'll summarize all of our changes here.\n\nWe greatly appreciate feedback on these thoughts and suggestions.\n\n### FAQ-1: Mixup methods with bad interpolation properties fell behind. What can we learn from good interpolation properties though?\n\nProbably not much.\n\nOur study suggests (with statistical significance) that bad interpolation\nproperties are detrimental for the performance of graph mixup, i.e., decent\ninterpolation appeared necessary. But, as you observed and our study also found,\ngood interpolation properties are clearly not sufficient and, consequently, the\nmIE does not appear to be well-suited to compare methods with decent\ninterpolation properties (and we do not claim it to be).\n\nIn more detail, there are many ways to interpolate graphs such that the\ninterpolation error is decent. Roughly speaking, the entire green region in Fig.\n1 of our paper can be seen as decent interpolation, but it's not clear which of\nthese interpolations is best w.r.t. downstream performance.\n\nFor example, GED-Mixup makes use of an optimal edit set. For each edit set,\nhowever, there are many possible edit paths, some of which may be more suitable\nthan others for mixup. GED-Mixup simply chooses one of these paths at random\n(subject to validity constraints), but this may indeed not be the most suitable\nchoice. We feel that an important direction for future work is to explore this\nline of thought further; this is one of the key takeaways of our work.\n\n### FAQ-2: The paper focuses on classification accuracy. What about other tasks or settings?\n\nWe followed the literature here.\n\nThe objective of most of the work on graph mixup is to improve graph\nclassification accuracy, and the primary experiments of these studies are\nevaluating classification accuracy. The goal of our paper was to study these\nmethods in a unified, solid, and statistically sound experimental setup for the\ntask they were designed for.\n\nThe impact of graph mixup for other tasks and on other settings---such as\nregression, robustness properties, or the ability to handle label noise---may be\ndifferent, indeed. While interesting and worth exploring, these directions\ngenerally exceed the scope of this paper.\n\nThat being said, we will use the rebuttal period to try to shed some insight\ninto these points. A natural candidate for exploration is the impact of label\nnoise; we thus plan to focus on this aspect."}}, "id": "tMyiG0GPLl", "forum": "jLItllJ5xm", "replyto": "jLItllJ5xm", "signatures": ["ICLR.cc/2026/Conference/Submission16401/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16401/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16401/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763138930160, "cdate": 1763138930160, "tmdate": 1763138930160, "mdate": 1763138930160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits graph mixup techniques for graph classification. It conducts an empirical evaluation of several state-of-the-art graph mixup methods and analyzes their behavior through the lens of graph edit distance. Through both statistical testing and interpolation-based analysis, the study finds that current graph mixup methods provide no significant improvement over the no-mixup baseline, even when interpolation quality is high."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides a unified and principled analysis of various graph mixup methods using edit distance as a common framework, which is conceptually elegant and insightful.\n- The paper is well written and easy to follow. The discussion of related work is clear and well organized, providing an informative overview of existing graph mixup approaches."}, "weaknesses": {"value": "- The experimental scope appears limited. The study evaluates only four datasets and contain relatively small graphs. This dataset selection may make it difficult to generalize the findings to other graph domains. Since the subsequent analyses rely heavily on these empirical results, expanding the dataset diversity would greatly strengthen the study’s conclusions.\n- Figure 2 provides intriguing evidence that lower mIE values are associated with better accuracy; however, this relationship remains somewhat inconclusive, as different mixup methods vary in several aspects beyond mIE. To better support the argument, the authors could run an ablation study using GED-Mixup. They could create edit sets that are not perfectly optimal (with higher mIE values) and gradually adjust how suboptimal they are. Observing how performance changes in this setting would help clarify whether mIE actually affects accuracy.\n- The domain composition of the datasets also warrants consideration. GED is particularly appropriate for molecular or bioinformatics graphs, where small structural edits correspond to meaningful chemical or biological variations. This might partly explain why interpolation quality correlates strongly with performance in these datasets. In contrast, on the IMDB-BINARY social-network dataset, methods with low mIE (e.g., GED-Mixup, SubMix, If-Mixup) do not exhibit clear performance advantages, suggesting that the observed trend could be domain-specific rather than universal."}, "questions": {"value": "- The study evaluates only four small-scale TUDataset benchmarks. How confident can we be that the findings generalize to larger or more diverse graph domains, such as molecular graphs with higher node counts, biochemical interaction networks, or large social networks?\n- Results on the IMDB-BINARY dataset appear inconsistent with the findings from molecular datasets. Is the link between interpolation fidelity and downstream accuracy dependent on domain semantics, such as chemically meaningful edit operations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "04zvx5t9NX", "forum": "jLItllJ5xm", "replyto": "jLItllJ5xm", "signatures": ["ICLR.cc/2026/Conference/Submission16401/Reviewer_CTcZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16401/Reviewer_CTcZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812642339, "cdate": 1761812642339, "tmdate": 1762926522408, "mdate": 1762926522408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Frequently Asked Questions (FAQs): FAQ-3 & FAQ-4"}, "comment": {"value": "### FAQ-3: The study uses four real-world datasets. What about other datasets?\n\nWe'd first like to highlight the rationale for choosing these four datasets:\n\n1. They are frequently used in the existing literature on graph mixup.\n2. They cover a variety of domains: social networks (IMDB), bioinformatics\n   (ENZYMES, PROTEINS), molecular data (MUTAG).\n3. The experimental study is computationally feasible. In particular, methods\n   such as FGW-Mixup and GeoMix have substantial computational cost, but we\n   wanted to include these methods.\n4. Our goal was to use a unified, solid, and statistically sound experimental\n   setup. This further enhances computational costs, as we performed extensive\n   hyperparameter optimization (including the no-mixup baseline), used\n   cross-validation throughout to obtain more trustworthy performance estimates,\n   and also analyzed interpolation properties (which requires GED computations).\n5. A key motivation of using graph mixup is to deal with data scarcity, i.e.,\n   relatively small training set sizes for complex tasks.\n\nIn our statistical analysis, we account for our use of these four datasets when\ncomputing standard errors (esp. under A1).\n\nEven adding a single dataset to the study increases cost substantially and,\ngiven the results so far, may provide only limited insight. We are nevertheless\nthinking about and willing to add an additional dataset, although it's not\nimmediately clear which one would likely provide most additional insight\n(suggestions are welcome).\n\nWe will also provide the source code of our framework: experiments with new\ndatasets and new mixup methods can then be performed easily.\n\n### FAQ-4: GED computation is hard. Is the method sufficiently efficient?\n\nYes. We will provide concrete runtime costs in the revised study (in fact, some\nof the prior methods are more costly than GED-Mixup).\n\nWhile the worst-case time complexity of GED appears impractical, its computation\ncan be feasible in practice and, in fact, it was feasible in our study. The\ndevelopment of high-performing methods for exact and approximate GED computation\nis an ongoing research direction; e.g., Blumenthal et al. (2020), Chang et al.\n(2023), Xu & Chang (2025). These directions are orthogonal to our work.\n\nIn our work, we used exact GED computation because we did not want the analysis\nto be biased by approximation errors. Using a state-of-the-art exact method, we\ncomputed GEDs for graphs with more than 500 vertices and more than 1000 edges in\nour study, without running into computational issues (75% of the GED\ncomputations took less than 0.1s, and these computations are embarrassingly\nparallelizable across pairs of graphs). These graph sizes cover a substantial\nfraction of the use cases for graph classification.\n\nFinally, note that we use GED computations as well as GED-Mixup as an analysis\ntool, not as a method for practical deployment so that runtimes are not as\ncritical.\n\n---\n\nBlumenthal, D. B., Boria, N., Gamper, J., Bougleux, S., & Brun, L. (2020).\nComparing heuristics for graph edit distance computation. The VLDB Journal,\n29(1), 419–458.\n\nChang, L., Feng, X., Yao, K., Qin, L., & Zhang, W. (2023). Accelerating Graph\nSimilarity Search via Efficient GED Computation. IEEE Transactions on Knowledge\nand Data Engineering, 35(5), 4485–4498. IEEE Transactions on Knowledge and Data\nEngineering.\n\nXu, M., & Chang, L. (2025). Graph Edit Distance Estimation: A New Heuristic and\nA Holistic Evaluation of Learning-based Methods. Proc. ACM Manag. Data, 3(3),\n167:1-167:24."}}, "id": "WIsA5XzcXQ", "forum": "jLItllJ5xm", "replyto": "jLItllJ5xm", "signatures": ["ICLR.cc/2026/Conference/Submission16401/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16401/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16401/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763138966617, "cdate": 1763138966617, "tmdate": 1763138966617, "mdate": 1763138966617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes whether graph mixup actually improves graph classification performance. It first conducts an empirical comparison of existing graph mixup methods against baselines on four benchmark datasets, finding that none achieve statistically significant improvements. To further test generality, the authors perform a pooled analysis across datasets and models, showing that even when aggregated, mixup methods fail to yield consistent benefits. They then introduce an interpolation-based analysis framework using the graph edit distance to quantify interpolation error and assess the quality of augmented graphs. Additionally, they propose a mixup method based on optimal graph alignment, termed GED-Mixup. Their results show that most existing methods have high interpolation error, and that while better interpolation tends to correlate with improved performance, even optimal interpolation does not lead to significant gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an independent and systematic empirical evaluation of existing graph mixup methods using a unified experimental setup and pooled analysis. This contributes to a clearer understanding of the empirical effectiveness of mixup in graph classification, addressing inconsistencies in prior studies.\n\n2. The introduction of an interpolation-based metric using graph edit distance is a useful addition, providing a quantitative way to assess how well mixup outputs interpolate between input graphs. This analysis helps connect structural properties of the generated graphs with their empirical performance - an aspect that has been largely overlooked in earlier work."}, "weaknesses": {"value": "1. The paper shows that existing graph mixup methods do not yield significant performance gains; however, it remains unclear why mixup fails. The authors demonstrate that many methods produce poor interpolations, yet even optimal interpolation (via GED-Mixup) does not improve accuracy significantly (Figure 2). This raises an unanswered question about the underlying cause of mixup’s ineffectiveness. The interpretation would benefit from a deeper diagnostic analysis.\n\n2. The paper also overemphasizes negative results without exploring other potential benefits of mixup. Prior work suggests that mixup can improve robustness to topology perturbations and label noise, but this study focuses solely on classification accuracy. A discussion or evaluation of such alternative objectives would provide a more balanced perspective and clarify whether mixup is universally ineffective or only for accuracy metrics.\n\n3. The evaluation scope is limited to relatively small TU datasets. Including larger and more diverse benchmarks (e.g., Reddit, DD) would strengthen the conclusions and assess generalizability to real-world or large-scale graph settings.\n\n4. The proposed GED-Mixup method is interesting but computationally impractical for larger graphs as mentioned in the paper. The paper does not discuss viable approximations or scalable alternatives, leaving open the question of how GED-based interpolation could be applied in realistic scenarios.\n\n5. Finally, as a suggestion, it would be valuable to compare newer methods such as MomentMixup (which mixes graph moment vectors and may reduce interpolation error) and SIGL (which modifies alignment in G-Mixup). Evaluating these under the proposed interpolation framework could yield further insights into the design of effective graph mixup strategies."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EZfDHAfDI0", "forum": "jLItllJ5xm", "replyto": "jLItllJ5xm", "signatures": ["ICLR.cc/2026/Conference/Submission16401/Reviewer_uWeG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16401/Reviewer_uWeG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828195757, "cdate": 1761828195757, "tmdate": 1762926521867, "mdate": 1762926521867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a graph mix-up method, a graph generator methodology which merges two input graphs, based on graph edit distance which is suitable for typically small graphs. \n\nThe authors use a novel evaluation methodology focused on interpolation error. The authors demonstrate that prior mixup works failed to generate graphs which interpolate between their inputs."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The authors demonstrate an important empirical finding in this research area, using a novel evaluation methodology. Prior mix-up works are fairly niche as a graph generative model, however, within this prior work the negative finding of structural coherence is very significant.  \n\nFurthermore, the presentation of the work is simple and understandable to a general AI research audience. This paper could be convincing for further work in this area. \n\n2. The GED-mixup method is well motivated and suitable in the many domains for small graphs. This seems like a reasonable assumption, where higher controllability is be better suited for smaller graphs; large graph generation could be bracketed as work of a graph foundational model, this is fine.\n\n3. The authors scope their research questions well and empirically support each of them. The two contributions (Sec 1) are significant."}, "weaknesses": {"value": "1. Over-reliance on fidelity: the authors argue but don't demonstrate the utility of measures such as mIE. That is, what is the qualitative impact of methods with similar ACC but higher mIE, e.g. in Fig 2? Similarly, the authors don't present an evaluation of downstream robustness, e.g. for distribution shift, etc, which are the common use-cases for graph augmentation. The same critique is true: the graph generator distribution need not necessarily have good mIE if it adds robustness along another problem dimension.\n\n2. The three levels of pooled analysis are difficult to follow and are not well reflected in the figures. e.g. is fig 2 representative under A2 assumptions? Is Fig 3 presented under A3? More space could be dedicated to contrasting results at these pooling levels."}, "questions": {"value": "1. What is the downstream effect of mixup with high ACC and high mIE (e.g. Fig 2)? \n\n2. Are there applications where high interpolation fidelity might not be best?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "APDx3GxMKZ", "forum": "jLItllJ5xm", "replyto": "jLItllJ5xm", "signatures": ["ICLR.cc/2026/Conference/Submission16401/Reviewer_BfwX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16401/Reviewer_BfwX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933673816, "cdate": 1761933673816, "tmdate": 1762926521294, "mdate": 1762926521294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}