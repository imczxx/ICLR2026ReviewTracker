{"id": "qRSlhvA38I", "number": 4154, "cdate": 1757613765912, "mdate": 1759898050118, "content": {"title": "SelfGrader: Detecting Jailbreak Attacks on Large Language Models with Token-Level Logit Distribution", "abstract": "Large Language Models (LLMs) are powerful tools for answering user queries, but remain highly vulnerable to jailbreak attacks. Existing guardrail methods typically rely on internal features or textual responses to detect malicious queries, which either introduce substantial latency or suffer from the randomness in text generation. To overcome these limitations, we propose $\\textbf{SelfGrader}$, a lightweight guardrail method that solves jailbreak detection as a numerical grading problem using token-level logits. SelfGrader prompts the LLM to evaluate an input query with a compact set of {numerical tokens} (NTs) (e.g., 0–9) and interprets their logit distribution as an internal safety signal. To align these signals with human intuition of maliciousness, SelfGrader introduces a novel in-context prompting strategy and a dual-perspective scoring rule that considers both the maliciousness and benignness of the query, yielding a stable and interpretable score that both reflects harmfulness and reduces the false positive rate. Extensive experiments across diverse jailbreak benchmarks, multiple LLMs, and state-of-the-art guardrail baselines demonstrate that SelfGrader achieves accurate and robust detection while maintaining low computational overhead and latency.", "tldr": "", "keywords": ["Jailbreak Attack", "LLM", "Guardrail"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0c31bf0223092871ff746b827982befa594347a0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes SelfGrader, a guardrail that converts jailbreak detection into a numerical grading task using NT-based (numeric token) logits and a Dual-Perspective Logit (DPL) score. The method aims to be lightweight (low latency / memory) and robust across diverse attack families without training or keyword matching. Main claims are laid out in the intro and method (ICL anchoring + DPL scoring) and evaluated on Llama-3-8B-Instruct and Vicuna-13B-v1.5 with numerous attacks and ablations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Simple, implementation-friendly idea (two forward passes, L=1 default, small logit slice), with clear latency/memory accounting\n2. Method clarity: the DPL rule is well specified (normalization, tail-trim w, balance parameter λ), and the ICL prompts are fully disclosed in the appendix. \n3. Broad evaluation: many attack families (static, model-specific, adaptive), two targets, and several competitive baselines."}, "weaknesses": {"value": "1. Lack of novelty:\nThe idea of using NT-based logits as safety signals is merely a direct variation of existing approaches such as Self-Evaluation (Ren et al., 2023) and Self-Certainty (Kang et al., 2025). This paper only applies that idea to jailbreak detection, without offering fundamental innovation. Moreover, the method of extracting values through logits is similar to QGuard: Question-based Zero-shot Guard for Multi-modal LLM Safety.\n\n2. Performance instability:\nLooking at Table 1, even SelfGrader (Q=101) shows wildly varying PGRs across attack types — 52.38% on MultiJail but 0% on ActorAttack. While the authors emphasize lower average values, individual attacks still exhibit high bypass rates. \n\n3. Vulnerable to adaptive attacks:\nIn Table 2, SelfGrader (Q=101) performs worse on the TAP attack (ASR 6.00%, PGR 20.00%) than GradientCuff (ASR 2.00%, PGR 3.67%). \n\n4. Increasing Q is meaningless:\nTable 1 shows little difference across Q = 2, 10, 101, 1000 (mean ASR = 1.75%, 1.75%, 1.32%, 1.08%). If performance is nearly identical, the design rationale for increasing the number of numeric tokens (NT) is weak. The claim that “Q=1000 is more precise” is not supported by the numbers.\n\n5. Prompt injection vulnerability:\nIn the appendix (Table 7), PGR spikes up to 73.66%. This implies that even simple injection attacks can effectively neutralize the guardrail.\n\nThe claim that SelfGrader is “robust” because ASR remains 0% under prompt injection is not convincing. Elevated PGR values (up to 73.66%) clearly indicate that the scoring mechanism was successfully manipulated. While no harmful outputs were observed in this limited setting, the fact that the guardrail accepts adversarial prompts as benign is itself a security failure. Moreover, the adversarial prompt tested was simplistic; more sophisticated injection strategies could plausibly yield nonzero ASR. Thus, the evidence suggests SelfGrader is vulnerable to scoring corruption, and robustness claims are overstated without broader evaluations and more realistic adversarial tests.\n\nTable 5.\nSelfGrader+SelfDefend improves performance under MultiJail but degrades performance under ActorAttack. \nUnder what conditions is the combination advantageous, and when does it hinder performance?"}, "questions": {"value": "1. Why is there almost no performance difference when increasing Q from 2 to 1000? What theoretical significance does expanding the NT set have?\n\n2. Why are the results unstable in the Vicuna experiments? Is this due to limitations of the NT set, or differences in the model itself? Clarification is needed.\n\n3. Given that the method is essentially defenseless against prompt injection, what methods do you propose to mitigate this weakness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YQQS0owfSX", "forum": "qRSlhvA38I", "replyto": "qRSlhvA38I", "signatures": ["ICLR.cc/2026/Conference/Submission4154/Reviewer_FHc7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4154/Reviewer_FHc7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760781472312, "cdate": 1760781472312, "tmdate": 1762917201846, "mdate": 1762917201846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a lightweight guardrail called SelfGrader for detecting jailbreak attacks against large language models (LLMs). The proposed method, SelfGrader, reframes jailbreak detection as a numerical grading task using token-level logits rather than generated text or auxiliary classifiers. Specifically, the model is prompted to “grade” the maliciousness of a query using a compact set of numerical tokens and uses the logit distribution as an evaluation metric. \nSelfGrader introduces two key innovations 1, An in-context learning (ICL) prompting strategy that curated query–score examples over time. And 2, A Dual-Perspective Logit (DPL) scoring rule that combines both maliciousness and benignness assessments to yield a stable and balanced safety score.\nExperiments across multiple jailbreak benchmarks and benign datasets show that SelfGrader achieves competitive or superior detection accuracy compared with state-of-the-art guardrails while maintaining low false positive rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1, Strong empirical performance\n\nExperiments across diverse jailbreak and benign benchmarks show consistently low attack success rates and false positive rates compared to stronger baselines.\n\n2, Dual-perspective scoring for robustness\n\nThe proposed Dual-Perspective Logit (DPL) is interesting, it seems to effectively balance maliciousness and benignness evaluations, reducing both false positives and false negatives.\n\n3, Lightweight and efficient design \n\nSelfGrader operates without external classifiers, gradient computations, or fine-tuning, achieving low latency and minimal GPU overhead, making it practical for real-world deployment."}, "weaknesses": {"value": "1, LLM-as-judge is well-trodden\n\nPlenty of recent work already uses an LLM to score harmfulness for defense and evaluation. For example, Agentharm benchmark (https://openreview.net/forum?id=AC5n7xHuR1) at ICLR 2025 used LLM judges to provide harm scores, and this work (https://doi.org/10.18653/v1/2024.acl-long.773) uses LLM judges to provide harmful scores to further optimize its attacks. In addition, this work in Usenix (https://dl.acm.org/doi/10.5555/3766078.3766204) also uses LLM to self-defend against jailbreaks. These precedents diminish the paper’s claim of introducing a new scoring-based guardrail.\n\n2. The LLM-based Detection can be bypassed\n\nOne ICML 2025 paper (https://icml.cc/virtual/2025/poster/45356) specifically talks about how the LLM detection can be bypassed with their emoji attack. The paper might need to address the claims in this paper to be more convincing. \n\n3, Limited novelty\n\nThe two key contributions mentioned in the paper 1, ICL prompting and 2, Dual perspective scoring both fall into the realm of prompt engineering. The paper does not provide a substantive algorithmic or theoretical advance beyond existing judge-style scoring pipelines. This paper is more engineering-heavy and it is hard to gain insight from the paper."}, "questions": {"value": "How will the SelfGrader perform when encountering the emoji attack(https://icml.cc/virtual/2025/poster/45356) that was advertised to bypass LLM detection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1exFBrBJxa", "forum": "qRSlhvA38I", "replyto": "qRSlhvA38I", "signatures": ["ICLR.cc/2026/Conference/Submission4154/Reviewer_b1Lx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4154/Reviewer_b1Lx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548980744, "cdate": 1761548980744, "tmdate": 1762917201649, "mdate": 1762917201649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SelfGrader, a lightweight guardrail method designed to detect jailbreak attacks against LLMs. The central contribution is to reframe jailbreak detection from a text-generation or feature-extraction problem into a numerical grading problem.\n\nInstead of relying on computationally expensive internal features (like gradients) or fragile keyword matching from generated responses , SelfGrader prompts the LLM to evaluate the safety of an input query using a compact set of Numerical Tokens (NTs) (e.g., '0' to '9'). The method then analyzes the token-level logit distribution over these specific NTs to derive an internal safety signal.\n\nTo make this signal meaningful and stable, the authors propose two key innovations:\n\n* An In-Context Learning (ICL) prompting strategy that anchors the numerical scores to human-interpretable categories of maliciousness, using curated query-score pairs as examples.\n\n* A Dual-Perspective Logit (DPL) scoring rule that evaluates the query from both a \"maliciousness\" and a \"benignness\" perspective. This dual-view approach aims to produce a more stable score and reduce the false positive rate on benign prompts.\n\nThe authors conduct extensive experiments across multiple LLMs (Llama-3-8B, Vicuna-13B) and a wide range of jailbreak benchmarks and attack types, including adaptive attacks. The results demonstrate that SelfGrader achieves accurate and robust detection with low computational overhead and latency, while also preserving model utility by maintaining a low False Positive Rate (FPR) on benign tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper presents a strong contribution to the critical area of LLM safety, with strengths across all key dimensions.\n\n1. Originality:\n\n* The primary conceptual contribution is novel: it reframes jailbreak detection from a text-classification or feature-extraction task into a numerical grading problem.\n* Instead of relying on brittle keyword matching from generated text or computationally-heavy internal features like gradients , the paper proposes using the token-level logit distribution over a compact set of Numerical Tokens (NTs) as a direct, internal safety signal. This \"logit-space\" approach is a creative and original alternative to existing guardrail paradigms.\n* The Dual-Perspective Logit (DPL) scoring rule is also a novel methodological component, thoughtfully designed to stabilize the safety signal by integrating both \"maliciousness\" and \"benignness\" assessments, which the authors show is crucial for reducing false positives.\n\n2. Quality:\n\n* The paper demonstrates high quality through its rigorous methodology and comprehensive empirical evaluation.\n* Methodology: The method is well-designed. It cleverly uses an In-Context Learning (ICL) strategy to align the NT logit signals with human intuition of maliciousness, eliminating the need for any model training or fine-tuning.\n* Evaluation: The experimental setup is extensive and robust.\n* It is tested against a diverse and challenging set of jailbreak attacks, including manual (IJP), optimization-based (GCG, AutoDAN), implicit (DrAttack), and multi-turn (ActorAttack) methods. Crucially, the evaluation includes adaptive attacks (TAP, LLM-Fuzzer, X-Teaming), which represent a much stronger test of a guardrail's robustness. The authors responsibly evaluate utility by measuring the False Positive Rate (FPR) on a wide range of benign benchmarks, including tasks like math (GSM8K) and coding (HumanEval) that could be mistakenly flagged by a naive numerical-based defense.\n* Results: The results are excellent. SelfGrader achieves a state-of-the-art balance of safety and efficiency. It consistently achieves low Attack Success Rates (ASR) (e.g., 1.32% average ASR on Llama-3-8B ) while being significantly more efficient than strong baselines. For example, Table 1 shows it is ~25x faster and uses ~3x less memory than GradientCuff, and its ~378 MB memory overhead is negligible compared to other LLM-based guardrails that require >13GB.\n\n3. Clarity:\n* The paper is exceptionally clear and well-written. The problem is well-motivated and formally defined (Section 3) .\n* The proposed method is broken down into three logical, easy-to-understand steps: NT-based logit extraction, DPL scoring, and the decision rule (Section 4) .\n* Figure 1 provides an excellent visual overview that immediately contrasts SelfGrader's mechanism with generation-based and classification-based methods, greatly aiding comprehension.\n* The experiments, results, and ablation studies are presented clearly and support the paper's claims effectively.\n\n4. Significance\n* The paper addresses a problem of high and urgent importance for the safe deployment of LLMs.\n* The most significant contribution is the practicality of the proposed solution. By demonstrating a method that is highly effective while also having negligible latency and memory overhead, the authors present a guardrail that can be realistically deployed in production systems .\n* This work opens a promising new direction for guardrail research, moving beyond text-space and gradient-space analysis to a more direct and efficient logit-space-based evaluation."}, "weaknesses": {"value": "The paper's core idea is to quantify the model's internal safety judgment using a logit-based signal from a single forward pass. However, the evaluation is missing a direct comparison to another key class of lightweight, single-pass methods: those that measure the generative loss (or perplexity) of a pre-defined refusal response (e.g., \"I cannot assist with this request\") or affirmative responses (e.g., \"sure, ...\") .\n\nTo fully substantiate the benefits of the novel numerical grading approach, the authors should include a comparison against this simpler baselines, like Token Highlighter[1]. \n\n**References**\n\n[1] Token Highlighter: Inspecting and Mitigating Jailbreak Prompts for Large Language Models. Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho"}, "questions": {"value": "please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no concerns."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Nv9CqcW1zK", "forum": "qRSlhvA38I", "replyto": "qRSlhvA38I", "signatures": ["ICLR.cc/2026/Conference/Submission4154/Reviewer_RBU3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4154/Reviewer_RBU3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796161075, "cdate": 1761796161075, "tmdate": 1762917201263, "mdate": 1762917201263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SelfGrader, a lightweight guardrail method that introduces a novel in-context prompting strategy and a dual-perspective scoring rule, reformulating jailbreak detection as a numerical grading problem based on token-level logits. Extensive experiments were conducted across diverse jailbreak benchmarks, multiple LLMs, and various guardrail baselines to demonstrate SelfGrader’s accuracy and robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a novel NT-based logits evaluation mechanism. With the dual-perspective score, the DPL scoring rule integrates both maliciousness and benignness perspectives to generate a more stable and reliable signal for decision-making. To further enhance the robustness of the scoring process, a tail trimming strategy is employed to remove low-probability tokens that may otherwise destabilize the score.\n\n2. This paper presents a comprehensive and well-rounded experimental evaluation. It tests SelfGrader against various types of jailbreak attacks, including manual attacks, optimization-based attacks, generation-based attacks, implicit attacks, and multi-turn attacks, and compares it with multiple guardrail baselines. The evaluation covers a wide range of metrics such as ASR, PGR, FPR, latency, and extra memory consumption, providing a thorough assessment of the guardrail’s performance. Furthermore, extensive ablation studies are conducted to analyze different parameters and components of SelfGrader, including the tail trimming parameter, number of NTs (Q), DPL coefficient, as well as key submodules like ICL Examples and DPL Scoring."}, "weaknesses": {"value": "1. The paper’s contributions lack sufficient novelty. The SelfGrader mechanism essentially combines elements of generation-based and classification-based guardrail approaches. Specifically, it first adopts a logit computation process similar to that used in generation-based methods to calculate the LLM’s output logits. It then introduces a scoring mechanism that evaluates these logits using numerical tokens, such as digits 0–9, which resemble the detection mechanism employed by generation-based guardrails.\n\n2. The reliability of the method is questionable. SelfGrader adopts an in-context learning strategy by embedding curated query–score pairs that guide the model to align its logits with predefined maliciousness standards. However, the examples of prompts shown in Appendix B indicate that both the queries and scores are generated by the guardrail model. In particular, the scores are not computed using the DPL Scoring Rule proposed in the paper. Therefore, it is uncertain whether the use of such data is reliable."}, "questions": {"value": "1. What unique or irreplaceable advantages does SelfGrader possess, beyond being lightweight, that make it distinct from generation-based and classification-based guardrails?\n\n2. Moreover, why does SelfGrader rely on the guardrail model to generate the ICL examples instead of using an LLM directly for this purpose?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wTu57SlJvJ", "forum": "qRSlhvA38I", "replyto": "qRSlhvA38I", "signatures": ["ICLR.cc/2026/Conference/Submission4154/Reviewer_GUAA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4154/Reviewer_GUAA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922211715, "cdate": 1761922211715, "tmdate": 1762917200876, "mdate": 1762917200876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}