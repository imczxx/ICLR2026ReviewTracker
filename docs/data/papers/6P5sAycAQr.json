{"id": "6P5sAycAQr", "number": 24481, "cdate": 1758357257652, "mdate": 1759896763707, "content": {"title": "DefNTaxS: The Inevitable Need for Context in Classification", "abstract": "To successfully use generalized vision-language models (VLMs) like CLIP for zero-shot image classification, the semantics of the target classes must be well defined and easily differentiated. However, test datasets rarely meet either criterion, implicitly encoding ambiguity in class labels, even when adding individual descriptors. Existing literature focuses on improving text inputs by using class-specific descriptors to further refine taxonomic granularity, but largely fails to leverage higher-order semantic relationships among classes. We introduce Defined Taxonomic Stratification (DefNTaxS): a fully automated, procedural, training-free framework that leverages large language models (LLMs) to cluster related classes into hierarchical subcategories and augment CLIP prompts with this taxonomic context. By sculpting text prompts to boost both semantic content and inter-class differentiability, DefNTaxS disambiguates semantically similar classes and improves classification accuracy. Across seven standard benchmarks, including ImageNet, CUB, and Food101, DefNTaxS achieves up to +12.9\\% absolute accuracy gain (average +5.5\\%) over vanilla ViT-B/32 CLIP and consistent improvement over other recent SOTA, all while enhancing semantic interpretability without any model retraining/modification, manual prompt alteration, or additional optimization data.", "tldr": "We introduce DefNTaxS, scalably leveraging LLM-generated class taxonomies to augment CLIP text inputs, yielding up to 12.9% (5.5 % avg) accuracy gains across seven benchmarks. DefNTaxS is entirely zero-shot and requires no direct intervention.", "keywords": ["zero-shot", "CLIP", "classification", "waffleclip", "chils", "cupl", "scale", "accessible", "low compute", "training-free", "automated", "semantics"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80303ceba20ba8aaa448de0f78179d52c9a2fffc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper organically integrates two lines of zero-shot CLIP enhancement methods—text prompt augmentation and hierarchy-based approaches—into a unified workflow to improve CLIP’s zero-shot classification performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method demonstrates a clear and rigorous workflow, supported by significant and well-validated experimental results."}, "weaknesses": {"value": "The performance of the proposed method may potentially depend on the class structure of the classification task. \n\nAlthough the paper emphasizes that taxonomic context is essential for classification, there is limited analysis supporting this claim beyond the reported accuracy of the proposed method. In Table 4, removing the taxonomic context (W-TaxS) outperforms removing the descriptors (TaxCLIP) in several cases, which does not appear to substantiate the assertion that taxonomic context is essential."}, "questions": {"value": "Could you please include additional experimental results comparing the proposed method with existing baselines on fine-grained benchmarks, such as Oxford Flowers 102 and Stanford Cars?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rMFLOCBxjl", "forum": "6P5sAycAQr", "replyto": "6P5sAycAQr", "signatures": ["ICLR.cc/2026/Conference/Submission24481/Reviewer_SrrG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24481/Reviewer_SrrG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902444072, "cdate": 1761902444072, "tmdate": 1762943096168, "mdate": 1762943096168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a simple approach for prompt adaptation to improve zero-shot classification of Vision Language models such as CLIP. The authors propose constructing prompts with supercategories appended to prompts for each class. Evaluation is conducted over multiple benchmarks. Ablations and improvements over baselines is shown."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors show improvement over baselines for most datasets. Extensive evaluation includes performance of the method over various CLIP architecures as well as ablations over different aspects such as prompt construction choices."}, "weaknesses": {"value": "Novelty is very limited. There is very little difference conceptually wrt CHiLS and CGPT-P (cited in the paper). Why have the authors decided to only include one level in their hierarchical tree? This has not been discussed. What happens if you construct a deeper tree? This also brings into concern the fact that simply appending the super category name to the class prompt should be suboptimal as CLIP often suffers with long context without specialized fine-tuning. The performance reported does not match numbers in other papers like CHiLS for some datasets. What could cause this? Table 1 does not state the numbers are for which CLIP architecture. The lack of examples of what the texts look like also leads to confusion about the reason why this approach will work better than CHiLS and CGPT-P."}, "questions": {"value": "Please refer to questions in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7YvzILYJeX", "forum": "6P5sAycAQr", "replyto": "6P5sAycAQr", "signatures": ["ICLR.cc/2026/Conference/Submission24481/Reviewer_HnWT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24481/Reviewer_HnWT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949340197, "cdate": 1761949340197, "tmdate": 1762943095714, "mdate": 1762943095714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DefNTaxS, which addresses taxonomic ambiguity in zero-shot classification by automatically generating LLM-based taxonomic context for class prompts. Building on D-CLIP's approach of adding descriptors to class names, DefNTaxS extends the template to include subcategory information: while D-CLIP uses \"[class] which [has/is] [descriptor]\", DefNTaxS adds taxonomic context as \"[class] which [has/is] [descriptor], [contextual phrase] [subcategory]\". The method uses GPT-X to discover subcategories, assign classes (targeting ~20 classes per group), and generate natural contextual phrases, achieving +5.5% average improvement over CLIP and +2.44% over D-CLIP across seven benchmarks without requiring model retraining."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The work explores an interesting aspect of vision-text alignment by investigating how taxonomic context can help resolve semantic ambiguities in CLIP-style models, addressing a real limitation where classes like \"boxer\" could refer to dogs or athletes depending on dataset context."}, "weaknesses": {"value": "-- marginal extension over existing work: The contribution is an incremental improvement over D-CLIP and related LLM-prompting methods (WaffleCLIP, CuPL, CHiLS), essentially adding one more field to existing prompt templates. The taxonomic discovery process reduces to basic LLM prompting with heuristic post-processing rules, offering limited technical novelty beyond existing descriptor-based approaches.\n\n-- small performance gains: The improvements over D-CLIP are modest (+2.44% average, table 1) and inconsistent across datasets, with some showing minimal gains (+0.16% on Places365) or even degradation (-1.05% on Food101). \n\n-- limited technical and scientific contribution: the paper lacks architectural insights about vision-language models and proposes no learnable components or model modifications. The study is mainly about prompt manipulation, which doesn't advance our understanding of why CLIP struggles with ambiguity or how to fundamentally improve VLM architectures."}, "questions": {"value": "--"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2Z6opWMX3O", "forum": "6P5sAycAQr", "replyto": "6P5sAycAQr", "signatures": ["ICLR.cc/2026/Conference/Submission24481/Reviewer_ndRT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24481/Reviewer_ndRT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762079820321, "cdate": 1762079820321, "tmdate": 1762943094810, "mdate": 1762943094810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends existing works on image-text matching within VLMs such as CLIP by augmenting class descriptors with taxonomical context, achieving a gain of on average 5.5% over CLIP on standard benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and simple to follow, and on standard benchmarks compared to methods such as D-CLIP or WaffleCLIP, consistent performance improvements can be found."}, "weaknesses": {"value": "I have several issues with the proposed setup, and would love for the authors to provide more context here:\n* Why would DEFNTAXS actually be a meaningful improvement over D-CLIP, which itself has significant issues, as descriptors proposed by LLMs are not necessary to be found in given query images, see Roth et al. 2023? The problems are the same, it's just that more context is provided that may or may not match the query image, no? I.e., adding \"commonly found in kitchen utils\" would maybe help to differentate a fork against, say, a motorcycle, but the fail when contrasting between potentially a single fork, and a general image on kitchen utensils, since redundancy is introduced.\n* Importantly, a lot of additional semantic information is introduced into a language encoder that is known to be very weak and often fails to distinguish finegrained or multiples of semantic features. It would be important for the authors to offer some support that the CLIP model actually meaningfully and explicitly leverages the semantic context; and its not just better chosen structured noise which is given the additional performance gains."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KCIXgpQNUu", "forum": "6P5sAycAQr", "replyto": "6P5sAycAQr", "signatures": ["ICLR.cc/2026/Conference/Submission24481/Reviewer_dXFM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24481/Reviewer_dXFM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762696554154, "cdate": 1762696554154, "tmdate": 1762943094191, "mdate": 1762943094191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}