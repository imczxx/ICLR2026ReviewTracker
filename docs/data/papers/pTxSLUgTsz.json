{"id": "pTxSLUgTsz", "number": 9795, "cdate": 1758140930452, "mdate": 1763665166223, "content": {"title": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis", "abstract": "Cooperative multi-agent reinforcement learning (MARL) struggles with sample efficiency, interpretability, and generalization. While Large Language Models (LLMs) offer powerful planning capabilities, their application has been hampered by a reliance on text-only inputs and a failure to handle the non-Markovian, partially observable nature of multi-agent tasks. We introduce COMPASS, a multi-agent framework that overcomes these limitations by integrating Vision-Language Models (VLMs) for decentralized, closed-loop decision-making. COMPASS dynamically generates and refines interpretable, code-based strategies stored in a skill library that is bootstrapped from expert demonstrations. To ensure robust coordination, it propagates entity information through a structured multi-hop communication protocol, allowing teams to build a coherent understanding from partial observations. Evaluated on the challenging SMACv2 benchmark, COMPASS significantly outperforms state-of-the-art MARL baselines. Notably, in the symmetric Protoss 5v5 task, COMPASS achieved a 57\\% win rate, a 30 percentage point advantage over QMIX (27\\%). Project page can be found at https://stellar-entremet-1720bb.netlify.app/.", "tldr": "We introduce COMPASS, a multi-agent framework where Vision-Language Models generate adaptive, code-based skills, achieving state-of-the-art performance in the StarCraft Multi-Agent Challenge (SMACv2).", "keywords": ["multi-agent system", "vision language model", "reinforcement learning", "partial observation", "decentralized decision making", "planning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e427c0dbf4f8f7174aa0bede0d0d768a2dfe85db.pdf", "supplementary_material": "/attachment/a00858ec95cf9eb66d5682ab3476a212276c7710.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes COMPASS, a cooperative multi agent planning framework that uses a vision language model to synthesize, refine, and reuse high level skills for decentralized multi agent control under partial observability, with StarCraft Multi Agent Challenge v2 (SMACv2) as the main testbed. Instead of relying purely on end to end multi agent RL, COMPASS equips each agent with an adaptive skill library, represented as code like procedural strategies that a vision language model can generate and update online. Agents also share information through a structured multi hop communication protocol that lets them build a more global situational picture despite only having local views. The claim is that this gives you coordinated team behavior that is interpretable, sample efficient, and robust, because agents reason and act using skills rather than raw actions, and those skills can be adapted during play. On SMACv2, which is harder than classic SMAC because of symmetry, partial information, and less scripted opponents, COMPASS outperforms traditional multi agent RL baselines like QMIX, for example achieving around 57 percent win rate in Protoss 5v5 versus 27 percent for QMIX, a 30 point gap. The authors argue this shows that LLM style skill synthesis plus decentralized multi hop communication can deliver better coordinated control in complex settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work is aiming directly at a problem that is getting a lot of attention in both cooperative RL and LLM agents, getting groups of agents to coordinate under partial observability without relying on brittle hand written tactics. COMPASS hits several pain points at once. First, interpretability, because behaviors are represented as reusable skills in a library rather than implicitly buried in a black box policy, and those skills are expressed in a code like, human readable way, so you can actually inspect, audit, and refine them. Second, coordinated execution, because the multi hop communication protocol is designed to stitch together distributed local observations into a consistent team view, which is essential in SMACv2 style micromanagement where you have to decide who focuses fire, who kites, who retreats, and when. Third, adaptivity, because the skill library is not frozen, the vision language model can refine skills online, so the system is not locked into whatever behavior it saw in the demonstrations. That is a big step beyond the current wave of “LLM plans once, then we blindly execute that static plan,” here the pitch is closed loop decentralized control. Empirically, the gains reported over QMIX are not small, a 30 percentage point difference in win rate on symmetric matchups like Protoss 5v5 is meaningful by SMAC standards. The paper also frames these results in SMACv2 rather than the older SMAC, which is widely considered a tougher and more coordination heavy benchmark. Finally, there is a strong story about practicality. The approach promises both clearer reasoning traces, because skills are explicit, and more reuse, because skills persist and get updated instead of being relearned from scratch every time, which is appealing for anyone who wants continual improvement without retraining a giant RL model end to end."}, "weaknesses": {"value": "There are several places where I still need clarity to judge how strong this is for ICLR, and these are all places where a good rebuttal could significantly strengthen the paper. First, data and supervision cost. COMPASS builds an initial skill library from demonstrations, then uses a vision language model to refine skills in a code like form. I need to know how expensive those demonstrations are, who generated them, a scripted expert, a human, a previous policy, and how dependent final performance is on that bootstrapping. If the strong coordination behaviors mostly come from high quality scripted maneuvers distilled into the library, then part of the performance gain could just be inherited supervision, not emergent decentralized reasoning. Second, inference cost and latency. The paper says COMPASS supports decentralized, closed loop decision making, but vision language models are not cheap to run every timestep for every agent in a fast paced environment like SMACv2. Is the large model actually queried during combat at high frequency, or is it only consulted occasionally to synthesize or adjust skills, after which lightweight controllers run those skills at step time. That difference matters, because calling a VLM every frame for five units in parallel is not realistic for deployment. Third, communication assumptions. The structured multi hop communication protocol is presented as the way agents fuse partial local views into something globally coherent. What bandwidth, latency, and reliability assumptions are built in. Does the system assume essentially lossless multi hop broadcast of local state summaries, and does it assume synchronous updates. If so, that is optimistic compared to real decentralized teams with radio bandwidth limits, contention, or jamming. I would like to see some robustness tests where communication is noisy or delayed, to understand if coordination collapses in that setting or degrades gracefully. Fourth, baselines and fairness. The abstract highlights big gains over QMIX, and possibly MAPPO, but QMIX and MAPPO are not state of the art MARL baselines in 2025, especially not for SMACv2. Recent transformer based CTDE methods, learned communication methods with bandwidth limits, hierarchical skill discovery approaches, and other coordination aware methods can be significantly stronger than plain QMIX or MAPPO on modern multi agent micromanagement benchmarks. To make a convincing empirical case at ICLR, COMPASS needs to compare against such stronger baselines, or at least discuss why that comparison is not yet included. In addition, cost fairness matters. It is easy to beat a baseline if you allow yourself more total tokens, more planning calls, or more structured communication. The paper should report sample efficiency, number of environment steps of training for each baseline, number of model queries, and token or wall clock cost at inference, and then show that COMPASS is not just buying accuracy with more compute or supervision. Finally, generality. All results described so far are in StarCraft style micromanagement, which is a good and challenging domain, but still fairly specific, symmetric combat with continuous skirmishing. The method is pitched as a general cooperative multi agent planning framework. I would like to see at least one additional domain that is structurally different, such as collaborative navigation or search and rescue under partial observability, to show that the recipe, reusable skill library plus in the loop VLM plus multi hop communication, is not overfit to RTS micro tactics."}, "questions": {"value": "How often do you query the vision language model at test time. Is it every environment timestep, every few timesteps, only when some trigger fires, for example unexpected enemy movement or a failed local objective. What is that trigger, and can you quantify the runtime overhead in tokens per episode and milliseconds per environment step. How large does the skill library get in practice for a given matchup. Do you end up with a compact set of reusable maneuvers, focus fire, kiting, retreat regroup, or do you accumulate many highly specific scripts that only make sense in narrow situations. If the library tends to bloat, how do you prune or merge skills to avoid overfitting and slow lookup. On the communication channel, what exactly is passed between agents, is it a fixed size structured summary of visible entities, or free form language, and do you simulate bandwidth limits, message delay, or drop. If you inject delay or drop into that channel, how quickly does coordination quality fall off. For fairness, can you report training steps, wall clock, and inference cost for COMPASS versus each baseline, and provide an accuracy or win rate versus cost Pareto style view. This is especially important because COMPASS uses both a learned policy and an external VLM planner, while baselines like QMIX and MAPPO do not. Related, please clarify which baselines you consider state of the art. QMIX and MAPPO are standard reference points, but they are not state of the art MARL methods for SMACv2 style tasks, so can you either include newer CTDE or communication aware baselines, or justify why they are out of scope. Finally, to support the claim of generality, can you provide at least one non StarCraft style scenario that has different structure, such as cooperative navigation with partial sensing, where there is no obvious “focus fire” tactic, and show that COMPASS still learns useful reusable skills and outperforms strong MARL baselines there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mSctFBxIOX", "forum": "pTxSLUgTsz", "replyto": "pTxSLUgTsz", "signatures": ["ICLR.cc/2026/Conference/Submission9795/Reviewer_5mUy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9795/Reviewer_5mUy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761597279689, "cdate": 1761597279689, "tmdate": 1762921281754, "mdate": 1762921281754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces COMPASS, a multi-agent framework that integrates Vision-Language Models (VLMs) into a decentralized, closed-loop decision-making process. The system is composed of three main parts: (1) a VLM-based planner that operates in a perceive-reason-reflect-act loop; (2) an adaptive skill synthesis mechanism that generates and refines Python-based skills stored in a library ; and (3) a structured multi-hop communication protocol to share information under partial observability. The skill library is notably \"bootstrapped from expert demonstrations\". The authors evaluate COMPASS on the SMACv2 benchmark, claiming it \"significantly outperforms state-of-the-art MARL baselines\" , such as achieving a 57% win rate in the Protoss 5v5 task compared to QMIX's 27%."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- **Significant Problem**: The paper addresses a highly relevant and significant challenge -- leveraging the planning capabilities of large models to improve cooperative MARL in complex, partially observable environments. The goals of improving sample efficiency, interpretability, and generalization are well-motivated.\n\n - **Comprehensive Architecture**: The conceptual design of COMPASS in Figure 1 is comprehensive. It thoughtfully integrates and borrows (from prior works) components for multi-modal perception, cognitive planning, skill management, and agent-to-agent communication, which are all necessary elements for an autonomous multi-agent system.\n\n- **Clarity**: The paper is generally well-written and clearly structured, with diagrams that effectively illustrate the proposed data flow and components."}, "weaknesses": {"value": "The paper's central claims are invalidated by a combination of a fundamentally flawed experimental comparison, unsubstantiated technical assertions, and ablation studies that contradict the authors' own conclusions.\n\n- **Fundamentally Flawed Experimental Comparison**: The primary claim that COMPASS \"significantly outperforms state-of-the-art MARL baselines\"  is based on an invalid comparison. COMPASS's skill library is \"bootstrapped from expert demonstrations\" , which the paper states are \"pre-collect[ed]\" using MAPPO. The baselines (QMIX, MAPPO, HAPPO, HASAC)  are standard online MARL algorithms trained from scratch. The authors are therefore comparing an offline, pre-trained system (COMPASS) against online, from-scratch systems (the baselines). This is not an apples-to-apples comparison and invalidates any conclusion about superior performance or sample efficiency. The performance gap shown in Table 1  is expected when one method has access to an expert dataset and the others do not.\n\n- **Unsubstantiated Claims of \"Skill Synthesis\"**: The paper's core technical contribution is \"adaptive skill synthesis\" , with the claim that the VLM \"generates a new Python script\" when needed. This is an extraordinary claim that requires strong evidence. However, the only example of a skill provided (Listing 2)  is a 400+ line, highly complex, and expertly-engineered Python script, complete with nuanced logic for kiting, pathfinding, and unit-specific counters. It is highly improbable that this script was autonomously generated by a VLM in a closed loop. The paper provides zero evidence (e.g., generation logs, examples of simpler generated skills, or diffs from refinement) to support its claim of de novo skill generation. The paper's contribution appears to be skill selection from a pre-written library, not skill synthesis.\n\n- **Ablation Studies Contradict the Paper's Thesis**: The paper's own ablation studies undermine its central argument. The \"Skill Initialization\" ablation (which uses only the bootstrapped skills without VLM refinement) achieves a 35% win rate in Protoss 5v5. This 35% win rate is already higher than the SOTA baselines QMIX (27%) and MAPPO (32%). \nThis strongly implies that the entire performance gain over baselines is attributable to the pre-collected expert data and the pre-engineered skill library, not the VLM-based planner. The VLM planner's contribution is merely an incremental improvement (from 35% to 57%) on top of an already-expert system.\nFurthermore, the communication ablation (removing communication drops the win rate from 57% to 6% ) is a strawman argument. It only proves that no communication is bad, not that the proposed multi-hop protocol is superior to any simpler communication baseline (e.g., a 1-hop broadcast).\n\n- **Astronomical and Ignored Computational Cost**: The paper casually mentions, \"Token usage is approximately 0.4 million per episode\". This is a computationally infeasible cost for a single, short SMACv2 episode, rendering the method completely impractical for real-world use or even academic research. This critical limitation is not analyzed, discussed, or justified, and it directly contradicts the stated goal of improving \"sample efficiency\"."}, "questions": {"value": "- Can the authors please confirm that the baselines in Table 1 (QMIX, MAPPO, etc.) were trained from scratch (online), while COMPASS was pre-trained using an expert MAPPO dataset (offline)? If so, how can this be justified as a fair comparison for claiming SOTA performance?\n\n- Was the 400+ line Python script in Listing 2 autonomously generated by the VLM? If not, can the authors provide a single, non-trivial example of a skill that was fully generated by the VLM during execution? Without this, how can the claim of \"skill synthesis\" be substantiated?\n\n- The \"Skill Initialization\" ablation (35% win rate) already outperforms SOTA baselines like MAPPO (32%). Doesn't this prove that the performance gain comes from the expert data and pre-written skills, and not the VLM planner itself?\n\n- How can the 0.4 million token cost be justified? Does this astronomical cost not make the method prohibitively expensive and fundamentally inefficient?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mTqyrLFEss", "forum": "pTxSLUgTsz", "replyto": "pTxSLUgTsz", "signatures": ["ICLR.cc/2026/Conference/Submission9795/Reviewer_HGhf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9795/Reviewer_HGhf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761647199661, "cdate": 1761647199661, "tmdate": 1762921281236, "mdate": 1762921281236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces COMPASS, a framework for cooperative multi-agent planning that includes three components: 1) VLM-base closed-loop decision-making planner; 2) code-based skill synthesis that generate and refine executable Python code as skills; 3) a multi-hop communication protocol that propagates observations via a shared global memory accessible to all agents. For experiments, COMPASS achieves convincing performance on the SMACv2 StarCraft benchmark, outperforming 4 state-of-the-art MARL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Overall good experimental performance: The proposed method achieves significant gains on SMACv2 Protoss 5v5 and 5v6 scenario.\n2. This decentralized planning with VLMs, adaptive skill learning are both innovative directions.\n3. The multi-hop communication protocol is effective for the multi-agent planning challenge with partial observability"}, "weaknesses": {"value": "1. The adaptive skill synthesis approach does not constitute a major innovation over prior work. Similar ideas of adaptively using code-generation as tools have been widely explored in earlier systems (e.g. Voyager and the Code-as-Policies).\n2. Limited ablations and analysis: For example, the authors should have more experiments on the communication mechanism (such as communication cost, effect of different structures); Also, although there are the ablation results, there could be more tables (For example, there is only one in-line result on Self-Reflection, while there should be more organized numbers like Table 2)\n3. Weak performance in certain conditions: For example, the proposed method has poor performance in ZERG combat, which requires more fine-grained control.\n4. The three components seems seperated. It could be better if there are some co-designs."}, "questions": {"value": "See weaknesses\n\n1. Is COMPASS different from the existing tool learning methods, or it is an application on different tasks?\n2. Could you provide more tables in the ablations section?\n3. Some visualizations can be better. For example, I can not read the text in the images within Figure 3"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mH4iWQxaxy", "forum": "pTxSLUgTsz", "replyto": "pTxSLUgTsz", "signatures": ["ICLR.cc/2026/Conference/Submission9795/Reviewer_mSCz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9795/Reviewer_mSCz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950274687, "cdate": 1761950274687, "tmdate": 1762921280847, "mdate": 1762921280847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (part 3)"}, "comment": {"value": "### 5. Skill Synthesis\nTo address concerns regarding the process of the generated code, we clarify that COMPASS utilises a **Structured Skill Synthesis** pipeline. We do not rely on open-ended code generation, which is error-prone, but rather on a two-stage logic injection process. Evidence of this process (including `tactics.json` and generation logs) has been added to the anonymous link.\n\n*   **Stage 1: Demonstration-Based Bootstrapping.** The VLM first analyses video demonstrations from the behaviour policy (MAPPO) to extract high-level strategic descriptions (tactics). These textual tactics are then used to fill the logic sections of a predefined Python code template.\n*   **Stage 2: Incremental Synthesis.** During online execution, when a new sub-task is proposed or a skill fails, the VLM analyses the specific deficiency. Crucially, to ensure stability, the VLM is constrained to refine **only one** component per iteration—either the target prioritisation (`score_target`) **OR** the decision rules (`control_logic`)—to better align with the current objective.\n\nWe provide a standardised wrapper that handles observation parsing and includes find_path, which implements standard A* pathfinding. This allows the VLM to focus on high-level logic rather than low-level navigation algorithms.\n\n**Template used for VLM logic injection:**\n```python\ndef race_*_A_star_score_type_*(obs: str):\n    \"\"\"\n    Zealot/Zergling/Baneling/Colossus/Stalker/Hydralisk/Marauder/Marine/Medivac Controls Logic:\n\n    Args:\n        obs (str): Observation string containing game state\n    \"\"\"\n\n    #Observation Parsing\n    obs_data = parse_obs(obs)\n    valid_actions = obs_data.available_actions\n\n    if 0 in valid_actions:\n        return 0\n\n    # This function calculates a score for a given unit (ally or enemy).\n    def score_target(unit):\n        \"\"\"\n        Calculates a score for a target unit to determine its priority.\n        The score should be based on unit type, health, distance, and combat advantage.\n        \"\"\"\n        if unit.health <= 0:\n            return -1\n\n        score = 0\n\n        return score\n\n    # This function contains the main decision-making tree for the agent.\n    def control_logic():\n        \"\"\"\n        Determines the agent's action based on its unit type and the current game state.\n        \"\"\"\n\n        if obs_data.own_unit_type.lower() == 'race_type':\n\n        # Implement the logic for ‘race_type’ units.\n        # Identify the highest-scoring target using `score_target`.\n        # If not in range, use `find_path` to move towards it.\n\n    return control_logic()\n```"}}, "id": "5J8zSsJKbi", "forum": "pTxSLUgTsz", "replyto": "pTxSLUgTsz", "signatures": ["ICLR.cc/2026/Conference/Submission9795/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9795/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9795/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763664964850, "cdate": 1763664964850, "tmdate": 1763664964850, "mdate": 1763664964850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces COMPASS, a decentralized vision language model agent framework for SMACv2 under the Extended Partial Observability setting. Each unit runs a closed loop planner with four modules for perception, task reasoning, self reflection, and action selection. Actions are implemented as executable Python skills stored in a dynamic library that is both initialized from MAPPO demonstration videos and expanded online through skill synthesis and repair. Agents exchange entity level observations via a structured communication protocol with multi hop propagation into a global entity memory. The evaluation spans Protoss, Terran, and Zerg scenarios in both symmetric and asymmetric categories. The authors report improvements over standard MARL baselines in several settings, strongest in Protoss, with mixed outcomes in Zerg. Ablations emphasize that the communication mechanism and the self reflection module contribute materially to performance, and the paper notes substantial token usage per episode."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The four module planner with perception, task reasoning, self reflection, and actor is well specified and easy to ablate and inspect.\n- The paper is well-written."}, "weaknesses": {"value": "- At the level of ideas, the novelty is limited. The method largely integrates established components from the LLM agent literature and applies them to one task in MARL (SMAC). Closed loop plan act reflect with self reflection is standard, code as policy with retrieval and iterative repair is widely used, and entity centered multi hop communication with a global memory is a familiar MARL pattern. The paper’s related work and method framing acknowledge these inspirations, which makes the contribution primarily an integration in the SMACv2 setting rather than a new principle.\n- Baselines do not isolate where the gains come from. The headline comparisons are to non communicating MARL methods, and also lack of other LLM agent comparison. There are no decentralized LLM agent baselines with matched observation interface, communication limits, and token budget. Given the nature of the contribution, these comparisons are necessary to attribute improvements to the specific architecture rather than to the use of an LLM planner with communication.\n- The method uses a large number of tokens per episode, yet the paper does not normalize performance by tokens or wall clock, which weakens claims about sample efficiency."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z4uWPOvhG0", "forum": "pTxSLUgTsz", "replyto": "pTxSLUgTsz", "signatures": ["ICLR.cc/2026/Conference/Submission9795/Reviewer_89kA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9795/Reviewer_89kA"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981030743, "cdate": 1761981030743, "tmdate": 1762921280238, "mdate": 1762921280238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (part 2)"}, "comment": {"value": "### 3. Communication and Frequency Ablations\nWe conducted rigorous ablations on the **Protoss 5v5** scenario to analyze the impact of the multi-hop communication protocol and VLM inference frequency.\n\n**Table 2: Communication Robustness**\n\n| Setting | Specification | Mean Win Rate | Std Dev |\n| :--- | :--- | :--- | :--- |\n| **Hop Count** | 1-hop | 0.46 | 0.19 |\n| | 2-hop | 0.54 | 0.06 |\n| | **3-hop (Default)** | **0.57** | **0.08** |\n| **Packet Loss** | 20% (0.2 fault) | 0.32 | 0.03 |\n| | 50% (0.5 fault) | 0.12 | 0.02 |\n| | 80% (0.8 fault) | 0.07 | 0.05 |\n| | 100% (No Comms) | 0.06 | 0.04 |\n\nThese results demonstrate that **multi-hop propagation** is crucial for expanding the effective receptive field. The system degrades under moderate packet loss and suffers a drastic drop at >50% loss.\n\n**Table 3: VLM Call Frequency (Protoss 5v5, avg episode length ~60 steps)**\n\n| Frequency | Mean Win Rate | Std Dev |\n| :--- | :--- | :--- |\n| Every 10 steps | 0.56 | 0.05 |\n| **Every 20 steps** | **0.57** | **0.08** |\n| Every 40 steps | 0.40 | 0.08 |\n\nAt higher call frequencies, the performance remains similar but incurs greater cost, while lower frequencies fail to keep up with the dynamics of the battlefield.\n\n\n### 4. Token Usage Analysis\nWe acknowledge the computational cost. Below is the breakdown of token usage per step per agent (Call Frequency: 20):\n\n*   **Average Episode Total Tokens:** 80,000\n*   **Average Tokens per Step:** 1,333 (Perception: 2k, Actor: 2k, Self-Reflection: 2k, Task Reasoning: 10k, Skill Synthesis: 10k).\n\nThe bottlenecks are **Task Reasoning** and **Skill Synthesis**, which require historical context and code history. Skill synthesis often requires multiple rounds of self-correction to generate executable code. Future work will focus on reducing this cost by exploiting structural symmetries among agents. We may also reduce token usage by appropriately lowering the frequency of queries."}}, "id": "GpU9udZREV", "forum": "pTxSLUgTsz", "replyto": "pTxSLUgTsz", "signatures": ["ICLR.cc/2026/Conference/Submission9795/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9795/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9795/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763665006211, "cdate": 1763665006211, "tmdate": 1763665006211, "mdate": 1763665006211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (part 1)"}, "comment": {"value": "We sincerely thank the Area Chair and all Reviewers for their insightful and constructive feedback. We are encouraged that reviewers found our approach of integrating VLMs for decentralized decision-making \"interesting\" and the problem setting \"challenging\". We have carefully addressed the concerns regarding baselines, communication robustness, efficiency, and skill synthesis. Below, we outline the major updates and additional experiments included in the revised manuscript and anonymous link https://anonymous.4open.science/r/COMPASS_ICLR26_rebuttal-7448.\n\n### 1. Clarification on Novelty and Contribution\nWhile COMPASS builds upon the foundation established by Cradle, our contribution extends significantly beyond a straightforward integration. Complex strategic environments like SMACv2 introduce unique challenges: **decentralization, partial observability, and high stochasticity**. To address these, we:\n1.  Adapted the planner to operate in a fully decentralized, closed-loop manner.\n2.  Introduced a structured **multi-hop communication protocol** to mitigate partial observability.\n3.  Designed an incremental skill synthesis mechanism that bootstraps from MARL demonstrations.\nEmpirical results validate that these components are not merely additive but essential for performance in this domain. COMPASS is the first framework to jointly and effectively address these challenges using VLMs.\n\n### 2. Expanded Baselines\nTo address concerns regarding the strength of baselines and the comparison between offline/online methods, we have evaluated three additional state-of-the-art algorithms: **MAT** (Advanced Transformer-based MARL) [1], **CommFormer** (Communication-based MARL) [2], and **Oryx** (Offline MARL) [3].\n\n**Table 1: Win Rate Comparison**\n\n| Method | Type | Protoss 5v5 | Protoss 5v6 | Terran 5v5 | Terran 5v6 | Zerg 5v5 | Zerg 5v6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **COMPASS** | **VLMs** | **0.57 ± 0.08** | **0.08 ± 0.04** | **0.39 ± 0.01** | **0.10 ± 0.03** | 0.16 ± 0.07 | 0.03 ± 0.01 |\n| MAT [1] | Online | 0.39 ± 0.03 | 0.04 ± 0.04 | 0.36 ± 0.11 | 0.05 ± 0.01 | **0.32 ± 0.06** | 0.11 ± 0.08 |\n| CommFormer [2] | Online | 0.39 ± 0.16 | 0.02 ± 0.01 | 0.30 ± 0.09 | 0.03 ± 0.01 | **0.39 ± 0.10** | **0.16 ± 0.01** |\n| Oryx [3] | Offline | N/A* | N/A* | 0.18 ± 0.04 | N/A* | 0.10 ± 0.06 | N/A* |\n\n*\\*N/A indicates no datasets in these settings.*\n\nCOMPASS consistently outperforms strong communication and offline baselines in Protoss and Terran scenarios, validating the efficacy of the VLM-based planner. While Zerg performance remains lower due to the requirement for high-frequency swarm micro-management, COMPASS remains competitive in asymmetric settings.\n\n**References:**\n\n[1] Wen et al., \"Multi-agent reinforcement learning is a sequence modeling problem.\" NeurIPS 2022.\n\n[2] Hu et al., \"Learning multi-agent communication from graph modeling perspective.\" ICLR 2024.\n\n[3] Formanek et al., \"Oryx: a Scalable Sequence Model for Many-Agent Coordination in Offline MARL.\" NeurIPS 2025."}}, "id": "mxyexSe8Eh", "forum": "pTxSLUgTsz", "replyto": "pTxSLUgTsz", "signatures": ["ICLR.cc/2026/Conference/Submission9795/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9795/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission9795/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763665063006, "cdate": 1763665063006, "tmdate": 1763665063006, "mdate": 1763665063006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}