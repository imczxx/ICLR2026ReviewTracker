{"id": "pTxSLUgTsz", "number": 9795, "cdate": 1758140930452, "mdate": 1759897695181, "content": {"title": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis", "abstract": "Cooperative multi-agent reinforcement learning (MARL) struggles with sample efficiency, interpretability, and generalization. While Large Language Models (LLMs) offer powerful planning capabilities, their application has been hampered by a reliance on text-only inputs and a failure to handle the non-Markovian, partially observable nature of multi-agent tasks. We introduce COMPASS, a multi-agent framework that overcomes these limitations by integrating Vision-Language Models (VLMs) for decentralized, closed-loop decision-making. COMPASS dynamically generates and refines interpretable, code-based strategies stored in a skill library that is bootstrapped from expert demonstrations. To ensure robust coordination, it propagates entity information through a structured multi-hop communication protocol, allowing teams to build a coherent understanding from partial observations. Evaluated on the challenging SMACv2 benchmark, COMPASS significantly outperforms state-of-the-art MARL baselines. Notably, in the symmetric Protoss 5v5 task, COMPASS achieved a 57\\% win rate, a 30 percentage point advantage over QMIX (27\\%). Project page can be found at https://stellar-entremet-1720bb.netlify.app/.", "tldr": "We introduce COMPASS, a multi-agent framework where Vision-Language Models generate adaptive, code-based skills, achieving state-of-the-art performance in the StarCraft Multi-Agent Challenge (SMACv2).", "keywords": ["multi-agent system", "vision language model", "reinforcement learning", "partial observation", "decentralized decision making", "planning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b5c3dcfbf1a9b93f4923bd91f733bc7fce93b7b1.pdf", "supplementary_material": "/attachment/a00858ec95cf9eb66d5682ab3476a212276c7710.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes COMPASS, a cooperative multi agent planning framework that uses a vision language model to synthesize, refine, and reuse high level skills for decentralized multi agent control under partial observability, with StarCraft Multi Agent Challenge v2 (SMACv2) as the main testbed. Instead of relying purely on end to end multi agent RL, COMPASS equips each agent with an adaptive skill library, represented as code like procedural strategies that a vision language model can generate and update online. Agents also share information through a structured multi hop communication protocol that lets them build a more global situational picture despite only having local views. The claim is that this gives you coordinated team behavior that is interpretable, sample efficient, and robust, because agents reason and act using skills rather than raw actions, and those skills can be adapted during play. On SMACv2, which is harder than classic SMAC because of symmetry, partial information, and less scripted opponents, COMPASS outperforms traditional multi agent RL baselines like QMIX, for example achieving around 57 percent win rate in Protoss 5v5 versus 27 percent for QMIX, a 30 point gap. The authors argue this shows that LLM style skill synthesis plus decentralized multi hop communication can deliver better coordinated control in complex settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work is aiming directly at a problem that is getting a lot of attention in both cooperative RL and LLM agents, getting groups of agents to coordinate under partial observability without relying on brittle hand written tactics. COMPASS hits several pain points at once. First, interpretability, because behaviors are represented as reusable skills in a library rather than implicitly buried in a black box policy, and those skills are expressed in a code like, human readable way, so you can actually inspect, audit, and refine them. Second, coordinated execution, because the multi hop communication protocol is designed to stitch together distributed local observations into a consistent team view, which is essential in SMACv2 style micromanagement where you have to decide who focuses fire, who kites, who retreats, and when. Third, adaptivity, because the skill library is not frozen, the vision language model can refine skills online, so the system is not locked into whatever behavior it saw in the demonstrations. That is a big step beyond the current wave of “LLM plans once, then we blindly execute that static plan,” here the pitch is closed loop decentralized control. Empirically, the gains reported over QMIX are not small, a 30 percentage point difference in win rate on symmetric matchups like Protoss 5v5 is meaningful by SMAC standards. The paper also frames these results in SMACv2 rather than the older SMAC, which is widely considered a tougher and more coordination heavy benchmark. Finally, there is a strong story about practicality. The approach promises both clearer reasoning traces, because skills are explicit, and more reuse, because skills persist and get updated instead of being relearned from scratch every time, which is appealing for anyone who wants continual improvement without retraining a giant RL model end to end."}, "weaknesses": {"value": "There are several places where I still need clarity to judge how strong this is for ICLR, and these are all places where a good rebuttal could significantly strengthen the paper. First, data and supervision cost. COMPASS builds an initial skill library from demonstrations, then uses a vision language model to refine skills in a code like form. I need to know how expensive those demonstrations are, who generated them, a scripted expert, a human, a previous policy, and how dependent final performance is on that bootstrapping. If the strong coordination behaviors mostly come from high quality scripted maneuvers distilled into the library, then part of the performance gain could just be inherited supervision, not emergent decentralized reasoning. Second, inference cost and latency. The paper says COMPASS supports decentralized, closed loop decision making, but vision language models are not cheap to run every timestep for every agent in a fast paced environment like SMACv2. Is the large model actually queried during combat at high frequency, or is it only consulted occasionally to synthesize or adjust skills, after which lightweight controllers run those skills at step time. That difference matters, because calling a VLM every frame for five units in parallel is not realistic for deployment. Third, communication assumptions. The structured multi hop communication protocol is presented as the way agents fuse partial local views into something globally coherent. What bandwidth, latency, and reliability assumptions are built in. Does the system assume essentially lossless multi hop broadcast of local state summaries, and does it assume synchronous updates. If so, that is optimistic compared to real decentralized teams with radio bandwidth limits, contention, or jamming. I would like to see some robustness tests where communication is noisy or delayed, to understand if coordination collapses in that setting or degrades gracefully. Fourth, baselines and fairness. The abstract highlights big gains over QMIX, and possibly MAPPO, but QMIX and MAPPO are not state of the art MARL baselines in 2025, especially not for SMACv2. Recent transformer based CTDE methods, learned communication methods with bandwidth limits, hierarchical skill discovery approaches, and other coordination aware methods can be significantly stronger than plain QMIX or MAPPO on modern multi agent micromanagement benchmarks. To make a convincing empirical case at ICLR, COMPASS needs to compare against such stronger baselines, or at least discuss why that comparison is not yet included. In addition, cost fairness matters. It is easy to beat a baseline if you allow yourself more total tokens, more planning calls, or more structured communication. The paper should report sample efficiency, number of environment steps of training for each baseline, number of model queries, and token or wall clock cost at inference, and then show that COMPASS is not just buying accuracy with more compute or supervision. Finally, generality. All results described so far are in StarCraft style micromanagement, which is a good and challenging domain, but still fairly specific, symmetric combat with continuous skirmishing. The method is pitched as a general cooperative multi agent planning framework. I would like to see at least one additional domain that is structurally different, such as collaborative navigation or search and rescue under partial observability, to show that the recipe, reusable skill library plus in the loop VLM plus multi hop communication, is not overfit to RTS micro tactics."}, "questions": {"value": "How often do you query the vision language model at test time. Is it every environment timestep, every few timesteps, only when some trigger fires, for example unexpected enemy movement or a failed local objective. What is that trigger, and can you quantify the runtime overhead in tokens per episode and milliseconds per environment step. How large does the skill library get in practice for a given matchup. Do you end up with a compact set of reusable maneuvers, focus fire, kiting, retreat regroup, or do you accumulate many highly specific scripts that only make sense in narrow situations. If the library tends to bloat, how do you prune or merge skills to avoid overfitting and slow lookup. On the communication channel, what exactly is passed between agents, is it a fixed size structured summary of visible entities, or free form language, and do you simulate bandwidth limits, message delay, or drop. If you inject delay or drop into that channel, how quickly does coordination quality fall off. For fairness, can you report training steps, wall clock, and inference cost for COMPASS versus each baseline, and provide an accuracy or win rate versus cost Pareto style view. This is especially important because COMPASS uses both a learned policy and an external VLM planner, while baselines like QMIX and MAPPO do not. Related, please clarify which baselines you consider state of the art. QMIX and MAPPO are standard reference points, but they are not state of the art MARL methods for SMACv2 style tasks, so can you either include newer CTDE or communication aware baselines, or justify why they are out of scope. Finally, to support the claim of generality, can you provide at least one non StarCraft style scenario that has different structure, such as cooperative navigation with partial sensing, where there is no obvious “focus fire” tactic, and show that COMPASS still learns useful reusable skills and outperforms strong MARL baselines there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mSctFBxIOX", "forum": "pTxSLUgTsz", "replyto": "pTxSLUgTsz", "signatures": ["ICLR.cc/2026/Conference/Submission9795/Reviewer_5mUy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9795/Reviewer_5mUy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761597279689, "cdate": 1761597279689, "tmdate": 1762921281754, "mdate": 1762921281754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces COMPASS, a multi-agent framework that integrates Vision-Language Models (VLMs) into a decentralized, closed-loop decision-making process. The system is composed of three main parts: (1) a VLM-based planner that operates in a perceive-reason-reflect-act loop; (2) an adaptive skill synthesis mechanism that generates and refines Python-based skills stored in a library ; and (3) a structured multi-hop communication protocol to share information under partial observability. The skill library is notably \"bootstrapped from expert demonstrations\". The authors evaluate COMPASS on the SMACv2 benchmark, claiming it \"significantly outperforms state-of-the-art MARL baselines\" , such as achieving a 57% win rate in the Protoss 5v5 task compared to QMIX's 27%."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- **Significant Problem**: The paper addresses a highly relevant and significant challenge -- leveraging the planning capabilities of large models to improve cooperative MARL in complex, partially observable environments. The goals of improving sample efficiency, interpretability, and generalization are well-motivated.\n\n - **Comprehensive Architecture**: The conceptual design of COMPASS in Figure 1 is comprehensive. It thoughtfully integrates and borrows (from prior works) components for multi-modal perception, cognitive planning, skill management, and agent-to-agent communication, which are all necessary elements for an autonomous multi-agent system.\n\n- **Clarity**: The paper is generally well-written and clearly structured, with diagrams that effectively illustrate the proposed data flow and components."}, "weaknesses": {"value": "The paper's central claims are invalidated by a combination of a fundamentally flawed experimental comparison, unsubstantiated technical assertions, and ablation studies that contradict the authors' own conclusions.\n\n- **Fundamentally Flawed Experimental Comparison**: The primary claim that COMPASS \"significantly outperforms state-of-the-art MARL baselines\"  is based on an invalid comparison. COMPASS's skill library is \"bootstrapped from expert demonstrations\" , which the paper states are \"pre-collect[ed]\" using MAPPO. The baselines (QMIX, MAPPO, HAPPO, HASAC)  are standard online MARL algorithms trained from scratch. The authors are therefore comparing an offline, pre-trained system (COMPASS) against online, from-scratch systems (the baselines). This is not an apples-to-apples comparison and invalidates any conclusion about superior performance or sample efficiency. The performance gap shown in Table 1  is expected when one method has access to an expert dataset and the others do not.\n\n- **Unsubstantiated Claims of \"Skill Synthesis\"**: The paper's core technical contribution is \"adaptive skill synthesis\" , with the claim that the VLM \"generates a new Python script\" when needed. This is an extraordinary claim that requires strong evidence. However, the only example of a skill provided (Listing 2)  is a 400+ line, highly complex, and expertly-engineered Python script, complete with nuanced logic for kiting, pathfinding, and unit-specific counters. It is highly improbable that this script was autonomously generated by a VLM in a closed loop. The paper provides zero evidence (e.g., generation logs, examples of simpler generated skills, or diffs from refinement) to support its claim of de novo skill generation. The paper's contribution appears to be skill selection from a pre-written library, not skill synthesis.\n\n- **Ablation Studies Contradict the Paper's Thesis**: The paper's own ablation studies undermine its central argument. The \"Skill Initialization\" ablation (which uses only the bootstrapped skills without VLM refinement) achieves a 35% win rate in Protoss 5v5. This 35% win rate is already higher than the SOTA baselines QMIX (27%) and MAPPO (32%). \nThis strongly implies that the entire performance gain over baselines is attributable to the pre-collected expert data and the pre-engineered skill library, not the VLM-based planner. The VLM planner's contribution is merely an incremental improvement (from 35% to 57%) on top of an already-expert system.\nFurthermore, the communication ablation (removing communication drops the win rate from 57% to 6% ) is a strawman argument. It only proves that no communication is bad, not that the proposed multi-hop protocol is superior to any simpler communication baseline (e.g., a 1-hop broadcast).\n\n- **Astronomical and Ignored Computational Cost**: The paper casually mentions, \"Token usage is approximately 0.4 million per episode\". This is a computationally infeasible cost for a single, short SMACv2 episode, rendering the method completely impractical for real-world use or even academic research. This critical limitation is not analyzed, discussed, or justified, and it directly contradicts the stated goal of improving \"sample efficiency\"."}, "questions": {"value": "- Can the authors please confirm that the baselines in Table 1 (QMIX, MAPPO, etc.) were trained from scratch (online), while COMPASS was pre-trained using an expert MAPPO dataset (offline)? If so, how can this be justified as a fair comparison for claiming SOTA performance?\n\n- Was the 400+ line Python script in Listing 2 autonomously generated by the VLM? If not, can the authors provide a single, non-trivial example of a skill that was fully generated by the VLM during execution? Without this, how can the claim of \"skill synthesis\" be substantiated?\n\n- The \"Skill Initialization\" ablation (35% win rate) already outperforms SOTA baselines like MAPPO (32%). Doesn't this prove that the performance gain comes from the expert data and pre-written skills, and not the VLM planner itself?\n\n- How can the 0.4 million token cost be justified? Does this astronomical cost not make the method prohibitively expensive and fundamentally inefficient?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mTqyrLFEss", "forum": "pTxSLUgTsz", "replyto": "pTxSLUgTsz", "signatures": ["ICLR.cc/2026/Conference/Submission9795/Reviewer_HGhf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9795/Reviewer_HGhf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761647199661, "cdate": 1761647199661, "tmdate": 1762921281236, "mdate": 1762921281236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces COMPASS, a framework for cooperative multi-agent planning that includes three components: 1) VLM-base closed-loop decision-making planner; 2) code-based skill synthesis that generate and refine executable Python code as skills; 3) a multi-hop communication protocol that propagates observations via a shared global memory accessible to all agents. For experiments, COMPASS achieves convincing performance on the SMACv2 StarCraft benchmark, outperforming 4 state-of-the-art MARL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Overall good experimental performance: The proposed method achieves significant gains on SMACv2 Protoss 5v5 and 5v6 scenario.\n2. This decentralized planning with VLMs, adaptive skill learning are both innovative directions.\n3. The multi-hop communication protocol is effective for the multi-agent planning challenge with partial observability"}, "weaknesses": {"value": "1. The adaptive skill synthesis approach does not constitute a major innovation over prior work. Similar ideas of adaptively using code-generation as tools have been widely explored in earlier systems (e.g. Voyager and the Code-as-Policies).\n2. Limited ablations and analysis: For example, the authors should have more experiments on the communication mechanism (such as communication cost, effect of different structures); Also, although there are the ablation results, there could be more tables (For example, there is only one in-line result on Self-Reflection, while there should be more organized numbers like Table 2)\n3. Weak performance in certain conditions: For example, the proposed method has poor performance in ZERG combat, which requires more fine-grained control.\n4. The three components seems seperated. It could be better if there are some co-designs."}, "questions": {"value": "See weaknesses\n\n1. Is COMPASS different from the existing tool learning methods, or it is an application on different tasks?\n2. Could you provide more tables in the ablations section?\n3. Some visualizations can be better. For example, I can not read the text in the images within Figure 3"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mH4iWQxaxy", "forum": "pTxSLUgTsz", "replyto": "pTxSLUgTsz", "signatures": ["ICLR.cc/2026/Conference/Submission9795/Reviewer_mSCz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9795/Reviewer_mSCz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950274687, "cdate": 1761950274687, "tmdate": 1762921280847, "mdate": 1762921280847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces COMPASS, a decentralized vision language model agent framework for SMACv2 under the Extended Partial Observability setting. Each unit runs a closed loop planner with four modules for perception, task reasoning, self reflection, and action selection. Actions are implemented as executable Python skills stored in a dynamic library that is both initialized from MAPPO demonstration videos and expanded online through skill synthesis and repair. Agents exchange entity level observations via a structured communication protocol with multi hop propagation into a global entity memory. The evaluation spans Protoss, Terran, and Zerg scenarios in both symmetric and asymmetric categories. The authors report improvements over standard MARL baselines in several settings, strongest in Protoss, with mixed outcomes in Zerg. Ablations emphasize that the communication mechanism and the self reflection module contribute materially to performance, and the paper notes substantial token usage per episode."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The four module planner with perception, task reasoning, self reflection, and actor is well specified and easy to ablate and inspect.\n- The paper is well-written."}, "weaknesses": {"value": "- At the level of ideas, the novelty is limited. The method largely integrates established components from the LLM agent literature and applies them to one task in MARL (SMAC). Closed loop plan act reflect with self reflection is standard, code as policy with retrieval and iterative repair is widely used, and entity centered multi hop communication with a global memory is a familiar MARL pattern. The paper’s related work and method framing acknowledge these inspirations, which makes the contribution primarily an integration in the SMACv2 setting rather than a new principle.\n- Baselines do not isolate where the gains come from. The headline comparisons are to non communicating MARL methods, and also lack of other LLM agent comparison. There are no decentralized LLM agent baselines with matched observation interface, communication limits, and token budget. Given the nature of the contribution, these comparisons are necessary to attribute improvements to the specific architecture rather than to the use of an LLM planner with communication.\n- The method uses a large number of tokens per episode, yet the paper does not normalize performance by tokens or wall clock, which weakens claims about sample efficiency."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z4uWPOvhG0", "forum": "pTxSLUgTsz", "replyto": "pTxSLUgTsz", "signatures": ["ICLR.cc/2026/Conference/Submission9795/Reviewer_89kA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9795/Reviewer_89kA"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981030743, "cdate": 1761981030743, "tmdate": 1762921280238, "mdate": 1762921280238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}