{"id": "UAarpRAiVN", "number": 4282, "cdate": 1757654333169, "mdate": 1763571251979, "content": {"title": "Block Rotation is All You Need for MXFP4 Quantization", "abstract": "Large language models (LLMs) have achieved remarkable success, but their rapidly growing scale imposes prohibitive costs in memory, computation, and energy. Post-training quantization (PTQ) is a promising solution for efficient deployment, yet achieving accurate W4A4 quantization remains an open challenge. While most existing methods are designed for INT4 formats, the emergence of MXFP4—a new FP4 format with various hardware support (NVIDIA, AMD, Intel)—raises questions about the applicability of current techniques. In this work, we establish a comprehensive benchmark of PTQ methods under the MXFP4 format. Through systematic evaluation, we find that methods like GPTQ consistently deliver strong performance, whereas rotation-based approaches, which are almost used by all state-of-the-art approaches, suffer from severe incompatibility with MXFP4. We further provide the first in-depth analysis of this conflict, tracing its root to a fundamental mismatch between MXFP4’s PoT (power-of-two) block scaling and the redistribution of outlier energy via global rotation. Building on this insight, we propose a simple yet effective block rotation strategy that adapts rotation-based methods to MXFP4, leading to substantial accuracy improvements across diverse LLMs. Our findings not only offer clear guidance for practitioners but also set a foundation for advancing PTQ research under emerging low-precision formats.", "tldr": "", "keywords": ["LLM", "quantization", "outlier reduction", "rotation", "MXFP4"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b168a0ad1c0f6d2c86334138eca9cfb42324c908.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the effect of rotations applied in the LLM post-training quantizations, focusing on the MXFP4 data format for both weights and activations.\n1. It evaluates the existing quantization methods (RTN, GPTQ, SmoothQuant, QuaRot, OmniQuant, SpinQuant) on MXFP4 in the same set of benchmarks, and compares the accuracy gains between INT4 and MXFP4 when applying rotations for RTN and GPTQ.\n2. It points out that the rotation's accuracy gain on MXFP4 is not as good as that on INT4. Through experimental analysis, the observed accuracy differences are attributed to the shape of MXFP4's error curve and the rotation-induced increase in inlier magnitudes.\n3. It proposes a blockwise rotation strategy for MXFP4 quantization, which outperforms the full rotation in accuracy and speed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "In terms of originality, this paper provides one of the first analyses of the newly emerged MXFP4 data format and shows comprehensive benchmark results across different methods, models, and tasks.\n\nIn terms of quality, the claims and results in the paper look correct and make sense.\n\nIn terms of clarity, this paper is well-written and easy to understand. This paper has nice visualizations.\n\nIn terms of significance, this paper points out a significant problem of why rotations do not work in MXFP4 quantization as well as in the INT4 quantization, and provides an easy and simple method (block rotation) to mitigate this problem."}, "weaknesses": {"value": "1. The title \"Block Rotation is All You Need for MXFP4 Quantization\" is an overclaim. From the accuracies in Table 3, there is a clear gap between the block rotation and the full-precision FP16 baseline. The authors do not prove their block rotation method is optimal over any potential methods for MXFP4 quantization.\n\n2. The claims in Section 4 (Why Rotation Transforms Hurt MXFP4) are purely supported by the analysis on empirical data. It would have been better to also have theoretical guarantees.\n\n3. The proposed method, BRQ, appears in Section 5 (Experiments) without a proper description. I can only know it from the caption of Table 3 that BRQ stands for block rotation transformation, but this is not sufficient. For example, I do not know if it uses RTN, GPTQ, or another quantization method as a backend after applying block rotations. It is also unclear whether the blockwise rotation matrices are shared across blocks or layers, and whether they are general orthogonal matrices or random Hadamard matrices.\n\n4. The performance analysis in Section 5.3 only compares the prefill latency. The decoding latency, which generally dominates the runtime of sequence generation, is missing despite being claimed in Line 466.\n\n5. In Appendix A.2, the calibration dataset used for SpinQuant and BRQ_spin is 800 sequences of length 2048, i.e., 1,638,400 tokens, whereas those of other methods are 262,144 tokens. This creates an unfairness in the benchmarks.\n\n6. The full-precision LLaMA-3 series models use float16 (FP16) while the LLaMA-3 series models and Mistral 7B use bfloat16 (BF16). The authors fail to distinguish the two data types in the paper."}, "questions": {"value": "1. I do not get the point D (Line 236) in Section 3.2. The statement says that, after rotation, BINT4 outperforms BFP4, and BFP4 outperforms MXFP4. How is it related to the divergent behaviors under FP16 vs PoT scaling? Shouldn’t the last sentence be MXINT4 underperforms compared to its FP4 counterpart (MXFP4)?\n\n2. Section 4.1 classifies the blocks into two types: regular blocks and outlier blocks. However, it is not clear what is considered an outlier. There is a definition in the caption of Figure 6 in Section 4.2. Does this definition also apply to Section 4.1?\n\n3. On Section 4.3, the last bullet point (Line 383), the online rotation should be applied to the input activations of all layers. Why are only the computations of the down-project layers reduced? Why is $R_4$ treated specially here (and in the conclusion on line 483) compared to the $R_1$ to $R_B$ in Equation 1?\n\n4. Potential typos:\n- Line 193: 3.35 should be 13.35.\n- Line 319: blue area should be yellow area.\n- Line 428: 7.68 should be 7.62."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ppbrLgetG7", "forum": "UAarpRAiVN", "replyto": "UAarpRAiVN", "signatures": ["ICLR.cc/2026/Conference/Submission4282/Reviewer_JVGA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4282/Reviewer_JVGA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761427674945, "cdate": 1761427674945, "tmdate": 1762917273550, "mdate": 1762917273550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an investigation into the applicability of existing post-training quantization (PTQ) methods for LLMs under the emerging MXFP4 format. The core finding that global rotation-based methods are fundamentally incompatible with MXFP4's block-wise scaling, thus they posit that a simple block-wise rotation (BRQ) is able to mitigate this issue. They provide extensive experiments and analysis. While the topic is timely, the paper suffers from a fundamentally incremental contribution and a lack of technical novelty that is enough to question its value to the quantization community. The core insight is deemed obvious for the target audience, and the solution does not constitute a significant algorithmic advance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The evaluation is comprehensive. It tests multiple models (LLaMA-2/3, Mistral, Qwen), includes both perplexity and downstream task accuracy, and compares against a wide range of strong baselines (GPTQ, OmniQuant, SpinQuant, etc.). The inclusion of a 70B model further strengthens the claims.\n\n2. The explanation of how MXFP4's PoT scaling struggles with large values and how global rotation amplifies small values in regular blocks is clear, intuitive."}, "weaknesses": {"value": "1. The central problem and its solution are a straightforward, expected outcome for anyone with deep expertise in quantization. Applying a block-level transformation to align with a block-level quantization scheme is a natural and almost trivial engineering adjustment, not a novel research contribution. The MXFP4 format, by design, uses local block scaling (PoT) to contain outliers. Applying a global operation that deliberately spreads out outlier energy directly counteracts the format's core design principle. Therefore, observing a performance collapse is not a discovery; it is a confirmation of a predictable hardware/algorithm mismatch.\n\n\n2. The proposed BRQ method is a direct and obvious application of existing concepts. It simply restricts the well-known rotation transform to the block granularity defined by the hardware. This does not represent a new algorithm or a conceptual breakthrough.\n\n3. They selected 'datasets and benchmarks'  as the primary area, but there is no new datasets or benchmarks provided."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kbYNQKd4xb", "forum": "UAarpRAiVN", "replyto": "UAarpRAiVN", "signatures": ["ICLR.cc/2026/Conference/Submission4282/Reviewer_G5Xj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4282/Reviewer_G5Xj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761196467, "cdate": 1761761196467, "tmdate": 1762917273316, "mdate": 1762917273316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a critical mismatch between global rotation‐based quantization methods and modern block-power-of-two (PoT) floating-point formats (e.g., MXFP4) in large language models, noting that traditional rotations can amplify error when applied across hardware blocks. To address this, the authors propose Block-Wise Rotation Quantization (BRQ), applying independent Hadamard rotations within each quantization block so as to contain outlier energy redistribution locally and reduce interference across blocks.\nExtensive experiments on models such as LLaMA-3 8B and Mistral 7B show BRQ recovers much of the performance loss seen by standard rotation methods under MXFP4 quantization, e.g., improving perplexity and accuracy by several percentage points while reducing memory and latency. The key takeaway is that quantization methods must align with the underlying hardware format’s scaling mechanics; in particular, block-sized rotations matched to PoT blocks can restore compatibility and effectiveness on next-generation low-bit formats."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is logically organized, providing a systematic analysis of the incompatibility between MXFP4 and rotation-based quantization methods, making the motivation and contributions easy to follow.\n\n- The authors evaluate across multiple mainstream LLMs (e.g., LLaMA-3 8B, Qwen2.5, Mistral 7B) and compare with various quantization baselines such as GPTQ, QuaRot+, and BINT4, using both perplexity and zero-shot benchmarks.\n\n- The proposed Block-Wise Rotation Quantization (BRQ) specifically addresses the degradation problem of traditional global rotation under MXFP4 format, providing a hardware-aware and theoretically motivated design.\n\n- BRQ significantly improves the performance of rotation-based quantization on MXFP4 (e.g., PPL reduced from 12.78 to 11.95, accuracy increased from 48.83% to 49.87%) while maintaining efficiency and deployment friendliness."}, "weaknesses": {"value": "-  MXFP4 is a block-wise quantization method, and adopting a block-wise rotation transform seems to be an intuitive idea.\n\n- While the proposed BRQ method is empirically effective, the paper lacks deeper theoretical justification or formal analysis explaining why block-wise rotation achieves better quantization stability.\n\n- In integer quantization, group-wise quantization is often required as well. Why does combining it with a rotation transform not harm accuracy.\n\n- The latest NVIDIA GPUs support NVFP4[1] format and use E4M3 instead of E8M0 scaling factors. Will the method described in this paper fail on more future hardware?\n\nReferences:\n\n[1] Pretraining Large Language Models with NVFP4"}, "questions": {"value": "Please refer to the  weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N1atQeV34N", "forum": "UAarpRAiVN", "replyto": "UAarpRAiVN", "signatures": ["ICLR.cc/2026/Conference/Submission4282/Reviewer_i2rU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4282/Reviewer_i2rU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819295330, "cdate": 1761819295330, "tmdate": 1762917273056, "mdate": 1762917273056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses post-training quantization (PTQ) for large language models (LLMs), focusing on the MXFP4 format. It identifies an incompatibility between rotation-based quantization and MXFP4’s power-of-two block scaling, proposing a Block-wise Rotation Quantization (BRQ) strategy to resolve this."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper establishes a comprehensive benchmark comparing state-of-the-art PTQ methods (GPTQ, SmoothQuant, QuaRot, SpinQuant) across multiple LLMs under MXFP4, providing strong empirical evidence and highlighting performance gaps in existing methods.\n\nThis paper conducts a detailed analysis of the destructive interaction between rotation-based methods and MXFP4’s power-of-two scaling.\n\nBuilding on the identified issues, this paper proposes a simple yet effective Block-wise Rotation Quantization (BRQ) strategy, which adapts rotation methods to the MXFP4 format and substantially enhances PTQ accuracy across various models and tasks."}, "weaknesses": {"value": "1. The paper primarily focuses on the theoretical aspects of BRQ and MXFP4 quantization but lacks a detailed evaluation on real-world hardware deployment, such as latency, memory overhead, and computational cost. This leaves a gap in understanding how BRQ performs in practical settings.\n2. The experiments predominantly focus on INT4 PTQ algorithms applied to MXFP4. However, the paper does not explore other quantization formats or different model sizes (e.g., INT8, mixed-precision), limiting the generalization of the findings across various use cases.\n3. While the paper highlights the issue of rotation method incompatibility with PoT scaling, it does not offer a detailed sensitivity analysis to quantify the impact of parameters such as block size, rotation strategies, or outlier distributions. A deeper exploration of these factors would provide a clearer understanding of the robustness and limitations of the proposed approach."}, "questions": {"value": "1. While the grouping size and block‑scale strategy for MXFP4 are described (e.g., block size = 32 channels), it’s not clear how sensitive BRQ’s performance is to the choice of block size. Could the authors provide results or ablations showing how performance varies for block sizes of, for example, 16 vs 32 vs 64 channels?\n\n2. The paper reports gains from BRQ in terms of accuracy/perplexity, but it lacks detailed metrics about inference runtime, memory overhead, and extra compute cost introduced by the block‑wise rotations. Could the authors include profiling (on GPU or accelerator) that quantifies the additional operations (e.g., rotation matrix multiplies) and the net latency/throughput impact of BRQ versus baseline quantization?\n\n3. While you benchmarked selected methods on multiple widely adopted LLMs —including LLaMA‑2 7B/13B, LLaMA‑3 8B, LLaMA‑3.2 1B/3B, and Mistral‑7B—the experiments still remain within a rather narrow family of models (primarily the LLaMA/Mistral lineage). Are there particular considerations (e.g., checkpoint availability, architecture uniformity, calibration data, hardware constraints) that motivated this choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Uawh9OoXRs", "forum": "UAarpRAiVN", "replyto": "UAarpRAiVN", "signatures": ["ICLR.cc/2026/Conference/Submission4282/Reviewer_cxUC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4282/Reviewer_cxUC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992394038, "cdate": 1761992394038, "tmdate": 1762917272642, "mdate": 1762917272642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}