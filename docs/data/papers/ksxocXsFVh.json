{"id": "ksxocXsFVh", "number": 13369, "cdate": 1758217048115, "mdate": 1759897442188, "content": {"title": "Context Tuning for In-Context Optimization", "abstract": "We introduce Context Tuning, a simple and effective method to significantly enhance few-shot adaptation of language models (LLMs) without fine-tuning model parameters. While prompt-based adaptation techniques have demonstrated the effectiveness of lightweight adaptation methods for LLMs, they typically initialize a trainable prompt or prefix with irrelevant tokens for the task at hand. In contrast, Context Tuning initializes the trainable prompt or prefix with task-specific demonstration examples, leveraging the model’s inherent In-Context Learning (ICL) ability to extract relevant information for improved few-shot learning performance. Extensive evaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard, and ARC demonstrate that Context Tuning outperforms traditional prompt-based adaptation methods and achieves competitive accuracy to Test-Time Training with significantly higher training efficiency.", "tldr": "We propose Context Tuning, a simple and effective method to enhance the performance of large language models on a broad range of in-context learning tasks.", "keywords": ["Large Language Model", "In-Context Learning", "Inference-Time Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5daa0669d3674e8a49585cd7ba5e6a8e4fba958.pdf", "supplementary_material": "/attachment/1074965207002d2c8cb5eb48a46917af6dabdbd7.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces **context tuning**, a variant of existing continuous (i.e., parametric) prompt optimization approaches.\nUnlike previous methods that typically initialize parameters randomly, the proposed approach initializes them using demonstration representations.\nFor training, it also incorporates additional techniques such as leave-one-out masking and token dropout, which resemble those used in the previous work called Test-Time Training (TTT).\nWhen evaluated on several benchmarks, including NLP-LR, MMLU, BBH, and ARC, the proposed method achieves performance comparable to existing methods and further improves results when combined with TTT.\n\nWhile the paper introduces a new paradigm termed **in-context optimization**, encompassing both the proposed method (i.e., context tuning) and TTT, it remains unclear whether this terminology provides genuine conceptual novelty or clarity. The idea can arguably be well captured by existing notions such as context engineering, prompt tuning, or test-time compute scaling."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The draft is well-written and easy to understand, with clear notations and explanations.\n- It is well-aligned with related work in the literature, presenting them within an integrated framework.\n- The experiments cover a reasonable range of possibilities, addressing different tasks, models, and configurations."}, "weaknesses": {"value": "- I appreciate the simplicity of the proposed idea, but it seems somewhat too incremental to merit a full-paper submission. This concern becomes particularly evident when compared with the previous work, TTT, where the only difference—at least as I understand it—lies in whether the model tunes LoRA adapters (possibly randomly initialized) or continuous prompts initialized with demonstration embeddings. Although ICLR does not offer a short-paper track, the contribution and scope of this work appear more suitable for a short-paper format in other conferences.\n- There is no clear analysis of why context tuning achieves further gains compared to foundational approaches such as prompt tuning or prefix tuning. For instance, do the improvements stem from initialization strategies, leave-one-out masking, or other factors? While Table 3 presents ablation results, the connection between these variants and the original baseline remains ambiguous. A more detailed investigation of this aspect could reveal potential directions for further improvement.\n- More fundamentally, it is intriguing that the trade-off inherent in these test-time training–like approaches can still be considered worthwhile, especially given their sensitivity to overfitting and their requirement for independent copies of task-specific model parameters—both of which contradict the philosophy of in-context learning and the recent paradigm of LLM usage. It is worth questioning whether, if we were to manually or automatically optimize our natural language prompts with an effort comparable to that required for such fine-tuning, we could achieve similar performance guarantees. I believe that the true merit of LLMs lies in their general capability to handle multiple tasks simultaneously within a single model, without the need for task-specific optimization once required for models such as BERT and its variants. From this perspective, it may be more appropriate to compare the performance of such approaches to that of traditional fine-tuning methods applied to encoder or decoder models, given their conceptual resemblance to fine-tuning in any case."}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yGYAzdNRxn", "forum": "ksxocXsFVh", "replyto": "ksxocXsFVh", "signatures": ["ICLR.cc/2026/Conference/Submission13369/Reviewer_Qgpc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13369/Reviewer_Qgpc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761186141024, "cdate": 1761186141024, "tmdate": 1762924012120, "mdate": 1762924012120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Context Tuning, which initializes and optimizes a task’s few-shot demonstration-derived context, either a soft prompt (CT-Prompt) or per-layer KV prefixes (CT-KV), to boost adaptation without updating model weights. CT-KV conditions all layers through the KV cache and trains with linear cost in the number of demos (vs. quadratic for CT-Prompt and Test-Time Training), aided by leave-one-out masking and token-dropout. Across CrossFit, UnifiedQA, MMLU, BBH, and ARC, CT-KV outperforms ICL and classic prompt/prefix-tuning, rivals TTT, and combining TTT + CT-KV yields the best accuracy–efficiency trade-off. The work unifies these under an In-Context Optimization (ICO) view of updating either the model or its context."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear and well-structured presentation.**\nThe paper is well-written and easy to follow. It clearly explains the intuition behind context optimization, the difference between CT-Prompt and CT-KV, and the rationale for efficiency gains. Figures and ablations effectively illustrate the role of leave-one-out masking and token dropout.\n2. **Consistent and measurable improvement.**\nThe proposed CT-KV achieves clear and repeatable performance gains over standard ICL, prompt/prefix tuning, and even approaches or complements test-time training performance at a fraction of the cost. The results are consistent across multiple datasets and noise settings."}, "weaknesses": {"value": "1. **Limited novelty.** The method conceptually extends test-time training by performing parameter-efficient adaptation with in-context examples, but mainly replaces LoRA with other PEFT methods such as prompt-tuning or prefix-tuning. As such, the contribution feels incremental rather than conceptually new. Moreover, there is prior work on few-shot prompt/prefix tuning since 2022 (e.g, studies exploring better initialization and adaptation strategies [1][2][3]), which are not discussed, weakening the positioning relative to earlier literature.\n\n2. **Unclear model–task pairing.**\nThe choice of models and benchmarks appears somewhat arbitrary. Different tasks are paired with different models without clear justification, making cross-task comparisons difficult. Additionally, some benchmarks (e.g., NLP-LR) are dated and may not fully reflect the challenges of modern large-model evaluation.\n\n3. **Limited generality and applicability.**\nThe method assumes explicit (x, y) demonstration pairs and does not naturally accommodate COT, which limits its applicability to more challenging tasks like math, or coding where intermediate steps matter. Since modern LLM use is increasingly zero-shot or instruction-driven, requiring curated few-shot pairs at test time may reduce its relevance to real-world interactive or open-ended scenarios.\n\n[1] Pan et al, Self-supervised meta-prompt learning with meta-gradient regularization for few-shot generalization, 2023 \\\n[2] Huang et al, Learning a better initialization for soft prompts via meta-learning, 2022 \\\n[3] Qin et al, Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?, 2023"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QtfSxpg91z", "forum": "ksxocXsFVh", "replyto": "ksxocXsFVh", "signatures": ["ICLR.cc/2026/Conference/Submission13369/Reviewer_HFNj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13369/Reviewer_HFNj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974698205, "cdate": 1761974698205, "tmdate": 1762924011563, "mdate": 1762924011563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Context Tuning, a method that improves few shot adaptation (with additional parameters) of LLMs without updating model weights. The core idea is to initializing existing prompt tuning and prefix tuning method with embeddings obtained directly from task specific demonstration examples. As a result, the paper introduces two variants: CT Prompt tunes a soft prompt initialized from the demonstrations. CT KV initializes and tunes layerwise key value prefixes extracted from the model activations on the same demonstrations. The authors further introduce leave one out masking to prevent target leakage during training, and token dropout to reduce overfitting. Across multiple benchmarks, Context Tuning outperforms standard prompt and prefix tuning and is competitive with test time training while being substantially more efficient, especially for CT KV."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Parameter efficiency and stability. The method keeps model weights frozen and only learns a small set of prompt or prefix parameters, which preserves the base model while enabling task adaptation.\n\n- Consistent gains in few shot settings. Experiments show improvements over vanilla in context learning and over standard prompt or prefix tuning on several benchmarks. The gains are robust across a range of demonstration counts and across different base model sizes. But, much of the empirical benefit appears to come from a stronger initialization of existing prompt or prefix tuning, rather than a fundamentally new optimization mechanism. I list this more fully under weaknesses."}, "weaknesses": {"value": "- Limited novelty in mechanism. The main algorithmic move is to initialize the trainable prompt or prefix from demonstration embeddings, then optimize as in standard prompt or prefix tuning. This is a strong and practical idea, but conceptually close to prior methods and may be seen as an improved initialization strategy rather than a new optimization principle.\n\n- Positioning relative to test time training. The paper reports results for TTT plus CT KV and presents this combined setting among the proposed methods. Since TTT is an existing and separate approach that updates model weights, it is not clear that TTT plus CT KV should be considered an integral contribution of this paper. A clearer separation of baselines, proposed methods, and combinations would help, along with ablations that isolate where the gains come from.\n\n- Interpretation of robustness to label noise. The paper highlights that CT KV remains strong even when many demonstration labels are corrupted. However, in an in context optimization view, a method should ideally adjust its behavior when labels change. If performance is largely unchanged under heavy label corruption, this can suggest that the method relies more on input side regularities or priors than on learning from the provided labels. This reduces the strength of the robustness claim as evidence of learning from context.\n\n- Missing specification details. Some core definitions and operations are underspecified. In leave one out masking, the construction of P^{\\text{minus i}}_{\\text{CT}} for prompts and of the corresponding context representation for prefixes is not formally detailed. The paper says the relevant tokens are masked out from the attention view, but it does not specify whether this is implemented by zeroing keys and values, by blocking attention through an attention mask, or by removing those prefix positions from the cache. A precise definition would improve clarity and reproducibility."}, "questions": {"value": "- Variable length to fixed size initialization. Demonstration contexts can vary in length. How do you map variable length demonstrations to a fixed number of trainable prompt tokens or to a fixed per layer prefix budget. For CT Prompt, do you truncate or pool embeddings when the demonstration context exceeds the prompt length. For CT KV, how are per layer prefix lengths chosen and how are multiple demonstrations combined or allocated across that budget."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MUuRQNu0tQ", "forum": "ksxocXsFVh", "replyto": "ksxocXsFVh", "signatures": ["ICLR.cc/2026/Conference/Submission13369/Reviewer_Tm2d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13369/Reviewer_Tm2d"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762220661567, "cdate": 1762220661567, "tmdate": 1762924011238, "mdate": 1762924011238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}