{"id": "h5gwkhrIO9", "number": 2323, "cdate": 1757058413284, "mdate": 1759898155810, "content": {"title": "VAMO: Efficient Zeroth-Order Variance Reduction for SGD with Faster Convergence", "abstract": "Optimizing large-scale nonconvex problems, common in deep learning, demands balancing rapid convergence with computational efficiency. First-order (FO) optimizers, which serve as today’s baselines, provide fast convergence and good generalization but often incur high computation and memory costs due to the large size of modern models. Conversely, zeroth-order (ZO) algorithms reduce this burden using estimated gradients, yet their slow convergence in high-dimensional settings limits practicality. We introduce VAMO (VAriance-reduced Mixed-gradient Optimizer), a stochastic variance-reduced method that extends mini-batch SGD with full-batch ZO gradients under an SVRG-style framework. VAMO's hybrid design utilizes a two-point ZO estimator to achieve a dimension-agnostic convergence rate of $\\mathcal{O}(1/T + 1/b)$, where $T$ is the number of iterations and $b$ is the batch-size, surpassing the dimension-dependent slowdown of purely ZO methods and significantly improving over SGD's $\\mathcal{O}(1/\\sqrt{T})$ rate. Additionally, we propose a multi-point variant that mitigates the $O(1/b)$ error by adjusting the number of estimation points to balance convergence and cost. Importantly, VAMO achieves these gains with smaller dynamic memory requirements than many FO baselines, making it particularly attractive for edge deployment. Experiments including traditional neural network training and LLM finetuning confirm that VAMO not only outperforms established FO and ZO methods, but also does so with a light memory footprint.", "tldr": "", "keywords": ["non-convex optimization", "neural network optimizer", "convergence rate", "efficient machine learning", "zero-order optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11a03fce94c038a8645b70457863a6a28e5307f5.pdf", "supplementary_material": "/attachment/ad5d6d6d206ec3769186a084edd7039c159e22ec.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes VAMO - a variance reduction optimization method that combines both zero-order gradient estimates and gradient to produce the update direction. Concretely, the algorithm uses the full batch and batch gradient estimates at the checkpoint (obtained from the standard zero-order method) and combines with the batch gradient at the current iterate ($g = \\nabla f(x, B) - \\alpha (\\hat{\\nabla} f (x_{\\text{cpt}}, B) - \\hat{\\nabla}f(x_{\\text{cpt}}))$). This gives a trade off between convergence and computational complexity compared with ZO-SVRG. In terms of convergence, the rate of VAMO is $1/T + 1/b$ where $b$ is the batch size. This has the extra  $1/b$ term is inherited from ZO-SVRG due to the bias in the estimator, but the first term is improved by a factor $d$. The computational complexity on the other hand is increased from $nS+bT$ to $nS+dbT$ (due to the gradient computation). An extension to this method is to use multi-point query, which can trade off between the improvement in the additional error term and the computational complexity. Experiments show that VAMO has faster convergence than other ZO methods and better memory footprint than FO methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Combining FO and ZO in SVRG is an interesting idea and the FO order component improves the performance of the algorithm compared with other ZO methods.\n- The presentation of the paper is easy to follow with detailed comparison with prior works.\n- Proofs seem all good."}, "weaknesses": {"value": "I have several concerns below.\n- I'm not sure I understand the memory analysis in Section 4.3, B.1 and Table 3. \n  - For VAMO, why is the memory for the optimizer states only $|x|$? The algorithm needs to store $\\hat{\\nabla}(x_{\\text{cpt}})$ but also $x_{\\text{cpt}}$ to compute the estimate $\\hat{\\nabla}(x_{\\text{cpt}}, B)$ (see also line 3-4 in Alg. 1). I don't understand how to reduce the memory to only $|x|$? If my understanding is true, I don't see an improvement in the memory for the proposed method.\n  - For Adagrad, why is the optimizer states $2|x|$? Doesn't the algorithm only store the accumulation of the gradients per coordinate so the memory needed is only $|x|$? Same for Adam, shouldn't it be $2|x|$?\n- The experiment appears quite weak. First of all, the paper only reports the training loss. While optimization algorithms only optimize the training loss, we also care about the test loss and accuracy. Second, one main motivation the paper mentioned is to overcome limitations of FO and ZO methods for efficient training of large models. However, the set of experiments is quite limited. MNIST is an outdated dataset as an optimization benchmark. Usually, for optimization papers, starting with CIFAR-10/100 will show clearer impacts. I also suggest that the authors use a similar experiment setup in the paper MeZO and add more experiments."}, "questions": {"value": "- See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gbW0oqJmtP", "forum": "h5gwkhrIO9", "replyto": "h5gwkhrIO9", "signatures": ["ICLR.cc/2026/Conference/Submission2323/Reviewer_zY9f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2323/Reviewer_zY9f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813650288, "cdate": 1761813650288, "tmdate": 1762916193693, "mdate": 1762916193693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VAMO, a new stochastic optimization algorithm for large-scale non-convex optimization.\nThe algorithm aims to bridge the gap between first-order (FO) methods, which have fast\nconvergence but high computational and memory costs, and zeroth-order (ZO) methods, which are\nmemory-light but suffer from dimension-dependent and slow convergence.\nVAMO is a two-loop \"SVRG-style\" algorithm. Its central idea is to replace the computationally\nexpensive full-batch FO gradient (∇f(ˆx)) used in FO-SVRG with a full-batch ZO gradient estimate\n(ˆ∇f(ˆx)). The inner-loop update is a novel hybrid estimator: vsk\n= ∇fIk (xsk) − α(ˆ∇fIk (ˆx) − ˆ∇f(ˆx)).\nThis construction cleverly uses a ZO-based variance correction term that has zero expectation, making\nvsk an unbiased estimator of the true gradient ∇f(xsk).\nThe authors provide a theoretical analysis showing the two-point (q = 1) version of VAMO achieves a\nconvergence rate of O(1/T +1/b), which is dimension-independent, a significant improvement over typical\nZO methods. They also propose a multi-point (q > 1) variant with a rate of O(1/T +(1−q/d)2/b),\nwhich can converge to a stationary point if q = d. The primary claims are that VAMO achieves FOSVRG-\nlike convergence speed with significantly lower computational and, critically, dynamic memory\ncosts. Experiments on neural networks and LLM finetuning are presented to support these claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novel Hybrid Estimator: The proposed gradient estimator (Eq. 6) is novel. The insight to\nuse a ZO-based correction term α(ˆ∇fIk (ˆx) − ˆ∇f(ˆx)), which has zero expectation, is technically\nsound. This ensures the full estimator vsk is an unbiased estimator of the true current gradient\n∇f(xsk), which is an elegant property for the analysis.\n\n2. Dimension-Independent ZO-Hybrid Rate: The theoretical analysis successfully breaks the\ndimension-dependency curse of pure ZO methods. Achieving a rate of O(1/T + 1/b) (Corollary\n1) that is independent of d is a strong theoretical contribution for a method that incorporates\nZO components.\n\n3. Strong Empirical Performance vs. FO-SGD: The experiments, particularly in Figure 2\n(LLM finetuning), show that VAMO (with q = 1) converges significantly faster in terms of both\nsteps and wall-clock time than FO-SGD. This empirically validates the theoretical advantage of\nthe O(1/T + ...) rate over FO-SGD’s O(1/√T) rate for reaching a certain precision."}, "weaknesses": {"value": "Despite its theoretical novelty, the paper’s core claims about its practical advantages are based on a\nseries of critical, and in some cases contradictory, flaws in the analysis of its computational and memory\ncosts.\n\n1. Fundamentally Misleading Convergence Claims: The paper repeatedly claims an O(1/T )\nrate, equating it with FO-SVRG (e.g., Abstract: \"significantly improving over SGD’s... rate\";\nConclusion: \"achieving convergence performance similar to FO-SVRG\"). This is a misrepresentation.\nThe actual derived rate is O(1/T + 1/b). This is not convergence to a stationary point.\nIt is linear convergence to a noise ball of size O(1/b). This non-vanishing error term, which\ndominates at large T, means VAMO (q = 1) cannot achieve high-precision solutions and may, in\nfact, converge to a worse solution than FO-SGD (which does converge to ϵ = 0, albeit slower).\nThis is a crucial distinction that is glossed over.\n\n2. Contradictory and Factually Incorrect Memory Analysis: This is the paper’s most severe\nflaw. The paper is motivated by reducing the high dynamic memory of FO methods (e.g., storing\nactivations for backpropagation). The inner-loop update (Algorithm 1, Step 7) explicitly requires\na mini-batch FO gradient, ∇fIk (xsk). Computing this FO gradient requires backpropagation\nand storing intermediate activations, leading to a dynamic memory cost of O(b ·P|al|). This is\nthe exact same dynamic memory cost as FO-SGD. The paper’s text (e.g., Section 4.3, Lines 325-\n328: \"thus do not need to store intermediate results\", \"dynamic memory only reaches maxl |xl|\")\nis factually incorrect and fundamentally misunderstands the cost of its own algorithm. The\ndata in the paper confirm this. Table 3 correctly lists VAMO’s dynamic memory as\nPl max{b ·|al|, |xl|}, identical to FO-SGD. Table 2 empirically shows VAMO’s memory (e.g., 21.46 GB) is\nalmost identical to FO-SGD’s (20.33 GB). The small increase is expected, as VAMO = FO-SGD\n(dynamic) + |x| (snapshot state). The central claim that VAMO is a low-memory algorithm\n(relative to FO-SGD) is false.\n\n3. No Asymptotic Computational Advantage for O(1/T ) Convergence: To achieve the true\nO(1/T ) convergence (i.e., remove the O(1/b) error), one must use the multi-point variant and set\nq = d (Theorem 2).The computational complexity of VAMO (q = d) is O(qnS + (bd + bq)T) =\nO(dnS + (bd + bd)T) = O(dnS + bdT ). Therefore, to achieve the same convergence rate as FOSVRG,\nVAMO requires the identical asymptotic computational complexity. The paper’s claim\nof computational efficiency is only valid when comparing the non-converging q = 1 version to the\nconverging FO-SVRG, which is an apples-to-oranges comparison.\n\n4. Incorrect Computational Complexity in Table 1: The complexity for VAMO (q = 1) is\nlisted as O(nS +bdT ). This is incorrect. The inner loop (Step 7) computes two gradients: ∇fIk\n(cost O(bd)) and ˆ∇fIk (cost O(bq), or O(b) for q = 1). The correct complexity is O(qnS +(bd+\nbq)T). For q = 1, this is O(nS + (bd + b)T), which is O(nS + bdT ) only if d ≫ 1. This O(bqT)\nterm is missing from the table.\n\n5. Empirical Evidence of Non-Convergence: In Figure 1b, VAMO(q = 1) clearly converges to\na final training loss that is visibly *higher* than that of FO-SGD. This plot empirically confirms\nthe O(1/b) noise ball limitation. This is a poor result, as it fails to match the solution quality of\nthe baseline FO-SGD, yet this is not discussed."}, "questions": {"value": "1. The text in Section 4.3 (Lines 325-328) claims that VAMO does not need to store intermediate\nactivations. However, Step 7 of Algorithm 1 computes an FO gradient ∇fIk (xsk), which necessitates storing activations. Your own Table 3 and Table 2 confirm that VAMO has the same dynamic memory as FO-SGD. Can you please clarify this fundamental contradiction? Is the premise of the paper’s memory-saving benefits not incorrect?\n\n2. The O(1/T + 1/b) rate converges to a noise ball, not a stationary point. Why is this\nmisleadingly presented as \"similar to FO-SVRG\" (which converges to 0) and an improvement\nover FO-SGD (which also converges to 0)? Figure 1b seems to confirm that VAMO(q = 1) converges\nto a worse solution.\n\n3. The inner loop (Step 7) computes both an FO-grad (cost O(bd)) and a ZO-grad (cost O(bq)).\nWhy is the O(bqT) term missing from the complexity analysis in Table 1?\n\n4. Given that VAMO(q = d) has the same rate and computation as FO-SVRG, why was this\nComparison (which is the only fair comparison of two O(1/T ) methods) is not included in the\nexperiments?\n\n5. How sensitive are convergence and stability to the choice of α, μ, and q? Could you provide\nempirical ablations or adaptive scheduling strategies, validating these settings across different\nmodel dimensions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "This paper introduces a technically interesting and novel hybrid gradient estimator. The theoretical\nanalysis (deriving a dimension-independent O(1/T + ...) rate) is a non-trivial contribution.\nHowever, the paper is built on a foundation of fundamentally incorrect and contradictory claims regarding\nits practical advantages. The central premise that VAMO offers dynamic memory savings over\nFO-SGD is demonstrably false, as refuted by the paper’s own algorithm, theoretical memory table,\nand empirical memory measurements. The claim of computational superiority over FO-SVRG is also\nillusory, as VAMO must match FO-SVRG’s asymptotic complexity (q = d) to match its O(1/T ) convergence\nrate. In addition, the q = 1 variant is simply a method that converges faster than FO-SGD\nto a worse solution (a noise ball of size O(1/b)), all while having the same dynamic memory footprint.\nThis is not the breakthrough claimed.\n\nBecause the paper’s central claims of computational and memory efficiency are not supported by (and\nare, in fact, refuted by a correct analysis of the proposed algorithm, the work in its current form does\nnot meet the standards for publication at ICLR. The paper would need to be fundamentally reframed\nto be honest about its actual (and much more modest) contributions: namely, a trade-off between\nconvergence speed and a final non-vanishing error."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vCqH4lOTjF", "forum": "h5gwkhrIO9", "replyto": "h5gwkhrIO9", "signatures": ["ICLR.cc/2026/Conference/Submission2323/Reviewer_mDeF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2323/Reviewer_mDeF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920747399, "cdate": 1761920747399, "tmdate": 1762916193519, "mdate": 1762916193519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The research presents VAMO as a hybrid optimizer that works with big models under memory restrictions. The optimizer uses First-Order (FO) speed together with Zeroth-Order (ZO) efficiency by substituting the expensive SVRG variance-reduction algorithm step with a low-cost ZO gradient estimate. The algorithm provides fast dimension-independent convergence speed that outperforms SGD yet requires memory levels similar to SGD and less than Adam. The empirical results show VAMO runs at a slower pace than Adam but provides an attractive trade-off between performance and memory usage for restricted resource scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. VAMO achieves a fast, linear convergence rate of $\\mathcal{O}(1/T)$, which is an improvement over the $\\mathcal{O}(1/\\sqrt{T})$ rate of standard SGD.\n2. A key advantage is that its convergence rate is independent of the model's parameter dimension $d$. This allows it to overcome the \"curse of dimensionality\" that makes purely Zeroth-Order (ZO) methods impractical for large models.\n3. This paper provides a strong theoretical guarantee. VAMO's gradient estimator is designed to be unbiased."}, "weaknesses": {"value": "1. The experimental results demonstrate that VAMO achieves better performance than SGD, but it fails to match the convergence speed and training efficiency of the Adam optimizer during large-scale fine-tuning.\n2. The theoretical analysis shows that VAMO's convergence rate, while faster than SGD's, includes an additional error term of $\\mathcal{O}(1/b)$ that is not present in the purely First-Order FO-SVRG algorithm.\n3. The paper introduces a multi-point variant to minimize the additional error term. The improvement requires additional computational resources, which increase with the number of query points $q$, thus users need to decide between faster convergence and higher processing expenses."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "20s6AsTElv", "forum": "h5gwkhrIO9", "replyto": "h5gwkhrIO9", "signatures": ["ICLR.cc/2026/Conference/Submission2323/Reviewer_P2tL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2323/Reviewer_P2tL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942139783, "cdate": 1761942139783, "tmdate": 1762916193262, "mdate": 1762916193262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VAMO, a hybrid variance-reduced optimizer that blends a mini-batch first-order (FO) gradient with a zeroth-order (ZO) SVRG-style correction computed at snapshot points. The key idea is to replace the expensive full-batch FO snapshot gradient with a full-batch ZO estimate and to weight the correction with a mixing coefficient \\alpha. The authors prove a  convergence rate of O(1/T+1/b) independent of dimension d. Experiments span a synthetic task, MNIST MLP, and fine-tuning GPT-2 / GPT-2-Medium / RoBERTa-Large on SST-2 and MNLI, with GPU memory comparisons versus FO-SGD/Adagrad/Adam."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. I like the discussion of memory: the paper argues VAMO’s snapshot uses forward-only ZO passes, so peak dynamic memory resembles ZO-SGD rather than FO-SVRG, and optimizer state is lighter than Adam/Adagrad. Included tables and a clear decomposition (weights / states / dynamics) are helpful.\n2. The main bound removes the typical d dependence of ZO methods and improves over FO-SGD’s O(1/\\sqrt{T}) in theory."}, "weaknesses": {"value": "1.ZO methods inherently trade off performance, memory, and wall-clock. The paper treats ZO snapshots as “cheap,” but a full-batch snapshot still requires multiple forward passes per direction over the entire dataset (or many mini-batches). In practice, if memory is the bottleneck then ZO can help. However, runtime can balloon unless the number of directions q is tiny—yet shrinking q raises estimator variance and hurts convergence. Therefore, without an accounting of function evaluations (FEs) and throughput (e.g., tokens/sec), it’s unclear when VAMO is actually preferable to tuned FO baselines (Adam/Lion/Adafactor) or to ZO-SVRG variants with different q.\n\n2. ZO can also be performed when the random direction is Gaussian, i.e., let u(x;\\theta) = E_{\\delta\\sim N(0,\\sigma^2 I) f(x+\\delta;\\theta), then \\nabla_x u(x;\\theta) = E_{\\delta\\sim N(0,\\sigma^2 I) [\\delta/\\sigma^2  f(x+\\delta;\\theta). In high dimension, Gaussian vs. coordinate-wise directions can yield different estimator variance and smoothing bias. Yet this is not compared in the paper.\n\n3. Fine-tuning results present training loss (showing convergence) and memory costs. However, I would expect also the task metrics (accuracy/F1 for SST-2/MNLI) and stability stats (divergence/NaNs) to show the effectiveness of VAMO.\n\n4. SVRG-style methods hinge on the outer-loop frequency S and inner length m. The paper would benefit from an empirical study of how often to recompute ZO snapshots (and with what q) under a fixed compute budget; otherwise, it’s hard to see when VAMO is preferable to carefully tuned FO-SGD/Adam or existing ZO-SVRG variants."}, "questions": {"value": "see weakenesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6wKQ9BBkXY", "forum": "h5gwkhrIO9", "replyto": "h5gwkhrIO9", "signatures": ["ICLR.cc/2026/Conference/Submission2323/Reviewer_mfXT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2323/Reviewer_mfXT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032766757, "cdate": 1762032766757, "tmdate": 1762916192832, "mdate": 1762916192832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}