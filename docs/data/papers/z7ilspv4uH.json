{"id": "z7ilspv4uH", "number": 19234, "cdate": 1758294634652, "mdate": 1763701150402, "content": {"title": "PDE-PFN: Prior-Data Fitted Neural PDE Solver", "abstract": "Despite recent progress in scientific machine learning (SciML), existing approaches remain impractical, as they often require explicit governing equations, impose rigid input structures, and lack generalizability across PDEs. \nMotivated by the success of large language models (LLMs) with broad generalizability and robustness to noisy or unreliable pre-training data, we seek to bring similar capabilities to PDE solvers. In addition, inspired by the Bayesian inference mechanisms of prior-data fitted networks (PFNs), we propose PDE-PFN, a prior-data fitted neural solver that directly approximates the posterior predictive distribution (PPD) of PDE solutions via in-context Bayesian inference. PDE-PFN builds on a PFN architecture with self- and cross-attention mechanisms of Transformer and is pre-trained on low-cost approximate solutions generated by physics-informed neural networks, serving as diverse but not necessarily exact priors. Through experiments on a range of two-dimensional PDEs, we demonstrate that PDE-PFN achieves strong generalization across heterogeneous equations, robustness under noisy priors, and zero-shot inference capability. Our approach not only outperforms task-specific baselines but also establishes a flexible and robust paradigm for advancing SciML.", "tldr": "", "keywords": ["in-context learning", "scientific foundation model", "zero-shot", "prior-data fitted network", "PINN-prior"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc56c90e7a1f04aa799b56603cd4930456d4cb1c.pdf", "supplementary_material": "/attachment/6afcc10635558834bc7bbd1e558079f81863f5c1.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposed to use prior-data fitted networks (PFNs) which uses a Transformer to perform in-context Bayesian inference to approximate the posterior predictive distribution (PPD) of PDE solutions. Prior knowledge is from the predictions of PINNs. The in-context correction is when solving a new PDE, accurate simulated known boundary conditions and initial conditions data will be provided for in-context learning by the Transformer. The main contribution is the robustness to noisy priors and strong generalization to different PDEs and the ability of Zero-shot inference. Comparing to current methods, this method does not require explicit governing PDE which increases the flexibility of data using."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper has high level of novelty in using PFNs and ICL and improved the generalizability of Neural PDE solver to a high level. \nThis paper has a rich baseline models to compare with and outperformed all of the baselines which is a pretty strong empirical result."}, "weaknesses": {"value": "The only question I have is this: PINNs are known to have failure modes when the PDE coefficient is large, causing ill-conditioning and very complex loss landscapes. I notice that the PDEs tested in this paper generally have low coefficients. My concern is that when the prior is based on PINNs solving high-coefficient PDEs, the prior data would be fundamentally wrong (i.e., highly inaccurate). Given this situation, how well can the ICL perform in accurately predicting the solution to new PDEs, especially when the new PDE also has a high coefficient?"}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fPpbEOWs78", "forum": "z7ilspv4uH", "replyto": "z7ilspv4uH", "signatures": ["ICLR.cc/2026/Conference/Submission19234/Reviewer_7z9q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19234/Reviewer_7z9q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856000776, "cdate": 1761856000776, "tmdate": 1762931215386, "mdate": 1762931215386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel architecture called PDE-PFN based on the Prior-data fitted network that, given a set of spatial samples from a trajectory, can predict the rest of the solution. This can include interpolation as well as extrapolation tasks. The method can be seen as in-context Bayesian inference. The base model is a transformer with attentive graph filter layers, and the input coordinates are encoded using a Fourier embedding. The method is evaluated mainly on convection-diffusion-reaction tasks, as well as Navier-Stokes and shallow-water equations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Interesting model architecture that can perform solution interpolation and temporal extrapolation at the same time. \n2. Introduces theory from prior-fitted neural networks to PDEs."}, "weaknesses": {"value": "1. The paper aims to overcome the limitations of neural PDE solvers to deal with irregular meshes. However, there are many methods that can deal with such domains [1,2,4]. \n2. The framing of the setup as \"in-context learning\" is a bit confusing. The tasks the model generalizes between are actually just solutions of different settings of the PDE (i.e., different ICs for the later experiments). Each task here refers to a single PDE solution/trajectory. In this sense, all neural operators that predict the solution given the IC would be in-context learners. In the original PFN paper, the datasets used in training are each independent tabular datasets, for example. The closest translation to the PDE domain would be to have a model that takes in different trajectories for a given target PDE parameter as the context, and uses this context to infer the solution for another IC of the same parameter. The setup presented in the paper is more akin to masked-pretraining strategies [3], where parts of the trajectory are masked out and the model is trained to fill in the blanks.  \n3. Only FNO-variants and two PDE foundation models are chosen as the baselines. Since the presented model is a transformer variant, it would be good to evaluate against other recent transformer models such as [1,2,4].  \n4. The experiments don't show how much the model actually profits from the ICL formulation. For example, one experiment to do would be to look at the unseen parameters in the temporal extrapolation case. Here, one could use the current architecture, only train with the input points at the first time step, and points on the solution trajectory as the query points. This would be the same setup as LNO, for example.\n\n[1] Luo, H., Wu, H., Zhou, H., Xing, L., Di, Y., Wang, J., & Long, M. (2025). Transolver++: An Accurate Neural Solver for PDEs on Million-Scale Geometries.  \n[2] Wang, T., & Wang, C. (2024). Latent neural operator for solving forward and inverse pde problems.  \n[3] Zhou, A., & Farimani, A. B. (2024). Masked autoencoders are PDE learners.   \n[4] Serrano, L., Wang, T. X., Le Naour, E., Vittaut, J. N., & Gallinari, P. (2024). AROMA: Preserving spatial structure for latent PDE modeling with local neural fields."}, "questions": {"value": "1. How are the other models conditioned on the PDE parameters in 4.2?\n2. During temporal extrapolation: The context consists only of all the spatial points at t=0.1, right? Why did you not start at t=0?\n3. What are the initial conditions in the CDR equations? \n4. What exactly do you mean by \"minimal fine-tuning? What happens if you also use, for example, the pretrained FNO from the first task and fine-tune it on 4.3? Does the PDE-PFN profit more from being pretrained than the other models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "13IqyRtP73", "forum": "z7ilspv4uH", "replyto": "z7ilspv4uH", "signatures": ["ICLR.cc/2026/Conference/Submission19234/Reviewer_HzTi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19234/Reviewer_HzTi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856800088, "cdate": 1761856800088, "tmdate": 1762931214915, "mdate": 1762931214915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PDE-PFN, a prior-data fitted neural network framework for solving partial differential equations via direct approximation of the posterior predictive distribution using in-context Bayesian inference. The proposed method extends the PFN architecture with specialized architectural choices including Fourier feature embeddings and attentive graph filter Transformers. The experimental results demonstrate flexibility in input formats, robustness to noisy priors, zero-shot inference, and improved or competitive performance against state-of-the-art neural PDE solvers and scientific foundation models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tPDE-PFN demonstrates strong generalization capacities across both parameterized PDE families and heterogeneous PDEs (e.g., SWE, CNSE), outperforming or matching established task-specific and foundation model baselines. \n2.\tThe method’s ability to provide stable and high-quality solutions even when pre-trained on approximated, noisy PINN priors is convincing\n3.\tUnlike neural operators constrained to grid data or fixed points, PDE-PFN supports mesh, irregular, and arbitrary-coordinate input/output configurations, as systematically compared in Table 7"}, "weaknesses": {"value": "1.\tAlthough the manuscript is explicit that extension to higher-dimensional or more complex PDEs is left to future work. The scope is currently limited to simple 2D systems. The generalizability and scalability to realistic 3D scientific settings or highly nonlinear, multi-physics PDEs remain untested, detracting from the claimed universality.\n2.\tWhile Fourier features, AGF layers, and the Transformer modifications are pitched as improvements, their individual impact isn’t adequately disentangled. For instance, no ablation is provided isolating the benefit (or necessity) of the AGF layer over, say, vanilla Transformer blocks or simple attention.\n3.\tEquation (3) defines an MSE on PINN priors, but there is little elaboration on the sampling strategies, distribution of priors, or potential class-imbalance effects if the PDE parameter space is highly inhomogeneous."}, "questions": {"value": "1.\tCan the authors provide an ablation study that isolates the incremental impact of AGF layers, Fourier feature embedding, and rational activation on generalization? How would vanilla Transformer, or sinusoidal vs. rational activation, affect baseline and robustness metrics?\n2.\tThe evaluation is mainly on 2D PDEs. Have the authors attempted extending PDE-PFN to 3D cases, higher spatial/temporal resolutions, or PDEs with complex/mixed boundary conditions? If not, what do the authors see as key obstacles?\n3.\tFor the regularization term on the AGF layer, does its influence on overall loss vary greatly with $N_D$ or network depth? How sensitive are results to this regularizer's weighting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nk0VrsHKE5", "forum": "z7ilspv4uH", "replyto": "z7ilspv4uH", "signatures": ["ICLR.cc/2026/Conference/Submission19234/Reviewer_AmpV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19234/Reviewer_AmpV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997609787, "cdate": 1761997609787, "tmdate": 1762931214513, "mdate": 1762931214513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PDE-PFN, a prior-data fitted neural solver that uses a PFN-style Transformer with self- and cross-attention to approximate the posterior predictive distribution of PDE solutions via in-context learning. The model is pre-trained on a broad “family” of 2D convection–diffusion–reaction equations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Methodological novelty and coherence: importing PFN/ICL ideas to PDE solving and training on noisy PINN priors as \"cheap\" prior data is conceptually neat, practical, and aligns with the Bayesian predictive distribution perspective.\n2. Input flexibility: ability to take arbitrary context/query sets (including irregular meshes) is important in real-world sensor-driven settings; the Airfoil and Darcy experiments substantiate this.\n3. Robustness to noisy priors: training with PINN-based approximations yet surpassing baselines indicates the proposed Bayesian-ICL mechanism is effective under noisy pretraining distributions."}, "weaknesses": {"value": "1. While the paper is upfront about this, claims toward “foundation” capabilities would be more compelling with complex PDEs (Navier–Stokes) or complex geometries with strong stiffness. \n2. PINNs are not universally cheap or stable; their training cost and failure modes vary across PDEs/BCs/ICs. The paper could better quantify the end-to-end cost trade-off (pretraining PINNs for 2187 CDR variants is nontrivial).\n3. Positioning vs. recent in-context operator learning SFMs (e.g., ICON-LM, OmniArch) is unclear. The paper claims zero-shot ICL without demos and emphasizes Bayesian-flavored PPD approximation, but it does not clearly articulate architectural and training differences relative to these closely related lines, nor why those differences matter empirically."}, "questions": {"value": "1. What is the total computational budget for generating the 2187 PINN priors and training PDE-PFN versus training a strong neural operator baseline on the same tasks?\n2. How does training and inference scale with (i) number of context points, (ii) query points, and (iii) spatial/temporal resolution? Any memory/time complexity analysis for AGF layers in this setting?\n3. Your theorem addresses zero-mean noise. PINN errors can be biased. How sensitive is PDE-PFN to biased priors? Can you simulate controlled bias in priors and report the impact?\n4. For CNSE, could you also report a rollout setting where DPOT is closer to its original usage (e.g., shorter rollout steps or per-step prediction with consistent teacher-forcing) to better contextualize results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rhh3F0u2ip", "forum": "z7ilspv4uH", "replyto": "z7ilspv4uH", "signatures": ["ICLR.cc/2026/Conference/Submission19234/Reviewer_k7Hr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19234/Reviewer_k7Hr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998094067, "cdate": 1761998094067, "tmdate": 1762931214189, "mdate": 1762931214189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}