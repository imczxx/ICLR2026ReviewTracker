{"id": "QN0PplUVdt", "number": 13564, "cdate": 1758219256123, "mdate": 1762971901425, "content": {"title": "LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems", "abstract": "Planning is one of the most critical tasks in autonomous systems, where even a minor error can lead to significant failures or losses. Current state-of-the-art neural planners struggle in complex domains, often producing plans with missing preconditions, inconsistent goals, or hallucinated steps, while classical planners provide guarantees but lack the flexibility and natural-language understanding needed in modern systems. Existing neuro-symbolic methods typically perform a one-shot translation from natural language to formal plans. In safety-critical autonomous systems, this leaves no mechanism to detect and correct specification errors before execution. To address this, we introduce LOOP, a neuro-symbolic planning framework that models planning as an iterative interaction between neural and symbolic modules. It synthesizes Planning Domain Definition Language (PDDL) models from task descriptions, refines them using feedback from a symbolic planner and execution rollouts, and builds a causal knowledge base from traces to guide subsequent plans. Across six International Planning Competition (IPC) domains, LOOP attains 85.8% task success, surpassing LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). Together, these results indicate that consistent planning arises from sustained interaction between neural and symbolic reasoning rather than one-shot translation.", "tldr": "LOOP is a closed-loop neuro-symbolic planner that iteratively synthesizes/refines PDDL and learns a causal knowledge base from executions; 85.8% success on six IPC domains, outperforming prior LLM-based planners.", "keywords": ["Planning with Language Models", "Neuro-Symbolic Learning", "Causal Learning", "Learning for Planning and Scheduling", "Coordination and Collboration"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b0d5ab1dce9a8e8f1f17c74fdd817d7dec0fd59f.pdf", "supplementary_material": "/attachment/f30a7214c661d366b4eccf1dbfa7461345914f11.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces LOOP, a plug-and-play neuro-symbolic framework designed to enhance automated planning in autonomous systems. LOOP enables iterative interaction between neural and symbolic modules, integrating causal learning, multi-agent validation, and hierarchical decomposition to progressively refine PDDL-based plans. The framework models planning as a closed-loop process, allowing neural components to generate candidate plans while symbolic planners provide feedback for correction and convergence. Evaluated on six IPC domains and two extended benchmarks, LOOP achieves an average success rate of 85.8%."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- LOOP combines neural reasoning, symbolic validation, and causal learning into a cohesive iterative pipeline that addresses key limitations of one-shot LLM planning.\n- The neuro-symbolic interaction and causal trace learning offer a degree of transparency uncommon in LLM-integrated planners."}, "weaknesses": {"value": "- It would be helpful to compare against PSALM [1], which reports higher success rates on the same IPC domains using environment feedback for action semantics induction.\n- The paper omits several relevant works from Kambhampatiâ€™s group, including Guan et al. [2] and others on feedback-guided planning and world-model construction, which should be cited and discussed for completeness.\n- Many tables are oversized and difficult to read in the current layout, consider splitting them across pages or reformatting for clarity.\n- The evaluation primarily focuses on GPT-4; it remains unclear whether similar performance can be achieved with smaller or open-weight models, which affects reproducibility and accessibility.\n- How does LOOP handle partial or noisy feedback, which is essential for demonstrating the benefits of causal memory and multi-agent validation?\n\nReferences:\n- [1] Zhu et al., Language Models can Infer Action Semantics for Classical Planners from Environment Feedback (NAACL 2025).\n- [2] Guan et al., Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning (Neurips 2023)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UCiWUK3d8G", "forum": "QN0PplUVdt", "replyto": "QN0PplUVdt", "signatures": ["ICLR.cc/2026/Conference/Submission13564/Reviewer_1UBs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13564/Reviewer_1UBs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943998552, "cdate": 1761943998552, "tmdate": 1762924161146, "mdate": 1762924161146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "DK1i7jnWsN", "forum": "QN0PplUVdt", "replyto": "QN0PplUVdt", "signatures": ["ICLR.cc/2026/Conference/Submission13564/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13564/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762971900712, "cdate": 1762971900712, "tmdate": 1762971900712, "mdate": 1762971900712, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LOOP, a neuro-symbolic planning framework that models planning as an iterative interaction between neural and symbolic modules. LOOP demonstrates good performance across six benchmark domains, outperforming several baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method achieves good performance on 6 domains."}, "weaknesses": {"value": "1. The paper is poorly written and contains several unreasonable or problematic aspects:\n\n(1) The authors claim that their method can refine PDDL models, but the paper does not clearly explain how this refinement is actually performed.\n\n(2) The Related Work section lacks a sufficient discussion distinguishing the proposed approach from existing methods.\n\n(3) The Theoretical Foundation section fails to provide the necessary theoretical background or analysis to justify the effectiveness of the proposed iterative optimization method.\n\n(4) The paper suffers from noticeable formatting issues:\n  (a) Tables 2 and 4 exceed the normal paragraph spacing;\n  (b) Quotation marks are incorrectly used. The left double quotation mark should be `` and the left single quotation mark should be `;\n  (c) Example (Line 228-232) is inserted directly into the main paragraph without following the proper paragraph formatting style.\n\n2. The evaluation only considers GPT-4 as the underlying LLM in the proposed system. Without testing additional language models, it is difficult to assess the general applicability and robustness of the proposed framework."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VCdx91MLfl", "forum": "QN0PplUVdt", "replyto": "QN0PplUVdt", "signatures": ["ICLR.cc/2026/Conference/Submission13564/Reviewer_pVHv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13564/Reviewer_pVHv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762026474388, "cdate": 1762026474388, "tmdate": 1762924160677, "mdate": 1762924160677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tries to explain an implementation of a LLM-based (sentence-transformer embedding) system that\nsupposedly uses classical planners as validators that provide a feedback to LLM,\na common approach in the recent natural-language-based planning literature.\n\nWhile the abstract claims that it achieves a good performance,\nit is unfortunately immediately obvious that\nthis paper is either not proofread enough or completely written by LLMs,\nmaking it not ready for submission or publication."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "None.\n\nIf there is anything that caught my eye,\nit is the idea of Cross-Domain Pattern Transfer;\nIt effectively stores a macro-action as a policy,\nbut in a way that generalizes the macro action with natural language semantics.\nThis sounds new and interesting,\nbut the paper in the current state cannot is not publishable."}, "weaknesses": {"value": "The writing issue starts from the very first page of the paper.\nHaving so many issues in the very face of the paper (= introduction) means that this paper has not be thoroughly proofread.\nIssues that are immediately spotted are\n\n-   Many \\cite / \\citet / \\citep command misuses in the introduction.\n-   Many style inconsistency in the references, e.g.,\n    -   ICAPS conference proceedings are sometimes with the number (Thirty-Fourth &#x2026;) and sometimes not\n    -   Citations without venues (e.g. just \"2024a\")\n-   Overfull hbox in many tables\n\nThe style in which the main text explains the method (Section 3) is so unorthodox, almost ChatGPT-like, and reads completely gibberish to me.\nThe explanation of the main method is so abrupt with so little justification for the design choice,\nleaving little room for the readers to gain meaningful insights.\n\nFirst of all, what is the task, what is the input, what is the output in Figure 1? Formalize it.\nThen, going over the text, more concretely,\n\n> LOOP solves these problems through a simple decision flow (Figure 1).\n\nFigure 1 is NOT simple at all!\n\n> Natural language tasks enter the system and get converted into features that measure how well the\n\nWhat features? How it is converted?\n\n> system knows this type of problem. If confidence is high, the system breaks the problem into\n> smaller pieces that can be solved in parallel. If confidence is low, it generates solutions step-by-\n\nHow does it decompose the problem? Note that this is possible only for a select subset of planning domains (serial decomposability),\nand domains with serial decomposability are considered easy. In other words, this system works only on easy tasks.\n\n> step with multiple agents checking each step. Both paths create PDDL files that get solved by\n> classical planners. After execution, the system learns from what worked and what failed, building\n> up knowledge for future problems. This creates a cycle where the system gets smarter over time\n> by remembering successful patterns and avoiding past mistakes. The following sections detail how\n> each component works mathematically and technically.\n\nWhat is the output? \n\n> Confidence-Based Strategy Selection\n\n> The weights reflect empirically determined importance rankings from &#x2026;\n\nHow did you determine the coefficients?\n\n-   experience similarity (Cexp)\n-   complexity assessment\n-   causal knowledge availability\n-   domain expert availability\n\nNone of them are defined yet.\n\n> where Cexp searches neural memory for similar task embeddings using cosine similarity,\n> Ccomplexity analyzes object count and constraint density through neural networks, Ccausal queries\n> the causal memory for relevant relationships, and Cdomain considers expert agent availability and\n> historical performance.\n\nFirst of all, this paragraph begins with an incomplete sentence.\n\n-   neural memory\n-   object count and constraint density\n-   causal memory for relevant relationships\n-   expert agent availability\n\nNone of these are defined yet.\n\n> causal relation types (ENABLES, REQUIRES, PRODUCES, PREVENTS, MODIFIES)\n\nNot explained at all.\n\n> Hierarchical Task Decomposition\n\nThe paper does not explain how it decomposes the task at all.\n\nI do not believe it is worth roasting the entire reminder of the paper this way,\nwhich resembles the bullet-point style output from ChatGPT.\n\nBeing the LLM-produced nature of this paper,\nI cannot reasonably trust the authors that they did not produce the tables using LLM.\nThis renders the entire empirical section and the numbers in the table unreliable, making the review meaningless."}, "questions": {"value": "None. Already explained above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "m8HkDYLpEy", "forum": "QN0PplUVdt", "replyto": "QN0PplUVdt", "signatures": ["ICLR.cc/2026/Conference/Submission13564/Reviewer_cSaU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13564/Reviewer_cSaU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061093482, "cdate": 1762061093482, "tmdate": 1762924159855, "mdate": 1762924159855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}