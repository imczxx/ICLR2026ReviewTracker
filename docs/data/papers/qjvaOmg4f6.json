{"id": "qjvaOmg4f6", "number": 2593, "cdate": 1757158410017, "mdate": 1759898138705, "content": {"title": "Mixture of Groups: Grouped Gating and Cross Mixing for Parameter-Efficient LLM Fine-Tuning", "abstract": "Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning (PEFT) method for large language models. However, by learning independent adapters for each layer, LoRA and its variants ignore the inherent functional similarity between adjacent layers, limiting their potential to fully exploit the hierarchical representations across depth. To address this, we propose Mixture of Groups (MoG), a novel group-sharing framework that partitions layers into functional groups, shares low-rank adapters within each group, and employs adaptive gating and cross-mixing mechanisms to enable flexible fine-tuning. This approach leverages inter-layer similarity to capture both commonalities and unique characteristics across layers, achieving a more efficient and expressive subspace than LoRA. Moreover, MoG is designed as a plug-and-play framework that can be seamlessly integrated into other PEFT methods such as DoRA and PiSSA to boost their performance. Extensive experiments on multiple benchmarks demonstrate that MoG achieves overall superior performance compared with prior methods under comparable parameter budgets, highlighting its ability to combine efficiency with strong downstream effectiveness.", "tldr": "", "keywords": ["large language model", "parameter-efficient fine-tuning", "low-rank adaptation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9228aea720a3806a1ed1048141574ad22705f941.pdf", "supplementary_material": "/attachment/2397f93cf36ba73388d5f755b8533e8e121163a1.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Mixture of Groups (MoG), a new parameter-efficient fine-tuning (PEFT) framework for large language models. It clusters adjacent layers into functionally similar groups that share low-rank adapters and augments them with adaptive gating plus cross-mixing, breaking the dichotomy between per-layer LoRA and fully shared approaches. Extensive experiments on several LLMs and benchmarks show that MoG delivers higher accuracy than previous PEFT baselines under the same or smaller parameter budgets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Thoughtful design: grouping, gating, and cross-mixing are well-motivated by empirical analyses of inter-layer similarity and provide a principled middle ground between fully shared and fully independent adapters.\n\n2. The orthogonal design enables MoG to benefit from a wide range of LoRA variants.\n\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. Comparative puzzle: vanilla LoRA underperforms DoRA in your tables, yet LoRA+MoG surpasses DoRA+MoG; please clarify why MoG benefits LoRA more than DoRA—does MoG overlap with DoRA’s decomposition or were hyper-parameters unequal?\n\n2. From my experience, llm-adapter framework is very sensitive to learning rates in the range {1e-4, 2e-4, 3e-4}. The vanilla LoRA result on Llama-3 8B was likely obtained with a learning rate of 3e-4, whereas the other methods were not. Could you rerun LoRA on Llama-3 8B with a 1e-4 learning rate and report the results to provide readers with a clearer comparison?"}, "questions": {"value": "please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UDG1nodbqd", "forum": "qjvaOmg4f6", "replyto": "qjvaOmg4f6", "signatures": ["ICLR.cc/2026/Conference/Submission2593/Reviewer_buem"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2593/Reviewer_buem"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761204935781, "cdate": 1761204935781, "tmdate": 1762916294289, "mdate": 1762916294289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MoG, a novel framework for PEFT of LLMs. The paper identifies a key limitation in the popular LoRA method: by learning independent adapters for every layer, LoRA ignores the functional similarity between adjacent layers, leading to parameter redundancy. Conversely, other methods that rely on global parameter sharing can overlook layer-specific functional diversity, thereby limiting expressiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Balanced Framework: MoG introduces a novel group-sharing framework that balances parameter efficiency by reducing redundancy with layer-specific expressiveness.\n- Flexible Architecture: The model uses adaptive gating and cross-mixing to create a highly flexible and expressive adaptation subspace that generalizes LoRA.\n- Versatile Plug-and-Play Module: MoG is designed as a general-purpose module that can be plugged into other PEFT methods, such as DoRA and PiSSA, to boost their performance.\n- Strong Empirical Results: The method is validated by extensive experiments across multiple LLMs and benchmarks, demonstrating superior performance over strong baselines with comparable parameter budgets."}, "weaknesses": {"value": "- `Limited Motivation and Static Mechanism`: The paper claims to employ adaptive gating and cross-mixing mechanisms to capture both commonalities and unique features across layers by leveraging inter-layer similarity. However, a significant limitation is that the fusion weights generated for each layer or group are static. This approach appears disconnected from the paper's own findings, as illustrated in Figure 1(a), which clearly indicates dynamic variations in cosine similarity between different layers. The potential relationship between these dynamic similarity scores and the static fusion weights is a critical, unexplored area that warrants further investigation.\n- `Limited Comparative Analysis`: The paper's methodology draws inspiration from MoE principles to some extent. Despite this, the experimental evaluation conspicuously omits comparisons against relevant and contemporary MoE-based LoRA variants ($e.g.$, [1-3]) as baselines. I recommend incorporating these methods to provide a truly comprehensive and meaningful comparison. Furthermore, the experimental scope is narrow regarding model scale and architecture. The analysis would be substantially strengthened by including evaluations on models of varying sizes, such as Qwen2.5-3B, Qwen2.5-7B, and Qwen2.5-14B, to demonstrate the method's scalability.\n- `Limited Application Scenarios`: The empirical validation is currently confined to single-task environments. This narrow focus significantly limits the method's potential applicability and generalizability, particularly for the increasingly prevalent and complex multi-task scenarios found in real-world applications. To substantiate the robustness and effectiveness of the proposed approach, it is essential to extend the evaluation to include challenging multi-task benchmarks ($e.g.$, OpenOrca).\n\n[1] When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications\n\n[2] HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning\n\n[3] CoLA: Collaborative Low-Rank Adaptation"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yLciFn2M90", "forum": "qjvaOmg4f6", "replyto": "qjvaOmg4f6", "signatures": ["ICLR.cc/2026/Conference/Submission2593/Reviewer_Cvwa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2593/Reviewer_Cvwa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381892726, "cdate": 1761381892726, "tmdate": 1762916294032, "mdate": 1762916294032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Large language models (LLMs) excel in NLP but are costly for full fine-tuning. Low-Rank Adaptation (LoRA), a common Parameter-Efficient Fine-Tuning (PEFT) method, uses independent per-layer adapters, causing redundancy and wasting inter-layer similarity. We propose **Mixture of Groups (MoG)**, a new PEFT framework: it splits LLM layers into groups (sharing low-rank adapters to cut redundancy), uses adaptive gating for layer-specific parameter combination, and adds a cross-mixing module to boost representational capacity.  \n\nAs a plug-and-play tool, MoG integrates with DoRA/PiSSA to enhance their performance. Experiments on commonsense reasoning, math, code generation, and GLUE show MoG outperforms LoRA/DenseLoRA under similar parameter budgets, balancing efficiency and effectiveness in LLM fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tFocusing on an important problem of existing PEFT methods: Clearly points out the limitations of LoRA (redundancy from independent per-layer adapters) and global sharing methods (e.g., VeRA, DenseLoRA, which ignore inter-layer functional diversity). It also verifies the functional similarity of adjacent LLM layers through experimental evidence like DOCS analysis and t-SNE visualization, providing solid theoretical and data support for the method design.\n2.\tFlexible architecture with strong compatibility: The MoG framework has a \"plug-and-play\" feature. It can be used independently as a PEFT method and seamlessly integrated into mainstream PEFT methods such as DoRA and PiSSA (e.g., DoRA(+MoG) and PiSSA(+MoG) both improve the performance of the original methods). It also supports architecture degradation (degenerates to LoRA when n=L and E is an identity matrix, and is similar to DenseLoRA when n=1), adapting to different fine-tuning needs.\n3.\tModular component design with high interpretability: The three core components of MoG (\"group sharing-adaptive gating-cross mixing\") have clear functions and divisions of labor. Ablation experiments prove that no component is dispensable (removing the cross-mixing module reduces performance by 1.2%, and removing gating reduces it by 1.4%). Meanwhile, gating coefficient visualization shows that the gating distribution changes from \"one-hot\" to \"flexible\" after training, which dynamically adjusts the weight of group parameters according to tasks. The working mechanism of components is interpretable, facilitating subsequent optimization and improvement.\n4.\tRigorous and comprehensive experimental design, with credible conclusions: Experiments cover multiple task types (reasoning, generation, understanding) and models of different scales. Baseline methods include mainstream PEFT methods like LoRA, RaSA, and DenseLoRA. A unified parameter budget ensures fair comparison. In addition, extended experiments (parameter efficiency analysis, group number impact analysis, resource consumption analysis) are added to verify the method’s advantages from different dimensions, making the conclusions more convincing."}, "weaknesses": {"value": "1.\tLimited exploration of group number selection. The optimal number of groups (n) varies by task/model, but no clear guidelines for choosing n are provided.\n2.\tLess testing on extremely large models. Most experiments use 7B/8B models; performance on larger models (e.g., 70B) remains unproven.\n3.\tHigher complexity than basic LoRA. The cross-mixing module and gating mechanism add minor computational overhead, though small.\n4.\tLimited analysis of hyperparameter sensitivity. How factors like rank (r) or learning rate affect MoG’s stability needs more study."}, "questions": {"value": "1. Could you provide practical guidelines or automated methods to determine the optimal number of groups (n) for different tasks and model scales?  \n2. Have you tested MoG on larger models (e.g., 70B LLMs)? If not, do you expect similar performance gains there?   \n3. How sensitive is MoG to hyperparameters like rank (r) or learning rate? Are there stable ranges to recommend?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8DWZTREf4c", "forum": "qjvaOmg4f6", "replyto": "qjvaOmg4f6", "signatures": ["ICLR.cc/2026/Conference/Submission2593/Reviewer_p58V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2593/Reviewer_p58V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897764675, "cdate": 1761897764675, "tmdate": 1762916293812, "mdate": 1762916293812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the parameter redundancy problem in LoRA, which ignores the functional similarity between adjacent layers. It proposes a new PEFT framework called MoG. MoG reduces redundancy through group sharing, while using gated adaptive aggregation and cross mixing mechanisms to keep each layer’s specificity. This design achieves a delicate balance between parameter sharing and model expressiveness. Through extensive experiments, the authors show that MoG significantly outperforms LoRA and other sharing methods under the same parameter budget, achieving much higher parameter efficiency. Moreover, MoG is a general plug-and-play module that can be combined with other PEFT methods such as DoRA and PiSSA to further improve performance. Overall, MoG introduces an innovative and efficient new PEFT paradigm."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written.\n- The method is simple, and the plug-and-play design brings some performance gains."}, "weaknesses": {"value": "- The paper lacks comparisons with related work. Many studies have optimized LoRA from the efficiency perspective, some focus on reducing the rank, while others focus on parameter sharing, but the baselines here do not include these methods. (https://arxiv.org/pdf/2406.10785 ; https://openreview.net/pdf?id=IXYBuwCOMl; https://arxiv.org/html/2410.11772v1 ; https://arxiv.org/html/2402.08562v1 ; https://arxiv.org/abs/2410.19694)\n- The idea of \"less is more\" in PEFT has been discussed in many works (including but not limited to those mentioned above). Therefore, the contribution of this paper may be limited.\n- The paper also lacks quantitative analysis of how much MoG actually reduces parameters. Since GPU memory usage mainly comes from the base model, the computational cost reduction of MoG may be limited."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rP4mGVYNjt", "forum": "qjvaOmg4f6", "replyto": "qjvaOmg4f6", "signatures": ["ICLR.cc/2026/Conference/Submission2593/Reviewer_NQti"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2593/Reviewer_NQti"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945726488, "cdate": 1761945726488, "tmdate": 1762916293636, "mdate": 1762916293636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}