{"id": "agRa60l7rn", "number": 2299, "cdate": 1757053890225, "mdate": 1759898157173, "content": {"title": "Failure makes the agent stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions", "abstract": "Tool-augmented large language models (LLMs) are typically trained via supervised imitation learning or coarse-grained reinforcement learning, approaches that primarily optimize one-shot tool calls. Existing practices of self-reflection largely rely on heuristic prompting or unidirectional reasoning traces: the model is encouraged to “think more,” rather than to treat error diagnosis and correction as a learnable capability. This makes them fragile in multi-turn interaction settings—once a call fails, the model tends to repeat the same mistake instead of recovering.\nTo address this issue, we propose structured reflection, which transforms the “from error to repair” process into a first-class, controllable, and trainable action. The agent produces a concise yet precise reflection process: specifically, the model diagnoses the error based on evidence from the previous step and then proposes a correct and executable follow-up call. During training, we combine DAPO and GSPO's objective functions and design a more principled reward mechanism tailored to tool calling, optimizing the stepwise strategy Reflect \\\\(\\\\to\\\\)  Call \\\\(\\\\to\\\\)  Final.\nTo evaluate this capability, we introduce Tool-Reflection-Bench, a lightweight benchmark dataset that programmatically verifies structural validity, executability, parameter correctness, and result consistency. Tasks in the benchmark are constructed as miniature trajectories of Erroneous Call \\\\(\\\\to\\\\)  Reflection \\\\(\\\\to\\\\)  Corrected Call and are split into disjoint training and testing sets.\nExperiments on BFCL v3 and Tool-Reflection-Bench show that our method achieves significant improvements in multi-turn tool-call success rates and error recovery, while also reducing redundant calls. These results demonstrate that making reflection explicit and treating it as an optimization objective can substantially enhance the reliability of tool interaction, providing a reproducible pathway for agents to grow stronger by learning from failure. We will release all the code and datasets as open source once the paper is accepted by the community.", "tldr": "", "keywords": ["Structured reflection", "GRPO for tool-use RL", "Error-driven self-correction"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76fa29787f200b97fbbb7adc34eee9dbec499f43.pdf", "supplementary_material": "/attachment/036ddcbf3c0a6622fe64caa331e1412c117b77fe.zip"}, "replies": [{"content": {"summary": {"value": "This paper adopts reinforcement learning (such as GRPO) to incentivize the reflective capabilities of large language models (LLMs). It first identifies four types of mistakes that frequently occur during LLMs' task-solving process. Based on these mistake types, the author proposes an automatic data synthesis approach to generate a prefix that includes predefined mistakes, forcing LLMs to reflect on the context and generate a new action. Experiments on several datasets validate the effectiveness of the proposed method."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Enabling \"reflection ability\" of LLMs is an urgent and important topic for tool-use agents. This paper builds a new dataset, Tool-Reflection-Bench, and uses reinforcement learning (e.g., GRPO) to incentivize LLMs' reflection abilities.\n\n2. The data synthesis approach in this work is well-organized and easy to follow. This simple yet efficient approach can automatically generate reflection-oriented training data (trajectories and prefixes)."}, "weaknesses": {"value": "1. I am not clear about the training data. Does each example in the training data contain a prefix $x$, which is then fed into LLMs for continuous action generation until reaching the final answer?\n\n2. What is the main difference between existing papers like [1], especially considering that \"reflection\" has been a common technique? It seems like the main contribution is enabling reflection using RL rather than supervised fine-tuning.\n \n3. Experiments in this work lack the necessary comparison with existing baselines, such as ToolLLama and ToolACE, other advanced baselines in the BFCL leaderboard.\n\n4. The main figure of this paper should be polished. The current version is not clear. I suggest that the author highlight the main methodology design and the differences with existing work.\n\n5. I am a bit confused about the experiment setup. Does the author put all candidate tools into the context of LLMs, and then prompt the LLMs to call appropriate tools on demand? If so, what if the tool scale is large and the tool description exceeds the model context length?\n\n---\n### Reference\n\n[1] Advancing Tool-Augmented Large Language Models via Meta-Verification and Reflection Learning\n\n[2] ToolACE: Winning the Points of LLM Function Calling"}, "questions": {"value": "See above weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ns51r13MP0", "forum": "agRa60l7rn", "replyto": "agRa60l7rn", "signatures": ["ICLR.cc/2026/Conference/Submission2299/Reviewer_p6We"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2299/Reviewer_p6We"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792465571, "cdate": 1761792465571, "tmdate": 1762916183039, "mdate": 1762916183039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a structured reflection method to improve tool-calling capabilities of large language models through explicit error diagnosis and correction. The authors introduce Tool-Reflection-Bench, a benchmark constructed by systematically perturbing correct tool calls to create failure cases, and design a multi-dimensional reward mechanism for reinforcement learning. The method combines DAPO and GSPO objectives to train models to reflect on failed tool calls and generate corrected ones. Experiments on BFCL v3 and Tool-Reflection-Bench demonstrate improvements in multi-turn tool-call accuracy and error recovery rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a genuine problem in tool-augmented LLMs by making error recovery an explicit, trainable capability rather than relying on heuristic prompting, which represents a meaningful shift from prior self-correction approaches.\n\n- The reward design is well-motivated with multiple components (format, tool name, parameters, semantic consistency) that provide granular signals for tool-calling scenarios.\n\n- The experimental results show consistent improvements across multiple base models on both benchmarks, with particularly strong gains in repair rates that surpass closed-source models of similar scale."}, "weaknesses": {"value": "- The benchmark construction methodology raises concerns about generalization, as all four perturbation types (call-order swap, redundant call, missing call, argument error) are synthetic and may not capture the full distribution of real-world tool-calling failures that occur naturally in deployed systems.\n\n- The paper lacks critical ablation studies to validate design choices—it seems there is no analysis of individual reward components, no comparison between DAPO+GSPO versus alternatives (standard PPO, pure DAPO, pure GSPO), and no investigation of how much improvement comes from the reflection mechanism versus the enhanced reward structure."}, "questions": {"value": "How does your method perform on naturally-occurring tool-calling errors from real user interactions or other benchmarks, and what percentage of real-world failures fall into your four perturbation categories versus other failure modes not covered by your taxonomy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6xJsEMbjHB", "forum": "agRa60l7rn", "replyto": "agRa60l7rn", "signatures": ["ICLR.cc/2026/Conference/Submission2299/Reviewer_CgsU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2299/Reviewer_CgsU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963971893, "cdate": 1761963971893, "tmdate": 1762916182831, "mdate": 1762916182831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces structured reflection in which transforming the \"from error to repair\" process into a trainable action, to solve the problem that models  often fail to recover from errors and tend to repeat the same mistakes. The authors combine objective functions of DAPO and GSPO in training and desigh a reward  mechanism for tool calling. The authors also introduce Tool-Reflection-Bench for evaluation. The results on BFCL v3 and Tool-Reflection-Bench show great improvements in multi-turn tool-call success rates and error recovery."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work introduces a novelty training mechanism about reflection, making contributions on models' tool-call abilities. And the results show great improvement for models.  \n\n2. This work proposes 4 pertubations and constructs Tool-Reflection-Bench for evaluation.\n\n3. The paper is well-written and easy to follow, with enough case studies."}, "weaknesses": {"value": "1. Limited Discussion on generalization and model scale: Although  the paper discusses about performance on 4 perturbations and BFCL v3, it still lacks analysis about how well the learned capability generalizes to more failures in real world. Also, the models in experiments are 4B to 8B. The results remains unclear on much larger models.  \n\n2. Comparison with other RL methods : The paper chooses a complex RL pipeline with DAPO+GSPO and increase the complexity and cost without doubt. However, the paper lacks a discussion on the cost-benefit trade-off with other traditional RL methods. \n\n3. Failure of reflection itself: This paper focuses on using reflection to fix tool-call errors. However, the failure of reflection itself also makes sense. I think authors should also pay attention to the error reflection or the inconsistency between reflection and execution in the process."}, "questions": {"value": "1. How does the learned reflection capability generalize to failures beyond the four specific perturbation types? Additionally, how do you expect the method's effectiveness to scale with much larger models (e.g., 70B+)?\n\n2. Could you analysis the trade-off between the complexity of the RL pipeline and the performance gains, particularly when compared to simpler, less costly alternatives?\n\n3. Have you analyzed instances of \"reflection failure,\" where the model generates an incorrect reflection or its subsequent action is inconsistent with its own reflection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vl4O4NMhH7", "forum": "agRa60l7rn", "replyto": "agRa60l7rn", "signatures": ["ICLR.cc/2026/Conference/Submission2299/Reviewer_rfh8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2299/Reviewer_rfh8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994986765, "cdate": 1761994986765, "tmdate": 1762916182623, "mdate": 1762916182623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to \"transform self-correction [by LLMs] into a trainable and controllable capability\" (line 186).\nThe basic idea is to convert self-correction into a supervised training problem x ↦ y.  Each synthetic training example starts with a correct y and synthesizes a plausible erroneous version x that must be corrected into y (lines 100-102, 112-113).\n\n(Cf. a denoising autoencoder, where a supervised model is trained to undo a corruption step, or similarly BART and its ilk.  However, those are merely artificial tasks used for representation learning, whereas this paper is interested in the real task of correcting errors that actually arise during LLM message sequences.  Such real-task settings in prior work include grammatical error correction, e.g., [Felice 2016](https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-895.pdf), [Kiyono et al. 2019](https://aclanthology.org/D19-1119/), and code correction, e.g., [Gupta et al. 2017](https://ojs.aaai.org/index.php/AAAI/article/view/10742).)\n\nThis is really a general recipe for training self-correction, and a sensible idea if not a new one.\nThe difficulty is in figuring out how to create \"plausible\" corruptions.\n\nThis paper specifically focuses on tool calling.  Some points worth noting:\n\n1. It uses a few simple hand-designed corruption operators (lines 220-227) based on manual analysis of error patterns (line 98).  \n\n1. The corruptions are not *learned* to match the actual distribution of errors, as I had imagined at first. \n\n1. While the corrections are observed, the chains of thought needed to achieve them (\"reflections\") are provided by humans.\n\n1. The method is trained by RL and not just by SFT, which introduces more complexity since a finicky loss function must be specified (section 3.2)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Quality: The basic idea makes sense and the execution seems competent.  I agree that self-correction by LLMs should be trainable, and that supervised trajectories can be modified to provide examples that need to be corrected back to the supervised trajectory.\n\nSignificance: The improvements on the existing BFCL benchmark (Table 1) seem quite strong -- although I am not familiar with the benchmark and may not be interpreting the results appropriately.\n\nThe approach might not be too hard to adapt to training some other kinds of LLM self-correction (this paper just focuses on tool calls).\n\nClarity: I was mostly able to follow the paper, although I have some confusions (see questions below).  Detailed appendices are provided; the \"case studies\" in section A.3 are helpful for making the method more concrete."}, "weaknesses": {"value": "The general direction is not exactly new.  The detailed method is somewhat complicated, and is not compared to simpler alternatives.\n\nThe results are presented very briefly, and are not really discussed in the text (just recapitulated from the tables).  I don't have a clear understanding of the qualitative patterns of the results.\n\nThere may have been some missed opportunities.  See questions below."}, "questions": {"value": "**Related work.** The paper seems at least loosely related to [Wang et al. (2024)](https://aclanthology.org/2024.acl-long.570/), [Xu et al. (2024)](https://arxiv.org/abs/2406.17465), and [Qu et al. (2025)](https://arxiv.org/abs/2410.08197), none of which are cited.  However, those papers actually try executions, and I think this paper doesn't.  Can you discuss the submitted work in the context of that line of prior work?\n\n**Reflection generation.** The prompt at line 691 doesn't seem able to generate a good context-dependent reflection+correction, because it doesn't give any documentation for the tool API or any context about what this particular call was trying to achieve!  Can you explain?  \n\n**Cost of human supervision.** Human supervision is used to correct (r,c) to (r*,c*) at (8).  Who provided the supervision?  How expensive was it?  Apparently (line 415) you produced at least 5k examples, though I'm not sure whether that means steps or multi-step trajectories.\n\n**STaR workflow.** The point of this is to get a good reflection r*, since c* is known already.  But after obtaining a few human examples of this to illustrate the style (e.g., for few-shot prompting), perhaps one could use STaR ([Zelikman et al. 2022](https://arxiv.org/abs/2203.14465)) to have the LLM itself fill in r* (given c*) that is achievable by the model and tends to lead to c*?\n\n**Imitation learning.**  Since supervised trajectories $m_0,m_1,\\dots$ are available to the learner, why not try imitation learning?  For example, instead of generating errors by artificial corruption, why not train to correct the learner's *currently predicted* action $\\tilde{m}\\_i$ in the context $m_0,\\dots,m_{i-1}$ to the supervised action $m\\_i$?  \n\n* Apologies that my notation here is a little sloppy.  The currently predicted action may be the result of preceding predictions that are self-corrected; here I am imagining that those preceding predictions and their reasoning steps fall between $m_{i-1}$ and $\\tilde{m}\\_i$.  If the currently predicted action is wrong, then we want to learn an additional self-correction action, otherwise an \"accept\" action.  \n\n* This standard supervised imitation learning method is imperfect because of exposure bias: the distribution of contexts $m_0,\\dots,m_{i-1}$ at test time is produced by the learned policy and probably won't match the training distribution ([Ross et al. 2010](https://arxiv.org/abs/1011.0686)).  But it may still be better than the method proposed in the submission.\n\n* There are certainly *non*-sequential settings where a post-correction model is learned to correct system output to the reference output.  The machine translation community has a whole shared task series around \"automatic post-editing\" (APE).  Here, exposure bias is not an issue.\n\n**Supervised training.** How important is it to train with the reward of section 3.2 rather than just log-loss (SFT to match the demonstrated (r*, c*) pair)?\n\n**Task reward.** Do you only have rewards from (24)-(25), which ask about matching the demonstration?  (That's what line 419 implies.)  Would it make sense to also reward trajectories where the tool calls manage to achieve the goal in some other way?\n\n**Reward design.** Can you give an example of (11)?  I'm not sure I understand what C\\_calls and G\\_calls look like.  Are they each a sequence of revisions to a call, with c\\_final or g\\_final being the final accepted revision in that sequence?  But if so, I have several confusions:\n* should the ground truth have $j=0$ in almost all cases?\n* such a sequence would be ordered, but you say \"multiset,\" which implies that they are unordered (and presumably ordered arbitrarily in the prompt).\n* why doesn't each $c_i$ or $g_j$ have its own reflection \n* if c\\_ref and g\\_ref are reflections, why isn't their main symbol $r$? (perhaps these should be called $r$ and $r*$ as in the previous section?)\n\n**Learning how many times to revise.** Section 3.1.3 creates supervised examples of tool calls that need to be corrected, but don't you also need supervised examples of tool calls that _don't_ need correction (hence, 0 corrections)?  Also, does your system learn how to iteratively correct until convergence?  I'm not sure.\n\n**Long-context recovery.** Mentioned at line 443: what do you mean?\n\n**Table 1 results.** Table 1 evaluates on BFCL, but almost nothing is said about this setup.  What is BFCL?  How many turns are there per trajectory?  What metric is being reported in Table 1, and is it per-turn or per-trajectory?  Do the columns correspond to error categories that are provided by the BFCL dataset?  Are the improvements statistically significant?  Can you say more about whether corrections are applied at the right times (precision/recall), and whether they are the correct type of correction and whether they are successful?  If they are unsuccessful, is a further correction then applied?\n\n**Table 2 results.** This is the test split of your own synthetic dataset, right?  Since your model was trained on the training split of the same dataset, you would expect it to do well, right?  So is this table just a sanity check that your training worked?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SMw9xSguvS", "forum": "agRa60l7rn", "replyto": "agRa60l7rn", "signatures": ["ICLR.cc/2026/Conference/Submission2299/Reviewer_Ni9n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2299/Reviewer_Ni9n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998846887, "cdate": 1761998846887, "tmdate": 1762916182387, "mdate": 1762916182387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}