{"id": "rex7s82Iav", "number": 16605, "cdate": 1758266673725, "mdate": 1759897229996, "content": {"title": "Error Feedback for Muon and Friends", "abstract": "Recent optimizers like Muon, Scion, and Gluon have pushed the frontier of large-scale deep learning by exploiting layer-wise linear minimization oracles (LMOs) over non-Euclidean norm balls, capturing neural network structure in ways traditional algorithms cannot. Yet, no principled distributed framework exists for these methods, and communication bottlenecks remain unaddressed. The very few distributed variants are heuristic, with no convergence guarantees in sight. We introduce EF21-Muon, the first communication-efficient, non-Euclidean LMO-based optimizer with rigorous convergence guarantees. EF21-Muon supports stochastic gradients, momentum, and bidirectional compression with error feedback–marking the first extension of error feedback beyond the Euclidean setting. It recovers Muon/Scion when compression is off and specific norms are chosen, providing the first efficient distributed implementation of this powerful family. Our theory covers non-Euclidean smooth and the more general3 (L0, L1)–smooth setting, matching best-known Euclidean rates and enabling faster convergence under suitable norm choices. We further extend the analysis to layer-wise (generalized) smoothness regimes, capturing the anisotropic structure of deep networks. Experiments on NanoGPT benchmarking EF21-Muon against uncompressed Muon/Scion/Gluon demonstrate up to 7× communication savings with no accuracy degradation.", "tldr": "", "keywords": ["optimization", "communication efficiency", "compression", "error feedback"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/41586e6a70703f15e39b77653563331bfcabf01c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes EF21-Muon, which aims to be a communication-efficient distributed learning framework for the Muon family of optimizers. A key insight is to reformulate the LMO update using the sharp operator which recasts the update as a normalized steepest descent step. As Muon is steadily gaining popularity, this area of research is particularly timely and well-placed. Additionally, it is important to provide a principles framework for Muon, including in the setting of distributed training which suffers from communication inefficiencies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The topic is particularly timely, and addresses the constraints of distributed training which is not typically encountered in centralized settings. \n\n2. The experiment setup is quite relevant. I believe that NanoGPT on FineWeb is a good choice for optimizer evaluation in the context of model LMs."}, "weaknesses": {"value": "1. Not much of a weakness, but the authors argue that due to increasing size, all training is distributed. This is true, but most frameworks tend to be parallelism (pipeline, model, etc) rather than the FL-type. \n\n2. Experiments are conducted on a single model/scale (NanoGPT 124M). Demonstrating the effectiveness of EF21-Muon on other domains (e.g., ViT) or at a larger scale would make the claims of general applicability significantly more robust."}, "questions": {"value": "Please see weaknesses. Not much questions to add."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tqEvCp7KLo", "forum": "rex7s82Iav", "replyto": "rex7s82Iav", "signatures": ["ICLR.cc/2026/Conference/Submission16605/Reviewer_VseT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16605/Reviewer_VseT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760656515001, "cdate": 1760656515001, "tmdate": 1762926678731, "mdate": 1762926678731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents EF21-Muon, a communication-efficient distributed optimizer based on recent LMO optimizers with rigorous convergence guarantees. This method achieves up to 7x communications savings compared to uncompressed baselines.\n\nFurthermore, a few notable insights include:\n1. Bidirectional compression with error feedback\n2. Support for non-Euclidean geometries through arbitrary norm choices\n3. Layer-wise treatment of neural network parameters\n4. Theoretical guarantees under many settings"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Significant theoretical contribution, rigorous convergence analysis for compressed, distributed LMO-based methods in non-Euclidean settings.\n2. Addresses a real bottleneck in the distributed training of large models, where Muon is gaining popularity.\n3. Comprehensive theory, a very long appendix with many proofs."}, "weaknesses": {"value": "1. Only one model size was trained, and it is very small by modern standards (120M).\n2. Only one evaluation (Loss) was used. It's good practice to include a few downstream evaluations just in case for bold claims.\n3. Compression overhead is not clearly addressed, topk requires transmitting indices and not thoroughly analyzed for the distributed setting with varying model architectures. This is especially important in optimization as a general-purpose optimizer should work on different model shapes.\n4. Very heavy and dense notation, difficult to read.\n5. The algorithm itself is not especially novel, error feedback comes from EF21."}, "questions": {"value": "1. How does this method scale when applied to bigger models? How does wall-clock time scale as you increase the model size? Is it a constant factor increase or is it quadratic, log, etc?\n2. How does EF21-Muon compare to other compressed distributed training variants of AdamW or SGD?\n3. The method proposed has a lot of hyperparameters, how sensitive is the training to choices beyond the limited ablations shown?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NDDMNgJV5j", "forum": "rex7s82Iav", "replyto": "rex7s82Iav", "signatures": ["ICLR.cc/2026/Conference/Submission16605/Reviewer_PPbv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16605/Reviewer_PPbv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951237909, "cdate": 1761951237909, "tmdate": 1762926677851, "mdate": 1762926677851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies a distributed non-Euclidean LMO-based optimization method that generalizes and recovers Muon and Scion in the non-compressed regime. Specifically, the authors introduce EF21-Muon, a unified and communication-efficient algorithm that incorporates stochastic gradients, momentum, and bidirectional compression with error feedback, while encompassing several existing compressed methods as special cases. Furthermore, the authors present comprehensive convergence guarantees for multiple settings, including both deterministic and stochastic cases, as well as for non-Euclidean smooth and generalized non-Euclidean smooth functions, under both layer-wise and joint parameter treatments. Experimental results on nanoGPT under various worker compressors show significant communication savings for the proposed algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work provides rigorous and comprehensive convergence guarantees for a wide range of settings.\n- It supports bidirectional compression for the smooth regime.\n- The proposed algorithm further supports non-Euclidean contractive compressors, thus enhancing generality.\n- The analysis of the algorithms is conducted in non-Euclidean norms.\n- The proposed algorithms achieve the state-of-the-art convergence rates.\n- The paper is clearly structured and easy to follow.\n- The discussion of the contribution of each term in the convergence guarantees is insightful."}, "weaknesses": {"value": "- The results in the generalized smoothness regime do not include primal compression.\n- The experimental results could additionally include some other baseline algorithms for comparison to better contextualize performance of EF21-Muon.\n- It may also have been beneficial to include wall-clock time experiments."}, "questions": {"value": "- Can the authors elaborate a bit on why in the generalized smooth setup the convergence guarantees are established without primal compression? Can these results potentially be extended to that setting as well?\n- Could the authors clarify how the combinations of compressors are applied in the experimental setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ornjludw5w", "forum": "rex7s82Iav", "replyto": "rex7s82Iav", "signatures": ["ICLR.cc/2026/Conference/Submission16605/Reviewer_NH9Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16605/Reviewer_NH9Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984012263, "cdate": 1761984012263, "tmdate": 1762926677406, "mdate": 1762926677406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a distributed optimizer that brings error‑feedback (EF) to layer‑wise linear minimization oracle (LMO) methods such as Muon/Scion/Gluon, extending EF beyond the Euclidean setting and enabling bidirectional compression with support for stochastic gradients and momentum."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper establishes non‑Euclidean EF convergence for LMO‑based methods, and shows how rates recover Euclidean EF21 results when specialized, which is a nontrivial generalization.\n\nOn NanoGPT-124M trained on FineWeb, they report up to 7x reduction in worker‑to‑server communication without loss of accuracy."}, "weaknesses": {"value": "The novelty is rather incremental since EF21-P introduces bidirectional EF with momentum/stochasticity, Gluon introduces non‑Euclidean layer‑wise LMO analyses, and Dion introduces distributed Muon‑style optimizers.\n\nExperiments show that compression reduces uplink bytes but degrades token efficiency and final loss at a fixed 5B budget, so the claim of \"no accuracy degradation\" is unsupported under equal‑budget comparisons.\n\nThe convergence results assume exact LMO steps, yet the implementation uses inexact Newton–Schulz updates. Recent literature emphasizes that this gap matters, but the paper neither analyzes nor bounds the approximation error."}, "questions": {"value": "Results are on NanoGPT‑124M, but how would the conclusions change for billion‑parameter LLMs and larger clusters?\n\nThe main results and plots focus on w2s, but how is the total communication cost including s2w, and what are the gains from s2w compression?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cD5gMNWCeE", "forum": "rex7s82Iav", "replyto": "rex7s82Iav", "signatures": ["ICLR.cc/2026/Conference/Submission16605/Reviewer_y1YB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16605/Reviewer_y1YB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179016307, "cdate": 1762179016307, "tmdate": 1762926676769, "mdate": 1762926676769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}