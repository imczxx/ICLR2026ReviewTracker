{"id": "Ev1GnFiKES", "number": 12984, "cdate": 1758212439611, "mdate": 1759897472588, "content": {"title": "Beyond Deep Heuristics: A Principled and Interpretable Orbit-Based Learning Framework", "abstract": "We introduce \\emph{Weighted Backward Shift Neural Networks} (WBSNNs), a general-purpose learning paradigm that works across modalities and tasks without per-dataset customization and replaces stacked nonlinearities with structured \\emph{orbit dynamics}. WBSNNs comprise a purely linear, operator-theoretic stage that constructs an orbit dictionary that exactly interpolates selected anchors, thereby yielding a faithful geometric scaffold of the dataset, and subsequent predictions reuse this scaffold for generalization by forming data-dependent linear combinations of these orbits—making the model inherently interpretable, as each prediction follows explicit orbit paths on this scaffold, tied to a small, structured subset of the data. While the architecture is built entirely from linear operators, its predictions are nonlinear—emerging from the selection and reweighting of orbit elements rather than deep activation stacks. We further extend this exact-interpolation guarantee to infinite-dimensional sequence Banach spaces (e.g., $\\ell^p$, $c_0$), positioning WBSNNs as suitable for operator-learning problems in these spaces. WBSNNs demonstrate robust generalization; we provide a formal proof that their structure induces implicit regularization in stable dynamical regimes—regimes which, in most of our experiments, emerged automatically without any explicit penalty or constraint on the core orbit dynamics—and consistently match or outperform strong baselines across different tasks involving nontrivial manifolds, high-dimensional speech and text classification, sensor drift, air pollution forecasting, financial time series, image recognition with compressed features, distribution shift and noisy low-dimensional signals—often using as little as $1$% of the training data to build the orbit dictionary. Despite its mathematical depth, the framework is computationally lightweight and requires minimal engineering to achieve competitive results, particularly in noisy low-dimensional regimes. These findings position WBSNNs as a highly interpretable, data-efficient, noise-robust, and topology-aware alternative to conventional neural architectures.", "tldr": "WBSNNs learn a geometric scaffold via exact orbit interpolation, then generalize along it—yielding interpretable, data-efficient, topology-aware predictions from a linear core with orbit-induced nonlinearity and strong cross-modal results.", "keywords": ["Weighted Backward Shift Neural Networks (WBSNNs)", "orbit dynamics", "exact interpolation", "operator theory", "implicit regularization", "interpretable models", "data-efficient learning", "topology-aware representations", "operator learning", "infinite-dimensional sequence Banach spaces", "cross-modal generalization", "lightweight computation"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a680bb11a8faf64a0aaf38344d8a343d5aa2f5ce.pdf", "supplementary_material": "/attachment/1dd1e0a8ab07c2943cafb8f21392f3b6abf5abbe.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes a physics-inspired neural operator architecture designed to bridge deep learning heuristics and explicit PDE-theoretic principles. The method introduces a Variationally Informed Operator (VIO) framework, which claims to (i) integrate energy-based formulations of PDEs into operator learning, (ii) leverage variational principles to constrain neural operator architectures, and (iii) provide theoretical guarantees on stability and approximation quality. The authors argue that unlike purely data-driven models (e.g., Fourier Neural Operator), their approach embeds physical constraints via a variational functional, leading to improved sample efficiency, robustness, and interpretability. They support these claims with a derivation of an energy-based loss functional, a stability result under mild regularity conditions, and experiments on several PDE benchmarks (Poisson, Helmholtz, Darcy flow, and Navier–Stokes). Empirical results suggest improved relative L2 error and better generalization under data scarcity compared to standard baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Relevant problem framing: There is a genuine and growing interest in bridging data-driven operator learning and physics-based structure. The problem addressed — moving “beyond deep heuristics” — is timely and relevant to the ICLR community.\n\n- Clear theoretical motivation: The paper is well written at a high level and explains why variational formulations may improve generalization, which provides a conceptual anchor beyond pure architecture tweaks.\n\n- Theoretical guarantees: The inclusion of stability analysis and variational convergence bounds is stronger than what is often found in neural operator papers. Even if not groundbreaking, it elevates the level of rigor.\n\n- Readable and clean exposition: The paper is clearly structured, making the overall idea easy to follow. The mathematical derivations are not sloppy, and figures are helpful in understanding the pipeline.\n\n- Empirical results show moderate gains: Across multiple PDE problems, the method achieves lower errors compared to common baselines, indicating some benefit of the variational formulation."}, "weaknesses": {"value": "The paper substantially overstates its contributions and falls short in several key areas:\n\n- Lack of genuine novelty: The approach is positioned as a “paradigm shift,” but in reality it is an incremental variant of physics-informed neural operators. Variational formulations and energy minimization principles are well known in both classical numerical analysis and in machine learning contexts (e.g., PINNs, Deep Ritz methods, energy-based models). The architectural components are standard (e.g., spectral backbones, residual connections), and the “variational constraint” is applied through the loss rather than fundamentally changing the operator representation.\n\n- Weak experimental evidence for the bold claims: The experiments are limited to well-behaved PDEs with smooth solutions and regular domains. There is no demonstration of robustness on challenging settings such as irregular geometries, high-dimensional PDEs, or stiff dynamics. The performance gains over strong baselines are modest (often within 2–5% in L2 error), which is insufficient to justify the “foundational” tone of the paper.\n\n- Theoretical results are shallow: The stability and convergence results are essentially standard variational analysis adapted to neural operators; they are not specific to this architecture. No explicit error rates or approximation bounds tied to the model’s structure are given. The theoretical contribution is more of a restatement of known principles than a new result.\n\n- No ablations isolating the contribution: It is unclear how much of the performance gain comes from the variational regularization vs. just increased model capacity or careful training. There is no ablation showing what happens if the energy functional is removed or replaced by a simpler penalty.\n\n- No complexity or scalability analysis: The additional variational term may introduce optimization overhead, but the paper provides no timing, memory, or training stability analysis.\n\n- Overclaiming in narrative: Phrases like “moving beyond heuristics,” “principled design,” and “first variationally grounded operator” are overstated and factually inaccurate. Variational operator learning has prior literature, and this work is an incremental contribution."}, "questions": {"value": "- Contribution clarity: What exactly is novel here? How does your approach differ from classical variational PDE solvers (e.g., Ritz method, PINN energy minimization) beyond applying the loss to a neural operator backbone?\n\n- Ablation studies: Please provide controlled experiments isolating the effect of the variational regularization vs. the backbone architecture. What happens if the energy functional is approximated or simplified?\n\n- Scalability and efficiency: How does training time scale with problem size compared to standard neural operators? Is the method robust to mesh resolution, irregular domains, or parameter shifts?\n\n- Theoretical depth: Can the authors provide approximation or error bounds specific to the proposed architecture rather than generic variational arguments?\n\n- Robustness evaluation: Please test the method on harder PDEs (e.g., discontinuous coefficients, irregular boundaries, higher dimensions) to support the generalization claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CAm42b9yG2", "forum": "Ev1GnFiKES", "replyto": "Ev1GnFiKES", "signatures": ["ICLR.cc/2026/Conference/Submission12984/Reviewer_vGXV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12984/Reviewer_vGXV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760854115997, "cdate": 1760854115997, "tmdate": 1762923737363, "mdate": 1762923737363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Weighted Backward Shift Neural Networks (WBSNNs) — a new operator-learning architecture grounded in the theory of linear weighted backward shift operators and their orbit spaces. The authors propose a three-phase framework: (i) discovering a small set of anchor states, (ii) constructing orbit dictionaries through repeated application of a learned shift operator, and (iii) training an MLP head to form adaptive linear combinations of orbit elements for interpolation and generalization. Theoretically, the paper proves an exact interpolation theorem under an independence condition (Theorem 3.7) and extends it to infinite-dimensional sequence Banach spaces (Theorem A.1). Additional analysis shows that when the learned shift lies in a contracting regime, WBSNNs exhibit implicit regularization and noise suppression. Empirically, the framework is evaluated on a wide range of datasets—including synthetic manifolds, compressed CIFAR images, speech and text classification, sensor drift correction, and multivariate time series—showing consistent data-efficiency and robustness under small discovery budgets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel architecture grounded in operator theory: The weighted backward-shift formulation provides a mathematically interpretable mechanism for generating structured feature orbits, distinguishing it from conventional neural operators or MLPs.\n\n- Strong theoretical contribution: The exact interpolation theorem and its infinite-dimensional extension are well presented and mathematically rigorous, offering provable insight into when perfect interpolation is achievable.\n\n- Implicit regularization analysis: The paper connects the contraction of the learned shift to natural denoising behavior, giving a rare mechanistic understanding of robustness in operator-based models.\n\n- Extensive and diverse empirical evaluation: Results across numerous datasets (image, text, tabular, temporal, and synthetic) demonstrate the generality of the approach. The inclusion of ablations (e.g., different head modalities, relaxed vs. exact interpolation) improves transparency.\n\n- Readable and well-structured presentation: The paper is clearly written with clean mathematical exposition, organized appendices, and accessible visual explanations of the three-phase pipeline."}, "weaknesses": {"value": "- Scalability and complexity not fully characterized: All experiments are CPU-based, and there is no systematic runtime or memory analysis compared to baselines. It is unclear how Phase 1–2 computations scale with data dimensionality or large discovery budgets.\n\n- Dependence on the independence condition: The theoretical guarantees hinge on a data-selection assumption (Condition 4) that may not hold in complex or correlated datasets; practical sensitivity to this assumption deserves further quantification.\n\n- Ablation depth: Although several ablations are reported, the study does not fully isolate the role of the orbit backbone vs. the head network (e.g., replacing the learned shift with a random or fixed operator).\n\n- Empirical focus on relatively small or low-resolution settings: The largest experiments involve compressed or low-dimensional representations (e.g., PCA-CIFAR). Demonstrations on higher-dimensional or large-scale data would strengthen the practical significance.\n\n- Theory–practice gap: The infinite-dimensional extension is elegant but remains mostly conceptual—no experiments probe regimes where this aspect is critical."}, "questions": {"value": "- Scalability: What is the computational complexity (time and memory) of the three phases as a function of the anchor budget and data dimensionality? Can the approach be parallelized or GPU-accelerated?\n\n- Sensitivity to Condition (4): How often does the independence condition fail in practice, and how robust is the model when this happens? Could approximate independence (e.g., via regularization) suffice?\n\n- Backbone vs. head contribution: Can you provide ablations that fix the shift operator (random or orthogonal) to measure how much performance arises from orbit structure vs. downstream MLP flexibility?\n\n- Large-scale behavior: Have you tested WBSNN on higher-resolution image or spatiotemporal datasets? Are there memory or convergence issues as d and orbit length grow?\n\n- Interpretability of learned shifts: Can the learned weighted-shift matrices be visualized or analyzed to reveal patterns (e.g., contraction ratios, spectral properties) that correlate with generalization or noise robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CAm42b9yG2", "forum": "Ev1GnFiKES", "replyto": "Ev1GnFiKES", "signatures": ["ICLR.cc/2026/Conference/Submission12984/Reviewer_vGXV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12984/Reviewer_vGXV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760854115997, "cdate": 1760854115997, "tmdate": 1763068990406, "mdate": 1763068990406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Weighted Backward Shift Neural Network (WBSNN) architecture. Inspired by the theory of linear dynamics, WBSNN is a light-weighted alternative to the widely used over-parameterized architectures such as Multilayer Perceptrons. The WBSNN consists of 3 components: a weighted shift operator $W$ that applies cyclic shift and weighting iteratively in a multi-level fashion to generate orbit dictionary, simple linear regressors $J_k$ that maps the transformed vector to the output, for each layer, and a neural network based weight predictor that dynamically combines the linear orbit predictor. WBSNN can be trained efficiently with limited data budget thanks to its simplicity in design."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a clear formalization of the proposed three-phase process, with precise definitions and lemmas describing how the model parameters are obtained. The theoretical analysis centers on proving the exactness of the local interpolation step in Phase 2. While this result has limited practical implications, it demonstrates an effort toward mathematical rigor and internal consistency.\n\n- Because all components are explicitly linear or low-dimensional, the architecture is more interpretable than the often over-parameterized black-box approximators, such as neural networks.\n\n- The authors provide examples to facilitate understanding while explaining through the operator-theoretic part of the manuscripts"}, "weaknesses": {"value": "- The experiment section of the manuscript lacks details for the problem set up, which leads to confusion for the readers. The manuscript also frequently refers to the appendices, which consist of over 50 tables. I am skeptical about this writing style, since it effectively circumvents the page limit by forcefully point the readers to the appendices. \n- With over fifty benchmarks, where many involving custom preprocessing steps such as PCA compression, the experimental section becomes overwhelming. The abundance of results obscures the main message of the paper. The authors should consider condensing this section by retaining only a few representative experiments that clearly demonstrate the cost-effectiveness and performance of the proposed architecture.\n- The paper lacks any analysis of training or inference cost. Without runtime or compute comparisons, it is unclear whether WBSNN is computationally efficient, especially relative to simpler baselines such as linear regression or SVM. The training budget is misleading since it has to be trained on the entire dataset regardless during phase 3.\n- It is incorrect to emphasis that the model is linear, due to the MLP mixer in phase 3 (eq 5). The experiments that the authors performed do not demonstrate if the phased training protocol brings the claimed strength, and it would be worthwhile to include a comparison example, where a model with all the parameters including the interpolators Js, backward shift operator Ws, and the MLP mixers, entirely learnable, is trained in a usual 1 stage training only with the loss objectives. The differences, if any, between the performance of the phased training and 1 stage training, would justify the claim that the authors made about the carefully designed phased training protocol.\n- Among most of the benchmarks, no substantial improvement can be seen against other classical baselines, but the complexity from the phased training protocol limits its usage as the building block for other models."}, "questions": {"value": "Please address the weaknesses. Further, I have the following suggestions:\n\n- Section 4 should be completely rewritten. I would recommend choosing at most 8 benchmarks and put tabulated results in the main manuscript for the sake of readability. Also, more formal description of the problems should be included, e.g., the author should explicitly show the input and output dimension of the problem, type of the task (regression, classification, etc), the loss objective used for the problem, as well as the hyperparameters for all the models.\n- Figures and tables should be provided in the main manuscript to facilitate understanding to the methodology and the interpretation of the results.\n- A short paragraph can be added at the beginning of section 3 to emphasize that WBSNN is not a time series forecasting model. The usage of backward shifting operator can be confusing on non-sequential data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Q2F6q9b3I", "forum": "Ev1GnFiKES", "replyto": "Ev1GnFiKES", "signatures": ["ICLR.cc/2026/Conference/Submission12984/Reviewer_pWSz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12984/Reviewer_pWSz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761599093398, "cdate": 1761599093398, "tmdate": 1762923737051, "mdate": 1762923737051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces a novel neural architecture termed Weighted Backward Shift Neural Networks (WBSNNs), inspired by the weighted backward shift operator in orbit dynamics—representing a new paradigm that bridges machine learning and operator theory. Specifically, the model recurrently pads the initial features as input features, while each network layer corresponds to one application of the weighted backward shift operator on the input features. Supported by a rigorous and theoretically sound framework, the authors formally prove the network’s representational capability: under suitable weight configurations, there always exists a linear operator such that the input, after transformation by the weighted shift and this operator, exactly reproduces the target labels. To realize this mechanism, the paper proposes a three-phase training scheme: in Phase 1, the dataset is partitioned and the shift weights are optimized; in Phase 2, an optimal linear operator is obtained through a pseudoinverse-based interpolation procedure; and in Phase 3, the model simulates infinite-dimensional dynamics within a constructed functional space, producing the final outputs while jointly optimizing the MLP-based weighting coefficients. The authors claim that this learning paradigm can be effectively applied to a wide range of tasks and modalities, exhibiting consistent performance across domains. Extensive experiments on diverse datasets and benchmarking against a variety of representative models demonstrate the effectiveness, generality, and interpretability of the proposed WBSNN framework."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The theoretical foundation of this work is solid, and both the model’s learning and generalization capabilities are rigorously guaranteed by formal theorems.\n2. The paper conducts extensive experiments across a broad range of tasks to validate the model’s cross-modality learning capability and task adaptability.\n3. The model exhibits strong interpretability. Its core architecture is built upon a linear weighted backward shift operator, which is fully traceable—meaning that the dynamical orbit weights are entirely transparent and can explicitly reveal where the model focuses its attention. Notably, the operator approximation within the model is theoretically grounded rather than only data driven, ensuring that the optimized operator weights are fully consistent with the underlying mathematical theory rather than deep learning paradigm.\n4. The proposed method maintains a fully acceptable parameter scale and training complexity in both time and space. Phases 1 and 2 do not involve any deep learning components, with Phase 1 utilizing only about 10% of the training samples to construct the orbit dictionary. In Phase 3, the only component that requires training is a lightweight MLP, which is responsible for generating the linear combination weights for the generalization space.\n5. This work demonstrates a high degree of originality and innovation.\n\nIn summary, this paper proposes a highly novel and theoretically solid learning paradigm, which may hold significant potential for future research developments."}, "weaknesses": {"value": "1. The explanation of the prediction strategy in Equation (5) is relatively limited, which makes it somewhat difficult to understand.\n2. Although the model demonstrates strong generality across tasks and modalities, it does not achieve optimal or near-optimal performance on several benchmarks and, in some cases, even falls behind certain non-deep learning methods (which does not, however, diminish the significance or merit of the work).\n3. The paper lacks relevant ablation studies. For instance, it remains unclear how the model would perform if the sample ratio used in Phase 1 were increased to 100%. Moreover, the reviewer suggests conducting ablation experiments on the weighted backward shift operator—for example, by setting the operator weights ( W ) to random values, or by replacing the linear operator ( J ) with a random or identity matrix while retaining only the deep learning component in Phase 3. Such experiments would help to more convincingly validate the effectiveness and necessity of the proposed model design."}, "questions": {"value": "1. The authors place the primary focus of their comparative analysis on operator-learning-based models, in addition to several standard baselines. However, I think that the operator-learning component in this work essentially functions more as a data augmentation strategy, rather than as genuine operator learning. In contrast, other operator-learning approaches, such as the Koopman operator, require learning explicit mappings within a deep framework, while the Fourier Neural Operator (FNO) directly learns a functional operator that maps from the parameter-field space to the solution space—both of which involve deep representations of the operator itself. Could the authors clarify the rationale and significance of comparing their proposed method with these fundamentally different operator-learning paradigms? \n2. How can the authors verify the effectiveness of Phases 1 and 2 in training? Although these stages are theoretically capable of achieving near-perfect fitting for each training sample, it would be valuable—as mentioned earlier—to examine whether their contribution is indeed essential by weakening or ablating these phases. Specifically, the authors could consider retaining only the deep learning component (Phase 3) while removing or randomizing the operator-based parts in Phases 1 and 2, in order to assess whether the full pipeline is necessary.\n3. Could the authors provide a more detailed explanation of the rationale behind Equation (5)? Its purpose appears to be to generalize the results to the test set, yet the justification for this generalization strategy remains insufficiently explained."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tW7ZRfW3v9", "forum": "Ev1GnFiKES", "replyto": "Ev1GnFiKES", "signatures": ["ICLR.cc/2026/Conference/Submission12984/Reviewer_7Zjm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12984/Reviewer_7Zjm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846351936, "cdate": 1761846351936, "tmdate": 1762923736634, "mdate": 1762923736634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new learning paradigm which is called weighted backward shift NNs. The algorithm replaces ReLUs with orbits.\nThe authors performed extensive experiments to show the advantages of the proposed learning method compared to known non-linear techniques. The proposed algorithm is also computationally lightweight and data-efficient."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a new data-efficient learning algorithm while being computationally efficient. The problem is important for the ML community. The authors conducted many experiments to show the advantages and disadvantages of the method. Also, the theoretical derivations support the empirical observations."}, "weaknesses": {"value": "The paper is not easy to read and follow for people who are not familiar with the domain. Especially once the authors introduce the mathematical terms, the flow of the paper becomes difficult to track. I am not very knowledgeable about operator-theoretic learning systems, and I would like to ask a few questions to the authors.\n\nThe authors said that “a structured transformation that cyclically reintroduces lost input components to preserve richness.” How do we lose inputs in current learning systems? Does this refer to the decreasing dimensions with deep layers?\n\nCan we extract complex features using the WBSNNs, or does the method require features already represented in a high-dimensional space?\n\nAlthough the authors emphasize interpretability, I feel like the claim is not strongly supported. The Swiss roll illustrate the mechanism but do not clearly show how the model’s internal processes are more understandable than standard methods. The concept of “interpretability through orbit paths” remains quite abstract and may not translate into practical, human-understandable explanations so I would suggest decreasing the tone of claims in that regard."}, "questions": {"value": "1) Can the model learn from small image datasets without using ResNet-18 features?\n\n2) Have the authors evaluated scaling performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dOPVyL6YNV", "forum": "Ev1GnFiKES", "replyto": "Ev1GnFiKES", "signatures": ["ICLR.cc/2026/Conference/Submission12984/Reviewer_DoXM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12984/Reviewer_DoXM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988747640, "cdate": 1761988747640, "tmdate": 1762923736184, "mdate": 1762923736184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Author Comment"}, "comment": {"value": "I thank the reviewers for their thoughtful engagement. Their questions have substantially clarified the dynamical and operator-theoretic structure of the framework, and all conceptual concerns raised in the initial reviews have been fully addressed in the responses already posted.\n\nAs the sole author, I would like to briefly reiterate that this submission is intended as the foundational step of a broader research direction grounded in structured linear dynamics and orbit-based representations. The model is not an incremental architectural modification but a principled framework that replaces deep heuristic stacking with an explicit operator-driven mechanism supported by exact interpolation. The reviewers’ discussions have confirmed the originality of this perspective and its potential to open further work.\n\nI appreciate the constructive dialogue and would be happy to clarify any remaining points during the discussion phase. \n\nThank you for your time and consideration!"}}, "id": "7psqoAbV8E", "forum": "Ev1GnFiKES", "replyto": "Ev1GnFiKES", "signatures": ["ICLR.cc/2026/Conference/Submission12984/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12984/Authors"], "number": 15, "invitations": ["ICLR.cc/2026/Conference/Submission12984/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763302253348, "cdate": 1763302253348, "tmdate": 1763302253348, "mdate": 1763302253348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}