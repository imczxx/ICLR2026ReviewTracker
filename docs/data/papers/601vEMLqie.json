{"id": "601vEMLqie", "number": 16948, "cdate": 1758270502517, "mdate": 1759897208489, "content": {"title": "DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion", "abstract": "Diffusion Transformer models can generate images with remarkable fidelity and detail, yet training them at ultra-high resolutions remains extremely costly due to the self-attention mechanism's quadratic scaling with the number of image tokens. In this paper, we introduce Dynamic Position Extrapolation (DyPE), a novel, training-free method that enables pre-trained diffusion transformers to synthesize images at resolutions far beyond their training data, with no additional sampling cost. DyPE takes advantage of the spectral progression inherent to the diffusion process, where low-frequency structures converge early, while high-frequencies take more steps to resolve. Specifically, DyPE dynamically adjusts the model's positional encoding at each diffusion step, matching their frequency spectrum with the current stage of the generative process. This approach allows us to generate images at resolutions that exceed the training resolution dramatically, e.g., 16 million pixels using FLUX. On multiple benchmarks, DyPE consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation, with gains becoming even more pronounced at higher resolutions.", "tldr": "DyPE lets pre-trained diffusion transformers generate ultra-high-res images (16M+ px) without retraining or extra cost, by matching positional encoding extrapolation to diffusion’s shift from low-freq structures to high-freq details.", "keywords": ["Diffusion", "Text to Image", "High Resolution"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6924a47c7e3eb6a527db5a8674b2c632065d98f3.pdf", "supplementary_material": "/attachment/0591b1bd2b77d5738554224028761b350c59cb83.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method for higher-resolution image synthesis. Specifically, it proposes to adjust the positional encoding along with the sampling progress to better fit the coarse-to-fine generation of transformer-based diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. adjusting positional encoding along the sampling process seems non-trivial"}, "weaknesses": {"value": "Missing comparison with existing works, leading to uncomprehensive evaluation and misleading conclusions.\n\n1. For example, FreCaS (ICLR2025), I-Max (arxiv, 2024), HiFlow(arxiv202504, NIPS2025), Diffusion-4K(CVPR2025) are four methods that generate higher-resolution images beyond training sizes on sd3 or flux models. And they are also open to the public far before the ICLR deadline (2025.09.25). Thus, missing comparison with those related works makes the conclusion misleading and unsound.\n\n2. Even if there indeed lacks methods that are suitable for transform-based generation, comparing with modified versions of the method for unet, such as demofusion or hidiffusion, is essential to provide a comprehensive comparison, as previous methods (HiFlow) did.\n\n3. Comparison with \"diffusion+SR\" should be provided since this simple framework can be efficient and also generate higher-resolution images.\n\n4. Missing path-based metrics. Since CLIP and FID would resize the image first before evaluating, adopting a patch-based version of those metrics is essential for evaluating images of higher resolution (such as 2K or 4K) as prior works did."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HleaYynpxt", "forum": "601vEMLqie", "replyto": "601vEMLqie", "signatures": ["ICLR.cc/2026/Conference/Submission16948/Reviewer_yBAr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16948/Reviewer_yBAr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842417960, "cdate": 1761842417960, "tmdate": 1762926968783, "mdate": 1762926968783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DyPE (Dynamic Position Extrapolation), a training-free method that enables pre-trained diffusion transformers (DiTs) to generate ultra-high-resolution images (up to 16M+ pixels) without retraining or additional inference cost. Unlike static positional extrapolation methods commonly used in language models, DyPE dynamically adjusts positional encodings across diffusion timesteps to match the spectral evolution of the denoising process—progressing from low-frequency structures to high-frequency details. The method consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The paper clearly identifies the gap between static positional extrapolation and the dynamic frequency progression required for high-resolution image generation.\n\n(2) DyPE extends existing positional encoding formulations with a time-dependent scaling term κ(t) that smoothly transitions from large scaling at early timesteps to unity at later ones, making it easy to integrate into existing extrapolation methods.\n\n(3) The method demonstrates consistent quantitative and qualitative improvements across multiple datasets and baselines.\n\n(4) The manuscript is well-organized, clear, and easy to follow."}, "weaknesses": {"value": "(1) Baseline comparison: Lumina-Next already incorporates timestep dynamics by interpolating from PI to NTK-aware scaling as denoising progresses. It would be important to include Lumina-Next as a baseline in Tables 1, 2, and 3 for a fair and comprehensive comparison.\n\n(2) Limited evaluation scope: The experiments focus primarily on FLUX and FiTv2 for image generation. Demonstrating results on additional tasks, such as image editing (e.g., EasyControl [1]) or video generation (e.g., Wan 2.1/2.2 [2]), would help validate the generalization ability of DyPE across broader diffusion-based applications.\n\nMinor Weaknesses:\n\n Some relevant prior works are missing in the related work section, including:\n\n (1) FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion (CVPR 2025)\n\n (2) FreeU: Free Lunch in Diffusion U-Net (CVPR 2024)\n (3) RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers (ICML 2025)\n\nReferences:\n\n [1] EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer (ICCV 2025)\n\n [2] Wan: Open and Advanced Large-Scale Video Generative Models (arXiv)"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dRfbbetL7t", "forum": "601vEMLqie", "replyto": "601vEMLqie", "signatures": ["ICLR.cc/2026/Conference/Submission16948/Reviewer_P3ke"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16948/Reviewer_P3ke"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929815112, "cdate": 1761929815112, "tmdate": 1762926968246, "mdate": 1762926968246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DYPE, a method for enabling a pre-trained diffusion model (DIT) to synthesize high resolution images beyond training resolution that possesses several favourable advantages, specifically being training-free and without introducing additional sampling cost. The main novelty lies in the observation that in diffusion models low-frequencies structures converge early, while high-frequencies require more steps. Based on this observation, the authors propose to adjust the positional encoding at each diffusion step to match their frequency spectrum with the current stage of the generative process. They show SOTA results on top of pre-trained FLUX allowing it to generate ultra-high resolution images in a training-free manner"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clear methodological contribution: the paper makes an important observation (higher frequency components show a fairly constant evolution while lower frequencies appear to cease to evolve early) that is validated empirically and based on that the authors propose a method that dynamically adjusts the model's positional encoding and can be implemented on top of existing methods\n- Clear positioning of the paper's contribution with respect to previous related work. Great exposition of previous work making the paper's contribution clear\n- The writing is clear and the paper reads very well. \n- Experimental validation is sufficient and multiple experiments on well established benchmarks are presented. Clearly, the presented method has a positive impact in all cases. A human study validates the effectiveness of the proposed formulation\n- The appendix provides a significant amount of further experiments and ablation studies that further solidify the paper's contributions and impact"}, "weaknesses": {"value": "No major weaknesses identified. A minor one is whether the authors could include results for other diffusion models (besides FiTv2)."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "poxGFRMuqR", "forum": "601vEMLqie", "replyto": "601vEMLqie", "signatures": ["ICLR.cc/2026/Conference/Submission16948/Reviewer_Z8u2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16948/Reviewer_Z8u2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940622442, "cdate": 1761940622442, "tmdate": 1762926967688, "mdate": 1762926967688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}