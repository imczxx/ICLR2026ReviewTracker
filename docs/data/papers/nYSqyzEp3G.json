{"id": "nYSqyzEp3G", "number": 20776, "cdate": 1758309955912, "mdate": 1759896959138, "content": {"title": "Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic", "abstract": "Large language models often require costly optimization, such as reinforcement learning, to master complex reasoning tasks. This work demonstrates that reasoning ability, once learned, can be extracted and transferred between models as a compact \\emph{task vector}. We source two publicly available, identically initialized Qwen2.5 models: one fine-tuned with supervised fine-tuning (SFT) and the other with group relative policy optimization (GRPO) on the same dataset. From these, we extract a reasoning vector: \n$$\nv_{\\text{reason}} = \\theta_{\\text{GRPO}} - \\theta_{\\text{SFT}}.\n$$\nWe hypothesize that this vector captures the reasoning capability instilled by reinforcement learning while factoring out shared knowledge from the SFT process. When added to compatible instruction-tuned models through simple arithmetic, this vector consistently improves performance across diverse reasoning benchmarks: GSM8K (+4.9\\%), HumanEval (+4.3\\%), SciQ (+1.7\\%), and BigBenchHard (+12.3\\% for the 1.5B model). The performance improvements persist under adversarial conditions. Conversely, subtracting the vector causes significant performance degradation ($-11.8\\%$ on GSM8K), demonstrating the vector's strong contribution to the model's reasoning abilities. \n\nThis work shows how reasoning capabilities, typically developed through expensive training, can be extracted from existing open-source models and reused through simple tensor arithmetic, offering a practical way to enhance models by recycling prior computational investments.", "tldr": "Reasoning capabilities from reinforcement learning can be extracted as a task vector and transferred to other models to improve performance on diverse benchmarks.", "keywords": ["Large Language Models", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f4bab1b8a040f47ea03abd62dacc49ead16be35.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces reasoning vectors, derived by taking the parameter difference between a GRPO-trained model and its SFT counterpart, and applying this offset to other compatible models. The approach enables the transfer of reasoning capabilities without additional training. Experiments on GSM8K, HumanEval, SciQ, and BigBenchHard demonstrate consistent improvements, with some cases even surpassing the original GRPO model. The authors attribute these gains to linear mode connectivity between the SFT and GRPO models in parameter space."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple, computationally efficient idea requiring no retraining.\n2. Consistent improvement across several reasoning benchmarks.\n3. Potentially impactful if the method generalizes across architectures."}, "weaknesses": {"value": "1. Missing donor SFT baseline: The paper does not report the performance of the donor SFT model that was used to compute the reasoning vector, making it hard to measure the actual gain from SFT to GRPO or to verify whether the reasoning vector truly encapsulates the reinforcement-learning-induced improvements.\n2. No training details for SFT or GRPO: The descriptions of both supervised fine-tuning and GRPO optimization are overly abstract, lacking crucial information such as reward formulation, rollout strategy, learning rates, or number of training steps, which prevents reproducibility and weakens methodological credibility.\n3. Figure 4 and Table 1 duplicate results: Figure 4 visually replicates the exact data already presented in Table 1 without introducing additional analysis or insights, suggesting that the space could have been used for more meaningful ablations or visual comparisons.\n4. The paper assumes that the difference between independently trained GRPO and SFT models—both initialized from the same base—captures the reasoning gain introduced by reinforcement learning. However, this setting is not equivalent to the standard sequential SFT to GRPO pipeline used in practice, where GRPO fine-tunes on top of SFT. Consequently, the extracted vector may not correspond to the true incremental improvements achieved through reinforcement learning, raising concerns about the practical validity and interpretability of the proposed reasoning vector."}, "questions": {"value": "1. Quantifying donor–target differences:\nThe reasoning vector is derived from a GSM8K-trained SFT model and applied to a separate instruction-tuned target (Qwen2.5-Instruct). Could the authors report or quantify the performance gap between these models to demonstrate that the transfer generalizes beyond closely aligned SFT configurations?\n2. Missing donor SFT baseline: The donor SFT model’s results are not reported, leaving it unclear how much GRPO improves over SFT or whether the extracted vector truly captures reinforcement-driven reasoning gains. Could the authors include this baseline to better support the claimed effectiveness?\n3. Transferability and attribution of gains: Have the authors tested whether the same reasoning vector remains effective when applied to base or differently fine-tuned models trained on other datasets? Such experiments would help determine whether the observed gains stem from the RL-derived vector itself or from synergy with the instruction-tuned target."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d6EWwfMOAs", "forum": "nYSqyzEp3G", "replyto": "nYSqyzEp3G", "signatures": ["ICLR.cc/2026/Conference/Submission20776/Reviewer_5e6S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20776/Reviewer_5e6S"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761138105388, "cdate": 1761138105388, "tmdate": 1762999983308, "mdate": 1762999983308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the task vector mechanism for reasoning enhancement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper demonstrates quality improvements using the proposed method."}, "weaknesses": {"value": "- I can't shake the feeling that the paper has very limited novelty. We all know that task vectors work and can be applied to extrapolate beyond some weights (such as the reasoning model in this paper). The idea that reasoning capabilities can be embedded into a compact vector is also not novel"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xuZmKpBWPB", "forum": "nYSqyzEp3G", "replyto": "nYSqyzEp3G", "signatures": ["ICLR.cc/2026/Conference/Submission20776/Reviewer_8JwL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20776/Reviewer_8JwL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560119700, "cdate": 1761560119700, "tmdate": 1762999982844, "mdate": 1762999982844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors show that there may be extracted a \"reasoning\" vector as the difference of weights of a reasoning and non-reasoning models. This vector may then be reused to power another compatible model, thus reducing the adaptation costs."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The paper is well written.\n* It addresses a timely topic: the inexpensive induction of reasoning capabilities and the interpretation of model capabilities."}, "weaknesses": {"value": "* The experimental setup should be substantially improved in both breadth and rigor — see the “Questions.”\n* The novelty beyond what is presented in [1] appears limited."}, "questions": {"value": "* I am interested in whether the conclusions transfer to other models (e.g., LLaMA 3.1). Evaluation only on the Qwen models is insufficient to justify your claim (see [2]).\n* GSM8K is an old and saturated benchmark. Please present results on datasets such as MATH500, AIME24/25, AMC23, Minerva-Math, and OlympiadBench (in that order of priority).\n* What are the scores for “Baseline + Think”?\n* In my experience, gains of 2.6 are within the noise range for math benchmarks. I would like to see a standard deviation reported for all results. I understand that you use greedy decoding, but please consider what you can do here.\n* How exactly do you design the perturbations? For example, what is meant by “extended numerical ranges and more reasoning steps”? Please also provide the details for the other perturbations.\n* Can you compare the “vector removal” with a random vector removal?\n\n[1] Ilharco et al. “Editing Models with Task Arithmetic”\n[2] Shao et al. “Spurious Rewards: Rethinking Training Signals in RLVR”"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Figure 2 is copied from Figure 1 in [1] with minimal changes, and I don’t see a reference."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2zJEHqB32m", "forum": "nYSqyzEp3G", "replyto": "nYSqyzEp3G", "signatures": ["ICLR.cc/2026/Conference/Submission20776/Reviewer_wk2i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20776/Reviewer_wk2i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655255510, "cdate": 1761655255510, "tmdate": 1762999983648, "mdate": 1762999983648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Reasoning Vectors, a simple way to transfer reasoning ability between language models by subtracting weights of a supervised model from a reinforcement-learned one and adding the resulting vector to new models. This yields consistent gains on reasoning benchmarks with minimal computation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Simple, elegant method requiring only weight arithmetic\n* Reproducible and leverages open-source models, enhancing accessibility"}, "weaknesses": {"value": "* Evaluations lack statistical significance and multiple runs, and the reported performance gains are very small likely within the margin of noise suggesting that the observed improvements may not represent a real enhancement but rather random variation.\n* The method is highly impractical in real-world settings because it requires donor and target models to have identical architecture and tokenizer, a condition rarely met even among models from the same family\n* The method adds a reasoning vector derived from a single task to a highly similar Qwen-Instruct model already trained on that task and many others. The resulting metric gains are minimal, suggesting that the improvement likely reflects task-specific overfitting rather than genuine general reasoning enhancement potentially improving one benchmark at the cost of degrading performance on others.\n* The idea of weight interpolation and vector arithmetic in model parameter space is not novel [1]\n\n[1] Rofin et al, Linear Interpolation In Parameter Space is Good Enough for Fine-Tuned Language Models"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "INMntkkigw", "forum": "nYSqyzEp3G", "replyto": "nYSqyzEp3G", "signatures": ["ICLR.cc/2026/Conference/Submission20776/Reviewer_xtRU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20776/Reviewer_xtRU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763098419, "cdate": 1761763098419, "tmdate": 1762999984560, "mdate": 1762999984560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper determines the difference vector supervise-fine-tuned models and RL-fine tuned models. The difference vector is then used (by addition) to steer the behavior of the model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper follows an interesting idea. The experimental results are promising. Overall, the paper is relatively easy to read and understand."}, "weaknesses": {"value": "In my opinion, the rationale of the paper is inconsistent: the paper argues that reinforcement learning would be (too) costly. However, in order to determine the reasoning vector, reinforcement learning is required.\n\nThe experimental evaluation is overall too weak. It only uses two models of the same type/provider. This is by far too less to make any generalizable statements. It also considers just one set of fine-tuning data. In fact, fine tuning on just one small dataset is not that compute intensive. The more interesting scenario (to me) would be if a model could be further improved using a similar method *after* specific reasoning capacities have already been achieved.\n\nThe paper only considers a single snapshot for determining the vector. This direction of this vector, however, will change over the course of the training process. The paper should investigate if the vector direction actually converges during the training. By continued training, also the overall directions could rotate. \n\nThe improvements in the evaluation as only moderate for most benchmarks. Did you check for statistical significance and random variations in the responses`.\n\nThe generalization capabilities from one domain with others is a contentious topic; the paper should include a balanced discussion with different view points, specifically given the limited experimental design in this paper."}, "questions": {"value": "One top of discussing the weaknesses mentioned before:\n\nPage 4: When are parameter spaces \"sufficiently aligned\"? What about distilled models, e.g.?\n\nI was also somewhat surprised that the transfer is just using the addition of the vector and not some parameter alpha multiplied with the vector as a summand. Why that? Could we increase the effect by adding it multiple times?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CoftjLt2Wt", "forum": "nYSqyzEp3G", "replyto": "nYSqyzEp3G", "signatures": ["ICLR.cc/2026/Conference/Submission20776/Reviewer_kUh4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20776/Reviewer_kUh4"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995338452, "cdate": 1761995338452, "tmdate": 1762999985755, "mdate": 1762999985755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}