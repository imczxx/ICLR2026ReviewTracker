{"id": "Q87kwGI6bx", "number": 10847, "cdate": 1758183240831, "mdate": 1759897624965, "content": {"title": "AesCoder: Code Aesthetics with Agentic Reward Feedback", "abstract": "Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce a new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, a large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, a multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, a benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480B–685B parameters, underscoring the effectiveness of our approach. We will release both the code and datasets to facilitate further research in code aesthetics.", "tldr": "We introduce \"agentic reward feedback\", a new reward framework for training LLMs in code aesthetic tasks, and release related dataset and benchmark.", "keywords": ["Large Language Model", "Code Aesthetics", "Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d46abe0c5a937a1e0cf6f4cb53514e9663004562.pdf", "supplementary_material": "/attachment/a95bebca8da2f7e5576dae9f1a466df197fcc98e.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies how to improve the visual quality of code produced by LLMs. It introduces a large dataset and a multi-agent reward framework that jointly evaluates code executability, static aesthetics, and interactivity. Using this feedback within a GRPO-based reinforcement learning setup, the authors train AesCoder models that perform well on a new benchmark, OpenDesign, for webpage aesthetics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear Problem Statement: The work identifies and addresses a neglected area in LLM code generation: “code aesthetics,” specifically for visually-oriented outputs that go beyond basic executability or syntactic correctness.\n\n2. Substantial Data and Benchmark Contribution: The AesCode-358K dataset is large and carefully curated for code aesthetics, and OpenDesign serves as a credible benchmark with both static and interactive evaluation modes.\n\n3. Human preference tests validate that improvements are not simply artifacts of the automatic scoring pipelines."}, "weaknesses": {"value": "1. The paper claims general applicability to “code aesthetics” broadly, but without evidence from other modalities (e.g., GUI design, visualization dashboards, game UIs). A brief transfer or zero-shot study in a different domain would make the generalization claim more convincing.\n\n2. The OpenDesign benchmark depends heavily on LLM-based judges, which may lead to circularity since similar models are used for training rewards. Although the authors mention strong correlation with human judgment, the potential for overfitting to automated metrics remains.\n\n3. Because both training rewards and evaluation rely on similar LLMs, the model might learn to exploit their aesthetic preferences rather than aligning with human aesthetic standards. The human–human agreement rate leaves some uncertainty regarding the real-world validity of “aesthetic improvement.” \n\n4. The paper does not analyze how the multi-agent reward system handles ambiguous or partially successful outputs, such as non-executable but visually correct code, or misaligned interactivity cases. Including failure case studies or qualitative analyses would increase confidence in the robustness and interpretability of the proposed framework."}, "questions": {"value": "1. How are the weights $w_{exec}$, $w_{static}$, and $w_{interact}$ chosen in reward aggregation? Were these tuned per dataset/model, or kept fixed? \n\n2. Given that the three reward sources operate on different scales, how are they normalized to ensure balanced optimization? Are there cases where one component dominates or destabilizes training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GStgM3b3n0", "forum": "Q87kwGI6bx", "replyto": "Q87kwGI6bx", "signatures": ["ICLR.cc/2026/Conference/Submission10847/Reviewer_Z31k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10847/Reviewer_Z31k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844532885, "cdate": 1761844532885, "tmdate": 1762922047585, "mdate": 1762922047585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AesCoder, a framework for enhancing the aesthetic quality of LLM-generated code for visually-oriented tasks like plot generation and webpage design. The key contributions include:\n\nAesCode-358K: A large-scale instruction-tuning dataset focused on code aesthetics\n\nAgentic Reward Feedback: A multi-agent system evaluating executability, static aesthetics (via screenshot analysis), and interactive aesthetics (via GUI interaction)\n\nGRPO-AR: Integration of the agentic reward framework with the GRPO algorithm for reinforcement learning\n\nOpenDesign Benchmark: A new benchmark with 840 real webpage cases for evaluating code aesthetics\n\nThe proposed AesCoder models (4B and 7B parameters) achieve state-of-the-art results, outperforming GPT-4o and GPT-4.1, and competing with much larger 480B-685B models on aesthetic quality metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**:\n\nThis work represents the first systematic effort to formalize and computationally address the concept of \"code aesthetics,\" a dimension of code quality long acknowledged by practitioners but largely neglected in automated code generation research. The proposed multi-agent reward framework is a novel architecture that synergistically combines textual analysis, visual rendering, and interactive evaluation to assess code quality holistically. A particularly creative contribution is the pioneering application of GUI-based autonomous agents for reward modeling in Reinforcement Learning, enabling the system to perceive and evaluate code outputs from a human-like, user-centric perspective.\n\n**Quality**:\n\nThe research is underpinned by a rigorous experimental protocol, with extensive evaluations conducted across multiple established benchmarks (including HumanEval and MBPP) and a range of model scales (from 160M to 7B parameters) to ensure robust findings. The validity of the proposed framework is further cemented by comprehensive ablation studies that meticulously isolate and confirm the contribution of each component. High-quality human evaluations are conducted, the results of which strongly align with the automated metrics, lending significant credibility to the findings. Furthermore, the construction of the dataset reflects a commitment to quality, involving careful data curation, filtering, and validation processes.\n\n**Clarity**:\n\nThe manuscript is exceptionally well-written, presenting complex concepts and a sophisticated multi-stage framework with remarkable clarity and logical flow. A comprehensive appendix is provided, offering full implementation details and facilitating replication and future research. The authors maintain transparency throughout, openly discussing the limitations of their approach and justifying key design choices, which enhances the work's credibility and scholarly value.\n\n**Significance**:\n\nThis research addresses a critical, real-world gap in LLM-powered code generation: the disconnect between functional correctness and user experience. By focusing on the aesthetic and usability aspects of code, the work provides tangible resources - including a novel dataset and benchmark - that will undoubtedly spur further investigation in this emerging area. Perhaps most significantly, the demonstration of practical efficacy even with smaller model sizes highlights the approach's potential for real-world deployment, making advanced code aesthetic optimization more accessible and scalable."}, "weaknesses": {"value": "**Reproducibility Concerns**\n\nThe reproducibility of this study is significantly hampered by its deep dependence on several proprietary, black-box models, specifically GPT-4 and GPT-4V, which serve as the core \"judges\" in the reward model and are pivotal for the initial dataset generation. This reliance creates a hard barrier for independent verification, as the internal mechanics and future versions of these models are opaque and subject to change, making it impossible to exactly replicate the evaluation criteria or data sources. Furthermore, the computational footprint of the proposed multi-agent evaluation system—which involves executing generated code, rendering graphical interfaces, and deploying interactive GUI agents—is substantial and likely prohibitive for academic research labs with limited resources. Finally, while the interactive agent is a novel idea, the paper provides insufficient detail on its specific failure modes and brittleness, leaving readers to wonder how often it fails due to environmental issues versus the actual quality of the generated code.\n\n**Technical Limitations**\n\nSeveral technical limitations warrant further investigation. The paper candidly acknowledges the relatively low success rate of the interactive aesthetics agent, but it does not delve into a quantitative analysis of how these frequent failures during the reward collection phase propagate through and potentially destabilize the Reinforcement Learning training loop. The complex, multi-stage RL setup, which blends rewards from multiple agents, naturally raises questions about training stability and sensitivity to hyperparameter choices, which remain entirely unaddressed. Additionally, the scope of the framework's generalization is demonstrated primarily on a narrow set of visually-oriented tasks like plots and web pages; its applicability to other critical domains such as game development, mobile UI creation, or data dashboard generation is left unexplored, leaving its broader utility an open question.\n\n**Evaluation Scope**\n\nWhile the introduced OpenDesign benchmark is a valuable contribution, the evaluation scope could be strengthened to more firmly establish the generalizability of the findings. The task types within OpenDesign, though comprehensive, are not fully representative of the entire spectrum of front-end coding challenges; incorporating more complex and diverse interactions, such as those involving dynamic data feeds, state management, or complex user input validation, would provide a more rigorous test. Moreover, the comparative analysis in the paper is primarily focused on benchmarking against general-purpose LLMs. A more telling comparison would involve pitting the method against other state-of-the-art code generation models that are specifically fine-tuned for front-end tasks, which would better isolate the unique benefits conferred by the aesthetics-focused training paradigm."}, "questions": {"value": "Reproducibility: Given the heavy reliance on GPT-5 and GPT-4o for reward judgment and data generation, what are your plans to make the framework more accessible to researchers without access to these proprietary models?\n\nComputational Efficiency: What is the approximate cost and latency of the full agentic reward evaluation per sample? How does this impact the practical feasibility of scaling this approach?\n\nInteractive Agent Limitations: You mention the low success rate of GUI agents. Could you provide more analysis of how these failures affect the reward signal quality and training stability?\n\nGeneralization: Have you tested the framework on other visually-oriented coding tasks beyond plots and webpages (e.g., GUI development, game level design)? What adaptations would be needed?\n\nAblation Details: In the GRPO-AR ablation, what specific reward model was used as the baseline? Was it trained on the same data or using similar principles as your agentic framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "blAkyAgYLE", "forum": "Q87kwGI6bx", "replyto": "Q87kwGI6bx", "signatures": ["ICLR.cc/2026/Conference/Submission10847/Reviewer_zdWS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10847/Reviewer_zdWS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904390213, "cdate": 1761904390213, "tmdate": 1762922047168, "mdate": 1762922047168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the aesthetic capabilities of code LLMs in visually-oriented coding tasks, such as plot generation and web design, where conventional execution-based metrics are insufficient. To enhance these capabilities, the authors curate a supervised instruction tuning dataset, AesCode358k, covering Python-based plot generation and web design instructions. The model is further improved by reinforcement learning using the GRPO algorithm combined with an Agentic Reward framework, which integrates execution, static aesthetic, and interactive aesthetic evaluations. Finally, the authors propose a new benchmark, OpenDesign, to assess aesthetic quality in LLM-generated web designs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an underexplored but important problem that is the aesthetic quality of code generated by LLMs for visual tasks, which measuring functional correctness is insufficient .\n\n- The AesCode358k dataset is a valuable contribution that can benefit future research in aesthetic-aware code generation once released.\n\n- The proposed OpenDesign benchmark is also a useful dataset that addresses the limitation of having human voters for evaluation, providing a reproducible way to evaluate aesthetic quality , which enhances scalability and objectivity."}, "weaknesses": {"value": "- The same aesthetic agents are used both during reinforcement learning and evaluation, introducing potential circularity and reward overfitting. The model may learn to exploit or mimic the judge model’s biases rather than improving true generalization in visual aesthetics.\n\n- While improvements on visual coding tasks are reported, the paper lacks an evaluation of whether the aesthetic alignment impacts general code generation ability (e.g., possible regression on standard code benchmarks such as BigCodeBench or LiveCodeBench).\n\n- The Agentic Reward framework relies on multiple large models to serve as static and interactive aesthetic judges. This heavy dependence on high-capacity teacher models will make the framework computationally expensive and difficult to scale, especially for community replication or deployment.\n\n- The definition of “aesthetic quality” remains somewhat opaque. A clearer operationalization or rubric (e.g., color harmony, layout balance, readability) would help readers understand what the models are actually learning to optimize."}, "questions": {"value": "- For the Python-based plot generation data, while executability ensures the code runs without errors, how do the authors verify that the visual output actually matches the instruction semantics (e.g., correct axes, legends, or visual style)?\n\n- In line 169, could the authors elaborate on the clustering method and representative sampling strategy used for data selection?\n\n\n- In line 175, what are the specific scoring criteria and aspects used by GPT-5 during the aesthetic evaluation?\n\n- How are the relative weights among the three reward components determined? Are they tuned empirically, heuristically, or via grid search?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "byBwjxd5nU", "forum": "Q87kwGI6bx", "replyto": "Q87kwGI6bx", "signatures": ["ICLR.cc/2026/Conference/Submission10847/Reviewer_ZReh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10847/Reviewer_ZReh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970232400, "cdate": 1761970232400, "tmdate": 1762922046586, "mdate": 1762922046586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training and evaluation pipeline to improve the visual quality (\"code aesthetics\") of LLM generated code in two domains: Python plotting and webpage design. The authors (i) build AesCode-358K, a supervised instruction tuning dataset comprising approximately 158k validated plotting examples and a large set of webpage design cases produced and filtered through a multi step process, (ii) introduce an agentic reward framework with three agents for execution, static aesthetics, and interactive aesthetics, and aggregate their signals with a weighted sum, (iii) train AesCoder-4B/7B with GRPO-AR, and (iv) propose OpenDesign, a benchmark for static and interactive webpage aesthetics whose rankings show high agreement with the Design Arena leaderboard and reasonable GPT and human alignment. On PandasPlotBench and OpenDesign, AesCoder-4B outperforms GPT-4o/4.1 and is competitive with much larger models, and ablation studies show benefits over DPO and RFT and over GRPO without the agentic reward."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem is well defined, the methodology is mostly sound, and the experiments support the main claims. The reliance on proprietary judges (GPT-5/GPT-4o) for both data curation and evaluation introduces possible bias, which the paper partially mitigates via correlation with Design Arena and human annotations. More analysis on sensitivity to judge choice and reward weights would strengthen soundness.\n\nThe execution agent uses HTMLHint rules rather than brittle strict parsing; the static aesthetics agent defines three explicit criteria and uses a single image of the rendered page; the interactive agent uses a GUI agent to act on the page and aggregates successes. These design choices are concrete and implementable.\n\nOpenDesign’s rankings have high correlation with Design Arena (Spearman 0.98, Kendall 0.91), and GPT–human agreement of 80.9% exceeds human–human agreement for the 200 pairwise comparisons, which supports the reliability of the evaluation protocol."}, "weaknesses": {"value": "GPT-5 (and GPT-4o) are used during dataset filtering, for static aesthetics scoring, and within the interactive agent. This may bias the training signal and the benchmark toward these judges’ preferences. While OpenDesign shows strong rank correlation with Design Arena and decent GPT–human agreement, results could be sensitive to the choice of judge. Please report results with at least one strong alternative judge and quantify changes.\n\nThe paper defines the weighted sum for r and also reports a concrete setting of the weights ($w_{exec}$, $w_{static}$, $w_{interact}$). However, it does not describe a normalization scheme across agents or provide an ablation over alternative weight choices. This limits reproducibility and makes it hard to assess sensitivity. Please report the exact normalization used and add a systematic ablation over plausible ranges, for example, varying one weight while keeping the total fixed, and include confidence intervals for all metrics.\n\n\nA few typos reduce polish (e.g., “sucess”, “Grammer”, “protion”). Figures are informative but Figure 1 would benefit from a clearer depiction of when each agent runs (serial vs parallel) and where screenshots are taken."}, "questions": {"value": "What are the exact definitions and numeric ranges of the three agent signals before aggregation, and is any normalization, clipping, or rescaling applied to each signal prior to the weighted sum?\n\nThe paper reports one specific setting of the weights w_exec, w_static, and w_interact. Were these exact weights used for all training runs and all evaluations, and how were they chosen in the first place for example by a validation set criterion or a simple search?\n\nHow sensitive are the results to the choice of weights? Please provide a systematic ablation that varies the weights over a grid while keeping the total weight fixed, and report primary metrics with confidence intervals and the stability of model rankings.\n\nHow should users of the method select weights in new domains or tasks that differ from plots and webpages? Please provide general guidance or a simple procedure for choosing weights based on observable properties of the three signals, and discuss when rebalancing the weights is necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0puQtvkdGV", "forum": "Q87kwGI6bx", "replyto": "Q87kwGI6bx", "signatures": ["ICLR.cc/2026/Conference/Submission10847/Reviewer_k5dx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10847/Reviewer_k5dx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980072996, "cdate": 1761980072996, "tmdate": 1762922046131, "mdate": 1762922046131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}