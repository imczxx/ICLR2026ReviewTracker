{"id": "rwo7bVlnzo", "number": 24728, "cdate": 1758359763565, "mdate": 1759896752420, "content": {"title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification", "abstract": "Verifiers--functions assigining rewards to agent behavior--have proven effective to guide test-time scaling and reinforcement learning, driving progress in domains such as math, code, and board games. However, extending these gains to domains without clear-cut success criteria (e.g., computer use) remains a challenge: while humans can recognize suitable outcomes, translating this intuition into scalable rules is non-trivial. Multimodal Large Language Models (MLLMs) emerge as a promising solution, given their world knowledge, human-preference alignment, and reasoning skills. We evaluate MLLMs as verifiers across web navigation, computer use, and robotic manipulation, and identify a critical limitation: a strong tendency to favorably evaluate agent behavior, often producing chains of thought to rationalize flawed behavior, a phenomenon we term agreement bias. We show that this bias persists across a range of MLLMs, reasoning models, and test-time scaling techniques, and poses risks to several methods relying on MLLM evaluations. Notably, it occurs despite MLLMs showing strong, human-aligned priors on desired behavior, suggesting a knowledge extraction bottleneck unresolved by existing techniques. We discuss how to mitigate this bias and introduce Self-Grounded Verification (SGV), a simple two-step framework that leverages MLLMs' knowledge and reasoning by modulating unconditional and conditional generation. In SGV: (i) the MLLM first generates priors conditioned only on minimal context to frame the task, and (ii) evaluates candidate trajectories conditioned on self-generated priors. SGV improves both accuracy and failure-detection rates by up to 20 percentage points, and enables MLLMs to provide real-time supervision, boosting task completion for a GUI specialist in OSWorld, a diffusion policy in robomimic, and a ReAct agent in VisualWebArena--establishing a new state of the art on the benchmark. Finally, we release an updated version of the (Visual)WebArena benchmark featuring more human-aligned evaluators, improved environment parallelization with higher execution fidelity, and runtime speedup of over 10x, thereby facilitating the development of digital agents.", "tldr": "", "keywords": ["Verifiers", "Verification", "Digital Agents", "Web Agents", "GUI Agents", "Robotics", "Multimodal Large Language Models", "Test Time Scaling", "WebArena", "OSWorld"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9467e64d2ea36e5ed962ab304a81fe89fbf252e7.pdf", "supplementary_material": "/attachment/4edaece0ff6d275783b412052a08c5d6eb1864c6.zip"}, "replies": [{"content": {"summary": {"value": "This work investigates the issue of agreement bias in vLLMs when used as evaluators. Specifically, vLLMs are employed to assess intermediate steps in multimodal agent tasks, serving as a form of reward or signal that can facilitate test-time scaling. However, the authors find that vLLMs may generate chains of thought to rationalize potentially flawed actions, even when their evaluative knowledge is aligned with human judgment. To mitigate this issue, the authors propose having the vLLM first generate a plausible prediction of the next state before conducting evaluation, which effectively reduces agreement bias. The problem is studied across domains such as web agents, GUI agents, and robotics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The research question is both interesting and important, though the phenomenon of agreement bias and the proposed solution might appear somewhat intuitive or straightforward. The experiments are comprehensive and the results are solid. The paper is also very well written — I appreciate that the abstract clearly conveys the main takeaways."}, "weaknesses": {"value": "A critical problem remains: I’m particularly curious whether you have any further experiments and analysis on the “generating chains of thought to rationalize flawed behavior” aspect. You also claim that MLLMs exhibit strong, human-aligned priors on desired behavior — so at which reasoning step exactly does the failure occur? Is this bias intrinsic to the model itself, and beyond prompting strategies, are there training or sampling techniques that could help mitigate this issue?"}, "questions": {"value": "Please check the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yctUUAnlcC", "forum": "rwo7bVlnzo", "replyto": "rwo7bVlnzo", "signatures": ["ICLR.cc/2026/Conference/Submission24728/Reviewer_Uw7g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24728/Reviewer_Uw7g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862690551, "cdate": 1761862690551, "tmdate": 1762943177819, "mdate": 1762943177819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper 1) identifies \"agreement bias\", a tendency for MLLMs to inappropriately favor agent trajectories in their context window, as a critical limitation for MLLM-based verifiers, and 2) proposes Self-Grounded Verification (SGV), a two-step method that retrieves task priors first then evaluates trajectories against them, achieving up to 20pp gains in verification accuracy and setting a new SOTA on VisualWebArena."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: Framing \"agreement bias\" as a distinct limitation from self-bias and targeting it via self-generated priors is a novel angle for MLLM verifiers.\n2. Quality: Experiments use diverse benchmarks (1,200+ tasks) and models, ensuring results are generalizable rather than model-specific.\n3. Clarity: The SGV method is described simply, with step-by-step breakdowns and concrete examples (e.g., Figure 5) making it easy to follow.\n4. Significance: Improving MLLM verifier reliability directly benefits downstream tasks like agent training, data filtering, and real-time supervision, a key for deploying AI agents safely."}, "weaknesses": {"value": "1. SGV does not address underlying vision-language flaws (e.g., Figure 7’s counting error), and the paper lacks discussion on combining SGV with specialist models for fine-grained perception.\n2. Current studies primarily focus on moderate-length trajectories (e.g., \"We set the maximum number of steps to 30\"). However, the scalability of SGV to extremely long sequences remains unclear. Such sequences are common in computer usage scenarios, and the context window pressure under extremely long sequences may cause biases to reemerge.\n3. The ablation on SGV’s prompt design (Appendix B.6) is limited. more tests on prior generation diversity (e.g., multiple priors vs. single) would strengthen claims about SGV’s mechanism."}, "questions": {"value": "Address the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nT2uJTmUNp", "forum": "rwo7bVlnzo", "replyto": "rwo7bVlnzo", "signatures": ["ICLR.cc/2026/Conference/Submission24728/Reviewer_twch"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24728/Reviewer_twch"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987467994, "cdate": 1761987467994, "tmdate": 1762943177586, "mdate": 1762943177586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies “agreement bias,” i.e., a preference of verifiers to return positive results, as a key issue for agent tasks. They then propose a simple method in which a verifier first proposes a task-specific rubric and scores trajectories according to this rubric in order to mitigate agreement bias. Finally, the paper shows that these improved verifiers can be used to improve model performance on VisualWebArena, OSWorld, and the robotic manipulation task robomimic."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a straightforward extension of the idea in Pan, et al. (2024) which shows that automatic evaluators can be used to improve the performance of web navigation and device control agents at training or inference time\n\n2. The paper makes a compelling case that models exhibit agreement bias when evaluating agent trajectories, i.e., a bias toward positive labels (Table 1a). The paper also clearly shows that its method leads to a reduction in agreement bias (Table 1b), and that this method works across a wide range of verifier models\n\n3. Most importantly, the paper shows that stronger verifiers are more useful, e.g., at improving agents at inference time using methods like Reflexion (Figure 2)"}, "weaknesses": {"value": "1. It would be nice to see some comparison of the proposed method with simpler strategies, e.g., different prompts to the verifier model, or prompting models to generate confidences and applying Platt scaling\n\n2. The paper could benefit from an additional round of proofreading. For example:\n     \n   - Line 101: missing a period\n   - Line 221: “Table Table 7” -> “Table 7”\n   - Line 263: broken reference (“??”)\n   - Line 1234: “AgentRewarBench” -> “AgentRewardBench”\n   - \\citet should be replaced with \\citep in many places"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UXXRBVFT74", "forum": "rwo7bVlnzo", "replyto": "rwo7bVlnzo", "signatures": ["ICLR.cc/2026/Conference/Submission24728/Reviewer_dsJd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24728/Reviewer_dsJd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993863957, "cdate": 1761993863957, "tmdate": 1762943177377, "mdate": 1762943177377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies \"agreement bias\" as a critical failure mode in Multimodal Large Language Models (MLLMs) when they are used as verifiers for agent trajectories. The authors find that MLLMs tend to favorably evaluate flawed agent behavior, even generating rationalizations for it, despite possessing strong, human-aligned priors on correct task execution. They attribute this to a retrieval bottleneck. To address this, the paper introduces Self-Grounded Verification (SGV), a two-step prompting method. SGV first elicits the MLLM's broad priors about task completion independent of the agent's trajectory, and then conditions the MLLM on these self-generated priors to evaluate the candidate trajectory. Experiments across web navigation, computer use, and robotics benchmarks show that SGV significantly improves failure detection and accuracy, boosting the performance of agents in online supervision settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work identifies a significant and practical problem, the \"agreement bias\" of MLLM verifiers. This is an important contribution as these verifiers are increasingly proposed for data filtering, self-refinement, and online agent guidance.\n2. The paper demonstrates strong empirical results, particularly in improving the True Negative Rate (failure detection). This is a crucial metric that is more informative than overall accuracy for this problem, as the primary goal of a verifier is to catch flawed behavior.\n3. The method is validated across a diverse and challenging set of multimodal environments (VisualWebArena, OSWorld, robomimic) and application settings (offline evaluation, self-refinement, and online supervision), which strengthens the generality of the claims."}, "weaknesses": {"value": "1. The core mechanism of SGV—generating \"broad priors\" in Step 1 independent of the agent's trajectory —may be a significant flaw. By being ungrounded from the specific context of the agent's current state, these priors may be overly generic or common sense hallucinations that are irrelevant to the task at hand. This could lead the verifier to be \"overly strict,\" unfairly penalizing valid or creative solutions that deviate from the generic script, a failure mode the authors acknowledge. The paper lacks a sufficient analysis of when these broad priors are helpful versus when they are harmful.\n2. The proposed solution is a heuristic-driven prompting technique. While it shows good results, it is not clear how the method will scale or interact with future model development. The paper claims the issue is a \"retrieval bottleneck\", but it is equally plausible that agreement bias is an artifact of current alignment techniques or context-window management. The paper does not provide evidence to suggest whether SGV is a durable solution or a temporary patch for a flaw that might be solved more fundamentally by future models, rendering the heuristic obsolete.\n3. The evaluation of the online supervision setting seems arbitrary in its implementation. For instance, in OSWorld, the verifier is called \"every 5 steps\". There is no justification for this hyperparameter, and it glosses over the significant trade-off between verification frequency (and thus, token/compute cost) and the ability to catch errors in real-time."}, "questions": {"value": "How does the performance benefit of SGV change with model scale and capability? The paper shows it helps both weaker and stronger \"reasoning\" models, but does the relative gain (SGV vs. baseline) shrink as models become more capable? This would provide insight into whether SGV is fixing a fundamental reasoning flaw or a specific weakness of current models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2pl9gAacSI", "forum": "rwo7bVlnzo", "replyto": "rwo7bVlnzo", "signatures": ["ICLR.cc/2026/Conference/Submission24728/Reviewer_187k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24728/Reviewer_187k"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762086413502, "cdate": 1762086413502, "tmdate": 1762943177205, "mdate": 1762943177205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}