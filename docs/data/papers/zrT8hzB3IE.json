{"id": "zrT8hzB3IE", "number": 17003, "cdate": 1758271102247, "mdate": 1759897204950, "content": {"title": "Unlearning Shapley: Data Valuation through Machine Unlearning", "abstract": "Data valuation is essential for understanding and improving machine learning models. However, existing approaches such as Shapley-value-based retraining or influence functions are either computationally prohibitive or require full access to the training data, which is often unrealistic in practice. This challenge is particularly pressing in real-world settings such as data markets, federated environments, or compliance with the right to be forgotten, where only partial access to data subsets is available. We introduce Unlearning Shapley, a framework that adapts machine unlearning for both full-data and partial-data valuation. Instead of repeatedly retraining on all subsets, our method leverages a single pre-trained model and applies approximate unlearning to remove the effect of the target data, thereby estimating its marginal contribution. This design uniquely enables valuation when the rest of the training data is inaccessible, offering a privacy-compliant and practically deployable solution. Through theoretical analysis, we show the connection between Unlearning Shapley and classical Shapley values, and we provide bias and error bounds for our estimator. Experiments on benchmark vision datasets and large-scale language models demonstrate that Unlearning Shapley achieves comparable or superior performance to state-of-the-art methods in identifying influential or noisy data, while reliably extending to the partial-data setting where existing approaches fail. Our study highlights the importance of partial data valuation and extends the applicability of machine unlearning beyond privacy to equitable and transparent data markets.", "tldr": "We propose Unlearning Shapley that computes Shapley values for full and partial data valuation through machine unlearning, enabling privacy-aware data valuation without retraining.", "keywords": ["data valuation", "machine unlearning", "shapley value"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68c488575df6a21290a8c4b798e63db41d61b59d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Unlearning Shapley, which estimates data value by unlearning subsets from a pretrained model instead of retraining on all coalitions. The paper shows the connection between Unlearning Shapley and classical Shapley through theoretical analysis, and provides bias and error bounds for their estimator."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The Unlearning Shapley is a clear reduction from retraining.\n\n2. The writing is clear and easy to follow."}, "weaknesses": {"value": "1. A key concern for the reviewer is that this work looks just a simple application of unlearning method, with only limited contributions. And it's performance is highly relied on the selected unlearning methods.\n\n2. For the approximate unlearning method used in the paper, the equation above the experiment setion, authors use a test set to replace the retain set. Is it reasonable to use the test dataset in the unlearning training? There are many unlearning methods that do not use the remaining data [R1, R2].\n\n[R1]. Tarun, Ayush K., et al. \"Fast yet effective machine unlearning.\" IEEE Transactions on Neural Networks and Learning Systems 35.9 (2023): 13046-13055.\n\n[R2]. Panda, Subhodip, and Shashwat Sourav. \"Partially Blinded Unlearning: Class Unlearning for Deep Networks from Bayesian Perspective.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 6. 2025.\n\n3. The theoretical bound focuses on closeness between unlearning and retraining utility, but does not provide a guarantee that the model no longer used information from the deleted data.\n\n4. In the experimental section, it has not introduced the purposes of these experiments. In Table 1, it seems that Unlearning Shapley only achieves a good performance on Adult and MNIST. And all these datasets are toy datasets, we should evaluate the method in a more complex dataset."}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jEi5YIZVV9", "forum": "zrT8hzB3IE", "replyto": "zrT8hzB3IE", "signatures": ["ICLR.cc/2026/Conference/Submission17003/Reviewer_24EF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17003/Reviewer_24EF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761285752866, "cdate": 1761285752866, "tmdate": 1762927026314, "mdate": 1762927026314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Unlearning Shapley, a framework that approximates Shapley values by replacing expensive retraining with machine unlearning. The primary contribution is an efficient data valuation method, especially for partial-data scenarios where existing methods are inapplicable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strength is its novel idea of connecting machine unlearning with Shapley value estimation. This approach tackles the significant and practical problem of partial data valuation."}, "weaknesses": {"value": "1. The paper's core approximation (Eq. 3) lacks validation (e.g., quantifying $\\varepsilon_{u}$), and the impact of different unlearning algorithms or configurations is not studied.\n2,  The claimed superiority of the method is not supported by the majority of the results in Figure 1 and Table 1. The explanation for its unique failure on the Adult dataset (lines 356-358) is particularly unconvincing. And the paper repeatedly emphasizes its applicability to large models, yet the experiments conducted on them are very limited."}, "questions": {"value": "1. Beyond approximation error, does the imperfect unlearning process introduce a systematic bias into the Shapley values?\n2. Could you provide a deeper analysis for the method's inconsistent performance where it often fails to outperform baselines, and explain the root cause of its unique failure on the Adult dataset?\n3. The paper claims superior efficiency over retraining-based methods. Could you provide a full, end-to-end time comparison between the Unlearning Shapley algorithm and a baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Oq34uW2Au", "forum": "zrT8hzB3IE", "replyto": "zrT8hzB3IE", "signatures": ["ICLR.cc/2026/Conference/Submission17003/Reviewer_ZUSd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17003/Reviewer_ZUSd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722721565, "cdate": 1761722721565, "tmdate": 1762927024588, "mdate": 1762927024588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Overall, this is an interesting study that tackles an important and practical problem for Shapley value-based XAI tools by incorporating ideas from machine unlearning. The novelty is sufficient; however, I have several questions regarding the current experimental design and results. It would be helpful if the authors could justify these and, ideally, provide more detailed experiments to strengthen the paper's contributions. My detailed questions are listed below."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The research gap this paper addresses is relevant and has the potential for significant impact in the field. The methods are clearly written and theoretically justified. The datasets used for evaluation are comprehensive, covering both structured and unstructured data. The overall writing is clear and straightforward."}, "weaknesses": {"value": "The current results presented lack sufficient evidence to justify that the proposed method is The current results lack sufficient evidence to justify that the proposed method has significant advantages over existing ones. For instance, in Table 1, the F1 score of the proposed method is outperformed by baselines in several datasets.\n\nFurthermore, no confidence intervals or measures of variance (e.g., from repeating experiments 10 times) are reported in Table 1. This lack of statistical rigor makes the authors' own conclusion—that \"Unlearning Shapley’s competitive but not uniformly superior performance in noise detection indicates that while unlearning effectively identifies examples that harm model performance, the relationship between harmfulness and label corruption is complex and dataset-dependent.\"—sound weak and unsubstantiated.\n\nIn Table 2, the Spearman correlation is quite low (all < 0.6), suggesting the approximation is not very reliable. More importantly, the results for all baseline methods are omitted. This makes it impossible to validate the paper's central claim: that it uniquely succeeds in the partial-data setting where other methods \"fail.\""}, "questions": {"value": "1. Can the authors please add results from multiple runs (e.g., mean and standard deviation) for Table 1 to demonstrate that their method's performance is statistically significant and not due to chance?\n\n2. Regarding Table 2, can the authors justify why the baseline methods are omitted? If it is not impossible for them to run, their results must be included for a fair comparison. If it is impossible, this should be explicitly justified and, if possible, demonstrated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tv98G84FkI", "forum": "zrT8hzB3IE", "replyto": "zrT8hzB3IE", "signatures": ["ICLR.cc/2026/Conference/Submission17003/Reviewer_LPLG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17003/Reviewer_LPLG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813803283, "cdate": 1761813803283, "tmdate": 1762927023891, "mdate": 1762927023891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Unlearning Shapley, a framework that uses machine unlearning to enable valuation when only a target subset of data is available and may be more efficient than computing the Shapley value. The key idea is to approximate the utility of a data subset by unlearning a complement set from the fully trained model instead of retraining from scratch."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Unlearning Shapley is original in the use of unlearning for data valuation and may be important for efficient data valuation of large models, such as LLMs when only a small subset is removed. The paper also identifies a reasonable use case of doing data valuation when the training data is not fully available."}, "weaknesses": {"value": "1. The contributions of the paper may be limited. The key idea is to approximate the utility of a data subset by unlearning a complement set from the fully trained model instead of retraining from scratch. In other words, it suggests a known approximation for the utility function instead of a more efficient way to approximate the Shapley value or a new data value that is an alternative to the Shapley value.\n    * The approximation quality depends on the unlearning approximation error $\\epsilon_u$ but this is unknown and not analysed in the experiments. The assumption (Eq 6) should appear in the main paper and be justified. \n2. There is a lack of comparison with other unlearning methods and data valuation approximation methods.\n    * The paper suggests a new approximate unlearning algorithm in Sec 3 but there is no theoretical or empirical guarantee that it approximates retraining well and is better than other methods.\n    * The related work section does not discuss other efficient approximation techniques to approximate the utility function e.g., using a single epoch in Data Shapley.\n3. The empirical results are not strong.\n    * In Fig 1 and 2,  Unlearning Shapley does not seem to outperform other methods on most experiments.\n    * More comprehensive benchmark experiments can be performed using OpenDataVal.\n    * The experiments do not analyse the quality of the utility approximation directly.\n    * In Table 2, the correlation with the exact Shapley value is only moderately high (above .5) for 2 datasets. A perfect approximation method should have a correlation of 1 for exact Shapley value but there is no ideal value for correlation with the performance of the retrained model so it may be less meaningful."}, "questions": {"value": "1. Explain the mild assumptions behind Equation (5). Specifically, also explain why the unlearning approximation error $\\epsilon_u$ is expected to be low for unlearning algorithms, including the new one proposed. \n\nMinor comments:\n* For equation (3), it may be clearer to use $T$ instead of $S$.\n* The citation should be in (author, year) instead of author (year)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fAh5rVlFb9", "forum": "zrT8hzB3IE", "replyto": "zrT8hzB3IE", "signatures": ["ICLR.cc/2026/Conference/Submission17003/Reviewer_vVYA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17003/Reviewer_vVYA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875763654, "cdate": 1761875763654, "tmdate": 1762927023531, "mdate": 1762927023531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}