{"id": "oql27GfyPD", "number": 11130, "cdate": 1758190017074, "mdate": 1759897606203, "content": {"title": "Rethinking Deep Safety Alignment: Reflective Safety Alignment for Balancing Harmlessness and Helpfulness of LLMs", "abstract": "Current safety alignment techniques for large language models (LLMs) face two key challenges: (1) under-generalization, which leaves models vulnerable to novel jailbreak attacks, and (2) over-alignment, which leads to the excessive refusal of benign instructions. Our preliminary study shows that guiding the base model with a safety-policy-driven reasoning process, which incorporates self-reflection steps, can effectively defend against jailbreak attacks while preserving response quality. This motivates internalizing and improving safety-policy-driven self-reflective reasoning capabilities in LLMs to better balance harmlessness and helpfulness. To this end, we propose the Reflective Safety Alignment Framework (ReAlign), which consists of two stages: (1) Reasoning-style Warmup (RW) that enables LLMs to internalize long-chain reasoning capability, and (2) Self-reflective Reasoning Process Optimization (SRPO) that further promotes reflection and correction during reasoning. Extensive experiments demonstrate the superiority of ReAlign over existing mainstream alignment methods.", "tldr": "This paper propose a Safety-aware Reflective Reasoning Optimization Framework (SaRO) to better balance harmlessness and helpfulness of LLMs.", "keywords": ["Large Language Models", "LLM Safety", "Safety Alignment", "Reasoning for Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a841d18fdd4edffe04bdeddcf53ca042b4c3910a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ReAlign, which improves the model safety generalization and mitigates over-alignmnt by enhancing the safety response with reasoning. The method consists of two stages: Reasoning-type warmup and self-reflective reasoning process optimization.  Experiments show that such approach helps improve the model's helpfulness and harmlessness trade-off."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The second stage of Self-Reflective Reasoning process optimization is innovative and interesting. It encourages the model to do early safety reflection, which helps the model to learn the nuance in safety related prompt and also save inference cost."}, "weaknesses": {"value": "1. The overall novelty is limited.  The idea of using reasoning to help align the model is not new. The author only compares with STAIR as a baseline, but there are other approaches using SFT [1] or RL [2] to teach LM to reason over harmful and harmless prompts to improve and mitigate over-alignment. The author also mentioned deliberate alignment from OpenAI, which also demonstrated the importance of reasoning for alignment. The major novelty lies in the second SRPO stage.\n\n2. It is not clear how much advantage we get by using the PP-COT besides the inference cost saving. \n\n\n\n\n\n\n[1] Si, S., Wang, X., Zhai, G., Navab, N., & Plank, B. (2025). Think Before Refusal : Triggering Safety Reflection in LLMs to Mitigate False Refusal Behavior. ArXiv, abs/2503.17882.\n[2] Kim, T., Tajwar, F., Raghunathan, A., & Kumar, A. (2025). Reasoning as an Adaptive Defense for Safety. ArXiv, abs/2507.00971."}, "questions": {"value": "see weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nGrXTJh9ne", "forum": "oql27GfyPD", "replyto": "oql27GfyPD", "signatures": ["ICLR.cc/2026/Conference/Submission11130/Reviewer_eNep"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11130/Reviewer_eNep"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843148969, "cdate": 1761843148969, "tmdate": 1762922301478, "mdate": 1762922301478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ReAlign, a reflective safety alignment framework that achieves deep safety alignment in large language models by balancing harmlessness and helpfulness. It addresses two key issues in existing alignment methods—under-generalization to unseen jailbreaks and over-alignment causing excessive refusals—arguing that current approaches (SFT, DPO, RLHF) only perform shallow token-level corrections rather than reasoning-level safety control."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "ReAlign advances safety alignment from surface-level refusal to reasoning-based self-reflection, encouraging models to reason about policy violations before responding. This deepens interpretability and robustness against jailbreak attacks."}, "weaknesses": {"value": "1. While the paper compares against STAIR and Recovery Examples, it lacks direct comparison with PPO-based alignment methods (e.g., RLHF, RLAIF). Including such baselines would better position the proposed approach within the standard alignment paradigm and strengthen the empirical claims.\n\n2. Since reflective reasoning is explicitly embedded during training, models may learn to superficially mimic reflection patterns (e.g., \"Wait, this is unsafe...\") without genuinely reasoning about safety. The paper would benefit from additional analysis demonstrating that the observed reflections represent authentic reasoning rather than learned surface patterns.\n\n3. The framework relies heavily on GPT-4o-generated reflective reasoning, which may introduce distributional biases or stylistic artifacts specific to GPT-4o. This raises concerns about generalization to other model families and real-world adversarial scenarios."}, "questions": {"value": "1. Would reflective reasoning be integrated into PPO for further safety gains?\n\n2. What is the actual increase in reasoning tokens and inference time compared to standard DPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dDnI4xNSfi", "forum": "oql27GfyPD", "replyto": "oql27GfyPD", "signatures": ["ICLR.cc/2026/Conference/Submission11130/Reviewer_erxr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11130/Reviewer_erxr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914439406, "cdate": 1761914439406, "tmdate": 1762922301088, "mdate": 1762922301088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReAlign, a reflective safety alignment framework for LLMs, aiming to balance harmlessness and helpfulness. The authors point out that existing alignment methods suffer from under-generalization and over-alignment. They address these issues via two stages, Reasoning-style Warmup (RW) and Self-reflective Reasoning Process Optimization (SRPO). Experiments across diverse safety and general benchmarks demonstrate that ReAlign significantly improves robustness against jailbreaks and reduces over-refusals without degrading general performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper identifies a real trade-off between harmlessness and helpfulness in safety alignment. The problem setup is clear and reasonable.\n* Authors provide thorough empirical evaluations across multiple safety benchmarks and general benchmarks, strongly supporting the effectiveness of the proposed method. The results are strong, demonstratring well-balanced performance on safety and utility.\n* The paper performs a sufficient discussion on the approach with ablation studies and qualitative analyses."}, "weaknesses": {"value": "* While the paper mentions that the “data generator” in ReAlign can be implemented by the target model itself, the experiments in practice rely heavily on GPT-4o to produce reasoning-style safety data and reflective trajectories. This raises a concern whether the satisfactory results come from high-quality data by a superior model or the method itself. There should be a study on how the method performs with other data sources or at least a discussion on this aspect.\n* The paper describes its approach as “safety-policy-driven”, but the details of policy are not formally defined. It is unclear what are the policies and how they interacts with the reasoning process. Moreover, a recent work [1] explicitly introduces a policy-based safety reasoning framework, which has not been included. \n* As there have been some papers studying safety alignment of Large Reasoning Models (LRMs), it is straightforward to think whether this method is applicable to LRMs. Since ReAlign itself builds upon reasoning-based safety alignment, it would be natural and valuable to analyze its relationship with such works to better define the position and contributions of this paper. Some relevant papers [2,3,4,5] are listed below.\n\n[1] ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning\n\n[2] SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities\n\n[3] STAR-1: Safer Alignment of Reasoning LLMs with 1K Data\n\n[4] RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability\n\n[5] SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5rPjxTS1OA", "forum": "oql27GfyPD", "replyto": "oql27GfyPD", "signatures": ["ICLR.cc/2026/Conference/Submission11130/Reviewer_Do6G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11130/Reviewer_Do6G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762067405271, "cdate": 1762067405271, "tmdate": 1762922300165, "mdate": 1762922300165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ReAlign, a reflective safety alignment framework for LLMs that aims to balance harmlessness and helpfulness. The approach consists of two stages: (1) Reasoning-style Warmup (RW) that enables LLMs to internalize long-chain reasoning capability, and (2) Self-reflective Reasoning Process Optimization (SRPO) that promotes reflection and correction during reasoning. The authors also claim their method reduces over-refusal while maintaining safety performance across multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses the practical problem of balancing safety and helpfulness in LLM alignment.\n\n- The probe-based analysis provides some insights into model behavior during alignment.\n\n- The writing is generally clear and the experiments cover multiple benchmarks."}, "weaknesses": {"value": "- **Limited Core Novelty.** The core contribution of this work is limited to data construction and reformatting. The two-stage training process (reasoning-style warmup + preference optimization) lacks genuine methodological innovation. Reasoning-style fine-tuning has been widely explored in recent reasoning model literature, and stepwise decomposition of reasoning chains with reflection is not a novel concept. The paper essentially applies existing techniques (SFT + DPO) on newly constructed safety-oriented reasoning data. While data construction can be valuable, the novelty threshold for a top-tier venue requires more substantial methodological contributions. The authors need to clearly articulate what is fundamentally new beyond \"collecting safety reasoning data and applying standard fine-tuning methods.\"\n\n- **Suboptimal Safety Performance Compared to SOTA.** Table 2 shows that ReAlign's safety performance is substantially worse than STAIR across most metrics. For example, on LLAMA3.1-8B-IT, STAIR achieves an ASR of 1.95% on WildJailbreak, while ReAlign achieves 4.95%, more than double the attack success rate. Similarly, on jailbreak attacks (SGB variants), STAIR consistently outperforms ReAlign by significant margins. While the authors argue that ReAlign achieves better balance by reducing over-refusal, the primary objective of safety alignment is to ensure safety first. A method that sacrifices substantial safety gains for marginal improvements in helpfulness or over-refusal is questionable. \n\n- **Limited Experimental Scope.** The experimental evaluation is limited to models with small parameters. Why not evaluate on larger models, like 14B and 32B? More critically, given the recent popularity of large reasoning models and your contribution on reasoning data construction, why is there no verification on these reasoning models? Lacking these results is unconvincing.\n\n- **Poor Figure and Table Presentation.** The presentation quality significantly detracts from the paper's professionalism. All figures and tables contain extremely small text, which is not suitable for reading."}, "questions": {"value": "The questions are listed in the weaknesses, and this paper needs massive revisons in my belief."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vPLmoPBGws", "forum": "oql27GfyPD", "replyto": "oql27GfyPD", "signatures": ["ICLR.cc/2026/Conference/Submission11130/Reviewer_ezrr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11130/Reviewer_ezrr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762075911512, "cdate": 1762075911512, "tmdate": 1762922299540, "mdate": 1762922299540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}