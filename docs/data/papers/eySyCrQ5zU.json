{"id": "eySyCrQ5zU", "number": 7104, "cdate": 1758007801534, "mdate": 1763366548123, "content": {"title": "Provably Safe Representation Learning in CMDPs: A Primal-Dual Approach", "abstract": "We study representation learning in low-rank  Constrained Markov Decision Processes (CMDPs) with unknown dynamics, where the agent must maximize rewards under safety constraints. While representation learning has significantly advanced for unconstrained MDPs, its extension to CMDPs remains open due to the critical challenge of safe exploration under learned features, particularly concerning the management of soft constraint violation. In this work, we propose REP-PD, the first algorithm that provably integrates representation learning with policy optimization in low-rank CMDPs. By iteratively learning a low-rank transition representation via MLE and utilizing a composite Q-function tied to the unconstrained Lagrangian, REP-PD guides policy updates to balance reward maximization, exploration, and robust constraint adherence. Through this approach, REP-PD achieves a near-optimal policy with a sampling complexity bound independent of the state space dimension without prior feature knowledge. Notably, REP-PD's regret matches the lower bounds for unconstrained low-rank MDPs, achieving strong performance concerning soft constraint violation. We then consider a stronger hard constraint violation metric, where the agent must strictly satisfy constraints at all times, and propose REP-PD-hard by designing a novel policy optimization\nmodule. Our work thus provides a robust and theoretically grounded approach to representation learning in constrained reinforcement learning, with guarantees on bounded soft and hard constraint violation.", "tldr": "", "keywords": ["Representation Learning; Safe Representation Learning; Constrained Markov Decision Process"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79c58ccc89c95fae6e5689a698e9f3f456ba6e7a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the problem of representation learning in low-rank Constrained Markov Decision Processes (CMDPs), where the transition dynamics are unknown. The work focuses on developing principled algorithms that can efficiently learn in structured environments while adhering to safety or constraint requirements.\n\nThe authors propose REP-PD, the first provably efficient algorithm for low-rank CMDPs that simultaneously learns latent representations and optimizes policies. A key strength of the approach is that it achieves this while ensuring bounded constraint violations under the soft-constraint formulation, providing both theoretical guarantees and conceptual clarity on how representation learning interacts with constrained optimization in RL.\n\nFurthermore, the paper extends its analysis to address both soft and hard constraint violation settings, offering a comprehensive treatment of constrained learning under the low-rank assumption. This dual focus enhances the generality and practical relevance of the proposed framework, positioning it as a meaningful step forward in the study of safe and structured reinforcement learning."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is well-written and highly readable, with a clear logical flow throughout. Both the presentation of the main ideas and the structure of the proofs are easy to follow, making the technical content accessible and well-motivated.\n\nThe results are effectively highlighted and concisely summarized in the accompanying table, which greatly helps readers grasp the broader context and compare the performance and theoretical bounds across different methods. This clarity of presentation substantially enhances the paper’s readability and impact.\n\nMoreover, the paper addresses an important and timely research direction, representation learning in safe reinforcement learning, which is a critical step toward building more efficient and reliable RL systems. Investigating how representation learning principles can be integrated into safety-constrained RL frameworks is both theoretically meaningful and practically significant."}, "weaknesses": {"value": "It would greatly enhance the paper to include empirical results that complement the theoretical analysis. Even a small-scale or illustrative experiment could help demonstrate how the proposed method behaves in practice and validate the theoretical trends or assumptions. Empirical evidence would also provide readers with a clearer sense of the method’s applicability and robustness under realistic conditions.\n\nIn addition, it would be valuable to include an investigation of possible lower bounds to assess the tightness of the presented upper bounds. Establishing or discussing such lower bounds would complete the theoretical picture, helping to clarify whether the proposed rates are near-optimal or if there remains room for improvement."}, "questions": {"value": "It would strengthen the theoretical contribution if the paper could include or discuss lower bound results to demonstrate the tightness of the presented upper bounds. Such results would help clarify whether the current rates are minimax-optimal and would provide a more complete understanding of the overall theoretical landscape.\n\nCould you provide more intuition behind the K 3/4 convergence rate obtained in the hard-constrained safe RL setting? In particular, what factors lead to this exponent. \n\nDo you believe that the rate could potentially be improved to K 1/2 under stronger assumptions or refined analysis? A brief discussion of the main bottlenecks limiting this improvement would also be insightful.\n\nThe paper appears to rely on the availability of an MLE oracle, which may not always be accessible in practical implementations. Could you elaborate on how the algorithm could be designed or approximated in the absence of such an oracle, and what impact this would have on the theoretical guarantees and computational efficiency?\n\nIt would be helpful if the authors could comment on the computational complexity of the proposed algorithms, especially regarding the scaling with respect to the rank, feature dimension, and number of samples. A comparison with existing approaches would further clarify the practical feasibility of the method.\n\nHave you considered extending the framework to a model-free setting, where only the feature class Phi or density class mu (reference: Reinforcement Learning in Low-Rank MDPs with Density Features) is available? \n\nSuch an extension and discussion on this connection could help position the current work better within the broader literature on representation learning in low-rank RL."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "nan"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vqsSnMTgfE", "forum": "eySyCrQ5zU", "replyto": "eySyCrQ5zU", "signatures": ["ICLR.cc/2026/Conference/Submission7104/Reviewer_yRUS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7104/Reviewer_yRUS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760681759229, "cdate": 1760681759229, "tmdate": 1762919278677, "mdate": 1762919278677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on representation learning within low-rank Constrained MDPs (CMDPs) where the feature $\\phi$ is initially unknown. The authors introduce two Primal-Dual algorithms, REP-PD and REP-PD-hard, to learn a policy that minimizes regret while providing theoretical guarantees for both soft and hard constraint satisfaction on the utility function."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This is the first work to study low-rank Constrained Markov Decision Processes (CMDPs), eliminating the typical requirement for prior knowledge of the feature mapping $\\phi$ in standard linear MDPs.\n\n2. The paper provides comprehensive theoretical guarantees for the regret while successfully addressing both soft and hard  utility function constraints."}, "weaknesses": {"value": "1. The novelty of this work appears limited. As outlined in Section 3.1, the proposed algorithm relies on three primary components: Representation Learning, Uncertainty-Aware Exploration, and Constraint-Guided Policy Optimization with Primal-Dual Adaptation. The first two elements are standard techniques widely adopted in previous analyses of low-rank MDPs. Similarly, the primal-dual method is a well-studied approach in Constrained MDPs (CMDPs). Consequently, the paper seems to present an integration of existing methods from the low-rank MDP and CMDP literature to solve the constrained low-rank MDP problem, raising concerns about its fundamental novelty.\n\n2. A significant weakness lies in the lack of clarity regarding the definition and usage of the reward and utility functions within Algorithm 1 and Algorithm 2.The paper highlights a challenging scenario (Line 177) where the agent only receives bandit feedback. This necessitates the construction of estimations for the unknown reward and utility functions directly from this sparse feedback. However, the presented algorithms appear to directly utilize the actual functions, despite these functions being neither known nor specified as inputs to the algorithms.\n\nThe authors must explicitly clarify how the unknown reward and utility functions are estimated or approximated from the bandit feedback and then integrated into the Primal-Dual framework. Furthermore, a discussion is required to assess whether the uncertainty and estimation errors introduced by the unknown nature of these functions pose additional technical challenges to the regret and violation analysis of low-rank CMDPs."}, "questions": {"value": "1. The citations in lines 1423 and 1436 are unclear. What is the mean of \"refer to (author?)\"?\n\n2. The claim in line 370—that the sum over episodes is bounded via the elliptical potential lemma—appears to be improper.The standard elliptical potential lemma requires a fixed feature prefix (e.g., $\\Sigma_k =\\phi(s,a)\\phi(s,a)^{\\top} + \\Sigma_{k-1}$). However, in the context of low-rank MDPs, the estimated feature $\\hat{\\phi}_k$ changes over time, which directly affects the prefix feature and the corresponding covariance matrix. Therefore, the authors cannot directly invoke the standard elliptical potential lemma. Further explanation and derivation are necessary when dealing with time-varying estimated features."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gCbnqat9Wa", "forum": "eySyCrQ5zU", "replyto": "eySyCrQ5zU", "signatures": ["ICLR.cc/2026/Conference/Submission7104/Reviewer_kdsa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7104/Reviewer_kdsa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760899858055, "cdate": 1760899858055, "tmdate": 1762919278141, "mdate": 1762919278141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a new multi-objective Markov game (MOMG) framework, and the Pareto-NE and its weaker variants are proposed. Preliminary results are established, including the connection between MOMG and MORL, scalarize multi-objective to single-objective,and two-phase algorithm for learning pareto-Nash front."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1 The study on MOMG is rather scarce and this work initiates the study on MOMG, addressing the critical gap in multi-agent research.\n\n2 The framework is intuitive and the preliminary results are technically sound.\n\n3 This work proposes the first algorithm for learning Pareto-NE front, with finite complexity guarantee."}, "weaknesses": {"value": "Most results in this work are preliminary, and the technical contribution of learning equilibria in MOMG is incremental.\n\n1 The definition of PNE and its property is a natural extension of Pareto-optimal policy and its property in MORL.\n\n2 When traversing multi-objective to single-objective problem, the algorithm and its analysis are standard.\n\n3 The idea of studying Pareto CE and proposed the V-learning for learning CE are intuitive and well-studied in literature."}, "questions": {"value": "1 In Definition 3, Pareto domination is not well defined. \"i.e., there is no .,..\" should be replaced by mathematical inequalities.\n\n2 There lacks comparison between this work and MORL [q1]. In fact, the definition of Pareto dominance and scalarizing multi-objective are quite similar in those works.\n\n[q1] Traversing Pareto Optimal Policies: Provably Efficient Multi-Objective Reinforcement Learning\n\n3 Regarding Sec. 3.3, the rationale for introducing ESR, SER and their properties are not clear, and they are not quite relevant to the remaining of this work.\n\n4 Regarding Remark 1, it would be helpful to give concrete examples explain the subtleties in the main context. \n\n5 While the two-phase multi-player learning algorithm is intuitive, it suffers from \"high\" sample complexity. However, it seems that learning Pareto-NE front directly to reduce the sample complexity is impossible for generic reward vector. Reward structure certainly helps and is critical to further reduce the complexity. It would be appreciated if the author could comment on this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V5bSJeht2x", "forum": "eySyCrQ5zU", "replyto": "eySyCrQ5zU", "signatures": ["ICLR.cc/2026/Conference/Submission7104/Reviewer_SCmB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7104/Reviewer_SCmB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968618617, "cdate": 1761968618617, "tmdate": 1762919277689, "mdate": 1762919277689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies constrained markov decision process (CMDP) under low-rank assumption. It proposed two algorithms for learning CMDP under soft-constraint and hard-constraint separately. The paper also theoretically proved the regret and constraint violation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The study of CMDP under low-rank assumption is relatively new and significant. In particular, the proposed algorithm for hard constraint use a principled primal-dual mechanism with adaptive Lagrange updates."}, "weaknesses": {"value": "1. The definition and use of regret seems strange to me. The regret is supposed to be the accumulative sub-optimal gaps of deployed policy by the RL agent. However, in the algorithm, the actual policy deployed is a concatenation of $\\pi_k$ and uniform exploration. Thus, there is a significant concern about the validity of the regret and also the constraint violation proved in the paper."}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hStRAR4EvZ", "forum": "eySyCrQ5zU", "replyto": "eySyCrQ5zU", "signatures": ["ICLR.cc/2026/Conference/Submission7104/Reviewer_Mp9D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7104/Reviewer_Mp9D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977803981, "cdate": 1761977803981, "tmdate": 1762919277023, "mdate": 1762919277023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}