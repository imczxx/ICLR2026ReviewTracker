{"id": "9AbJO130G8", "number": 15802, "cdate": 1758255415306, "mdate": 1759897281116, "content": {"title": "Missingness Bias Calibration in Feature Attribution Explanations", "abstract": "Popular explanation methods often produce unreliable feature importance scores due to \"missingness bias\", a systematic distortion that arises when models are probed with ablated, out-of-distribution inputs.\nExisting solutions treat this as a deep representational flaw that requires expensive retraining or architectural modifications.\nIn this work, we challenge this assumption and show that missingness bias can be effectively treated as a superficial artifact of the model's output space.\nWe introduce MCal, a lightweight post-hoc method that corrects this bias by fine-tuning a simple linear head on the outputs of a frozen base model.\nSurprisingly, we find this simple correction consistently reduces missingness bias and is competitive with, or even outperforms, prior heavyweight approaches across diverse medical benchmarks spanning vision, language, and tabular domains.", "tldr": "Model calibration can fix missingness bias in feature attribution explanations", "keywords": ["explainability", "feature", "attribution", "calibration", "missingness", "bias", "medical", "medicine", "LLMs", "Machine Learning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aca325ec634368b680d1d323342264a607c0e8f6.pdf", "supplementary_material": "/attachment/ae0767542c7d43b2bf0793d26db73258d87e3b25.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents MCal, a method that addresses the missingness or out-of-distribution (OOD) bias inherent in perturbation-based XAI methods. Unlike alternative approaches, MCal does not require computationally expensive retraining or architectural modifications. The paper demonstrates quantitatively that MCal produces improved explanations, provide comparative analysis against baseline methods, and verify that the approach preserves overall model accuracy."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed solution is elegantly simple yet well-motivated and thoroughly evaluated, though this simplicity could raise questions about the scope of the contribution.\n- The method exhibits modality- and architecture-agnostic properties, being largely model-agnostic depending if the logins can be accessed.\n- The manuscript is very well-structured and clearly written, facilitating reader comprehension through effective organization.\n- The paper provides a pip-installable code repository, enhancing practical accessibility and reproducibility."}, "weaknesses": {"value": "**W1:** Medical images, particularly in radiology and histopathology, exhibit relatively low inter-patient visual variability, making OOD examples straightforward to generate. But how is this for natural images? As these are more diverse, I suspect less sensitivity to such missingness biases through the perturbations.\n\n**W2:** The evaluation relies exclusively on two metrics that are known to potentially introduce bias. The addition of qualitative visualizations comparing saliency maps before and after debiasing would significantly enhance reader understanding and provide practical insight into the method's real-world performance.\n\n**W3:** Given the method's claimed model-agnostic nature, evaluation or at minimum code implementation with closed-source API-based models would strengthen the work. Such demonstrations, contingent on API access to model logits, would underscore the method's relevance for the broader research community that relies predominantly on closed-source models."}, "questions": {"value": "See Weaknesses W1 - W3.\n\nComment: An error.txt file remains in the code repository (maintaining anonymity)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GiHQvDH13N", "forum": "9AbJO130G8", "replyto": "9AbJO130G8", "signatures": ["ICLR.cc/2026/Conference/Submission15802/Reviewer_3ZQX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15802/Reviewer_3ZQX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760827157738, "cdate": 1760827157738, "tmdate": 1762926034283, "mdate": 1762926034283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel method called MCal, which aims to mitigate \"missingness bias\" in model explanations. Missingness bias occurs when input features are ablated or removed in explanation methods, leading to biased predictions that undermine the reliability of feature importance scores. The authors propose MCal, a lightweight, post-hoc calibration technique that corrects missingness bias by fine-tuning a linear head on the outputs of a frozen base model. The method is model-agnostic, cost-effective, and offers strong theoretical guarantees. The results demonstrate that MCal outperforms more complex, computationally expensive methods across a variety of domains, including medical image classification and natural language processing tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Practicality and Efficiency: MCal is lightweight and does not require extensive computational resources, making it more accessible than existing solutions that require retraining or architectural adjustments. The use of a linear transformation and a cross-entropy loss function ensures minimal overhead.\n2. Theoretical Guarantees: The paper offers strong theoretical guarantees of convergence for the proposed method, ensuring that the calibrator will consistently find an optimal solution. This gives confidence in the reproducibility and stability of the approach.\n3. Empirical Validation: The authors provide extensive experimental results demonstrating MCal's effectiveness across diverse medical domains (vision, language, and tabular). It outperforms other methods such as retraining and architecture-based solutions, making it a compelling baseline for addressing missingness bias."}, "weaknesses": {"value": "1. **Lack of Real-World Application Scenarios**: While the method is demonstrated on several benchmarks, the paper lacks concrete real-world use cases where MCal is applied to solve specific problems. Providing experimental results from a more tangible application would strengthen the paper’s argument and demonstrate the method's practical value in more complex scenarios.\n\n\n2. **Lack of Discussion on the Training Process**: The authors mention that the calibration process requires training a new parameter. However, if the training dataset is too small, the model may overfit, while larger datasets may increase computational costs. The paper could benefit from a discussion and experimental analysis of how the choice of training data size affects the performance of MCal and its scalability."}, "questions": {"value": "1. Could you provide a detailed case study or experiment in a real-world application, such as medical diagnostics where MCal has been implemented to solve a specific problem?\n\n2. In situations with large class spaces (e.g., language models), how do you prevent overfitting during calibration?\n\n3. How does the size of the training dataset affect the performance of MCal? What was the specific training time of your experiments? How do you balance the data size and the computational cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QyVmP1hYNS", "forum": "9AbJO130G8", "replyto": "9AbJO130G8", "signatures": ["ICLR.cc/2026/Conference/Submission15802/Reviewer_ojnE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15802/Reviewer_ojnE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817486297, "cdate": 1761817486297, "tmdate": 1762926033867, "mdate": 1762926033867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a straightforward plugin to enhance explanation quality, which is clearly described and compatible with perturbation-based feature attribution methods. The authors propose to append the to-be-explained model with an additional dense layer, and fine-tune the added layer with the expected manipulations on the input samples while keeping the major part of the model frozen. The proposed solution is intuition-guided and empirically evaluated under small-scale settings, and the experimental results have an emphasis on reporting model performances rather than the intended explanation quality improvement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well-motivated and self-contained, with a clear description of the proposed method.\n- The described plugin is flexible and can be appended to arbitrary models regardless of input modalities or model architectures.\n- The empirical results demonstrate the capability of MCal in aligning the predictions on manipulated inputs with the standard model outcomes, supporting the method design."}, "weaknesses": {"value": "- The discussion on the central motivation is insufficient. Particularly, line 120 states that “masking non-critical regions …”, yet it is arguable that considering certain regions as “critical” already injects human inductive biases. If the model truly learned to look at a meaningless/wrong region, would this additional “correction” for the missingness bias consequently cover up its problematic behavior?\n- While the objective of the paper is clearly stated, the line of discussion deviates from how the added component will enhance explanation quality, instead leaning towards the effectiveness in resolving missingness bias. The modification, which can be considered a form of data augmentation, is intended to facilitate the derivation of more faithful explanations; however, the designed experiments focus more on the impact of this augmentation on model stability and do not sufficiently demonstrate how the modification leads to better explanations.\n- In fact, this paper consistently mixes the discussion on feature attributions with model robustness. Although related in some ways, they are different subjects, where the missingness bias is a particular challenge that feature attribution methods are facing, and robustness is an inherent property of a model. A properly designed explainer should be able to faithfully reflect model behaviors, no matter whether it is robust or not. Mixing these two topics loses the concentration on explainability, and particularly disconnects the latter part of the discussion in this paper from its stated motivation.\n- The faithfulness of the derived explanations becomes questionable given the added dense layer. While adapting the tested model to the disturbed manifold, it arguably also changes the model behavior as a whole.\n- Some experimental settings and result interpretations are questionable; see questions for more details."}, "questions": {"value": "- See the first point in Weakness.\n- Could the authors better explain the objective of this paper? What are the takeaways of the stability test on the calibrated models? How do they support the claim of improved explanation quality?\n- Do the explanations generated on the calibrated model still faithfully reflect the behavior of the original model?\n- Figure 5 presents the only results that are relevant to explanation quality. Could the authors elaborate on the interpretation of the sensitivity metrics reported there? Given the focus is on explanation quality, the sensitivity measure should reflect the effectiveness of an explainer in identifying the most relevant features. Excluding them should lead to a larger drop in prediction confidence (as stated in line 726); thus, a higher sensitivity score should indicate more effective explanations. In this sense, the results are in favor of explanations from uncalibrated models, which contradicts the interpretation presented in the paper. Also, I do not follow the argument related to model robustness and its connection to the evaluation of explanation quality.\n- Could the authors clarify the different training settings for retrain and MCal? Particularly, why is the retraining scheme fitted to a different ablation distribution than MCal? MCal was fine-tuned exactly for the stratified mask sampling that is exactly used for the validation, which has a balanced distribution of ablation fractions, whereas the retraining scheme only sees the results of uniformly ablated inputs, lacking observations of rich and rare feature presences (e.g., ablation fractions of 2/16 and 14/16 are rarely possible to be sampled). I’m surprised and, in fact, skeptical about the conclusion due to the misaligned ablation strategies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZcWN1its11", "forum": "9AbJO130G8", "replyto": "9AbJO130G8", "signatures": ["ICLR.cc/2026/Conference/Submission15802/Reviewer_3Csh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15802/Reviewer_3Csh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834161640, "cdate": 1761834161640, "tmdate": 1762926033370, "mdate": 1762926033370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a calibration technique to mitigate the missingness bias problem encountered when finding feature importance scores. The technique proposed is lightweight and requires access only to logits. They evaluate their technique on various medical data across vision, language, and tabular modalities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The proposed calibration technique is lightweight, not requiring retraining, access to weights, or architectural modifications of the original model.\n2) The proposed technique can be well adapted to common perturbation-based explainability techniques.\n3)The paper is generally well written, easy to follow, and the motivation is well presented."}, "weaknesses": {"value": "1)In baseline comparisons, for the replace and retrain categories, the paper uses basic techniques instead of comparing against more advanced techniques in those categories. For example, in the retrain category, some of the recent techniques for imputation, like ROAD, GOAR, are not compared against. Since these are relevant methods to tackle the missingness bias, it would be better to compare them. \n2)The paper claims the missingness bias is a superficial artifact, while the original model embeddings might still be facing this issue.\n3)Some of the crucial details of the experiments are missing. For Figure 7, it isn’t clear if the accuracy is computed using ablated or clean input.\n4)The details of input ablation rates for the baselines in Table 1 are not mentioned, while the proposed method reports an average of calibrators conditioned on various ablation rates."}, "questions": {"value": "Based on the argument in the paper that missingness bias can cause class distributional shift, couldn’t the ablation of seemingly unrelated or spurious features, which the model might rely o,n contribute to this distributional shift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eenGBRPgUN", "forum": "9AbJO130G8", "replyto": "9AbJO130G8", "signatures": ["ICLR.cc/2026/Conference/Submission15802/Reviewer_Tchr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15802/Reviewer_Tchr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925919825, "cdate": 1761925919825, "tmdate": 1762926032968, "mdate": 1762926032968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The objective of this paper is to enhance explanation quality by resolving the missingness bias, which is a consequence of distribution shifts caused by input manipulations during the explanation procedure. The motivation is clearly stated, and the proposed solution, while intuitive, is reasonable. However, the discussion throughout the paper consistently mixes the pursuit of better explanation quality with model robustness, which are two distinct topics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is straightforward and well-presented, but the analyses and discussions on its impacts lack depth."}, "weaknesses": {"value": "- In fact, if taking a data augmentation perspective, the proposal of this work is not brand new.\n\n- While the experiments focus on elaborating how MCal corrects the missingness bias, the results fail to show how the calibration on model outcomes affects explanation quality. There is no qualitative example that illustrates the differences, nor quantitative assessments following standard regimes that effectively compare explainers under different settings."}, "questions": {"value": "- The validity of the conclusion from the observations is somewhat questionable. For example, Figure 5 claims that “calibrated models have better explanations”; however, two versions generally show the same perturbation tendencies. The differences in magnitude appear to be more relevant to the changed model behavior due to the additional dense layer. Additionally, it is noteworthy that the two tested models have different functionalities due to the appended dense layer in one version. It is unclear how the results from different explanations on different models can be effectively compared.\n\n- The results presented by the middle chart of Figure 7 appear suspicious. Why is the accuracy a constant with increasing ablation fraction for Conditioned MCal? An ablation rate of up to 90% is very likely to remove all informative features in an input. How does the model manage to maintain the accuracy without “seeing” anything relevant?\n\n- This is more about a question that is relevant to the previous point. The example in Figure 2 motivates a correction of model outcomes when the relevant region is not masked, but what should be the target label if all relevant regions are masked out? Enforcing the output of disease on a manipulated input without indications is arguably another form of “skewed” prediction, particularly when the model originally predicts healthy correctly. I think the argument of this paper only builds upon one side of the coin and leaves the other side insufficiently discussed, and the relevant difficulty unsolved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MxYtBLRXoh", "forum": "9AbJO130G8", "replyto": "9AbJO130G8", "signatures": ["ICLR.cc/2026/Conference/Submission15802/Reviewer_7kkY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15802/Reviewer_7kkY"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989582410, "cdate": 1761989582410, "tmdate": 1762926032636, "mdate": 1762926032636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}