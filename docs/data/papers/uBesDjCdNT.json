{"id": "uBesDjCdNT", "number": 8713, "cdate": 1758095692056, "mdate": 1759897768150, "content": {"title": "Sequential Least-Squares Estimators with Fast Randomized Sketching for Linear Statistical Models", "abstract": "We propose a novel randomized framework for the estimation problem of large-scale linear statistical\nmodels, namely Sequential Least-Squares Estimators with Fast Randomized Sketching (SLSE-FRS),\nwhich integrates Sketch-and-Solve and Iterative-Sketching methods for the first time. By iteratively\nconstructing and solving sketched least-squares (LS) subproblems with increasing sketch sizes to\nachieve better precisions, SLSE-FRS gradually refines the estimators of the true parameter vector,\nultimately producing high-precision estimators. We analyze the convergence properties of SLSE-FRS,\nand provide its efficient implementation. Numerical experiments show that SLSE-FRS outperforms\nthe state-of-the-art methods, namely the Preconditioned Conjugate Gradient (PCG) method, and the\nIterative Double Sketching (IDS) method.", "tldr": "", "keywords": ["Iteration Method", "Large Scale", "Least-Squares Optimization", "Linear Statistical Model", "Randomized Sketching"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/923ba0981dca44564488a6ab8e834a7f1d8f6eab.pdf", "supplementary_material": "/attachment/af5f133b3ae24d0b103cb407013e841b49b3c8e7.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents Sequential Least-Squares Estimators with Fast Randomized Sketching (SLSE-FRS) for large-scale linear statistical models. The method appears to be the first to unify Sketch-and-Solve with Iterative-Sketching. It enlarges the sketch size progressively and solves a sequence of sketched least-squares subproblems, which incrementally refines the estimate of the true parameter vector. The authors also provide a systematic treatment of convergence behavior and computational complexity."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The main contribution is the novel integration of Sketch-and-Solve and Iterative-Sketching within a single framework. \n* The analysis is comprehensive:\n   1)  it explains how to construct the sequence of sketched subproblems to balance estimation accuracy with computational cost;\n   2) it proposes a theoretically sound and computationally tractable stopping criterion to achieve optimal iterative accuracy;\n  3) it establishes, both in theory and in experiments, that the estimator attains noise-level accuracy comparable to ordinary least squares (OLS)."}, "weaknesses": {"value": "1) The *o(1)* term in equation (11) of Theorem 4.2 is not quantified. Moreover, letting $M \\to \\infty$ appears at odds with the paper’s non-asymptotic stance. In line with IDS-style results, please specify the scale of $M$ (e.g., explicit bounds or rates). Without such quantification, the result is not fully convincing.\n\n2) Aside from the added momentum component, the proposed algorithm is essentially equivalent to the two-stage process used in IDS. Could the authors explicitly and theoretically demonstrate how much improvement their method achieves over IDS?\n\n3) Theorem 4.4 provides a complexity analysis but does not present an explicit convergence rate. As a result, it is not immediately clear how the proposed algorithm achieves the claimed convergence–complexity trade-off."}, "questions": {"value": "1) It is recommended that the authors cite the following works for completeness:\nDerezinski, Michal, et al. “Newton-LESS: Sparsification without trade-offs for the sketched Newton update.” Advances in Neural Information Processing Systems, 34 (2021): 2835–2847.\nGarg, Sachin, Kevin Tan, and Michał Dereziński. “Distributed least squares in small space via sketching and bias reduction.” Advances in Neural Information Processing Systems, 37 (2024): 73745–73782.\nThese works emphasize that bias removal can significantly improve estimation accuracy and convergence performance.\n\n2) The paper compares the proposed method with M-IHS and shows that the second-stage procedure improves performance over M-IHS. However, it lacks an ablation comparison between SLSE-FRS and the estimator (\\boldsymbol{\\beta}_T) obtained using only the first-stage iterative form (7). It is recommended to include such an ablation study to better isolate and demonstrate the contribution of the second stage.\n\n3) Could the authors clarify the purpose of the condition number  $ \\kappa $ used in the experiments? Specifically, how is it defined and what role does it play in the analysis or performance evaluation?\n\n4) The paper presents numerical experiments showing that the proposed SLSE-FRS method outperforms IDS in terms of the convergence–complexity trade-off. However, it lacks an explicit, formula-based comparison of the computational complexities of SLSE-FRS and IDS. Providing such a comparison would make the claimed improvement more transparent and convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0TsGXMNqvK", "forum": "uBesDjCdNT", "replyto": "uBesDjCdNT", "signatures": ["ICLR.cc/2026/Conference/Submission8713/Reviewer_dNyU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8713/Reviewer_dNyU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8713/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746154782, "cdate": 1761746154782, "tmdate": 1762920516098, "mdate": 1762920516098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a two-stage framework that addresses a sequence of \u0013increasingly large\u0014 sketched least-squares subproblems, followed by a few full LS iterations to achieve OLS-level accuracy. The authors provide a contraction guarantee (under specific SRHT-based conditions and momentum selection) and an implementation using M-IHS. Experiments demonstrate faster convergence than traditional random sketched least square methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "$\\textbf{Clear two-stage recipe.}$ Stage-1 builds $K$ sketched LS problems with growing sketch sizes and uses those solutions to warm-start Stage-2 on full data; the framework is solver-agnostic and explains the cost savings intuitively. \n\n $\\textbf{Convergence statement and practical parameters.}$ Theorems provide an exponential decay bound and a global contraction rate for suitable $(\\mu,\\eta)$, with a data-driven surrogate stopping rule (via $\\omega$) and a lower bound on the per-subproblem iteration count $a_i$. \n\n$\\textbf{Empirical speedups.}$ Plots/tables indicate faster time-to-accuracy than IDS and PCG on synthetic settings; M-IHS comparisons are also included."}, "weaknesses": {"value": "$\\textbf{Scope and assumptions feel narrow.}$ The method is largely a combination of standard components—Sketch-and-Solve least squares, iterative Hessian sketching (IHS), SRHT/CountSketch embeddings, and preconditioned Richardson/gradient iterations. The contribution appears only to lie in how these blocks are combined.\n\n$\\textbf{Literature position.}$ The authors need to clarify whether similar “sketch-warmstart + full-data polish” patterns have ever been explored, and articulate the specific differences with this literature.\n\n$\\textbf{Ablations.}$ Due to the combination nature of the paper, I expect more ablation studies to be performed to determine which components are the key to the success of the performance. See questions.\n\n$\\textbf{Unclear synthetic settings.}$ Experiments have some unclear and unexplained settings, see questions.."}, "questions": {"value": "1. To better evaluate the paper's “integration-and-scheduling” perspective is reasonable, but novelty claims should be framed accordingly. Clear positioning is recommended by clearly stating that each component is established.\n\n2. Why do the simulation settings have unrealistically small noise level $\\sigma^2=1e-8$? This narrows the problem, only placing the problem in an ultra–high-SNR regime. Adding a noise sweep is necessary.\n\n3. The synthetic data description is under-specified: while $X$ is said to be i.i.d. Gaussian with an “artificially adjusted” condition number and \n$\\beta$ is Gaussian, the paper does not describe the actual procedure to impose a target condition number $\\kappa$ on $X$ (e.g., singular-value planting vs. diagonal scaling), nor whether columns are standardized, whether an intercept is used, or how \n$X$ is normalized before sketching.\n\n4. Additional ablations are necessary. e.g., are there differences between one-shot big sketch vs. the schedule? How about the comparison between fixed estimated Hessian vs. updating after several iterations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mW2VMBPrky", "forum": "uBesDjCdNT", "replyto": "uBesDjCdNT", "signatures": ["ICLR.cc/2026/Conference/Submission8713/Reviewer_cy5G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8713/Reviewer_cy5G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8713/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765529557, "cdate": 1761765529557, "tmdate": 1762920515695, "mdate": 1762920515695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors provide a randomized algorithm for the least squares problem. The algorithm is simple, and amounts to repeatedly performing a sketch-and-solve procedure with a doubling sketch size until some tolerance is achieved. In practice, this is done with a sketched Hessian + sketched gradient update. After which, the authors perform sketched Hessian updates on the full gradient. The authors show a high-probability guarantee for the first stage, and an asymptotic guarantee for the second. The number of iterations of the first stage is lower bounded by a logarithmic term in the reciprocal of the tolerance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of likening a series of sketch-and-solve solutions to an iterative sketched Hessian + sketched gradient method for computing an approximate least squares solution is nice and may be original. The proofs are rigorous. The writing within the paper is above average in quality. If the algorithm works well in practice, it could improve the performance of libraries for randomized methods for least squares by improving the warm-starts given to sketch-and-precondition solvers."}, "weaknesses": {"value": "1. The algorithm is very simple, and amounts to using the doubling trick to determine the sketch size that should be employed when performing a sketch-and-solve operation. The authors solve each sketch-and-solve problem along the way efficiently via sketching the data, computing the sketched gradient, and then performing a sketched Hessian update on the coefficient obtained from the last iteration with the sketched gradient. Once the optimal sketch size is found, the authors then employ a sketched Hessian update on the full gradient. \n- The problem with this is that the authors ultimately still need a sketch size $r$ on the order of $d/\\epsilon^2$, as seen in Theorems 4.1 and 4.2. In a sense, we already know the necessary sketch size, or at least its order. The doubling trick allows for only a logarithmic inflation, but it is difficult to see this as anything more than an engineering tweak. \n- After this, a sketched Hessian update is employed, that obtains an asymptotically linear convergence rate. I say asymptotically because the second term only goes to zero asymptotically at an unknown rate, and it is unclear how large the \"constant\" $M$ should be. This guarantee is not very sophisticated, and I believe the proof of this is folklore. \n\n2. The paper does not compare the runtime of its method to other papers within the literature that solve the least squares problem with sparse sketches, e.g. Garg et al. (2024), Chenakkod et al. (2024), Anari et al. (2022), etc. One can generally expect to solve the problem (of the first stage) in $\\text{nnz}(A) + O(d^2/ / \\epsilon)$ time with sparse sketches. It is odd that the procedure does not yield an improvement from $O(Nd)$ to $\\text{nnz}(A)$. I suspect that this is an issue with the analysis. \n- In Theorem 4.2 the authors rely on a large constant $M$ and ultimately employ an asymptotic analysis of the runtime, in contrast to the non-asymptotic analyses common in the literature. \n- Accordingly, the benefit of this rather complicated first stage is not evident in the guarantee for the second stage. If these guarantees are tight (I do not think they are), then there is no benefit to performing the first stage over simply initializing the sketch-and-precondition second stage with a sketch-and-solve solution. \n- As such, the numerical experiments are somewhat unfair to IDS and PCG -- they are not initialized with a sketch-and-solve solution, while SLSE-FRS almost is. \n\n\n### References\n- Garg et al. (2024), Distributed Least Squares in Small Space via Sketching and Bias Reduction\n- Chenakkod et al. (2024), Optimal Embedding Dimension for Sparse Subspace Embeddings\n- Anari et al. (2022), Optimal sublinear sampling of spanning trees and determinantal point processes via average-case entropic independence."}, "questions": {"value": "See weaknesses. This score is somewhat harsh. At the moment, I am on the fence between a 2 and a 4, and am willing to increase my score if I am proven wrong or my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PZDENS2CXF", "forum": "uBesDjCdNT", "replyto": "uBesDjCdNT", "signatures": ["ICLR.cc/2026/Conference/Submission8713/Reviewer_jZBq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8713/Reviewer_jZBq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8713/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861235738, "cdate": 1761861235738, "tmdate": 1762920515318, "mdate": 1762920515318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}