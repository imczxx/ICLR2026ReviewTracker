{"id": "3tm37YNMdl", "number": 5245, "cdate": 1757875843560, "mdate": 1763698057515, "content": {"title": "CUMath: A Benchmark and Evaluation Framework for LLMs on Mathematical Reasoning in Undergraduate Computational Math", "abstract": "Large Language Models (LLMs) perform well on popular math benchmarks but still struggle with fundamental undergraduate tasks such as basic integrals. This suggests a diagnostic gap: existing datasets are either trivial, synthetic, or overly advanced, limiting their usefulness for exposing reasoning failures. To address this, we introduce CUMath, a benchmark of 2,100 real problems from undergraduate courses in Calculus, Linear Algebra, Differential Equations, and related fields. Each problem includes step-by-step solutions, enabling evaluation of both final answers and intermediate reasoning. Moreover, current evaluations treat accuracy and reasoning separately, overlooking their joint role in problem-solving. To address this, we propose a multi-layered evaluation framework that combines automatic metrics with an LLM-as-a-grader pipeline, integrating symbolic encoding and external verification. Using this setup, we evaluate 15 LLMs across various prompting strategies. Our results show that even advanced models often misuse symbolic methods and rely on shortcuts, leading to polished but flawed solutions. Our findings reveal the ongoing issue of inconsistent reasoning, highlighting the need for improved benchmarks, evaluation frameworks, and the development of models with enhanced consistency and reasoning capabilities. The code and data will be available upon publication.", "tldr": "", "keywords": ["Large Language Models", "Symbolic Reasoning", "Mathematical Reasoning", "Evaluation Framework", "Evaluation Metrics", "Error Analysis", "Reasoning Failures"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6316d3011f5e925d5bd1e2766596fe9a47721ca6.pdf", "supplementary_material": "/attachment/b91ff3cf143574a150abb41dcf4cecb038dd608f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a dataset called CUMath which can be used as a benchmark of 2,100 real problems from undergraduate courses in Calculus, Differential Equations, Discrete Mathematics, Linear Algebra, Multivariable Calculus, Precalculus, and Trigonometry. Each problem includes step-by-step solutions, enabling evaluation of both final answers and intermediate reasoning. It categorize the problems into three answer formats: Free Response (FR), Short Answer (SA), and True/False (TF). \n\nThe motivation behind releasing this dataset stems from the observation that current generation LLMs perfom well on existing popular math benchmark dataset but still they strggle with math reasoning. \n\nFurther, this paper proposes a multi-layered framework to jointly evaluate answer accuracy and the reasoning of the model.  This framework combines automatic metrics with an LLM-as-a-grader pipeline. \n\nThrough the propsed CUMath dataset and evaluation framework, this paper shows that SOTA LLMs make mistakes in the symbolic manipulation and procedural reasoning even when prducing final correct answer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The dataset is balanced across sever core subjects of the Maths so that no single subject dominates or remains underrepresented.\n- This paper proposes a multi-layered evaluation framework where it combines MathBERT for symbolic encoding, LLM for step-level reasoning assessment, and Wolfram Alpha for answer verification. This pipelines, thus, captures both answer correctness and reasoning quality. \n- The proposed benchmarking dataset in this paper would be valuable towards advancing the SOTA of LLM’s reasoning abilities.\n- Sections 6.1 and 6.2 provide interesting insights about LLMs behavior at large when they fail in math reasoning."}, "weaknesses": {"value": "- The technical novelty of the paper is limited but that is understandable because it is more of a data set contribution paper.\n- It will be good to see the quantitative comparison of the proposed Dataset+Eval framework against well known Math reasoning dataset under the same proposed eval framework and the same set of models. This will help readers buy the key selling points of the paper. See my comment in the Questions section also."}, "questions": {"value": "- In Section 4.1, why use two different notations $\\hat{s}_i^j$ and $e_i^j$ for the same thing.\n- In Line 228, the quantity $m_k(e_i)$ is not defined.\n- In Line 269, it is written that “The encoded steps are passed to an LLM..” I was wondering how do you pass embeddings to an LLM? Can you elaborate? My understanding is that you are passing embeddings obtained from MathBERT (in Step 2) to an LLM in Step 3.\n- In Table 2, it will be good if you can also add the performance of these models for some of the popular Math reasoning datasets but using your eval framework. This will help readers buy the points that your trying to drive home."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sNhZ4Kdb2F", "forum": "3tm37YNMdl", "replyto": "3tm37YNMdl", "signatures": ["ICLR.cc/2026/Conference/Submission5245/Reviewer_Ckz3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5245/Reviewer_Ckz3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761188558307, "cdate": 1761188558307, "tmdate": 1762917968806, "mdate": 1762917968806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to address two core issues in evaluating the mathematical reasoning capabilities of Large Language Models (LLMs) at the undergraduate level: (1) existing benchmarks are either too elementary or too advanced, lacking diagnostic value for reasoning failures, and (2) current evaluation paradigms often decouple final answer accuracy from the quality of the reasoning process. To this end, the authors introduce CUMath, a new benchmark of 2,100 problems from real undergraduate computational math courses, with each problem annotated with step-by-step solutions. Concurrently, they propose a multi-layered evaluation framework that combines automatic metrics with an \"LLM-as-a-grader\" pipeline, which is augmented with external tools like MathBERT and Wolfram Alpha for verification. Through an evaluation of 15 LLMs, the authors conclude that even frontier models exhibit systematic errors in symbolic manipulation and procedural reasoning, arguing for the necessity of integrated evaluations that assess both reasoning validity and answer correctness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper accurately identifies a critical gap in the current landscape of LLM evaluation. As models approach saturation on benchmarks like GSM8K, there is a pressing need for more challenging, realistic, and diagnostically useful benchmarks. The focus on undergraduate mathematics is an excellent choice for a domain that can effectively distinguish between superficial fluency and deep reasoning abilities.\nThe construction of the CUMath dataset is a solid and meaningful effort. Its grounding in authentic instructional materials (quizzes, exams, textbooks) ensures the practical relevance of the problems. Most importantly, providing detailed step-by-step solutions is crucial for enabling fine-grained analysis of where models' reasoning chains fail, which will greatly benefit future research in this area.\n\nThe authors' advocacy for an integrated assessment of \"answer correctness\" and \"reasoning quality\" is insightful. Highlighting the evaluation blind spot of \"correct answers derived from flawed reasoning\" demonstrates a deep understanding of the limitations of current LLM evaluation. The conceptual direction of the proposed multi-layered framework, which attempts to synthesize automated metrics with qualitative LLM-based feedback, is both correct and worthy of exploration."}, "weaknesses": {"value": "The authors repeatedly claim to evaluate \"state-of-the-art LLMs\" or \"frontier LLMs.\" However, the list of evaluated models (Table 12) primarily consists of older models such as GPT-3.5, an early version of GPT-4.1, and smaller-scale open-source models (e.g., LLaMA 3 8B/70B). Given the rapid pace of development in the field (and a target publication date of ICLR 2026), these models are no longer representative of the cutting edge. More recent and powerful reasoning models, such as the latest GPT and Claude series or other specialized math models, are conspicuously absent. This outdated selection invalidates the paper's main conclusion that \"even the strongest LLMs achieve an accuracy of less than 25%.\" A rigorous claim about the capabilities of \"frontier models\" must be substantiated by testing the models widely considered to be the most capable at the time of submission. Without such experiments, the observed failures could be limitations of the specific models tested rather than a general bottleneck for all LLMs.\nWhile the concept of a \"multi-layered evaluation framework\" is appealing, its components are largely direct applications or combinations of existing work (e.g., SRS from ROSCOE, VR from ReasonEval). The main claimed novelty, the \"LLM-as-a-grader\" pipeline, lacks the most critical piece of validation: there is no quantitative analysis comparing its outputs to those of human experts. A reliable automated grading system must demonstrate high inter-rater reliability (e.g., using Cohen's Kappa or Krippendorff's Alpha) with human graders. Without this evidence, the reliability and fairness of the LLM grader cannot be trusted, rendering the scores it produces (the \"LLM\" column in Table 2) unsubstantiated.\n\nThe framework's reliability is highly dependent on its automated preprocessing modules, especially the \"Math Segmentation\" component. As described in Section 4.2, this module relies on simple heuristics—looking for explicit \"step k\" markers and defaulting to \"line-based segmentation\" otherwise. This approach is extremely brittle when processing the free-form, structurally diverse outputs of LLMs. A single complex reasoning step can span multiple lines, and models may use different delimiters or none at all. Incorrect segmentation leads to cascading errors in all subsequent evaluation steps (e.g., semantic F1, SRS, LLM-as-a-grader). Yet, the paper provides no evaluation of this module's accuracy (e.g., against a human-annotated ground truth) nor does it discuss its fault tolerance or potential impact on the final results. This oversight regarding a core component's robustness casts serious doubt on the entire framework's practical usability."}, "questions": {"value": "1.\tCould you explain the decision to exclude more recent, top-performing models renowned for their mathematical reasoning abilities (e.g., the latest GPT-4 series, Claude 3 series)? Given that your central conclusion is about the upper-bound capabilities of \"frontier LLMs,\" how can this claim be supported by the current selection of models?\n2.\tDo you have any plans to conduct, or have you already performed, a study comparing the ratings from your \"LLM-as-a-grader\" pipeline against scores from human mathematics experts? Without such a comparison, how do you ensure the reliability and impartiality of the automated grader, preventing it from being merely a black box?\n3.\tHave you evaluated the accuracy of the \"Math Segmentation\" module? What is its error rate on LLM outputs that lack explicit step markers or follow non-standard formatting? How significantly do these potential segmentation errors impact the downstream F1, SRS, and LLM-grader scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FeAGQXhNQb", "forum": "3tm37YNMdl", "replyto": "3tm37YNMdl", "signatures": ["ICLR.cc/2026/Conference/Submission5245/Reviewer_ELDK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5245/Reviewer_ELDK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922579322, "cdate": 1761922579322, "tmdate": 1762917968500, "mdate": 1762917968500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CUMath, a benchmark to evaluate LLM reasoning in undergraduate computational math. The authors state a diagnostic gap exists, as LLMs struggle with fundamental undergraduate tasks and existing datasets are either trivial, synthetic, or overly advanced. To address this, the authors provide a dataset of 2,100 problems, each with step-by-step solutions for evaluation. The paper also proposes a multi-layered evaluation framework that integrates automatic metrics with an LLM-as-a-grader pipeline. This pipeline uses MathBERT for symbolic encoding and external verification with Wolfram Alpha. The authors' analysis of 15 LLMs shows models misuse symbolic methods and rely on shortcuts, leading to polished but flawed solutions. The findings reveal failure modes, including invalid reasoning leading to correct results, and show accuracy alone is an insufficient measure of mathematical competence."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper clearly identifies a diagnostic gap in current math benchmarks (being either trivial or overly advanced) for evaluating LLMs.\n2.  The inclusion of detailed step-by-step solutions enables fine-grained evaluation of model reasoning processes.\n3. The proposed LLM-as-a-grader framework offers a novel evaluation perspective by using external CAS verification loops."}, "weaknesses": {"value": "1.  The low accuracy (less than 25% for even the best models) makes meaningful performance comparisons between models difficult, and we still do not have a comprehensive metrics to evaluate the ability of each model.\n2. The paper shows a significant divergence between automatic metrics (like Accuracy) and its own LLM-as-a-grader score , and argues the LLM score is more comprehensive. However, this claim is weakened because the paper does not report consistency data between its LLM-as-a-grader framework and human expert scores."}, "questions": {"value": "1.  What is the agreement (e.g., kappa score) between your LLM-as-a-grader and human experts on a subset of CUMath?\n2. Given the divergence between automatic metrics (like Accuracy) and LLM-as-a-grader scores, why should the LLM-grader be trusted as a comprehensive measure without a reported correlation to human expert evaluation?\n3. The results table  shows that smaller open-source models (e.g., LLaMA 4 Scout 17B Instruct) achieve scores similar to top-tier models (e.g., OpenAI o3). Does this lack of differentiation suggest the benchmark is not reliably capturing capability differences, or is this an intended finding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kj4sXyUIOI", "forum": "3tm37YNMdl", "replyto": "3tm37YNMdl", "signatures": ["ICLR.cc/2026/Conference/Submission5245/Reviewer_ApXz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5245/Reviewer_ApXz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065796352, "cdate": 1762065796352, "tmdate": 1762917968119, "mdate": 1762917968119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Response to Reviewers"}, "comment": {"value": "We are thankful to all reviewers for carefully reading our submission and for their valuable feedback and suggestions. The comments raised three key issues: (1) the need for human validation of our LLM-as-a-grader framework, (2) clarification of how we chose and evaluated the models, and (3) concerns about the reliability of components such as segmentation and reasoning metrics. We have addressed each of these points in the revised manuscript.\n\n**1. Human validation of the LLM-as-a-grader.**\n\nA central concern from multiple reviewers was whether the automated grader could be trusted without comparison to human graders. In the revised version, we added human evaluation components (Sections 4.3 and 5.3, Appendix F). We recruited three independent annotators with sufficient mathematical background to evaluate multi-step reasoning at the undergraduate level. All annotators were instructed to use the same scoring rubric as our automatic grading pipeline. In total, they manually graded 105 model-generated solutions uniformly sampled across topics. Results show strong agreement: human–human Krippendorff’s $\\alpha = 0.829$ and human–LLM $\\alpha = 0.832$, with topic-level $\\alpha$ and Cohen’s $\\kappa$ showing consistent patterns. This demonstrates that the LLM-as-a-grader aligns closely with human graders and preserves inter-rater reliability.\n\nTo avoid hallucinations or errors from relying only on the LLM, our pipeline integrates symbolic encoding (MathBERT) and external CAS verification (Wolfram Alpha), with enforced revision when conflicts arise. The study of inter-rater consistency and the verification steps help increase confidence in the transparency and reliability of our pipeline, though we acknowledge this as an ongoing area for refinement.\n\n**2. Clarification of evaluated models and “frontier LLMs.”**\n\nOne reviewer asked why recently released models were not included. Our experiments were conducted within a fixed evaluation window (May-August 2025), during which GPT-4.1 and Claude~3.7 were state-of-the-art models available at the time. We have clarified this explicitly in our revised manuscript. It is also important to note that the failure modes we identify, such as incorrect symbolic manipulation, surface-level reasoning patterns, and inconsistent reasoning, appear across all model families and are not specific to a single model. If accepted, we will evaluate the newest models available at camera-ready time.\n\n**3. Robustness of segmentation and reasoning metrics.**\n\nOne reviewer noted the importance of segmentation accuracy and its impact on step-level metrics. We clarify that our segmentation procedure follows established practices from ROSCOE and ReasonEval. Furthermore, our LLM-as-a-grader design intentionally avoids over-penalizing minor formatting or boundary differences. Nonetheless, we acknowledge this module as a limitation and discuss how more advanced semantic segmentation methods (e.g., R1-Compress (Wang et al., 2025) and StepWiser (Xiong et al., 2025)) could be incorporated in future work.\n\n**4. Clarifying metric interpretability and model comparisons.**\n\nA reviewer pointed out that small open models and top-tier models achieved similar accuracy levels. We highlight that this is precisely why final-answer accuracy is insufficient: it compresses performance and hides differences in reasoning quality. In contrast, our process-based metrics (Semantic F1, SRS, VR) and human-aligned LLM-as-a-grader reveal substantially clearer differences. Our study also shows that human graders consistently perceived differences in solution quality across models, even when accuracy was similar.\n\n**References:**\n\n[1] Wang, Y., Luo, H., Yao, H., Huang, T., He, H., Liu, R., Tan, N., Huang, J., Cao, X., Tao, D., & Shen, L. (2025). R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search.\n\n[2] Xiong, W., Zhao, W., Yuan, W., Golovneva, O., Zhang, T., Weston, J., & Sukhbaatar, S. (2025). StepWiser: Stepwise Generative Judges for Wiser Reasoning."}}, "id": "b8pQvRKJQD", "forum": "3tm37YNMdl", "replyto": "3tm37YNMdl", "signatures": ["ICLR.cc/2026/Conference/Submission5245/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5245/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission5245/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763700820881, "cdate": 1763700820881, "tmdate": 1763700849750, "mdate": 1763700849750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}