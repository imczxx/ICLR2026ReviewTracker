{"id": "E7VL9Zl1Nc", "number": 484, "cdate": 1756742084096, "mdate": 1759898258243, "content": {"title": "FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation", "abstract": "Despite recent progress in 3D Gaussian-based head avatar modeling, efficiently generating high fidelity avatars remains a challenge. Current methods typically rely on extensive multi-view capture setups or monocular videos with per-identity optimization during inference, limiting their scalability and ease of use on unseen subjects. To overcome these efficiency drawbacks, we propose FastGHA, a feed-forward method to generate high-quality Gaussian head avatars from only a few input images while supporting real-time animation. Our approach directly learns a per-pixel Gaussian representation from the input images, and aggregates multi-view information using a transformer-based encoder that fuses image features from both DINOv3 and Stable Diffusion VAE. For real-time animation, we extend the explicit Gaussian representations with learnable Gaussian features and introduce a lightweight MLP-based dynamic network to predict 3D Gaussian deformations from expression codes. Furthermore, to enhance geometric smoothness of the 3D head, we employ point maps from a pre-trained large reconstruction model as geometry supervision. Experiments show that our approach significantly outperforms existing methods in both rendering quality and inference efficiency, while supporting real-time dynamic avatar animation.", "tldr": "A new method for creating high quality 3D Gaussian Head Avatars from a few input images, which allows real-time dynamic animation.", "keywords": ["Animation", "Gaussian Avatar", "Feedforward Gaussian Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4458e7aa2fc41ba7779f600988b4902d4d60902a.pdf", "supplementary_material": "/attachment/c7973767e325e9871fd45c8b1d3fc44c92f99d68.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a feed-forward framework for reconstructing animatable 3D Gaussian head avatars from only a few input images. The model first reconstructs a canonical Gaussian head using multi-view features extracted from pre-trained DINOv3 and Stable Diffusion VAE encoders, fused by a transformer-based module. Then, a lightweight MLP conditioned on FLAME expression codes predicts per-Gaussian deformations for real-time facial animation. Additionally, a geometry prior from the VGGT is employed as regularization to improve 3D consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear two-stage design that separates canonical reconstruction and dynamic animation.\n\n- Efficient inference: <1s reconstruction and >60 FPS animation.\n\n- Use of VGGT geometry prior enhances structural consistency.\n\n- Outperforms Avat3r quantitatively in controlled datasets.\n\n- Technically elegant combination of DINOv3, diffusion VAE, and Gaussian Splatting."}, "weaknesses": {"value": "-  Methodological innovation over Avat3r is relatively incremental, primarily combining known modules (DINOv3, VAE, VGGT) rather than introducing a fundamentally new approach.\n- Strong dependency on accurate camera poses, which significantly limits real-world usability. The method cannot operate from uncalibrated or monocular video, unlike many recent “in-the-wild” approaches, even though the paper acknowledges it in the limitations.\n\n- In-the-wild results are weak and underexplored. Only a few examples are shown, and they exhibit degraded appearance and noticeable artifacts. The paper should provide more analysis on why generalization drops and how to mitigate it.\n\n- Mouth region artifacts are frequent (see Figs. 2–3 and video results), with incomplete geometry and texture distortions, suggesting the method struggles with open-mouth or speaking expressions.\n\n- Most visualizations show static reconstructions, not novel-view or novel-expression results, which are more important in avatar applications.\n\n- The paper lacks convincing results for large-viewpoint novel renderings; side-view artifacts are visible in the supplementary video, contradicting the claim of a fully 3D avatar."}, "questions": {"value": "- How does the model behave if camera calibration is slightly inaccurate or unavailable? Could the method be extended to work with monocular or unposed multi-view inputs?\n\n- Can diffusion-based priors help improve in-the-wild generalization?\n\n- Please provide more quantitative metrics for novel-view (side/back-view) and novel-expression performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SuiHnTXEiu", "forum": "E7VL9Zl1Nc", "replyto": "E7VL9Zl1Nc", "signatures": ["ICLR.cc/2026/Conference/Submission484/Reviewer_4bsZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission484/Reviewer_4bsZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747575903, "cdate": 1761747575903, "tmdate": 1762915528223, "mdate": 1762915528223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FastGHA, a novel feed-forward method designed to overcome the challenges of efficiently generating high-fidelity 3D Gaussian-based head avatars by utilizing only a few input images.\n\nThis approach notably supports real-time animation and achieves high fidelity through a two-stage pipeline: first reconstructing a canonical Gaussian head representation, which disentangles identity from the input expressions, followed by a deformation process. The system learns a per-pixel Gaussian representation, aggregating multi-view information using a transformer-based encoder that fuses semantic features from DINOv3 and color features from a Stable Diffusion VAE. For animation, the explicit Gaussian representation is augmented with learnable per-Gaussian features, and a lightweight MLP-based dynamic network is introduced to predict fine-scale 3D Gaussian deformations using descriptive expression codes, such as FLAME parameters. To improve the geometric consistency and robustness of the resulting 3D head, the method employs point maps derived from a pre-trained large reconstruction model (VGGT) as geometry supervision in the form of a regularization loss during training.\n\nExtensive experiments confirm that FastGHA significantly outperforms existing state-of-the-art methods in both rendering quality and inference efficiency, enabling avatar reconstruction in less than one second."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a clear methodology and is well-structured.\n- FastGHA employs a feed-forward approach to directly predict a per-pixel Gaussian representation. This design is explicitly chosen to enable instantaneous reconstruction of unseen subjects, avoiding lengthy per-identity optimization or template Gaussians rigged to 3DMMs required by many prior methods. Previous approaches struggled to handle fluffy elements like hair due to the rigging constraint.\n- The method significantly addresses efficiency drawbacks by supporting real-time dynamic avatar animation. This speed is achieved through a lightweight MLP-based dynamic network that predicts fine-scale Gaussian deformations from expression codes, contrasting with computationally slower methods like those using cross-attention blocks. For a standard four-image input, the avatar reconstruction takes less than one second, and the animation speed remains high (e.g., 62 FPS for four inputs).\n- FastGHA demonstrably outperforms existing state-of-the-art methods (including Avat3r, InvertAvatar, and GPAvatar) in reconstruction fidelity, showing a large gap in quantitative metrics such as PSNR, SSIM, LPIPS, and identity preservation."}, "weaknesses": {"value": "- The paper identifies Avat3r as the \"most related\" state-of-the-art method. However, the crucial quantitative comparison table (Table 1) lacks the results for Avat3r on the Nersemble dataset. This prevents a full comparison, especially given that FastGHA's best performance is achieved when training on both Ava-256 and Nersemble (\"Ours (both)\").\n- The animation pipeline depends on the accuracy of FLAME expression codes ($z_{exp}$) obtained using \"off-the-shelf head tracking tools\". Errors generated by these trackers when processing complex or extreme expressions (e.g., a wide open mouth or squinting, as shown in the input examples) could compromise the fidelity of the downstream deformation network $D$."}, "questions": {"value": "- In the qualitative comparisons (as shown in video), I still observe flicker in the Gaussian primitives. Could the authors explain the cause of these artifacts? Would increasing the number of input views alleviate the issue? Including curves of performance versus number of views in the final version would help analyze the impact of few-shot inputs.\n\n- The method reconstructs a canonical Gaussian Head even when the input images contain expressions, and the authors state that the canonical Gaussian is unsupervised. I am curious why this representation can be learned. Could the authors offer a deeper discussion? Is this related to using neutral faces during training, enabling the model to learn that, when the expression code is absent, it should output a canonical Gaussian?\n\n- Could the authors discuss whether the pre-processing of VGGT—specifically the alignment of its reconstructed point cloud using 2D landmarks and corresponding 3D keypoints from a tracked mesh—introduces additional errors or complexities? In particular, how do inaccuracies in landmark detection and mesh tracking affect the reliability of the geometry prior and the regularization loss $L_{geo}$, given that the VGGT point cloud quality is reported as \"not high enough\" and \"inconsistent\" (Line 849-850)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k2FXHCRzlv", "forum": "E7VL9Zl1Nc", "replyto": "E7VL9Zl1Nc", "signatures": ["ICLR.cc/2026/Conference/Submission484/Reviewer_XoYB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission484/Reviewer_XoYB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969515653, "cdate": 1761969515653, "tmdate": 1762915528114, "mdate": 1762915528114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the task of reconstructing an animatable 3DGS-based avatar from 4 posed input images.\n\nFollowing the recent success of LRM-style approached, like Avat3r,, the method employs a multi-view transformer which use features from a stable diffusion VAE, DINOv2 and plucker ray embeddings to predict per-pixel Gaussian attributes. \n\nImportantly, the set of gaussian attributes is extended to contain per-Gaussian latent features, similar to GHA and NPGA, which are then used to condition an MLP which animates the Gaussian based on FLAME expression code conditoning. Compared to Avat3r, the MLP is more light weight than the cross-attention based architecture, which is relevant for real-time applications.\n\nAdding supervision against aligned VGGT depth predictions helps to produce more 3D consistent avatars.\n\nThe method is thoroughly evaluated against comparable sota baselines, and additionally shows examples on real-world images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The task of accessable avatar creation from a few selfie images finds importance in many down-stream applications. Therefore, the improved visual quality and faster animation speed immediatley become more significant. \n- Smart and simple usege of pretrained VAE weights for encoder/decoder, which is also ablated to be beneficial. The same can be said for the VGGT supervision.\n- Overall the architecture seems to be slightly simplified comapred to Avat3r, which seems quite helful for future improvements that build upon the presented method.\n- The paper is well-written and easy to follow.\n- The method is thoroughly evaluated against to most relevant baselines, and on two different datasets, leaving little room for doubt about the evaluation. Furthermore, the SOTA baselines are significantly outperformed.\n-"}, "weaknesses": {"value": "- The main weakness of the paper that I am still seeing is the limited novelty, since it mainly introduces some technical changes compares to Avat3r. However, the quality and animation speed are improved, and the paper is well evaluted. Therefore, we can be sure that the method solidly advances the field on such a highly relevant task. Therefore, I don't mind the limited novelty.\n- Currently, the method is limited by FLAME expression codes. However, anything else would likely be out-of-scope for this project, and it would make animation from 2D videos arguably harder, bc. obtaining FLAME codes is so easy.\n- Somewhow the number in the ablation study don't match the main results. Is there an explanation for this? Otherwise, this gives the impression that the ablations were sloppily exectued."}, "questions": {"value": "- The text states that the 4 input views have different facial expressions during training. However, all results show identitcal expressions, or quite similar expressions. Does the model still perform well with very different expressions?\n- Fig. 5: would be interesting to see how much the canonical point cloud changes with different inputs, e.g. the first example has the mouth slightly open in canonical space, which might be caused by the input images.\n-  Is the VGGT superivsion competed offline? How good do the recoinstructions look? In my experience they can be rather blurry for faces and exhibit many stiching artifacts. Could this have been done with e.g. COLMAP as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c4P92TtYXW", "forum": "E7VL9Zl1Nc", "replyto": "E7VL9Zl1Nc", "signatures": ["ICLR.cc/2026/Conference/Submission484/Reviewer_uEfC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission484/Reviewer_uEfC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990047672, "cdate": 1761990047672, "tmdate": 1762915528000, "mdate": 1762915528000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FastGHA, which efficiently generates high-quality, real-time animated 3D Gaussian head avatars with few-shot inputs. The method fuses multi-view features through a Transformer encoder and utilizes Gaussian features and a lightweight MLP network to achieve real-time animation based on facial expression encoding. Simultaneously, geometric supervision is introduced to improve the geometric smoothness of the 3D head."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The method demonstrates satisfactory few-shot reconstruction quality and visualization effects.\n* The method's reconstruction design is reasonable, effectively integrating information from different views, and can be well driven by new expressions."}, "weaknesses": {"value": "* This method may struggle with \"unreasonable\" user input and the paper does not show these results. Furthermore, reasonable input is difficult to define. For example, if user input from a particular perspective is missing, the result is currently unknown. \n* This method may struggle to handle an arbitrary number of input viewpoints. Because it relies on VxHxW self-attention, too many views exponentially increase the computational cost of this part.\n* Due to the linear increase in the number of Gaussians and the presence of the driving MLP, the driving speed decreases linearly with increasing viewpoint. Merging and controlling the total number of reconstructed Gaussians might be a good solution."}, "questions": {"value": "* Why does the MLP need to modify the color of the Gaussians during animation? What would happen if it didn’t?\n* The paper conducts ablation experiments on removing the VAE feature. What would be the result if only the VAE feature is kept and the DINO v3/Sapiens feature is removed?\n* Removing the per-Gaussian feature shows a significant difference in results. What would happen with other dimension choice of the per-Gaussian feature, for example, 16 or 64?\n* This method seems to have a hidden limitation: the requirement for a reasonable distribution of input viewpoints. Does this mean that the method lacks or has limited reconstruction ability when certain viewpoints are not covered? Structurally, the method can reduce the input to just one image. What is the performance when only a single frontal image is provided, or what if one frontal image and one side image, with the other side / up side missing?\n* As a few-shot method, it is also reasonable and beneficial to discuss and compare with some of the latest one-shot methods, which can highlight the unique advantages of few-shot approaches. The authors could discuss and compare with methods such as GAGAvatar and LAM.\n* Since the method is completely unsupervised in the canonical space, the statement “By design, the canonical head\nis devoid of expression and thus does not correspond to any of the input expressions, allowing animation to happen downstream. ” is too arbitrary. It is possible for the model to learn a canonical space with an open mouth, and in fact, a canonical space with an open mouth may make it easier to model teeth than the closed-mouth shown in the Figure 1. What causes the model converges to a closed-mouth canonical space?\n* There is no explanation for C_d and C_e in Equation 3. After reading, it seems refer to the DINO feature and encoder feature, but this should be explicitly stated.\n* The additional Gaussian features used in animation MLP should not be claimed as \"learnable\", because it is not parameters of the model, but rather the result of inference. They are essentially the same thing as other inferred Gaussian parameters."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ePEGC19Nzn", "forum": "E7VL9Zl1Nc", "replyto": "E7VL9Zl1Nc", "signatures": ["ICLR.cc/2026/Conference/Submission484/Reviewer_2Epj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission484/Reviewer_2Epj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762200046423, "cdate": 1762200046423, "tmdate": 1762915527851, "mdate": 1762915527851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}