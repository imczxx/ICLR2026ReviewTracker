{"id": "5xOh6xNY8z", "number": 223, "cdate": 1756731684932, "mdate": 1763253661617, "content": {"title": "MePo: Meta Post-Refinement for Rehearsal-Free General Continual Learning", "abstract": "To cope with uncertain changes of the external world, intelligent systems must continually learn from complex, evolving environments and respond in real time. This ability, collectively known as general continual learning (GCL), encapsulates practical challenges such as online datastreams and blurry task boundaries. Although leveraging pretrained models (PTMs) has greatly advanced conventional continual learning (CL), these methods remain limited in reconciling the diverse and temporally mixed information along a single pass, resulting in sub-optimal GCL performance. Inspired by meta-plasticity and reconstructive memory in neuroscience, we introduce here an innovative approach named **Me**ta **Po**st-Refinement (MePo) for PTMs-based GCL. This approach constructs pseudo task sequences from pretraining data and develops a bi-level meta-learning paradigm to refine the pretrained backbone, which serves as a prolonged pretraining phase but greatly facilitates rapid adaptation of representation learning to downstream GCL tasks. MePo further initializes a meta covariance matrix as the reference geometry of pretrained representation space, enabling GCL to exploit second-order statistics for robust output alignment. MePo serves as a plug-in strategy that achieves significant performance gains across a variety of GCL benchmarks and pretrained checkpoints in a rehearsal-free manner (e.g., 15.10\\%, 13.36\\%, and 12.56\\% on CIFAR-100, ImageNet-R, and CUB-200 under Sup-21/1K).", "tldr": "We propose a meta post-refinement approach to address general continual learning based on parameter-efficient tuning of pretrained models.", "keywords": ["continual learning", "catastrophic forgetting", "transfer learning", "prompt tuning", "meta learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07d15008c511be42163b8e3e0671ae2026006245.pdf", "supplementary_material": "/attachment/1a0fa66a57638367b4f7e9b8c4528bee81651241.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a novel method involving meta learning + covariance alignment for real world Continual Learning tasks. This requires them to refine pretrained models first using some \"pseudo\" tasks, and then simply align the features during online learning. This seems to show 10-15% improvements as a plug-and-play method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper shows strong empirical evidence towards REALISTIC CL scenarios, and is not just constrained to ideal conditions. They also cover multiple datasets / types.\n2. The method is generalist enough to be broadly applicable.\n3. The ablations are sensible and comprehensive."}, "weaknesses": {"value": "1. The paper is a bit too empirical - there is no theory regarding why these \"pseudo\" tasks transfer downstream.\n2. There is no numerical stability analysis for the strong distributional assumptions the paper makes.\n3. There seems to be substantial overhead to this sort of meta-training - this is not a simple \"one-time-cost\".\n4. The choice of using weighted combination seems a little ad-hoc, and the paper limits itself to the Si-Blurry setting only."}, "questions": {"value": "1. I would appreciate a stronger theoretical justification for why the downstream task transfer works. Could you provide some?\n2. What is the performance like without pretraining data availability?\n3. What is the numerical stability of the Cholesky decomposition in this setting?\n4. Is there evidence of generalization of this method beyond the Si-Blurry setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VG3bzjip2P", "forum": "5xOh6xNY8z", "replyto": "5xOh6xNY8z", "signatures": ["ICLR.cc/2026/Conference/Submission223/Reviewer_jnQ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission223/Reviewer_jnQ5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800829855, "cdate": 1761800829855, "tmdate": 1762915474441, "mdate": 1762915474441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Response (Part 1)"}, "comment": {"value": "Dear reviewers,\n\nWe sincerely thank all reviewers for their effots and valuable suggestions. We are pleased that our work on **Meta Post-Refinement (MePo)** was recognized as tackling a fundamental problem in general continual learning (GCL), with strengths such as being \"promising and effective performance\" (Reviewers EmKK, KQGT, and jnQ5), \"easy to follow and generalist enough to be broadly applicable\" (Reviewers EmKK and jnQ5), etc.\n\nHere we summarize following **Common Questions (CQs)**, and also include a point-to-point response to all other questions:\n\n---\n\n**CQ1: Novelty and Computational Cost**\n\nPrevios work that employs pretrained models (PTMs) for continual learning (CL) and general CL (GCL) typically employs a **two-phase paradigm**, i.e., upstream pretraining and downstream CL/GCL. In comparison, our approach equips the upstream pretraining with an additional post-refinement using pretraining data, which is essentially an **extended pretraining phase** tailored to CL/GCL. The key idea lies in pre-adjusting pretrained representations for accommodating dynamic data distributions. \nTo our knowledge, this is the first attempt to **prepare PTMs for CL/GCL in advance**, fundamentally different from exisiting PTMs-based CL/GCL methods. Following this idea, we propose meta-learning of pseudo task sequences for backbone refinement and meta-covariance for output alignment.\n\nWe highlight that the **computational cost** of post-refinement, as an extended pretraining phase, is only **one-time** and is **almost negligiable** compared to the entire pretraining. Thanks to this advance preparation, downstream CL/GCL becomes remarkably **effective and efficient**. Here we provide detailed analyses:\n\n**(1) Post-Refinement (Offline, One-Time Cost):**\n\nThe proposed meta learning strategy refines the pretrained backbone in a **method-agnostic and reusable** manner. Once the refined backbone is obtained through pseudo-task sequences constructed from pretraining data, it can be **reused across any downstream CL or GCL sequences without re-running Post-Refinement**. This design aligns with practical deployments, where a single pretrained backbone is repurposed for multiple downstream tasks and scenarios.\n\nMoreover, we find that using only a modest amount of pretraining data and meta epochs is sufficient to obtain strong performance. Specifically, we randomly sample 400 images per class from the pretraining dataset (ImageNet-1K), performing **only 50 epochs** for Sup-21K and **only 10 epochs** for Sup-21/1K to reach convergence. \nThe training time is evaluated with three-card 3090 GPU, AMD EPYC 7402 (2.8G Hz):\n\n|Meta Epoch | 1 | 2 | 3 | 4 | 5 | Average |\n|----------|---|---|---|---|---|---|\n|           |14.43 mins| 14.20 mins| 14.13 mins | 14.00 mins| 13.96 mins| 14.14 mins/epoch | \n\nIn particular, Sup-21K on ImageNt-21K needs to train ~1.3 billion image instances for 90 epochs, but MePo on ImageNt-1K (randomly sampling 400 images per class) only needs to trains additionally ~2 million image instances for 50 epochs, accounting for **only 0.15%** of the entire pretraining.\n\n| Training               | Image Size| Epoch | Batch size | Total Step | Total Images Processed  |\n| - | - | - | - | - | - |\n| Sup-21K (Pretraining) [1] | 224 x 224 | 90    | 4096       | ~310k      | ~1.3B  |\n| Sup-21K (MePo)         | 224 x 224 | 50    | 256        | ~12.8k     | ~2M  | \n\n[1] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR, 2021.\n\n**(2) GCL (Online, Per-Batch Cost):**\n\nDuring GCL, the refined backbone ($\\theta^*$) is frozen (see Fig. 1). Only lightweight parameters with parameter efficient tuning ($\\Delta\\theta$) and the output head ($\\psi$) are updated. As shown in Table 2, the **GCL phase incurs negligible overhead**, e.g., +0.67% parameters and no additional batch time. \n\nWe have incorporated the above explanations and additional results in our revised manuscript (lines 84-87, 419, 422)."}}, "id": "Kuqw0gORjU", "forum": "5xOh6xNY8z", "replyto": "5xOh6xNY8z", "signatures": ["ICLR.cc/2026/Conference/Submission223/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission223/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission223/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763253953887, "cdate": 1763253953887, "tmdate": 1763253953887, "mdate": 1763253953887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the setup of general continual learning (GCL) with pretrained models, where the model performs a single pass through a sequence of tasks, with potentially blurred task boundaries. The paper proposes (a) an extended pretraining phase, where the model meta-learns to refine its pretrained representations for improved adaptation to sequential learning, and (b) a method to align the output feature covariance with that from pretraining. The proposed method can be applied on top of existing GCL approaches, and the authors empirically demonstrate its effectiveness on CIFAR-100, ImageNet-R, and CUB-200 task sequences using different pretrained backbones. Ablation studies further show the benefit of each component of the proposed method.\n\nThis paper is generally sound, and the proposed method shows convincing performance improvements over baselines. The authors also conduct extensive analyses of their approach. My main concerns are the motivation for the proposed output alignment method and the clarity of certain claims and statements."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is clean, and the meta-representation learning component is intuitive.\n- Extensive experimental analysis demonstrates the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The arguments made in Sec. 2.2 are not clearly stated; the offline, online, and GCL settings are not defined and the claims in the paragraph starting at L159 lack sufficient evidence. For instance, from Fig. 2b, MVP-Rep appears equally ineffective in both online and offline settings, but the authors state that it does not address online data streams as well. Several claims related to Fig. 2c also seem somewhat overstated, as all methods show similar performance between the online and GCL settings.\n2. I find the motivation for output alignment unclear. Specifically, if the data in the incoming batch are drawn from a different distribution than that of pretraining (e.g., imbalanced classes), why should we expect the covariance matrix of their features to match the pretrained one (e.g., balanced classes)? Also, Fig. 4 suggests that pre-aligned features are more separable; but in classification, wouldn’t greater separability generally be desirable?\n3. It seems that the pseudo task sequence used in this work always mirrors the structure of the downstream continual learning sequence. In practice, how should such a sequence be constructed during pretraining without knowing the structure of the downstream sequence? It might be useful to analyze the impact of a mismatch between the two."}, "questions": {"value": "1. L426: \"...due to imbalanced classes.\" It’s unclear how the performance drop can be attributed to class imbalance without comparing against a balanced setup under otherwise identical conditions.\n2. L428: Isn’t this finding contradictory to prior work suggesting that SSL representations are more robust to continual learning than supervised ones [Gal+21, Dav+22]? Could the authors hypothesize why?\n3. Fig. 7 shows that MePo produces sparser activations, but *why* are sparser activations desirable?\n\n\n### Questions/comments that did not impact the score\n4. L465: \"We empirically validate...\" Which specific result does this statement refer to?\n5. I wonder if the joint training phase should be considered as the final task in the inner loop, since its only difference from other pseudo-tasks appears to be data composition.\n6. The initialization of $\\psi$ is not described in the main text but is provided in Algorithm 1 in the Appendix.\n7. $\\Sigma\\text{pre}$ >> $\\Sigma_\\text{pre}$ in Eq. 8.\n8. In Table 1, I recommend either bolding the better method within each of the two rows (w/ vs. w/o MePo) or explicitly reporting the improvement for clarity.\n\n[Gal+21] Self-Supervised Training Enhances Online Continual Learning. Gallardo et al. BMVC 2021.\\\n[Dav+22] Probing Representation Forgetting in Supervised and Unsupervised Continual Learning. Davari et al. CVPR 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1efKWxijly", "forum": "5xOh6xNY8z", "replyto": "5xOh6xNY8z", "signatures": ["ICLR.cc/2026/Conference/Submission223/Reviewer_KQGT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission223/Reviewer_KQGT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924222902, "cdate": 1761924222902, "tmdate": 1762915474274, "mdate": 1762915474274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes meta post-refinement (MePo for rehearsal-free general continual learning. The main ideas of the proposed method are MePo for representation learning and Mepo for feature alignment. The paper is well presented and shows a promising performance as reported in the numerical results. However, the paper has several issues that need to be clarified; please see the weaknesses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "(1). The paper is presented in a clear yet comprehensive way, and it is easy to follow. \n\n(2). The numerical results show a promising margin over the existing methods."}, "weaknesses": {"value": "(1). From the perspective of methodology, it is questionable whether the paper has enough novelty. Training a base model with meta-learning guidance and feature alignment (combination) by weighted sum is nothing new. I do not see any new/novel mechanisms in either of the ideas.\n\n(2). The proposed method is not supported by theoretical analysis, e.g., how the meta learner helps the base learner to achieve a better stability-plasticity, or how it elevates the base model to achieve a better adaptability to a new task.\n\n(3). It is questionable why \"self-supervised PTM is more realistic\", while we have free/open supervised PTM models such as pretrained ViT on imagenet-21K or DINO.\n\n(4). Continual learning is the art of defying catastrophic forgetting (CF). But I do not see forgetting measurement and discussion about it. \n\n(5). GCL that has online learning should be concerned about model throughput (training and inference) as it works on streaming data. Similarly, I do not see measurements and discussion about it.\n\n(6). It is arguably unfair to compare the GCL/Online CL method with offline CL methods such as L2P, DualPrompt,  and CODA-P, especially when you did not search for their best hyperparameter settings in your experiment. Their best parameter setting should be suitable for CL but not for GCL/OCL.  Also, please kindly compare it to the other GCL/OCL SOTAs, i.e., RanPAC[1], RanDumb[2], F-OAL[3], PROL[4], and the newest PEFT (prompt, LoRA, and Adapter) structure.\n\n(7). The pseudo code in Algorithm 1 is not clear on how the model processes each streaming chunk and how the learned knowledge is consolidated from many processed chunks.\n\n\nreference:\n[1]. Ranpac: Random projections and pre-trained models for continual learning (NeurIPS 2024)\n\n[2]. Randumb: A simple approach that questions the efficacy of continual representation learning (NeurIPS 2024)\n\n[3]. F-OAL: Forward-only online analytic learning with fast training and low memory footprint in class incremental learning (NeurIPS 2024)\n\n[4]. PROL: PROL: Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning. (ICCV 2025)"}, "questions": {"value": "(1). Please address the weakness.\n\n(2). Is 256 batch size considered as a realistic batch/chunk-size on GCL, as the previous online CL uses a small batch/chunk-size, e.g., 10.\n\n(3). The post-pretraining process (Figure 1) shows that the whole pre-trained model is finetuned (learnable). But Table 2 shows far lower parameters. Could you please clarify this issue? From my perspective, computing additional learnable parameters only by the number of parameters for GCL phase is not fair and objective.\n\n(4). Table 2 shows that w/ MePO requires larger parameters but lower time than w/o MePO. How is this possible? \n\n(5). Could you explain in more detail how each class samples (both from disjoint tasks and blurry tasks) are partitioned into 2 models (base learner and meta learner)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kEWtuxyldH", "forum": "5xOh6xNY8z", "replyto": "5xOh6xNY8z", "signatures": ["ICLR.cc/2026/Conference/Submission223/Reviewer_EmKK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission223/Reviewer_EmKK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933144481, "cdate": 1761933144481, "tmdate": 1762915474092, "mdate": 1762915474092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}