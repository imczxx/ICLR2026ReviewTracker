{"id": "lZ2C7WcWce", "number": 21121, "cdate": 1758313976828, "mdate": 1759896941030, "content": {"title": "Federated Agent Reinforcement Learning", "abstract": "Autonomous AI Agents powered by LLMs have shown remarkable abilities in diverse domains. However, the training process typically require centralized collection of large amounts of real-world user data, posing substantial privacy and regulatory concerns. To this end, we explore a new decentralized training paradigm, namely FedAgent (Federated Agent Reinforcement Learning), which enables collaborative learning of AI agents across distributed clients without sharing local data. Moreover, we construct the first decentralized agent learning environment FedAgentGym, which includes four types of LLM agents, two application scenarios (WebShop and ALFWorld), three variations of decentralized settings, and three newly defined heterogeneity challenges (Preference Heterogeneity, Coverage Heterogeneity, and Hardness Heterogeneity), to systematically investigate its effectiveness and impact factors. Extensive theoretical and empirical studies show that FedAgent can have comparable performance to the centralized training paradigm and exhibit strong robustness against heterogeneities, which shows the feasibility of training AI agents without sacrificing data privacy. The code is available.", "tldr": "We explored FedAgent, a new collaborative paradigm to train LLM agents across distributed clients without data sharing, and built FedAgentGym, the first decentralized agent learning environment to investigate its effectiveness and impact factors.", "keywords": ["LLM Agent", "Federated Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/629fa2333c6740c6c09b3c72875ab5f652293c8d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes FedAgent, a federated reinforcement learning approach for training LLM-based agents without centralizing user data. The authors build FedAgentGym as a benchmark with 4 LLMs, 2 environments (WebShop, ALFWorld), and introduce three types of heterogeneity specific to agent learning. They provide theoretical convergence analysis and experiments showing FedAgent matches centralized performance.\n\nThe problem is genuinely important - training agents on sensitive user data is a real concern. The experimental setup is systematic and the heterogeneity characterizations are thoughtful. But I have serious concerns about the novelty claims, theoretical assumptions that aren't validated, and some experimental choices that make me question the conclusions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **The problem motivation is solid.** Privacy in agent training is genuinely important and underexplored. With regulations like GDPR, we need solutions here.\n\n2. **The heterogeneity partitioning strategies are clever.** PreferencePartition, CoveragePartition, and HardnessPartition are well-designed with single hyperparameters to control heterogeneity intensity while keeping other factors constant. This is good experimental design.\n\n3. **Comprehensive evaluation across multiple dimensions.** Testing 4 LLMs, 2 environments, varying decentralized settings (samples per client, clients per round, epochs per round), and three heterogeneity types shows thoroughness.\n\n4. **Strong robustness to heterogeneity.** The results in Figure 4 surprised me - FedAgent maintains stable performance even under extreme heterogeneity . This is actually pretty impressive and suggests the approach has practical promise."}, "weaknesses": {"value": "### Critical\n\n**1. The FedRLHF elephant in the room**\n\nI keep coming back to this because it's just so glaring. Fan et al. 2025 publishes \"FedRLHF: Federated Framework for Privacy-Preserving and Personalized RLHF\" at AAMAS 2025. You cite it. But you never discuss it in the main text. How is FedAgent different? Here's what I need to see:\n\n- Side-by-side comparison table of assumptions, algorithms, theoretical results\n- Empirical comparison on at least one shared benchmark  \n- Clear positioning statement: \"FedRLHF focuses on X, while FedAgent addresses Y\"\n- Discussion of why agent learning requires different treatment than general RLHF\n\nWithout this, the entire novelty claim falls apart. For all I know, you're just running FedRLHF's algorithm on agent tasks and calling it new.\n\n**2. Unvalidated theoretical assumptions**\n\nThe PL condition (Assumption 4) needs empirical validation. This is straightforward: for a sample of your training runs, plot $\\|\\nabla J(\\theta_t)\\|^2$ versus $J(\\theta^*) - J(\\theta_t)$ on log-log scale. Do you see the required linear relationship? What's the estimated μ value? \n\nAlso, Assumption 5 (bounded heterogeneity) claims $\\frac{1}{K}\\sum_k \\|\\nabla J_k(\\theta) - \\nabla J(\\theta)\\|^2 \\leq \\zeta^2$. Can you compute actual $\\zeta^2$ values for your different heterogeneity settings? This would connect the theory to your empirical heterogeneity measures.\n\nRight now the theory section feels like it was included to check a box rather than actually inform the experimental work.\n\n**3. Weak baseline and inconsistent error reporting**\n\nThe local training baseline is just wrong. You test clients 21, 42, 84 - why? How were these chosen? Are they representative of average, best, worst performance? Looking at Table 1, client 84 often does worse than others - is that typical or did you cherry-pick a weak client to make FedAgent look better?\n\nA proper baseline would be:\n- Average local training performance across all 100 clients (with std dev)\n- Best and worst single-client performance\n- Or stratified sampling across heterogeneity levels\n\nAlso: centralized and FedAgent report mean±std over 3 seeds, but local training shows single runs with no error bars. Why?.\n\n### Significant Issues\n\n**4. Missing key related work**\n\nYour related work section is too sparse. You should discuss:\n- Recent federated instruction tuning for LLMs (2023-2024 literature)\n- Privacy-preserving LLM training methods beyond basic FL\n- Distributed RLHF approaches\n- Recent agent learning work that may touch on decentralized training\n\nThe comparison with traditional federated RL (Liu 2024, Qi 2021) is superficial. The seminar convergence analysis work for federated RL is missing from discussion, e.g, FedPG (Fan et al. NeurIPS 2021). You claim LLM agent learning has \"fundamentally new challenges\" but don't deeply analyze what's actually different. Multi-step reasoning and complex interactions exist in traditional RL too. Also as noted in issues#1, how is your paper fundamentally different from FedRLHF (Fan et al. AAMAS 2025)? Necessary discussed is needed.\n\n**5. Privacy claims without privacy analysis**\n\nYour entire motivation is privacy, yet you provide:\n- No differential privacy guarantees\n- No privacy-utility tradeoff analysis  \n- No discussion of gradient inversion attacks\n- No membership inference analysis\n- No secure aggregation protocols\n\nThere's extensive literature showing that model parameters leak training data information. You need at least a basic DP analysis or discussion of why DP isn't needed here.\n\n**6. Severely limited practical grounding**\n\nSome practical questions that aren't addressed:\n\n- **Communication costs:** A single round of FedAgent with Qwen-7B requires each client to download ~14GB, train, and upload ~14GB. Over 70 rounds that's nearly 2TB per client. Is this realistic?\n  \n- **Who are the clients?** Individual users on smartphones? Organizations with their own data? This drastically changes the problem.\n\n- **Incentive mechanisms:** Why would clients participate? Training LLMs locally is expensive.\n\n- **Stragglers and dropouts:** What happens when clients disconnect? Your algorithm assumes all selected clients complete training.\n\n- **Only evaluated on simulated benchmarks,** not real federated deployments. WebShop and ALFWorld are fine for initial evaluation but tell us nothing about real-world viability.\n\n**7. Theory-practice gap**\n\nBeyond the PL condition issue:\n\n- Theorem 1 claims O(1/T) convergence. Can you verify this empirically? Plot the suboptimality gap versus rounds on log-log scale.\n  \n- The stepsize η=1/(Lτ) appears in the theory. What stepsize did you actually use in experiments? Was it tuned or derived from estimated L?\n\n- The \"noise floor\" terms in Theorem 1 predict how variance depends on M, K, τ. Do your experiments validate these dependencies?\n\nThere's basically zero connection between your theory and experiments beyond \"we have a theorem.\"\n\n**8. Experimental design details**\n\nSeveral important details are missing or unclear:\n\n- **Client overlap:** You mention \"potential overlap\" but never quantify it. On average, how many clients share each instruction? This is absolutely critical for interpreting results.\n\n- **Hyperparameter selection:** How were learning rates, batch sizes, etc. chosen? If you did cross-validation across clients, that leaks information and violates privacy. If you used a held-out test set, how was it created without central data access?\n\n- **Why GRPO?** You use GRPO for policy optimization but don't justify this choice. Would PPO or A3C work similarly? Is GRPO particularly suited to federated settings?\n\n- **Why FedAvg?** Simple model averaging is the most basic aggregation. FedProx handles heterogeneous objectives better. SCAFFOLD reduces client drift. Did you experiment with alternatives?\n\n### Minor Issues (But Still Worth Fixing)\n\n**9. Statistical rigor**\n\n- No statistical significance testing. Are differences between FedAgent and centralized actually significant?\n- Some error bars are quite large (e.g., ALFWorld Pick2 results) but there's no discussion of stability or variance.\n\n**10. Claims that need verification**\n\n- \"First decentralized agent learning environment\" - have you done a thorough search? This is a strong claim.\n  \n- \"Fundamentally new challenges\" - Your heterogeneity types are agent-specific but conceptually similar to existing FL heterogeneity (preference to label skew, coverage to quantity imbalance, hardness to quality differences). \"Fundamentally new\" is overstated unless discussed in depth.\n\n11. Figure 2's circle marks for individual clients are hard to visually interpret."}, "questions": {"value": "These are things where I wish to see in the manuscript:\n\n1. **What's the relationship with FedRLHF?** Please provide a detailed comparison. If they're similar, why is a separate paper needed? If different, what are the key algorithmic or empirical differences?\n\n2. **Can you validate the PL condition?** Plot gradient norm squared versus suboptimality gap to check if the linear relationship holds.\n\n3. **Why clients 21, 42, 84 for local training?** Can you report aggregated statistics across all clients instead?\n\n4. **What's the actual client overlap?** On average, how many clients share each instruction?\n\n5. **How were hyperparameters selected without violating privacy?**\n\n6. **What are the communication costs in GB?** Is this practical for real deployments?\n\n\n7. **Figure 2b (ALFWorld) is way more volatile than WebShop. Why?** Does this affect your convergence guarantees?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0bdr0g9XO1", "forum": "lZ2C7WcWce", "replyto": "lZ2C7WcWce", "signatures": ["ICLR.cc/2026/Conference/Submission21121/Reviewer_LsvX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21121/Reviewer_LsvX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761593460251, "cdate": 1761593460251, "tmdate": 1762941399774, "mdate": 1762941399774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores agent reinforcement learning in federated learning settings and constructs a comprehensive benchmark environment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Generally, this paper is easy to follow. \n\n2. The three types of heterogeneity (Preference, Coverage, Hardness) are well-motivated."}, "weaknesses": {"value": "1. FedAgent essentially applies standard FedAvg with GRPO. Although the application domain is somewhat novel, the algorithm contribution is limited. \n\n2. During the experiment, the authors only compare the proposed method with centralized and local training, ignoring comparisons with other federated reinforcement learning methods. \n\n3. The paper proposes three types of heterogeneity (Preference, Coverage, Hardness), but this taxonomy is insufficient to cover the diverse heterogeneity challenges that exist in real-world federated agent learning."}, "questions": {"value": "1. Have you experimented with more sophisticated federated optimization algorithms (FedProx, FedNova, SCAFFOLD)? Why is vanilla FedAvg sufficient for this setting?\n\n2. The experiments use uniform random client selection. Have you explored other strategies (e.g., importance sampling, clustered selection) that might leverage the heterogeneity structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UeW9M4etwg", "forum": "lZ2C7WcWce", "replyto": "lZ2C7WcWce", "signatures": ["ICLR.cc/2026/Conference/Submission21121/Reviewer_wc3o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21121/Reviewer_wc3o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807097317, "cdate": 1761807097317, "tmdate": 1762941398411, "mdate": 1762941398411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FEDAGENT, a decentralized federated agent reinforcement learning paradigm for collaboratively training AI agents across distributed clients without sharing local data. It introduces the first decentralized benchmark environment, FEDAGENTGYM, featuring four types of LLM agents (Qwen2.5-{1.5B, 3B, 7B}, Llama-3.2-3B), two application scenarios (WebShop, ALFWorld), and three decentralized control dimensions (samples per client, clients per round, local epochs per client). The authors define three new heterogeneity types unique to decentralized LLM-agent learning: (i) Preference Heterogeneity: clients prefer different task types; (ii) Coverage Heterogeneity: clients have different task sampling scopes; and (iii) Hardness Heterogeneity: clients face tasks of varying difficulty. The paper also provides a convergence theorem showing an $\\mathcal{O}(1/T)$ rate to the optimal solution up to a noise and heterogeneity floor. Empirical results show that FEDAGENT matches centralized performance, outperforms local training, and remains robust under high heterogeneity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper opens a new research direction in federated agent reinforcement learning, combining decentralized federated learning, reinforcement learning, and large language model (LLM) research in a way not previously explored. It directly tackles a critical real-world challenge, training efficient AI agents without centralizing sensitive user data, making it highly relevant.\n\n- The work presents a well-structured experimental setup that spans diverse models, tasks, and heterogeneity settings. The authors analyze multiple dimensions, including sensitivity to decentralized parameters and robustness under different heterogeneity conditions. The results are consistent, well-documented, and convincingly support the paper’s claims.\n\n- The theoretical analysis is thorough and mathematically sound. The proofs are detailed and clearly presented.\n\n- The exposition is clear and well-organized, with informative figures that effectively illustrate the framework and results. Experimental details are thorough and transparent, ensuring reproducibility and aiding understanding."}, "weaknesses": {"value": "- The paper currently compares only against centralized and local training baselines. To ensure a fair and comprehensive evaluation, the authors should consider including additional federated reinforcement learning methods such as FedRLHF [1], FedPG [2], and Federated Actor-Critic [3].\n\n- The Polyak–Łojasiewicz ( assumption 4) is extremely strong. In fact, even in the simpler single-agent reinforcement learning, the agent's objective satisfies only a weaker non-uniform Łojasiewicz inequality [4].  In the heterogeneous federated reinforcement learning setting, such a type of inequality is provably not satisfied by the global objective [2].\n\n- The analysis is not novel and closely resembles what has been done in [1]\n\n- The paper does not provide any sample complexity in its theoretical analysis.\n\n- Client heterogeneity is simulated using Gaussian or Beta noise, which limits interpretability and realism. It would be more meaningful to define heterogeneity in ways that directly reflect user-level variations (e.g., real shopping logs or user interaction histories)\n\n[1] Fan et al, FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF, AAMAS 2025\n\n[2] Labbi et al, On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment, ARXIV 2025\n\n[3] Zhu et al,  Single-Loop Federated Actor-Critic across Heterogeneous Environments, AAAI 2025\n\n[4]  Mei et al, On the global convergence rates of softmax policy gradient, ICML 2020"}, "questions": {"value": "- The paper currently compares only against centralized and local training baselines. Could the authors include additional federated reinforcement learning methods such as FedRLHF [1], FedPG [2], or Federated Actor-Critic [3] to provide a fairer and more comprehensive evaluation?\n\n- The Polyak–Łojasiewicz (PL) condition used in Assumption 4 is quite strong. Do the authors have any empirical or theoretical evidence that the global objective in their setting satisfies this condition? Even in standard single-agent reinforcement learning, the objective typically satisfies only a non-uniform Łojasiewicz inequality [4]. How do the authors justify applying a stronger uniform PL assumption to the heterogeneous federated RL setting, where such inequalities are provably violated [2]?\n\n- The theoretical analysis does not include a sample complexity bound. Can the authors provide such an analysis?\n\n- Would it be possible to define heterogeneity in ways that more directly reflect real-world user behavior?\n\n[1] Fan et al, FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF, AAMAS 2025\n\n[2] Labbi et al, On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment, ARXIV 2025\n\n[3] Zhu et al,  Single-Loop Federated Actor-Critic across Heterogeneous Environments , AAAI 2025\n\n[4]  Mei et al, On the global convergence rates of softmax policy gradient, ICML 2020"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UsMwKBvDDV", "forum": "lZ2C7WcWce", "replyto": "lZ2C7WcWce", "signatures": ["ICLR.cc/2026/Conference/Submission21121/Reviewer_ByMC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21121/Reviewer_ByMC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996964826, "cdate": 1761996964826, "tmdate": 1762941371233, "mdate": 1762941371233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FedAgent, a framework for federated reinforcement learning (FRL) applied to LLM-based agents. The goal is to train multiple agents collaboratively without sharing raw data. The authors also build FedAgentGym as an experimental environment including several LLMs (Qwen and Llama series), two benchmarks (WebShop and ALFWorld), and configurable decentralized settings. They propose three new types of heterogeneity in terms of Preference, Coverage, and Hardness to study how client diversity affects learning. Experiments show that FedAgent achieves performance close to centralized training while maintaining data privacy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Extends federated learning to LLM-agent reinforcement learning.\n+ FedAgentGym provides a unified testbed for studying decentralized agent training with multiple models and environments.\n+ Experiments systematically study decentralization parameters and heterogeneity factors."}, "weaknesses": {"value": "- Algorithmic novelty is low, since the method is essentially FedAvg with policy-gradient updates.\n- There is no regret or sample-complexity analysis.  The theoretical analysis directly follows standard federated SGD convergence proofs, but not efficiency or optimality results.\n- the authors assume the global objective satisfies the PL condition. This allows them to show that the federated updates converge at rate O(1/T).  However, for LLM-based reinforcement learning, this assumption might not be realistic, since the rewards are highly non-smooth and multiple local optima might exist.\n-  There is no study of system aspects, such as communication cost, latency, or client synchronization.  So the possible privacy and scalability advantages mentioned are not quantified."}, "questions": {"value": "- Can you provide insight into communication overhead and how it scales with LLM size?  Are there compression or parameter-efficient alternatives?\n- Can you provide empirical evidence supporting the PL or smoothness assumptions for LLM-based policies?\n- Could the framework extend to multi-modal or embodied environments beyond WebShop and ALFWorld?  Or is this out of scope?\n- Could you provide a regret or sample-complexity analysis to strengthen the theoretical foundation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TYNPBqFehD", "forum": "lZ2C7WcWce", "replyto": "lZ2C7WcWce", "signatures": ["ICLR.cc/2026/Conference/Submission21121/Reviewer_iZBy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21121/Reviewer_iZBy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762148632279, "cdate": 1762148632279, "tmdate": 1762941302142, "mdate": 1762941302142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}