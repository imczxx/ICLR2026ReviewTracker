{"id": "sXPaVl9KU6", "number": 6170, "cdate": 1757955919289, "mdate": 1759897931984, "content": {"title": "Heterogeneous Federated Fine-Tuning with Parallel One-Rank Adaptation", "abstract": "Large Language Models (LLMs) have demonstrated remarkable effectiveness in adapting to downstream tasks through fine-tuning. Federated Learning (FL) extends this capability by enabling collaborative fine-tuning across distributed clients using Low-Rank Adaptation (LoRA), while keeping data private. However, practical deployments face challenges when clients have heterogeneous resources and thus adopt different LoRA ranks, leading to substantial initialization and aggregation noise that undermines performance. To address these challenges, we propose Fed-PLoRA, a novel lightweight heterogeneous federated fine-tuning (FFT) framework. Fed-PLoRA introduces Parallel One-Rank Adaptation (PLoRA), a new LoRA variant that replaces the classic multi-rank LoRA module with multiple parallel one-rank modules, and a novel Select-N-Fold strategy that folds untrained PLoRA modules into the pre-trained weights before local training, thereby accommodating heterogeneous client resources. We provide a unified analysis of initialization and aggregation noise of Fed-PLoRA and demonstrate how it addresses the limitations of state-of-the-art methods. Extensive experiments on diverse LLM fine-tuning tasks demonstrate that Fed-PLoRA consistently outperforms existing methods in both accuracy and efficiency. Our code will be openly available.", "tldr": "", "keywords": ["Federated Learning", "Low-Rank Adaptation", "Large Language Models", "Resource Heterogeneity"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/57459f3690ac9c16a8f9c65fc6e36e1d81e52460.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Fed-PLoRA, a framework for heterogeneous federated fine-tuning (FFT) of large language models. The key idea is to decompose each LoRA module into multiple parallel one-rank modules (PLoRA) and introduce a Select-N-Fold strategy that folds untrained modules into the frozen backbone to mitigate initialization and aggregation noise. The paper provides a unified noise analysis and reports performance gains over prior methods such as FLoRA, FlexLoRA, and HETLoRA on GLUE, Natural Instructions, and other datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper addresses a timely problem of heterogeneous resource constraints in federated fine-tuning of LLMs. It proposes a modular reformulation, PLoRA, that can in principle generalize to other LoRA-based methods, and introduces a Select-N-Fold strategy to mitigate initialization and aggregation noise. The experiments span multiple models and benchmarks, providing empirical evidence of the proposed method’s efficiency."}, "weaknesses": {"value": "1. The paper overlooks the training FLOPs and storage overhead of the frozen base model on the client side. From my understanding, each client fine-tunes only its LoRA adapters while keeping the base model parameters fixed. However, even though the base model is frozen, it is still fully involved in the forward passes. When the base model is large (e.g., LLaMA-3.1-8B), its forward FLOPs can dominate the total computation, likely exceeding the resource capacity of many local clients. This makes the proposed setup impractical for real-world federated environments.\n\n2. The implementation code is currently unavailable. Releasing it would greatly improve reproducibility and allow the community to validate its efficiency."}, "questions": {"value": "1. Please describe in more detail how the non-IID data distribution is constructed, and the definition of heterogeneity ratio.\n2. For a fair comparison, the number of trainable parameters should be explicitly reported in the main experimental results. For instance, what are the trainable parameter counts for each baseline method presented in Table 1, Table 2, and Table 3? This information is critical for interpreting both the performance and communication efficiency.\n3. How is the rank $r_i$ for the $i$-th client determined? Can $r_i$ be adaptively assigned according to the complexity of the client’s local task, or it only depends on the client resources. \n4. In Table 2, FLoRA performs near random guessing on the CoLA benchmark. Could the authors provide an explanation for this? Even though there exists large initialization noise in the stage of broadcast and initialization. \n5. What is the definition of the communication cost in Figure 5, please clarify it or provide the clear reference. Does the communication occur every local training iteration？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A47rfd21Kl", "forum": "sXPaVl9KU6", "replyto": "sXPaVl9KU6", "signatures": ["ICLR.cc/2026/Conference/Submission6170/Reviewer_x1y8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6170/Reviewer_x1y8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714341740, "cdate": 1761714341740, "tmdate": 1762918514525, "mdate": 1762918514525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the critical challenge of heterogeneous LoRA ranks in Federated Fine-Tuning (FFT). The authors identify that current methods suffer from initialization noise and aggregation noise due to rank mismatches. Their proposed solution, Fed-PLORA, uses PLORA (Parallel One-Rank Adaptation) to re-parameterize LoRA modules as a sum of parallel rank-1 components . This enables a novel Select-N-Fold strategy, where resource-constrained clients train a random subset of modules and \"fold\" the rest into the frozen weights. This method is theoretically shown to achieve zero initialization noise and is empirically demonstrated to consistently outperform existing heterogeneous FFT methods like FLORA, HETLORA, and FlexLoRA."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear and Compelling Motivation:** The paper is exceptionally well-motivated. It formalizes the *specific failure modes* of prior art: initialization noise and aggregation noise. The entire paper is a clear and focused effort to solve these two problems.\n- **Strong Theoretical Analysis:** The noise analysis theoretically proves that the proposed Fed-PLORA framework eliminates initialization noise and provides a powerful and fundamental justification for the method's design.\n- **Simple and Effective Methodology:** The proposed solution is both elegant and practical.\n- **Comprehensive Empirical Validation:** The experimental results are strong and thorough. The authors test on a wide variety of models and diverse datasets."}, "weaknesses": {"value": "- Adding pseudocode for the algorithm would improve clarity.\n- It appears that PLoRA requires downloading the entire global LoRA model. In contrast, other methods—if rank is publicly available—can use much smaller downloads. Although Section 4.2 addresses this, the R − ri downlink cost could be quite high when R is large and ri is small, potentially causing synchronization issues."}, "questions": {"value": "PLoRA folds the remaining R − ri untrained rank modules into the pretrained weight. Is this different from sparse LoRA tuning where the corresponding R − ri untrained rows are frozen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DeVT4YXLAb", "forum": "sXPaVl9KU6", "replyto": "sXPaVl9KU6", "signatures": ["ICLR.cc/2026/Conference/Submission6170/Reviewer_AmKz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6170/Reviewer_AmKz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923258460, "cdate": 1761923258460, "tmdate": 1762918514047, "mdate": 1762918514047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets a problem of resource heterogeneity in Federated Learning (FL) for fine-tuning LLMs. A common solution for federated fine tuning for clients with different computational resources is to adapt LoRA modules of different ranks. The authors argue that this heterogeneity introduces two primary issues: initialization noise (when low-resource clients must truncate or discard parts of the global model) and aggregation noise (when the server attempts to combine modules of different dimensions).\n\nTo solve this, the paper proposes Fed-PLORA, a novel framework built on two core ideas: 1) Parallel One-Rank Adaptation  or PLORA where instead of having a standard rank-R LoRA module, they propose utilizing the  sum of R parallel one-rank modules. 2) Select-N-Fold Strategy which is A new initialization and training protocol.\n\nThe authors claim that the \"Select-N-Fold\" strategy completely eliminates initialization noise, as no information from the global model is \ndiscarded. They analyze the remaining aggregation noise and argue it is minimal. Through extensive experiments, they demonstrate that Fed-PLORA outperforms existing heterogeneous FFT methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well-written and the authors did a good job explaining the existing problems. \n\n* Through various settings and different empirical results the authors show the merits of their algorithms. \n\n* The authors did a proper ablation study, explaining the importance of each component."}, "weaknesses": {"value": "One important aspect of the paper is the Downlink Communication cost. The \"Select-N-Fold\" strategy has one clear limitation that is understated: downlink communication cost."}, "questions": {"value": "Can you make a table (at least for one setting) to show all the new costs of your method and compare with the prior works?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IBtvWzcK9G", "forum": "sXPaVl9KU6", "replyto": "sXPaVl9KU6", "signatures": ["ICLR.cc/2026/Conference/Submission6170/Reviewer_G2mv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6170/Reviewer_G2mv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982741230, "cdate": 1761982741230, "tmdate": 1762918513680, "mdate": 1762918513680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Fed-PLoRA addresses federated fine-tuning with heterogeneous LoRA ranks by proposing a new framework to mitigate both issues where rank mismatches introduce substantial initialization and aggregation noise. It introduces PLoRA, which replaces a single rank-R LoRA module with R parallel rank-1 modules that are mathematically equivalent to standard LoRA. Combined with a Select-N-Fold strategy, the method achieves zero initialization noise and reduces aggregation noise under heterogeneous budgets. Experiments across multiple LLM fine-tuning tasks (e.g., GLUE and instruction-following) show consistent accuracy gains over FLoRA, FlexLoRA, and HETLoRA, while avoiding the heavy SVD overhead that hurts communication and training time. It’s easy to adopt in practice though broadcasting all R modules does add some downlink cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper precisely defines initialization and aggregation noise in heterogeneous LoRA settings and shows Fed-PLoRA removes the former and reduces the latter.\n2.  PLoRA’s parallel rank-1 decomposition is mathematically equivalent to standard LoRA yet naturally supports heterogeneity; paired with Select-N-Fold, it guarantees zero initialization noise while curbing aggregation noise. \n3. Strong and robust empirical results. Fed-PLoRA consistently outperforms FLoRA/FlexLoRA/HETLoRA across tasks (e.g., GLUE), and remains robust as client counts and rank distributions vary, including challenging non-IID scenarios."}, "weaknesses": {"value": "1. The method broadcasts all R parallel rank-1 modules to every client and asks clients to keep folded modules, downlink traffic and on-device storage could become non-trivial in weak-network or mobile scenarios.\n2. The paper’s empirical validation relies on relatively small or outdated and non-unified base models, which limits generalizability; it would be stronger to standardize on modern backbones like Qwen3 and Llama 3.2 across multiple sizes.\n3. Its benchmark suite skews toward easier tasks (e.g., GLUE and basic instruction following) and should incorporate more rigorous reasoning/knowledge evaluations such as MMLU-Pro, GPQA, MuSR, MATH, IFEval, and BBH.\n4. The authors do not clearly report the untuned base-model performance, obscuring absolute gains."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "AMLLyqbZjK", "forum": "sXPaVl9KU6", "replyto": "sXPaVl9KU6", "signatures": ["ICLR.cc/2026/Conference/Submission6170/Reviewer_HFN8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6170/Reviewer_HFN8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762245082563, "cdate": 1762245082563, "tmdate": 1762918513225, "mdate": 1762918513225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}