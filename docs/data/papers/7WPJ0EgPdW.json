{"id": "7WPJ0EgPdW", "number": 20900, "cdate": 1758311536228, "mdate": 1763759845741, "content": {"title": "AMiD: Knowledge Distillation for LLMs with $\\alpha$-mixture Assistant Distribution", "abstract": "Autoregressive large language models (LLMs) have achieved remarkable improvement across many tasks but incur high computational and memory costs. Knowledge distillation (KD) mitigates this issue by transferring knowledge from a large teacher to a smaller student through distributional alignment. Previous studies have proposed various discrepancy metrics, but the capacity gap and training instability caused by near-zero probabilities, stemming from the high-dimensional output of LLMs, remain fundamental limitations. To overcome these challenges, several approaches implicitly or explicitly incorporating assistant distribution have recently been proposed. However, the past proposals of assistant distributions have been a fragmented approach without a systematic investigation of the interpolation path and the divergence. This paper proposes $\\alpha$-mixture assistant distribution, a novel generalized family of assistant distributions, and $\\alpha$-mixture distillation, coined AMiD, a unified framework for KD using the assistant distribution. The $\\alpha$-mixture assistant distribution provides a continuous extension of the assistant distribution by introducing a new distribution design variable $\\alpha$, which has been fixed in all previous approaches. Furthermore, AMiD generalizes the family of divergences used with the assistant distributions based on optimality, which has also been restricted in previous works. Through extensive experiments, we demonstrate that AMiD offers superior performance and training stability by leveraging a broader and theoretically grounded assistant distribution space.", "tldr": "This paper proposes $\\alpha$-mixture assistant distribution, a novel generalized family of assistant distributions, and $\\alpha$-mixture distillation, coined AMiD, a unified framework for knowledge distillation using the assistant distribution.", "keywords": ["Knowledge distillation", "Large language model", "Information geometry"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0d87df0955d4354b1aa0661416f5ece1ea31233f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an α-mixture assistant distribution and a corresponding knowledge distillation framework named AMiD for large language models (LLMs). It unifies existing methods (e.g., m-mixture and e-mixture) as special cases within a broader, theoretically grounded family parameterized by α. The framework is supported by theoretical analysis (optimality, gradient behavior, support properties) and extensive experiments showing consistent improvements over prior approaches across various tasks, model scales, divergences, and data generation strategies."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Introduces a principled and generalized family of assistant distributions via α-mixture, offering a unified view of prior fragmented methods.  \nProvides solid theoretical grounding, including optimality guarantees, support analysis, continuity, and gradient-based interpretation of mode-covering vs. mode-seeking behavior.  \nComprehensive experiments validate the effectiveness of AMiD across instruction-following, task-specific distillation, different student sizes, divergences, and SGO strategies.  \nThe new design variable α offers practical control over the quality-diversity trade-off, independent of the interpolation weight λ."}, "weaknesses": {"value": "The writing is not very clear and significantly hampers readability.** Despite strong technical content, the exposition is often dense and poorly structured, especially in Section 3. Key concepts (α-mixture, f-mean, α-divergence) are introduced rapidly without sufficient intuition or gradual buildup, making it difficult for readers to follow.  \nFigures (e.g., Figure 1 and 2) lack detailed captions and fail to fully clarify the geometric impact of α on interpolation paths.  \nLimited discussion on why α ≠ ±1 performs better in practice beyond empirical results; the gap between theoretical optimality and practical instability (e.g., DRKL with α=1) is noted but not deeply analyzed.  \nSome notation is ambiguous or inconsistently used (e.g., r vs. r̃, θ vs. θ′), adding unnecessary cognitive load."}, "questions": {"value": "1. How should one choose the optimal α in practice? Are there adaptive or task-aware strategies for tuning α during training?  \n2. The paper fixes the divergence (e.g., DAB) while varying α. Have the authors explored joint optimization or tuning of both α and divergence parameters (e.g., α_AB, β_AB)?  \n3. Theorem 3.4 claims optimality for any divergence and α, yet Table 3 shows catastrophic failure for DRKL with α=1. Does this indicate that the “perfect optimization” assumption is too strong, and how should practitioners navigate the theory-practice gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nwTMcCNuQQ", "forum": "7WPJ0EgPdW", "replyto": "7WPJ0EgPdW", "signatures": ["ICLR.cc/2026/Conference/Submission20900/Reviewer_RkqC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20900/Reviewer_RkqC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816180639, "cdate": 1761816180639, "tmdate": 1762938027987, "mdate": 1762938027987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents  a unified framework for knowledge distillation with an assistant distribution, where the teacher and student distributions are mixed to bridge the capacity gap between them. Previous distillation mixing strategies (e.g., using arithmetic and geometric mean) become special cases under this framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors presents an interesting point of view on existing distillation methods.\n2. The experiments are generally comprehensive, showing consistent improvements."}, "weaknesses": {"value": "1. The motivation of adjusting alpha is still a bit unclear to me. The authors mention alpha adjusts the mode-seeking and mode-covering properties. However, this can also be addressed in the divergence function, as mentioned in 2.1. I am not too sure about the intuition behind doing this again in the mixing stage.\n\n2. The paper mainly focuses on instruction following, evaluated by rouge. This is slightly concerning, because the approach may be hacking the ROUGE score rather than making the model better at instruction following. The paper showed very limited results on reasoning (GSM8K, omitting ABKD for some reason, and no standard deviations). \n\n3. The paper is not very clear on its hyper-parameters. For example, the author mentions that their approach work with any divergence functions, but it's not clear which one is used for Table 1. Also, the values of alpha/lambda is not shown on the table. The author mentions D_AB being used for Table 2, but how about Table 1. Even in Table 2, it says alpha is not -1/+1, but it doesn't state what alpha is. My main worry is that the authors may have spent lots of efforts on tuning these individually, which gave the approach an unfair advantage."}, "questions": {"value": "See weaknesses about hyper-parameters."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QHjC8pbUKF", "forum": "7WPJ0EgPdW", "replyto": "7WPJ0EgPdW", "signatures": ["ICLR.cc/2026/Conference/Submission20900/Reviewer_yQkB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20900/Reviewer_yQkB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890219808, "cdate": 1761890219808, "tmdate": 1762937977671, "mdate": 1762937977671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the capacity gap between teacher and student models and the training instability caused by high-dimensional output in LLM knowledge distillation (KD). It proposes α-mixture assistant distribution and a unified distillation framework AMiD (α-mixture distillation). By systematically expanding the auxiliary distribution and divergence selection, it achieves better performance and training stability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Generalization and Unification of Fragmented Methods.\n\nProposes a novel α-mixture assistant distribution by extending generalized \\(f_\\alpha\\)-mean to KD, introducing the tunable parameter α. This generalizes prior isolated assistant distributions (m-mixture for α=-1, e-mixture for α=1) into a continuous, flexible family, covering new distributions (e.g., harmonic mean for α=3) not explored in LLM KD before.\n\n\n2. Rigorous Theory and Comprehensive Experiments. Provides formal proofs for key properties (continuity of α-mixture distribution, optimality of AMiD, gradient analysis of f-divergence), establishing a solid mathematical foundation for the framework. Validates AMiD across diverse settings, task-agnostic (5 instruction-following datasets) and task-specific (translation, summarization, reasoning) distillation; multiple model scales (GPT-2 series, OpenLLaMA); various divergences (KL, RKL, AB) and SGO strategies. Results are consistent and statistically robust (5 random seeds), confirming reliability.\n\n3. Addressing Core Limitations and Advancing Practical KD. Directly mitigates the capacity gap between teacher and student models and training instability from near-zero probabilities, two fundamental limitations of prior KD methods. AMiD’s flexibility (compatible with arbitrary divergences and datasets) and robustness (stable performance across λ values) make it adaptable to real-world LLM compression scenarios. The α parameter offers a simple control knob to balance quality and diversity, addressing a longstanding trade-off in generation tasks."}, "weaknesses": {"value": "1. The lack of a systematic strategy for α parameter tuning limits its practicality. The paper demonstrates that the α parameter can control the balance between \"pattern coverage\" and \"pattern finding\" in the student model, but it fails to provide the basis for α selection and efficient tuning methods. The experiments only demonstrate the effects of fixing α (e.g., α=-5, -3, etc.), without explaining the optimal range and tuning logic of α under different tasks (translation/summarization/inference), different model capacity differences (e.g., 10B→0.1B), and different divergences (D), leading to extensive trial and error for users in practical applications.\n\n2. The paper theoretically proves the optimality of AMiD under \"perfect optimization,\" but two unresolved contradictions exist in practice: a) Conflict between divergence and α: Experiments show that the performance of D_RKL combined with α=1 is extremely poor (Avg. only 3.94 in Table 3), attributed to the narrow intersection of support sets, but no specific criteria are given on how to avoid this conflict;\n\nb) Adaptation of optimizer and hyperparameters: The impact of different optimizers (such as AdamW, Lion) and learning rate scheduling on AMiD is not discussed, while in practice, optimizer selection significantly changes the gradient propagation effect of α-mixture.\n\n3. The ablation experiments do not fully decompose the core contributions of AMiD: The interaction between α and λ is not verified: α is only tested with λ=0.1, without analyzing whether the optimal value of α changes under different λ (such as 0.3/0.7), and the collaborative tuning strategy between the two; the contributions of α-mixture and divergence are not isolated: Is the performance improvement of AMiD due to the auxiliary distribution of α expansion or the flexibility of divergence? Verification needs to be conducted by comparing \"fixed α = ±1 (i.e., baseline auxiliary distribution) + arbitrary divergence\" with \"fixed divergence + α ≠ ±1\".\n\n4. The paper's baseline does not include cutting-edge LLM distillation methods since 2025, resulting in insufficient proof of advancement. For example, it does not compare distillation methods based on contrastive learning or reinforcement learning-based distillation (RL-KD), which may have already surpassed traditional divergence-based baselines in some scenarios."}, "questions": {"value": "1. Adaptive Selection of α Parameter.  The paper demonstrates that α controls the trade-off between mode-seeking and mode-covering, but it does not provide a systematic method for selecting α in different scenarios (e.g., different task types, model size gaps, or data characteristics). Is there an adaptive strategy to determine α (e.g., curriculum learning-based scheduling, data-driven tuning) instead of manual adjustment? For small student models (e.g., 0.1B) vs. large gaps (e.g., 7B→0.5B), does the optimal α range differ significantly?\n\n2. Extensibility to Larger Model Scales and Complex Tasks. The experiments focus on GPT-2 (up to 1.5B teacher) and OpenLLaMA-7B→3B. Have the authors tested AMiD on larger teacher models (e.g., 10B+ LLMs like LLaMA 3 70B) or smaller student models (e.g., <0.1B)? Additionally, do the performance gains hold for complex tasks such as code generation, mathematical reasoning with multi-step logic, or cross-lingual understanding?\n\n3. Mitigation of Instability in Specific Divergence-α Combinations. The paper notes that the combination of \\(D_{RKL}\\) and α=1 leads to extremely poor performance due to support intersection issues. Have the authors explored mitigation strategies (e.g., adjusting λ, adding regularization terms, or modifying the assistant distribution’s normalization) instead of simply avoiding this combination? Are there general principles to avoid such conflicting pairs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "STSJmYknvw", "forum": "7WPJ0EgPdW", "replyto": "7WPJ0EgPdW", "signatures": ["ICLR.cc/2026/Conference/Submission20900/Reviewer_Gj8D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20900/Reviewer_Gj8D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959121204, "cdate": 1761959121204, "tmdate": 1762937912563, "mdate": 1762937912563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response [1/3]"}, "comment": {"value": ">**GQ1. Is there a range or tuning strategy that can be used to effectively identify generally good values of $\\alpha$? Moreover, are there any strategies for selecting $\\alpha$ in an adaptive manner?**\n\n**[Theoretical insight based tuning guidelines for $\\alpha$]** Our proposed $\\alpha$-mixture assistant distribution employs a new distribution design variable $\\alpha$, which unifies and generalizes existing assistant distributions into a coherent distribution family. While this additional variable provides substantial flexibility and opens up unexplored assistant distribution candidates, identifying appropriate $\\alpha$ values that achieve consistently stable and strong performance is important.\n\nWe address this concern by providing tuning guidelines grounded in the theoretical properties of $\\alpha$ analyzed in the manuscript. First, in Section 3.2, we show that the support of the $\\alpha$-mixture assistant distribution varies with $\\alpha$: $supp(r_\\theta^{(\\alpha,\\lambda)}) = supp(p) \\cup supp(q_\\theta)$ when $\\alpha < 1$, $supp(r_\\theta^{(\\alpha,\\lambda)}) = supp(p) \\cap supp(q_\\theta)$ when $\\alpha \\geq 1$. In KD for LLMs, the teacher and student models often exhibit a capacity gap and produce high-dimensional outputs with many near-zero probabilities. As a result, they do not share a sufficiently large common support in general. For this reason, we recommend using $\\alpha < 1$ in most practical settings, as this choice improves training stability and enables more reliable knowledge transfer. \n\nFurthermore, our gradient analysis and experimental results demonstrate that $\\alpha$ enables to control the trade-off between mode-covering and mode-seeking behavior of the optimized student distribution. Under the $\\alpha \\leq 1$, increasing $\\alpha$ relatively encourages mode-covering, thereby improving the diversity of outputs. Conversely, smaller $\\alpha$ emphasizes mode-seeking, which enhances fidelity to the teacher. Therefore, for enhancing the teacher-student alignment and performance, we suggest using small $\\alpha$ values. However, since too small $\\alpha$ can induce high curvature in the geometry of interpolation path, which may reduce optimization efficiency, so such choices should be used with caution.\n\nBased on these theoretical insights, we basically consider $\\alpha$ over [−5, −3, −1, −0.5, 0, 0.5, 1.0]. We observe consistently stable and strong performance across most of these candidates.\n\n---\n**[Overlap-based adaptive $\\alpha$ scheduling]** The theoretical insights based $\\alpha$ tuning guidelines efficiently exclude low potential candidates. However, applying a single fixed global $\\alpha$ value can still be sub-optimal in certain cases. \n\nTo address this concern, we introduce a curriculum-based adaptive $\\alpha$ scheduling based on the degree of overlap between token-level teacher distribution $p(y_l | y_{<l}, x)$ and student distribution $q_\\theta(y_l | y_{<l}, x)$. The intuition is that when the teacher and student distributions are highly overlapped, we encourage mode-covering to align further, whereas when the overlap is low, we enhance mode-seeking to find the mode first.\n\nWe define token-level overlap as $ovl_{i,l} := \\sum_{y_l} \\min(p(y_l | y_{<l}, x), q_\\theta(y_l | y_{<l}, x))$. Obviously, the overlap value $ovl_{i,l}$ is bounded into $[0, 1]$. Also, $ovl_{i,l}$ approaches $0$ when $p(y_l | y_{<l}, x)$ and $q_\\theta(y_l | y_{<l}, x)$ significantly differ, and approaches $1$ when they are well-aligned. Furthermore, $ovl_{i,l}$ can be expressed in terms of total variation distance $1-TVD(p(y_l | y_{<l}, x), q_\\theta(y_l | y_{<l}, x))$, which provides same interpretation. \n\nGiven the predefined $\\alpha_{min}$ and $\\alpha_{max}$, we set the token-level $\\alpha_{i,l}$ as the linearly increasing value along the line passing through $(0, \\alpha_{min})$ and $(1, \\alpha_{max})$ i.e. $\\alpha_{i,l} \\leftarrow (\\alpha_{max} - \\alpha_{min}) * ovl_{i,l} + \\alpha_{min}$. Under this idea, when the teacher and student distributions differ substantially, the $ovl_{i,l}$ becomes small, leading to a smaller assigned $\\alpha$, which strengthens mode-seeking. Conversely, when the teacher and student distributions are similar, both $ovl_{i,l}$ and $\\alpha$ have larger values, thereby reinforcing mode-covering. This mechanism systematically determines $\\alpha$ by combining the degree of alignment between the teacher and student distribution, which continuously changes through training, with the theoretical characteristics of $\\alpha$. *(Continued below)*"}}, "id": "75DPT0A8kc", "forum": "7WPJ0EgPdW", "replyto": "7WPJ0EgPdW", "signatures": ["ICLR.cc/2026/Conference/Submission20900/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20900/Authors"], "number": 17, "invitations": ["ICLR.cc/2026/Conference/Submission20900/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763759725940, "cdate": 1763759725940, "tmdate": 1763759725940, "mdate": 1763759725940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response [2/3]"}, "comment": {"value": "*(Continued)* Below pseudocode presents the batch-wise training scheme of AMiD with proposed overlap-based adaptive $\\alpha$ scheduling.\n```\nGiven: Dataset $\\mathcal{D}$, Divergence $D$, $\\alpha_min$, $\\alpha_max$, $\\lambda$\n1. For each iteration\n2.     Sample mini-batch $B$ from $\\mathcal{D}$\n3.     Obtain batch-level distribution of teacher $p$ and student $q$    // [B, L, V] shape\n4.     Calculate batch-level overlap value $ovl \\leftarrow \\min(p, q).sum(-1)$    // [B, L] shape\n5.     Calculate batch-level $\\alpha \\leftarrow (\\alpha_{max} - \\alpha_{min}) * ovl + \\alpha_{min}$    // [B, L] shape\n6.     Reshape $\\alpha$    // [B, L, V] shape\n7.     Calculate $r_\\theta^{(\\alpha, \\lambda)}$    // [B, L, V] shape\n8.   Update $\\theta$ by minimizing $D(\\cdot, r_\\theta^{(\\alpha, \\lambda)})$    // either $p$ or $q_\\theta$ is possible.\n```\n\nThe table below presents the performance of fixed $\\alpha$ and overlap-based adaptive $\\alpha$ scheduling under the $D_{AB}$ with $\\alpha_{AB}=0.2, \\beta_{AB}=0.7$ and $\\lambda = 0.1$. For fixed approach, we set $\\alpha = -5.0$ which is best performer among the fixed methods. For adaptive approach, we set $\\alpha_{min} = -5.0, \\alpha_{max} = -1.0$. As shown in the table below, the overlap-based adaptive $\\alpha$ scheduling achieves higher performance than the best fixed global $\\alpha$ strategy, demonstrating its effectiveness.\n\n| | Val. (↑) | Dolly Eval (↑) | Self Inst (↑) | Vicuna (↑) | Super NI (↑) | UnNI (↑) | Avg. (↑) |\n|-|-|-|-|-|-|-|-|\n| AMiD (Fixed $\\alpha$) | 29.24 | 26.44 | 13.74 | 16.76 | 29.71 | 30.35 | 23.40 |\n| AMiD (Overlap-based adaptive $\\alpha$) | 29.31 | 26.50 | 14.02 | 16.59 | 29.87 | 30.60 | **23.52** |\n\nWe also compare the wall-clock time for each training step. As shown in the table below, the proposed overlap-based adaptive $\\alpha$ scheduling shows a negligible time complexity increase compared to no-assistant baseline (ABKD) and fixed $\\alpha$ approach due to the simple structure while theory-based approach.\n\n| | Avg. of wall-clock time of single training step (sec) |\n|-|-|\n| ABKD | 1.17 |\n| AMiD (fixed $\\alpha$) | 1.26 |\n| AMiD (overlap-based adaptive $\\alpha$) | 1.29 |\n\nWe believe that more sophisticated adaptive $\\alpha$ scheduling, such as bi-level optimization $\\min_{(\\alpha, \\lambda)} D(p, q_{\\theta’})$ where $\\theta’ = argmin_{\\theta} D(p, r_{\\theta}^{(\\alpha, \\lambda)})$, could further enhance performance. Investigating such adaptive $\\alpha$ and/or $\\lambda$ scheduling could be interesting research direction.\n\n---\n>**GQ2. Why does specific combination of divergence and $\\alpha$ e.g., $D_{RKL}(p || r_\\theta^{(\\alpha,\\lambda)})$ with $\\alpha = 1$ exhibit low performance? Is there a mitigation strategy for this combination?**\n\n**[Narrow support is the primary cause]** We have discussed this issue in Appendix D of the previous manuscript. In particular, for certain divergences, such as $D_{RKL}$, involve an expectation with respect to $r$ by the definition. When $\\alpha = 1$, the support of $\\alpha$-mixture assistant distribution equals to the intersection of the teacher and student distribution’s supports, which can be narrow in high-dimensional LLM outputs. Consequently, this small support might induce unstable training and insufficient knowledge transfer. Therefore, restricting $\\alpha < 1$ is simple and guarantees the theoretical stability in most cases.\n\n**[Mitigation via distribution softening]** Since the primary cause of this issue is the narrow support of the $\\alpha$-mixture assistant distribution, we conjecture that applying distribution softening technique could alleviate the instability even for such problematic combinations. To verify this conjecture, we employ temperature $T > 1$, which is a widely used flattening technique in various area. The table below shows the performance when using various temperature values under $D_{RKL}(p || r_\\theta^{(\\alpha,\\lambda)})$ with $\\alpha = 1$. The results exhibit that introducing the temperature leads to stable training and can even yield high performance with an appropriately chosen value. However, large temperature causes an over-flattening effect, inducing large shift in the $\\alpha$-mixture assistant distribution and consequently degrading performance. Overall, we demonstrate that employing temperature scaling can empirically mitigate the instability associated with problematic combinations under the appropriate temperature value.\n\n| | Val. (↑) | Dolly Eval (↑) | Self Inst (↑) | Vicuna (↑) | Super NI (↑) | UnNI (↑) | Average (↑) |\n|-|-|-|-|-|-|-|-|\n| AMiD ($\\alpha = 1.0, T = 1.0$) | 0.16 | 4.27 | 2.81 | 9.12 | 1.64 | 1.84 | 3.94 |\n| AMiD ($\\alpha = 1.0, T = 1.5$) | 27.40 | 24.61 | 11.55 | 17.11 | 21.41 | 23.39 | 19.61 |\n| AMiD ($\\alpha = 1.0, T = 2.0$) | 28.64 | 26.83 | 12.74 | 17.62 | 23.56 | 27.01 | 21.55 |\n| AMiD ($\\alpha = 1.0, T = 5.0$) | 28.78 | 26.78 | 12.43 | 17.30 | 25.87 | 27.74 | 22.02 |\n| AMiD ($\\alpha = 1.0, T = 10.0$) | 27.33 | 24.73 | 12.11 | 16.95 | 23.65 | 26.80 | 20.85 |"}}, "id": "tU91KqMp9s", "forum": "7WPJ0EgPdW", "replyto": "7WPJ0EgPdW", "signatures": ["ICLR.cc/2026/Conference/Submission20900/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20900/Authors"], "number": 18, "invitations": ["ICLR.cc/2026/Conference/Submission20900/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763759754893, "cdate": 1763759754893, "tmdate": 1763759754893, "mdate": 1763759754893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response [3/3]"}, "comment": {"value": ">**GQ3. What is the relationship between $\\alpha$ and $\\lambda$?**\n\n**[Theoretical relationship between $\\alpha$ and $\\lambda$]** The proposed $\\alpha$-mixture assistant distribution is the mixture of the teacher distribution $p$ and student distribution $q_\\theta$ via the generalized $f_\\alpha$-mean. From the information geometry perspective, $\\alpha$ controls the geometry of interpolation path and $\\lambda$ determines the portion of interpolation as depicted in Figure 3a of the revised manuscript (Figure 2a of the revised manuscript). Since the generalized $f_\\alpha$-mean only depends on $\\alpha$, when $\\alpha$ is fixed, $\\lambda$ only enables to adjust the ratio between $p$ and $q_\\theta$ along the determined path.\n\n---\n**[Empirical relationship between $\\alpha$ and $\\lambda$]** We employ $\\lambda = 0.1$ as our default setting by following the prior work [1]. To further investigate the empirical relationship between $\\alpha$ and $\\lambda$, we conducted additional experiments on $\\lambda = 0.5, 0.9$ with $D_{AB}(p||r_\\theta^{(\\alpha,\\lambda)})$. The table below presents the average performance of five instruction-following datasets among the various $\\alpha$ and $\\lambda$ combinations. The analyses of experimental results are as follow:\n\n* Across all tested values of $\\lambda$, using smaller $\\alpha$ consistently achieves higher performance. This observation aligns with our theoretical analysis indicating that smaller $\\alpha$ relatively induces a more mode-seeking behavior. \n* When $\\lambda$ is too large ($\\lambda = 0.9$), performance slightly degrades and exhibits a larger standard deviation. We attribute this to the $\\alpha$-mixture assistant distribution being overly close to the teacher distribution, which (1) limits effective knowledge transfer and (2) makes the optimization more sensitive to curvature variations. In contrast, $\\lambda = 0.5$ achieves both high and stable performance, demonstrating that choosing a midpoint provides robustness against curvature changes. \n* Compared to $\\lambda$, the performance is generally more robust to changes in $\\alpha$, as indicated by lower standard deviations. However, the $\\alpha$ values close to 1 show higher standard deviations. We conjecture that this instability arises because the change of mixing coefficient ($\\lambda$) between the teacher and student distributions makes hard mode-covering. \n\nWe present these summarized results as Figure 6 and entire results as Table 14 in the revised manuscript.\n\n| | $\\alpha = -5.0$ | $\\alpha = -3.0$ | $\\alpha = -1.0$ | $\\alpha = -0.5$ | $\\alpha = 0.0$ | $\\alpha = 0.5$ | $\\alpha = 1.0$ | Avg. along $\\alpha$ | std. along $\\alpha$ |\n|-------------------------|----------|----------|----------|----------|---------|---------|---------|---------------|----------------|\n| $\\lambda = 0.1$ | 23.40    | 23.10    | 22.45    | 22.51    | 22.25   | 21.87   | 15.92   | 21.64         | 2.58           |\n| $\\lambda = 0.5$ | 23.29    | 23.02    | 23.13    | 22.80    | 21.87   | 20.02   | 17.46   | 21.66         | 2.17           |\n| $\\lambda = 0.9$ | 23.12    | 23.25    | 22.89    | 22.75    | 21.99   | 18.58   | 15.04   | 21.09         | 3.12           |\n| Avg. along $\\lambda$ | 23.27    | 23.12    | 22.82    | 22.69    | 22.04   | 20.16   | 16.14   | —             | —              |\n| std. along $\\lambda$ | 0.14     | 0.12     | 0.34     | 0.16     | 0.19    | 1.65    | 1.22    | —             | — |\n\n[1] DistiLLM: Towards Streamlined Distillation for Large Language Models"}}, "id": "W824qHn6KQ", "forum": "7WPJ0EgPdW", "replyto": "7WPJ0EgPdW", "signatures": ["ICLR.cc/2026/Conference/Submission20900/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20900/Authors"], "number": 19, "invitations": ["ICLR.cc/2026/Conference/Submission20900/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763759793320, "cdate": 1763759793320, "tmdate": 1763759793320, "mdate": 1763759793320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Dear Reviewers"}, "comment": {"value": "Dear Reviewers\n\nWe sincerely appreciate the constructive feedback on our work and are grateful for the time and effort you have dedicated to reviewing it. In the following, we address the concerns commonly raised across reviewers, followed by individual responses to each reviewer’s specific concerns. We have revised the manuscript accordingly, with highlighted in purple. Please check our responses and feel free to provide any additional comments. We would be happy to offer further clarification on any points that remain unclear.\n\nBest regards"}}, "id": "MrvxoHG2gN", "forum": "7WPJ0EgPdW", "replyto": "7WPJ0EgPdW", "signatures": ["ICLR.cc/2026/Conference/Submission20900/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20900/Authors"], "number": 20, "invitations": ["ICLR.cc/2026/Conference/Submission20900/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763759919467, "cdate": 1763759919467, "tmdate": 1763759919467, "mdate": 1763759919467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}