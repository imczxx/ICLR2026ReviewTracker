{"id": "I29dOHf74X", "number": 10318, "cdate": 1758166996520, "mdate": 1759897659043, "content": {"title": "Regret Analysis of RMSProp and AdamNC for Training Deep Interpolating Neural Networks", "abstract": "We provide a theoretical analysis for RMSProp and AdamNC (Adam without corrective terms) for training deep fully-connected neural networks with smooth activations, in the online learning setting. We focus on the binary classification tasks with logistic loss or exponential loss. We assume that the model can interpolate data, i.e., it can obtain an arbitrarily small loss $\\varepsilon$, while the distance to the initialization is bounded by a decreasing function $g(\\varepsilon)$. We show that the regret is upper bounded by $\\mathcal{O}(\\text{poly} [g(1/T )])$ provided that the width is at least $\\mathcal{O}(\\text{poly} [g(1/T )])$, where $T$ is the total iteration number. We further show that under NTK-separability, the regret is less than $\\mathcal{O}(\\text{poly}(\\log T))$ when the width is larger than $\\mathcal{O}(\\text{poly}(\\log T))$. We also provide a comparable regret bound for the scalar version of RMSProp and AdamNC, without requiring prior knowledge of problem parameters for learning rates. Our analysis can also be applied to smooth losses, leading to similar regret bounds.", "tldr": "", "keywords": ["regret analysis", "root mean square propagation", "deep neural network", "online learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e3e317601eb246c0ae9250c4e8b692d73ef5242.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper gives a regret analysis of RMSProp/AdamNC when training deep smooth neural networks in an online manner specifically in the NTK interpolating setting. The main results are sufficient conditions on the width of the network and iterations numbers that ensure regret analysis and the derivation of the regret bounds which parallel online SGD bounds. The approach is based on exploiting the weak-convexity of the objective trained by losses such as logistic loss in the kernel regime."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The majority of the paper and the stated results are well-written. The studied problem is new, whereas prior works mainly tackled the convex case. The results show that even small networks of poly-logarithmic width can have favorable regret bounds. The analysis nicely extends the current GD analyses to ADAM and covers the constant step-size for this algorithm."}, "weaknesses": {"value": "-The take-away message of the paper is not well-stated. In particular, a brief discussion on the following questions seems lacking from the current version: what are the distinctions between the bounds resulting from this over bounds resulting from using S/GD? Can ADAM improve upon GD in width requirements or final regret bounds? Are the current bounds tight? How does the analysis stand compared to the analysis of convex objectives? \n\n-The analysis seems to rely on the known methods (especially (Taheri and Thrampoulidis, 2024)). It can be clarifying if you can include a discussion on the distinct steps from the current SGD analyses. \n\n\n-Do experiments verify the bounds on the regret bounds or the width conditions?"}, "questions": {"value": "please see above section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X9GCJ70IbU", "forum": "I29dOHf74X", "replyto": "I29dOHf74X", "signatures": ["ICLR.cc/2026/Conference/Submission10318/Reviewer_Csr8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10318/Reviewer_Csr8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809267939, "cdate": 1761809267939, "tmdate": 1762921658754, "mdate": 1762921658754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides convergence guarantees of training a deep neural network with RMSProp and AdamNC under interpolation assumption. Their assumptions and proof resemble [1] in fashion: the two important points are using an approximate convexity of deep neural network, and using the interpolation point as a \"reference point\" that has low training loss. With the interpolation assumption the regret becomes a function of $g$ that dictates the interpolation property, which is potentially better than standard results for well-behaving $g$. \n\n[1] Taheri, Hossein, and Christos Thrampoulidis. \"Generalization and stability of interpolating neural networks with minimal width.\" Journal of Machine Learning Research 25.156 (2024): 1-41."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "It is a solid contribution to extend certain results to different optimizers. Convergence guarantee better than O(\\sqrt(T)) can be attained by certain assumptions look interesting.\n\nAlso, the paper is very well written and easy to understand the mathematical formalism. The lemmas are well stated, with exact assumptions, with theorems that look valid."}, "weaknesses": {"value": "It would be better if more motivation was given for studying neural networks in the interpolation setting. Especially Assumption 4 seems like a very strong assumption to me, and I was a bit confused because in line 57-58 it states that the setting has been studied in different papers, whereas when I read the papers they do have min-margin assumptions but not exactly the one discussed in Assumption 4, except for [1]. So I have two questions:\n\n- Is this theoretical assumption widely used in the exact form proposed in the paper? e.g. are there different papers that exactly show this form of assumption? If yes, it would be good to mention how they are used in different papers. If not but they are associated somehow, it would also be good to clarify it. It could be the case that I am missing something apparent.\n\n- Is this theoretical assumption valid? e.g. is it verifyable by experiments? Are there any experiments that support this assumption?\n\nClarifying the questions would make the paper stronger."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oJO2sYwrxc", "forum": "I29dOHf74X", "replyto": "I29dOHf74X", "signatures": ["ICLR.cc/2026/Conference/Submission10318/Reviewer_cAqU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10318/Reviewer_cAqU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902965371, "cdate": 1761902965371, "tmdate": 1762921658446, "mdate": 1762921658446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the regret analysis of AdamNC (without debiasing correction) and RMSPropNC (as a special case of AdamNC without first momentum) with a structured loss function which is constructed as $F_t(w)=f(y_t\\Phi(w,x_t))$ where $\\Phi$ denotes a fully connected MLP neural network for binary classification tasks with data $\\\\{(x_t,y_t)\\\\}$. The main assumptions are that $f$ is self-bounded or smooth and that the model $\\Phi$ has interpolation ability such that there exists a decreasing function $g(\\epsilon)$ such that any $\\epsilon$ corresponds to a nearly optimal parameter $w^{(\\epsilon)}$ such that $\\sum F_t(w^{(\\epsilon)}) / T \\le \\epsilon$ and $\\\\|w^{(\\epsilon)}-w_1\\\\|\\le g(\\epsilon)$. Under these assumptions, this paper provides a convergence guarantee of AdamNC and shows that it achieves $O(g^3(\\epsilon/T))$ for self-bounded or smooth $f$ when the model width is larger than certain threshold. In particular, if the model is NTK-separable and $f$ is the exponential loss or logistic loss, then $g(\\epsilon)$ has an explicit form $g(\\epsilon) \\sim \\log(1/\\epsilon)$, and the previous regret bound becomes $\\mathrm{polylog}(T)$. The analysis is also extended to AdamNC-Norm, where the preconditioner aggregates the norm of gradient instead of per coordinate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of this paper lies in its novelty significance. In particular, this paper provides a novel theoretical analysis of regret bound of training neural networks with the AdamNC optimizer, which is rarely studied in any prior work. This helps to better understand the empirical effective of the popular Adam optimizer from a different perspective. Moreover, the technical results on the theoretical analysis is very concrete. It provides systematic analysis under different assumption, e.g., the loss being self-bounded or smooth, and different model structures, e.g., with and without the dimension normalization per-layer."}, "weaknesses": {"value": "One limitation is that the convergence results in this paper requires a minimum model width to be true, and that threshold is usually asymptotically larger than the convergence rate (e.g., the width needs to be $O(g^4(\\epsilon/T))$ to achieve $g^3(\\epsilon/T))$ in Thm 1 and 2). This setup does not reflect the practical setting of training neural networks, where the total iteration usually has larger orders compared to the model width."}, "questions": {"value": "- In general (without NTK-separability), is there an explicit form for $g(\\epsilon)$? Could the author provides some example to help understand the shape of this function and how it's related to practical training in real life?\n- Assumption 4: does it implicitly assumes $\\inf f = 0$ so that $F(w)\\le \\epsilon$ is always achievable for any small $\\epsilon$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AMz7nz54cW", "forum": "I29dOHf74X", "replyto": "I29dOHf74X", "signatures": ["ICLR.cc/2026/Conference/Submission10318/Reviewer_U4Xb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10318/Reviewer_U4Xb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989959627, "cdate": 1761989959627, "tmdate": 1762921658034, "mdate": 1762921658034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the convergence of adaptive methods (RMSProp and AdamNC) in deep fully connected networks for online binary classification with smooth activation functions. The authors prove an $O(polylog(T))$ regret bound for sufficiently wide networks in the NTK regime. This is comparable to rate for strongly convex online optimization. They also analyze scalar variants (RMSProp-Norm and AdamNC-Norm) that achieve similar bounds without requiring prior knowledge of problem constants. The proof builds on the idea of Bregman Proximal Gradient and applies it within the NTK framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tAnalyzing the convergence of adaptive methods is an important research question.\n-\tThe paper is overall clearly written and includes a helpful proof sketch to illustrate the main idea.\n-\tThe paper establishes the convergence of adaptive methods for deep networks in the NTK regime, which appears to be new."}, "weaknesses": {"value": "-\tThe paper focuses on the NTK regime, where neural networks are known to behave similarly to linear or kernel methods. However, this setting does not always reflect the behavior of practical networks.\n-\tThe main technique appears similar to those in Duchi et al. (2011) and Alacaoglu et al. (2020) for handling adaptive methods, and is applied here to the specific NTK setting. It would be helpful to clarify whether any new challenges arise in this context or if additional techniques were required to address them."}, "questions": {"value": "- For Theorems 1 and 2, I wonder why the stepsize $\\eta$ must have the exact order specified in the statement, rather than simply being any sufficiently small value. What is the intuition behind this requirement?\n- Can Assumption 4, which assumes the existence of such a function $g(\\epsilon)$, hold in more interesting regimes beyond the NTK setting (Assumption 5)?\n- Do the results provide any insight into the potential advantages of using adaptive methods over vanilla gradient descent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U5Dkc1gtjQ", "forum": "I29dOHf74X", "replyto": "I29dOHf74X", "signatures": ["ICLR.cc/2026/Conference/Submission10318/Reviewer_jdbY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10318/Reviewer_jdbY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146407746, "cdate": 1762146407746, "tmdate": 1762921657655, "mdate": 1762921657655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}