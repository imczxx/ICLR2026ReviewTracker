{"id": "TJxRIsc1By", "number": 18566, "cdate": 1758289153911, "mdate": 1759897095457, "content": {"title": "KAN-Semi: A Semi-Supervised Approach Combining Self-Supervised Pre-training, Hierarchical Priors, and Kolmogorov-Arnold Networks for Landmark-based Biometry Estimation", "abstract": "Ultrasound (US)-based biometric estimation is crucial for monitoring labor progression and diagnosing fetal and maternal abnormalities. Reliable biometry estimation relies heavily on accurate landmark localization on standard planes, a process traditionally performed by sonographers. However, manual measurement is time-consuming, operator-dependent, and prone to variability. Although automated segmentation methods based on fully supervised models show promise, they often suffer from multi-stage error accumulation and a lack of expertly annotated data. To address these challenges, we introduce KAN-Semi, a semi-supervised network that combines self-supervised pre-training, hierarchical priors, and Kolmogorov-Arnold Networks (KANs). First, we utilize in-domain self-supervised pre-training with a Masked Autoencoder (MAE) to learn robust, domain-adapted representations for a novel CNN-ViT hybrid backbone. Next, we propose a Hierarchical Guidance Decoder, which encodes symbolic medical priors to regularize the model’s reasoning, progressively guiding it from stable to variable structures. Finally, we explore Kolmogorov-Arnold Network (KAN)-enhanced heads as an alternative to conventional predictors, demonstrating their efficacy in complex spatial regression tasks. We perform extensive experiments on three intrapartum ultrasound datasets collected from 24 medical centers and institutions, showing that our approach significantly outperforms fully supervised models in landmark detection performance. Our work offers a structured framework for designing effective learning systems that integrate self-supervision, knowledge-based architectural design, and emerging network paradigms.", "tldr": "We propose KAN-Semi, a semi-supervised framework that synergizes self-supervised pre-training, hierarchical architectural priors, and KAN-enhanced heads to achieve robust landmark localization in medical ultrasound.", "keywords": ["Semi-Supervised Learning", "Self-Supervised Learning", "Medical Image Analysis", "Landmark Detection", "Kolmogorov-Arnold Networks (KAN)"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/736a8bdacead5efd00ba93b591da2de9b97f2e7e.pdf", "supplementary_material": "/attachment/e9f6afb8a9d893fcd326889d4935157fa77ced8b.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical problem of Ultrasound (US)-based biometric estimation, which is essential for monitoring fetal and maternal health but suffers from issues of being time-consuming and operator-dependent when performed manually. The authors propose KAN-Semi, a novel semi-supervised framework for accurate landmark localization on standard US planes. The framework integrates three modern machine learning concepts: Kolmogorov-Arnold Networks (KANs), MAE-based self-supervised pre-training, and hierarchical priors, all within a two-stage semi-supervised pipeline (pre-training followed by fine-tuning). The authors also note that the source code and implementation details are provided to ensure reproducibility."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The submission presents a highly novel combination of cutting-edge deep learning techniques—specifically, integrating Kolmogorov-Arnold Networks (KANs) with an MAE-based self-supervised pre-training strategy and hierarchical priors—for the challenging, data-scarce medical imaging task of US biometry estimation. This specific fusion of methods has not been widely explored and represents a creative approach to model interpretability and data efficiency in this domain.\n2.The experimental design is rigorous: large-scale multi-center data, fair baselines (including modern architectures like Swin-Unet and ConvNeXt-Unet), ablation studies, and both pixel-level (MRE) and clinical (APD) metrics. The semi-supervised protocol (Mean Teacher + consistency loss) is well-implemented with dynamic scheduling.\n3.The paper is well-written, logically structured, and figures (e.g., Fig. 2, 3, 5) are informative. The methodology is described with sufficient detail for reproduction (e.g., hyperparameters in Table 1, architecture in Fig. 1–2).\n4.The work addresses a real clinical need—reducing operator variability in intrapartum ultrasound—with a data-efficient approach that leverages unlabeled data. The performance gains, especially in APD (4.99° vs. 5.77°), may be clinically meaningful. The exploration of KANs in medical imaging is timely and could inspire follow-up work."}, "weaknesses": {"value": "1.While the combination is new, the foundational components (MAE, KANs, semi-supervised learning) are established concepts. The paper must clearly articulate how the \"hierarchical priors\" are implemented and how the KAN architecture specifically benefits this task over standard CNN/MLP architectures, especially considering the potential computational overhead of KANs.\n2.The precise nature of the \"hierarchical priors\" and the specific mechanism for integrating KANs into the US image processing pipeline are currently unclear, necessitating a detailed methodological section.\n3.It hinges on the magnitude of performance improvement shown in the results. If the gains over simpler baselines are marginal, the increased complexity from the KAN and combined framework would diminish its overall impact.\n4.Are there cases where hierarchical guidance hurts performance (e.g., if base landmarks are mislocalized)? A failure analysis would strengthen the paper."}, "questions": {"value": "1.Please provide a detailed ablation study comparing the proposed KAN-Semi framework against a method that is identical in every respect (MAE pre-training, semi-supervised fine-tuning, use of hierarchical priors, etc.) but replaces the Kolmogorov-Arnold Network layers with standard Multi-Layer Perceptrons (MLPs) or Convolutional layers. This is essential to quantify the specific benefit of KANs in terms of accuracy, interpretability, and the associated trade-offs in computational cost (training time, memory, inference speed).\n2.The concept of \"hierarchical priors\" is central to the paper. Please explicitly define what these priors encode (e.g., anatomical relationships between landmarks, known growth patterns) and provide a mathematical formulation detailing how they are integrated into the network's loss function or architecture.\n3.The core of the paper is a semi-supervised approach. The evaluation must clearly demonstrate the method's efficiency under data scarcity. Please provide a clear sensitivity analysis showing the performance of KAN-Semi as a function of the percentage or absolute number of labeled training samples, comparing it to all baselines.\n4.The paper addresses \"landmark-based biometry estimation.\" Please specify which standard planes and biometry measurements (e.g., Biparietal Diameter - BPD, Head Circumference - HC, Femur Length - FL) are included in the evaluation, and report results for each independently to show comprehensive performance across the full clinical task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LLZMPzXGO0", "forum": "TJxRIsc1By", "replyto": "TJxRIsc1By", "signatures": ["ICLR.cc/2026/Conference/Submission18566/Reviewer_sPzy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18566/Reviewer_sPzy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904030685, "cdate": 1761904030685, "tmdate": 1762928277049, "mdate": 1762928277049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a framework for semi-supervised learning for image data, motivated by medical imaging from ultrasound exams.\nThe authors first train a ViT-based masked autoencoder (as a pre-training phase), and then fine-tuning stage for the desired task. Most notably, this fine-tuning stage uses the pre-trained model in a \"hierarchical\" manner, and the prediction head uses a learnable activation function via splines (a la KAN)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "As demonstrated in their experiments, the proposed framework yields better quality results than existing baselines.\nSuch improved performance can be of immediate use for many practitioners."}, "weaknesses": {"value": "Despite the apparent increase in performance, I have my doubts with regards to the broader conclusions that can be drawn from this work.\nFrom my understanding, the work introduces two key mechanisms:\n\n- the \"hierarchical prior\" architecture, described in equations 2-4; and\n- the use of a KAN prediction head (particularly instead of a 1x1 convolution).\n\nThese are shown to lead to improved performance in experiments, but I am unconvinced that the baselines in question are appropriate.\nIn no particular order:\n\n- Judging by tables 2 and 3, even without most of the proposed components the proposed method beats prior work (best MRE of prior work in table 2 is >=16, while the worst MRE in table 3 is <=15, with most being <=14). So it's not clear what were the actual drivers behind the improvements.\n- The use of the KAN head is only compared to a simple 1x1 convolution. While 1x1 convolutions are a popular choice, they are by no means unique, and it feels like quite a leap to embrace KAN as the particular solution here, especially considering that it obviously boasts higher representation capacity than 1x1 convolutions. The use of KANs in the prediction head should be contrasted to other ways to improve representation capacity, such as having small MLPs at the end of the network, and increasing the sizes of the neural network beforehand (e.g. more depth, more width, etc.).\n- It's not clear at all to me what the \"w/o hierarchical guidance\" ablation is comparing. What is the alternate architecture being considered here? This is important to know so that the fairness of the comparison can be ascertained.\n\nIt's also worth noting that the paper makes occasional use of some very strong language, which I do not belive is justified. For example, lines 431-433 \"this results provides *unequivocal evidence* that our in-domain, self-supervised pre-training strategy is the most critical factor for success, [...]\" (emphasis mine).\n\nThe presentation of the paper could also be slightly improved, in particular with regards to the organization of Section 3 (which describes the method). It is currently a bit hard to get a quick understanding of what the proposed method actually is, beyond understanding the presence of certain described components (e.g. the KAN prediction head).\nBut perhaps most troubling is that I was unable to understand the mechanism proposed at the end of Section 3.4, where \"the teacher's parameters are an exponential moving average (EMA) of the student's parameters\" (lines 279-280).\n\nAltogether, these concerns lead me towards a \"borderline reject\" score. \\\n**Why not higher:** It is very much not clear what broader conclusions can be drawn from this work, rather than just improved performance on one particular ultrasound imaging dataset. The presentation should also be improved before publication. \\\n**Why not lower:** More precise semi-supervised learning is of definite interest to the community, and the paper does seem to make a meaningful contribution in that direction.\n\nOther minor comments:\n- Line 151/152: re, $\\mathcal{L}\\_{MAE}$, MAE can be confused here for \"mean absolute error\", whereas the losss is actually a MSE. Perhaps it's best to rename it to something like $\\mathcal{L}\\_{MA}$?\n- Line 105: the authors mention that the models were trained on NVIDIA RTX 4090 GPUs (plural). How many GPUs were used, and how was the distribution among GPUs arranged?\n- Line 42: should the citation \"(Organization et al., 2020)\" not be something like \"(WHO., 2020)\"?\n- Equation 5: $\\mathcal{L}\\_\\mathrm{s}$ should be $\\mathcal{L}\\_{sup}$, and $\\mathcal{L}\\_\\mathrm{u}$ should be $\\mathcal{L}\\_{unsup}$.\n- Lines 294/295: `**24 medical centers**` should probably be `\\emph{24 medical centers}`"}, "questions": {"value": "- Could the authors please clarify the procedure described in Section 3.4?\n- Could the authors refine and present the ablations following my comments in the 'Weaknesses' sections?\n- What exactly makes the design outlined in equations 2-4 \"hierarchical\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ojzar6K0QH", "forum": "TJxRIsc1By", "replyto": "TJxRIsc1By", "signatures": ["ICLR.cc/2026/Conference/Submission18566/Reviewer_tS2V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18566/Reviewer_tS2V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922869194, "cdate": 1761922869194, "tmdate": 1762928276476, "mdate": 1762928276476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets landmark localization and quantitative biometry in intrapartum ultrasound, and proposes a two-stage framework **KAN-Semi**. It also introduces two architectural designs: a **Hierarchical Guidance Decoder** and a **KAN-enhanced prediction head**."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method combines data efficiency with structured medical priors in a sound way.  \n2. The expressiveness of the prediction head is improved with quantitative and visual evidence. Replacing the conventional 1×1 convolution with the **KAN-Head** yields clear APD gains; the spline activation curves visualize its non-linear advantages.  \n3. The experiments are relatively complete, and the paper provides implementation details, key hyperparameters, and code."}, "weaknesses": {"value": "1. The combination **MAE pretraining + Mean-Teacher semi-supervision + CNN–ViT hybrid + hierarchical priors + KAN-Head** is reasonable, but most individual modules are natural combinations or minor modifications of existing directions, with limited theoretical analysis and unclear boundary conditions.  \n2. The task scope focuses on intrapartum ultrasound standard planes related to AoP. The method has not been verified on other modalities or anatomical sites (for example, general abdominal scans, cardiac scans, or other fetal anatomies) or on cross-task transfer; broader evidence is needed for generalization.  \n3. Fairness and distribution shift analyses are limited. Although the data come from 24 centers, there are no results stratified by center or population, and no confidence-based triggering strategy is employed. Systematic robustness tests under noise, occlusion, and out-of-distribution settings are also missing."}, "questions": {"value": "1. Please provide stratified results and statistical tests by **center / device type / population factors** (gestational age, age, BMI, parity, and so on), analyze whether there are systematic biases, and describe how you handle site imbalance.  \n2. The data come from 24 hospitals. Were ethics approvals obtained? Which hospitals obtained approvals, and what are the approval identifiers?  \n3. Does the **KAN-Head** provide stable gains under both weaker and stronger backbones? Under distribution shifts such as noise perturbation, low contrast, and strong acoustic shadows, does the advantage of the KAN-Head remain?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The study claims to use data collected from 24 hospitals for algorithm development and validation. However, the ethical approval process and details regarding patient informed consent are unclear. Please provide additional clarification on these aspects."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Six1xxWuCF", "forum": "TJxRIsc1By", "replyto": "TJxRIsc1By", "signatures": ["ICLR.cc/2026/Conference/Submission18566/Reviewer_xpN3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18566/Reviewer_xpN3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928332048, "cdate": 1761928332048, "tmdate": 1762928275232, "mdate": 1762928275232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}