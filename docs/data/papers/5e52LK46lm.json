{"id": "5e52LK46lm", "number": 25072, "cdate": 1758363704390, "mdate": 1763250386093, "content": {"title": "Subject-Invariant Normalization: A Simple Principle for Robust Sequence Modeling", "abstract": "Accurately estimating fixation depth from gaze signals is essential for applications in extended reality, robotics, and human-computer interaction. However, existing methods rely heavily on subject-specific calibration and dataset-specific preprocessing, limiting their generalization. We introduce FOVAL, a calibration-free framework for fixation depth estimation that combines spatiotemporal sequence models with a novel subject-invariant normalization strategy. Unlike prior work, FOVAL prevents train-test leakage by enforcing train-only normalization and leverages cross-dataset evaluation across three heterogeneous benchmarks (Robust Vision, Tufts Gaze Depth, Gaze-in-the-Wild). We further provide rigorous statistical testing (bootstrap confidence intervals, Wilcoxon tests, effect sizes) and noise robustness analysis to quantify stability under realistic perturbations. Empirically, FOVAL consistently outperforms alternative architectures (Transformers, TCNs, 1D-CNNs, GRUs) and prior baselines, reducing mean absolute error by up to 20% in cross-dataset scenarios. Our results demonstrate that subject-invariant normalization is a simple yet powerful principle for robust gaze-based depth estimation, with implications for broader subject-independent sequence modeling tasks.", "tldr": "We introduce FOVAL, a calibration-free framework that uses subject-invariant normalization to robustly estimate fixation depth across users, devices, and datasets.", "keywords": ["subject-invariant learning", "calibration-free models", "fixation depth estimation", "eye tracking", "invariant normalization", "cross-dataset generalization", "spatiotemporal sequence modeling", "robustness", "LSTM", "TCN", "Transformer", "deep learning", "extended reality (XR)", "human-computer interaction"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4eb4d38681b5238d02062681832da59d7305592a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the task of fixation depth estimation. To address limitations in scalability and usability caused by user-specific calibration, it introduces a robust, calibration-free approach that models spatiotemporal sequences using a Long Short-Term Memory (LSTM) network with subject-invariant feature engineering. The authors conducted experiments on benchmark datasets and cross-dataset evaluations to validate the effectiveness of the proposed method, which demonstrates strong generalization without calibration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors conducted extensive experiments on public datasets in both within-dataset and cross-dataset settings to evaluate the proposed method.\n\n2. The source code and pre-trained weights are provided; this is a great step toward evaluating the proposed method."}, "weaknesses": {"value": "1. The proposed method seems somewhat trivial. It is built on a Long Short-Term Memory (LSTM) network and lacks modules specifically designed for the task of fixation depth estimation.\n\n2. The paper reports only quantitative experimental results and lacks a qualitative analysis of the model’s predictions.\n\n3. The subject-invariant feature engineering and normalization are hard to understand. How can this approach prevent the need for user-specific calibration?"}, "questions": {"value": "Basically, I have no questions about this paper. My major concerns about this paper have listed in the Weaknesses. Could the authors explain how the proposed subject-invariant feature engineering and normalization prevent the need for user-specific calibration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TWwEbTCAAK", "forum": "5e52LK46lm", "replyto": "5e52LK46lm", "signatures": ["ICLR.cc/2026/Conference/Submission25072/Reviewer_zhUf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25072/Reviewer_zhUf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761108549540, "cdate": 1761108549540, "tmdate": 1762943315169, "mdate": 1762943315169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We thank the reviewers for their constructive feedback. Due to unforeseen circumstances and the need for additional experiments, we won't be able to provide a full rebuttal in the given time. We appreciate the reviewers’ efforts and plan to extend this work and resubmit it in the future substantially."}}, "id": "ikTO0el43T", "forum": "5e52LK46lm", "replyto": "5e52LK46lm", "signatures": ["ICLR.cc/2026/Conference/Submission25072/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25072/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763250385297, "cdate": 1763250385297, "tmdate": 1763250385297, "mdate": 1763250385297, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FOVAL, a calibration-free, subject-invariant approach for estimating fixation depth from eye tracking data using an LSTM-based spatiotemporal sequence model. The method focuses on feature engineering and normalization to achieve strong generalization across subjects and datasets (RV, GazeCapture), reporting a mean absolute error (MAE) of 9.1 cm on the Robust Vision (RV) dataset. The authors contrast their LSTM approach with other sequence models (TCN, Transformer)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Eye tracking cross-platform, cross-condition, and cross-subect without calibration is a challenging problem and the authors bring ML to bear on addressing this problem.\nCalibration-free and subject-invariant fixation depth estimation is a highly relevant and commercially important topic, particularly for Extended Reality (XR) applications.\nThe attempt to validate the model using both Leave-One-Out Cross-Validation (LOOCV) and cross-dataset testing is appreciated and is a necessary step to support the claim of subject-invariance."}, "weaknesses": {"value": "The paper suffers from significant deficiencies in its empirical rigor, depth of analysis, and presentation.\nThe entire empirical argument for the proposed FOVAL architecture rests on a single table of performance metrics that appears in the appendix (Table 4), comparing mean MAE values against baselines. This is insufficient evidence and analysis (I am also unclear if the authors are claiming SOTA performance?)\nThe figures are simplistic. More complex, information-rich visualizations would help the reader interpret the model's behavior, not just summary plots. Key figures showing the model's errors (e.g., error heatmaps, error distributions, or detailed qualitative results) are necessary but are notably absent or relegated to a basic format.\nThe core claim of Subject-Invariant performance is not rigorously demonstrated. The reported 9.1 cm MAE lacks context and a deep statistical breakdown. It should go beyond reporting the mean and instead provide a full analysis of the distribution and source of errors. This includes a detailed breakdown of the MAE to confirm the claim of subject-invariance, rather than just reporting the average and the \"Best Fold\" MAE.\nThe paper's attempt at statistical comparison (Wilcoxon signed-rank tests in Appendix D) is the absolute bare minimum and does not constitute interesting or insightful analysis.\nThe technical approach of using an LSTM network with feature engineering is incremental. The authors fail to articulate a compelling novel learning principle or architectural breakthrough beyond normalizing per-subject. The contribution seems to be a combination of existing sequence modeling and specific feature engineering/normalization, which makes the work feel more like a competent technical report on an application rather than a strong algorithmic contribution to deep learning"}, "questions": {"value": "Figure 1 This is a very different design than the standard LSTM diagram. Please explain how your model differs from a stock LSTM. Including a diagram of how various types of the gaze data included are fed into the model through time would bring more clarity overall\nTable 4 is the main result interesting to the ICLR community. This should be included in the main text\n“We explored multiple architectures and found that LSTMs consistently outperform GRUs, CNNs, and Transformer variants in our setting, especially under conditions of limited training data and high noise” Refer to the table with these results\nThe list of features chosen should be included as a table in the main paper. Include ablation studies to evaluate which of these are necessary and sufficient to achieve SOTA results.\nTo support the \"Subject-Invariant\" claim, include a figure (perhaps box plot or histogram) of the MAE across all subjects/LOOCV folds for FOVAL and the best baseline. Analyzing the inter-subject variance in detail would strengthen the paper.\nLikewise, more statistical analysis that would support the cross-dataset claim is needed, beyond the simple MAE reported. For example, provide a deep analysis on the context of the 9.1 cm MAE. Is this error constant, or does it correlate with other factors (e.g., higher error for distant fixations, specific gaze angles, or movement patterns)?\nProvide a figure that clearly visualizes the necessity of the proposed subject-invariant normalization. How does the distribution of the normalized features compare to the unnormalized features across different subjects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JdADJtTtSd", "forum": "5e52LK46lm", "replyto": "5e52LK46lm", "signatures": ["ICLR.cc/2026/Conference/Submission25072/Reviewer_dGAg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25072/Reviewer_dGAg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752137197, "cdate": 1761752137197, "tmdate": 1762943313939, "mdate": 1762943313939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper proposed a novel LSTM method to estimate fixation depth from calibration-free gaze data. It employs a novel subject-invariant normalization strategy. Experimental results outperform SOTA method, mix-tcn and other baseline methods: GRU, 1D-CNN, TCN, and multi-head transformers for the Robust Vision dataset."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality. Paper has proposed a novel method for the depth estimation of fixation with the LSTM approach. The method is shown to be superior to the SOTA method and other baseline methods.\n\n2. Quality. Paper is of high quality. It put up a clear hypothesis which is supported by the experimental results. The proposed method is clearly explained, and the extensive experiments were done to validate and investigate the proposed method.\n\n3. Clarity. The paper's hypothesis, related works, methodology were clear. The experimental setup and design is also well described."}, "weaknesses": {"value": "The presentation of experimental results are unconventional and confusing/incomplete. There was no table which directly compares the various baselines for each dataset. There were several references to Table 5.1, which is actually Table 1.\n\nWhile it is shown in Table 4, the comparison between the different baselines. There were no other comparisons for the other datasets, Tufts, Gaze-In-the-Wild. It is unclear if the experiments were not done, or that the results were omitted.\n\nThe fixation depth estimation problem is rather niche and its significance is limited. This can be inferred from the lack of published works in this area."}, "questions": {"value": "What's the comparison of the experimental results for proposed method and the baseline methods for the Tufts, Gaze-In-the-Wild, and cross-domain datasets?\n\nThe baseline methods used training data heavy transformer when there are more data efficient transformer methods, e.g. DeIT. Will the author also include comparison with such backbone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZykthnWwUg", "forum": "5e52LK46lm", "replyto": "5e52LK46lm", "signatures": ["ICLR.cc/2026/Conference/Submission25072/Reviewer_v9vg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25072/Reviewer_v9vg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836845744, "cdate": 1761836845744, "tmdate": 1762943313018, "mdate": 1762943313018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a LSTM based method for estimating fixation depth during eye tracking. They present a featurization strategy to make this system robust to new participants. They compare MAE in a LOOCV task averaged across several datasets on a few baseline architectures, and one existing method in the literature. They analyze inter-subject variation and distribution of residuals in their model."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Nice evaluation of results and investigation of error cases, inter subject variability, and residuals\nGood baseline dataset selection in table 1\n- Nice discussion of related work\n- Nice statistical handling of ablation results, though I think the issue with these ablations are not the statistics but rather whether these models were properly tuned before reporting metrics\n- The domain seems relevant to the research community"}, "weaknesses": {"value": "- This paper is a direct resubmission of a poorly performing NeurIPS paper. There are no changes in any of the writing. I reviewed it once and recieved no comments or modifications, my review and scores are the same as the first submission. \n\n- Main dataset results (Table 1) do not compare to any baseline methods in the field\n- Not much architecture innovation – it looks like an MLP on top of a max-pooled LSTM, if the preprocessing is the important part then it might be best to feature that more strongly and clearly in the work\n- Architecture ablations in the supplement seem a bit suspicious – did they apply the same level of rigor in optimizing them via optuna that they did for their own method. In particular the transformer looks like its got a key bug stopping the fitting\n- It seems the paper is padding a bit to get to the length, the main text of the work is a bit light on nontrivial contributions. Consider moving things like the impact statement and code release into the supplement and expanding on your featurization strategy which makes a better case for your novelty. You can also tighten up future contributions as it’s a bit long for a main section\n- It would be better to use the whitespace for more important results rather than an analysis of errors which can go in supplement. Use this space to expand on your comparison of baselines\n- Include a table with performance metrics across all of your datasets and baselines. Don’t just report numbers averaged over all datasets, include error bars on these numbers"}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "- This paper is a direct resubmission of a poorly performing NeurIPS paper. There are no changes in any of the writing. I reviewed it once and recieved no comments or modifications, my review and scores are the same. \n\nhttps://openreview.net/forum?id=AtXs8X1kDA&referrer=%5BReviewers%20Console%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F2025%2FConference%2FReviewers%23assigned-submissions)"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "abFf0ILhCX", "forum": "5e52LK46lm", "replyto": "5e52LK46lm", "signatures": ["ICLR.cc/2026/Conference/Submission25072/Reviewer_WB1s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25072/Reviewer_WB1s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867767707, "cdate": 1761867767707, "tmdate": 1762943312489, "mdate": 1762943312489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}