{"id": "6ekx5J6mnK", "number": 3191, "cdate": 1757357385303, "mdate": 1759898102962, "content": {"title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder", "abstract": "We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a **Deep Compression Video Autoencoder** with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) **AE-Adapt-V**, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. We will release both the code and pre-trained models upon publication.", "tldr": "", "keywords": ["efficient video diffusion model", "deep compression video autoencoder", "post-training model acceleration"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2ddd570729fb7b2b4c622f5cda25d07b475f97a.pdf", "supplementary_material": "/attachment/63016c8e6498cc88d494277b4def7899967d0037.zip"}, "replies": [{"content": {"summary": {"value": "The paper's central thesis is that existing large-scale video diffusion models can be migrated to an extremely compressed latent space via a post-training approach, achieving over an order of magnitude in inference acceleration with negligible loss in visual quality. The authors propose a two-step framework to this end. First, they design a new, highly compressive video autoencoder (DC-AE-V) to create this compact latent representation. Second, they introduce a specialized fine-tuning procedure (AE-Adapt-V) that is claimed to \"seamlessly\" adapt a pre-trained large model to this new, smaller latent space without causing model collapse. The paper asserts that the proposed method achieves state-of-the-art results in both reconstruction quality and generation performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors' commitment to open-sourcing their work is commendable.\n2. The provision of an HTML file in the supplementary materials for direct and intuitive visual comparison is a welcome feature.\n3. The paper's reconstruction and generation results are highly impressive.\n4. The proposed method is straightforward and easy to follow."}, "weaknesses": {"value": "1. Unlike magvit-v2, the authors do not treat the first frame of a video differently. I'm curious whether the paper's method can handle the image frame effectively and how its image reconstruction performance compares to Hunyuan VAE. Additionally, I'd like to know how the authors trained the images under the chunk causal format.\n2. The chunk-causal VAE proposed by this method significantly outperforms the non-causal VideoDC-AE, which appears counterintuitive. Since VideoDC-AE employs temporal slicing during testing to ensure the number of tested frames is less than or equal to the number of training frames. Additionally, it overlaps different slices, for example, by 8 frames, to ensure subsequent slices have sufficient access to historical frames. Therefore, conceptually, the proposed approach and VideoDC-AE's method are fundamentally similar. The primary difference lies in the mechanism for passing history, latent feature passing versus overlapping input frames. The paper's chunk-causal method occupies a middle ground between IV-VAE and OpenSora2, offering limited innovation.\n3. The paper's AE-Adapt-V appears to be over-designed. What is the necessity of introducing the MSE loss? Why not simply freeze the DiT backbone and fine-tune the embedding and output heads simultaneously in a single stage? This more straightforward and sensible baseline is conspicuously absent from the paper's comparative experiments.\n4. It is well-known that VBench scores can be gamed by fine-tuning on carefully curated datasets. Therefore, I place more emphasis on perceptual quality than the reported metrics. The qualitative comparisons in the appendix are unconvincing. Aside from exhibiting greater motion, the generations from DC-VideoGen-Wan-2.1-14B appear inferior to the original Wan 2.1 in terms of texture and aesthetics. This is particularly concerning given that the provided cases are few and likely cherry-picked."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xPVmgC4KLp", "forum": "6ekx5J6mnK", "replyto": "6ekx5J6mnK", "signatures": ["ICLR.cc/2026/Conference/Submission3191/Reviewer_3xJk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3191/Reviewer_3xJk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922278253, "cdate": 1761922278253, "tmdate": 1762916590074, "mdate": 1762916590074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a post-training acceleration framework named DC-VideoGen, which aims to improve the inference efficiency and reduce the training cost of existing video diffusion models. The framework includes two main innovations: DC-AE-V and AE-Adapt-V. DC-AE-V is a video VAE that achieves 32x/64x spatial compression through a \"chunk-causal temporal design,\" while maintaining high-quality reconstruction and generalization capability for longer videos. AE-Adapt-V is an efficient adaptation method that migrates a pre-trained diffusion model into the highly compressed latent space of DC-AE-V through \"Video Embedding Space Alignment\" and LoRA fine-tuning. Experimental results show that the framework reduce inference latency while maintaining or even slightly improving the base model's quality (VBench score), and it enables the generation of 2160p (4K) video on a single H100 GPU."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Improving the efficiency of the video generation models sounds and still needs exploration.\n\nThe writing is great and easy to follow."}, "weaknesses": {"value": "Novelty and practical consideration of the temporal design (DC-AE-V): The paper's core 'chunk-causal' temporal modeling appears fundamentally similar to the Causal Group Convolution used in IV-VAE (CVPR'25). The authors claim the main advantage is a flexible chunk size, which they set to 40 for optimal results. This raises several concerns: (a) It is unclear how the model handles videos shorter than 40 frames. (b) For high-resolution generation, this large, fixed chunk size might lead to excessive VRAM consumption as it may not be divisible into smaller sub-chunks. (c) The claim of a dynamic chunk size being a core innovation is debatable, as a similar principle could seemingly be applied to IV-VAE, even if they only experimented with a size of 4.\n \nLimited novelty at the framework level: the overall paradigm of this paper—training a VAE with a higher compression ratio and then adapting an existing model via post-training — is not entirely novel. For example, related work (like OpenSora 2.0) has already explored similar ideas. The innovation of this paper lies more at the component level (i.e., how to solve the specific technical challenges during adaptation)."}, "questions": {"value": "See the weakness for the questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QLYP7UYOVu", "forum": "6ekx5J6mnK", "replyto": "6ekx5J6mnK", "signatures": ["ICLR.cc/2026/Conference/Submission3191/Reviewer_iJN1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3191/Reviewer_iJN1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762068535105, "cdate": 1762068535105, "tmdate": 1762916589822, "mdate": 1762916589822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DC-VideoGen, a post-training acceleration framework designed to make existing video diffusion models more efficient. It relies on two key innovations: 1) a Deep Compression Video Autoencoder (DC-AE-V) featuring a novel \"chunk-causal\" design, which achieves high spatial compression (up to 64x) while preserving reconstruction quality; and 2) AE-Adapt-V, an efficient strategy to rapidly and stably adapt pre-trained models to the new compressed latent space. Experiments show this framework delivers up to a 14.8x inference speedup and enables 4K video generation on a single GPU, all while maintaining the original model's quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a critical and urgent issue in the video generation field: the high cost of training and inference. This research direction is highly valuable for democratizing and advancing high-resolution video synthesis.\n2. The paper proposes a reasonable two-pronged solution. First, the novel chunk-causal video autoencoder (DC-AE-V) offers a clever theoretical approach to balancing high compression with long-video generalization. Second, the AE-Adapt-V strategy specifically addresses the instability issues encountered when adapting pre-trained models to new latent spaces, and its two-stage alignment is a technically sound design.\n3. The paper reports astonishing efficiency gains (e.g., up to 14.8x speedup) while maintaining comparable or even slightly better performance on the VBench benchmark. If fully verifiable, this work would have a significant impact on the field of video generation."}, "weaknesses": {"value": "1. In Section 3.3.1 and Figure 6, the authors construct a baseline method by retraining the DiT while randomizing the embedder and head. This approach is clearly suboptimal. A more conventional and intuitive baseline would be to first freeze the DiT and fine-tune only the embedder and head using the diffusion loss. I believe this approach would not suffer from training collapse and could achieve results comparable to AE-ADAPT-V. Therefore, the necessity and novelty of AE-ADAPT-V are questionable.\n2. For video VAE reconstruction performance, the trend causal < chunk-causal < non-causal is clear. My doubt is whether the transition from causal to non-causal can yield a PSNR improvement of over 4 dB (as shown in Figure 5). In Table 1, increasing the number of channels by 4x results in only a 4.5 dB PSNR increase. In other words, the performance gain from causal to non-causal is claimed to be equivalent to a 4x increase in channel count, which seems implausible. In my past experience, the improvement from causal to non-causal is only around 2 PSNR points. In other papers that adopt a non-causal design, such as OpenSora 2 and VideoVAEPlus, the PSNR improvement over causal models like Hunyuan or CogVideoX is also less than 2 dB. Therefore, I have significant doubts about the upper limit of the performance gain from the chunk-causal design.\n3. Could the authors provide a comparison with other methods on shorter frame counts (e.g., 40 frames)? The authors attribute the poor performance of non-causal models to their difficulty in generalizing to long videos, and their tests are conducted on 80 frames. When the test frame count is 40, the proposed method degenerates into a non-causal VAE. In this scenario, does the method still hold an advantage over Video DC-AE and VideoVAEPlus? If so, why? My understanding is that non-causal VAEs lack good temporal generalization, and their performance may drop significantly if the test and training frame counts do not match (e.g., Video DC-AE was trained on 32 frames, VideoVAEPlus on 16 frames). However, the authors' test frame count matches their own training frame count, which could introduce unfairness.\n4. The VBench metrics reported in the paper are outstanding and astonishing. In Table 2, using DC-AE-V-f64t4c128 as the VAE, the model achieves a comparable VBench score with only 1/22 of the latency of Wan-2.1-1.3B. Since the paper does not provide model weights, I cannot verify the authenticity of this result. Therefore, I would like to know if the authors plan to open-source the weights of both the VAE and the generation models for the community to reproduce. If so, could they provide a reliable, latest possible release date?"}, "questions": {"value": "Please refer to weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bwUWUyJx4Q", "forum": "6ekx5J6mnK", "replyto": "6ekx5J6mnK", "signatures": ["ICLR.cc/2026/Conference/Submission3191/Reviewer_Ygub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3191/Reviewer_Ygub"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762069957995, "cdate": 1762069957995, "tmdate": 1762916589594, "mdate": 1762916589594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper's central thesis is that massive, pre-trained video diffusion models can be made dramatically faster with negligible quality loss by forcing them to operate in a much smaller latent space. To achieve this, the authors first build a new, highly aggressive video autoencoder (DC-AE-V) and then propose a specific fine-tuning recipe (AE-Adapt-V) to port the original diffusion model over. The core of this recipe is an explicit alignment stage meant to prevent the model from collapsing during this transfer. The paper then presents a suite of VBench scores across various settings to argue that this aggressive compression strategy works as claimed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed chunk-causal mechanism is a specific and novel architectural contribution for video autoencoders.\nThe explicit embedding space alignment stage in AE-Adapt-V is a technically sound solution for the latent space mismatch problem.\nThe work provides a strong empirical demonstration of achieving extreme (e.g., 64x) spatial compression in a video generation pipeline."}, "weaknesses": {"value": "1.\tChunk-causal is one of the paper's contributions, but its implementation is not well-described in the main paper. I initially assumed that the proposed chunk-causal design was implemented via attention masks. It was not until I reviewed the appendix, which shows the VAE employs ResNet blocks, that I came to suspect the process is similar to IV-VAE. Implementing chunk-causal with convolutions is not a conventional approach, and the authors are advised to describe this in the main text.\n\n2.\tAs seen in Figure 9, the authors delay the temporal downsampling in the VAE and use a very large number of channels. This would lead to very high computational and parameter costs. The authors should provide a comparison with other VAEs in this regard.\n\n3.\tI noticed that the reconstruction results for Video DC-AE (with tiling) in the appendix show severe degradation before the 32nd frame. This is quite peculiar, as Video DC-AE itself was trained on 32 frames, so there is no reason for it to exhibit such poor reconstruction quality between frames 24 and 32. I suggest that the authors check the correctness of their relevant testing code to ensure the fairness of the results.\n\n4.\tThe novelty of this paper is limited. For instance, AE-ADAPT-V combined with LoRA fine-tuning seems like a conventional approach. Chunk-causal appears to be an improvement based on IV-VAE, and while its reported gains are significant, I am not convinced it is truly that effective. In fact, after reading the paper, I attempted to reproduce the work, but my preliminary results suggest it is far less effective than reported, particularly in terms of SSIM and LPIPS metrics."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VYXCumMtmZ", "forum": "6ekx5J6mnK", "replyto": "6ekx5J6mnK", "signatures": ["ICLR.cc/2026/Conference/Submission3191/Reviewer_5X8c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3191/Reviewer_5X8c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070764723, "cdate": 1762070764723, "tmdate": 1762916589387, "mdate": 1762916589387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}