{"id": "gwhQO3KaHI", "number": 2869, "cdate": 1757294354397, "mdate": 1759898121865, "content": {"title": "Bridging Graph Worlds: Neural Approximation of Gromov-Wasserstein Distances", "abstract": "Graph-structured data is crucial in various domains like biology and social networks. Comparing graphs, which is a fundamental problem in graph data analysis, is nonetheless highly challenging. Recently, the Gromov-Wasserstein (GW) distance provides a principled way to compare two graphs. However, computing the GW distance involves solving a complex non-convex optimization problem, making it computationally expensive, especially when the graphs are large. \nIn this work, we propose a neural approximation of the GW distance, called NeuralGW. In NeuralGW, we use a combination of a graph isomorphism network and a graph transformer to represent the nodes of two graphs as two sets of vectors, treated as two discrete distributions, on which we compute a few maximum mean discrepancy values given by different kernels. We then use a multilayer perceptron to convert the vector formed by these values into a single value, which is the prediction of the GW distance. Once trained, the model allows for efficient inference, enabling fast structural comparisons between graphs across diverse domains. We also provide an extension of NeuralGW to predict the transport plan. Experiments demonstrate the effectiveness and practical applicability of our approach on real-world datasets.", "tldr": "", "keywords": ["Gromov-Wasserstein Distances", "graph neural network", "graph kernels"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/476ab83aa8f825024bff69075a618e2a25e78958.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Assuming that a collection of N graphs is given, as well as the pairwise \"true\" Gromov-Wasserstein (GW) distance between them, the main contribution of this paper is the introduction of a neural architecture combining GINs and Transformer layers in order to i) learn the GW distance between the graph training pairs (with the aim to generalize to new unseen test pairs) ii) learn the optimal transport plans between graph pairs. Although the idea is not entirely original, as the author acknowledge (NeuralGWD-Naive), their architecture outperform previous alternatives via the introduction of an original Maximum Mean Discrepancy (MMD) layer. A theoretical result, bounding the risk linked to the proposed estimator is claimed and proved and several experiments highlight the improved performance of the proposed approach together with its reduced computation cost, notably with respect to a direct calculation of the GW distance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "Without no doubt, the line of research proposed by the authors is of interest, since a direct computation of the optimal transports distances, in general, is quite prohibitive."}, "weaknesses": {"value": "The main weakness of the paper is that it is really written too quickly and/or with some errors making it impossible to assess whether the main claims are entirely true or not."}, "questions": {"value": "Main points:\n\ni) At line 268 and following authors claim that NeuralGW induces a pseudo-metric. There is a proof for it? For instance, point 1. requires that your architecture is permutation invariant/equivariant. Is it the case? \nMoreover point 4 (l. 299) is meaningless: you state $d_{NGW}(G_1, G_2) = d_p(G_1, G_2)$. However you stated a few lines before that $(\\mathcal{G}_p, d_p)$ is a metric space, so $d_p$ measures the distance between the nodes of $\\mathcal{G}_p$ and *not* the distance between two graphs;\n\nii) I take Th. 1 as given (I'll be back to the proof in a while). Let me call $\\mu$ the upper bound on the right hand side of Eq. (10). At line 325 you state  \"We have $U_N(\\ell_{\\hat{f}}) - R(\\ell_{f^*})  \\leq 2\\mu$\".  Now, my guess is: \n\n$ U_N(\\ell_{\\hat{f}}) - R(\\ell_{f^*}) = (U_N(\\ell_{\\hat{f}}) - U_N(\\ell_{f^*)) + (U_N(\\ell_{f^*) -  R(\\ell_{f^*})) $ \n\nthe first term on the r.h.s. of the equality is negative by definition of $ f^* $ and hence the quantity on the l.h.s. is majored by  $ U_N(\\ell_{f^{\\*}}) -  R(\\ell_{f^{\\*}}) \\leq \\mu $. \n\nSo where does the 2 in front of $\\mu$ come from?\n\niii) At line 337 you state: \"the time complexity in the optimization is $\\mathcal{O}(Bn^2dL + \\dots )$\". Where does this term come from?? Even assuming that your architecture simply is a MLP with width $\\overline{d} \\geq d $ and depth $L$: your positional encodings for one graph form a matrix $n \\times d$ to be processes by a matrix $d \\times \\overline{d}$ in the first layer and $\\overline{d} \\times \\overline{d}$ later on, for $L$ times, which gives $\\mathcal{O}(n\\overline{d}L)$. You simply consider that $n \\leq n^2$? Moreover your architectures are not simple MLPs? And the gradient calculations? Please detail.\n\niv) In the experiments: unless I am wrong, you never specify the size of your trining data set, which is crucial since you need to compute $N_{train}^2$ GW distances with POT in order to have your ground truth...\n\nv) About appendix A. I tried to read it. Ok, I acknowledge that I am not someone who bounds empirical risks everyday, so my expertise might be too limited in order to fully understand this section. However... At line 618: there is something missing on the r.h.s. of the inequality? Maybe $\\sum_{I=1}^{N/2}$? In the proof of that lemma you care about the first term $U_N(\\ell_f)$. What about the second? At line 629 you meant \"to move sup *inside* the permutation average? Moreover: what is $\\mathcal{R}_N(\\mathcal{F})$ at line 634 ? It is never defined. \n\nvi) About appendix C. I think in Eq. (13) the second sum should be $\\sum_{j = i+1}^N$ and, more important, is your estimated transport plan admissible? How to be sure that the marginals are fulfilled? \n\nMinor remarks:\n\ni) Be careful around line 041: when adding an entropic regularization term the Wasserstein distance is no longer a distance, so the term \"sinkhorn distance\" might be misleading. \nii) line 106: it should be $GW^2(\\mu, \\nu)$ in place of $GW^2(X, Y)$.\niii) line 158: it should be $\\alpha_{uv}$ in place of $\\alpha_{ij}$\niv) Eq. (9): who is $\\theta$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I5EvTa8Eje", "forum": "gwhQO3KaHI", "replyto": "gwhQO3KaHI", "signatures": ["ICLR.cc/2026/Conference/Submission2869/Reviewer_ioen"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2869/Reviewer_ioen"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760980640652, "cdate": 1760980640652, "tmdate": 1762916421522, "mdate": 1762916421522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the Gromov-Wasserstein Distances for comparing two graphs and proposes an approximation using a neural network. The proposed architecture combines the well-known GIN (graph isomorphism network) and a transformer module. The embedded graphs are then compared as empirical distributions using multiple maximum mean discrepancy (MMD) values."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important topic, which is the estimation of graph distances. It explores recent tools, such as graph isomorphism networks and transformers."}, "weaknesses": {"value": "The choice of using the graph isomorphism network (GIN) is motivated by its expressive power. However, they are comparable to the Weisfeiler–Lehman (WL) graph isomorphism test, namely they are as powerful as the 1-WL test in graph discrimination. This might not be enough in general, as one would require more discriminative power, beyond the 1-WL test.\n\nIt is well-known that GIN, as well as other related graph neural networks, suffer from over-smoothing issues. It is not clear how the proposed method overcomes this issue.\nWhile it would be recommended to have fewer layers, the impact of the number of layers needs to be better analyzed in the light of Theorem 1 (namely the increase of $K_1$).\n\nConsidering the MLP at the end of the architecture (not the one of the GIN), it is written in the main body that the MLP has only one layer at Page 6. Figure 1 shows 2 layers. Experimental settings as given in Page 15 consider a 3-layer MLP.\n\nIn the analysis of the computational complexity, it is written “If we treat L and S as constants”. It is not clear what the authors mean. Technically, they are not variables, but they are hyperparameters to be set by the user. \n\nThe provided numerical results are missing comparative analysis with related methods. The proposed method has essentially a Siamese architecture, which would be similar to many other methods from the literature, including those cited in Section 4.2 (and many others). The authors have chosen to compare to only the Wasserstein Wormhole. The other compared methods are sampled GW and Spar-GW, which are less related to the current architecture and are a bit old.\n\nThere are some sentences that are poorly written, such as “Modeling a graph … is a choice for comparing graph pairs”, “The aforementioned methods are lacking of …”…"}, "questions": {"value": "No further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3E1yl2dLsn", "forum": "gwhQO3KaHI", "replyto": "gwhQO3KaHI", "signatures": ["ICLR.cc/2026/Conference/Submission2869/Reviewer_qoyc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2869/Reviewer_qoyc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687205282, "cdate": 1761687205282, "tmdate": 1762916421036, "mdate": 1762916421036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article proposes a neural network architecture for learning the Gromov–Wasserstein (GW) distance. The approach consists of embedding pairs of graphs using the concatenation of two GNNs—a GIN and a Graph Transformer—and then computing distances via an MMD over the embeddings with multiple kernel parameters, followed by a final MLP that predicts the GW distance. The authors describe the architecture in detail and validate their method on small graph datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an interesting and important problem: scaling up the computation of the GW distance. While the use of neural networks to approximate Wasserstein distances has been explored in several prior works, extending this idea to the Gromov–Wasserstein setting is relatively novel. This formulation provides a natural way to handle graphs of varying sizes.\nSome empirical results seem also to show taht the approach is working well against another NN approach for GW.\n\nHowever, the paper currently feels somewhat incomplete, which prevents me from giving a clear recommendation for a clear acceptance."}, "weaknesses": {"value": "- About the experiments:\n\nThe experimental section appears somewhat limited, and several aspects of the setup are unclear.\n\nOverall, the experiments mostly evaluate how close the predicted distance is to the POT GW distance. While this is a reasonable starting point, it remains a limited and a bit narrow evaluation.\nA more insightful analysis of the approximation error would strengthen the paper. For example, experiments on synthetic data, such as Stochastic Block Models (SBMs) with varying numbers of communities, could illustrate whether the learned distance preserves meaningful structure. A heatmap or distance matrix visualization could reveal clustering among graphs with the same community structure.\nSimilarly, scatter plots comparing “POT GW distance” versus “Neural GW distance” on held-out test pairs (not used during training) could demonstrate whether the predicted distances correlate well beyond the average error.\n\nRegarding the spectral clustering experiment, I am not fully convinced by the results. Without simple baselines, it is difficult to assess whether the proposed method provides real benefits. How do standard spectral clustering methods with simple kernel on graphs perform on the same datasets?\n\nOn the clarity of the experiments: the meaning of “in-prediction” is not clearly defined. Is the error computed on pairs of graphs seen during training, or on unseen test pairs? This distinction is essential to assess generalization. The setup of sampled GW is also insufficiently explained. For instance, how is the number of sampled points chosen for the sampled GW baseline? Additionally, the authors should clarify the statement “...can be the adjacency matrices or the shortest path matrices”: which one is used in the reported experiments? It should also be emphasized that the so-called “true GW” distances are computed with POT — currently mentioned in a footnote. The most relevant comparison, in my view, would be against Wormhole, which shows that indeed the Neural GW has a interest.\n\n- About the generalization error bound:\n\nAlthough the theoretical result on the generalization bound is potentially interesting, its presentation is somewhat “dry”. The statement is not discussed or contextualized, and its concrete significance is unclear. The result appears to be a bound between an expectation and its empirical average — a type of inequality that is very standard in statistic and learning theory.\nWhat makes this particular result specific to GW learning? why is this of particular interest? Is the proof technique unique to this setting? These points should be clarified. Moreover, the notations are particularly dense and difficult to parse, and there are several typos in the theorem statement.\n\n\n\n- Minor remarks:\n\nTable 4 is not very readable, as it is difficult to clearly observe how the error evolves with the number of layers. A line plot would be more appropriate, including the naive Neural GW baseline for reference.\n\nThe article also contains several awkward or imprecise formulations, for example:\n    - Some sentences are a bit awkward or imprecise:\n        - \"These datasets vary in size from small to big.\": Datasets with a maximum of 2000 nodes hardly qualify as \"big.\"\n        - \"If the model designed does not match the data well, we will encounter significant overfitting or underfitting\"\n        - \"While a graph represents the relationships among entities, the relationships and entities of graphs from different domains have different meanings and sizes, which results in huge challenges in cross-domain learning and generalization\"\n\nFinally, a recent relevant reference that could be cited (though not necessarily compared against, as it is very new) is [1].\n\n[1] Unsupervised Learning for Optimal Transport plan prediction between unbalanced graphs, Sonia Mazelet, Rémi Flamary, Bertrand Thirion, NeurIPS, 2025."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AWq7b1YTfh", "forum": "gwhQO3KaHI", "replyto": "gwhQO3KaHI", "signatures": ["ICLR.cc/2026/Conference/Submission2869/Reviewer_mnXv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2869/Reviewer_mnXv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741603182, "cdate": 1761741603182, "tmdate": 1762916420890, "mdate": 1762916420890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to estimate the Gromov-Wasserstein (GW) distance between any pair of graphs with siamese graph neural network architectures, called NeuralGW-naive and NeuralGW, as well as an extension mostly discussed in the supplementary material to estimate transport plans. To this end, they propose to first encode each graph via a MPNN (GIN) and transformers operating on SVD-based positional encoding, providing node embeddings accounting for local and global structures respectively, before concatenating them to get final node embeddings. The latter are generated for two graphs independently, then node embeddings are compared across graphs, either using an Euclidean distance after a global (mean or sum) pooling step for NeuralGW-naive, or MMD distances with Gaussian kernels for NeuralGW. Authors provide a generalization bound for their approach and show that NeuralGW outperforms several baselines to estimate GW within a given domain and study the transferability of the models across different datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tAuthors adapt siamese networks to the graph domain for the estimation of GW distance with an original pooling strategy to compare node embeddings of each graph leveraging MMDs.\n-\tThey provide a theoretical analysis of the generalization of their models\n-\tAuthors benchmark their approach against stochastic estimators for GW (Sampled GW, Spar-GW) and a deep learning baseline (Wormhole) and achieves the best GW estimations on several small-scale datasets.\n-\tThey study the transferability of their models across different datasets showing rather good results.\n-\tThey provide an ablation study over the number of MMD kernels considered in NeuralGW, showcasing that their approach is rather robust to this hyperparameter and outperforms other baselines in many settings."}, "weaknesses": {"value": "- **W1. clarity of the paper.** Overall, I believe that the clarity of the introduction and certain sections should be improved on several aspects:\n    -  In the context of OT including GW, graphs are always empirical distributions so it might be clearer to refer to methods which rely on the design of node embeddings in the second paragraph, either non-parametric ones like Weisfeilher-Lehman variants or deep learning based ones.\n    -  Most theoretical results for GW hold for any network following [A] and its not clear in the context of the paper why measurable metric spaces are more relevant as methods operate on arbitrary adjacency matrices. This could be used to clarify the 3rd paragraph as well as Section 2.1.\n    - In the 4th paragraph, the reference to FGW does not seem particularly appropriate as its goal is not to estimate GW. However, solvers to estimate exactly GW instead of an entropically regularized variant, such as the PPA from [B] or BAPG from [C] should be considered in the paper (in the introduction but also Section 4.1).\n    - Many arguments are made w.r.t computational complexity in Section 2, so it could be nice to refer the reader to Section 3.4 at an earlier stage in the paper.\n    - It can be clearer to formulate pseudo metric properties of neuralGW as a proposition in the main paper with proofs in the supplementary material.\n\n\n- **W2. choice of architecture** : authors propose an architecture where local and global node features are learned by two independent encoders (MPNN and transformer), whereas many recent models propose instead to fuse both local and global information within each layer as well summarized in [D]. Authors should further justify their choices and potentially include such well established graph transformer architectures in their study.\n\n\n- **W3. pooling mecanism** :\n    - could authors clarify their choices for MMD compared to other kernels such as Wasserstein or Sinkhorn ? Could you also explain how did you select the parameters $\\gamma_s$ for each kernel in the experiment section, for the in-domain and across-domain benchmarks as well as the ablation study on the number of kernels ? \n    - It seems relevant to refer to template-based approaches which also mix GNN with kernels, for more informative global pooling than in neuralGW-naive as well as related works for the paper [E, F]  \n\n- **W4. potentially incomplete baselines**: I am concerned by the comprehensiveness of the literature review made by authors on estimators for GW in Section 4.2 and Section 5.\n   - Courty & al (2017) focus on Wasserstein estimators hence i am not sure that the reference is properly used.\n   - Zhang & al (2024) mentioned in the paper seems to be a highly performant competitor for GW estimation, it seems like it should be included in the benchmark.\n   - Most neural estimators for GW discussed in [G] including GENOT from Klein & al (2024) seem relevant to address the task at end, could authors whether there were well-founded reasons to omit them from the benchmark ? \n\n\n- **W5. relevance of the loss function**: Overall it is not clear to me whether picking an estimator of the GW distance as ground truth is a good global objective compared to methods which estimate directly the transport plan before plugging it within the GW loss. The former choice makes highly sensitive the model to the initial quality of the GW estimation, implying that the model is very likely learned with \"label noise\", which is not discussed in the paper. Hence could authors clarify how did they estimate the ground truth GW distances and clarify why they did not consider more robust learning strategies ? \n\n- **W6. Transferability benchmark**: The choice of the different datasets used for learning models and evaluating them in Section 5.2 is not clear. It seems more natural to simply take models learned in the benchmark of Section 5.1 on each dataset individually and evaluate them on the other unseen datasets. Could authors provide a such benchmark ?  I guess models struggle to generalize while learning on very few datasets which motivated the choice of authors to learn from combination of datasets. However it seems that this study was only done considering datasets of molecules, which might have very similar structures. Wouldn't it be more relevant to stress diversity in the chosen datasets e.g a molecular one, a bio-informatic one and social network one to learn more transferable models ? \n\nTipos:\n-  L35. By graph search, do you mean nearest graph search ? otherwise from my understanding graph search or traversal relates more to node-level tasks which might be a bit out of the scope of this paper.\n-  L73. “are lack of” -> “lack of”\n-  L181. It seems confusing to refer to the l1 loss here while I guess the l2 loss was used in experiments as it is used in the theoretical analysis.\n\n[A] Chowdhury, S., & Mémoli, F. (2019). The Gromov–Wasserstein distance between networks and stable network invariants. Information and Inference: A Journal of the IMA, 8(4), 757-787.\n\n[B] Xu, H., Luo, D., Zha, H., & Duke, L. C. (2019, May). Gromov-wasserstein learning for graph matching and node embedding. In International conference on machine learning (pp. 6932-6941). PMLR.\n\n[C] Li, J., Tang, J., Kong, L., Liu, H., Li, J., So, A. M. C., & Blanchet, J. A Convergent Single-Loop Algorithm for Relaxation of Gromov-Wasserstein in Graph Data. In The Eleventh International Conference on Learning Representations.\n\n[D] Rampášek, L., Galkin, M., Dwivedi, V. P., Luu, A. T., Wolf, G., & Beaini, D. (2022). Recipe for a general, powerful, scalable graph transformer. Advances in Neural Information Processing Systems, 35, 14501-14515.\n\n[E] Chen, B., Bécigneul, G., Ganea, O. E., Barzilay, R., & Jaakkola, T. (2020). Optimal transport graph neural networks. arXiv preprint arXiv:2006.04804.\n\n[F] Vincent-Cuaz, C., Flamary, R., Corneli, M., Vayer, T., & Courty, N. (2022). Template based graph neural network with optimal transport distances. Advances in Neural Information Processing Systems, 35, 11800-11814.\n\n[G] Carrasco, X. A., Nekrashevich, M., Mokrov, P., Burnaev, E., & Korotin, A. (2023). Uncovering Challenges of Solving the Continuous Gromov-Wasserstein Problem. arXiv preprint arXiv:2303.05978."}, "questions": {"value": "I invite authors to discuss the weaknesses mentioned above and have a last question to clarify computational performances of their neuralGW model:\n- Could you detail which device (CPU, GPU etc) was used in the benchmark in supplementary B.2 ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FuGfdYXq1E", "forum": "gwhQO3KaHI", "replyto": "gwhQO3KaHI", "signatures": ["ICLR.cc/2026/Conference/Submission2869/Reviewer_u6um"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2869/Reviewer_u6um"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762122369715, "cdate": 1762122369715, "tmdate": 1762916420692, "mdate": 1762916420692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}