{"id": "y4791a4W9E", "number": 12429, "cdate": 1758207741021, "mdate": 1759897510454, "content": {"title": "SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models", "abstract": "Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose \\textbf{SASQ}: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.", "tldr": "SASQ proposes a QAT framework that optimizes activation scaling factors for static quantization, achieving high accuracy and lower perplexity than FP16 models.", "keywords": ["Model Quantization", "Quantization-Aware Training", "Efficient Inference", "Large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe7905bc417fdfc762c37cbb3c05b5af7a44f841.pdf", "supplementary_material": "/attachment/ff1c76fb061a23208951bab7758269beb560d5d7.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes SASQ (Static Activation Scaling Quantization-aware Training) — a lightweight quantization-aware training (QAT) framework for large language models (LLMs).\nUnlike prior QAT methods that fine-tune model weights, SASQ only optimizes activation quantization factors, keeping the pre-trained weights fixed. This enables static quantization (low inference cost) with accuracy comparable to or even better than FP16 baselines.\nIt introduces a phased quantization for autoregressive generation, using static per-channel quantization in the prefill phase and dynamic per-token quantization during generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Focusing QAT solely on activation scaling factors is conceptually elegant and avoids expensive weight fine-tuning."}, "weaknesses": {"value": "The paper explains why SASQ works mainly empirically. A more formal analysis of why optimizing scaling factors alone can work would strengthen the contribution.\n\nThe paper lacks validation on instruction-following models, which are essential for evaluating practical performance and generalization.\n\nThe baselines compared in this paper are mainly LLM-QAT and SpinQuant, both of which are relatively early methods."}, "questions": {"value": "What is the calibration dataset used in this paper? \n\nThe paper raises some factual concerns. For example:\n\n“The rapid parameter growth in LLMs (e.g., GPT-4’s 1760B; OpenAI et al., 2024) far exceeds hardware memory capacity growth, drastically increasing deployment costs.”\n\nHow is the number “1760B parameters” for GPT-4 obtained? Is this figure accurate or supported by any official source?\n\nSimilarly, the statement\n\n“Crucially, model sizes plateaued at around 1600B post-2022, demonstrating that GPU/TPU memory constraints now limit model advancement.”\n\nalso lacks clear evidence. How was “1600B” determined? Are there reliable data or citations supporting this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g9ZEZm7cAz", "forum": "y4791a4W9E", "replyto": "y4791a4W9E", "signatures": ["ICLR.cc/2026/Conference/Submission12429/Reviewer_GYVV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12429/Reviewer_GYVV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761407093843, "cdate": 1761407093843, "tmdate": 1762923317379, "mdate": 1762923317379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a method that focuses on optimizing activation quantization factors while keeping the model weights frozen, thereby reducing the training cost compared to traditional Quantization-Aware Training (QAT) methods. The authors validate their method on several downstream accuracy tasks using models ranging from 1.5B to 13B parameters. The experimental results demonstrate measurable improvements over other QAT baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-written and demonstrates clear logical flow.\n- The phase-based quantization strategy for handling prefill and decoding stages is interesting and has practical value for real-world deployment."}, "weaknesses": {"value": "- The core technical innovation is limited.\n- The main claimed advantage is reduced tuning cost, but the paper lacks a thorough analysis and quantitative comparison (e.g., in terms of computational FLOPs, training time, or energy consumption) against traditional QAT methods to substantiate this claim robustly."}, "questions": {"value": "Given that the prefill and decoding phases use different quantization strategies, what is the overhead in a real deployment scenario? Could you provide an analysis or estimation of the latency/throughput impact compared to a uniformly static approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aekljcRTME", "forum": "y4791a4W9E", "replyto": "y4791a4W9E", "signatures": ["ICLR.cc/2026/Conference/Submission12429/Reviewer_6m57"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12429/Reviewer_6m57"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811987611, "cdate": 1761811987611, "tmdate": 1762923317060, "mdate": 1762923317060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SASQ which does static activation quantization for large language models. The proposed technique learns activation scale factors using quantization aware training which helps to reduce quantization error. The paper is well written and easy to follow. However, the paper fails to convincingly demonstrate the effectiveness of the approach. The hardware performance benefits obtained by SASQ are only marginal compared to dynamic quantization and the algorithm performance is worse. Additionally, many static quantization baselines are missing."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written."}, "weaknesses": {"value": "Manuscript related:\n1. The paper advocates static activation quantization for efficient LLM inference, but resorts to dynamic activation quantization during decoding phase. \n2. Line 188-191 : \"Some studies attempt to mitigate this by shifting such outliers through mathematical transformations Xiao et al. (2023); Ashkboos et al. (2024), but these approaches inevitably alter the model weights, which can disrupt the delicate internal representations learned during pre-training Kumar et al. (2024).\" Can the authors explain what do they mean by this? Both Smooth quant and Quarot introduce transformations which maintain computational invariance, the LLM's output distribution remains unchanged in the absence of quantization.\n3. Please use \\citep{} or parenthesis when citing references.\n\nEvaluation related:\n1. The hardware performance implication with SASQ is not impressive. It is barely faster than SMQ-dynamic quantization (less than 5%). Therefore the authors do not convincingly provide a case for static activation quantization.\n2. Lot of results in algorithm evaluation are missing. the code for baselines is open source and it should be possible to get results atleast for post-training quantization baselines on all the benchmarks presented.\n3.  Compared to Quarot, SASQ is worse in all the zero shot accuracy benchmarks. \n4. Paper is missing important static quantization baselines : PrefixQuant (https://arxiv.org/pdf/2410.05265), OmniQuant (https://arxiv.org/abs/2308.13137), CushionCache (https://arxiv.org/pdf/2406.12016), etc.\n5. Most recent LLM weight and activation quantization papers also show results on W4A4, which is missing."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FaGD1u8aq4", "forum": "y4791a4W9E", "replyto": "y4791a4W9E", "signatures": ["ICLR.cc/2026/Conference/Submission12429/Reviewer_UtvQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12429/Reviewer_UtvQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878737021, "cdate": 1761878737021, "tmdate": 1762923316637, "mdate": 1762923316637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SASQ, a lightweight Quantization-Aware Training (QAT) framework designed to address the deployment challenges of large language models (LLMs). Unlike traditional QAT which fine-tunes all model weights , SASQ exclusively optimizes only the activation quantization factors, leaving the pre-trained weights untouched."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The implementation description is detailed and easy to follow.\n2. This paper test quantized model in generation task, which is omit in previous paper."}, "weaknesses": {"value": "2. The writing and logical of this paper is poor and should be improved. Though I am a expert in this area, it also take me long time to understand this paper. For example, Line 260 mention Table 3, the jumping is too large to understand. This paper should be re-organized.\n3. I donot agree with the claim \"However, our experiments show that such transformations can be realized simply by adjusting the quantization factors\". Smoothquant and QuaRot solve outlier with equivalent transformation, which do not introduce clamp error, while the clamp operation in this paper would introduce additional clamp error.\n4. This paper tests on W8A8 quantization, which is out of data. W8A8 quantization is nearly a solved problem.\n5. The perplexity results on Wiki2 and Wiki103 are unriliable due to the over-fitting problem.   For example, the perplexity of wiki* even better than FP16 baseline because of over-fitting.\n6. This paper should compare with more recent state-of-the-art methods, such as PrefixQuant [1], DuQuant [2], OSTQuant [3], FlatQuant [4].\n[1]  Prefixquant: Eliminating outliers by prefixed tokens for large language models quantization\n[2] Duquant: Distributing outliers via dual transformation makes stronger quantized llms\n[3] Ostquant: Refining large language model quantization with orthogonal and scaling transformations for better distribution fitting\n[4] Flatquant: Flatness matters for llm quantization"}, "questions": {"value": "1. What is the meaning of g(X) in Eq.6?\n2. Why low-bits model can even surpas the baseline, it is counterintuitive?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9ITCDEzvfE", "forum": "y4791a4W9E", "replyto": "y4791a4W9E", "signatures": ["ICLR.cc/2026/Conference/Submission12429/Reviewer_keBX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12429/Reviewer_keBX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892357641, "cdate": 1761892357641, "tmdate": 1762923316128, "mdate": 1762923316128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}