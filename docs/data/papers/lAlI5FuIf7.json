{"id": "lAlI5FuIf7", "number": 9935, "cdate": 1758150611710, "mdate": 1759897684495, "content": {"title": "Planner Aware Path Learning in Diffusion Language Models Training", "abstract": "Diffusion language models have emerged as a powerful alternative to autoregressive models, enabling fast inference through more flexible and parallel generation paths. This flexibility of sampling is unlocked by new engineered sampling strategies, or *planners*, that select more favorable generation paths by iteratively planning---versus uniformly at random---where to denoise along the sequence. However, by modifying the reverse paths via planning, planners create an irrevocable mismatch between the uniformly random denoising paths during training and planning-based inference. In this paper, we systematically investigate the mismatch of discrete diffusion training and inference under planning and theoretically prove that the standard discrete diffusion training evidence lower bound (ELBO) does not accurately describe a denoiser that uses a non-uniform planner. To address this gap, we derive a new planned evidence lower bound (P-ELBO) that incorporates planner-based reverse dynamics directly into the training objective.\nUsing the P-ELBO, we introduce *Planner Aware Path Learning* (PAPL), a novel training scheme that aligns training and inference under a planned denoiser.\nPAPL is implemented as a simple yet effective modification to the standard masked discrete diffusion loss, making it widely applicable and easy to adopt.\nEmpirically, we show PAPL delivers consistent gains across domains, including a 40\\% relative improvement in protein sequences, improved text generation with up to a $4\\times$ relative MAUVE gain, and 23\\% relative improvement in code generation HumanEval pass@10.", "tldr": "We propose Planner Aware Path Learning (PAPL), a simple planner-aligned training method for Diffusion Language Models that resolves the training–inference mismatch and consistently improves generation quality.", "keywords": ["Diffusion Language Models", "Discrete Diffusion", "Diffusion Models", "code generation", "protein generation", "text generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d6b681bc0975aac234254ce1c446d9b04634a94f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the training-inference mismatch in masked diffusion language models (DLMs). While DLMs are trained with uniform random masking/unmasking, they are deployed with planners that bias the denoising order (e.g., greedy, path planning). The authors prove that this mismatch causes the standard DLM ELBO to be violated at inference (Proposition 3.1). They derive a Planner-Aware ELBO (P-ELBO) that incorporates planner dynamics into the training objective (Proposition 3.2), then propose PAPL—a practical algorithm that simplifies P-ELBO into a planner-weighted cross-entropy requiring only a one-line code change. Experiments demonstrate consistent improvements: ~40% relative gain in protein foldability, up to 4× MAUVE improvement in text generation, and ~23% relative gain in code generation pass@10, all under identical model architectures and inference planners."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core contribution is original and well-motivated. Proposition 3.1 formally proves that greedy planner-based sampling can violate the standard ELBO inequality (log p_θ < ELBO), meaning the training objective no longer lower-bounds the inference distribution. This is a concrete theoretical finding that justifies rethinking the training procedure, not just an empirical observation.\n2. The theoretical framework is rigorous and general. The P-ELBO formulation (Proposition 3.2) unifies multiple existing planning strategies: uniform (standard DLM), greedy (MaskGIT), and soft-greedy, as special cases. \n3. The implementation is practical and low-friction. PAPL reduces to standard MDLM loss augmented with planner weights and requires no architectural changes or additional forward passes during training. Algorithm 1 shows this is essentially a one-line modification, making adoption straightforward.\n4. Empirical validation is comprehensive across diverse domains. The paper demonstrates improvements in protein generation (Table 1: higher pLDDT, pTM, lower pAE, +40% foldability while maintaining diversity), text generation (Table 2: consistent MAUVE and perplexity improvements across sampling budgets), and code generation (Tables 3-4: gains in both completion and infilling). The ablations (Figure 2, Figure 4) support design choices and show robustness.\n5. The presentation is clear and well-structured. The method section logically progresses from dynamics formulation to ELBO violation to the new objective. Experimental setups, metrics, and baselines are clearly described with sufficient detail for reproduction."}, "weaknesses": {"value": "1. The gap between P-ELBO and implemented objective is not quantified. Section 3.4 drops the correction term by detaching gradients through the planner, converting the full P-ELBO to a simpler weighted cross-entropy. The paper provides no analysis of the induced approximation error, theoretical bounds, or empirical measurements of this gap. \n2. Although the framework is generally applicable for any planner, the derived training objective and results are only for soft greedy decoding. For non-differentiable or hard planners, the correction term becomes non-trivial, and the paper does not provide a full derivation or empirical evidence beyond soft greedy-like planners. \n3. Text evaluation is limited to unconditional generation. While MAUVE and generative perplexity on OpenWebText are standard, they don't test whether planner-aware training improves conditional generation capability."}, "questions": {"value": "1. Can you measure the correction empirically at different training stages across domains? Some figures may be helpful.\n2. How should practitioners choose the two hyperparameters?\n3. How tight is the P-ELBO bound in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hbIL7I8ZLQ", "forum": "lAlI5FuIf7", "replyto": "lAlI5FuIf7", "signatures": ["ICLR.cc/2026/Conference/Submission9935/Reviewer_TRnB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9935/Reviewer_TRnB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761510932936, "cdate": 1761510932936, "tmdate": 1762921386349, "mdate": 1762921386349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Open source code if possible"}, "comment": {"value": "Hi author, I am a phd student in UW Madison and I am interested in your research project and want to test on other kinds of planner in training. I plan to do a final project for my course on this. If it is possible, would you mind sharing your codes with me, thank you so much for help."}}, "id": "TlB7fE6g0F", "forum": "lAlI5FuIf7", "replyto": "lAlI5FuIf7", "signatures": ["~Boyuan_Zou1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Boyuan_Zou1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9935/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763353594651, "cdate": 1763353594651, "tmdate": 1763353594651, "mdate": 1763353594651, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes planner-aware path learning for masked diffusion language models. The authors learn a model  $G_\\phi(z, x_k)$, where $z$ is a sample from the denoiser probabilities and $x_k$ is the k-th state, which outputs the next index to unmask. \n\nThe authors derive an evidence lower bound to lower bound the denoiser + planner log-likelihood, however as the objective requires $L^2$ function calls for sequences of length $L$, the authors propose a different tractable objective. The tractable objective re-weights the denoiser log-likelihood with an extra planner term."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "See questions"}, "weaknesses": {"value": "See questions"}, "questions": {"value": "1. The proof for the counter-example in proposition 3.1 requires that the denoisers are inconsistent. However, uniform un-masking with inconsistent denoisers can lead to inconsistent sampling as well. \n\nIt would be beneficial to a reader to see a re-written proposition 3.1, as the authors construct a counter-example rather than ***prove*** that greedy decoding does not sample from a unique learned joint distribution. Under the constructed counter-example, even uniform sampling would not sample from a single joint distribution as the denoisers are inconsistent.\n\n1. If the denoiser is consistent, then would greedy sampling work? As MDLMs learn an any-order autoregressive model, would any decoding order at inference would produce valid samples? \n2. If the denoisers are consistent, then is the following line correct:\n    1. *The key takeaway is that the ELBO in equation 1 is only valid for the uniform unmasking process (line 247)*. \n3. what does the planner learn at optimality\n\nExperiments\n\n1. Can the authors compare PAPL to greedy sampling in the experiments? \n2. In several experiments, the gap between the MDLM+planned and MDLM+baseline with  experiments are not substantial. Can the authors provide a confidence interval?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cZDShssAeJ", "forum": "lAlI5FuIf7", "replyto": "lAlI5FuIf7", "signatures": ["ICLR.cc/2026/Conference/Submission9935/Reviewer_jzen"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9935/Reviewer_jzen"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939606716, "cdate": 1761939606716, "tmdate": 1762921385879, "mdate": 1762921385879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed PAPL, a lightweight modification to the classic MDLM training objective that brings a performance boost. The authors first indicate a problem in the domain of diffusion language models, the mismatch between training and evaluation. Based on this insight, the authors derived an analytical form of the planner-aware ELBO (P-ELBO) and show that P-ELBO is theoretically general. Given that P-ELBO is computationally expensive, the authors proposed a cheap approximation, PAPL, and show that across protein modeling, language modeling, and coding benchmarks, adding PAPL to the MDLM loss can boost performance without extra computational expense."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* The storyline of the paper is clear. The paper aims to address a significant problem in the field of diffusion language models, and its solution is convincing.\n* The mathematical foundation is abundant. I especially appreciate that the authors proved the P-ELBO in an analytical form. The theory part is valuable and useful for the community.\n* The experimental analysis is solid, and the empirical results are strong. I am particularly surprised by the increase in pass@10 in the coding part.\n* The method is simple and practical. With some minimal changes to the training objective, PAPL can improve downstream performance."}, "weaknesses": {"value": "* As a person with proficiency in discrete diffusion, I have to say the math is a bit confusing. I hope the authors can add more explanation, at least in the appendix, to enhance comprehension (see Question No. 1 for details).\n* The transition from Proposition 3.2 to Equation 7 is not apparent, and more explanation might be needed."}, "questions": {"value": "* Is the CTMC perspective necessary for the derivation of the ELBO, i.e., Proposition 3.2? If not, can the authors add the derivation from the discrete Markov chain perspective in the Appendix? \n* Can the authors add more explanation on the transition from Proposition 3.2 to Equation 7, at least in the Appendix?\n* The authors claim the new ELBO is planner-aware, so it should be better than the MDLM ELBO. But the final training objective is also a fixed objective; are there any justifications for why this objective is better for various commonly used planners? In other words, is this objective only better for the greedy-sampling planner, or is this objective better for all the commonly used samplers for masked diffusion language models?\n* I would like the authors to clarify the planners used in their experiments. Are the planners just greedy sampling? If so, can the authors provide results with other planners, such as block decoding topk confidence from Nie et al. [1], confidence thresholding from Wu et al. [2], adaptive MDM inference from Kim et al. [3], and remasking samplers from Wang et al. [4]?\n* From a theoretical perspective, the authors claim that P-ELBO is a unified framework for all possible planners. Can the authors provide an instantiation for the four planners mentioned above?\n* In the right subplot of Figure 4, the downstream performance monotonically grows with $\\alpha$; what if $\\alpha$ goes beyond 5? I am assuming that at some point the downstream performance will go down, otherwise we should not interpolate between the MDLM loss and PAPL loss, but use PAPL loss alone instead.\n\n---\n**References**\n\n[1] Nie, S., Zhu, F., You, Z., Zhang, X., Ou, J., Hu, J., Zhou, J., Lin, Y., Wen, J.R. and Li, C., 2025. Large language diffusion models. arXiv preprint arXiv:2502.09992.\n\n[2] Wu, C., Zhang, H., Xue, S., Liu, Z., Diao, S., Zhu, L., ... & Xie, E. (2025). Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618.\n\n[3] Kim, J., Shah, K., Kontonis, V., Kakade, S., & Chen, S. (2025). Train for the worst, plan for the best: Understanding token ordering in masked diffusions. arXiv preprint arXiv:2502.06768.\n\n[4] Wang, G., Schiff, Y., Sahoo, S. S., & Kuleshov, V. (2025). Remasking discrete diffusion models with inference-time scaling. arXiv preprint arXiv:2503.00307."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P2nieZnLc7", "forum": "lAlI5FuIf7", "replyto": "lAlI5FuIf7", "signatures": ["ICLR.cc/2026/Conference/Submission9935/Reviewer_LjAj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9935/Reviewer_LjAj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946084449, "cdate": 1761946084449, "tmdate": 1762921385344, "mdate": 1762921385344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows the mismatch of diffusion objective between training and testing under planning and address it by revising the ELBO to incorporate planner basics in training. The new ELBO allows to effectively exploit any planning strategies at test time and it is simple to implement. The method show performance improvements on various tasks, such as protein sequences, text generation, code generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackle the relevant and important mismatch issue of training and testing of diffusion model under planning.\n2. The method is theoretically grounded though it is quite dense."}, "weaknesses": {"value": "1. Using confidence as a heuristic indicator could be unreliable for training. This is exactly shown in Figure 5 in the appendix. Though I see the authors using the vanilla DLM loss as a rescue, I think this is not an efficient way since at earlier training iterations, the confidence from model is not reliable, inducing noise into training (this is bad). One alternative is to do annealing where earlier step, use uniform weight as a sole vanilla DLM and it is annealed gradually to planner-weighted DLM.\n2. Look at the Table 1 and Table 2, it seems that the method negatively affect the data entropy since both are worse than vanilla model. Why?\n3. What is limitation? It is worth to mention."}, "questions": {"value": "1. Is the model sensitive to the temperature and the weight alpha? Need more ablation on text generation and coding. \n\n2. What exactly P2 sampling is  used? Since the P2 sampling has different variants. \n\n3. Apart from P2 sampling, have the authors tried other planning strategies like greedy decoding like MaskGIT, top-k, top probability margin (from Kim et al, https://arxiv.org/abs/2502.06768)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C5BvgZ6lxG", "forum": "lAlI5FuIf7", "replyto": "lAlI5FuIf7", "signatures": ["ICLR.cc/2026/Conference/Submission9935/Reviewer_2qD5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9935/Reviewer_2qD5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970723100, "cdate": 1761970723100, "tmdate": 1762921384686, "mdate": 1762921384686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}