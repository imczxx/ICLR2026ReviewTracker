{"id": "u7lXflJQX9", "number": 9616, "cdate": 1758130547459, "mdate": 1763360101793, "content": {"title": "PluriHarms: Benchmarking the Full Spectrum of Human Judgments on AI Harm", "abstract": "Current AI safety frameworks, which often treat harmfulness as binary, lack the flexibility to handle borderline cases where humans meaningfully disagree. To build more pluralistic systems, it is essential to move beyond consensus and instead understand where and why disagreements arise. We introduce PluriHarms, a benchmark designed to systematically study human harm judgments across two key dimensions—the harm axis (benign to harmful) and the agreement axis (agreement to disagreement). Our scalable framework generates prompts that capture diverse AI harms and human values while targeting cases with high disagreement rates, validated by human data. The benchmark includes 150 prompts with 15,000 ratings from 100 human annotators, enriched with demographic and psychological traits and prompt-level features of harmful actions, effects, and values. Our analyses show that prompts that relate to imminent risks and tangible harms amplify perceived harmfulness, while annotator traits (e.g., toxicity experience, education) and their interactions with prompt content explain systematic disagreement. We benchmark AI safety models and alignment methods on PluriHarms, finding that while personalization significantly improves prediction of human harm judgments, considerable room remains for future progress. By explicitly targeting value diversity and disagreement, our work provides a principled benchmark for moving beyond \"one-size-fits-all\" safety toward pluralistically safe AI.", "tldr": "PluriHarms is a new benchmark of 15,000 human ratings that reveals how annotator traits and prompt features drive disagreement in harm judgments, showing personalization helps but leaving ample room for existing models to improve alignment.", "keywords": ["AI safety", "annotator disagreement", "personalized alignment", "value pluralism", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8b60546cb7e0b316a8c9df1c19fa63b7d017e09f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces PLURIHARMS, to move beyond binary safety labels, a safety benchmark aimed at modeling not only harmfulness but also human disagreement about harm. The dataset contains 150 prompts spanning an ordinal harm spectrum, each annotated by 100 crowdworkers (15,000 total ratings) with rich demographic and psychological traits, and coupled with prompt-level features derived from SafetyAnalyst (actions/effects) and KALEIDO (values/rights/duties). Methodologically, the authors curate prompts via LLM-based synthesis and a genetic algorithm to over-sample borderline cases; they analyze determinants of judgments using mixed-effects regressions with lasso selection, and evaluate several safety models and personalization strategies (e.g., value-profile and k-shot steering). Results show (i) human ratings correlate with synthetic harm levels and exhibit higher variance/entropy in the mid-range, (ii) both prompt features and annotator traits (and their interactions) significantly shape harm judgments, and (iii) personalized alignment improves prediction over consensus-based approaches, though headroom remains."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper squarely advances the field beyond binary safety labels by treating harm as an ordinal and distributional phenomenon, making disagreement itself a first-class signal rather than noise and motivating pluralistic safety evaluation.\n- The authors implement a multi-step data-hygiene pipeline (e.g., attention checks, time-on-task floors, duplicate and consistency screening, and clear exclusion criteria), which materially improves label reliability and reduces typical crowd-sourcing artifacts. And annotator metadata are carefully documented, with clear distributions over demographics and psychological traits; this transparency enables reproducible research on personalization, fairness, and cross-group heterogeneity instead of relying on unverifiable assumptions. \n- By showing that annotator traits and their interactions with prompt features explain systematic disagreement, the paper offers a convincing, data-driven account of value pluralism in harm judgments—an insight that is both methodologically careful and theoretically meaningful for personalized alignment.\n- The benchmarking of safety models and personalized alignment methods on PLURIHARMS demonstrates consistent gains from personalization over consensus predictors, signaling practical utility today while clearly delineating room for future improvement in sample efficiency and generalization.\n- Figures and narrative are clear and well-scoped, with limitations explicitly acknowledged, which increases trust in the findings."}, "weaknesses": {"value": "- The paper commendably reports rich annotator trait distributions (demographic and psychological), but the pool is entirely U.S.-based (as the authors referred in Appendix D), so cultural/linguistic coverage is narrow and may limit the external validity of personalization/pluralism findings; a multilingual, multi-region replication with stratified sampling would strengthen claims.\n- The paper posits that \"harm\" and \"values\" are the two key dimensions for human harm perception (lines 145-147). While this operationalization is reasonable, the paper does not show that a two-dimensional schema is sufficient or unique; a comparison with alternative ontologies or a data-driven dimensionality analysis would make the claim more compelling.\n- Prompt spectrum relies on a single generator (DeepSeek-V3-0324) and AIR-Bench seeds. Although Appendix A specifies an 11-level grid and human ratings show a monotonic correlation with the synthetic levels, the benchmark's mid-spectrum may still be wording-sensitive or generator-specific. Robustness checks would strengthen the claim that disagreement is value-driven rather than phrasing-driven.\n- The evaluation probes only three models, which limits conclusions about cross-model generality. For a benchmark that aims to study pluralism and disagreement, broader coverage is important—not only more backbones but also diversity along key axes."}, "questions": {"value": "- During prompt generation (Appendix A), did high-harm prompts (like ≥0.8) frequently trigger safety refusals by the generator? If so, could you share refusal/abort rates by harm level and any backoff strategies (e.g., re-prompting, softer wording)? It would help to know whether refusals biased the realized spectrum (e.g., under-sampling at the top end).\n- In Final Dataset Curation (around lines 157–161), you describe targeting the desired harm-level distribution. Could you also report the dataset composition for other key features? This would clarify coverage and potential imbalances beyond the harm level.\n- Would you report a small cold-start slice? For example, leave-one-annotator-group-out (by trait strata) or leave-one-topic-cluster-out. This would test whether personalization via traits still helps for unseen users and content.\n\nIf the authors can substantively address the questions and weaknesses outlined above, I would be inclined to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ac2bclaAtB", "forum": "u7lXflJQX9", "replyto": "u7lXflJQX9", "signatures": ["ICLR.cc/2026/Conference/Submission9616/Reviewer_3ftX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9616/Reviewer_3ftX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816196738, "cdate": 1761816196738, "tmdate": 1762921155323, "mdate": 1762921155323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a PluriHarms, a benchmark for safety that consists of 150 prompts with annotations from 100 human annotators from various backgrounds, for a total of 15,000 ratings. PluriHarms aims to capture prompts that have subjective or ambiguous safety ratings, and capture differing views about safety from a diverse range of human annotators."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Annotator demographics are included, and are relatively diverse across a number of demographic categories reported in Appendix D.3.\n- The paper argues that disagreement should be treated as diverse, legitimate viewpoints, which is an important principle for pluralism.\n- As a benchmark, PluriHarms is challenging; WildGuard only performs about as well as the random baseline. This demonstrates that there is still a gap for developing safety models that can be aligned to diverse viewpoints.\n- This paper contributes a concrete dataset that can be used to ground the community’s discussion on pluralism and safety."}, "weaknesses": {"value": "- PluriHarms has only 150 prompts, which is a small number compared to other datasets such as AIR-Bench 2024 with 5,692 prompts.\n- More information could have been provided in Section 2 about the genetic algorithm used to curate prompts.\n- Annotator recruitment was done on Prolific, thus the annotator sample is subject to the biases of the annotator population on that platform, and may not generalize to other populations."}, "questions": {"value": "- For the final curation of 150 prompts, why was a genetic algorithm chosen over alternative simpler sampling methods?\n- Were all annotators based in the United States? If so, this should be acknowledged as a limitation.\n- Could you share your assessment of privacy risks associated with releasing annotation and demographic data of the annotators, and how you have mitigated these risks? Was this adequately addressed in your IRB research plan?\n- In Figure 8, does each dot represent the MAE of the model against a single annotator?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GLiSiKmT60", "forum": "u7lXflJQX9", "replyto": "u7lXflJQX9", "signatures": ["ICLR.cc/2026/Conference/Submission9616/Reviewer_96EC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9616/Reviewer_96EC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934162743, "cdate": 1761934162743, "tmdate": 1762921154447, "mdate": 1762921154447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark designed to move AI safety beyond binary harm classifications and toward a more pluralistic understanding. The authors present a scalable framework for generating prompts that systematically vary along two axes: harm severity (from benign to harmful) and human agreement (from consensus to disagreement). The benchmark contains 150 prompts, each with ratings from 100 human annotators, and is enriched with comprehensive annotator traits (demographic and psychological) and prompt-level features (actions, effects, values). Through extensive statistical analysis, the paper demonstrates that both prompt content-- especially tangible, imminent risks--and annotator traits jointly shape harm perception, with disagreement emerging from structured interactions between them. Finally, the authors benchmark several AI safety models and alignment methods, showing that personalized alignment significantly outperforms traditional consensus-based approaches, highlighting the need for systems that can adapt to diverse human viewpoints."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The inclusion of deep annotator profiles, incorporating psychological measures like the Moral Foundations Questionnaire and Schwartz Value Survey, and structured prompt features from SafetyAnalyst and Kaleido provides a useful resource for analysis. This rich data enables the paper's core investigations into the drivers of disagreement, moving beyond simple demographics to the underlying values and psychological traits of annotators.\n- The paper provides strong empirical evidence for a central hypothesis in pluralistic AI: that personalized alignment is superior to consensus-based alignment. The evaluation in Section 5 shows that for multiple models and methods, predicting individual ratings results in lower MAE than predicting aggregated ratings (Figure 8). This finding directly challenges the common practice of treating annotator disagreement as \"statistical noise to be averaged out\"."}, "weaknesses": {"value": "- The paper's core methodological claim is the creation of a \"calibrated\" harm spectrum by prompting an LLM (DeepSeek) to generate variants along a 0.0 to 1.0 ordinal scale. This process implicitly assumes the LLM is a neutral tool for varying a single latent dimension of \"harm.\" However, the LLM itself is a complex model that may introduce systematic stylistic artifacts (e.g., changes in syntax, vocabulary, or tone) that correlate with the requested harm level. These artifacts could act as confounding variables. While the human validation shows a significant correlation (Spearman r = 0.59 in Figure 3), it does not rule out the possibility that humans are partially responding to these stylistic cues rather than solely the intended semantic variation in harm. The study could be improved by controlling for or analyzing these potential linguistic confounders to ensure the benchmark's harm axis is not entangled with unintentional stylistic variations introduced by the generator model.\n\n- The analysis in RQ1 uses features extracted by SafetyAnalyst and Kaleido to predict human harm judgments. Finding that a feature like \"Child Harm\" extracted by SafetyAnalyst is a top predictor for human harm ratings (Figure 4) is somewhat circular. It demonstrates a strong correlation between one safety model's output and human labels, but it provides less independent explanatory power about the fundamental components of human judgment than implied. The analysis essentially confirms that a model built to identify specific harms generates features that align with human perceptions of those same harms. A more insightful approach might involve using more foundational, model-agnostic linguistic features to see if they can predict human ratings, thereby avoiding the conceptual loop of using one harm model to explain another (i.e., humans).\n\n- The paper concludes that GPT-4.1 is more accurate than specialized safety models like WildGuard and SafetyAnalyst, attributing this to the \"advantages of a more capable base model\". This interpretation glosses over the vast difference in model scale. The comparison is between a massive, state-of-the-art frontier model and much smaller, open-source models. It is therefore unsurprising that the larger model performs better, especially in a few-shot, in-context learning setting. The experiment does not provide a fair comparison of a \"general\" vs. \"specialized\" model architecture or training objective, but rather a comparison of model scale. A more rigorous experimental design would involve fine-tuning the smaller specialized models on the personalized training data to compare their adaptability against GPT-4.1's in-context learning, which would provide a more nuanced understanding of the trade-offs between model scale, specialization, and alignment techniques.\n\n- The study's design involves each annotator rating all 150 prompts in a single session, with a median completion time of 65 minutes. This block design is susceptible to ordering effects, where exposure to one prompt can prime or influence the judgment of subsequent prompts. The analysis models harm judgments as a function of stable annotator \"traits\" (demographics, psychological profiles) but does not account for transient psychological \"states\" (e.g., fatigue, desensitization, mood) that could evolve over the course of the long annotation task. Given the low variance explained by the annotator trait model (R^2 = 0.0231), these unmodeled, state-dependent effects could be a significant source of variance. Randomizing prompt order per participant and modeling for potential order effects would be necessary to more accurately isolate the influence of stable traits from the noise introduced by the experimental procedure itself."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6panFGtMfS", "forum": "u7lXflJQX9", "replyto": "u7lXflJQX9", "signatures": ["ICLR.cc/2026/Conference/Submission9616/Reviewer_wR6w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9616/Reviewer_wR6w"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015947596, "cdate": 1762015947596, "tmdate": 1762921153949, "mdate": 1762921153949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the benchmarks Pluriharms to study how people with diverse backgrounds and values judge harmfulness, emphasizing disagreement aspects in safety evaluation.\n\nThe authors construct the dataset through a multi-stage process: generating over 60,000 prompts spanning a fine-grained harm spectrum using LLMs, extracting interpretable harm- and value-related features, and finally creating 150 diverse, disagreement-prone prompts for human annotation. Each prompt receives ratings from 100 participants whose demographic and psychological traits (values, morality, empathy, AI literacy, etc.) are also collected.\n\nTheir findings show that both prompt features (such as physical vs. psychological harm, rights, and duties) and annotator traits (e.g., education, political believes, exposure to online toxicity) shape harmfulness judgments. \nWhen evaluating safety models  such as WildGuard and SafetyAnalyst on Pluriharms, they find that ''personalized alignment methods'' that are specific to individual annotators’ value profiles outperform general approaches ignoring annotators' traits, suggesting that pluralistic alignment better captures real human diversity.\n\nFinally, they conclude that safety assessment should model pluralism and disagreement rather than averaging it out."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Dataset (creation)\n* The process is well-explained. They achieve broad coverage and controlled variation across harm levels.\n* Through annotation with specialized safety rating models, they add semantic structure, linking prompts to human-understandable ethical dimensions.\n* Each participant rated all prompts. This allows within-subject comparisons of how individual traits affect harm judgments.\n* The correlation between synthetic harm levels and human ratings supports the validity of the synthetic harm generation procedure.\n\nAnalysis\n* Modeling trait × trait and trait × prompt reveals how disagreement stems from structured social and cognitive differences rather than noise.\n\nExperiments \n* Including both binary and probabilistic safety outputs (e.g. for WildGuard) provides insight into how uncertainty affects safety prediction.\n* The profile-based alignment approaches are well motivated and improve model-human alignment."}, "weaknesses": {"value": "Dataset (creation)\n* The dataset is limited in size, covers only English prompts, and considers limited demographic diversity\n* The focus on single prompts rather than (more realistic) full human–AI conversatinos/interactions.\n* The reliability of the model-based feature extraction (SafetyAnalyst, KALEIDO) is assumed instead of validating themselves, potential biases in those models could affect prompt selection and impact the dataset curation process.\n* The paper does not contain any details on data quality control for human ratings, such as attention checks or interrater reliability.\n* Can you describe more clearly describe how the LLM was prompted to produce controlled harm variants?\n\nAnalysis \n* The variance (~ 27% for prompt features, ~2–7% for traits and interactions) is noteworthy, but the discussion does not address what drives the remaining unexplained variance.\n* The section presents many correlations and models but gives little conceptual synthesis connecting quantitative results to theories of moral judgment or harm perception.\n* The analysis relies on automatically extracted features (from SafetyAnalyst and KALEIDO) which are not validated, so the interpretability of coefficients depends on potentially noisy input features.\n* Sec 4 would benefit from qualitative examples showing how specific prompts or participant profiles led to divergent ratings.\n\nFigures\n* Readability of figures can be improved, e..g Fig 4: difficult to align x axis labels to their respective bars, Fig 5: captino is not mentioning what Factor 1-3 are but one needs to search in the text, Fig 6: the placement of the legend, etc. \n\nExperiments \n* The dataset used for evaluation is quite small & might  generalizability.\n* It is unclear how the “value profile” steering process was conducted (e.g., whether GPT-4.1 generated summaries consistently or varied across runs) - more details would be useful.\n* There is little discussion on wht personalized methods outperform aggregated ones — the analysis remains descriptive rather than diagnostic. What insights can we extract from these experiments and how can they guide development of future safety evaluators?"}, "questions": {"value": "See section weaknesses for questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BKTMGuk331", "forum": "u7lXflJQX9", "replyto": "u7lXflJQX9", "signatures": ["ICLR.cc/2026/Conference/Submission9616/Reviewer_EMxJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9616/Reviewer_EMxJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762245103777, "cdate": 1762245103777, "tmdate": 1762921153649, "mdate": 1762921153649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}