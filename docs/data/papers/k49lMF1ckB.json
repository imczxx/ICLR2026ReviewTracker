{"id": "k49lMF1ckB", "number": 12646, "cdate": 1758209237835, "mdate": 1759897496324, "content": {"title": "MVGE: Scale-invariant and Temporal-consistent Monocular Video Geometry Estimation", "abstract": "We present MVGE, a novel approach for estimating 3D geometry from extended monocular video sequences, where existing methods struggle to maintain both geometric accuracy and temporal consistency across hundreds of frames. Our approach generates affine-invariant 3D point maps with shared parameters across entire sequences, enabling consistent scale-invariant representations. We introduce three key innovations: viewpoint-invariant geometry aligning multi-perspective points in a unified reference frame; appearance-invariant learning enforcing consistency across exponential timescales; and frequency-modulated positioning enabling extrapolation to sequences vastly exceeding training length. Experiments across diverse datasets demonstrate significant improvements, reducing relative point map error by 24.2% and temporal alignment error by 34.9% on ScanNet compared to state-of-the-art methods. Our approach handles challenging scenarios with complex camera trajectories and lighting variations while efficiently processing extended sequences in a single pass. Code will be publicly released, and we encourage readers to explore the interactive demonstrations in our supplementary materials.", "tldr": "MVGE achieves both geometric precision and long-range temporal consistency in 3D point map estimation from monocular videos through viewpoint-invariant transformations and frequency-modulated temporal modeling.", "keywords": ["monocular video", "3D geometry", "temporal consistency", "scale-invariant", "point maps", "depth estimation", "long-range modeling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88856eafff418b43f0cc8e6ad467da52b2d9b672.pdf", "supplementary_material": "/attachment/a9fcb85987aa701471e1ab1d0ce0386528daaf3b.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a temporal-consistent monocular video depth method. Given a set of multiview frames up to hundreds of frames, the method output temporally consistent affine-invariant depth maps. With an off-the-shelf pose estimator, the method aligns the depths to pointmaps. Qualitative and quantitatively, the method outperforms state-of-the-art feedforward methods that directly output pointmaps at the canonical coordinate (eg., VGGT) or monocular depth methods (MoGe or DepthPro).\n\n---\n\nThere are several concerns on insufficient comparison, unclear details, insufficient ablation study, and the novelty concerns. Thus the recommendation is **2: reject**. It would be appreciated if any justifications on those concerns are provided."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Quite interesting technical components for training**\n\n  In section 3.1 and section 3.2 (or Figure 2), the paper introduces quite interesting technical components to extend the previous monocular depth method (MoGe) to be temporal consistent along the video frames. This include multi-scale loss, temporal consistency loss, and RoPE with dynamic NTK scaling to handle longer sequence at test time.\n\n* **Good accuracy**\n\n  According to Table 1, the method achieves better accuracy than direct competitors such as DepthPro, VGGT, and MoGe. The attached supplementary includes several qualitative examples. The visual quality is very good."}, "weaknesses": {"value": "* **Insufficient comparison**\n\n  One of the end goals of the paper is temporally consistent video depth estimation. In that regards, there are missing baselines from the video depth literatures, eg., DepthCrafter, DepthAnyVideo, Video Depth Anything, etc. Also, 3D pointmap or long-range reconstruction is the other goal. So it makes sense to compare other relevant baselines, eg., Spann3R, CUT3R, etc., or SLAM baselines, eg, DROID-SLAM, MASt3R-SLAM, DPVO, ORB-SLAM, MegaSaM.\n\n\n* **Unclear details**\n\n  The paper needs better clarity. There are lots of missing details that makes understanding technical details difficult.\n  * At line 160, LHS and RHS have the same variables, $P_i,$ which maybe needs to be different\n  * At line 255, what is $w_i$, depth-aware weight? There is no explanation or equation.\n\n  Also some technical designs are not easy to understand and may need better justifications. \n  * At line 255, why ($s_c$, $t_c$) are computed independently for each cell? For consistency among each cell, shouldn't they be considered together?\n  * Why is multi-grid or local alignment needed in section 3.1? To be spatio-temporally consistent over all frames, isn't it enough if we have one global alignment? What benefit does the multi-grid or local alignment provide?\n  * At line 259 how is the derivative ($\\delta D / \\delta t$) computed? Is it a difference between two depth maps at the different time steps?\n\n* **Ablation study**\n\n  In section 4.3, the ablation study on geometric and temporal constrains is not so clear. Would it be possible to provide it as a table with evaluation on multiple datasets? (providing other qualitative examples could be a plus). In the current presentation format (only by looking at few numbers), it is difficult to understand how strong its effectiveness is. \n\n* **Novelty concern**\n\n  The method uses an off-the-shelf pose estimation method, which can be an unfair advantage over other methods that jointly estimate camera pose or pointmaps at the canonical coordinate. What would be the main novelty over those methods?"}, "questions": {"value": "It's included in the weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CV0mTdTKR7", "forum": "k49lMF1ckB", "replyto": "k49lMF1ckB", "signatures": ["ICLR.cc/2026/Conference/Submission12646/Reviewer_kBmV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12646/Reviewer_kBmV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883447198, "cdate": 1761883447198, "tmdate": 1762923487508, "mdate": 1762923487508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MVGE, a video model that predicts affine-invariant 3D point maps for all frames in a long monocular sequence with shared per-sequence scale/shift parameters. It combines (i) cross-frame geometric alignment using poses during training, (ii) hierarchical temporal consistency via multi-scale temporal derivative losses, and (iii) NTK-adapted RoPE with \"sequence stretching\" to extrapolate beyond the training context. The method reports improved point/depth accuracy and temporal alignment over MoGe, VGGT, and video-depth baselines, and demonstrates single-pass inference over hundreds of frames reasonably fast."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A timely solution for an important problem.\n- Well-motivated temporal losses across multiple strides; goes beyond adjacent-frame flow constraints.\n- Single-pass inference over hundreds of frames avoiding overlapping-window redundancy.\n- Good numbers in the experiments."}, "weaknesses": {"value": "Most importantly: \n- Missing SLAM comparisons. There is no quantitative comparison to modern SLAM/SGM pipelines that jointly estimate geometry and camera motion (e.g., VGGT-SLAM, Mast3R-SLAM, DPVO, DROID-SLAM, ORB-SLAM3, or recent learned SfM/SLAM hybrids). Given the focus on long sequences and drift, such baselines are crucial and the lack of them really does not allow me to judge how well the method really works.\n\nMajor things:\n- Assumption of shared intrinsics (single focal) and z-shift per sequence may fail under zooming/auto-focus or rolling-shutter effects. I know many paper assume this, but this is what makes many actual SLAM method not work on long sequences of in-the-wild videos. It would be important to analyze how much the methods suffer if zooming happens. \n- Memory footprint is very high (≈76 GB for 300 frames). The practical deployment constraints and scaling trade-offs (resolution vs. length vs. VRAM) are not analyses at all. If we talk about reconstructing videos, we usually mean minutes/hours long ones. This method with 30 FPS will work on 10 secs video on an A100. I am curious whether there is a way to solve the problem on smaller, more practically interesting GPUs and how would it work / degrade performance. For example, keyframing with wider frame steps, etc. \n\nMinor things:\n- Training uses ground-truth poses “optionally” for cross-frame loss, but the fraction of data using poses and its impact on generalization are unspecified. This should be provided."}, "questions": {"value": "All in all, I like the paper but my main problem is the lack of comparison to SLAM baselines. \nThe authors can convince me to improve my rating by providing comparison to SOTA SLAM methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "62cAX8Wirv", "forum": "k49lMF1ckB", "replyto": "k49lMF1ckB", "signatures": ["ICLR.cc/2026/Conference/Submission12646/Reviewer_hGv8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12646/Reviewer_hGv8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924336455, "cdate": 1761924336455, "tmdate": 1762923486995, "mdate": 1762923486995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MVGE, a novel framework for monocular video geometry estimation that aims to maintain both geometric accuracy and temporal consistency over long video sequences.\nThe authors introduce three major innovations: viewpoint-invariant geometry, appearance-invariant learning, frequency-modulated positioning. Experiments on ScanNet, KITTI, and Sintel show that MVGE improves geometric accuracy and temporal alignment by 34.9%, while running at high frequency. However, the system still depends on external pose estimation for full 3D reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses an important problem: how to maintain both geometric accuracy and temporal consistency over long video sequences.\n2. The three key components of the method form a cohesive end-to-end geometry estimation pipeline.\n3. Experiments show large improvements across multiple datasets, and cover diverse domains including indoor and outdoor.\n4. The proposed approach is well designed for cross-frame geometric consistency and hierarchical temporal consistency."}, "weaknesses": {"value": "1. MVGE cannot estimate camera poses directly. It relies on external methods like MegaSAM for reconstruction.\n2. Theoretical grounding for NTK scaling in the pipeline is limited."}, "questions": {"value": "1. Does the proposed pipeline account for moving objects or non-rigid motions? \n2. How robust is the method when the above dynamic elements are present?\n3. How does the shared scale and shift parameter assumption hold when the scene contains moving objects, depth discontinuities, or partial occlusions?\n4. The temporal loss uses hierarchical derivative, does this encourage over-smoothing in rapidly changing sequences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i7XV8BvUiG", "forum": "k49lMF1ckB", "replyto": "k49lMF1ckB", "signatures": ["ICLR.cc/2026/Conference/Submission12646/Reviewer_o339"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12646/Reviewer_o339"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981421965, "cdate": 1761981421965, "tmdate": 1762923486703, "mdate": 1762923486703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MVGE, a novel method for estimating 3D geometry from monocular video sequences. The primary goal is to address the trade-off between per-frame geometric accuracy and long-range temporal consistency, a key challenge in this domain.The core contributions are threefold: \n1. Viewpoint-Invariant Geometry: A cross-frame geometric consistency loss ($\\mathcal{L}_{cross}$) that aligns points from different viewpoints into a common reference frame\n2. Appearance-Invariant Learning: A hierarchical temporal derivative loss ($\\mathcal{L}_{temp}$) that enforces consistency across exponential time scales (e.g., $\\delta=1, 2, 4, 8$ frames)\n3. Frequency-Modulated Positioning: A novel adaptation of Rotary Position Encoding (RoPE) with NTK-aware scaling and a \"train-time sequence stretching\" strategy"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Shared sequence-level scale/shift with per-cell affine fits strikes a pragmatic balance between geometric fidelity and global consistency.\n2. Standard Transformers struggle to generalize beyond trained sequence lengths. MVGE addresses this with an NTK-guided, frequency-modulated rotary positional encoding and training-time simulation over exponential time scales. This design lets its temporal attention extrapolate to sequences orders of magnitude longer than seen in training while maintaining stable attention kernels.\n3. Method is decomposed into geometric, temporal, and positional-encoding components with equations and a helpful figure."}, "weaknesses": {"value": "1. (Minor) Outputs are affine/scale-invariant; limits plug-and-play use in metric 3D tasks.\n2. Long-range claim may be overstated. VGGT also handles ~300 frames in one pass as well. There are alredy works[1] that could process more than 3000 frames with the same memory constraint. \n3. Asymmetric comparison. MVGE uses external poses; VGGT predicts poses, making the fairness of comparisons unclear.\n\n\n[1]Deng, Kai, et al. \"VGGT-Long: Chunk it, Loop it, Align it--Pushing VGGT's Limits on Kilometer-scale Long RGB Sequences.\" arXiv preprint arXiv:2507.16443 (2025)."}, "questions": {"value": "1. Metric scale: What prevents direct metric prediction? Would removing per-frame normalization and training with metric supervision enable it?\n2. It would be nice to have an ablation of performance vs frame number.\n3. Pose parity study: With identical external poses provided, how do MVGE, VGGT, and DUSt3R compare on geometry alone?\n4. Temporal geometry consistency is effective in static scenes. Could the authors elaborate on how the method handles moving objects? The supplementary material also seems to demonstrate strong performance on this case."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YNtflqPFtm", "forum": "k49lMF1ckB", "replyto": "k49lMF1ckB", "signatures": ["ICLR.cc/2026/Conference/Submission12646/Reviewer_9jsx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12646/Reviewer_9jsx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762467069170, "cdate": 1762467069170, "tmdate": 1762923485764, "mdate": 1762923485764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}