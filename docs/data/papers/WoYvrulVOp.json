{"id": "WoYvrulVOp", "number": 2969, "cdate": 1757309187248, "mdate": 1759898116721, "content": {"title": "Reference-Free Rating of LLM Responses via Latent Information", "abstract": "How reliable are single-response LLM-as-a-judge ratings without references, and can we obtain fine-grained, deterministic scores in this setting? We study the common practice of asking a judge model to assign Likert-scale scores to free-text responses and show two systematic issues: scores are unstable under sampling and poorly calibrated, leading to compression near the top of the scale and frequent ties. We then evaluate Latent Judges, which derive scalar ratings from internal model signals: (i) probability-weighted scores over integer ratings, (ii) verifier-style probabilities of yes, and (iii) linear probes trained on hidden activations at the rating position. Across a broad suite of pairwise and single-rating benchmarks, latent methods match or surpass standard prompting, with consistent gains on pairwise accuracy and listwise ranking relevant to Best-of-N selection. Probability-weighted scores achieve the strongest single-rating correlations, while probes recover useful signals when output logits are miscalibrated. These results indicate that latent information provides deterministic and more discriminative signals for reference-free evaluation, and can improve selection and training approaches like Best-of-N, multi-teacher distillation, and routing.", "tldr": "", "keywords": ["large language model", "llm", "llm-as-a-judge"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a5001f82d29138b379412e8ec72dbd5fe86249ea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper challenges the reliability of the common practice of using LLMs to assign single-response, reference-free Likert-scale scores to text. The authors identify two primary weaknesses with this approach: **instability** (scores vary with stochastic decoding) and **poor calibration** (scores are compressed near the top of the scale, leading to frequent ties and low discriminability). To address these issues, this paper proposes and evaluates **Latent Judges**, a set of three methods that derive deterministic, fine-grained scores from internal model signals rather than generated tokens.\n\nThe authors present an evaluation spanning a wide range of both pairwise and single-rating benchmarks. Their results show that latent approaches, especially probability-weighted and verifier-style methods, consistently match or outperform standard prompting baselines on pairwise accuracy. While results on single-rating correlation tasks are more mixed, probability-weighted scores remain strong. Overall, the paper shows that leveraging latent information yields more stable and discriminative signals for reference-free evaluation, which could benefit applications such as Best-of-N selection and model routing."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Originality: While probing and using logits have been explored before, as I know, this work is the first to systematically identify the instability and calibration issues of single-response LLM judging and then propose and validate a suite of \"Latent Judge\" methods as a direct solution. This reframing of the problem is a novel conceptual contribution.\n* Quality: The paper demonstrates good quality through its methodologically rigorous and extensive empirical evaluation. The problem is diagnosed systematically and quantified with clear metrics across a large dataset. The evaluation of the proposed solutions is conducted across 10 pairwise and 5 single-rating benchmarks, providing robust and comprehensive evidence for the claims.\n* Clarity: The paper's clarity is a significant strength. The core problem is visualized in Figures 1 and 2, which clearly show the inconsistency and score compression. The proposed methods are explained concisely. The extensive results are well presented in organized tables for direct comparison.\n* Significance: The work is of considerable significance to the MLOps and evaluation communities. It addresses a practical and increasingly common failure mode in LLM evaluation. By providing a set of solutions—some of which are training-free and easy to implement (like probability-weighted scores)—the paper offers an immediate and actionable path for practitioners to improve the reliability and discriminative power of their evaluation pipelines for tasks like Best-of-N selection, routing, and multi-teacher distillation."}, "weaknesses": {"value": "* Limited novelty: The core ideas of using next-token probabilities or training probes on internal model states have been explored in other contexts (especially for explainability). The main novelty lies in the application and systematic study, so work feels more incremental than paradigm-shifting.\n* Degraded performance on single-rating benchmarks: While the latent methods excel at pairwise comparisons, their advantage is less clear on single-rating correlation tasks. The paper acknowledges this but could benefit from a deeper analysis of why this discrepancy exists.\n* Effectiveness of probes is data-dependent: The success of the latent probe method is shown to be dependent on the training data source (Tülu-trained probes outperform model-generated-data probes). This highlights a dependency that complicates its application as a truly general, \"reference-free\", \"off-the-shelf\" solution."}, "questions": {"value": "* Your results show that fine-tuned judges like Prometheus can fail entirely on alternative prompting formats (verifier and probability-weighted). This suggests a form of \"prompt overfitting.\" Do you believe this is a fundamental limitation of fine-tuning for judge models, or could it be addressed with a more diverse fine-tuning data mixture that includes different evaluation formats?\n* The performance of verifier-style ratings is strong in pairwise tasks but weaker in single-rating correlation tasks. Your analysis suggests this is due to scores concentrating near 0 or 1. Could this be mitigated by temperature scaling or other calibration methods applied to the yes/no logits before calculating the probability?\n* For the latent probes, you train on the residual stream activations. Did you experiment with probing activations from different layers? Is there a reason to believe the final layer is the most informative for this task, or could earlier layers capture different or more robust signals of quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RZhggtkCj7", "forum": "WoYvrulVOp", "replyto": "WoYvrulVOp", "signatures": ["ICLR.cc/2026/Conference/Submission2969/Reviewer_XxtF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2969/Reviewer_XxtF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761286555410, "cdate": 1761286555410, "tmdate": 1762916469886, "mdate": 1762916469886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes key reliability gaps in existing LLM-as-a-Judge frameworks (how to produce consistent and calibrated responses for free-text responses) and proposes solutions guided by internal model signals (e.g., Probability-weighted ratings, Verifier-style probabilities, and Latent probes). Across a suite of pairwise and single-rating benchmarks, their proposed methods that rely on internal model signals outperform or match standard prompting in aligning with human-annotators (or stronger LLM judges). Results suggest that latent internal signals may be more stable and calibration-friendly for preference-based evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tWell-motivated and methodologically sound. The paper addresses a highly relevant problem—instability and calibration collapse in LLM-based evaluation—and proposes solutions (probability-weighted ratings and latent probes) that are both rigorous and intuitively grounded.\n2.\tComprehensive benchmarking. The study systematically evaluates across ten preference datasets, covering diverse judge models and scoring regimes.\n3.\tInsightful empirical findings. The result that fine-tuned judgment models do not necessarily outperform general-purpose LLMs is particularly interesting and challenges common assumptions about specialized post-training."}, "weaknesses": {"value": "1.\tNovelty. While the latent probing approach is interesting, the “verifier-style” probability concept has been explored previously in domains like math/code. The paper should better articulate what is new or generalizable about extending this idea to preference judgment.\n2.\tDistinguishing LLM-as-a-Judge vs. Reward Models. The motivation would benefit from a clearer conceptual separation between these paradigms. For example, Best-of-N selection pipelines may use trained reward models rather than LLM judges. Including a SOTA reward model (e.g., top of RewardBench) (or justification for the choice of already-included models that are fine-tuned for judging) in the determinism and calibration analysis could reveal whether reward-model scores are inherently more deterministic.\n3.\tComparative baseline. Table 1 currently benchmarks against GPT-4 judgments but not against post-trained reward models. Comparing agreement with a strong, fine-tuned reward model would add to the evaluation.\n4.    Sensitivity to Temperature. The determinism experiment should test multiple temperature settings to confirm that observed variance is not an artifact of decoding randomness.\n5.\tExplanation of why instability occurs. The paper attributes inconsistency and poor calibration to discrete categories and stochastic decoding but does not elaborate. A dedicated section explaining the intuition behind these mechanisms would round out the storyline.\n6.\tPotential label noise. It would be interesting to replicate Table 4 using only clearly separable examples (e.g., where human preference strongly agrees across annotators) to test robustness (Frequent ties might indicate noise or low-signal regions in human-labeled preference data.)\n7.\tPlease add confidence intervals for the mean estimates in Table 4.\n8.\tGeneralizability of Latent Probes. The finding that probes trained on Tülu responses perform best raises concerns about domain overfitting. Further analysis of cross-domain or cross-model transferability would strengthen claims of general utility."}, "questions": {"value": "If a probe trained on one judge is applied to another, how does performance degrade? This would test whether latent representations capture universal judgment signals."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c6dVvnlW85", "forum": "WoYvrulVOp", "replyto": "WoYvrulVOp", "signatures": ["ICLR.cc/2026/Conference/Submission2969/Reviewer_bhBs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2969/Reviewer_bhBs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761323664879, "cdate": 1761323664879, "tmdate": 1762916466308, "mdate": 1762916466308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses single-response, reference-free evaluation in LLM-as-a-Judge settings, identifying two systematic issues: (1) sampling instability, and (2) poor calibration leading to score compression and ties. To address these, the authors propose Latent Judges deriving ratings from internal model signals: (1) probability-weighted scores via next-token distributions, (2) verifier-style binary probabilities, and (3) latent probes using linear classifiers on hidden activations. Evaluations across more than 10 benchmarks show these approaches match or surpass standard prompting, particularly for pairwise accuracy and listwise ranking in Best-of-N selection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-Motivated Problem\nThe paper clearly articulates why single-response, reference-free evaluation matters for practical applications (Best-of-N, GRPO, multi-teacher distillation, routing). The systematic analysis in Section 3.1 convincingly demonstrates the limitations of current approaches:\nEven the best models achieve only 70-80% agreement with mode across 10 samples (Figure 1)\nSevere score compression near the top of the scale (Figure 2)\nLarge gaps between strict and lenient agreement metrics (Table 1)\n\n2. Comprehensive Experimental Evaluation\nThe experimental design is thorough:\n12 judge models including specialized judges (Prometheus, Selene)\n10+ diverse benchmarks covering both pairwise and single-rating settings\nPractical applications (listwise ranking, LLM routing)\nMultiple prompt templates and probe architectures tested\n\n3. Diversity of methodology\nThe three proposed approaches are complementary and theoretically grounded:\nProbability-weighted ratings directly leverage model uncertainty\nVerifier-style extends successful approaches from verifiable domains\nLatent probes can recover signals when output logits are miscalibrated\nThe connection to KTO loss (Appendix D) is elegant and adds theoretical depth\n\n4. Empirical Gains\nResults show consistent improvements:\nPairwise benchmarks: Up to 5 percentage points improvement in average accuracy (Table 4)\nListwise ranking: Superiority in Spearman correlation with GPT-5 rankings (Table 6)"}, "weaknesses": {"value": "1. Training Instability - Acknowledged but Not Solved\nIn Section 4.4, lines 369-375, the authors acknowledge the problem: \"we notice considerable variation over different training runs.\" This is serious - it means training the probe multiple times with the same data and settings produces different performance each time.\nBut the paper doesn't quantify this problem. What does \"considerable\" mean? Is the standard deviation 0.02 or 0.10? What's the gap between best and worst performance - 5% or 20%? Did they train 10 times or 100 times? None of this basic information is provided.\nImagine a practical scenario: I'm deploying a probe at my company. The first training run achieves 75% performance. Looks good. But my colleague says \"maybe you got lucky, try again.\" The second run gives 70%. \"Hmm, which is real?\" The third gives 78%. \n\n\n2. Data Requirements - Hidden costs and circular logic\nProbes require labeled preference data. The paper presents two approaches: using existing datasets like Tulu, or generating data with strong/weak model pairs. There's also circular logic here. We're trying to build a good evaluator, but to do so we already need a good evaluator (the strong model).\n\n\n3. Single-Rating Performance: Severe Degradation Compared to Alternatives\nThe paper's title is \"Reference-Free Rating of LLM Responses.\" \"Rating\" - evaluating individual responses - is the core task, yet probes perform worst at this. They're okay for pairwise comparison but worst for single rating.\n\n\n4. Cost-Benefit and Risks in Practical Implications"}, "questions": {"value": "1. Training Instability - Acknowledged but Not Solved\nHow many times should a researcher or user train? Can I pick the best run? Or should I average them? The paper provides no guidance for these practical questions. Moreover, there's no proposed solution. The paper only says \"more distinct training data helps mitigate this,\" but doesn't specify how much it helps or exactly how to prepare such data.\n\n2. Data Requirements - Hidden costs and circular logic\nFor labelling preference data,\nThe first approach : How much hidden cost we need?\nThe second approach : We need to choose strong and weak models, but how different should they be? The paper only says \"more clearly separated models make training more stable.\" Do you need a huge gap like GPT-4 vs Llama-3B? Would GPT-4 vs Llama-70B suffice? There are no concrete criteria. If we're using GPT-4 to generate training data, why not just use GPT-4 as the evaluator? Why bother training a probe? \n\n3. Single-Rating Performance: Severe Degradation Compared to Alternatives\n3-1. In Table 5, for single-rating benchmarks, probes achieve Pearson correlations averaging 0.40-0.48. This is much lower than probability-weighted methods at 0.59-0.64 - almost a 20% difference. The authors explain why in lines 289-293: the BCE (Binary Cross-Entropy) objective function \"squashes outputs toward extremes.\" This means probes tend to output values close to 0 or 1, and struggle to predict intermediate values.\n\nThis is exactly the same problem as the verifier-style method. The verifier also has P(yes) concentrated at 0 or 1, reducing discriminative power. If probes suffer from the same extreme value bias as verifier-style methods, what advantage do they offer to justify the additional training cost?\n\n3-2. \"Rating\" - evaluating individual responses - is the core task, yet probes perform worst at this. They're okay for pairwise comparison but worst for single rating. Why must we use BCE? Could we check other loss functions like MSE (Mean Squared Error)? What about focal loss or other variants? \n\n4. Missing Cost-Benefit Analysis\nUsing probes requires: Preparing training data (time, cost), Multiple training attempts, Selecting the best probe,\nExtracting hidden states during inference. Is all this cost worthwhile? How could we measure Cost-Benefit ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YcgFkGC7X0", "forum": "WoYvrulVOp", "replyto": "WoYvrulVOp", "signatures": ["ICLR.cc/2026/Conference/Submission2969/Reviewer_9WLH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2969/Reviewer_9WLH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883735465, "cdate": 1761883735465, "tmdate": 1762916465884, "mdate": 1762916465884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of using an LLM as a judge to rate LLM responses in the non-verifiable setting, typically the text format responses are rated by the judge LLM on the scale of 1-5 or 1-10. These ratings may have large variability under sampling, poor calibration and frequent ties leading to suboptimal input for downstream tasks. The authors evaluate 12 judge-LLMs on a prompt + chosen/rejected response dataset to show that by and large the LLM judges are inconsistent in their outputs under sampling, and poorly calibrated with the chosen responses.  \nTo mitigate this, the authors propose three techniques to derive response scores from latent knowledge: probability-weighted ratings, verifier-style ratings and latent probing. The first method gives a deterministic score which is the probability weighted rating, the second one is the case when only yes/no answers are elicited and the rating is the probability of yes, while the last method trains a linear regressor over the activations at the next token to predict a quality score.  \nThe authors conduct experiments using several pairwise response preference and single rating benchmarks, with GPT-4 and human rates as the reference, for 6 judge LLM models, evaluating the proposed rating methods against the 10-scale rating as the baseline. The experiments empirically show some performance improvements."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The methods are novel and aim to address important challenges faced in using LLM as judge, which is required for several important downstream applications."}, "weaknesses": {"value": "1. The experiments presented in the paper only show modest accuracy improvements over the 10-point baseline. Further, no one method seems to be consistently the best across the judge-LLMs or the benchmarks. \n2. The experiments only measure the quality metrics against reference human or GPT scores. However, the methods are not evaluated on their impact on the performance of downstream tasks like ranking, distillation or RL policy optimization.\n3. The paper does not provide much analytical insights on why the proposed techniques would improve performance."}, "questions": {"value": "1. The experiments are mostly on smaller LLMs as judges .. can the authors discuss whether/why they expect their techniques to apply to larger LLMs?\n2. How is the temperature tuned for the probability-weighted rating and verifier-style rating methods?\n3. What is layer $l$ on line 214?\n4. Why is the variance of latent probe not included in Fig. 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Rb02myOrrt", "forum": "WoYvrulVOp", "replyto": "WoYvrulVOp", "signatures": ["ICLR.cc/2026/Conference/Submission2969/Reviewer_urQp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2969/Reviewer_urQp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762344925961, "cdate": 1762344925961, "tmdate": 1762916465245, "mdate": 1762916465245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}