{"id": "rz6V1RI3rH", "number": 13344, "cdate": 1758216792462, "mdate": 1759897443553, "content": {"title": "Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models", "abstract": "Instruction-following is essential for aligning large language models (LLMs) with user intent.  While recent reasoning-oriented models exhibit impressive performance on complex mathematical problems, their ability to adhere to natural language instructions remains underexplored.  In this work, we introduce MathIF, a dedicated benchmark for evaluating instruction-following in mathematical reasoning tasks.  Our empirical analysis reveals a consistent tension between scaling up reasoning capacity and maintaining controllability, as models that reason more effectively often struggle to comply with user directives. We find that models tuned on distilled long chains-of-thought or trained with reasoning-oriented reinforcement learning often degrade in instruction adherence, especially when generation length increases.  Furthermore, we show that even simple interventions can partially recover obedience, though at the cost of reasoning performance. These findings highlight a fundamental tension in current LLM training paradigms and motivate the need for more instruction-aware reasoning models.", "tldr": "", "keywords": ["large language models", "reasoning", "instruction following"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/425e5dd399d3358bd38707e78a2cf12b8b63270e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper examines the pressure between reasoning ability and controllability in Large Reasoning Models. The authors introduce MathIF, a benchmark designed to assess instruction-following in mathematical reasoning contexts. Testing 23 models across scales, they find that improvements in reasoning achieved through methods such as extended CoT supervision or reinforcement learning often lead to poorer adherence to user constraints. Longer CoTs and reasoning-oriented training degrade instruction compliance, while enforcing brevity or re-emphasising constraints can restore obedience at the expense of reasoning accuracy. The study highlights an intrinsic trade-off between intelligence and obedience, underscoring the need for training paradigms that balance high reasoning competence with reliable controllability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- novel evaluation and benchmark: MathIF is a nice systematic and domain-specific framework for measuring instruction adherence in mathematical reasoning tasks.\n\n- good evaluation: 23 LRMs, offering a robust empirical foundation across model sizes and architectures.\n\n- useful study: It identifies and quantifies the reasoning adherence trade-off, demonstrating that scaling reasoning capabilities often undermines instruction-following."}, "weaknesses": {"value": "- Domain limitation: MathIF is confined to the mathematical domain; results may not generalise to broader reasoning contexts, for instance, commonsense or multimodal reasoning.\n\n- limited training scope: The study primarily evaluates models trained via GRPO-based RL, limiting insights into other training paradigms.\n\n- mitigation strategies: The interventions (repeating constraints) offer partial and ad hoc solutions rather than principled training methods.\n\n- interpretability analysis missing: The work identifies the trade-off empirically but does not deeply explore cognitive or architectural causes behind the loss of control."}, "questions": {"value": "1. Beyond empirically confirming a known trade-off, what new theoretical understanding does your study provide about why reasoning scaling reduces controllability?\n\n2. Given that MathIF focuses exclusively on mathematical reasoning, how can you support claims of generalisability to other reasoning domains or real-world alignment scenarios?\n\n3. How can you be sure that reasoning-oriented training, rather than factors like data or decoding strategy, causes the loss of instruction adherence? Have you tested this through controlled ablations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "61ygrQQPvH", "forum": "rz6V1RI3rH", "replyto": "rz6V1RI3rH", "signatures": ["ICLR.cc/2026/Conference/Submission13344/Reviewer_JtxE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13344/Reviewer_JtxE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761397825975, "cdate": 1761397825975, "tmdate": 1762923998964, "mdate": 1762923998964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MathIF, an instruction-following (IF) evaluation benchmark for reasoning models that isolates IF performance from domain mismatch. Authors show that as models scale reasoning capacity, their obedience to instructions degrades. (When CoT length gets longer, this tendency gets stronger.) As a result, they argue an intelligence-obedience trade-off, which gains in reasoning come at the cost of controllability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is easy to follow, and authors provide comprehensive experiments and analyses to support their claim.  \n2. MathIF is the first IF benchmark particularly targeted for reasoning models that isolate IF ability from domain mismatch. The setups are well-curated and sound.  \n3. Authors use 25 reasoning models, which provide sound experimental supports, and further analysis in Section 5 reconfirms the limitation in training methods for reasoning models."}, "weaknesses": {"value": "1. My biggest concern is the lack of novelty. The underperformance of instruction following of reasoning models is a well-known phenomenon in the community, and we could already observe this phenomenon using the existing benchmarks as authors provide at the first paragraph of Section 3. Leaving that aside, MathIF is an IF benchmark for math domain, where most of the settings follow the existing benchmarks. If authors want to truly isolate domain mismatch, they should conduct experiments on other domains (e.g., coding, logic, etc.). Providing single evidence in math domain and generalize into all domains feels like an over-generalization.  \n2. Is IF for reasoning models an important question to ask? (Can't we simply re-format the output of the reasoning models to better IF?) I (partially) agree with this, but if we assume it is important, the natural research question could be: how can we improve IF ability of reasoning models? . If this is possible, then the argued intelligence-obedience trade-off could be resolved (although authors conduct comprehensive experiments in Section 5).  \n3. In Section 1, I expect authors to emphasize the difference with the existing benchmark, which is isolating IF from domain mismatch. I was finally convinced while reading Section 3 first paragraph.\n\nI'm expecting authors to persuade me during the rebuttal period. If the justifications are reasonable and persuasive, then I'm happy to raise my score."}, "questions": {"value": "1. line 62-65: Why does longer CoT benefits reasoning performance but degrades instruction-following ability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U5QIPJmnVF", "forum": "rz6V1RI3rH", "replyto": "rz6V1RI3rH", "signatures": ["ICLR.cc/2026/Conference/Submission13344/Reviewer_MkMN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13344/Reviewer_MkMN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595517524, "cdate": 1761595517524, "tmdate": 1762923998661, "mdate": 1762923998661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Summary**: This paper analyzes how increasing a language model’s reasoning ability often leads to longer responses and reduced controllability.\n\nPrevious works that evaluate instruction-following abilities mostly focus on general-purpose tasks - simple instructions that do not require extended chains of thought. In contrast, this paper explores how reasoning-oriented training can negatively affect a model’s ability to follow user instructions.\n\nTo study this phenomenon, the authors design tasks with dual or triple constraints, ensuring that these constraints are non-contradictory through manual inspection. They introduce two evaluation metrics:\n\n- Hard Accuracy measures whether all constraints are satisfied.\n\n- Soft Accuracy measures whether a subset of the constraints is satisfied.\n\nOverall, the study highlights a trade-off between reasoning strength and instruction adherence, emphasizing the need for models that are both reasoning-capable and instruction-aware."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of this paper lies in its introduction of **MathIF**, a reasoning-oriented instruction-following dataset specifically designed for the mathematical domain. This benchmark extends existing instruction-following datasets by focusing on tasks that require complex reasoning chains rather than simple directive compliance. Moreover, it introduces meaningful evaluation criteria, such as hard and soft accuracy, that better capture how well models can balance reasoning performance with instruction adherence."}, "weaknesses": {"value": "### **1. Unclear and inconsistent reasoning - base model pairing**\nThe selection of base and reasoning-enhanced models is questionable. For example, DS-R1-distill-LLaMA is not a direct counterpart of Llama-3.3-70B-Instruct, as it is a distilled model derived from DeepSeek-R1. Other well-established base–reasoning model pairs such as Gemini–Gemini-1.5-Pro, Claude–Claude 3 Opus, or GPT-4o–GPT-4o-mini are ignored. This inconsistent model pairing undermines the validity of the comparisons and weakens the empirical foundation of the paper’s conclusions.\n\n### **2. Limited novelty of the proposed benchmark**\nThe decline in instruction-following performance for reasoning-oriented models has already been demonstrated in prior benchmarks such as IFEval and FollowBench. The proposed MathIF dataset is thus largely an **incremental extension** rather than a fundamentally new contribution. It reuses the same conceptual framing - testing adherence to surface-level instructions, introducing deeper methodological or theoretical advances.\n\n### **3. Misalignment between claimed goal and actual design**\nAlthough the paper claims to focus on mathematical reasoning, the instructions tested are **primarily lexical or formatting constraints**, such as token-length limits, structured responses (e.g., bullet points, affixes), and multilingual formatting requirements. These factors are largely irrelevant to mathematical reasoning itself and do not alter or probe the underlying reasoning process. As a result, the benchmark measures surface obedience rather than true instruction-following in reasoning contexts.\n\n### **4. Overstated claim of a “reasoning–control trade-off”**\nThe authors assert that there exists a fundamental trade-off between instruction-following and mathematical reasoning ability. However, their evidence (Table 2) is based on experiments where reasoning models are prompted with artificial constraints—such as token-length caps or specific output structures—that directly interfere with the reasoning process. The observed performance drop may therefore result from prompt-induced noise or confusion, not from an intrinsic trade-off.\n\nTo rigorously support their claim, the authors should conduct controlled experiments that isolate reasoning ability from instruction adherence, holding model size, training data, and training objectives constant. Measuring how reasoning and instruction-following evolve across training steps under fixed conditions would provide a more reliable causal interpretation. As it stands, the presented results support only a correlational observation, not a causal relationship."}, "questions": {"value": "The paper is clearly written, so I do not have any questions regarding the details of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5ce4aiYrUI", "forum": "rz6V1RI3rH", "replyto": "rz6V1RI3rH", "signatures": ["ICLR.cc/2026/Conference/Submission13344/Reviewer_xNg7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13344/Reviewer_xNg7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707550142, "cdate": 1761707550142, "tmdate": 1762923998315, "mdate": 1762923998315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates a persistent reasoning-control trade-off in large reasoning models (LRMs). The authors introduce MathIF, a math-domain benchmark with 15 Python-verifiable constraints (namely, length, lexical, format, affix) composed into instruction sets and embedded in problems from GSM8K, MATH-500, Minerva, Olympiad, and AIME. Evaluating LRMs, they find: (i) instruction-following is low overall wrt to the target feature sets, (ii) controllability worsens with problem difficulty and constraint complexity, and (iii) model size doesn’t predict obedience. Training that boosts reasoning (SFT on long CoTs, RL variants, longer rollouts), do often erode compliance, while inference-time fixes that ‘bring constraints closer’ (e.g. repeating constraints near the answer) improve obedience but reduce correctness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well designed testbed, combining different inference task types with deterministic control features.\n\n- Comprehensive and systematic empirical analysis and interpretation.\n\n- Clear empirical signal across different base models and tasks.\n\n- Target properties of practical relevance."}, "weaknesses": {"value": "- Provides further systematic corroboratory evidence to a previously known phenomena, but does not bring light to the deeper mechanisms."}, "questions": {"value": "- Do MathIF’s constraint categories capture the instruction types real users care about (beyond format/lexical)? Any plans for pragmatic/semantic constraints with objective scoring?\n\n- Can you isolate whether obedience loss is due to attention drift, prompt interference, or decoding heuristics (e.g., length bias)? Any controlled reordering/position ablations?\n\n- Under equal token budgets and matched prompts, how do instruction-tuned models fare versus reasoning-RL models on MathIF? \n\n- How sensitive are results to prompt style, delimiter choices, or different verifiers?\n\n- ‘Intelligence’ feels like an anthropomorphic impulse here. Would stick to CoT reasoning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tWbKI9pLzy", "forum": "rz6V1RI3rH", "replyto": "rz6V1RI3rH", "signatures": ["ICLR.cc/2026/Conference/Submission13344/Reviewer_6nnw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13344/Reviewer_6nnw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762187921139, "cdate": 1762187921139, "tmdate": 1762923998047, "mdate": 1762923998047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}