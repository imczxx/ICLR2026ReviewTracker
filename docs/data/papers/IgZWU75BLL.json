{"id": "IgZWU75BLL", "number": 25208, "cdate": 1758365328501, "mdate": 1759896729890, "content": {"title": "SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning", "abstract": "Continual learning, one's ability to adapt to a sequence of tasks without forgetting previously acquired knowledge, remains a major challenge in machine learning and a key gap between artificial and human intelligence. While regularisation and replay perform well in vision, they lag behind multi-task learning for large language models (LLMs), especially at scale with many tasks. We revisit replay and argue that two failure modes drive this gap: selection (what to rehearse) and integration (how to consolidate new knowledge). To address selection, we propose Surprise-prioritised Replay (SuRe), a simple, architecture-agnostic rule that ranks and stores the most surprising (high Negative Log-Likelihood) sequences. SuRe alone achieves state-of-the-art results on both the Standard CL and the Large Number of Tasks (LNT) benchmarks. To address integration, we add a dual-learner design with fast and slow LoRA adapters merged via an exponential moving average (EMA), enabling rapid adaptation while stabilising long-term knowledge. Combining SuRe with the dual learner yields further gains, including improvements of up to +5 accuracy points on LNT over prior SOTA. Ablation studies confirm that our proposed method remains robust under reduced replay frequency and small buffer size, demonstrating both effectiveness and sample efficiency. Taken together, our results establish replay as a strong baseline for LLM continual learning and demonstrate that surprise-based selection and slow-weight consolidation are complementary components for mitigating catastrophic forgetting.", "tldr": "", "keywords": ["continual learning", "large language models", "replay", "surprise"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/443585eb9cb3b989527ce9bec62ed513904af7bb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SURE: surprise-driven prioritised replay for continual LLM learning. The main ideas of the paper are a surprise-based replay strategy and LoRA with a slow-fast learning mechanism. The consolidated methods are evaluated by standard CL and large number of task benchmarks.  The paper shows its strengths in several aspects, but also has several weaknesses that need to be addressed. Please see strengths and weaknesses."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "(1). The has clear motivation and ideation.\n\n(2). The numerical result shows a promising performance on a large number of tasks benchmark.\n\n(3). The paper presents a comprehensive theoretical analysis.\n\n(4). The paper presents a rigorous numerical analysis."}, "weaknesses": {"value": "(1). Methodology: The slow-fast learning mechanism is arguably not a novel idea, since it has been proposed in a few existing CL methods, e.g., DualNet, and Slow-fast Prompt. Second, while surprise replay offers an innovative idea to select the buffer, it lacks of theoretical foundation and the details on how it works.\n\n(2) Learning mechanism: Figure 1 shows that slow and fast learners are updated on two different phases, but the pseudo-code shows that both learners are trained in the same steps.\n\n(3). Performance: The proposed method achieves only a small (or negative) margin for some cases in the standard CL benchmark. It is questionable that the proposed method significantly outperforms the existing methods.\n\n(4). Forgetting: I do not see the measurements and analysis of models' forgetting as the answer to the catastrophic forgetting problem.\n\n(5). Theoretical Analysis: While it shows the boundary for the slow learner, Lemma 2 does not show the better handling of slow-fast learners in comparison to a single learner.\n\n(6). Performance on budget memory: I appreciate the measurement of the proposed method's performance on different memory sizes. However, it is expected to be compared to the existing methods in those memory budgets. \n\n(7). Only one of the competitor methods is up-to-date. It should include more latest methods for comparison.  \n\n\nReferences:\n\n[1]. Dualnet: Continual learning, fast and slow\n\n[2]. Brain-inspired fast-and slow-update prompt tuning for few-shot class-incremental learning. (Slow-fast prompt)"}, "questions": {"value": "Please address the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d7UBteo9TM", "forum": "IgZWU75BLL", "replyto": "IgZWU75BLL", "signatures": ["ICLR.cc/2026/Conference/Submission25208/Reviewer_n5Un"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25208/Reviewer_n5Un"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962243042, "cdate": 1761962243042, "tmdate": 1762943365133, "mdate": 1762943365133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers the problem of continual learning on text classification tasks. It focuses on buffer-based replay methods. It derives an upper-bound on the forgetting incurred by such methods, which includes two complimentary terms: 1) how well the samples from the buffer approximate the model's loss on past data 2) a term which captures how “well” new samples are consolidated. The paper then proposes to reduce the forgetting upper-bound by 1) storing past samples with high loss inside the buffer 2) combining slow and fast changing weights during training.\nThe paper outperforms other CL baselines on text classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I identified the following strengths of this paper:\n\n- Theoretically, it provides an upper-bound on the forgetting experienced by a buffer-based replay method. I think the way the quality of the buffer is defined (D_{F_loc}(P_{1:T}, q)) is novel and might be interesting for others. The selection term and consolidation term being complementary is an important contribution. This was later backed up by experimental results.\n\n- Experimentally, it provides evidence that: 1) Replay-based with reservoir sampling can outperform regularization-based CL methods. 2) Surprised-based replay methods can outperform reservoir sampling methods.\n\n- The limitations section is comprehensive."}, "weaknesses": {"value": "Aside from the theoretical contribution, the rest of the methodology section has limited novelty - it appears to combine two already established ideas. Moreover, the text never makes it clear (from what I could see) how each component reduces the terms in the upper-bound.\n\nReadability: I don’t think that the integration (consolidation) term in Eq. 3 is well explained. Reading the main text, I do not have a good idea of what $B(\\psi)$ is, apart from it being a “mechanism-specific factor”.\n\nThe paper claims to be applicable to large language models, which at least to me suggests the task of language modelling, while it is evaluated on sequences of text classification tasks. Therefore, claims such as “our results establish replay as a strong baseline for LLM continual learning” seem too general to me, and unsubstantiated by the experiments.\n\nThe paper would benefit from a clear “contributions” statement in the introduction."}, "questions": {"value": "How does your method of storing high-surprise samples in the buffer relate to importance sampling (instead of uniform sampling) of past data?\n\nYour derivation and method seems to be general for any continual-learning setup - including both dataset and architecture. Is there a reason it would best perform on large language models? (Perhaps, relying on the local neighbourhood containing the optimal solution?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MT4TNjsP6H", "forum": "IgZWU75BLL", "replyto": "IgZWU75BLL", "signatures": ["ICLR.cc/2026/Conference/Submission25208/Reviewer_NcZB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25208/Reviewer_NcZB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973685122, "cdate": 1761973685122, "tmdate": 1762943364290, "mdate": 1762943364290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the continual learning for large language models (LLMs) with replay. The authors first define forgetting as the sum of a selection mismatch and the knowledge consolidation variance. To address this, the authors proposed a surprise-based sampling strategy to populate the replay buffer. Moreover, the authors proposed a dual-learner framework to deal with long-term and short-term learning. Experiments showed the improved performance of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well motivated\n\nIt is an interesting idea to select surprising samples for replay.\n\n2. The method is straightforward\n\n3. The decomposition of forgetting is interesting"}, "weaknesses": {"value": "1. The surprise measure might not be reliable\n\n2. The idea of dual-learner is not novel, and the implementation seems confusing\n\n3. The comparison is not sufficient and up-to-date\n\nPlease see details in the Question section."}, "questions": {"value": "1. The surprise measure might not be reliable\n\nAccording to Equation 9, the authors used the sum of native log-likelihood over the entire sequence. There might be several issues with this measure. (1) Taking the sum of the entire sequence might dilute the actual signal, since LLM might have a long answer, but what matters would just be a few words. Although the authors claimed that the full-sequence measure is better than the label-level measure in lines 363-369, there are only some hypotheses to explain this without factual evidence to support them. Moreover, could the authors elaborate on the average length of the generated sequence? (2) This surprise measure is not able to detect hallucination, as the model might just be confidently generating hallucinations. \n\n2. The idea of dual-learner is not novel, and the implementation seems confusing\n\nFirst, the idea of slow and fast learners is not new. This idea has been explored in [a], and the EMA implementation of LoRA has been proposed in [b]. Second, the implementation is confusing to me. I don't see where the slow learner is being used in the learning process, nor in the Figure. 1 or Algorithm 1. I am wondering how this slow learner helps the model.\n\n[a] Pham, Quang, Chenghao Liu, and Steven Hoi. \"Dualnet: Continual learning, fast and slow.\" Advances in Neural Information Processing Systems 34 (2021): 16131-16144.\n\n[b] Gao, Qiankun, et al. \"A unified continual learning framework with general parameter-efficient tuning.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n3. The comparison is not sufficient and up-to-date\n\nIt is totally fine for the proposed method to focus on replay-based CL. However, the comparison in the experiment section should therefore prioritize replay-based methods. The current comparison contains too many replay-free methods, and only compares with one replay method [c] without citing this paper. I don't find this comparison fair and up to date to the recent advance of replay strategy in the CL community.\n\n[c] Rolnick, David, et al. \"Experience replay for continual learning.\" Advances in neural information processing systems 32 (2019)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8XbzBVvJFh", "forum": "IgZWU75BLL", "replyto": "IgZWU75BLL", "signatures": ["ICLR.cc/2026/Conference/Submission25208/Reviewer_uMAn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25208/Reviewer_uMAn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986016708, "cdate": 1761986016708, "tmdate": 1762943363881, "mdate": 1762943363881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper attributes catastrophic forgetting in continual LLM learning to replay selection and integration errors. It proposes to rectify these errors by selecting 'surprising' examples (characterized by high nll) to replay, and learning a 'slow' learner that is updated using EMA over the 'fast' learner (i.e., the one optimized directly over the incoming and buffered samples). The two learners are implemented using LoRA adapters. Empirical evaluations support the claims and provide appreciable improvements over baselines."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written, claims are intuitive, and theoretically and empirically validated.\n- To the best of my knowledge, the paper is the first to propose NLL for sample selection. Combining this with a slow learning strategy empirically shows significant improvements, as shown in Table 1.\n- SuRE is implemented using LoRA and is therefore architecture agnostic.\n- Evaluations and ablations are sufficient, and SuRE outperforms SOTA continual LLM learners on both benchmarks."}, "weaknesses": {"value": "- To my knowledge, there are no significant weaknesses."}, "questions": {"value": "- What is the value of $\\beta$ used for evaluation? Can the authors include an ablation to show its impact?\n- Can the reliance on high surprise cause the model to overfit to outliers (such as mislabeled samples) and inadvertently hurt the performance of the model? Perhaps buffering a combination of surprising samples and some randomly selected samples instead can help?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mdPUkjKI6F", "forum": "IgZWU75BLL", "replyto": "IgZWU75BLL", "signatures": ["ICLR.cc/2026/Conference/Submission25208/Reviewer_Bq5a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25208/Reviewer_Bq5a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097182423, "cdate": 1762097182423, "tmdate": 1762943363688, "mdate": 1762943363688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}