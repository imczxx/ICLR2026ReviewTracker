{"id": "sMIiRjEuLL", "number": 17761, "cdate": 1758280222254, "mdate": 1759897155584, "content": {"title": "TAME the BALROG: Task-Adaptative Modular Emergent framework for Game Agents", "abstract": "Interactive games have proven to be key benchmarks for advancing artificial intelligence, requiring capabilities like long-term planning, exploration, and adaptation to stochastic environments. While Large Language Models (LLMs) have achieved notable results across many domains, they struggle in complex gaming environments like those in the BALROG benchmark. The absence of adaptive frameworks that can dynamically configure themselves based on environmental characteristics, limits the progress of AI in games.  To this end, we introduce the Task-Adaptive Modular Emergence (TAME) framework, which employs genetic algorithms to evolve environment-specific structures from modular components, enabling significant performance improvements of LLMs across diverse domains. TAME discovers high-performing configurations by selecting between baseline and hierarchical structures, selectively incorporating specialised modules, and fine-tuning each component through systematic mutations. Evaluating TAME across the BALROG benchmark, we find that the emergent modular structures discovered by TAME significantly enhance LLM performance, raising average progression scores of Gemini 2.0-Flash from 27.15\\% to 34.77\\%. Moreover, these structures demonstrate transferability across models. Directly employing TAME discovered structures for Gemini-2.0-Flash to a population of Gemini-2.5-Pro, we achieve new state-of-art performance on BALROG. This transferability suggests that TAME identifies fundamental structural principles for game-playing agents that adapt their cognitive architecture to match task demands.", "tldr": "We introduce an emergent framework that adapts LLM agents to games and achieves state-of-art performance on the BALROG benchmark.", "keywords": ["open-endedness", "agents", "LLMs", "framework", "evolution", "games", "hierarchy"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2772baac2de33de7fb5fba6683b5c44932c36b67.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes TAME, a framework that uses a genetic algorithm to evolve modular architecture for LLM agents in gaming environments. Each agent (member) is equipped with varying number of activated modules and the population of agents evolves to keep the best ones. The experiment results on BALROG benchmark show that TAME outperform BALROG's original method on various tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Using genetic algorithms for dynamic configuration of agent architectures is interesting.\n2. The experiments show significant improvements of TAME's performances on various gaming tasks."}, "weaknesses": {"value": "1. The writing is hard to follow. Many terms are interchangeably used in different contexts (like \"modole\", \"framework\"). \n2. Lacking crucial implementation details for the genetic algorithm, such as the precise encoding of the genome, the specific mechanics of genetic operators (selection, crossover, mutation), and the calculation of diversity metrics.\n3. TAME framework resembles \"evolutionary hyperparameter optimization\", but it regards the number and composition of modules of an agent as part of the parameters. This core idea is just using more trails (agents with differnt genomes) to find the \"optimal hyperparameter setting\". It seems to be resource-heavy and difficult to apply in production."}, "questions": {"value": "1. How does this work's core idea differ from existing relevant work? The authors are suggested to provide detailed comparison and connection between this work and existing work in Section 2 (Related Work) to better position their novelty.\n2. What are the details of evolutionary algorithm?\n3. The authors are suggested to present the workflow of TAME framework more clearly in Figure 2. It would be better to show the initial population where each member is one agent with different modules, and how the population and parameters evolves in each generation. \n4. The authors are suggested to polish the writing to avoid ambiguity, particularly by distinguishing a single agent’s framework and modular architecture from TAME’s evolutionary framework. In many places, these two concepts are used interchangeably."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n59TzvtnMs", "forum": "sMIiRjEuLL", "replyto": "sMIiRjEuLL", "signatures": ["ICLR.cc/2026/Conference/Submission17761/Reviewer_dxNh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17761/Reviewer_dxNh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720198442, "cdate": 1761720198442, "tmdate": 1762927603703, "mdate": 1762927603703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a new evolutionary method for augmenting the capabilities of LLMs to play videogames in the BALROG benchmark. The authors define a set of high-level modules (e.g. long-term memory or explicit exploration) and use an evolutionary approach to select which modules are deployed (along with relevant hyperparameters and modifications to the prompt) for each game in the benchmark. The authors demonstrate that this approach significantly improves the performance of the Gemini 2.0-Flash model. In addition, the authors show that adapting the best genotype found for the Gemini 2.0-Flash model to other models results in zero-shot performance gains, including a new SOTA for the BALROG benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The core method of the paper (evolutionary optimization to determine which modules are most appropriate for a given game) seems both novel and reasonable. The paper is also quite thorough, with a variety of ablations and hyperparameters -- I feel that the reproducibility of the experiments is high. Barring the caveats described below, I think the impact of the paper could also be high."}, "weaknesses": {"value": "My primary concern with this paper is in the comparisons to baselines. The performance gains are indeed impressive, but at present it’s somewhat difficult to tell how much of the improvement is attributable to the evolutionary search procedure and how much is the result of the various modules simply causing the LLMs to “reason more” than the baseline prompts. The prompts in the original BALROG paper appear to be quite simple (i.e. just stating that the LLM is a player and enumerating the valid actions). I think the paper would benefit from an additional baseline which introduces more reasoning but perhaps without the fully decomposed module structure (or an explanation of why the original BALROG prompts act as a fair baseline).\n\nI also think that the clarity of the paper could also be improved. There are a few technical terms which are used but not introduced (e.g. “wheel selection” or options). I also found Table 1 confusing at first -- the “Full Pop. Score” column seems like it could be referring to the gains of the whole TAME population over the baseline LLM instead of the gain of the TAME + adaptation model over the TAME[full] model, since “full” is a somewhat overloaded term. I also think it’s more common to state the performance gain in terms of percentage points (i.e. 34.7 - 27.2 = 7.5%) as opposed to percent improvement (i.e. 34.7/27.2 ~= 1.28). Relatedly, it’s not clear if the “+12.18%” gain of TAME+adaptation over TAME[full] is a raw percentage point increase or another percent improvement measure and it would be good to clarify (perhaps by simply including the raw performance of the TAME[full] model).\n\nWhile these points affect my rating, I would be happy to increase my score if they are addressed."}, "questions": {"value": "- How much of the gain in performance over the BALROG baseline is attributable to more reasoning or longer prompts as opposed to the specific modules selected?\n- Line 240: what is “wheel selection”?\n- There are two different citations to Eureka -- (Ma et al. 2023) and (Ma et al. 2024)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xNR5BMiNQZ", "forum": "sMIiRjEuLL", "replyto": "sMIiRjEuLL", "signatures": ["ICLR.cc/2026/Conference/Submission17761/Reviewer_mETx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17761/Reviewer_mETx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842337384, "cdate": 1761842337384, "tmdate": 1762927603080, "mdate": 1762927603080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the poor performance of Large Language Models in complex, interactive gaming environments, such as those in the BALROG benchmark. The authors introduce the Task-Adaptive Modular Emergence framework, which employs a genetic algorithm to automatically discover effective, environment-specific agentic structures. TAME evolves a genome that specifies which human-designed modules to activate, their hyperparameters, and their prompts. The core contributions are: (1) the TAME framework itself; (2) a novel, efficient long-term memory system; (3) achieving SOTA performance on the BALROG benchmark by improving a baseline Gemini 2.0-Flash score from 27.15% to 34.77% ; and (4) demonstrating that these evolved structures are transferable, enabling a Gemini 2.5-Pro model to achieve a new SOTA score (47.65%)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and uses easy-to-understand language.\n\n2. The paper provides rich implementation details in the appendix, which is commendable and crucial for reproducibility.\n\n3. Rigorous ablation and analysis."}, "weaknesses": {"value": "1. Limited novelty: Evolutionary algorithms have already been applied in the field of agent optimization/search, for example, in AgentSquare[3] and EvoAgent[4].\n\n2. Poor scope/generalizability of the framework: The framework proposed in this paper is applied to the domain of interactive games. In contrast, related agent optimization/search works, such as Aflow[2] and MaAS[5], can be applied across multiple domains.\n\n3. Insufficient discussion of related work: The core ideas of this paper, including agent evolution, evolutionary algorithms, merge components, and mutation modules, are all highly related to works like ADAS[1], AFLOW[2], AgentSquare[3], and MaAS[5], yet the paper does not discuss them.\n\n4. Limited experiments: The paper is only evaluated on the BALROG benchmark. To my knowledge, other benchmarks for interactive games exist, such as Minecraft.\n\nReference\n\n[1]Zhang J, Xiang J, Yu Z, et al. Aflow: Automating agentic workflow generation[J]. arXiv preprint arXiv:2410.10762, 2024.\n\n[2]Hu S, Lu C, Clune J. Automated design of agentic systems[J]. arXiv preprint arXiv:2408.08435, 2024.\n\n[3]Shang Y, Li Y, Zhao K, et al. Agentsquare: Automatic llm agent search in modular design space[J]. arXiv preprint arXiv:2410.06153, 2024.\n\n[4]Yuan S, Song K, Chen J, et al. Evoagent: Towards automatic multi-agent generation via evolutionary algorithms[J]. arXiv preprint arXiv:2406.14228, 2024.\n\n[5]Zhang G, Niu L, Fang J, et al. Multi-agent architecture search via agentic supernet[J]. arXiv preprint arXiv:2502.04180, 2025."}, "questions": {"value": "1. Can experiments be conducted on other benchmarks?\n\n2. Can experiments be conducted on models other than the Gemini series (e.g., the GPT series, open-source models)?\n\n3. Can the authors provide a detailed explanation of the differences from related work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hy6Fux3hG1", "forum": "sMIiRjEuLL", "replyto": "sMIiRjEuLL", "signatures": ["ICLR.cc/2026/Conference/Submission17761/Reviewer_s3bn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17761/Reviewer_s3bn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983128706, "cdate": 1761983128706, "tmdate": 1762927602670, "mdate": 1762927602670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TAME (Task-Adaptive Modular Emergence), a framework based on genetic algorithms that evolves modular architectures for large language model (LLM) agents in interactive games. Each agent configuration comprises module combinations, hyperparameters and prompts, which are optimised through evolutionary selection and mutation. When evaluated on the BALROG benchmark, TAME was found to significantly improve LLM performance (e.g. Gemini-2.0-Flash: 27.16% to 34.78%), and demonstrate cross-model transferability. This study contributes a novel task-adaptive agent framework and an efficient long-term memory design, as well as providing empirical evidence that modular adaptation can enhance LLMs' reasoning and planning abilities in complex environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper introduces a modular emergence framework driven by genetic algorithms that automatically configures LLM agent architectures based on task environments. This innovative approach combines evolutionary search, modular agent design, and prompt optimisation, offering a novel way to extend LLM adaptation beyond prompt tuning or static scaffolds. \nS2. The paper presents a thorough analysis of experiments conducted on the BALROG benchmark, demonstrating consistent enhancements across various game environments and models. Demonstrating cross-model transferability, where evolved architectures generalise from Gemini-2.0-Flash to Gemini-2.5-Pro, supports the robustness and practical value of the approach.\nS3. The paper clearly articulates the design of each module and the evolutionary process, supported by well-structured figures and ablation studies. Its findings make a significant contribution to the emerging field of LLM-based agent architecture search by offering insights into how adaptive structural configurations can improve reasoning and planning in dynamic environments."}, "weaknesses": {"value": "W1. Limited conceptual novelty: Although the paper presents TAME as an emergent intelligence framework, its core mechanism essentially involves evolutionary search over prompt and module configurations without introducing any new fundamental learning principles. Therefore, the originality lies more in system integration than in theoretical advancement.\n\nW2. Dependence on hand-crafted modules and uncertain generalisation: TAME' s adaptive capacity is limited by a modular library designed by humans. Each component (e.g. memory, exploration and the amygdala) contains pre-defined functional assumptions. Consequently, its success may hinge on the designer's prior knowledge and understanding of the domain. While this approach is effective in game-based environments, it remains unclear whether such a manually curated framework can be applied to other games and other reasoning or planning domains beyond games.\n\nW3. Evaluation metrics focus narrowly on progression scores: The empirical evaluation primarily relies on game progression percentages as the fitness signal. While this metric captures task success, it does not capture reasoning quality, sample efficiency or adaptation dynamics. Consequently, it is unclear whether the observed performance gains reflect genuine improvement in reasoning or merely the exploitation of heuristic patterns."}, "questions": {"value": "Q1. The paper frequently refers to 'modular emergence'. How do the authors formally define or measure emergence in this context, besides the performance improvements discovered through genetic search?\nQ2. What is the approximate computing budget required for one evolutionary run on BALROG? Are there any efficiency-improving mechanisms, such as early stopping, surrogate evaluation or population pruning?\nQ3. Could the authors provide qualitative or behavioural analyses demonstrating why certain module combinations outperform others? \nQ4. What were the reasons for the genetic algorithm being selected over other architecture or prompt optimisation approaches?\nQ5. Will the authors release the full codebase? And the configuration files?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7gkKIcF1gL", "forum": "sMIiRjEuLL", "replyto": "sMIiRjEuLL", "signatures": ["ICLR.cc/2026/Conference/Submission17761/Reviewer_NH6Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17761/Reviewer_NH6Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762328021973, "cdate": 1762328021973, "tmdate": 1762927602314, "mdate": 1762927602314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **TAME (Task-Adaptive Modular Emergence)**, a method that uses a genetic/evolutionary search over a pre-designed module space (e.g., hierarchical planner, explorer, long-term memory, loop-detection) to discover an agent “genome” — a combination of modules, prompts, and hyperparameters — tuned for a given game environment. The approach is evaluated on the BALROG benchmark; the authors report improvements over a baseline LLM (e.g., Gemini-2.0-Flash) and claim a transferred genome produces SOTA results when applied to a stronger model (Gemini-2.5-Pro). The paper also provides ablations on the memory module (vs. Jarvis / A-MEM) and includes evolved genome JSONs in the appendix."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **The research is practically relevant**. Automating discovery of modular agent architectures for interactive game tasks is a worthwhile goal for agentic LLM research.\n2. **The engineering is reasonably complete**, proposing a concrete genome encoding (modules + hyperparams + prompts), evolutionary procedure.\n3. The paper **includes targeted ablations** (memory module comparisons) rather than only reporting final aggregate scores, indicating attempts to analyze component contributions."}, "weaknesses": {"value": "1. **Stability and statistical rigor are weak.** The evolutionary search uses very small budgets (ngen = 4, nchild = 5) and some evaluations use few episodes (e.g., NetHack/NLE evaluated with 5 episodes), making results noisy and potentially non-robust. The paper lacks multiple independent runs, confidence intervals, and statistical tests.\n2. **The claim of transferability is under-supported.** The transfer experiment that moves a genome from Gemini-2.0 to Gemini-2.5-Pro and claims SOTA lacks tests on models from different families; the result could be specific to closely related models rather than generally transferable. Full leaderboard context and uncertainty estimates are missing.\n3. **Missing budget-matched baselines.** The paper does not compare against budget-matched RL agents or other automated search methods (Bayesian Optimization, Population-Based Training, Evolution Strategies) under comparable compute budgets, leaving open whether evolutionary search is the best choice.\n4. The paper **claims benefits from long-term memory** for long-horizon tasks, but provides **no qualitative retrieval to decision case study or horizon-sensitive ablation** to show how memory changes behavior."}, "questions": {"value": "1. How does performance scale with search budget? Report results for at least two larger budgets (e.g., ngen ∈ {4, 8, 16} and nchild ∈ {5, 10}) and state whether performance consistently improves, saturates, or degrades.\n2. Can you evaluate transfer across model families, applying the same evolved genome to at least one model from a **different family** (e.g., an OpenAI, Anthropic, or an open-source LLM). Report per-environment performance and compare to the original target model.\n3. Show per-method learning curves and final performance ± CI under a matched budget. If Genetic Algorithm still performs best, explain WHY (e.g., better parallelism, robustness) rather than relying on single-number superiority.\n4. Please include one retrieval→decision trace and a simple ablation by episode length."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "48XKV271eV", "forum": "sMIiRjEuLL", "replyto": "sMIiRjEuLL", "signatures": ["ICLR.cc/2026/Conference/Submission17761/Reviewer_eUZK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17761/Reviewer_eUZK"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762332987051, "cdate": 1762332987051, "tmdate": 1762927601909, "mdate": 1762927601909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}