{"id": "0XP33CRtQF", "number": 5608, "cdate": 1757922693944, "mdate": 1759897965252, "content": {"title": "IBGraphRAG: Enhancing Medical Knowledge Graph Retrieval Based on Semantic Consistency and Information Bottleneck", "abstract": "Large Language Models (LLMs) have achieved remarkable progress in recent years, but they still struggle to generate reliable and precise responses in domains such as medicine that rely heavily on specialized knowledge. Retrieval-Augmented Generation (RAG) offers a scalable solution for integrating external knowledge into LLMs without additional training, and while it performs well in general domains, its effectiveness is limited in medical settings that demand high terminological precision and factual consistency. To address this, we propose a medical-domain-oriented RAG framework, IBGraphRAG, which integrates medical knowledge graphs with two key technical innovations: (1) Medical Semantic Consistency Alignment, which improves entity recognition and linking by enforcing semantic consistency with a structured medical knowledge base; and (2) Information Bottleneck-based Reasoning Path, which prioritizes retaining highly relevant contextual information during knowledge graph retrieval while avoiding irrelevant or superficial paths. Experimental results show that IBGraphRAG achieves state-of-the-art performance on multiple medical question-answering benchmark datasets, effectively improves the specialization and accuracy of the retriever in selecting reasoning paths over the knowledge graph, helping the LLM better identify relevant knowledge for reasoning.", "tldr": "", "keywords": ["LLM", "Retrieval-Augmented Generation", "Medica"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3cf1878ec50fe84a64b1b0f6d46f29444fdffcda.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The main contribution of this paper is a three-phase knowledge-graph-powered approach for medical question answering / fact verification: 1) use an LLM to extract entities from a user query, then augment with a medical knowledge base (this includes enriching with synonyms and encoding with BioWordVec); 2) information bottleneck over a query graph to find \"reasoning paths\" between a question and answer; 3) an LLM takes as input information retrieved from the knowledge graph to generate a final answer. Experiments over several QA and fact verification datasets show strong performance versus many baselines (and across many base LLMs)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The idea of bringing information bottleneck to this medical KG scenario makes a lot of sense. \n\n+ The experimental results appear quite strong and comprehensive. There are also some good ablations to showcase the importance of the entity alignment and the IB component."}, "weaknesses": {"value": "- The authors acknowledge in the limitations that the information bottleneck method results in \"non-trivial computational overhead during both training and inference\". It would be good to quantify this overhead. In particular, the method itself is quite complex (even beyond the IB part) with lots of steps. \n\n- I believe the IB component is trained over known QA-pairs. Do you have any evidence on how the method performs \"in-the-wild\" in scenarios where there is not clean training data? Does it generalize from one dataset to another?\n\n- There is some missing highly-relevant work -- KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques https://arxiv.org/abs/2403.05881 -- that also uses LLMS + KGs. The technique there uses ranking and no learning, but it would be worthwhile to discuss the connections.\n\n- Figure 3 shows two graphs -- one with IB guidance and one without. It is not clear to me what we are meant to learn from this figure. How to interpret which nodes are highlights since we don't know the context? The discussion may be sufficient without the graph."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ScyIrKiYov", "forum": "0XP33CRtQF", "replyto": "0XP33CRtQF", "signatures": ["ICLR.cc/2026/Conference/Submission5608/Reviewer_ZVYq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5608/Reviewer_ZVYq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761692870343, "cdate": 1761692870343, "tmdate": 1762918157360, "mdate": 1762918157360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "IBGraphRAG introduces a medical-domain oriented RAG framework that integrates medical knowledge graphs to enhance factual accuracy and terminological precision in LLM-based QA. It contributes two main innovations: Medical Semantic Consistency Alignment, which improves entity recognition and linking by aligning LLM-extracted entities with the UMLS knowledge base using semantic consistency checks, and an Information Bottleneck-based Reasoning Path mechanism, which optimizes KG traversal to retain only contextually relevant, information-rich reasoning paths. Together, these components make IBGraphRAG better suited for medical reasoning, characterized by highly specialized reasoning and complex KG reasoning paths, achieving state-of-the-art results on multiple biomedical QA benchmarks.\nWhile the results seem promising, the relationship with related work is not sufficiently clear, and it seems that core competitors are missing from the empirical evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method demonstrates improved performance compared to current architectures. \n\t- The results are evaluated on 7 widely used question answering and fact verification tasks.\n\t- The proposed methodology seems sound and resonates with the need in scientific community of having highly efficient and scalable retriever component able to determine the paths in the graph that are relevant to a particular query. \n\t- The authors present ablation studies testing the importance of both of their introduced components"}, "weaknesses": {"value": "Major:\n- W1 The paper could be better positioned with respect to existing work. Each paragraph in the related work section should clearly highlight how the proposed method addresses specific research gaps compared to prior approaches.\n- W2 It is unclear what the exact relationship of the proposed method is with the existing SubGraphRAG (Li et al 2024a) - is it the same approach, applied to medical data? Why is it not part of the empirical evaluation?\n- W3 Similarly, it is unclear how the proposed method compares with Su, Xiaorui, et al. \"KGARevion: an AI agent for knowledge-intensive biomedical QA.\" ICLR 2025, or Yang, Rui, et al. \"KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques.\" BioNLP 2024.\n\n- The paper would benefit from a more detailed description - either in the main text or appendix - of the formulation of both tasks involved in the experimental setup, and how each task components are mapped to the IB-guided target path optimization process.\n\t- The analysis of temporal performance is missing. It would be useful to report time-related metrics, such as the duration of retriever training, the time required for generating training set alignments, and inference time.\n\t- Additional details about the UMLS KG would be neededâ€”for example, the number of related terms and variants used to produce enriched entities within the MSCA component.\n\t- It is unclear how the retriever is trained or which retriever model is used. \n\t- An ablation study examining the impact of the number of IB-guidance epochs on performance would strengthen the paper. Currently, this is only shown for two graph examples in Figure 3.\n\t- Some statements are overly vague and should be clarified for reproducibility. For example:\n\t\t- (Line 178) \"If relevant entities remain insufficient\" - specify how this insufficiency is determined or quantified.\n\t\t- (Line 177) \"A domain-specific medical model encodes\" - clarify which model is used (e.g., BioWordVec?).\n\t\t- (Line 249) \"Precompute semantic embeddings of all entities and relations in the knowledge graph using pre-trained text encoders\" - specify which encoders are employed.\n- (Line 109) \"RAG... relatively underexplored in medical applications\" seems a bit of a bold claim\n\t- Related work: The paper could be strengthened by discussing or benchmarking against:\n\t\t- Wen, Yilin, et al. \"MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models.\"\" ACL 2024.\n\t\t- Zhu, Xiangrong, et al. \"Knowledge Graph-Guided Retrieval-Augmented Generation\" NAACL 2025\n\nErrata: \n\t- \"semantics.and\" --> line 165\n\t- \"overlap,as\" --> line 82\n\t- \"Baselines with MSCA\" to \"Baselines without MSCA\" --> line 441"}, "questions": {"value": "- W1-3 above\n\n- (line 309) Recent studies on the information bottleneck --> which studies? \n\t- (line 173) \"To train the retriever\" - which retriever are you training? please provide additional details for reproducibility. \n\t- (line 178) \"If relevant entities remain insufficient,\", I think this is too vague, provide additional details on how this insufficiency is evaluated. \n\t- (line 177) \"a domain-specific medical model encodes\"\n\t\t- not clear which model exactly? BioWordVec? \n\t- (line 249) \"precompute semantic embeddings of all entities and relations in the knowledge graph using pre-trained text encoders\"\n\t\t- which ones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MDlMUtYTgY", "forum": "0XP33CRtQF", "replyto": "0XP33CRtQF", "signatures": ["ICLR.cc/2026/Conference/Submission5608/Reviewer_R3yy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5608/Reviewer_R3yy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930819754, "cdate": 1761930819754, "tmdate": 1762918156631, "mdate": 1762918156631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, \"IBGraphRAG,\" proposes a new Retrieval-Augmented Generation (RAG) framework specifically designed for the medical domain, where terminological precision and factual accuracy are critical. The authors proposed (1) medical semantic consistency alignment (MSCA), which improves entity recognition by using an LLM to extract initial entities, enriching them with the Unified Medical Language System (UMLS), and then filtering them based on semantic consistency with both the query and the other candidate entities; (2) and information bottleneck-based reasoning path (IBRP), which selects reasoning paths from the KG. It is optimized using the Information Bottleneck (IB) principle to find paths that are maximally relevant to the answer while filtering out irrelevant information. The authors conduct extensive experiments on 11 medical QA datasets with different base models. The results show that the proposed method improve over the baselines by a large margin."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper does an excellent job identifying and demonstrating the precise failings of existing RAG methods in the medical domain, where the intuition makes sense.\n2. The MSCA module is a strong and practical contribution. The use of UMLS for enrichment combined with a semantic consistency check ($R(e)$) is a very sensible approach to filtering entities\n3. The use of the Information Bottleneck principle is a novel and theoretically sound way to frame the path selection problem.\n4. The method consistently outperforms all baselines across 11 datasets and multiple LLMs, demonstrating its effectiveness."}, "weaknesses": {"value": "1. It seems that the effectiveness of the proposed method largely rely on the completeness and accuracy of the external KG used, including the reasoning path from Fig. 1. I'm not sure about the effectivenss compared to the vanilla RAG if the information from KG is not that useful.\n2. The IBRP module is not a simple retriever. It involves training a GNN, an MLP scoring function, and a separate \"parametric variational decoder $r_{\\psi}$\" just to make the IB loss function tractable. This is a massive level of complexity for a retrieval component. I would like to see some complexity and latency analysis for the proposed framework.\n3. The authors use  shortest path between the question and answer entities as a supervisory signal but \"such a heuristic path is not always the most informative or diagnostically meaningful\" is the fundamental motivation for the retriever construction, which seems quite confusing to me."}, "questions": {"value": "1. See the weaknesses above.\n2.  How were the hyperparameters $\\lambda=0.4$ and $\\tau=0.6$ for the MSCA module chosen? How sensitive is the model's performance to changes in these values?\n3. Typo: line 323 \"concreate\" should be \"concrete\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gm55yQkjB8", "forum": "0XP33CRtQF", "replyto": "0XP33CRtQF", "signatures": ["ICLR.cc/2026/Conference/Submission5608/Reviewer_ecWz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5608/Reviewer_ecWz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149960926, "cdate": 1762149960926, "tmdate": 1762918156365, "mdate": 1762918156365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}