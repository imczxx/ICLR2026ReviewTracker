{"id": "H5QWmvze4g", "number": 11629, "cdate": 1758202644738, "mdate": 1759897563788, "content": {"title": "Latent Reasoning with Recurrent Depth for Sequential Recommendation", "abstract": "Sequential recommender systems play a pivotal role in modern applications by modeling user behavior sequences to predict their preferences. However, current approaches primarily adopt non-reasoning paradigms, which constrain their computational capacity and lead to suboptimal performance. To overcome these limitations, we propose LARES, an innovative and scalable \\textbf{LA}tent \\textbf{R}easoning framework for \\textbf{S}equential Recommendation that unlocks deep thinking with a recurrent depth. Unlike conventional parameter scaling methods, LARES enhances the model's representational power by increasing the computational density of parameters through depth-recurrent latent reasoning. Its recurrent architecture allows flexible expansion of reasoning depth without extra parameters, thereby effectively capturing complex and evolving user interest patterns. To fully exploit the model's reasoning potential, we introduce a two-stage training strategy: (1) Self-supervised pre-training (SPT) with \\textit{trajectory-level alignment} and \\textit{step-level alignment}, where the model learns latent reasoning patterns tailored for sequential recommendation tasks without annotated data, and (2) Reinforcement post-training (RPT), which leverages reinforcement learning (RL) to encourage exploration of diverse reasoning paths and further refine its reasoning capabilities. Extensive experiments on real-world benchmarks demonstrate LARES's superiority. Notably, the framework exhibits seamless compatibility with existing advanced models, consistently improving their recommendation performance. Our code is available at https://anonymous.4open.science/r/LARES-E458/.", "tldr": "", "keywords": ["sequential recommendation", "latent reasoning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d2057f7cc9c41d41cdd440643f0b3eaa832e8ab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "LARES introduces a depth-recurrent latent-reasoning module for sequential recommendation: a pre-block projects sequences to a latent space, and a core block refines all token states over K steps. Training is two stage, with self-supervised pre-training (TLA, SLA) followed by RL post-training (GRPO-style) optimizing ranking rewards. On four Amazon-2023 subsets it improves over strong baselines, supported by ablations and a latency/FLOPs study."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Solid empirical gains across multiple approaches (SASRec, FMLP-Rec, BSARec, TedRec).\n\nPractical deployment angle: controllable test-time computation; latency/FLOPs study.\n\nThoughtful ablations and step-count study; the “overthinking” dip is informative."}, "weaknesses": {"value": "The theoretical justification is weak. What is \"reasoning\" in recommendation? How does it differ from just deeper models?\n\nIt appears that the TLA constraint contradicts the main claim. It can't handle variable-length paths in training, but claims flexible test-time scaling\n\nThe RL design is weak. Simple rewards, filters hard examples.\n\nPlease see questions for more specific points."}, "questions": {"value": "Please explicitly define the \"reasoning step\" formally for recommendations. The \"reasoning\" appears to be iterative refinement of representations in latent space, not reasoning about behavioral motivations. I'm unclear on what reasoning actually means here.\n\nRL formalization: please define state, action, policy, reward, advantage, and the exact KL with the reference policy; show how gradients flow (p. 6, Eq. 11–12 and the reformulation paragraph).\n\nPruning impact: what share of training instances is dropped by the top-100 filter per dataset? Please report results with no pruning and with hard cases.\n\nWhy require matched steps across positives? \n\nTable 4: what exactly do “2x+2x” and “4(1+δ)x” denote, and how are FLOPs “matched” across different kernels/memory patterns? \n\nTLA treats sequences sharing the same target item as positives. Provide robustness by item-popularity decile and an alternative that controls for item identity. Is there popularity leakage? \n\nPlease show intermediate-step probes (entropy, calibration, retrieval accuracy by step), and variance of latent states over steps to demonstrate SLA prevents collapse."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HMxocwHWA9", "forum": "H5QWmvze4g", "replyto": "H5QWmvze4g", "signatures": ["ICLR.cc/2026/Conference/Submission11629/Reviewer_K6R5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11629/Reviewer_K6R5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678510794, "cdate": 1761678510794, "tmdate": 1762922701826, "mdate": 1762922701826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LARES, a latent-reasoning framework for sequential recommendation that deepens a model’s “thinking” by repeatedly applying a compact reasoning module—i.e., recurrent depth—to increase the computational density of existing parameters rather than adding new ones. This depth-recurrent design flexibly scales reasoning without extra parameters, enabling richer modeling of complex, evolving user interests. LARES is trained in two stages: (1) Self-supervised pre-training with both trajectory-level and step-level alignment to learn task-specific latent reasoning signals without annotations; and (2) Reinforcement post-training to explore diverse reasoning paths and refine decision quality. Experiments on real-world benchmarks show consistent gains and plug-and-play compatibility with strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, the paper is strong across originality, quality, clarity, and significance. It is original in framing latent reasoning with recurrent depth as a compute-scaling mechanism for sequential recommendation, innovatively increasing computational density without adding parameters and pairing this with tailored trajectory- and step-level self-supervision plus reinforcement post-training that optimizes ranking-metric rewards. The empirical quality is high: results on real-world benchmarks use full-ranking evaluation, show consistent and statistically reliable gains, include careful ablations and reasoning-depth analyses, and demonstrate plug-and-play improvements across multiple backbones. The exposition is clear, with a coherent narrative from motivation to architecture and training objectives, precise mathematical specification, informative figures, and explicit discussion of limitations such as compute overhead and hyperparameter sensitivity. In significance, the approach offers a practically attractive path to “deeper thinking” via test-time compute rather than parameter growth, increases the likelihood of deployment under memory budgets, and helps bridge LLM-style reasoning paradigms into recommendation, setting the stage for broader follow-up work on latent reasoning in discrete-ID tasks."}, "weaknesses": {"value": "- The paper does not evaluate on more complex, large-scale benchmarks such as Tmall or Yelp, and it does not include long horizon behavior datasets such as MovieLens 1M or MovieLens 20M. Without these results, it is difficult to judge scalability to dense catalogs and the ability to model very long user histories.\n\n- The paper does not compare with graph-based recommendation methods that model high-order item transitions and relations through message passing or more recent graph contrastive sequence models. This gap weakens the empirical claim of broad superiority. I recommend expanding the related work to systematically review sequential recommendation across attention, convolutional, recurrent, state space, and graph-based families, and to clearly state which limitations LARES addresses in each line. It would also help to add a focused discussion of large model-based recommendation, including prompt-driven and adapter-style approaches, recent work on latent reasoning and test-time compute scaling for recommendation, and to include representative LLM baselines in the experiments.\n\n- A major weakness is the absence of a direct comparison of the reinforcement learning stage’s training efficiency against strong non-RL alternatives under matched compute and data. It is therefore unclear whether reinforcement learning brings distinctive benefits in recommendation beyond well tuned supervised objectives. The paper should report learning curves of Recall and NDCG versus wall clock time and versus number of consumed examples, provide compute normalized comparisons to pairwise and listwise losses, and include policy gradient–free preference optimization such as DPO style ranking, as well as off policy bandit training with IPS or doubly robust estimators. \n\n- Training stability, variance across seeds, and convergence speed should be documented, together with sensitivity to reward scale and KL control. The authors should also delineate when reinforcement learning helps, for example under sparse delayed feedback, exposure bias, or longer horizon objectives, and when it does not. Finally, demonstrate combinations that may outperform pure reinforcement learning, such as supervised pretraining followed by short RL refinement, multi objective training that mixes listwise cross entropy with the RL reward, or using reinforcement learning only for a depth controller while keeping the backbone trained with supervised losses. This evidence would clarify whether reinforcement learning is uniquely advantageous in specific recommendation regimes or primarily an effective complement to standard training."}, "questions": {"value": "- How does LARES perform on complex, large scale datasets such as Tmall or Yelp, and on long horizon histories such as MovieLens 1M and 20M？\n\n- How does LARES compare to graph based sequence models that capture high order transitions with message passing and recent graph contrastive approaches？\n\n- What is the relative benefit of LARES versus prompt-driven or adapter-based large model recommenders？\n\n- What is the cost of deeper reasoning at inference in terms of FLOPs, wall clock latency at median and tail, and throughput？\n\n- Under what regimes does reinforcement learning help the most, for example sparse delayed feedback, severe exposure bias, or longer horizon goals？\n\n- What do intermediate reasoning states capture and how do they change the ranked list？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SmvnjJeKem", "forum": "H5QWmvze4g", "replyto": "H5QWmvze4g", "signatures": ["ICLR.cc/2026/Conference/Submission11629/Reviewer_1a8J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11629/Reviewer_1a8J"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971694967, "cdate": 1761971694967, "tmdate": 1762922701387, "mdate": 1762922701387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LARES, a scalable latent reasoning framework for sequential recommendation, aiming to address the limitations of current non-reasoning paradigms and parameter scaling methods. LARES adopts a depth-recurrent architecture consisting of a pre-block and an iterable core-block, enabling multi-step latent reasoning with all input tokens without additional parameters. To unlock its reasoning potential, a two-stage training strategy is designed: Self-supervised Pre-training (SPT) with trajectory-level and step-level alignment, and Reinforcement Post-training (RPT) leveraging GRPO algorithm. Extensive experiments on four Amazon datasets demonstrate LARES outperforms existing non-reasoning and reasoning baselines, with seamless compatibility with advanced sequential recommendation backbones."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "## 1. Originality.\nThe depth-recurrent latent reasoning paradigm is innovative, differing from prior work (e.g., ReaRec) that only generates single tokens per reasoning step. LARES refines all input tokens iteratively, effectively improving computational density and reasoning efficiency.\n\n##  2. Quality\nExperimental design is rigorous: four real-world datasets and comprehensive baselines ensure fair comparison.\nAblation studies systematically verify the necessity of core components (pre-block, SLA, TLA, RPT), and additional analyses (reasoning steps, alignment coefficients, inference efficiency) deepen the understanding of the framework.\n\n## 3. Significance\nLARES's compatibility with existing backbones enables easy integration into practical systems, promoting industrial application."}, "weaknesses": {"value": "## 1.  Insufficient Discussion on Computational Overhead Trade-offs.\n\nAlthough the paper identifies computational overhead as a limitation, it fails to conduct quantitative analysis in practical application scenarios. This makes it impossible to clearly demonstrate key performance metrics of the framework such as inference latency and memory usage.  Additionally, there is no horizontal comparison of computational efficiency with mainstream baselines, leaving readers unable to evaluate its feasibility and cost trade-offs in real-world deployments.\n\n## 2. Inadequate Comparison with Baselines.\nThe concurrent work Reinforced Latent Reasoning for LLM-based Recommendation (LatentR3) shares a highly similar technical route with this paper: both leverage reinforcement learning to implement latent reasoning mechanisms for sequential recommendation. However, the paper does not include LatentR3 in comparative experiments, which prevents a clear illustration of the framework’s advantages in core designs and its potential for performance improvement, thereby weakening the persuasiveness of the research conclusions.\n\n## 3. Incomplete Ablation Experiments for Reinforcement Learning Components.\nThe data selection strategy adopted in the paper lacks targeted ablation validation. No control group experiment with \"retained hard samples (samples where the target item ranks below 100)\" is designed, making it impossible to verify the specific impact of the current filtering strategy on the model’s generalization ability and reasoning accuracy."}, "questions": {"value": "## 1. Technical Questions\n- In the GRPO adaptation, why is the joint probability reformulated as the product of target item recommendation probabilities at each step? Is there theoretical support for this approximation?\n\n- Your work and the concurrent study Reinforced Latent Reasoning for LLM-based Recommendation (LatentR3) both integrate reinforcement learning with latent reasoning for sequential recommendation. Could you provide a detailed comparative analysis between LARES and LatentR3?\n\n## 2. Experimental Suggestions\n- Supplement computational overhead experiments: Test inference latency and memory usage under different sequence lengths and batch sizes, and compare with baselines under the same hardware conditions.\n\n- Complete RPT component ablation: Compare the performance of different reward functions and data selection strategies to demonstrate the necessity of the proposed design."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8nFW8r7VCW", "forum": "H5QWmvze4g", "replyto": "H5QWmvze4g", "signatures": ["ICLR.cc/2026/Conference/Submission11629/Reviewer_Z3f7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11629/Reviewer_Z3f7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987769288, "cdate": 1761987769288, "tmdate": 1762922700575, "mdate": 1762922700575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a sequence recommendation framework called LARES, which significantly improves the model's expressive and recommendation performance without increasing parameters by performing multi-step recurrent reasoning in the latent space. The model employs a two-stage training approach: self-supervised pre-training to learn latent reasoning patterns, followed by reinforcement learning post-training to further optimize reasoning capabilities through reward signals. Experimental results show that LARES significantly outperforms existing methods on multiple real-world datasets, validating its effectiveness in deep reasoning and recommendation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well-written, organized, and clear, with excellent illustrations. The experimental section provides detailed discussion and analysis, and the experimental improvements are significant compared to the baseline. On four real-world Amazon datasets, LARES significantly outperforms mainstream methods in metrics such as Recall@K and NDCG@K (improvements of 10%–20%), and the contribution of each module is verified through ablation experiments."}, "weaknesses": {"value": "This paper claims to focus on reasoning for recommendation, but it lacks discussion and comparison with related works [1,2]. Compared to PRL, its main addition is an RL module, making the contribution relatively incremental. Moreover, although the paper claims to perform reasoning, in essence, it only passes the hidden representations through network blocks multiple times, which can hardly be considered genuine reasoning. I understand that the authors are trying to draw inspiration from the latent reasoning setup in Latent CoT, but unlike Latent CoT, which incorporates semantic supervision for chain-of-thought reasoning, this work lacks such semantic guidance signals. Therefore, the reasoning claim seems somewhat overstated.\n\n\n[1] Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning\n[2] R1-Ranker: Teaching LLM Rankers to Reason"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YoHy3K0llo", "forum": "H5QWmvze4g", "replyto": "H5QWmvze4g", "signatures": ["ICLR.cc/2026/Conference/Submission11629/Reviewer_eZEV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11629/Reviewer_eZEV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762492318635, "cdate": 1762492318635, "tmdate": 1762922699349, "mdate": 1762922699349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}