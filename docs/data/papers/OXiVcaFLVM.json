{"id": "OXiVcaFLVM", "number": 9608, "cdate": 1758129856239, "mdate": 1759897709349, "content": {"title": "UniSpike: Boosting the Performance of Spiking Neural Network with Hybrid Training", "abstract": "Spiking neural networks (SNNs) are increasingly studied for their brain-inspired computing paradigm, offering high efficiency and sparse activation. \nHowever, achieving high accuracy with a small time-step remains challenging for SNNs. \nIn this paper, we propose UniSpike, a hybrid training framework that combines the high-accuracy feature of ANN-to-SNN conversion algorithms and the ultra-low time-step inference feature of direct training algorithms. \nUniSpike converts a quantized ANN into its SNN counterpart, then fine-tunes the converted SNN.\nTo replace the SNN-unfriendly operators in the ANN, UniSpike proposes Unified-Clip. \nUnified-Clip is equivalent to spike neurons and can replace SNN-unfriendly operators (i.e., softmax, layernorm, and GeLU) without degrading ANN accuracy. \nWith Unified-Clip, UniSpike proposes UniFormer, a novel transformer that is addition-only and supports step-by-step inference. \nUniFormer allows all the matrix multiplications except the patch embedding to be realized by simple addition and eliminates synchronization operators across time-steps. \nWith UniFormer,  UniSpike achieves 80.9% accuracy on ImageNet, outperforming the SOTA direct training algorithm Spike-Driven TransformerV2 (80.0%) with addition-only and step-by-step features with 4 time-steps. Compared to the SOTA conversion-based algorithm SpikeZIP-TF, UnSpike reduces 5.7$\\times$ energy with comparable accuracy.", "tldr": "", "keywords": ["Spiking Nerual Network", "Hybrid Training"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a097f2ae7bc6763ec220253c4224a381422073a3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a unified clip approach for hybrid training of Spiking Neural Networks (SNNs), which transforms non–SNN-friendly operators in Artificial Neural Networks (ANNs) into SNN-compatible forms. Theoretically, it demonstrates that the proposed clip operator is equivalent to a type of neuron in SNNs. Finally, the converted network is fine-tuned using Backpropagation Through Time (BPTT) to achieve a fully converted SNN with low latency and high performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. In the field of hybrid training, the authors improved performance from 69%/65.19% to 80.83%, demonstrating the effectiveness of the proposed framework.\n\n2. A simple yet effective clip function is used to uniformly convert multiple operators, and the equivalence between this clip function and existing spiking neurons in SNNs is also theoretically proven.\n\n3. The literature review is up to date, taking into account recent advances in the field.\n\n4. The problem statement is thorough, and the challenges faced are comprehensively presented."}, "weaknesses": {"value": "See questions."}, "questions": {"value": "1. In Table 1, the row referring to SDformer V2 states that the work uses the SNN-unfriendly SEW-Shortcut operation. However, SDformer V2 actually adopts the MS-Shortcut connection method.\n\n2. For E-Spikeformer, as the authors mentioned, it is indeed difficult to achieve step-by-step computation. However, the paper discusses possible asynchronous implementation schemes. In this case, is it still necessary to incur high costs to obtain an SNN through hybrid training (Conversion + Learning), especially when its performance is inferior to directly trained SNNs? Admittedly, such asynchronous implementations rely on specific neuromorphic chips, but I believe that for SNNs, leveraging neuromorphic hardware is inherently essential to fully realize their advantages.\n\n3. The hybrid learning framework used by the authors relies on A2S, which means it first requires selecting an ANN architecture. What kind of ANN architecture did the authors use in this case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MqbWIEdews", "forum": "OXiVcaFLVM", "replyto": "OXiVcaFLVM", "signatures": ["ICLR.cc/2026/Conference/Submission9608/Reviewer_XRYA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9608/Reviewer_XRYA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760969766366, "cdate": 1760969766366, "tmdate": 1762921149270, "mdate": 1762921149270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes UniSpike, a hybrid training framework that combines the high-accuracy feature of ANN-to-SNN conversion algorithms and the ultra-low time-step inference feature of direct training algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well-presented, and I am glad to see that it highlights the difference between layer-by-layer and step-by-step inference, which helps audiences outside the field understand the core distinction between ANNs and SNNs."}, "weaknesses": {"value": "1. Regarding the benefit of \"hybrid training\". The benefit of ANN-to-SNN conversion is the saved training time, while we can enjoy the high performance brought by the pretrained model in ANN. However, this work requires first finetuning the ANN into a quantized ANN and then conducting another round of finetuning for the converted SNN. According to the Appendix, I found the UniFormer actually requires a total of **620** training epochs, far more than the directly trained models/previous converted SNN benchmarks. Then, what's the actual benefit of this method in terms of training cost? \n\n2. Regarding the effectiveness. Hybrid Training also originates from a pretrained ANN. Performance evaluation should focus more on the performance gap between ANN and SNN, rather than just the performance of the converted SNN. On ImageNet, UniFormer still suffers a -2.47% performance drop, while the ANN benchmarks on the other datasets are not reported. The performance gap is significant with the advancements in the conversion method. For instance, ECMT only suffers from ~-1% performance drop at T=4 in their ICML version; their DCGS achieved up to 1% performance loss in a training-free manner. Actually, UniFormer cannot compete with SOTA directly trained SNN Transformers (QKFormer was actually presented in Neurips 2024, nearly 1 year ago). \n\n3. Regarding energy consumption comparisons of QKFormer, equating possible ternary spike operations with \"MAC\" operations is inappropriate. In fact, the \"SEW shortcut\" is currently the only shortcut mechanism supported by existing neuromorphic hardware; on asynchronous hardware, even if the number of input spikes is greater than one, it is still processed as a synaptic operation. Therefore, energy consumption estimates should be based on actual synaptic operation count. In fact, QKFormer has already taken into account the case where the input spike is greater than 1 in their energy consumption estimates. Thus, the authors should instead follow the energy consumption results originally reported in QKFormer and modify the data presented in Table 2 and Table 4 to ensure a fair comparison and avoid potential misleading information.\n\n4. In Table 1, QCFS and MST used BatchNorm instead of LayerNorm. The authors should correct this mistake."}, "questions": {"value": "Please address the points listed in Weakness.\n\nFurthermore, although the authors claimed they proposed the \"Unified Clip\" operator that can replace softmax, layer normalization, and GeLU operation, I found that the core idea behind \"unifying\" these operators is still to avoid using them (the original softmax, LN, GeLU operation in the pretrained model must be eliminated before conversion). This is well-adopted in prior works; it's just that the previous work didn't state it in this way. The direct result is a -1.67% performance drop in QANN before the conversion. I think this contribution is somewhat exaggerated, and I would like to ask the authors to clarify this point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "F1phUHWHWB", "forum": "OXiVcaFLVM", "replyto": "OXiVcaFLVM", "signatures": ["ICLR.cc/2026/Conference/Submission9608/Reviewer_h7XN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9608/Reviewer_h7XN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729340514, "cdate": 1761729340514, "tmdate": 1762921148908, "mdate": 1762921148908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UNISPIKE, a hybrid training framework for Spiking Neural Networks (SNNs) that combines the high accuracy of ANN-to-SNN conversion (A2S) and the ultra-low time-step inference of direct training (DT). It introduces UnifiedClip to replace SNN-unfriendly operators (e.g., softmax, layernorm, GeLU) and builds UniFormer, an addition-only, step-by-step inference transformer. Experiments on ImageNet, CIFAR datasets, and CIFAR10-DVS show UNISPIKE outperforms SOTA SNN algorithms in accuracy, energy efficiency, and latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.UNISPIKE effectively merges A2S and DT advantages, addressing the trade-off between SNN accuracy and inference time-steps, a key challenge in SNN research.\n2.UnifiedClip solves SNN-unfriendly operator issues without degrading ANN accuracy, enabling the novel UniFormer with energy-saving addition-only computation.\n3.Comprehensive experiments on multiple datasets (static and neuromorphic) demonstrate UNISPIKE’s superiority over SOTA methods in accuracy, energy use, and latency."}, "weaknesses": {"value": "1. I am confused that since the paper proposes UnifiedClip and it is equivalent to ST-BIF neurons, what are the differences between UnifiedClip and ST-BIF neurons, or what advantages does UnifiedClip have over ST-BIF neurons?It seems that UnifiedClip (UClip) is a operator of Artificial Neural Networks (ANNs), and due to its equivalence to ST-BIF neurons, the accuracy loss during the ANN-to-SNN (A2S) conversion is relatively small?\n\n2.In Table 3, UniSpike with UniFormer-T1 uses more time-steps than QKFormer (6 > 4) and achieves higher accuracy than QKFormer on CIFAR10 and CIFAR100. This part of the comparison seems to fail to fully reflect its advantages. How about the accuracy of UniSpike with timestep=4?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cOonsAtKaZ", "forum": "OXiVcaFLVM", "replyto": "OXiVcaFLVM", "signatures": ["ICLR.cc/2026/Conference/Submission9608/Reviewer_f9dh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9608/Reviewer_f9dh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921350844, "cdate": 1761921350844, "tmdate": 1762921148474, "mdate": 1762921148474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}