{"id": "TlRrhTU7WR", "number": 10209, "cdate": 1758164054316, "mdate": 1759897666299, "content": {"title": "USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models", "abstract": "Despite their remarkable achievements and widespread adoption, Multimodal Large Language Models (MLLMs) have revealed significant vulnerabilities, highlighting the urgent need for robust safety evaluation benchmarks. However, the limited scope, scale, effectiveness, and consideration of multimodal risks in existing MLLM safety benchmarks yield inflated and contradictory results, hindering the effective discovery and management of vulnerabilities. In this paper, to address these shortcomings, we introduce Unified Safety Benchmark (USB), which is one of the most comprehensive evaluation benchmarks in MLLM safety. Our benchmark features extensive risk categories, comprehensive modality combinations, diverse and effective queries, and encompasses both vulnerability and over-refusal evaluations. From the perspective of two key dimensions: risk categories and modality combinations, we demonstrate that the available benchmarks—even the union of the vast majority of them—are far from being truly comprehensive. To bridge this gap, we design a sophisticated data synthesis pipeline that generates extensive and efficient complementary data addressing previously unexplored aspects. By combining open-source datasets with our synthetic data, our benchmark provides 4 distinct modality combinations for each of the 61 risk sub-categories. Furthermore, beyond evaluating vulnerability to harmful queries, we pioneer the simultaneous assessment of model over-refusal to benign inputs. Extensive experimental results, conducted across 12 mainstream open-source MLLMs and 5 closed-source commercial MLLMs, demonstrates that existing MLLMs still struggle with the trade-off between avoiding vulnerabilities and over-refusal, and are more vulnerable to  image-only risky or cross-modal risky inputs, highlighting the need for refined safety mechanisms. Warning: This paper contains unfiltered and potentially harmful content that may be offensive.", "tldr": "We present the Unified Safety Benchmark (USB) for MLLMs, which provides 4 distinct modality combinations for each of the 61 risk sub-categories,  across both vulnerability and over-refusal dimensions.", "keywords": ["safety evaluation benchmarks", "multimodal large language models", "modality combinations"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ddf2af9928a806a02632038391691b354765fb6.pdf", "supplementary_material": "/attachment/4b5d17740c0bf680ca9f34da67a36dc34d1fee90.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces the Unified Safety Benchmark (USB), a new comprehensive benchmark designed to address the fragmented, limited, and inconsistent nature of existing safety evaluations for Multimodal Large Language Models (MLLMs). The authors construct a highly granular, three-tiered safety taxonomy of 61 risk categories and systematically evaluate MLLM performance across four distinct image-text modality combinations. To achieve this comprehensive coverage, they develop a sophisticated data synthesis pipeline to fill the gaps identified in prior benchmarks. A key contribution of USB is its dual-pronged evaluation, which jointly assesses a model's vulnerability to generating harmful content and its tendency to over-refuse benign queries, thus providing a more balanced view of safety alignment. The paper validates this new benchmark through an extensive evaluation of 17 leading open-source and commercial MLLMs, offering significant insights into their current safety limitations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's primary strength is its sheer scale and meticulous organization. The creation of a 61-category risk taxonomy and the systematic analysis across four modality combinations provide an unprecedentedly fine-grained tool for diagnosing MLLM safety weaknesses. This represents an engineering effort of high quality.\n\n- The authors don't just create a new dataset; they first perform a rigorous analysis of over 13 existing benchmarks to identify coverage gaps. Their data synthesis pipeline is then purposefully designed to fill these gaps, particularly for cross-modal risks (RIST/SIST), which they correctly identify as a key weakness in prior work."}, "weaknesses": {"value": "- The main weakness is the limited conceptual novelty. The paper follows the established paradigm of creating a static dataset of prompts and evaluating model responses. While the scale is impressive, it essentially represents a superset of existing efforts rather than a new way of thinking about evaluation. The field is arguably saturated with safety benchmarks (e.g., HarmBench, MMSafetyBench, VLSafe, VLSBench, etc.). This work feels like an escalation in a \"benchmark arms race\" rather than a paradigm shift towards more dynamic, adaptive, or reasoning-based evaluation methodologies that could offer more durable insights.\n\n- The evaluation protocol heavily relies on GPT-4o as the final arbiter of safety. The authors themselves note in Appendix C that GPT-4o achieves only 82% agreement with human annotators on a subset of their data. This 18% disagreement rate is substantial and casts doubt on the reliability of the fine-grained results across 61 different categories. It is plausible that the judge's accuracy varies significantly across different risk types and modalities, potentially skewing the results and model rankings in non-obvious ways. This dependency on a single, proprietary, and imperfect model is a significant limitation for a benchmark aiming to be a unified standard.\n\n- As with all static benchmarks, USB is susceptible to becoming stale. As it gains popularity, developers may inadvertently or intentionally train their models to pass its specific test cases, a phenomenon known as Goodhart's Law. While the USB-Hard subset attempts to mitigate this, it is still a static collection. The paper does not propose a framework for continuous evolution or adaptation, which limits its long-term value in a rapidly advancing field."}, "questions": {"value": "- Beyond scale and comprehensiveness, what do you consider to be the primary conceptual takeaway from your work for future benchmark designers? In other words, what fundamental principle of evaluation does USB introduce that is distinct from simply being a larger and more organized collection of prior ideas?\n\n- Regarding the LLM-as-Judge limitation: Given the 82% agreement with human annotators, have you analyzed whether this disagreement is uniformly distributed across the 61 risk categories and 4 modality combinations? Could it be that GPT-4o is significantly less reliable for certain nuanced categories (e.g., ethical safety vs. public safety)?\n\n- The creation of such a comprehensive benchmark is a massive effort. How do you envision maintaining USB's relevance over time? Are there plans or a proposed framework for community-driven updates to add new scenarios and adversarial examples as MLLMs evolve and new vulnerabilities are discovered?\n\n- In your data curation process, you define adequate coverage as having a minimum of 20 data samples. Could you elaborate on how this threshold was determined? Was it based on a statistical power analysis or is it a heuristic choice? Understanding the rationale would help in assessing the statistical robustness of the evaluations for each fine-grained category."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kJ7POEjw7v", "forum": "TlRrhTU7WR", "replyto": "TlRrhTU7WR", "signatures": ["ICLR.cc/2026/Conference/Submission10209/Reviewer_ZMah"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10209/Reviewer_ZMah"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798617643, "cdate": 1761798617643, "tmdate": 1762921569450, "mdate": 1762921569450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes USB, a comprehensive benchmark for evaluating the safety of multimodal large language models across 61 risk categories and 4 modality combinations, covering both vulnerability and over-refusal (oversensitivity). Experiments show that current MLLMs still struggle to balance safety and utility, exhibiting trade-offs between avoiding vulnerabilities and excessive refusals, and remain particularly vulnerable to multimodal risky inputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a thorough diagnosis of the key weaknesses in existing MLLM safety benchmarks—including insufficient data quality, limited risk coverage, and the neglect of modality combinations—and proposes targeted solutions such as automated data validation, expanded risk taxonomy, and the systematic design of four modality configurations. These are genuine and timely challenges that the work addresses in a structured and convincing manner.\n\n2. The authors make a commendable effort to aggregate and analyze prior benchmarks and to define a broader taxonomy of risk categories.\n\n3. The evaluation covers a large number of models and modalities, which provides some practical value for comparing current MLLMs."}, "weaknesses": {"value": "1. The paper primarily integrates existing benchmarks and standard practices into a larger dataset. The methodology—combining prior datasets, generating synthetic examples, and measuring refusal/safety rates—follows well-known recipes and lacks a genuinely new insight into safety measurement or model behavior.\n\n2. The contribution reads more like a benchmark report than a scientific study. There is little theoretical motivation or analysis beyond data aggregation, and the results mostly confirm known trends (commercial models safer; trade-off between safety and refusal).\n\n3. The paper does not discuss why certain models fail under specific modality combinations, nor does it propose new evaluation metrics or diagnostic insights. The “unified” benchmark concept remains descriptive rather than analytical."}, "questions": {"value": "A sensitivity analysis on GPT-4o’s evaluation stability (e.g., by varying prompts, temperature, or random seeds) would help determine whether USB’s reported consistency genuinely reflects benchmark robustness or merely GPT-4o’s internal bias."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gHGsfkDE21", "forum": "TlRrhTU7WR", "replyto": "TlRrhTU7WR", "signatures": ["ICLR.cc/2026/Conference/Submission10209/Reviewer_aaks"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10209/Reviewer_aaks"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807210900, "cdate": 1761807210900, "tmdate": 1762921568761, "mdate": 1762921568761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces USB (Unified Safety Benchmark), a comprehensive safety evaluation benchmark for Multimodal Large Language Models (MLLMs) that covers 61 risk subcategories across 4 modality combinations (risky-image/risky-text, risky-image/safe-text, safe-image/risky-text, safe-image/safe-text), and pioneers the simultaneous assessment of both model vulnerability and over-refusal."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Category coverage is a strength of the paper. Users only need to test on the Unified Safety Benchmark to obtain a comprehensive and reliable safety assessment without combining multiple benchmarks."}, "weaknesses": {"value": "1. The paper states that current benchmarks have limited data volume, citing less than 5K as an example, but notes that 5K data points are already quite substantial for a benchmark.\n\n2. It is unclear if the synthetic prompts accurately reflect real-world scenarios, particularly with the synthetic images, and whether they are sufficient for safety evaluation. This aspect appears to be a potential limitation of the paper.\n\n3. The evaluated models do not appear comprehensive, and some may be outdated. As a result, the conclusions might not extend to all current models. Some mainstream models are not included, such as Gemini-2.5-pro.\n\n4. The paper does not seem to introduce new insights. The main observation is that models exhibit a high degree of homogeneity across most categories, suggesting their vulnerabilities may be systematic rather than isolated cases. However, the paper does not analyze the underlying reasons for this observation."}, "questions": {"value": "1. I suggest the authors add more recent models, for example, those released before September-October.\n2. Given the benchmark's characteristic of comprehensiveness, I suggest using it to uncover new insights, such as whether certain models have unique advantages/disadvantages and why."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P0HCAIPkVt", "forum": "TlRrhTU7WR", "replyto": "TlRrhTU7WR", "signatures": ["ICLR.cc/2026/Conference/Submission10209/Reviewer_DiVA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10209/Reviewer_DiVA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909807710, "cdate": 1761909807710, "tmdate": 1762921568321, "mdate": 1762921568321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces USB, a unified safety benchmark for multimodal LLMs covering 61 tertiary risk categories crossed with four modality combinations (RIRT/RIST/SIRT/SIST), plus an over-refusal slice. USB combines curated prior datasets with a sizable synthetic pipeline to fill sparsely covered areas, yielding 13,175 (Base) and 3,785 (Hard) samples. Evaluations on 12 open-source and 5 commercial MLLMs report lower safety rates on USB than on prior benchmarks, expose weaknesses on image-only and cross-modal risks, and visualize the safety–refusal trade-off."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Coverage & structure. A clear two-axis design (taxonomy × modality combos) with explicit attention to RIST/SIST, which most benchmarks miss.\n2.Difficulty & discrimination. Lower average SR across models than prior sets; USB-Hard further separates systems while correlating with USB-Base (ρ≈0.98).\n3.Trade-off view. Joint reporting of vulnerability (SR) and over-refusal (RR) is practical and often neglected. Metrics are defined and computed consistently."}, "weaknesses": {"value": "1.A large share of items is synthetically generated, which can imprint generator-specific artifacts and cause domain shift from real photos/screenshots. This clouds external validity. I recommend: report metrics by source (public vs. synthetic) and add out-of-domain tests (real photos/screens vs. synthetic).\n2. The evaluation relies on a single closed-source judge, which risks judge bias and hides sensitivity to hyperparameters. I suggest: add a multi-judge setup (mix of open- and closed-source) with small-scale human checks, and report inter-judge agreement."}, "questions": {"value": "1.Recent studies suggest VLMs can be especially brittle on real-world artifacts (memes, natural photos/screens) and in multi-turn settings compared with synthetic, single-turn cases. Could you share your thoughts on adding a curated OOD slice (real photos/screenshots/memes) and a session-level multi-turn track, and—if feasible—reporting the resulting OOD gaps and rank stability?\n\n2.Many recent safety benchmarks adopt multi-judge ensembles, report inter-judge agreement (e.g., Cohen’s κ), and include calibration/meta-evaluation; some also provide bootstrap CIs and rank stability across decoding/safety settings. Would you consider complementing your evaluation—currently based on a single closed-source judge and default settings—with agreement and calibration analyses, plus CIs and stability checks across alternative decoding/safety configurations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jL3iFVSUhr", "forum": "TlRrhTU7WR", "replyto": "TlRrhTU7WR", "signatures": ["ICLR.cc/2026/Conference/Submission10209/Reviewer_gzxS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10209/Reviewer_gzxS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762693810518, "cdate": 1762693810518, "tmdate": 1762921567955, "mdate": 1762921567955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}