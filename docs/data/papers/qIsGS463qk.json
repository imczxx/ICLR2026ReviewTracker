{"id": "qIsGS463qk", "number": 191, "cdate": 1756730672180, "mdate": 1759898272093, "content": {"title": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries", "abstract": "Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well agents can solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual revision, that require coordinated use of multiple MCP tools. To address temporal variability in real-world tool responses, we introduce a parallel evaluation framework where a reference agent executes a validated plan simultaneously with the evaluated agent to produce real-time reference outputs, rather than relying on static ground-truth answers. Experiments show that even frontier LLMs achieve a task success rate below 60\\%, highlighting major challenges in multi-step tool use. Comprehensive error analysis identifies seven failure modes spanning tool planning, parameterization, and output handling, pointing to concrete directions for improving current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous agent systems that reliably execute complex tasks through MCP tool orchestration.", "tldr": "", "keywords": ["Agent", "Tool calling", "Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a498166ff2d300a0358c766d46a124c682fca311.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LiveMCP-101, a benchmark of 101 real-world, multi-step tasks designed to evaluate MCP-enabled agents across diverse tools (web search, file ops, math, data analysis). Key ideas are: (i) curating queries via iterative LLM rewriting + manual review; (ii) scoring against ground-truth execution plans to mitigate temporal drift; and (iii) a paired evaluation where a reference agent executes the plan while a test agent acts autonomously. The authors report <60% TSR for frontier LLMs, analyze seven failure modes, and present ablations on iteration budgets and MCP-server pool size."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Timely topic: Evaluating live tool use under MCP is important and under active development.\n- Evaluation method: Comparing an autonomous agent against a simultaneously run plan-following reference execution is a good way to reduce brittleness from time-varying tools and to enable trajectory-level diagnosis.\n- Useful error taxonomy: The seven failure modes (planning, parameter, parsing) align with what many practitioners see in production agents and could guide method development."}, "weaknesses": {"value": "1) The bench scale feels too small for the paper’s claims. 101 tasks across 41 servers / 260 tools is modest given the heterogeneity claimed. Competing datasets reach notably larger scales or cover broader API/tool surfaces. Without a power analysis, it’s hard to accept broad conclusions (e.g., token-efficiency “log-shape”) as general rather than sample-specific. Please consider expanding coverage (more domains, more servers, more tasks per domain).\n\n2) Related-work positioning is underdeveloped and too qualitative. The narrative briefly cites MCP-specific efforts (e.g., MCP-RADAR, MCPEval) but does not systematically contrast LiveMCP-101 against:\n   - MCP-centric evaluations: MCP-RADAR, MCPEval, MCP-Bench, LiveMCPBench. What exactly is novel beyond the “plan-following reference execution”? A careful table comparing #servers, #tools, #tasks, avg steps, on-/off-policy, real vs mock tools, drift handling, live or not, scoring method (LLM-judge vs references), and release artifacts is needed.\n   - Non-MCP but influential agent/tool benchmarks: ToolSandbox (stateful, on-policy conversational evaluation), StableToolBench (stability via virtual APIs), API-Bank (runnable APIs + dialogues), ShortcutsBench, FAIL-TaLMs, τ-bench and τ²-bench (user-agent-tool interaction & dual-control), AssistantBench/Web agents lines. The paper’s novelty claims should be tempered and clarified against these lines. \nEven if some of the works were released only recently, the community still cares deeply about seeing a comparative discussion with them.\n\n3) Metric design.\n   - The Likert-to-{0, .25, .5, .75, 1} mapping is reasonable, but the paper lacks inter-run variance, confidence intervals, and significance tests. Given live endpoints, variance across days/tools should be quantified; otherwise, model ranking may be unstable.\n\n4) Clarity/organization issues. \n   - The related-work discussion is “high-level” and doesn’t crisply delineate LiveMCP-101 from LiveMCPBench (also live MCP, multi-server) or from MCP-RADAR/MCPEval (multi-dimensional and deep evaluation). Right now, “dynamic live tools + LLM-as-judge” is not novel. Even if some of the works were released only recently, the community still cares deeply about seeing a comparative discussion with them.\n   \n5) Not open-source."}, "questions": {"value": "1) Please solve the weakness provided.\n\n2) Reproducibility: will you release task specs, plans, tool pools, traces, and the judge prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eAShqT1sZH", "forum": "qIsGS463qk", "replyto": "qIsGS463qk", "signatures": ["ICLR.cc/2026/Conference/Submission191/Reviewer_1b2H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission191/Reviewer_1b2H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536460695, "cdate": 1761536460695, "tmdate": 1762915466947, "mdate": 1762915466947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LiveMCP-101, a benchmark to fill a key gap in evaluating AI agents: while tool calling is vital for agents to interact with the real world and solve complex tasks , existing benchmarks fail to test how well agents handle multi-step tasks with diverse MCP tools in realistic, dynamic scenarios. LiveMCP-101 includes 101 curated real-world queries (refined via LLM rewriting and manual checks), all requiring coordinated use of multiple MCP tools. To deal with real-world tool responses’ temporal variability (a flaw of static ground-truth evaluation), the paper introduces a parallel framework: a reference agent runs a pre-validated plan simultaneously with the evaluated agent, generating real-time reference outputs. Experiments show even frontier LLMs have a Task Success Rate (TSR) below 60%—highlighting big challenges in multi-step MCP tool use."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  This paper addresses the flaws of existing MCP benchmarks (static, single-step) by proposing LiveMCP-101—the benchmark for dynamic real-world scenarios. It covers multi-step, cross-domain tasks and aligns with the practical deployment needs of agents.\n2.  The parallel real-time evaluation (synchronized execution of dual agents) avoids the timeliness bias of dynamic data. The validated execution plans provide reliable evaluation anchors, making the design innovative.\n3.  The experimental section encompasses 18 representative LLMs, quantifies capability disparities across models, and identifies 7 distinct failure modes—thereby providing clear guidance for model optimization. Several findings are interesting: for instance, open-source models are most severely impacted by syntax errors and unproductive reasoning; additionally, when increasing iterations from 25 to 50, performance plateaus (e.g., GPT-5 shows no improvement in TSR) occurs, showing the importance of   \"planning quality\" rather than \"iteration count\".\n4.  The appendices include execution plans, Prompt templates, and other supporting materials, alongside clearly outlined experimental settings."}, "weaknesses": {"value": "1.  Lacks verification of differences across multiple LLM judges.\n2.  The long-term iteration of dynamic APIs may lead to changes in their call logic. This raises questions about whether the current dual agent verification framework (synchronized execution of reference and evaluated agents) remains feasible—since pre-validated execution plans for reference agents could become obsolete due to API changes, undermining the accuracy of real-time result alignment."}, "questions": {"value": "please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cBD1Y0eFyV", "forum": "qIsGS463qk", "replyto": "qIsGS463qk", "signatures": ["ICLR.cc/2026/Conference/Submission191/Reviewer_mauR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission191/Reviewer_mauR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840813593, "cdate": 1761840813593, "tmdate": 1762915466746, "mdate": 1762915466746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced the LiveMCP-101 dataset for benchmarking the tool-use capability of LLM agents. The LiveMCP-101 dataset comprises 101 synthetic task queries, accompanied by task-specific MCP tool pools and human-revised execution plans as reference trajectories. To obtain real-time, correct answers for time-sensitive tasks during the evaluation, the author designed a parallel evaluation mechanism, where a reference agent executes the groundtruth plans to produce real-time references for evaluating the tested models. An extensive evaluation across 18 proprietary and open-sourced LLMs shows that frontier models fail to achieve a success rate over 60%."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The new benchmark offers more challenging test tasks, as evidenced by longer average tool-calling steps required and lower success rates by mainstream LLMs. Given the rapid development of LLMs and the quick saturation of evaluations, this challenging new benchmark is valuable for advancing research on agentic LLMs.\n\n2. The parallel evaluation framework provides a practical assessment for time-sensitive tasks.\n\n3. The paper is overall well written. The core ideas and evaluation details are well presented."}, "weaknesses": {"value": "1. Task distribution and quality are crucial for agent evaluation benchmarks. As described in 3.1, LiveMCP-101 uses queries generated by OpenAI o3 model, but the details of the generation process remain unclear (e.g., workflows and key prompts). And using synthetic task queries may raise concerns, as these test cases may deviate from real user needs or be biased towards the LLM used for synthesis. Given that existing agent benchmarks like GAIA and SWEBench provide test tasks from real people, can you elaborate on the necessity of using generated queries?\n\n2. LLM-as-a-judge, although widely used in LLM evaluation, may still bring risks like inconsistent evaluation results and preference leakage. The author may consider further analyzing the correlation between the tested model and the judge model, and adding rule-based metrics as a supplement."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "imYvlLWnLe", "forum": "qIsGS463qk", "replyto": "qIsGS463qk", "signatures": ["ICLR.cc/2026/Conference/Submission191/Reviewer_1kBR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission191/Reviewer_1kBR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902967975, "cdate": 1761902967975, "tmdate": 1762915466614, "mdate": 1762915466614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LiveMCP-101, a benchmark of 101 multi-step real-world tasks requiring coordinated use of multiple tools via the Model Context Protocol (MCP). A parallel reference-agent evaluation is also introduced to handle dynamic tool outputs."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Massive MCP servers and tools are considered, rendering the benchmark comprehensive.\n2. A new evaluation framework is proposed to handle dynamic tool outputs.\n3. Extensive experiments are conducted to benchmark LLMs."}, "weaknesses": {"value": "1. **Weak Motivation:** From the perspective of LLMs, there is no apparent difference between using MCP and function calling. In this regard, I am not convinced about the motivation of this paper. Why do we need to use MCP to benchmark LLMs given that massive function-calling benchmarks already exist? For example, the latest BFCL V4 benchmark already covers multi-turn and complex tool usage scenarios. The authors are recommended to clarify the motivation of this paper. Note that the cited works on MCP benchmarking in section 2 have not been accepted by any top-tier conferences, which cannot strongly support the motivation of this paper.\n2. **Soundness of the Evaluation Framework:** The proposed parallel reference-agent evaluation framework is interesting. However, I have some concerns about its soundness. Specifically, the reference agent is designed to strictly follow the ground-truth solution steps, which may not reflect the actual behavior of real-world agents. Besides, the final evaluation is based on an LLM-based judge, which always suffers from LLM's non-determinism and instability issues. Given that benchmarking is a critical task in the research of LLMs, the soundness of the evaluation framework is of great importance. The authors are recommended to further justify the soundness of their proposed evaluation framework.\n3. **Unclear Difficulty Levels:** It is commendable that the authors design three difficulty levels to enable fine-grained analysis. However, the criteria for difficulty level design are not clearly illustrated in the paper. For example, what is the difference between \"easy\" and \"hard\" tasks? For different LLMs, does there exist a discrepancy in task difficulty levels?"}, "questions": {"value": "1. What is the motivation of using MCP to benchmark LLMs given that massive function-calling benchmarks already exist?\n2. How to ensure the soundness of the proposed evaluation framework?\n3. What are the criteria for difficulty level design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q98Xkz69Tu", "forum": "qIsGS463qk", "replyto": "qIsGS463qk", "signatures": ["ICLR.cc/2026/Conference/Submission191/Reviewer_3NZi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission191/Reviewer_3NZi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930503198, "cdate": 1761930503198, "tmdate": 1762915466315, "mdate": 1762915466315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}