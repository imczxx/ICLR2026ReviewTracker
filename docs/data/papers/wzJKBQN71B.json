{"id": "wzJKBQN71B", "number": 20396, "cdate": 1758305486801, "mdate": 1759896979707, "content": {"title": "MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes", "abstract": "A key frontier for Multimodal Large Language Models (MLLMs) is the ability to move beyond semantic description and perform structured spatial analysis directly from images. Mathematical surface plots provide a rigorous testbed for this capability, as they isolate systematic visual reasoning from the semantic noise of natural images. To measure progress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over Visual Landscapes), a new benchmark designed to quantitatively evaluate these foundational skills. The benchmark comprises two novel tasks: Topological Counting, which requires models to identify and enumerate local extrema; and Transformation Recognition, which tests their ability to detect applied geometric transformations. Generated from a curated library of functions with rigorous ambiguity filtering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs struggle significantly, often resorting to superficial heuristics instead of robust strategies. We present MaRVL-QA as a challenging diagnostic tool to expose current limitations and to guide the development of MLLMs with stronger and more systematic visual-mathematical abilities.", "tldr": "", "keywords": ["Multimodal Reasoning", "Spatial Reasoning", "Visual Reasoning", "Mathematical Reasoning", "Multimodal Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e28697117c3c367a30dda9b316f5cb19d6ffdb9.pdf", "supplementary_material": "/attachment/e2b36d4a289b97f571d6b7671204d7c30255aa07.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces MaRVL-QA, a diagnostic benchmark for visual–mathematical reasoning over function plots with two tasks: Topological Counting (count local extrema) and Transformation Recognition (identify a single applied rotation/translation). The authors generate >80k QA pairs with multi-stage ambiguity filtering, and release a balanced MaRVL-QA-Mini (1,548 counting items + 1,200 transformation items). Ten MLLMs are evaluated."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- MaRVL-QA provides a well-defined, controllable testbed for visual–mathematical reasoning over function plots. \n\n- Two-way ambiguity filtering (e.g., excluding symmetric cases; distinguishing rotations vs. translations) and manual review improve label reliability. \n\n- Confidence intervals are provided; format robustness is probed by comparing an LLM parser to a rule-based extractor."}, "weaknesses": {"value": "- Synthetic, stylized plots may diverge from real scientific/engineering figures (e.g., noisy measurement fields, varied projections, non-uniform grids). The generated plots are mostly all in one distribution, which limited the representativeness of  MaRVL-QA. \n\n- The benchmark is limited to a narrow set of skills, which might over-fit models toward a few primitives.\n\n- The correlation analysis in Section 4.4 needs more explanation.\n\n- Some important and recent models are missing from the baseline models, such as Gemini-2.5-pro.\n\n- Some ablations are missing. For example, does the model performance improve if you add grids to the plot?"}, "questions": {"value": "- What is the performance of Gemini models on MaRVL-QA?\n\n- How the correlation with other benchmarks are performed?\n\n- Does fine-tuning helps model on such tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bTTwQ4qg6h", "forum": "wzJKBQN71B", "replyto": "wzJKBQN71B", "signatures": ["ICLR.cc/2026/Conference/Submission20396/Reviewer_kggk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20396/Reviewer_kggk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761503256903, "cdate": 1761503256903, "tmdate": 1762933845089, "mdate": 1762933845089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MaRVL to quantitatively evaluate the ability of performing structured spatial analysis in MLLMs. The benchmark comprises two major types of tasks: (i) Topological counting, to test MLLMs on their ability to identify and enumerate local extrema and (ii) Transformation Recognition, to test their ability to detect applied geometric transformations. The authors also contribute a curated library of 80000 QA pairs of mathematical functions with multi-stage ambiguity filtering. Moreover, they analyse systematic failure modes in MLLMs such as the \"one-trick pony\" performance for smaller models such as LLaVA, while top-performing models such as o4-mini demonstrates a higher accuracy of 67.12% on the transformation recognition tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper was clear and fluid to understand. Majorly, the strengths of the paper include: \n\n1) Failure mode analysis: The study's dissection of failure modes - catastrophic, near-miss, heuristic collapse and bias profiles - maxima and minima salience, rotation, translation confusion.\n2) Ambiguity Filtering: The paper employs algorithmic ambiguity filtering based on normalised RMSE thresholds, explicit rejection of symmetric or visually confounding transformations, and prominence-based feature validation."}, "weaknesses": {"value": "Mainly, I have one weakness to point out, which is not necessary to fulfil in immediacy:\n\n1) Synthetic Task Design: The benchmark relies on programmatically designed mathematical surface plots, which cover most aspects of visual reasoning, but lack in noise and the complexities of real-world visual reasoning."}, "questions": {"value": "Clarification required in one aspect:\n\nLimited Statistical and Error Robustness Analysis: While mean accuracies and confidence intervals are reported, the paper lacks statistical significance testing between models. It does not analyse potential dataset biases, such as the correlation between surface type. This leaves some uncertainty about whether observed differences are robust or dataset-specific."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RdRMcRj8eB", "forum": "wzJKBQN71B", "replyto": "wzJKBQN71B", "signatures": ["ICLR.cc/2026/Conference/Submission20396/Reviewer_azUV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20396/Reviewer_azUV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540665999, "cdate": 1761540665999, "tmdate": 1762933843861, "mdate": 1762933843861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MaRVL-QA (Mathematical Reasoning over Visual Landscapes), a novel diagnostic benchmark designed to evaluate the foundational visual-mathematical reasoning skills of Multimodal Large Language Models (MLLMs). The authors posit that current MLLMs excel at high-level semantic description but fail at a more fundamental level of precise spatial and structural analysis. To test this, the benchmark uses mathematical surface plots (e.g., 2D heatmaps of 3D functions) as a testbed, which isolates structural reasoning from the semantic \"noise\" of natural images.\n\nMaRVL-QA consists of two core tasks:\n\n1. Topological Counting: Requiring models to identify and enumerate local extrema (maxima or minima) from a single plot.\n2. Transformation Recognition: Requiring models to identify which geometric transformation (e.g., translation, rotation) maps one plot to another.\n\nThe benchmark is programmatically generated from a curated library of mathematical functions, employing a rigorous, multi-stage ambiguity filtering process to ensure objective ground truth. Critically, tasks are designed to be \"OCR-proof\" (e.g., by holding axis labels constant) to force genuine geometric reasoning.\n\nEvaluations on 10 SOTA MLLMs reveal profound deficits. The paper demonstrates that models (1) fail to scale as feature counts increase, (2) are biased by visual salience (e.g., better at counting bright \"maxima\" than dark \"minima\"), and (3) often resort to \"collapsed\" non-reasoning heuristics, such as always guessing the same rotation type regardless of the visual input. The paper concludes that these foundational skills are critical, showing strong correlations between MaRVL-QA performance and scores on complex benchmarks like MMMU and MathVista."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The benchmark's premise is highly original and insightful. While many benchmarks test math reasoning (e.g., GSM8K) or chart QA (e.g., ChartQA), MaRVL-QA is the first I have seen to so effectively isolate foundational geometric and topological reasoning from high-level semantics. Using semantically-sparse \"visual landscapes\" as a diagnostic tool is a novel and powerful idea.\n\n* The methodological quality of the benchmark's construction is a significant strength. The decision to hold axis labels constant in the Transformation Recognition task is a critical and intelligent design choice. It successfully preempts models from \"cheating\" by simply reading and comparing axis values, forcing them to reason about the surface geometry itself. The paper details a rigorous, multi-stage process to ensure objective ground truth. This includes (1) excluding ambiguous topological features like saddle points, (2) manually curating all counting questions to ensure features are visually unambiguous, and (3) programmatically filtering out confounding transformations (e.g., a rotation that looks like a translation, or a symmetric rotation that results in no change). This rigor is commendable. The benchmark is not static; it's generated from a curated library of 32 function families. Furthermore, it tests robustness by rendering plots across diverse styles (heatmaps, contours) and colormaps. The released MaRVL-QA-Mini test set is not just a random sample; it is a carefully stratified subset, balanced for factors like visual style consistency and transformation type, which enables fair and robust analysis.\n\n* The paper is exceptionally well-written. The problem statement is clear, the methodology is justified at every step, and the results are presented effectively. Figures 1, 3, 5, and 6, in particular, do an excellent job of visualizing the tasks and the models' systematic, heuristic-based failures.\n\n* The paper's findings are highly significant for the MLLM community.\n    * MaRVL-QA serves as a powerful diagnostic tool. It moves beyond \"models are bad at counting\" to show *how* they fail. The discovery of \"collapsed strategies\" (e.g., Mistral Small's 99.0% accuracy on 180-degree rotations and ~0% on translations) is a \"smoking gun\" that proves a complete lack of generalizable reasoning.\n    * The paper expertly identifies and distinguishes different failure modes: \"Robust Reasoners\" (O4-mini) that fail gracefully, \"Brittle Reasoners\" (Claude Sonnet 4) that collapse under difficulty, and \"Disengaged Heuristic Models\" (LLaVA) that are not reasoning at all.\n    * The analysis in Section 5 provides a clear, plausible hypothesis for these failures, linking them to architectural bottlenecks where vision encoders trained for semantic \"gist\" (like CLIP) discard the fine-grained geometric detail needed for these tasks.\n    * The strong, significant correlations with established benchmarks like MMMU (ρ=0.85) and MathVista (ρ=0.85) make a compelling case that these foundational skills are predictive of success on complex, real-world reasoning tasks."}, "weaknesses": {"value": "* The evaluation includes a strong set of SOTA closed-source models but is very weak on the open-source side. It primarily features the LLaVA family (which are now several years old and known to be poor at this) and one Qwen model. The LLaVA models perform at or near random chance, offering little insight beyond \"they can't do this at all.\" Including a wider range of modern, capable open-source MLLMs (e.g., InternVL, newer Llama-V) would be necessary to claim these failures are universal and not just an artifact of older architectures.\n\n* The paper strongly justifies using an LLM (GPT-4.1) as a parser, stating rule-based parsers are \"brittle\" and models \"failed to consistently adhere to the requested format\". However, the appendix (A.6.1) shows that for the Transformation Recognition task, the results with a strict rule-based parser are \"much more stable\" with \"negligible accuracy changes\". The brittleness only seems to dramatically affect the LLaVA models on the counting task. This makes the justification in the main paper feel slightly overstated.\n\n* The paper brilliantly analyzes the failure modes of low-performing models (e.g., LLaVA, Mistral Small). However, it only analyzes the *statistics* of the best model, O4-mini (e.g., its errors are \"near-misses\" and its most common error is \"No Change\"). It never explores *why* O4-mini fails. When it makes a \"near-miss\" count, is it consistently missing low-salience minima? When it guesses \"No Change,\" is it on visually subtle transformations? This is a missed opportunity to understand the failure modes of competent models."}, "questions": {"value": "1. The correlation analysis in Section 4.4 is a key part of your significance argument. However, the sample sizes (N) are very small: N=9 for MMMU, N=8 for MathVista, and N=5 for CharXiv. Given that 10 models were evaluated, could you clarify why the N-values are not 10? Is this due to public scores for some models being unavailable on those external benchmarks?\n\n2. The analysis of \"collapsed strategies\" in weaker models is a highlight of the paper. Could you provide any qualitative insight into the failure modes of the strongest model, O4-mini? For instance, when it fails on Topological Counting with a \"near-miss,\" does it exhibit a pattern, such as consistently missing features near the boundary or features with lower visual salience (minima)?\n\n3. The open-source evaluation focuses heavily on the LLaVA family, which performs at chance level. This demonstrates a floor, but doesn't tell us much about capable open models. Have you considered evaluating more recent open-source MLLMs to see if they also adopt these heuristic strategies or if any are beginning to show \"brittle\" or \"robust\" reasoning profiles?\n\n4. You provide a clear and strong justification for excluding saddle points due to their lack of a \"clear visual signature\". As a forward-looking question, do you believe this is a permanent limitation of 2D representations, or could a model trained specifically on the mathematical definition of a saddle point learn to identify them from contour plots, even when they are visually ambiguous to humans?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sUBFS9eCvw", "forum": "wzJKBQN71B", "replyto": "wzJKBQN71B", "signatures": ["ICLR.cc/2026/Conference/Submission20396/Reviewer_gLLJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20396/Reviewer_gLLJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877408057, "cdate": 1761877408057, "tmdate": 1762933843361, "mdate": 1762933843361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MaRVL-QA, a visual mathematical reasoning benchmark of function plots. It contains two tasks:\n\n1. Topological Counting: count local maxima/minima from a plot\n2. Transformation Recognition: identify the rotation/translation maps an original plot to a transformed one (5 multiple choice)\n\nThe authors describe a synthetic corpus (1,548 QA items for Topological Counting and 79,542 for Transformation Recognition) and a smaller MaRVL-QA-Mini subset (2,748 items). They evaluate ten MLLMs in zero shot mode with deterministic decoding (tempreture = 0), and argue that models rely on brittle heuristics rather than robust geometric reasoning. A full dataset and code release is promised but not yet included."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Great idea overall.\n2. Clear task definitions.\n3. Good analysis of model behavior: The paper doesn’t just report accuracy; it actually looks at how models fail. For example, some models always guess the same rotation or default to “No Change” when uncertain. Those observations clearly show where reasoning breaks down and make the results more insightful.\n4. Potential for extensibility: The idea (function-plot generator) could produce many more different controlled tasks."}, "weaknesses": {"value": "I want to call this sections \"Potential Improvements\" rather than weaknesses. And this is the part where I want to say most of the stuff I think needed to be discussed.\n\n### **1) Dataset maturity and consistency:**\nThe sample provided with the paper includes 100 examples per task. “TopologicalCounting” files include labels in their names (e.g., example_001_inferno_heatmap), while “TransformationRecognition” uses random UUIDs. Plus \"“TopologicalCounting” \" comes with a detailed config.json file with extra metadata which is not present on “TopologicalCounting” at all. This inconsistency makes it look like the two were made by two different teams and just put together inside a single file in a rush. A unified structure would make the benchmark easier to use and trust.\n\n### **2) Go beyond zero-shot results:**\nRight now all results are zero-shot evaluations. Adding fine-tuned and trained results (both for MLLMs and for pure vision models) would be extremely informative. Because the tasks are text-independent and have fixed answer spaces (5 multiple choice), this setup is perfect for training vision models to compare directly with text based multimodal models. It would also show whether current models’ limitations are about reasoning or simply training mismatch.\n\n### **3) Expand task diversity:**\nThe core idea is very good. Only two tasks are included, but the idea could support many. You could add symmetry classification, noise robustness (I mean mathematical high frequency noise, not image noise), or even better, parameter estimation tasks. These would highlight the full flexibility of your idea, but right now the code idea is just undersold with two simple tasks. \n\n### **4) Include in-depth cross-representation analysis:**\nWhile reading the paper, I noticed an interesting connection with another recent work. Your emphasis on representation, specially use of different \"Plot Types\" mentioned on A.2, and the conclusion you make: \"The data reveals a clear performance drop for most models when plot styles differ\" is the core idea of the VGA paper (vga.csail.mit.edu) which they call it \"conceptualization\". Your paper is the third I’ve seen to clearly demonstrate this representation-sensitivity problem in LLMs. It would be very informative to include a deeper analysis of how different visual representations affect model performance. An interesting extension would be to replicate VGA’s core experiment in your functional plots: test whether models can recognize that two plots represent the same function when rendered in different visual styles.\n\n### **Minors:**\n#### **5) Scale up to make a dataset, not only a benchmark:** Since the data are synthetic it should be easy to generate hundreds of thousands of examples. A dataset let us train and fine-tune models for deeper analysis.\n#### **6) Structure and define  `config.json` :** It's not very clear what to do with `config.json`. It always has 4 `null` values. I really couldn't make sense what is `func_id` (which is always `null` for the second function). This should be clearly document in your paper.\n\n### **Final Remarks**\nI want to emphasize that I really like the idea behind this paper. The current paper feel unfinished, and it can be much better. This could become a strong and influential benchmark if enough effort were to be put into it. My recommendation to reject is purely about the current state, not the idea. I want to see this paper at it's mature point then I definitely vote for acceptance."}, "questions": {"value": "I think the paper is clear and I don't have much questions. only one.\n\nAbout “OCR-proof” you said \"xes/labels are held constant across pairs, so models must reason\nabout the surface itself rather than read values\". But that's not the case for the actual numbers shown in the plots. Don't you think models can be biased toward certain numbers? In other words, if you mask the values would the results be consistant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zq81am9BLo", "forum": "wzJKBQN71B", "replyto": "wzJKBQN71B", "signatures": ["ICLR.cc/2026/Conference/Submission20396/Reviewer_FBeL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20396/Reviewer_FBeL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762100169767, "cdate": 1762100169767, "tmdate": 1762933842888, "mdate": 1762933842888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}