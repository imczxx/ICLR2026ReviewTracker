{"id": "SQVxBJhIrK", "number": 1366, "cdate": 1756875747687, "mdate": 1759898212489, "content": {"title": "DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning", "abstract": "Large language models (LLMs) have achieved remarkable success in many natural language tasks but still struggle with complex, multi-step reasoning, particularly across diverse disciplines. Existing reasoning datasets often lack disciplinary breadth, reasoning depth, and diversity, and lack guiding principles for question synthesis. We propose DESIGNER: a DESIGN-logic-guidEd Reasoning data synthesis pipeline that leverages naturally available, extensive raw documents (e.g., book corpus and web corpus) to generate multidisciplinary challenging questions. We introduce the concept of \"design logic\" and instruct LLMs to mimic human educators' question-creation process, enabling automated synthesis of large-scale, high-difficulty questions. We use LLMs to reverse-engineer and abstract over 120,000 design logics from existing questions across various disciplines. By matching these design logics with source documents, we are able to create reasoning questions that far surpass the difficulty and diversity of existing datasets. Using this pipeline, we synthesized two large-scale reasoning datasets that span 75 disciplines: DLR-Book (3.04 million questions from the book corpus) and DLR-Web (1.66 million questions from the web corpus). Data analysis indicates that the questions synthesized by our method exhibit greater difficulty and diversity compared to those in the baseline datasets. We validate our synthesized data through supervised fine-tuning (SFT) on the Qwen3 and Llama3 model families. Our data substantially enhances their multidisciplinary reasoning capabilities, outperforming existing datasets. Notably, after SFT on our datasets, the base versions of these models even surpass their official instruction-tuned counterparts.", "tldr": "", "keywords": ["Synthetic Data", "Large Language Models", "Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e28ed2bb7d48242dff39432b9fe1bacea3a72d28.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Large language models have achieved strong performance on many natural language processing tasks, but they struggle with complex reasoning tasks. Many existing studies use synthetic data to train large language models to improve their reasoning ability. However, these studies often lack disciplinary guidance. \n\nTo address this issue, this paper uses some unlabeled documents to generate synthetic data, including books and web data. Specifically, the process is divided into three parts: Data Extraction and Preprocessing, Code Synthesis, and Filtering and Output. The authors conduct an in-depth analysis of the synthetic data, examining data difficulty, diversity, and disciplinary distribution. \n\nFinally, they train the model using the synthetic data and find that it leads to better training results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ This paper proposes a new method for data synthesis that can generate useful data for the model from unlabeled documents.\n\n+ This paper conducts an in-depth analysis of the data, including data difficulty, data diversity, and disciplinary distribution, helping readers better understand the synthetic data and providing more information for future research.\n\n+ The synthetic data in this paper can effectively improve the model's reasoning ability and proves to be effective across tests in multiple disciplines."}, "weaknesses": {"value": "+ Previous studies have also generated training data from unlabeled data (such as DeepSeek-Math, JiuZhang3.0, and MAmmoTH 2.0). Even without using complex logic extraction and logic retrieval processes, their results are more significant than existing methods.\n\n+ The process of generating synthetic data uses very large models, which incurs high costs. Does this indicate that the method has limitations in practical applications?"}, "questions": {"value": "+ The paper tests multiple-choice tasks. Does it also perform well on open-ended questions, such as GSM8k or MATH500?\n\n+ Why does combining web data and book data lead to better results? What are the differences and characteristics of these two types of data when merged?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V4FeYZSbGt", "forum": "SQVxBJhIrK", "replyto": "SQVxBJhIrK", "signatures": ["ICLR.cc/2026/Conference/Submission1366/Reviewer_wEhN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1366/Reviewer_wEhN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760604512966, "cdate": 1760604512966, "tmdate": 1762915749413, "mdate": 1762915749413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a data synthesis approach aimed at alleviating the data scarcity problem in cross-disciplinary reasoning tasks. The authors construct two large-scale reasoning datasets: DLR-Book and DLR-Web, covering 75 scientific disciplines. The core contribution of this work lies in its logic-guided data design, which enhances data quality through difficulty and diversity control. Experimental results show that the proposed datasets outperform existing ones on multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of the work is clear and easy to understand.\n2. The main contribution lies in building a large-scale multidisciplinary dataset, which achieves competitive or even superior results across multiple benchmarks (e.g., Table 1).\n3. The carefully designed synthesis pipeline demonstrates thoughtful engineering and could provide useful insights for other researchers working on data generation and reasoning dataset construction."}, "weaknesses": {"value": "1. The current approach is essentially a heuristic replacement for existing data synthesis methods, mainly relying on hand-crafted design and rejection sampling to obtain high-quality data.\n2. The complexity of the data construction process might lead to error accumulation and make it difficult to control noise in the final dataset.\n3. Some experimental results require further clarification:\n+ In Figure 5, why does the model show better scalability only on GPQA-Diamond, while other datasets do not exhibit similar improvements?\n+ In Table 5, the ablation study shows that removing Design Logic (w/o Design Logic) results in only minor performance drops. Other ablation variants also show limited improvements?\n4. The data efficiency of the synthesis process is unclear. For example, during data filtering, what proportion of data was discarded? Understanding this would help assess the trade-off between data quality and synthesis cost."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LmJlubd6zk", "forum": "SQVxBJhIrK", "replyto": "SQVxBJhIrK", "signatures": ["ICLR.cc/2026/Conference/Submission1366/Reviewer_VTR2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1366/Reviewer_VTR2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734334415, "cdate": 1761734334415, "tmdate": 1762915749288, "mdate": 1762915749288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DESIGNER, a novel pipeline for synthesizing large-scale, multidisciplinary reasoning data by leveraging \"design logic\"—a structured abstraction of human educators' question-creation process. The method reverse-engineers design logics from existing questions. It matches them with raw text corpora (books and web) to generate challenging questions across 75 disciplines, resulting in two datasets: DLR-Book (3.04 million questions) and DLR-Web (1.66 million questions). Experiments demonstrate that models fine-tuned on these datasets achieve significant improvements in multidisciplinary reasoning, even surpassing official instruction-tuned counterparts. The work addresses data scarcity in complex reasoning domains but raises questions about scalability and quality control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The step-by-step pipeline is highly systematic, enabling large-scale query data generation with strong operability and reproducibility. The clear phases—from data curation and design logic extraction to question synthesis—provide a structured framework that can be easily adapted or extended by other researchers. This practicality enhances the method's value for real-world applications."}, "weaknesses": {"value": "1) The work resembles a complex engineering effort with multiple sub-tasks (e.g., discipline labeling, difficulty classification) pieced together, which may lack the novelty.\n\n2) The pipeline relies heavily on prompt engineering (PE) at multiple stages (e.g., discipline classification using Figure 7, difficulty classification using Figure 8), but lacks rigorous quality assessments for each step. For instance, the discipline labels and difficulty scores are derived from LLM judgments without validation against ground-truth metrics or inter-annotator agreement. The difficulty classification, in particular, depends on only three examples in Figure 8, which may not capture nuanced criteria for \"Very Hard\" questions. Consequently, the claim in Section 4.1—that the synthesized data is more difficult based on LLM-labeled distributions—is less convincing without evidence of labeling reliability.\n\n3) The focus on difficulty and diversity (Sections 4.1–4.2) overlooks critical aspects of data usability and accuracy, such as whether questions have unique answers or sufficient conditions to derive solutions. No analysis is provided on logical consistency or answer correctness, which are fundamental for reasoning data quality. This gap undermines the practical utility of the synthesized datasets.\n\nOverall, the paper presents a scalable data synthesis pipeline with practical benefits, but the engineering-heavy approach, unvalidated quality controls, and overlooked usability issues limit its conceptual impact. Addressing these concerns could elevate the contribution."}, "questions": {"value": "1) Will the synthesized datasets (DLR-Book and DLR-Web) be openly released to facilitate reproducibility and broader research? \n2) In Line 400 (Section 5.5), the claim that \"book corpora are of higher quality than web corpora\" is presented as a widely acknowledged consensus but lacks references. \n3) There appears to be a discrepancy in MMLU-Pro scores: Table 4 reports 68.4 for DLR-Book, while Table 5 reports 68.1 for the full DESIGNER method. Is this a typo? Additionally, Table 3 shows that DLR-Web+Book yields the best performance, yet Tables 4 and 5 use DLR-Book or ablated versions for comparisons. Justifying why the optimal dataset wasn't used in later analyses would improve consistency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cTfDP72B84", "forum": "SQVxBJhIrK", "replyto": "SQVxBJhIrK", "signatures": ["ICLR.cc/2026/Conference/Submission1366/Reviewer_mg22"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1366/Reviewer_mg22"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934924010, "cdate": 1761934924010, "tmdate": 1762915749151, "mdate": 1762915749151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies the primary bottleneck of improving the complex, multi-step reasoning capabilities of Large Language Models (LLMs) as a scarcity of large-scale, high-quality, and diverse training data, especially for specialized, university-level subjects. To solve this, the paper introduces **DESIGNER**, a novel data synthesis pipeline. The core innovation is the concept of \"Design Logic\", which the authors define as a form of \"meta-knowledge\" that abstracts the structured process human educators use to construct complex and insightful questions.\n\nThe paper's primary contributions are:\n- The **DESIGNER** pipeline, centered on the novel \"Design Logic\" concept.\n- The two resulting multi-million-question datasets, DLR-Book and DLR-Web, whose analysis shows significantly more difficult and semantically diverse than existing baseline datasets.\n- Extensive validation through Supervised Fine-Tuning (SFT) on Llama3 and Qwen3 model families. The key result is that base models trained on the proposed datasets substantially outperform existing datasets."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed **DESIGNER** data synthesis pipeline is novel and well-motivated. The structured process human educators use to construct complex and insightful questions.\n1. Using this pipeline, the authors created two new, large-scale reasoning datasets: **DLR-Book** (3.04 million questions) and **DLR-Web** (1.66 million questions), which benefit the community to improve existing models.\n1. Detailed analyses prove with quantitative metrics that the DLR datasets are more difficult and more semantically diverse, validating the effectiveness of the proposed **DESIGNER** pipeline.\n1. Applying SFT on the DLR datasets, existing models outperform their official versions and gain performance improvement across all reasoning benchmarks.\n1. Throughout ablation studies justify the effectiveness of the **DESIGNER** pipeline"}, "weaknesses": {"value": "1. The pipeline's success relies on massive proprietary assets. Though the authors state they will release a subset of the final synthesized data, this still limits the community's ability to fully reproduce the pipeline or build upon the design logic library.\n1. The pipeline itself depends on existing very large, capable models (e.g., Qwen3-30B, DeepSeek-R1-0528), and this means massive computational resources are required to apply this pipeline.\n1. The pipeline only creates large-scale datasets, and these datasets are only validated on very large models, missing validation on small corpora and small models."}, "questions": {"value": "This paper is quite solid, detailed, and well-written, and I only have a few questions:\n1. In Figure 4, we can see that DLR-Book contains more than half of the data categorized as \"Other\" (and more than 1/3 on DLR-Web). Is this beneficial? If so, how do these benefit the model? If not, can we drop them?\n1. Can the **DESIGNER** pipeline apply to open-source raw data or existing datasets?\n1. How many GPU hours have been spent on creating the DLR datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vQs1HnsyzC", "forum": "SQVxBJhIrK", "replyto": "SQVxBJhIrK", "signatures": ["ICLR.cc/2026/Conference/Submission1366/Reviewer_VsnW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1366/Reviewer_VsnW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962865099, "cdate": 1761962865099, "tmdate": 1762915748848, "mdate": 1762915748848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Designer, a data-synthesis pipeline that extracts reusable design logics (structured problem-solving flow charts) from a large question bank. Using filtered snippets form existing text corpora, the system coarse-retrieves several candidate logics, then has an LLM select one to use, and synthesizing snippet and logic to form a new question plus a short reference answer. A separate thinking model produces long chain-of-thought (CoT) responses. To test this pipeline, popular base models are SFT'd on these synthetic examples. They find that Designer SFT outperforms the instruciton-tuned variants of the base models. Several ablations are performed, including analysis of the corpora quality and the key components of the design logic process. The produced training data is also compared with identically-sized subsets of other synthetic datasets, finding the DLR-trained models consistently outperform the other 4 baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Design logic direction seems like a novel and scalable method for condensing multi-domain learnings in a reusable manner.\n- Resultant datasets yields solid improvements over other synthetic pre-training data methods.\n- Paper is well written and easy to follow."}, "weaknesses": {"value": "My biggest concerns are with the use of a proprietary question bank, and also general concerns with the impact of the design logics, versus just having relevant and high-quality data:\n- For the design logic process, I like the idea of having a static design logics bank, however, it seems the design logics themselves don't have a massive impact on performance; the 'w/o Design Logic' ablations note fairly small gains from using design logics (as opposed to just providing examples from the question bank). I'd love to see more extensive analysis into the design logic creation as this is the most novel component of this work.\n- Given the strong dependency on the question bank, I am concerned about its proprietary nature. What happens if we don't have a carefully-crafted question bank? Is the Designer process is only pertinent if we have a challenging question bank available? If this process is largely reproducible with public data, then it would be ideal to publish these results. \n- No comparison with other simple SFT baselines. For example, what are the base model results (no SFT), and what if you perform SFT on the instruction-tuned variants of the baseline models? It seems these results would paint a clearer picture."}, "questions": {"value": "- **Significance of question bank**: Can you provide any results using only public / reproducible data? A key distinction between your work and others is the difficulty of the generated questions (Figure 3). Is it possible your data is simply more challenging due to this question bank? Also, if difficulty is a large factor, could one simply just upsample hard questions in other dataset to match the distribution of yours?\n- **Additional SFT baselines**: Can you provide the raw base model results, and also Instruction-tuned base model + DLR SFT?\n- **Analysis of synthetic generated responses**: Does the long-CoT Qwen3-235B responses ever disagree with the DeepSeek-R1 answer? If so, do you filter these examples out? Also, have you explored what happens if you include the corresponding design logics as problem-solving guidance (e.g. \"for these types of problems some things to think about are A -> B -> C\")\n- **Contribution of design logics for question generation** -- again, the ablation in Table 5 notes that the gains from design logics are fairly minimal; it appears that the retrieved examples alone (w/o Design Logic) perform well, so could this mean that design logics are perhaps just useful as a structured and abstracted method for indexing a large question bank?\n\nSmall writing suggestion: The caption in Figure 6 ('source corpus quality') is ambiguous until you look at Section 5.5 (which discusses the acknowledged differences)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eP8uLWqbuz", "forum": "SQVxBJhIrK", "replyto": "SQVxBJhIrK", "signatures": ["ICLR.cc/2026/Conference/Submission1366/Reviewer_TpGs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1366/Reviewer_TpGs"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116409916, "cdate": 1762116409916, "tmdate": 1762915748629, "mdate": 1762915748629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}