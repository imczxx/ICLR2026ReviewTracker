{"id": "xEsJW4xqQC", "number": 23343, "cdate": 1758342454915, "mdate": 1759896819787, "content": {"title": "FlatLand: Personalized Graph Federated Learning via Tailored Lorentz Space", "abstract": "Personalized Federated Learning (PFL) has gained attention for privacy-preserving training on heterogeneous data. However, existing methods fail to capture the unique inherent geometric properties across diverse datasets by assuming a unified Euclidean space for all data distributions. Drawing on hyperbolic geometry's ability to fit complex data properties, we present FlatLand, a novel personalized federated learning method that embeds different clients' data in tailored Lorentz space. FlatLand is able to directly tackle the challenge of heterogeneity through the personalized curvatures of their respective Lorentz model of hyperbolic geometry, which is manifested by the time-like dimension. Leveraging the Lorentz model properties, we further design a parameter decoupling strategy that enables direct server aggregation of common client information, with reduced heterogeneity interference and without the need for client-wise similarity estimation. To the best of our knowledge, this is the first attempt to incorporate hyperbolic geometry into personalized federated learning. Empirical results on various federated graph learning tasks demonstrate that achieves superior performance, particularly in low-dimensional settings.", "tldr": "", "keywords": ["Federated Learning", "Hyperbolic Geometry"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/674af27a29d1bdc34c75b56a573840b0a38ddbd8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work studies federated learning on graph-structured data with heterogeneous client distributions.\nThe authors observe empirically that client graphs tend to exhibit negative Forman–Ricci curvature, indicating that hyperbolic geometry provides a better fit than Euclidean space. Motivated by this, they embed each client’s data in a tailored Lorentz (hyperbolic) space whose curvature can adapt per client and train neural networks directly in that geometry.\n\nWithin the Lorentz model, they show theoretical factorization of heterogeneity, i.e., the space-like coordinates encode information common across clients, while the time-like coordinate captures client-specific variation (formalized via mutual information).\n\nLeveraging this separation, they propose FlatLand, a personalized federated learning method that extends FedAvg with a parameter decoupling strategy, aggregating shared (space-like) parameters globally while keeping personalized (time-like) parameters local.\nEmpirically, FlatLand outperforms Euclidean and prior hyperbolic baselines on multiple federated graph learning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work proposes a novel view of statistical heterogeneity that culminates in a novel algorithm that theoretically justifies how to treat joint vs individual knowledge among the clients during federation.\n\n- The proposed algorithm, FlatLand, is shown to converge and does not impose additional overhead compared to FedAvg. Moreover, it is shown that the dimensionality of the GNNs can be shrunk while still maintaining performance, yielding benign utility vs communication tradeoffs.\n\n- The experiment section includes multiple benchmarks and ablations."}, "weaknesses": {"value": "1) The motivation for the Hyperbolic approach is made solely from empirical observations in Fig 7-8. It is unclear how general these observations are, hence, FlatLand's applicability to general graphs.\n\n2) There are some inconsistencies in the paper. Some examples:\ni) the curvature is defined as -1/K in preliminaries but as -K in Theorem 1\nii) The Lorentz network is defined using W (sec. 2) but is later changed to M (sec 4.2).\niii) deviation -> derivation\n\n3) RQ3 is missing in the experimental section. It seems to be provided in Appendix E but it is not mentioned in the main body."}, "questions": {"value": "1) The paper assumes that real-world client graphs exhibit negative curvature and are therefore well modeled in hyperbolic space. What underlying mechanisms make this a reasonable assumption? is this an intrinsic property of the topology (e.g., scale-free structure), or simply an empirical regularity observed in selected benchmarks under the considered partitioning?\n\n2) If a few client graphs are approximately flat or positively curved, does the Lorentz formulation still provide meaningful embeddings and aggregation behavior, or does it introduce geometric distortion?\n\n3) Given that curvature initialization has only marginal impact and curvature is optimized jointly with model parameters, to what extent is the learned per-client curvature $K_c$ meaningful? Does it reflect the intrinsic graph geometry or simply act as a tunable scaling parameter?\n\n4) Theorem 2 claims that client heterogeneity lies in the time-like dimension based on mutual information. Given that mutual information is unchanged by coordinate transformations and doesn’t depend on geometry. How can one tell that this separation isn’t just a result of the chosen coordinate system, rather than an intrinsic property of the Lorentz space?\n\n5) In Fig. 6, it can be seen that Local(L) outperforms FlatLand for some clients, e.g., client 5 and client 6 on Cora and client 5 on Citeseer.  What governs which clients benefit or degrade?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "63Wl0dxgbo", "forum": "xEsJW4xqQC", "replyto": "xEsJW4xqQC", "signatures": ["ICLR.cc/2026/Conference/Submission23343/Reviewer_fYK9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23343/Reviewer_fYK9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601747150, "cdate": 1761601747150, "tmdate": 1762942616937, "mdate": 1762942616937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**FlatLand** proposes a personalized graph federated learning framework that embeds each client in a tailored **Lorentz (hyperbolic) space**, motivated by empirical observations that client graphs often exhibit *negative Ricci curvature* with substantial cross-client variability. The method decouples parameters into **time-like** (client-specific/heterogeneity) and **space-like** (shared) components, aggregating only shared parameters to avoid client-similarity estimation and auxiliary modules. Theoretically, the paper argues for *client-specific curvature* and shows that heterogeneity is encoded along the **time-like dimension**. Experiments on several graph datasets show *consistent gains* over Euclidean counterparts, especially in *low-dimensional settings* that are communication-efficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Originality**: Introduces a *geometric perspective* for PFL on graphs, leveraging tailored **Lorentz curvature per client** and a principled **time-like vs. space-like parameter decoupling**.\n- **Quality**: Provides *theoretical support* (necessity of tailored curvature; time-like dimension encodes heterogeneity) and a clear algorithmic instantiation with a fully Lorentz network. Empirical results show *consistent improvements*, notably in *low-dimensional regimes*.\n- **Clarity**: Overall clear motivation and framework; figures effectively convey intuition. Some sections (notation for Lorentz layers/curvature mapping) are dense but tractable.\n- **Significance**: Addresses a core pain point in graph FL—*heterogeneity*—without extra clustering or similarity estimation modules, suggesting a potentially *simpler and more general recipe* for PFL."}, "weaknesses": {"value": "- **Lack of empirical analysis** of relationship between curvature $K_c$ and data heterogeneity. A *sensitivity analysis* to mis-specified curvature would strengthen the claims.\n- **Ablations on decoupling**: While time-like vs. space-like decoupling is motivated theoretically, more ablations isolating these choices (e.g., aggregating subsets, partial decoupling) would clarify what drives gains.\n- **Scalability**: Discussion and measurements for very large graphs, many clients, and long training rounds are limited; communication-computation trade-offs vs. strong PFL baselines could be expanded."}, "questions": {"value": "1. **Curvature initialization and learning**: Curvature $K_c$ is initially estimated via Forman–Ricci curvature and is learnable during training. How large is the gap between the learned $K_c$ and its initialization? Does the learned curvature align with client heterogeneity? Please provide *visualizations or empirical analysis*.\n2. **Sensitivity analysis**: How sensitive is performance to mis-estimation? Please include an *ablation study* where the assigned curvature is perturbed.\n3. **Aggregation stability**: What constraints ensure stable aggregation when only space-like parameters are averaged? Any observed drift or incompatibility across clients with very different $K$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kSjQc7TYhA", "forum": "xEsJW4xqQC", "replyto": "xEsJW4xqQC", "signatures": ["ICLR.cc/2026/Conference/Submission23343/Reviewer_zZ4X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23343/Reviewer_zZ4X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929918453, "cdate": 1761929918453, "tmdate": 1762942616014, "mdate": 1762942616014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FlatLand, a personalized graph federated learning method that leverages tailored Lorentz spaces to address data heterogeneity. Key contributions include: 1) recognizing real-world client graphs’ inherent hyperbolic properties, 2) using Lorentz space’s time-like dimension to encode client-specific heterogeneity and space-like dimension to preserve shared knowledge, 3) a parameter decoupling strategy enabling direct aggregation without extra similarity estimation or auxiliary modules. Experiments on node/graph classification datasets demonstrate better performance over baselines, especially in low-dimensional scenarios. The work introduces a geometric perspective to PFL, with solid theoretical grounding and practical efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Innovative geometric design for graph federated learning：The paper abandons the Euclidean space limitation of existing PFL methods, adopts Lorentz space to model graph data, and verifies that real client graphs mostly have negative Ricci curvature with varying curvature. Customizing exclusive spaces for each client based on graph curvature fits the intrinsic properties of graphs, avoiding structural distortion in Euclidean space.\n2. Efficient parameter decoupling：The paper splits parameters into shared and personalized parts. Only shared parameters are aggregated, without extra client similarity estimation or auxiliary modules, balancing PFL needs while controlling overhead."}, "weaknesses": {"value": "1. Lack of learnable curvature details：It mentions curvature is learnable but fails to clarify update basis or curvature change impacts. Relying only on initial Forman-Ricci initialization, it’s unclear if curvature can match dynamic client data.\n2. Unaligned baseline parameter：When comparing with FedHGCN, FED-PUB (Table 1/2), it does not confirm if baselines’ key parameters (hidden dimension, local epochs) match FlatLand, making comparison results unreliable.\n3. Weak node classification performance：From Table 1, FlatLand lags FED-PUB on Cora (10 clients) and FedGTA on Photo; even on better datasets like CiteSeer, it only outperforms optimal baselines by less than 1 percentage points, with weak advantages."}, "questions": {"value": "1. For learnable curvature, could you supplement details like the basis for calculating curvature gradients and how to avoid numerical instability from excessive updates?\n2. Could you add experiments under extreme heterogeneity to clarify FlatLand’s applicable boundaries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M1DNoQf7rL", "forum": "xEsJW4xqQC", "replyto": "xEsJW4xqQC", "signatures": ["ICLR.cc/2026/Conference/Submission23343/Reviewer_JPEk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23343/Reviewer_JPEk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980267520, "cdate": 1761980267520, "tmdate": 1762942614154, "mdate": 1762942614154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of personalized graph federated learning where the data is highly heterogeneous among clients. By modeling the data in a Lorentzian hyperbolic space, the authors argue for a natural separation between client specific heterogeneous data, encoded in the time-like coordinate, and other homogeneous parts of the data, encoded in space-like coordinates. This leads to a procedure to decouple personalized model parameters from shared common parameters, resulting in a principled and efficient graph federated learning method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of modeling the data in a way that naturally separates heterogeneous parts of the data and allows for more efficient and principled personalized federated learning is an interesting approach to an important problem. The approach seems  novel and is interesting. The paper is well-structured with illustrative figures and  boxes to highlight important remarks. The experiments are extensive in the sense that many datasets and baseline methods are considered, with several good results in favor of the FlatLand method."}, "weaknesses": {"value": "The terminology used in regards to Lorentzian spaces and geometries does seem to be incorrect or imprecise. Technically, Lorentz spaces commonly refer to generalisation of $L^p$ spaces in functional analysis, which are different from Lorentzian spaces, that refer to pseudo-Riemannian metric spaces with time-like coordinates. However, we acknowledge that it is clear from the context that the authors are referring to the latter.\n\n The authors seem to use Lorentzian space and hyperbolic space interchangeably, although they are, in fact, two different concepts. A hyperbolic space has constant negative curvature, and such spaces exist in both Riemannian geometry and Lorentzian geometry (it is known as anti-de Sitter space in the Lorentzian case, which is the type of space the paper considers). Lorentzian geometry can also be flat (Minkowski space) or have constant positive curvature (de Sitter space). \n \n These distinctions have important implications for the paper. A technical implication is that the Lorentz transformations stated in the appendix, which the fully Lorentz neural networks build on, are only valid in flat Minkowski space. In curved spaces, the Lorentz symmetry is in general not global and meaningful Lorentz transformations can only be defined on local (Minkowski) tangent spaces. In the case of anti-de Sitter space, due to it being maximally symmetric, global generalized Lorentz transformations can be defined in a larger embedding space with 2 time-like coordinates, which is common practice in, e.g., string theory. This issue is not properly addressed in the paper and has implications for its overall soundness, including the \"correctness\" part of Section 5 and Lemma 8 and the design of the Lorentz neural networks.\n\nOn a more conceptual level, the paper does not clearly justify why a Lorentzian space with constant negative curvature (anti-de Sitter space) is needed, as opposed to the arguably more natural choice of a hyperbolic Riemannian geometry. For instance, in Section 6.4 \"The Necessity of Lorentz Space\", the negatively curved Lorentzian space is compared to a Euclidean space (flat Riemannian geometry). We believe a more fair comparison, given the negative curvature of the data, would be a Riemannian hyperbolic space. Furthermore, in the Lorentzian geometry, where the time-like and space-like coordinates are fundamentally different, there are physical notions of light-cones, causality and a speed limit. It is not explained how one should think about this in when modeling the data in a Lorentzian space.\n\nOn a more practical level, the Ricci curvatures takes only into consideration the graph structure and not the node features/labels. However, data heterogeneity in the distribution of features and labels arguably matters just as much. The paper does not address this question in a clear way.\n\nFinally, the paper motivates the federated learning scenario by its privacy-preserving capabilities. However, questions regarding the privacy of the proposed method are not addressed. How does the proposed method compare with other PFL approaches in terms of privacy?"}, "questions": {"value": "The paper raises several questions that I believe have to be addressed.\n\n1. Why is a Lorentzian space actually necessary, and why does a hyperbolic Riemannian space not suffice?\n \n2. The linear global Lorentz transformations as stated in the appendix are only valid for flat Minkowski space. In curved spaces, the Lorentz transformations can in general only be defined locally (on Minkowski tangent spaces). What are the Lorentz transformations for constant negative curved Lorentzian spaces, and how does this affect the validity of the Lorentz neural networks and the proof of Lemma 8?\n\n3. What is the interpretation of light-cones or boosts when modeling the graph data in a Lorentzian space?\n\n4. The Ricci curvature computation involves only the graph structure (topology) and does not account for heterogeneity in the features/labels of the data. Is this not a major limitation of the proposed approach?\n\n5. How does the FlatLand framework compare with other PFL methods in terms of privacy?\n\n6. The statistical results are only computed over 5 independent runs, yet it is claimed that bold numbers are statistically significant ($p<0.05$). We ask the authors to clarify this point; how is the statistical significance computed and guaranteed with such few sample"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "An ethics statement is included that adequately addresses potential concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FzECw95pCP", "forum": "xEsJW4xqQC", "replyto": "xEsJW4xqQC", "signatures": ["ICLR.cc/2026/Conference/Submission23343/Reviewer_j8kB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23343/Reviewer_j8kB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000828721, "cdate": 1762000828721, "tmdate": 1762942613784, "mdate": 1762942613784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}