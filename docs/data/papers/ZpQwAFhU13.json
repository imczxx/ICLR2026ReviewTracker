{"id": "ZpQwAFhU13", "number": 1473, "cdate": 1756885591270, "mdate": 1759898207474, "content": {"title": "Tree Search for LLM Agent Reinforcement Learning", "abstract": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs).\nIn long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision.\nTo address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step.\nBy sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls.\nMoreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward.\nBased on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels.\nThrough theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning.\nExperiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.", "tldr": "", "keywords": ["Tree Search", "LLM", "Agent", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3152bb12db118e7ae527fe4a5579eec09c19f34b.pdf", "supplementary_material": "/attachment/e2919657b0ffea648f189098ffc83187f4659468.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Tree-GRPO, a novel method designed to enhance LLM agents in long-term, multi-turn RL tasks. The authors address two critical challenges in current agentic RL: the heavy computational budget associated with LLM rollouts, and the sparse supervision problem arising from outcome-only rewards.\n\n1. Tree-GRPO proposes a tree-search based online rollout strategy where each tree node represents a complete agent interaction step (Thought-Action-Observation). By sharing common prefixes within the tree structure, the method significantly increases the number of effective rollouts under a fixed token/tool-call budget.\n2. This method leverages the tree structure to naturally derive fine-grained, step-wise process supervision signals purely from outcome rewards by estimating grouped relative advantages at both intra-tree and inter-tree levels.\n3. The paper theoretically demonstrates that intra-tree level group relative policy optimization is equivalent to step-level direct preference learning.\n4. Empirical evaluations across 11 datasets spanning Single-Hop, Multi-Hop, and hard Web-Agent QA tasks, using various LLM models, demonstrate Tree-GRPO's superiority over chain-based RL methods. Notably, it achieves higher performance with a significantly reduced rollout budget, particularly for smaller models and in complex multi-hop interactions.\n5. The paper also provides ablation studies on tree node granularity and tree structure parameters, as well as analyses on budget efficiency and the method's ability to encourage longer, more complex interactions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a relatively original approach by integrating tree-search sampling with GRPO for online LLM agent RL. The key innovation is the agent step-level node definition within the tree, which is a clever adaptation for efficient and meaningful process supervision in multi-turn agent tasks.\n2. The overall quality of the paper is exceptional. The method is clearly described, and the theoretical analysis provides valuable insights into why the method works. The experimental evaluation is thorough, covering different model families (Qwen, Llama), multiple sizes (1.5B-14B), and diverse task types (Single-Hop QA, Multi-Hop QA and Web-Agent QA). The detailed ablation studies, especially on the granularity of tree nodes (token/sentence vs. agent step) and tree structure parameters (M, N, L), examine important design choices.\n3. The paper is written with outstanding clarity. The explanations are precise, the technical details are well-articulated, and the figures effectively convey complex ideas.\n4. The work offers substantial significance to the field. It provides a highly effective and efficient method for training LLM agents, addressing the critical issues of sparse supervision and high computational costs. By showing how to derive fine-grained process signals from outcome rewards in an online setting, it opens new avenues for scalable and robust agent learning. The ability to encourage longer, more complex interactions is particularly important for the development of next-generation foundation models."}, "weaknesses": {"value": "1. Detailed Breakdown of Rollout Budget Savings: Although the paper emphasizes \"less rollout budget\" and shows its effectiveness, a direct quantitative comparison of the actual training time and memory consumption (e.g., GPU hours, peak memory usage) between Tree-GRPO and chain-based GRPO would provide a more complete picture of its efficiency. While \"less rollout budget\" implies efficiency, explicit benchmarks on wall-clock time for training on large models would solidify this claim, especially since tree search itself can introduce overheads. This would complement the token/tool-call budget analysis.\n2. Restrictive Theoretical Assumptions: Assumption 3.1 (binary preference setting) is quite strong and unlikely to hold with continuous outcome rewards. The theoretical analysis would be more convincing with relaxed assumptions or empirical validation that the approximation holds reasonably well.\n3. Inconsistent Improvements: Performance gains vary considerably across settings. On single-hop QA with larger models (14B), improvements are marginal (~1%). On web-agent tasks, gains are limited by training data quality. This suggests the method's benefits are context-dependent.\n4. Experimental Limitations: There lackscomparison with explicit process reward models, which would be a natural baseline. There is also missing analysis of computational overhead for tree construction/management during training.\n5. Sensitivity Analysis for Key Hyperparameters Beyond Tree Structure: The paper conducts an excellent ablation on tree structure parameters (M, N, L) and shows LR warmup sensitivity. However, a more comprehensive sensitivity analysis for other critical hyperparameters, such as the KL coefficient or max response length across different tasks, could provide further guidance for practitioners on how to tune Tree-GRPO effectively. For example, how robust is the method to changes in the KL regularization term, which is often crucial in RL?"}, "questions": {"value": "1. In Figure 1, the paper claims \"less rollout budget (both on tokens and tool-calls)\", and mentions achieving 1.5x samples under the same budget later. Does there exist a more detailed breakdown of how these savings in tokens and tool-calls are realized in practice?\n2. What is the actual wall-clock time comparison between chain-based and tree-based training? Considering the complexity of tree management, does this offset the token savings?\n3. How does your implicit process supervision compare to explicit process reward models when both are available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "85eZxLJqdW", "forum": "ZpQwAFhU13", "replyto": "ZpQwAFhU13", "signatures": ["ICLR.cc/2026/Conference/Submission1473/Reviewer_uD4w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1473/Reviewer_uD4w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794286741, "cdate": 1761794286741, "tmdate": 1762915778030, "mdate": 1762915778030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new efficient tree-search rollout approach for training an RL-based LLM agent in which each node of the rollout tree represents a complete thought-action-observation step of the LLM agent. This tree-search based method is shown to obtain more samples than existing chain-based method. The paper then introduces a group-relative advantage estimation at both intra and inter-tree levels, incorporating preference objectives while leveraging rollouts from all trees, allowing for a more stable training. Experiments across different settings including multi-hop QA, single-hop QA, and web-agent QA are conducted, showing improved performance of the proposed method compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written. Explanations on how rollout search trees are built and how group relative advantages are computed are easy to follow.\n2. The idea of rollout search trees based on a complete thought-action-observation step is promising as it is shown to be cost-effective in collecting samples. \n3. Experiments are extensive. Results on various QA settings show that the proposed method outperforms other SOTA baselines in most of the evaluated settings."}, "weaknesses": {"value": "1. In sparse-reward RL, there is a long line of research on credit assignments or reward redistributions. There lacks discussions on why these existing sparse-reward RL algorithms were not applied in this RL-based LLM agent. \n2. In the proposed tree-based credit assignment method, it appears that stepwise advantages are identical across all time steps within each trajectory. This uniform credit distribution is quite simplistic and may fail to capture the varied contributions of different steps to overall performance."}, "questions": {"value": "1. Could you please justify why uniform distribution of credits across different steps is reasonable in this learning setting?\n2. Can you provide some thoughts on applying existing sparse-reward RL methods for LLM agents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Au2lWED56Q", "forum": "ZpQwAFhU13", "replyto": "ZpQwAFhU13", "signatures": ["ICLR.cc/2026/Conference/Submission1473/Reviewer_TDPe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1473/Reviewer_TDPe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946379088, "cdate": 1761946379088, "tmdate": 1762915777920, "mdate": 1762915777920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper \"Tree Search for LLM Agent Reinforcement Learning\" introduces Tree-based Group Relative Policy Optimization (Tree-GRPO), a novel method to address the high costs and sparse supervision challenges in training LLM agents for multi-turn tasks . Instead of inefficient \"chain-based\" rollouts, Tree-GRPO employs a tree-search sampling strategy where each node represents a complete agent step (Thought, Action, Observation). This structure significantly improves budget efficiency by sharing common prefixes and, crucially, allows the model to derive fine-grained, step-level process supervision signals directly from sparse outcome rewards by comparing the performance of different branches. The paper's key contributions include this highly efficient agent-step-level tree-search method, an implicit preference learning objective that is theoretically shown to be structurally equivalent to step-level DPO , and strong empirical results demonstrating superior performance over baselines while using as little as one-quarter of the rollout budget."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper re-frames the inefficient \"chain-based\" sampling process  as a problem that can be optimized with a \"tree-based\" shared-prefix structure.\n- The authors don't just propose a method; they provide a theoretical justification (Proposition 3.1) that connects their intra-tree advantage estimation to the well-established DPO framework , lending it significant credibility.\n- The quality is further reinforced by a series of insightful ablation studies. These include testing performance under different rollout budgets, analyzing the contribution of the $\\hat{A}_{intra-tree}$ and $\\hat{A}_{inter-tree}$ advantage components, and comparing against the intuitive token-level tree search."}, "weaknesses": {"value": "- While Tree-GRPO showed consistent improvements, the authors note that for highly challenging web-agent benchmarks like BrowseComp, the gains from RL (both chain-based and tree-based) were marginal. They attribute this primarily to the limited quality and difficulty of the available training data rather than a failure of the method itself .\n- The method introduces new hyperparameters that govern the tree structure, specifically the number of initial trees ($M$), the number of nodes to expand ($N$), and the number of expansion iterations ($L$). This will introduce additional hyperparameter choice under different situations."}, "questions": {"value": "- In Table 4, it seems that intra-tree advantage will lead to collapse and the main performance gain is from the inter-tree. So the mainly the advantage estimated by different initialization contribute most to the performance. So, it seems that large $M$ is more important than large $N$ and large $L$. Is that understanding correct? \n- How do you choose M,N,L in prior? In practice, it seems that $L=1$ is the best choice. But how to balance $M$ and $N$ given the same computational budget? For example, to choose larger M or larger N given the same budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7eeJWct4k3", "forum": "ZpQwAFhU13", "replyto": "ZpQwAFhU13", "signatures": ["ICLR.cc/2026/Conference/Submission1473/Reviewer_PcpF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1473/Reviewer_PcpF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973356337, "cdate": 1761973356337, "tmdate": 1762915777768, "mdate": 1762915777768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}