{"id": "iQsKotob31", "number": 4, "cdate": 1756728033368, "mdate": 1759898279036, "content": {"title": "HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant potential to advance a broad range of domains. However, current benchmarks for evaluating MLLMs primarily emphasize general knowledge and vertical step-by-step reasoning typical of STEM disciplines, while overlooking the distinct needs and potential of the Humanities and Social Sciences (HSS). Tasks in the HSS domain require more horizontal, interdisciplinary thinking and a deep integration of knowledge across related fields, which presents unique challenges for MLLMs, particularly in linking abstract concepts with corresponding visual representations. Addressing this gap, we present HSSBench, a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks in multiple languages, including the six official languages of the United Nations. We also introduce a novel data generation pipeline tailored for HSS scenarios, in which multiple domain experts and automated agents collaborate to generate and iteratively refine each sample. HSSBench contains over 13,000 meticulously designed samples, covering six key categories. We benchmark more than 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant challenges even for state-of-the-art models. We hope that this benchmark will inspire further research into enhancing the cross-disciplinary reasoning abilities of MLLMs, especially their capacity to internalize and connect knowledge across fields.", "tldr": "", "keywords": ["MLLMs", "Benchmark", "Dataset", "Humanities and Social Sciences"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff3c4ae9941ee045727fd87e2601e013ed3c6f69.pdf", "supplementary_material": "/attachment/c208fbf6969a7a84b43b0fc88841c399c07c508e.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents HSSBench, a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks in multiple languages, including the six official languages of the United Nations. The paper also introduces a novel data generation pipeline tailored for HSS scenarios, in which multiple domain experts and automated agents collaborate to generate and iteratively refine each sample. HSSBench contains over 13,000 meticulously designed samples, covering six key categories."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper convincingly identifies and addresses the significant bias in current MLLM evaluations towards STEM and \"vertical reasoning,\" providing a much-needed tool to assess the \"horizontal reasoning\" required for HSS.\n2. With 13,152 samples across 6 major categories and 45 specialized subtypes, the dataset is substantial and covers the HSS domain broadly and systematically.\n3. The inclusion of six UN official languages significantly enhances the benchmark's utility for studying MLLM performance across diverse linguistic and potentially cultural contexts."}, "weaknesses": {"value": "1. The automated portion of the VGP heavily relies on powerful, closed-source MLLMs (GPT-4o/4.1) for tasks like summarization, extraction, and question generation. While expert validation is used, this reliance on the very class of models being tested raises concerns about potential model-specific bias or \"leakage\" in the generated data subset.\n2. I encourage the authors to evaluate more closed-source models such as seedvl-1.6 and gemini"}, "questions": {"value": "Please refer to the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Dmp6r4zhoF", "forum": "iQsKotob31", "replyto": "iQsKotob31", "signatures": ["ICLR.cc/2026/Conference/Submission4/Reviewer_Tyr7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4/Reviewer_Tyr7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761187891091, "cdate": 1761187891091, "tmdate": 1762915435239, "mdate": 1762915435239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HSSBench, a new large-scale, multilingual benchmark designed to evaluate the capabilities of Multimodal Large Language Models (MLLMs) in the Humanities and Social Sciences (HSS). The authors argue that existing benchmarks predominantly focus on STEM or general knowledge, overlooking the unique \"horizontal\" reasoning—interdisciplinary, context-dependent, and abstract—required for HSS tasks. The benchmark comprises over 13,000 visual question-answering samples across six categories and 45 subtypes, available in the six official UN languages. A key contribution is a novel data generation pipeline that combines domain expert knowledge with a multi-agent automated system. The authors benchmark over 20 MLLMs, demonstrating that HSSBench poses a significant challenge even for state-of-the-art models, whose performance often falls below 60%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work compellingly argues for and addresses the lack of dedicated, in-depth benchmarks for HSS domains, which require different reasoning skills than typical STEM tasks (Section 1).\n- The paper proposes a sophisticated VQA Generation Pipeline (VGP) that leverages both domain experts and a multi-agent framework (Figure 3, Section 2). The multi-stage validation process (Section 2.3) ensures data quality and that questions are truly multimodal.\n- The paper provides useful qualitative analyses, such as the performance drop on open-ended questions (lines 388-390) and the inconsistent benefit of CoT prompting (lines 370-373), which reveal key limitations of current models."}, "weaknesses": {"value": "-  The group of human experts appears to have a strong majority of Chinese speakers (Table 2, Appendix A.2). While the authors acknowledge this may benefit certain models like Qwen (lines 362-365) and describe mitigation efforts (lines 818-827), this demographic skew could introduce subtle cultural biases into the dataset's content and framing, despite best efforts.\n- The multilingual aspect of the benchmark is created by translating an original set of questions using LLMs, followed by expert validation (lines 268-269). This is a practical approach, but it may not capture the full cultural and linguistic nuance that would be present in questions originally authored in each target language.\n-  The evaluation of Retrieval-Augmented Generation (RAG) in Appendix C.7 shows that general-purpose retrieval is ineffective, which is an interesting but not entirely surprising finding. This section could be strengthened by discussing what a more effective, domain-specific RAG approach for HSS might entail."}, "questions": {"value": "1. Could you elaborate further on the validation process used by bilingual experts for the translated questions (lines 268-269)? What was the rate of rejection or significant modification, and were there particular language pairs or HSS domains that proved more challenging to translate accurately?\n2. The expert demographics (Table 2) are heavily skewed towards Chinese speakers. Beyond the performance advantage for certain models, how did you ensure that the core concepts and visual materials selected are globally representative and not biased towards a specific cultural perspective? The process described in lines 818-822 is noted, but more detail would be helpful.\n3. The multi-agent construction pipeline (Section 2.2) is a key contribution. Could you provide more detail on the specific guidelines and examples provided to the question generator agent (lines 216-220)? The quality of this automated step seems critical to the overall quality of the benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VwN3lqZNuo", "forum": "iQsKotob31", "replyto": "iQsKotob31", "signatures": ["ICLR.cc/2026/Conference/Submission4/Reviewer_geww"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4/Reviewer_geww"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621966171, "cdate": 1761621966171, "tmdate": 1762915435091, "mdate": 1762915435091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HSSBench, a new 13,000-sample multilingual VQA benchmark for evaluating MLLMs in the Humanities and Social Sciences (HSS). The authors argue current benchmarks focus on STEM-based \"vertical reasoning,\" while their new dataset, created via a novel expert-and-agent pipeline, targets more nuanced \"horizontal, interdisciplinary thinking\". Experiments show that even top MLLMs struggle with these tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Addresses a Clear Gap:** The paper convincingly argues for the need for a benchmark beyond STEM, focusing on the challenging and underserved HSS domain.\n* **High-Quality Data Pipeline:** The 3-stage VGP, combining domain experts and agents, is a robust methodology. The validation step to ensure true multimodality (checking text-image dependencies) is a key strength.\n* **Insightful Analysis:** The findings are valuable, particularly that CoT can *increase* hallucinations on HSS tasks  and that standard RAG provides no consistent benefit, highlighting the unique challenges of this domain."}, "weaknesses": {"value": "* **Format Mismatch:** The paper's stated goal is to test \"horizontal reasoning\" (implying divergent thought and multiple interpretations), but the benchmark uses an MCQ format, which enforces a single correct \"vertical\" answer.\n* **Potential Cultural Bias:** The authors disclose that \"most of the data experts... are Chinese\". This poses a significant risk of cultural bias in a global benchmark, a limitation the authors concede may have skewed the results (e.g., Qwen outperforming GPT-4o in some categories).\n* **Limited Multilingual Evaluation:** While the dataset supports 6 languages, the main evaluation is English-only. Full Chinese evaluation is in the appendix, but the other 4 languages are only tested on a small 900-item sample, which doesn't fully substantiate the multilingual claim."}, "questions": {"value": "1.  Could you elaborate on the tension between the single-correct-answer MCQ format and your definition of \"horizontal reasoning\" as supporting \"multiple valid interpretations\"?\n2.  Regarding the expert demographics, what specific validation steps were taken to ensure the data was free from a specific cultural perspective, especially in the Social Science and History categories?\n3.  Was the full 13,152-item benchmark evaluated on all six UN languages? If not, Table 10 (using a 900-item sample) seems to be the only multilingual evaluation for 4 of the 6 languages. Could you clarify?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QRC3A5I9VH", "forum": "iQsKotob31", "replyto": "iQsKotob31", "signatures": ["ICLR.cc/2026/Conference/Submission4/Reviewer_6Fgp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4/Reviewer_6Fgp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803969541, "cdate": 1761803969541, "tmdate": 1762915434957, "mdate": 1762915434957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HSSBench, a large-scale benchmark designed to evaluate multimodal large language models (MLLMs) in humanities and social sciences (HSS) domains. The benchmark comprises 13,152 multiple-choice visual question-answering samples spanning six major categories (Economics, Art, Culture, Social Science, History, and Geography) with 45 professional subtypes, and supports six UN official languages. The authors propose a VQA Generation Pipeline (VGP) that combines domain expert curation with automated agent-based generation and validation. Experimental evaluation of over 20 state-of-the-art MLLMs demonstrates that HSSBench poses significant challenges, with most models achieving accuracy below 60%, substantially lower than their performance on existing benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**[+] Comprehensive and Challenging Benchmark:** The paper presents a valuable evaluation resource with strong empirical validation. The experimental results (particularly in Appendix C.5) convincingly demonstrate that HSSBench captures unique challenges in HSS domains. For instance, InternVL3-8B achieves 77.21% on MME and 68.10% on MMMU but only 42.14% on HSSBench-Art, showing that the benchmark effectively tests capabilities that existing benchmarks may not fully capture.\n\n**[+] Unprecedented Depth and Multilingual Coverage:** I appreciate the fine-grained categorization into 45 professional subtypes within HSS domains, which provides valuable diagnostic capability for understanding model strengths and weaknesses at a granular level. The support for six UN official languages is particularly commendable, as it enables cross-cultural and cross-linguistic evaluation that is often missing in existing benchmarks.\n\n**[+] Transparent Data Construction Process:** The paper provides good transparency about the data generation pipeline, including the proportion of expert-curated versus agent-generated samples and quality validation procedures. The multi-stage verification process involving both domain experts and automated agents demonstrates careful attention to data quality.\n\n**[+] Thorough Experimental Analysis:** I find the experimental design to be comprehensive, including evaluations across different prompting strategies (CoT vs. Direct), question formats (multiple-choice vs. open-ended), and ablation studies examining the impact of visual information, distractors, and RAG augmentation."}, "weaknesses": {"value": "**[-] Positioning and Novelty Claims Could Be More Precise:** I observed that the paper states existing benchmarks have been \"overlooking\" HSS domains and positions HSSBench as \"addressing this gap.\" However, I note that established benchmarks like MMMU and CMMMU already include \"Humanities & Social Science\" and \"Art & Design\" as core evaluation categories. I believe the paper's true contribution lies in providing deeper granularity (45 subtypes vs. broader categories) and expanded multilingual support, rather than being the first to cover HSS. I would encourage the authors to reframe their contribution as deepening and extending HSS evaluation, which would more accurately represent the work's value while properly acknowledging prior efforts in this space.\n\n**[-] Methodological Contribution Needs Clearer Differentiation:** The VGP pipeline is presented as a key contribution, but I find that similar expert-AI collaborative pipelines have been employed in concurrent work (such as MicroVQA for biological domains). The \"expert curation → LLM-assisted generation → agent refinement → expert validation\" workflow appears to be an emerging best practice in the field rather than a novel methodological innovation. I suggest repositioning VGP as a successful adaptation of this emerging paradigm to HSS domains rather than as an independent methodological contribution.\n\n**[-] Data Quality Validation Could Be More Comprehensive:** While the paper describes quality assurance procedures, I would have appreciated more quantitative validation metrics. Specifically: (1) inter-annotator agreement measures (e.g., Fleiss' Kappa) for expert-curated samples, (2) more extensive comparison between expert-generated and agent-generated data quality beyond the single model shown in Table 6, and (3) consistency metrics for the multilingual versions to ensure equivalence across translations."}, "questions": {"value": "**Q1:** Given that MMMU and CMMMU include dedicated HSS evaluation categories, could you clarify the specific incremental value that HSSBench provides? I would find it helpful to see a detailed comparison showing how the 45 professional subtypes enable evaluation insights that cannot be obtained from existing benchmarks' broader HSS categories.\n\n**Q2:** Could you provide more systematic evidence of data quality consistency? \n\n**Q3:** Regarding the VGP pipeline, could you elaborate on what aspects are specifically tailored to HSS domains compared to similar pipelines used in other domains? This would help clarify the domain-specific insights that emerged from your data construction experience."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XcBojUK6zQ", "forum": "iQsKotob31", "replyto": "iQsKotob31", "signatures": ["ICLR.cc/2026/Conference/Submission4/Reviewer_vNZD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4/Reviewer_vNZD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159704204, "cdate": 1762159704204, "tmdate": 1762915434739, "mdate": 1762915434739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}