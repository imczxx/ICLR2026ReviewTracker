{"id": "gLCKZEjmWg", "number": 9026, "cdate": 1758107743673, "mdate": 1759897747532, "content": {"title": "Sample by Step, Optimize by Chunk: Chunk-Level GRPO for Text-to-Image Generation", "abstract": "Group Relative Policy Optimization (GRPO) has shown strong potential for flow-matching-based text-to-image (T2I) generation, but it suffers from two key limitations: inaccurate advantage assignment, and the neglect of temporal dynamics of generation. In this work, we argue that shifting the optimization paradigm from the step level to the chunk level can effectively alleviate these issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level GRPO-based approach for T2I generation. The central insight is to group consecutive steps into coherent 'chunk’s that capture the intrinsic temporal dynamics of flow matching, and to optimize policies at the chunk level. In addition, we introduce an optional weighted sampling strategy to further enhance performance. Extensive experiments show that Chunk-GRPO achieves superior results in both preference alignment and image quality, highlighting the promise of chunk-level optimization for GRPO-based methods.", "tldr": "", "keywords": ["GRPO", "text-to-image generation", "reinforcement learning", "flow matching"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea3ae538b918e4e2e7efb445c6d560b685bc9fb6.pdf", "supplementary_material": "/attachment/666fce2517de1cc68efe65f9359a2b4c3e5a64d2.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Chunk-GRPO, a novel reinforcement learning optimization paradigm for flow-matching-based text-to-image (T2I) generation. The method extends the Group Relative Policy Optimization (GRPO) by addressing two of its core limitations: (1) inaccurate advantage attribution across timesteps and (2) the neglect of temporal dynamics.\n\nInstead of applying optimization at every single generation step, the proposed method groups consecutive timesteps into temporally coherent chunks and performs optimization at the chunk level. The paper also proposes an optional weighted sampling strategy, which biases optimization toward chunks that correspond to higher-noise regions of the generation trajectory.\n\nThe authors present both theoretical justification and comprehensive experiments demonstrating improvements in preference alignment and image quality over prior methods such as Dance-GRPO."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-organized and easy to follow. \n2. Chunk-GRPO is conceptually straightforward, requires only minor modifications to existing GRPO frameworks, and can be easily implemented.\n3. The paper includes a clean mathematical analysis showing why chunk-level optimization yields smoother gradients and more accurate updates under imperfect advantage attribution.\n4. The analysis of chunk sizes and the discussion of how temporal dynamics guide segmentation provide clear practical guidance for choosing chunk configurations in future applications.\n5. The authors perform extensive experiments across multiple datasets and reward models, demonstrating consistent gains in preference alignment and image fidelity."}, "weaknesses": {"value": "1. Missing significance analysis. Reported metrics are presented without standard deviations. Without these, it is difficult to assess whether the observed improvements are statistically significant.\n2. A user study is missing. Since the paper focuses heavily on preference alignment and perceptual image quality, a user study would provide validation of the improvements.\n3. The improvement is rather small, especially considering the results on WISE."}, "questions": {"value": "1. Have you experimented with adaptive chunking, where the chunk boundaries evolve during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hdJ5VmXmOo", "forum": "gLCKZEjmWg", "replyto": "gLCKZEjmWg", "signatures": ["ICLR.cc/2026/Conference/Submission9026/Reviewer_xpH9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9026/Reviewer_xpH9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817560970, "cdate": 1761817560970, "tmdate": 1762920747448, "mdate": 1762920747448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that existing reinforcement learning methods for text-to-image generation place excessive emphasis on global advantages while neglecting local advantages, which may prevent trajectories from converging to the optimal solution. The authors propose a chunk-GRPO approach that segments the generated trajectory into chunks and computes advantages for each segment to address this issue. In addition, temporal dynamics is incorporated to dynamically adjust chunk sizes. Experimental results demonstrate improved performance on text-to-image generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1)\tThe authors propose the idea of incorporating both global and local advantages to evaluate the optimality of trajectory sequences, which is an interesting direction worthy of further exploration.\n(2)\tThe proposed temporal dynamics method avoids complex hyperparameter configurations, thereby enhancing the generality of the approach."}, "weaknesses": {"value": "(1)\tMore intuitive results: The example provided in Figure 2 of the paper is merely a schematic illustration. The authors are encouraged to present real image cases demonstrating whether, during the early or middle stages of generation, intermediate images exhibit higher quality, yet the final convergence results in an inferior output.\n(2)\tMotivation concern: Although the authors argue that certain steps in the generation process may possess local advantages, I believe that a well-formed generation trajectory does not—and need not—ensure local optimality at every step. The objective of reinforcement learning should still focus on achieving global optimality.\n(3)\tExperimental concerns: Based on the experimental results presented in the tables, the performance gains from chunking are minimal. This further calls into question the necessity of the chunking operation."}, "questions": {"value": "All of my concerns are presented in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2MsC9fY042", "forum": "gLCKZEjmWg", "replyto": "gLCKZEjmWg", "signatures": ["ICLR.cc/2026/Conference/Submission9026/Reviewer_k7kg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9026/Reviewer_k7kg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965783456, "cdate": 1761965783456, "tmdate": 1762920747048, "mdate": 1762920747048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Chunk-Level GRPO, which shifts GRPO optimization from step-level to chunk-level for flow-matching text-to-image models to address inaccurate advantage attribution and the neglect of temporal dynamics during generation.​ Consecutive timesteps are grouped into chunks guided by prompt-invariant relative L1 latent dynamics, and policies are optimized using a chunk-level importance ratio, with an optional weighted sampling strategy that emphasizes high-noise chunks.​ Experiments on FLUX Dev with HPDv2.1 show consistent gains in preference alignment (HPSv3, ImageReward) and competitive WISE benchmark results, alongside ablations on chunk configurations, per-chunk training, and reward-model robustness, plus analysis of a stability trade-off introduced by weighted sampling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation claimed in Figure 1 is very interesting and insightful. Additionally, the findings on temporal dynamics are beneficial to the community.\n\n- Chunk boundaries are informed by prompt-invariant temporal dynamics via relative L1 distance, yielding a principled, dynamics-aware segmentation rather than arbitrary chunking."}, "weaknesses": {"value": "I thank the authors for their efforts in this work. Below are some concerns about this paper.\n- This work claims to be the first “chunk‑level” method but does not compare against other GRPO variants like Flow-GRPO, Pref-GRPO, it cited, weakening the contribution boundary beyond a single Dance‑GRPO baseline. Moreover, the proposed method performs only on par with Dance‑GRPO on WISE.\n\n- The chunking implementation is heuristic. Boundaries are precomputed from relative L1 latent dynamics and kept fixed, lacking adaptivity and making performance sensitive to the sampling step $T$, the model, and certain prompts.\n\n- Despite the insight the authors claimed in Figure 1, such a chunk-based design did not show an optimal approach to solving such issues. In some cases, there are still issues, such as $ Chunk_{1}$ has the greater final reward (advantage), its $t=1$ timestep is worse\nthan that in $Chunk_{2}$."}, "questions": {"value": "Please see the #Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K9yd8obPFQ", "forum": "gLCKZEjmWg", "replyto": "gLCKZEjmWg", "signatures": ["ICLR.cc/2026/Conference/Submission9026/Reviewer_VUdY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9026/Reviewer_VUdY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985382694, "cdate": 1761985382694, "tmdate": 1762920746722, "mdate": 1762920746722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method that involves fine-tuning the image generation process at the chunk level instead of at separate timesteps. Furthermore, it introduces a weighted sampling strategy derived from the proposed chunk split design."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea of optimizing at the chunk level instead of at separate timesteps is an interesting and novel direction for image generation.\n2. The proposed method is explained with clarity."}, "weaknesses": {"value": "1. Proposition 1 and its corresponding proof raise concerns.\nSpecifically, Proposition 1 claims that a smaller chunk size leads to better performance. Since sampling through separate timesteps is equivalent to sampling with a chunk size set to $K=1$, the authors should further clarify the optimal limit for \"how small is enough.\"\n\n2. The proof of Proposition 1 also presents issues.\nEq. (35) states that $J_{\\text{chunk}} = \\frac{1}{T}J_{\\text{GRPO}}$, suggesting the objective function for the proposed chunk split is simply a scaled version of the original GRPO objective.\nWhile the optimal parameters $\\theta_{\\text{chunk}} = \\arg\\max_{\\theta} J_{\\text{chunk}}(\\theta)$ and $\\theta_{\\text{GRPO}} = \\arg\\max_{\\theta} J_{\\text{GRPO}}(\\theta)$ would be mathematically equal due to scaling, the proof's objective should focus on demonstrating how the policy parameters $\\theta$ are affected by the chunking scheme, rather than comparing the squared errors in the form $\\|\\hat{J}(\\theta) - J_{\\text{GRPO}}(\\theta)||^{2} \\geq \\|\\hat{J}(\\theta) - J_{\\text{chunk}}(\\theta)||^{2}$. The change in the optimal parameter $\\theta$ should be explicitly shown. \n\nMinors:\n1. Typo errors were noted in some equations (e.g., in **Eq. (18)**, the notation should likely be $T_a \\cup T_{ia} = \\{1, 2, \\cdots, T\\}$).\n2. The experimental results appear to show **only a marginal improvement** over the current state-of-the-art method, **Dance-GRPO**."}, "questions": {"value": "1. Could the authors provide a more detailed justification for the selected chunk sizes of $[2, 3, 4, 7]$? (e.g provide the specific details of the $\\ell_1(x, t)$ values across all timesteps to validate the chunk split design)\n2. Could the authors elaborate on why the final attained policy parameters $\\theta$ yield superior results? Is this improvement primarily attributable to enhanced stability in the reinforcement learning training process, or is there another underlying mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3Vp4Ov3Rhc", "forum": "gLCKZEjmWg", "replyto": "gLCKZEjmWg", "signatures": ["ICLR.cc/2026/Conference/Submission9026/Reviewer_ZLe3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9026/Reviewer_ZLe3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101571480, "cdate": 1762101571480, "tmdate": 1762920746351, "mdate": 1762920746351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}