{"id": "rRjYHBHq8X", "number": 13721, "cdate": 1758221584228, "mdate": 1763654046530, "content": {"title": "Bayesian Symbolic Regression with Entropic Reinforcement Learning", "abstract": "Symbolic regression is the problem of finding an algebraic expression describing a stochastic dependence of a target variable on a set of inputs. Unlike forms of regression that fit parameters assuming a fixed model structure, symbolic regression is a search problem over the space of expressions, represented, for example, as abstract syntax trees using a library of operators. Symbolic regression is typically used in settings with limited, noisy data in the natural sciences. However, searching for a single best-fitting expression fails to capture the epistemic uncertainty about the expression, which motivates a Bayesian perspective that enables uncertainty quantification and specification of natural priors to constrain the search space. In this work, we propose ERRLESS (Entropy-Regularised Reinforcement Learning for Expression Structure Sampling), a scalable approach for sampling the posterior distribution over expressions given data using maximum-entropy reinforcement learning. ERRLESS learns a neural policy that constructs expressions sequentially by building up their abstract syntax trees. At convergence, the policy samples expressions from the posterior. At test time, expressions can be sampled by rollouts of this policy. We demonstrate that ERRLESS achieves near state-of-the-art exact symbolic recovery on the AI Feynman benchmark. Beyond exact recovery, we demonstrate that the mean of the posterior predictive approximated by ERRLESS achieves a coefficient of determination ($R^2$) of $0.98$, highlighting the benefits of the Bayesian perspective in symbolic regression.", "tldr": "We use maximum-entropy reinforcement learning to approximate a posterior over expressions for symbolic regression.", "keywords": ["maximum-entropy reinforcement learning", "symbolic regression", "bayesian methods", "uncertainty quantification"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6765faeaaa703221abac6bcd3b8f7f2d8b586617.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper study the a Bayesian symbolic regression (SR) framework and proposes  Entropy-Regularized Reinforcement Learning for Expression Structure Sampling \n\nERRLESS models a posterior distribution over expressions using maximum-entropy reinforcement learning (RL). The core idea is to amortize posterior sampling of symbolic expressions using a neural policy trained via trajectory balance (a GFlowNet-style objective)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Solid mathematical definition. The paper provides formal definitions of priors, likelihoods, unit constraints, and posterior training objectives.\n- Novel idea on Translating Bayesian symbolic regression into a maximum-entropy RL setting."}, "weaknesses": {"value": "1. **Organization and Focus of the Paper**\n\n   * The paper’s organization is not well-balanced. A large portion of the text focuses on the details of post-order traversal and unit constraints, which could be greatly simplified or moved to the appendix.\n   * Instead, the section on translating Bayesian symbolic regression into a **maximum-entropy reinforcement learning (RL)** framework should be expanded and clearly explained.\n   * The motivation and benefits of using **maximum-entropy RL** are not adequately discussed — the authors should explicitly explain why this formulation is preferable compared to standard RL or other probabilistic inference methods.\n\n2. **Comparison to Deep Symbolic Regression (DSR)**\n\n   * The authors should more clearly differentiate their approach from **Deep Symbolic Regression**, which also employs RL and includes an entropy regularizer. It is unclear what the theoretical or practical advantage of the proposed method is relative to DSR.\n\n3. **Clarification of Prior Work Description**\n\n   * The statement that existing methods “have the goal of finding a single best expression” is inaccurate. Many symbolic regression methods explicitly search for a **Top-K set** or a **Pareto front** of optimal expressions under different objectives (e.g., accuracy and simplicity). The description of prior work should be corrected accordingly.\n\n4. **Comparison with Monte Carlo Tree Search-based Methods is missing.**\n\n5. **Improvement of Figure 1**\n\n   * Figure 1 currently illustrates only the internal mechanism of the proposed approach. It should also highlight the **advantages or improvements** of this method compared to existing baselines — for example, how it better models uncertainty, enforces constraints more effectively, or improves sampling efficiency."}, "questions": {"value": "- Could you clarify the conceptual and practical differences between deep reinforcement learning with entropy regularization and maximum-entropy reinforcement learning (MaxEnt RL)? \n- What concrete advantages did you observe in your experiments when using MaxEnt RL compared to standard RL with entropy regularization\n- What underlying factors contribute to this improvement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Vj6vsIIFp9", "forum": "rRjYHBHq8X", "replyto": "rRjYHBHq8X", "signatures": ["ICLR.cc/2026/Conference/Submission13721/Reviewer_VK1w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13721/Reviewer_VK1w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772321077, "cdate": 1761772321077, "tmdate": 1762924265497, "mdate": 1762924265497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to all reviewers"}, "comment": {"value": "We thank the reviewers for their efforts in reviewing our paper. We answer each review separately, but in this comment we address some recurring points that have been raised.\n\n### Theoretical and methodological comparisons\n\nMany reviewers asked what distinguishes our work from existing reinforcement-learning-based approaches for symbolic regression, namely DSR [1] and PhySO [2].\n\n**Similarities:** As noted by several reviewers (and discussed in our related work section), ERRLESS, DSR, and PhySO all train a neural-network policy via a reinforcement learning objective. All three incorporate mechanisms to regularize entropy. In addition, both ERRLESS and PhySO enforce unit constraints during expression-tree generation to reduce the effective search space. Because ERRLESS employs a novel *bottom-up* generation procedure, our unit-constraint mechanism required a redesigned algorithm despite this shared high-level objective.\n\n**Differences/Novelties:** We highlight two central novelties that distinguish ERRLESS from prior work:\n\n1. **Explicitly encoding an expression prior in the reward.** Both DSR and PhySO use the reward $1 / (1 + \\text{NRMSE})$, where for an expression tree $(T, \\theta)$ with evaluation function $f_{T,\\theta}$,\n\n    $$\n    \\text{NRMSE}(\\text{T}, \\theta) = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\frac{(y_i - f_{T,\\theta}(x_i))^2}{\\sigma_y^2}},\n    $$\n    \n    This quantity is the square root of a negative log-likelihood under i.i.d. Gaussian noise with variance $\\sigma_y^2$. Consequently, the reward in DSR and PhySO (1) **does not have a natural probabilistic interpretation as a likelihood** and (2) **reflects only data fit and does not model any prior over equations**. While both methods use “masking priors” to invalidate grammatically or dimensionally inconsistent operators, these masks enforce syntactic constraints rather than expressing a well-specified probabilistic prior over equations.\n    \n    In contrast, the ERRLESS reward is\n    $$\\begin{align*}\n    R(T, \\theta) &= \\log p(T, \\theta) + \\log \\prod_{i=1}^N \\exp\\left(-\\frac{(y_i - f_{T,\\theta}(x_i))^2}{2\\sigma^2}\\right)\n    \\\\\\\\&=\\log p(T, \\theta) + \\sum_{i=1}^N \\left(-\\frac{(y_i - f_{T,\\theta}(x_i))^2}{2\\sigma^2}\\right),\n    \\end{align*}\n    $$\n    which **explicitly incorporates an expression prior** $p(T, \\theta)$ together with a (Gaussian) likelihood, making the posterior $p(T, \\theta | \\mathcal D) \\propto \\exp R(T, \\theta)$. The full Bayesian formulation adopted by ERRLESS provides additional informative signals during search and mitigates overfitting under noisy data.\n    \n2. **Training objective and its minimizer/maximizer.** Both DSR and PhySO optimize their policies using a *risk-seeking* policy-gradient objective combined with entropy regularization. Concretely, the entropy term $\\lambda_{\\mathcal H}\\mathcal{H}(T, \\theta)$ is weighted by a parameter $\\lambda_\\mathcal{H}$ that controls the level to which we want the policy's samples to be diverse. Because the loss is computed only on the top–$\\epsilon$ fraction of high-reward trajectories (risk-seeking conditioning) and because $\\lambda_\\mathcal{H}$ is freely adjustable, there is **no well-defined theoretical characterization for the maximizing policy $p^*$ of their training objectives**. We also empirically verify this in the section below.\n    \nERRLESS optimizes the trajectory balance (TB) objective [3], which is a particular case of a path consistency objective [4,5]. The TB loss is theoretically grounded in that its minimizing policy $p^*$ is guaranteed to sample any expression $(T,\\theta)$ **with probability proportional to its exponentiated reward $\\exp R(T,\\theta)$**. In our formulation, this reward corresponds to the log-posterior, so the learned policy serves as an amortized sampler over the posterior landscape of symbolic expressions. Moreover, existing results [5] show that the TB-optimal policy $p^*$ also solves a maximum-entropy RL objective -- specifically, the one used by DSR/PhySO without risk-seeking truncation and with $\\lambda_{\\mathcal H}=1$. This connects ERRLESS's training dynamics to a principled RL formulation with a fully characterized optimum.\n\n\n3. **Handling of continuous parameters**. To our knowedlge, ERRLESS is the first approach to learn a posterior over scalar parameters appearing in expressions as well as the expression structure. All existing deep-reinforcement-learning-based approaches find the best fit parameters for a given expression, which can incur a significant computational cost (see \"Computational runtime cost\" below), while MCMC methods rely on a Laplace approximation [6]. ERRLESS is able to circumvent that by learning a policy that samples the parameters.\n\nTo sum up, unlike the prior RL-based methods, ERRLESS is a principled Bayesian symbolic regression strategy, in the sense of seeking an optimal policy that samples the Bayesian posterior in a probabilistic model with a prior over expressions and i.i.d. observation noise.\n\n(continued below)"}}, "id": "Ew5cyV3oKp", "forum": "rRjYHBHq8X", "replyto": "rRjYHBHq8X", "signatures": ["ICLR.cc/2026/Conference/Submission13721/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13721/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13721/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763653605550, "cdate": 1763653605550, "tmdate": 1763653605550, "mdate": 1763653605550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ERRLESS, a Bayesian symbolic regression framework that learns to sample both expression structures and parameters through an entropy-regularized reinforcement learning policy. Instead of finding a single best-fit formula, the method models a posterior distribution over symbolic expressions, aiming to capture uncertainty and improve robustness. It also proposes a bottom-up tree-generation process to enforce dimensional consistency of physical units. Experiments on synthetic and Feynman datasets show competitive symbolic recovery and strong predictive performance under noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality\n\nThe paper provides a coherent Bayesian framing of symbolic regression, formulating it as joint inference over expression structures and parameters and linking entropy-regularized reinforcement learning with posterior sampling.\n\nWhile the use of a neural policy and RL training is standard, the integration of a Bayesian interpretation for uncertainty-aware symbolic regression offers a conceptually unified perspective that may help reframe how probabilistic ideas are applied in interpretable model discovery.\n\nThe paper also emphasizes dimensional consistency and structured priors in the generation process, aligning symbolic regression more closely with physical reasoning tasks.\n\nQuality\n\nThe methodology is technically consistent and implemented carefully, combining entropy-regularized RL, structural constraints, and parameter sampling in a logically sound way.\n\nThe empirical section is comprehensive, evaluating ERRLESS on both synthetic and Feynman datasets with appropriate metrics (symbolic recovery, predictive accuracy, and uncertainty calibration).\n\nThe results show competitive performance and credible uncertainty estimates, demonstrating that the framework is functional and not merely theoretical.\n\nClarity\n\nThe paper is clearly written and well organized, with intuitive explanations, clean mathematical notation, and helpful figures.\n\nThe connection between the Bayesian posterior objective, the entropy-regularized RL formulation, and the sampling procedure is explained transparently, making the framework accessible to both symbolic regression and probabilistic ML audiences."}, "weaknesses": {"value": "Originality and Positioning\n\nThe claimed methodological contributions are not sufficiently novel relative to existing symbolic regression frameworks. Using a neural policy trained via reinforcement learning to generate symbolic expressions has been extensively explored in prior works such as Deep Symbolic Regression (Petersen et al., 2019), the Finite Expression Method (Liang et al., 2022), PhySO (Tenachi et al., 2023), Neural-Guided Genetic Programming (Li et al., 2023). ERRLESS largely follows this paradigm, differing mainly in applying a Bayesian interpretation.\n\nThe paper could improve by clearly distinguishing what aspects of the Bayesian formulation are algorithmically new (e.g., how entropy regularization meaningfully approximates posterior inference beyond standard policy exploration) and by acknowledging that probabilistic sampling of expressions is already a common practice in symbolic regression.\n\nThe introduction and related work should also discuss Parsing the Language of Expression (Huang et al., 2025), which incorporates domain-aware symbolic priors, and other grammar-based Bayesian approaches (e.g., Probabilistic Regular Tree Priors, Grammar-Guided GP). This would provide a more accurate contextual positioning.\n\nBayesian Formulation and Computational Feasibility\n\nThe proposed Bayesian approach introduces significant computational overhead without evidence of efficiency or scalability benefits.\n\nModeling a posterior over both expression structures and parameters entails sampling from a high-dimensional, multimodal distribution, which is computationally intractable for expressions with many constants. In practice, the method collapses this to a simple Gaussian approximation, undermining the Bayesian claim.\n\nThe paper could improve by (a) including runtime and complexity analyses, (b) providing ablation studies showing how posterior sampling scales with model size, and (c) discussing approximations (e.g., variational or factorized posteriors) that might make the Bayesian framework practical.\n\nBottom-Up Generation Justification\n\nThe claimed advantages of bottom-up tree generation—namely producing valid intermediate expressions and easier dimensional checks—are not unique to this design.\n\nTop-down generation frameworks can also ensure dimensional compatibility through type constraints or operator masking (as in PhySO) and can extract valid subexpressions from partial trees.\n\nTo strengthen this component, the authors should provide comparative experiments or ablation results demonstrating concrete benefits (e.g., reduced invalid expressions or faster convergence) of bottom-up versus top-down generation.\n\nExperimental Scope and Analysis\n\nThe quantitative results show comparable or slightly worse performance than existing methods (e.g., 44% recovery on Feynman versus ~59% for PhySO and 50–60% for DSR/PySR). Yet, the discussion emphasizes conceptual rather than numerical advantages.\n\nThe paper would be stronger with statistical analyses (variance, confidence intervals) and runtime comparisons across methods, which would help clarify whether ERRLESS provides any tradeoff between accuracy, uncertainty, and efficiency.\n\nAdditionally, more real-world or scientific case studies (beyond synthetic and Feynman benchmarks) would help demonstrate the claimed benefits in uncertainty quantification or robustness.\n\nUse of Structural and Physical Priors\n\nThe inclusion of structural priors and physical constraints is not new. Prior work such as Parsing the Language of Expression (Huang et al., 2025), SciMED (Nature 2024), ParFam (OpenReview 2025), and Grammar-Guided GP already incorporate these ideas.\n\nThe authors could strengthen their contribution by proposing a more formal or learnable prior mechanism (e.g., data-driven estimation of structural likelihoods or adaptive physical-unit embeddings) rather than manually specified constraints.\n\nPresentation and Scope of Claims\n\nWhile the writing is generally clear, several claims (e.g., “amortizes sampling of both structures and parameters efficiently” and “bottom-up generation enables better physical reasoning”) are overstated relative to the evidence.\n\nThe authors should moderate such claims or substantiate them with direct quantitative or ablation evidence. Clarifying the limitations of the Bayesian approximation would improve credibility."}, "questions": {"value": "Clarification on Bayesian Inference Approximation\n\nThe paper frames ERRLESS as performing Bayesian inference over both structures and parameters. Could the authors clarify how the high-dimensional posterior over constants is represented and sampled in practice?\n\nIs the policy sampling parameters from a fixed Gaussian, or is there an adaptive approximation (e.g., variational or amortized posterior)?\n\nA more detailed description of the Bayesian approximation would help assess whether ERRLESS meaningfully captures parameter uncertainty or simply adds stochasticity to RL sampling.\n\nComputational Complexity and Runtime Evidence\n\nThe paper claims that ERRLESS amortizes parameter fitting into policy training, implying computational advantages. Could the authors provide runtime comparisons (e.g., training time or sample efficiency) versus existing methods such as DSR, PySR, PhySO, or FEX?\n\nHow does the method scale with expression depth, number of constants, or dataset size? Concrete complexity or scaling curves would make the efficiency claim more convincing.\n\nBottom-Up Generation vs. Top-Down Approaches\n\nThe authors argue that bottom-up tree construction ensures valid intermediate expressions and easier enforcement of dimensional consistency.\n\nCould they provide an empirical comparison or ablation study showing how bottom-up generation affects search efficiency, validity rates, or symbolic recovery compared to a top-down baseline?\n\nIf both strategies are possible, under what conditions is bottom-up generation more advantageous?\n\nQuantitative Analysis of Uncertainty Quality\n\nThe paper highlights uncertainty quantification as a strength. Could the authors provide quantitative metrics for uncertainty calibration (e.g., negative log-likelihood, coverage probability) and compare them with ensemble-based or bootstrapped SR methods?\n\nDemonstrating that the posterior uncertainty is useful (e.g., for model selection or active learning) would significantly strengthen the paper’s claims.\n\nEvaluation Breadth and Reproducibility\n\nThe benchmarks are limited to synthetic and Feynman datasets. Are there plans to test ERRLESS on real-world scientific or engineering problems, where uncertainty estimation might be critical?\n\nProviding open-source code and pre-trained policies would also enhance the paper’s impact and reproducibility.\n\nTreatment of Structural and Physical Priors\n\nThe use of structural and dimensional priors resembles earlier approaches such as Parsing the Language of Expression (Huang et al., 2025), PhySO (Tenachi et al., 2023), and ParFam (2025).\n\nCould the authors clarify what is new in their prior formulation—does it introduce learnable or data-driven components beyond manually specified grammar rules?\n\nIf possible, an ablation comparing ERRLESS with and without these priors would help quantify their contribution.\n\nInterpretation of “Amortized Sampling”\n\nThe phrase “amortizing sampling of both expression structures and parameters” is central to the paper’s narrative. Could the authors more precisely define this term?\n\nIs the amortization referring to shared policy parameters across different expressions (as in DSR) or to efficient reuse of parameter inference results? A clearer explanation would clarify the claimed efficiency benefits.\n\nPositioning Relative to Prior Work\n\nThe paper would benefit from a clearer comparison to Bayesian symbolic regression literature (e.g., Probabilistic Regular Tree Priors, Grammar-Guided GP, and Neural-Guided GP).\n\nCould the authors explain how ERRLESS differs conceptually or computationally from these methods and whether the probabilistic formulation offers tangible new capabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "anrGaf1p7b", "forum": "rRjYHBHq8X", "replyto": "rRjYHBHq8X", "signatures": ["ICLR.cc/2026/Conference/Submission13721/Reviewer_DjoN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13721/Reviewer_DjoN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968353072, "cdate": 1761968353072, "tmdate": 1762924264522, "mdate": 1762924264522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Bayesian formulation for reinforcement learning of symbolic regression. The idea is to construct a joint probability distribution over the expression tree and the measurement data. The logarithm of the joint probability is used as the reward, and then an entropy regularization term is added to conduct reinforcement learning. The experiments on a small manually constructed synthetic dataset and Feynman SR database demonstrate the effectiveness of the method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is clear.\n2. the method formulation is correct."}, "weaknesses": {"value": "1. Novelty is limited. Entropy regularization is a common technique for RL used to encourage exploration. This has already been used in the recent most related work, DSR. It is not the unique contribution of this work. \n\n2.  The motivation is not reasonable enough. In fact, all RL based symbolic regression is learning a probabilistic expression generator. For example DSR uses an RNN to implement Eq (3) (based on the preoder traversal of the tree). The recent CADSR [1] uses transformer to implement Eq(3) based on breadth-first search.  Via RL, all these methods are learning a posterior expression tree sampler --- given the measurement data. The proposed method mainly differs in using bottom-up order for tree generation.  It does not seem clear about the advantage.  \n\n3. The empirical results are limited. At least, the proposed method should be tested on the more comprehensive SR bench database, which includes multiple datasets (including Feynman) and many black-box problems. \n\n\n[1] Bastiani Z, Kirby R M, Hochhalter J, et al. Complexity-Aware Deep Symbolic Regression with Robust Risk-Seeking Policy Gradients[J]. arXiv preprint arXiv:2406.06751, 2024."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uFZaYCopa9", "forum": "rRjYHBHq8X", "replyto": "rRjYHBHq8X", "signatures": ["ICLR.cc/2026/Conference/Submission13721/Reviewer_Hh12"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13721/Reviewer_Hh12"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179278539, "cdate": 1762179278539, "tmdate": 1762924263845, "mdate": 1762924263845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}