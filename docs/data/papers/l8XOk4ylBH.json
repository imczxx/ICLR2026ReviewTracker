{"id": "l8XOk4ylBH", "number": 20348, "cdate": 1758304996295, "mdate": 1763413592385, "content": {"title": "Learn to Guide Your Diffusion Model", "abstract": "Classifier-free guidance (CFG) is a widely used technique for improving the perceptual quality of samples from conditional diffusion models. It operates by linearly combining conditional and unconditional score estimates using a *guidance weight* $\\omega$. While a large, static weight can markedly improve visual results, this often comes at the cost of poorer distributional alignment.\nIn order to better approximate the target conditional distribution,\nwe instead learn *guidance weights* $\\omega_{c,(s,t)}$, which are continuous functions of the conditioning $c$, the time $t$ from which we denoise, and the time $s$ towards which we denoise. \nWe achieve this by minimizing the distributional mismatch between noised samples from the true conditional distribution and samples from the guided diffusion process.  We extend our framework to reward guided sampling, enabling the model to target distributions tilted by a reward function $R(x_0,c)$, defined on clean data and a conditioning $c$. We demonstrate the effectiveness of our methodology on low-dimensional toy examples and high-dimensional image settings, where we observe improvements in Fréchet inception distance (FID) for image generation. In text-to-image applications, we observe that employing a reward function given by the CLIP score leads to guidance weights that improve image-prompt alignment.", "tldr": "Learn Classifier-Free guidance (CFG) weights as a function of time and conditioning to better approximate the target distribution", "keywords": ["Diffusion models", "Classifier-free Guidance", "Conditional sampling", "generative mode"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9bbcd6914b36a788c5c46a43a67f673727713a19.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes to introduce the Maximum Mean Discrepancy (MMD) to quantitatively evaluate the distance between the CFG-guided distribution and the ground-truth one. By doing so, it is enabled to design both time-dependent and condition-dependent guidance weight, which is more flexible and controllable. To realize such a pipeline, the authors employ an auxiliary network for guidance weight prediction, and further discuss the influences under different objectives. Both qualitative and quantitative results confirm the superiority of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well structured and easy to follow. All technical details are carefully discussed and clear.\n- The motivation is intuitive and effective, alleviation the discrepancy between CFG-guided and the ground-truth distributions is capable of improving the performance of guided sampling.\n- The further discussion about different objective is detailed and inspiring, encouraging future study for better training efficiency and synthesis performance."}, "weaknesses": {"value": "- The whole pipeline of narrowing the distribution discrepancy by optimizing some scalars is not new. Beyond the main objective being directly motivated by prior works, similar idea of canceling mismatch (or \"self-consistency\") between forward diffusion and reverse denoising stages has also been proposed [1].\n\n  [1] Towards More Accurate Diffusion Model Acceleration with A Timestep Tuner. Xia et al., CVPR 2024.\n\n- The main concern about the proposed method is the optimality. There is no theoretical analysis about the optimality or convergence about the proposed objective. Then why the optimized guidance weight could guarantee the self-consistency? What further confirms my worries is in Appendix C.1. The most intuitive objective is the L2 norm in Eq. (23), which is consistent with the vanilla training loss of diffusion models, *i.e.*, the ELBO loss. However, as the authors themselves claim, such an objective leads to zero guidance weight. That is to say, **diffusion model converges well and both conditional and unconditional scores are accurate**, thus there is no need to employ CFG. Under this discussion, I am curious about the theoretical optimality of Eq. (20) or Eq. (21). Could the authors provide theoretical analyses under some trivial toy data to verify the correctness of the method? From my opinion, Eq. (21) is somewhat an upper bound of ELBO (*i.e.*, Eq. (23)). Given that ELBO itself is an upper bound of KL-divergence (by the native theory of diffusion models), the optimization of a looser bound may lead to meaningless results.\n\n- What further weakens the soundness of the paper is the employment of an auxiliary network. To be honest, such a setting could lead to more flexible guidance weight given different conditions. However, what if the network fails to converge well and only predicts sub-optimal guidance weight? The accuracy issue is more severe under open-vocabulary setting, *i.e.*, text-to-image or text-to-video. Considering that the authors employ a light-weight network, such a concern is also crucial. Could the author provide some closed-form expressions or some empirical solutions avoiding the employment of an auxiliary network?\n\n- The qualitative results of text-to-image is somewhat not convincing. The improvements in Figs. 4-7 are inconspicuous. This strengthens my concerns above."}, "questions": {"value": "Beyond the Weaknesses part, I am curious about the general rule of the guidance weights. Current analyses in Fig. 3 is poor. Could the authors provide more quantitative analyses about the weights under different conditions and timesteps? For example, what if a text condition gradually becomes complex and a long caption? What about the mean or variance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ssRi73qZzv", "forum": "l8XOk4ylBH", "replyto": "l8XOk4ylBH", "signatures": ["ICLR.cc/2026/Conference/Submission20348/Reviewer_A5xC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20348/Reviewer_A5xC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761104760850, "cdate": 1761104760850, "tmdate": 1762933808516, "mdate": 1762933808516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method for learning the guidance schedule for diffusion model generation. In particular, they train a shallow network to minimize the distributional mismatch between a frozen denoiser and the true distribution. The authors provide solid theoretical motivation for their choice of objective function, and also experimental evidence for class and text-conditional models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors replace a hyper-parameter (guidance strength) with a learned approach\n- The objective function is well theoretically motivated"}, "weaknesses": {"value": "- The authors train their own diffusion models for use in evaluations. It would greatly strengthen the paper if they also showed FID and reward improvements for existing pre-trained diffusion models from prior work. For example:\n     - EDM for cifar-10 (Karras et al, https://github.com/NVlabs/edm)\n     - EDM-2 for ImageNet (Karras et al, https://github.com/NVlabs/edm2)\n     - MicroDiffusion for text-to-image (Sehwag et al, https://github.com/SonyResearch/micro_diffusion)\n- The authors compare their learned approach to simple baselines such as constant or limited-interval guidance. It would strengthen the paper if they also showed improvements over stronger un-trained baselines such as in Wang et al (https://arxiv.org/abs/2404.13040).\n\nMy score recommends reject, but I would be happy to raise my score to accept if these additional experiments are included in the paper and my questions below are addressed."}, "questions": {"value": "- What do the authors mean by a high-variance objective?\n- What is the training cost of training the learned guidance network as a percentage of diffusion model training cost?\n- Why is the FID on COCO for the constant guidance model so high? FID 31 seems incredibly high for a 1B model. How was this model trained?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z3O6qtCYR9", "forum": "l8XOk4ylBH", "replyto": "l8XOk4ylBH", "signatures": ["ICLR.cc/2026/Conference/Submission20348/Reviewer_h7dP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20348/Reviewer_h7dP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841562212, "cdate": 1761841562212, "tmdate": 1762933808028, "mdate": 1762933808028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper puts forward learning a classifier-free guidance schedule that varies with both time and the input condition (class or text), enabling the sampler to better match the desired conditional distribution.\nCompare with handcrafting guidance strengths, the method optimizes a self-consistency objective: if one adds noise to a real image and then denoises it with guidance, the intermediate result should match what the forward noising process would produce at that timestep.\nFollowing this desin mechnism, this reframes guidance tuning as a distribution-matching problem and naturally extends to reward-guided generation by combining an external reward (e.g., CLIP-based) with the same self-consistency regularizer.\nImortantly, exntensive experiments on standard image datasets and text-to-image benchmarks show consistent quality gains over unguided sampling, fixed guidance, and simple time-limited guidance, with learned schedules that adapt across prompts and timesteps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It reframes classifier-free guidance tuning as a distribution-matching problem, replacing hand-crafted schedules with a learned, time- and prompt-aware policy—an original and practical angle.\nThe self-consistency objective is elegant and low-variance, making the method easy to implement on top of existing diffusion backbones without architectural changes.\nExperiments span multiple datasets and backbones and include sensible ablations, showing consistent gains over unguided, fixed-guidance, and limited-interval baselines.\nThe learned guidance schedules are interpretable (varying with prompt and timestep), which improves clarity and aids real-world debugging.\nOverall, the approach offers a broadly applicable and deployable improvement to conditional diffusion sampling with a favorable quality-to-complexity trade-off."}, "weaknesses": {"value": "1. The paper argues that enforcing self-consistency is stronger and low-variance, but the conditions under which minimizing the proposed loss actually improves conditional sampling are not formalized.\n\n2. On text-to-image, self-consistency plus CLIP reward improves FID yet struggles to surpass strong guided baselines on CLIP score.\n\n3. Learning a guidance network and evaluating the self-consistency loss add additional overhead."}, "questions": {"value": "1. How sensitive are results to the kernel/parameters in the MMD objective versus the simpler L2 variant?\n\n2. What are the training and inference costs attributable to guidance learning and self-consistency evaluation?\n\n3. Since the learned weights vary strongly with prompt, do certain semantic categories systematically demand higher guidance?\n\n4. Can practitioners easily inspect how guidance evolves with time and prompt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BsOdWrHTWh", "forum": "l8XOk4ylBH", "replyto": "l8XOk4ylBH", "signatures": ["ICLR.cc/2026/Conference/Submission20348/Reviewer_sh9D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20348/Reviewer_sh9D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964919167, "cdate": 1761964919167, "tmdate": 1762933807731, "mdate": 1762933807731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common answer part 1"}, "comment": {"value": "We thank all the reviewers for their feedback. In this section, we provide a common answer for all the reviewers which we will refer to. It contains additional ablations and experiments reviewers have asked for.\n\n## Training costs and inference costs of guidance network\n\nTraining the guidance network involves training a lightweight MLP with a few layers. The cost of training it is insignificant compared to the cost of training a diffusion model. For example, in ImageNet experiments, we train a U-Net with ~ 260M parameters for around 7M iterations with a batch size of 128, while the guidance network with ~ 600K parameters is trained for up to 100K with batch size 256. In text-to-image, the model has about ~1B parameters and is trained for 1.6M steps with batch size of 2048, while guidance network has about 6M parameters which we train for 50K iterations with batch size 256.\n\nAt inference, we use a guidance network to produce a scalar $\\omega$ which involves evaluating the MLP on precomputed $(t,s)$ and conditioning embeddings $c$. The computation cost is minimal compared to unrolling the diffusion model.\n\nWe have added a section in Appendix H.1 of the revised version of the paper about it.\n\n## Ablation over $\\beta$ and $m$ in the MMD computation\n\nWe ran an ablation study over $\\beta$, the exponent applied to the $L_2$ norm, and $m$, the  number of particles used for the MMD computation for the self-consistency loss. We did this ablation on ImageNet.\n\nThe results are given in the following tables.\n\nTake-aways: We see that the performance is not very sensitive to $m$, we see that $m>=4$ works well.  As for $\\beta$, we see that values  $\\beta \\in [1. 1.75]$ lead to good results.\n\n| $\\beta$ | FID | IS |\n| -- | -- | -- |\n| 0.1 | 2.07 | 78.18 |\n| 0.5 | 2.04 | 77.72 |\n| 1.0 | 1.98 | 73.33 |\n| 1.5 | 2.0 |  71.17 |\n| 1.75 | 1.99 | 73.69 |\n\n| m | FID | IS |\n| -- | -- | -- |\n| 2 | 2.00 | 70.66 |\n| 4 | 1.99 | 73.69 |\n| 8 | 1.99 | 73.02 |\n| 16 | 2.00 | 77.11 |\n\nWe have added it to Appendix H.2 of the revised version of the paper.\n\n## Comparison to Clamp-linear schedule and limited interval guidance (LIG)\n\nWe compare our method to the Clamp-linear schedule $w_t = max(c0,w_0 2t/T)$, see [1], and to Limited Interval Guidance (LIG) on T2I and ImageNet. For LIG on ImageNet, we take results from Table 1.\n\nOn ImageNet, for clamp-linear schedule, we swept over parameter $w_0$ and $c$ (see Appendix H.3 for more details).\n\nWe report the results in a table below. Our method is the same as the one reported in Table 1, i.e., self consistency with all the conditioning information. We observe that clamp-linear works worse than our method and baselines.\n\n| Method | FID | IS |\n| -- | -- | -- |\n| Ours | 1.99 | 73.62 |\n| LIG | 2.11 | 71.60 |\n| Clamp-linear | 2.24 |  76.34 |\n| Constant guidance | 2.4 | 66.72 |\n\nWe added this to Appendix H.3 in our paper.\n\nOn T2I, for a clamp-linear schedule, we swept over $w_0 \\in \\\\{7.5,10.0,14.0,16.0\\\\}$. For $w_0 = 14.0$ and over clamp parameter $c_0 \\in \\\\{1.0,2.0,4.0\\\\}$. The best parameters we found are $w_0 = 14, c_0=4$. For LIG, we swept over the guidance scales in $\\{7.5, 15.0\\}$ and intervals $[0.1,0.9]$, $[0.1,0.8]$, $[0.2,0.9]$, $[0.2,0.8]$. The best parameters are the guidance scale of $15.0$, interval $[0.1,0.9]$. We report performance of the best parameters.\n\nThe results are given below. We refer to our method as the one using self-consistency loss, clip reward and all conditioning information, see Table 3. We see that both LIG and clamp-linear underperform wrt our method and constant guidance in terms of CLIP score. We also provide detailed quantitative results for both clamp-linear schedule and LIG as well as qualitative results for these methods in the Appendix H.4 of our paper. Interestingly, we observe that in some cases, the guidance schedules predicted by our method resemble those of clamp-linear as shown in Figure 11 and Figure 15 in our paper. \n\n| Method | CLIP | FID |\n| -- | -- | -- |\n| Ours | 0.306 | 28.37 |\n| LIG | 0.304 | 24.95 |\n| Clamp-linear | 0.305 | 27.14|\n| Constant guidance | 0.306 | 31.2 |\n\n[1] Analysis of Classifier-Free Guidance Weight Schedulers Xi Wang, Nicolas Dufour, Nefeli Andreou, Marie-Paule Cani, Victoria Fernandez Abrevaya, David Picard, Vicky Kalogeiton, TMLR, 2024\n\n## Ablation over conditioning information in T2I experiments\n\nWe ablated the choice of conditioning embeddings for the MLP in our T2I experiments, specifically comparing T5-only and CLIP-only embeddings against our default method which uses both. In all the experiments we also use time conditioning. We found that while T5-only conditioning yields a slightly lower CLIP score than the other two configurations, it achieves slightly better FID. See table below. We also report qualitative results in the Appendix H.5 of the paper.\n\n| Conditioning | FID | CLIP |\n| -- | -- | -- |\n| T5 | 28.28 | 0.305 |\n| CLIP | 28.63 | 0.306|\n| T5 and CLIP | 28.37 | 0.306|"}}, "id": "nUykOdwtz7", "forum": "l8XOk4ylBH", "replyto": "l8XOk4ylBH", "signatures": ["ICLR.cc/2026/Conference/Submission20348/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20348/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20348/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763412677614, "cdate": 1763412677614, "tmdate": 1763412677614, "mdate": 1763412677614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to learn the classifier-free guidance (CFG) weight by setting it as the output of a neural network. While previous CFG approaches relied on heuristics or trial-and-error to find an optimal (often large) weight, which could lead to saturation and degraded sample quality, this work optimizes guidance weights as a continuous function of time and condition. The objective is to minimize the distributional mismatch between the generated guided samples and the noised true conditional samples, using a novel self-consistency loss. This loss successfully mitigates the high-variance problem associated with theoretical approaches, leading to stable training and demonstrated performance gains across various datasets, from toy examples to COCO T2I sampling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The idea of learning the optimal guidance weight continuously via a neural network, rather than relying on grid search for optimal guidance intervals or empirical adjustments, is highly novel.\n- Despite the inherent risk of the guidance weight collapsing to a trivial solution, the successful stabilization of learning via the newly proposed self-consistency loss is a significant achievement. The simplified L2 objective (Eq. 21) also appears surprisingly straightforward to implement."}, "weaknesses": {"value": "The proposed method involves a large number of hyperparameters to determine the optimal guidance weight, and additionally requires performing $m$ rounds of noising and comparison at every denoising step, which introduces significant computational overhead. Given this, I wonder whether the computational cost difference compared to existing guidance distillation methods is actually substantial. It would be valuable to include a cost comparison table or discussion.\n\nMoreover, since the loss function in this paper formalizes the matching between the true sample distribution and the guided one, I suspect that a parameter-efficient fine-tuning (PEFT) approach such as LoRA could optimize a student model directly using the same loss formulation, without needing a separate guidance network at inference time. This would result in a more inference-efficient model, and I’m curious whether the authors have considered or experimented with this alternative.\n\nRegarding the design of the guidance network, the paper focuses heavily on the loss formulation for learning guidance weights but provides limited discussion or justification for the network’s architecture. The rationale behind its input-output design is unclear. Based on Table 1 and Fig. 1, 3, the conditioning input seems to have a stronger influence than expected, suggesting that incorporating the currently sampled image as an additional input could further improve weight estimation. Were such variants or conditioning combinations explored?\n\nIn the COCO experiments, the guidance network uses the text encoder output as a conditioning signal, and Fig. 3 shows that the learned guidance weights vary across prompts. Does this imply that the network has learned semantic understanding of the text embeddings? If so, how heavy must the model be to capture such semantics effectively? Additionally, from the qualitative results on COCO, the outputs often resemble those with weak or no guidance, which may indicate that the diffusion model’s original objective still struggles to fully capture the conditional distribution, similar to previous findings.\n\nFinally, in the loss formulation, the authors mention using $m=4$ particles for computing the MMD in the image domain. Even if self-consistency reduces variance, I question whether the MMD loss computed from only four samples can be sufficiently accurate. Was this number chosen primarily to reduce computational cost? If so, it would be helpful to report how performance varies with different particle counts, to justify the design choice."}, "questions": {"value": "- How does this method compare to guidance Interval approaches? The guidance interval method empirically finds an optimal guidance schedule, assigning near-zero guidance weight in early steps and large weights in the final few steps. Since the proposed method can measure how well the estimated distribution matches the true one via its loss, it may be possible to evaluate the optimality of the interval-based scheduling using the same loss metric. If so, this could open new directions for systematic guidance interval optimization beyond the work of LIG.\n- Once the optimal guidance weights are obtained through this method, could they be further distilled into a single model to achieve both high performance and inference efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LDUPtVtTny", "forum": "l8XOk4ylBH", "replyto": "l8XOk4ylBH", "signatures": ["ICLR.cc/2026/Conference/Submission20348/Reviewer_6vA2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20348/Reviewer_6vA2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012593572, "cdate": 1762012593572, "tmdate": 1762933807275, "mdate": 1762933807275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}