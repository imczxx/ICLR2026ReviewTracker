{"id": "iR2hMzVu1k", "number": 9954, "cdate": 1758152295617, "mdate": 1763397006723, "content": {"title": "Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings", "abstract": "Selective state-space models excel at long-sequence modeling, but their capacity for language representation -- in complex hierarchical reasoning -- remains underexplored. Most large language models rely on *flat* Euclidean embeddings, limiting their ability to capture latent hierarchies. To address this, we propose *Hierarchical Mamba (HiM)*, integrating efficient Mamba2 with hyperbolic geometry to learn hierarchy-aware language embeddings for deeper linguistic understanding. Mamba2-processed sequences are projected to the Poincar\\'e ball or Lorentzian manifold with \"learnable\" curvature, optimized with a hyperbolic loss. Our HiM model facilitates the capture of relational distances across varying hierarchical levels, enabling effective long-range reasoning for tasks like mixed-hop prediction and multi-hop inference in hierarchical classification. Experimental results show both HiM effectively capture hierarchical relationships across four linguistic and medical datasets, surpassing Euclidean baselines, with HiM-Poincar\\'e providing fine-grained distinctions with higher h-norms, while HiM-Lorentz offers more stable, compact, and hierarchy-preserving embeddings.", "tldr": "Hierarchical Mamba for Structure-Aware Language Embedding", "keywords": ["Hierarchical language modeling", "non-Euclidean Neural Networks", "hyperbolic geometry", "state-space models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b82efbbaa2a39231a92a0bd359f74cc8bd82e0ab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Hierarchical Mamba (HiM), a model that integrates the Mamba2 state-space model with hyperbolic geometry (both Poincaré and Lorentz manifolds) for hierarchical reasoning tasks such as ontology and taxonomy inference. It introduces a SentenceMamba-16M encoder for sequence embedding and projects outputs into hyperbolic space via learnable curvature. A hyperbolic loss combining centripetal and clustering terms enforces parent–child relationships.\nExperiments on WordNet, DOID, FoodOn, and SNOMED-CT show improved F1 scores over Euclidean and hyperbolic transformer baselines, with claims of better scalability for long sequences."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) First work of intergrating Mamba2 and hyperbolic embeddings. \n\n2) The paper includes both Poincaré and Lorentz formulations, curvature regularization, and stability approximations.\n\n3) empirical study across four ontology datasets, including δ-hyperbolicity analysis and visualization of embedding hierarchies."}, "weaknesses": {"value": "1) Questionable motivation for long-range dependency modeling. Ontology reasoning is structurally hierarchical but not sequential. It’s unclear why Mamba’s sequence modeling is advantageous here. The link between “long-range dependencies” and “multi-hop ontology reasoning” is weak and mismatched. \n\n2) Limited novelty. Integrating a known efficient sequence model (Mamba2) with hyperbolic embeddings is incremental, given prior works like SHMamba (2024) and Hyperbolic BERT / HiT (He et al 2024) / HypLoRA (Yang et al) already address hierarchy in both Euclidean and hyperbolic spaces.\n\n3) The “hyperbolic loss” largely reuses existing centripetal and triplet loss ideas without strong new formulation.\n\n4) The paper mixes terminology between hierarchical, long-range, and multi-hop reasoning without a clear theoretical bridge.\n\n5) Evaluation tasks are limited. Only ontology/knowledge graphs datasets are tested. There is no evidence of generalization to natural language or other structured tasks."}, "questions": {"value": "1) Why does ontology reasoning require long-range sequence modeling? Are there sequential dependencies that justify using Mamba rather than a hyperbolic Transformer like HiT?\n\n2) How is HiM different from SHMamba (2024), HiT (2024), or Hyperbolic LoRA (2024) beyond substituting the backbone?\n\n3) Can you quantify how much of the performance gain arises from Mamba2 vs. the hyperbolic embedding itself?\n\n4) Have you compared HiM with strong non-hyperbolic ontology reasoning baselines like KG embeddings?\n\n5) What does “long-range dependency” mean in hierarchical reasoning? Can you formalize or visualize this relationship?\n\n6) How does the model handle non-tree or cyclic structures present in real ontologies like SNOMED-CT?\n\n7) Does learnable curvature consistently converge, or is it dataset-dependent?\n\n8) Why not test the approach on natural language tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0j49AI8qod", "forum": "iR2hMzVu1k", "replyto": "iR2hMzVu1k", "signatures": ["ICLR.cc/2026/Conference/Submission9954/Reviewer_gJLi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9954/Reviewer_gJLi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757604806, "cdate": 1761757604806, "tmdate": 1762921401865, "mdate": 1762921401865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "2E9olUS14l", "forum": "iR2hMzVu1k", "replyto": "iR2hMzVu1k", "signatures": ["ICLR.cc/2026/Conference/Submission9954/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9954/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763397005963, "cdate": 1763397005963, "tmdate": 1763397005963, "mdate": 1763397005963, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Hierarchical Mamba (HiM), a new model architecture designed to efficiently create language embeddings that capture hierarchical relationships. The authors identify two main problems with existing models like Transformers: they use flat Euclidean embeddings, which are poorly suited for tree-like data, and their attention mechanisms are computationally expensive.HiM solves this by combining the Mamba2 state-space model with hyperbolic geometry. It uses Mamba2 as an efficient $O(L)$ encoder to process long sequences, creating a 16-million parameter model called SentenceMamba-16M. The model's output is then mean-pooled and projected into a hyperbolic space (either the Poincaré ball or the Lorentzian manifold), which naturally represents hierarchical structures.The model is trained with novel hyperbolic loss functions (centripetal and clustering) to explicitly organize the embeddings, pulling parent nodes toward the origin and grouping related child nodes. Experiments on four ontology datasets (DOID, FoodOn, WordNet, SNOMED-CT) show that HiM significantly outperforms both Euclidean baselines and a hyperbolic Transformer (HiT). The paper also finds that the HiM-Lorentz variant offers more stable and compact embeddings, particularly for datasets with deep, tree-like structures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core idea of combining the $O(L)$ efficiency of Mamba2 with the $O(L)$ representational power of hyperbolic geometry for hierarchical data is novel and well-motivated.\n\nThe introduction of the SentenceMamba-16M model provides a lightweight and efficient backbone for sentence embedding tasks.\n\nThe inclusion of a zero-shot comparison against GPT-4o on the WordNet task is a strong addition, demonstrating that a small, specialized model can outperform a massive, general-purpose one on a specific reasoning task."}, "weaknesses": {"value": "The paper's claim that Mamba2's selective properties are key to its success is not fully proven. Since the methodology applies mean pooling to the Mamba2 block outputs to get a single vector, it is unclear if Mamba's selectivity is providing a benefit beyond just being an efficient $O(L)$ encoder (maybe LSTM?)\n\nA critical detail seems vague. The paper mentions a \"geometric stabilization technique that periodically projects the model parameters back onto the manifold\" every 100 steps. It does not specify which parameters or the mathematical operation used for this projection, making it difficult to replicate.\n\nThe use of both a \"learnable curvature\" $c$ and a \"learnable scaling parameter\" $\\gamma$ seems redundant. While justified as a stability measure, their interaction (creating an $\\mathcal{K}_{eff}$) isn't deeply explored, and it's unclear if this dual-parameter approach is truly necessary.\n\n**NOTE**: The model projects data onto the Lorentz manifold using hyperbolic cosine ($\\cosh$) and sine ($\\sinh$) functions. Unlike the tanh function used for the Poincaré model, these functions are unbounded and grow exponentially. This creates a high risk of numerical overflow (getting numbers too big for the computer to handle) and exploding gradients, which would cause the training to fail. \n\nTherefore, as I have not independently verified the correct implementation of these highly advanced and sensitive numerical methods, my confidence in a full assessment of this paper's technical soundness is properly lowered."}, "questions": {"value": "Regarding the stabilization technique, could you please provide the exact mathematical formula used to \"project the model parameters back onto the manifold\"? Which specific parameters (e.g., the entire Mamba2 block, just the final projection layer?) does this operation apply to?\n\nYou attribute the model's success to Mamba's selective mechanism. Given the use of mean pooling, did you run any experiments to isolate this? For example, how does HiM's performance compare to a model that replaces Mamba2 with a different $O(L)$ encoder, like an LSTM, but uses the same hyperbolic projection and loss functions?\n\nWhy did you choose to implement a dual-parameter system for the geometry (learnable curvature $c$ and learnable scaling $\\gamma$) rather than just learning the curvature $c$ alone, which is a more common approach? Did you find that learning $c$ by itself was too unstable?\n\nThe terminology for the baselines in Table 1 is confusing. The \"Finetuned\" model is described as being randomly initialized and trained from scratch, making \"Finetuned\" a misnomer that could confuse the reader. Could you clarify if I understand this point correctly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LIm45Ud0fm", "forum": "iR2hMzVu1k", "replyto": "iR2hMzVu1k", "signatures": ["ICLR.cc/2026/Conference/Submission9954/Reviewer_A5xX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9954/Reviewer_A5xX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938265391, "cdate": 1761938265391, "tmdate": 1762921401353, "mdate": 1762921401353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Hierarchical Mamba (HiM) fuses Mamba2 with hyperbolic geometry to learn hierarchy-aware language embeddings, projecting sequences onto the Poincaré ball or Lorentz manifold with learnable curvature and a hyperbolic loss. This design better captures relational distances across hierarchical levels, improving long-range reasoning for mixed-hop and multi-hop inference. Across four linguistic and medical datasets, HiM outperforms Euclidean baselines; the Poincaré variant offers finer-grained distinctions (higher h-norms), while the Lorentz variant yields more stable, compact, hierarchy-preserving embeddings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Significant performance gains. The method delivers consistently strong improvements across tasks.\n- Systematic manifold/curvature study. It compares Poincaré and Lorentz models across multiple datasets and tasks, analyzing how curvature and manifold choice affect performance and stability."}, "weaknesses": {"value": "1. The paper compares only against Mamba, lacking head-to-head evaluations with Hyperbolic Transformers and Euclidean Transformers under matched settings to quantify both accuracy and runtime gains.\n2. There is no theoretical or empirical accounting of the constant-factor cost of hyperbolic operations versus Euclidean ones (e.g., training/inference latency,  memory), despite relying on projections/distances (cosh/sinh, exp/log maps).\n3. Incomplete approximation analysis. The Maclaurin approximation for cosh/sinh is invoked for |z| < 1e-3, but the trunction order k, error bounds, switching criteria back to exact functions, and fallback for |z| ≥ 1e-3 are unspecified. Please report k, provide an error bound (or empirical error), clarify whether k is fixed or adaptive, and include a small ablation of k vs. training stability/runtime.\n4. Overall, the contribution largely applies hyperbolic geometry to Mamba without introducing sufficiently new technical or theoretical innovations."}, "questions": {"value": "Please see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EM0tMeLZ41", "forum": "iR2hMzVu1k", "replyto": "iR2hMzVu1k", "signatures": ["ICLR.cc/2026/Conference/Submission9954/Reviewer_B2t6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9954/Reviewer_B2t6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957911055, "cdate": 1761957911055, "tmdate": 1762921400508, "mdate": 1762921400508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper directly combines Mamba2 SSM with hyperbolic geometry (including the Poincaré ball and Lorentz manifold), utilizing learnable curvature and loss functions tailored for hyperbolic space to generate hierarchy-aware language embeddings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "New exploitation, Mamba2+Hyperbolic"}, "weaknesses": {"value": "The paper assumes that the fusion of Mamba2 and hyperbolic space can significantly enhance long-sequence and hierarchical reasoning capabilities, but the experiments mainly focus on four preset Ontology-class datasets, failing to cover large-scale real-world scenarios in natural language processing (such as open-domain question answering or multi-document reasoning), resulting in insufficient model generalizability.\n\nAlthough the authors compared their approach with Euclidean Mamba and Hyperbolic Transformer, there is a lack of direct horizontal evaluation against open-source large models (such as Llama, GPT-4, and stronger Transformer variants), and no comprehensive comparison with state-of-the-art hyperbolic methods or RAG-type methods of similar parameter sizes, which affects the objectivity of the conclusions.\n\nAll tasks concentrate on classification for mixed-hop and multi-hop reasoning, neglecting the performance of language embeddings in mainstream tasks such as generation, retrieval, and structured question answering. Moreover, the evaluation metrics are primarily F1, Precision, and Recall, without introducing hierarchy-specific evaluations (such as Tree Edit Distance or hierarchical consistency scores), resulting in insufficient experimental scope.\n\nRegarding the Maclaurin expansions proposed by the authors, this has already been provided in the geoopt repo, which the authors did not cite or refer to.\n\nRegarding these hyperbolic losses, this method is not novel, and for a fair comparison with Euclidean space models, shouldn't you also include such losses to them?"}, "questions": {"value": "Although the authors modeled and experimented with both Poincaré and Lorentz isomorphic hyperbolic space manifolds, the paper does not systematically explore whether their combination truly brings irreplaceable expressive power improvement under the premise of theoretical equivalence. Why not consider two Poincaré balls or two Lorentz models? Wouldn't introducing multiple manifolds potentially introduce more instability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "huSrpyyKrl", "forum": "iR2hMzVu1k", "replyto": "iR2hMzVu1k", "signatures": ["ICLR.cc/2026/Conference/Submission9954/Reviewer_sr3o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9954/Reviewer_sr3o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990752564, "cdate": 1761990752564, "tmdate": 1762921399268, "mdate": 1762921399268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}