{"id": "xpxgepkpRO", "number": 7281, "cdate": 1758014006655, "mdate": 1763739522712, "content": {"title": "Inference-time Scaling for Time-series Processing", "abstract": "Scaling laws have fundamentally driven AI progress, particularly in large-scale models. However, as Web-scale pretraining data for such models nears saturation, focus increasingly shifts to new paradigms like inference-time scaling. While validated across various AI domains, its application to time-series tasks remains largely unexplored. This study addresses this gap by investigating whether inference-time scaling can be successfully adapted for time-series processing. First, multiple candidate outputs for a given input are generated based on a trained model. Second, motivated by the principle that better candidates reconstruct the observed data more accurately, we compute the reconstruction error for each candidate output. Third, these errors are used to determine weights of each candidate, and the final prediction is then formed as a weighted combination of the candidates. We present specific algorithmic instantiations of this new framework for two fundamental time-series tasks, namely forecasting and missing value imputation. Furthermore, we provide a theoretical analysis for the forecasting case to support the method's validity from a Bayesian uncertainty perspective. Extensive experimental evaluation across 7 benchmark datasets for both tasks convincingly verifies the effectiveness of our methodology: Incorporation of our methodology during the inference phase led to performance improvements in all 9 recent time-series methods. Source codes have been uploaded in the supplementary files.", "tldr": "This is the first study that investigates inference-time scaling for time-series forecasting and imputation, combining multiple candidate outputs via reconstruction-based weighting to achieve SOTA performance.", "keywords": ["Time-Series Forecasting; Time-Series Imputation; Bayesian Learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1d09d51f086a37e07cdef9b95e80ad1ab8c4f84.pdf", "supplementary_material": "/attachment/8da1f6233a2caaabc188eef96b69a380e5102932.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an ensembling method of time series forecasting/imputation models. Basically, the idea is, given a piece of time series to-be-forecasted/imputated and a forecasting model, the authors use MC drop-out to generate multiple forecasting/imputation candidates. After generating, a backward model backcasts the original time series (in forecasting cases); or some original pieces are masked and a backward model backcasts the original time series (in imputation cases). Authors carry out theory analysis based on Bayesian uncertainty theory. Authors also provide experiment results for forecasting and imputation, showing that the proposed method performs better than WTA or average. Higher mse reduction is shown for larger number of candidates."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper studies the interesting idea to use multiple predictions from same model.\n2. Authors provide reasonable theory analysis and support for the proposed method.\n3. Authors provide comprehensive experiments to validate that using weighted average is better than simple average or best of N."}, "weaknesses": {"value": "1. Could you please provide more ablation study on the choice of $\\sigma_e$? Especially, please specify is $\\sigma_e$ globally chosen or is it specially tuned for each model each dataset on each prediction horizon/imputation mask percentage. As shown in Figure 4, Figure 16, 17, etc., WTA in some cases would lead to worse performance. Please specify how you choose $\\sigma_e$ in more detail, and perhaps carry out sensitivity study of $\\sigma_e$."}, "questions": {"value": "Please see weakness. Also questions that are not considered as Cons:\n\n1. The reverse model here is somewhat similar to reward model for LLM. The authors train a backward model with same architecutre/design for each foreward model for consistency, which I think is indeed reasonable. Given that said, according to my understanding and common sense, a stronger reward model would lead to better inference time scaling result for LLM, and perhaps a stronger backward model would lead to better inference time scaling result for time series forecasting. I notice in FIgure 18 and Figure 26 that in the PatchTST as foreward \\& backward model case, the WTA and weighted-avg-of-8 lead to worse performance in some cases. Would this be caused by, that the PatchTST backward model is not a nice backward model? Could you use some other backward model with PatchTST foreward model, to see if this would improve PatchTST inference time scaling performance?\n2. Following 1., what's your considered best backward model? Would the backward model performance related to the forward forecasting performance? (I know to answer this question in great detail would require a lot of experiments. I'm not requiring the authors to conduct this amount of thorough experiments just to answer this question. Perhaps some intuitive answer would be enough for this question. My main concern is still written in Weakness section.)\n\nOne further question for discussion:\nRecent observations (e.g. https://www.arxiv.org/abs/2510.02729) show that these time series forecasting benchmarks have almost been saturated. Other concerns come from the fact that it doesn't seem to make sense that one simple neural network can reach sota for all time series tasks without any context (e.g. https://neurips.cc/virtual/2024/108471), so perhaps we should combine more things together (e.g. features, NNs, strategies, etc.). What's your opinion on these thoughts? Compared to proposing methods that risk overfitting these datasets, what future direction do you think would be good for further research of our time series community?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c4B43sPjww", "forum": "xpxgepkpRO", "replyto": "xpxgepkpRO", "signatures": ["ICLR.cc/2026/Conference/Submission7281/Reviewer_yT6j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7281/Reviewer_yT6j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760503661548, "cdate": 1760503661548, "tmdate": 1762919407098, "mdate": 1762919407098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a general inference-time scaling (ITS) framework for enhancing time-series processing tasks, including forecasting and imputation. The key idea is to enhance model performance at inference time without retraining by generating multiple candidate outputs via Monte Carlo Dropout, evaluating each candidate through reconstruction errors using a backward model, and aggregating them through error-based weighting. The authors justify the approach with a Bayesian uncertainty interpretation, showing that the weighting scheme corresponds to a probabilistic ensemble that reduces epistemic uncertainty. Experiments are performed across seven benchmark datasets and nine models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an underexplored direction — inference-time scaling for time-series models — and presents a clear, well-motivated framework. Most current work focuses on squeezing more data into pre-training to obtain better foundation models; shifting attention to smarter inference is an important and complementary perspective. I would, however, have appreciated a deeper discussion comparing ITS to other inference-focused approaches (e.g., meta-learning based ([1], [2]) test-time adaptation or context-based methods [3] / in-context learning).\n2. The Bayesian interpretation of the weighting scheme gives theoretical grounding to what would otherwise be a heuristic ensemble technique.\n3. The authors validate their procedure across multiple tasks (forecasting and imputation) and over a large set of methods, which supports the claimed generality of ITS.\n\n[1] Learning deep time-index models for time series forecasting, Woo et al, ICML 2023\n\n[2] Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations, Le Naour et al, TMLR 2024\n\n[3] From Tables to Time: How TabPFN-v2 Outperforms Specialized Time Series Forecasting Models, Hoo et al, 2025"}, "weaknesses": {"value": "1. The relative improvements are small. For forecasting, the average relative gain for methods like PatchTST and iTransformer (Table 1 and Appendix F.2) is under 2%, which is modest. Moreover, some experimental choices for forecasting are questionable: using a look-back window of 96 to predict horizons of 96, 192, 336, and 720 is not convincing in my view (longer history windows are typically required for long horizons).\n2. For imputation, relative gains are larger, but the experimental design weakens the claim. Most baselines are forecasting models rather than imputation-specific methods; including dedicated imputation baselines (e.g., BRITS, SAITS, CSDI) would be more convincing. Also, the sequences used for imputation are very short (L = 96), which reduces realism and makes it harder to judge real-world efficacy.\n3. The computational-cost discussion is underdeveloped and not fully convincing. Section 4.3 and Appendix F.5 only report runtime analyses on two small datasets and for the smallest settings (forecasting input=96 then predict=96, imputation input=96 with 12.5% masking). Because the performance gains are relatively mild, a thorough runtime comparison is essential to justify the extra inference cost. I would expect a comparative table showing end-to-end inference time (in seconds) on a single GPU across multiple datasets and settings."}, "questions": {"value": "Here are my suggestions :\n\n1. Please substantially expand the inference-time computational analysis (Weakness 3). Provide a comparative inference-time table (seconds per example) on the same GPU across representative datasets and multiple settings (varying L and S, and different mask rates). This will clarify the trade-off between accuracy and cost.\n2. Strengthen the discussion comparing ITS with other inference-focused approaches, such as meta-learning-based test-time adaptation and context/in-context learning methods (see Strength 1). A short empirical or conceptual comparison would help place ITS in the broader landscape.\n3. Improve experimental quality: use longer look-back windows for long-horizon forecasting, include imputation-specific baselines (BRITS, SAITS, CSDI), and evaluate on longer sequences to better reflect practical imputation scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6PDFTDJdtE", "forum": "xpxgepkpRO", "replyto": "xpxgepkpRO", "signatures": ["ICLR.cc/2026/Conference/Submission7281/Reviewer_wRpv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7281/Reviewer_wRpv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760803480906, "cdate": 1760803480906, "tmdate": 1762919406743, "mdate": 1762919406743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper applies inference-time scaling to time-series models by generating multiple predictions via MC Dropout, scoring them using reconstruction error from a backward model, and combining them with learned weights. Experiments on forecasting and imputation tasks show consistent improvements across 9 methods and 7 datasets, at the cost of 1-15x slower inference."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is the first to address inference time scaling in time series forecasting.\n- Using a reverse-temporal model for reconstruction verification is a good domain-specific adaptation.\n- First systematic study of inference-time scaling for mainstream time-series models (prior works seem to have only worked on LLMs for TSF).\n- Comprehensive evaluation across architectures and tasks."}, "weaknesses": {"value": "- The theoretical insight is naive and does not provide much insight. The method is not novel in other domains, so it is more like an engineering work with good empirical results.\n- The baselines are simple (see Figure 4). Although there are no prior works but there should be more sophisticated weighting methods they can compare with. This can demonstrate if the backward model actually provides effective guidance or if the effect simply comes from averaging.\n- The computation cost is somewhat high (1-15x slower with 2-12% improvement), and they did not mention or count the computation cost for training the backward model."}, "questions": {"value": "Can you try to justify the effectiveness of the backward model as a guidance more clearly? I believe comparing with more comprehensive baselines (for example entropy-guided or average) would also solve this.\nI will raise my score if you solve this well (since the score I would like to give now is somewhat between 4 and 6)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xtGfktfQm6", "forum": "xpxgepkpRO", "replyto": "xpxgepkpRO", "signatures": ["ICLR.cc/2026/Conference/Submission7281/Reviewer_tnTg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7281/Reviewer_tnTg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813186101, "cdate": 1761813186101, "tmdate": 1762919406405, "mdate": 1762919406405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper applies inference-time scaling to time-series models by generating multiple predictions via MC Dropout, scoring them using reconstruction error from a backward model, and combining them with learned weights. Experiments on forecasting and imputation tasks show consistent improvements across 9 methods and 7 datasets, at the cost of 1-15x slower inference."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is the first to address inference time scaling in time series forecasting.\n- Using a reverse-temporal model for reconstruction verification is a good domain-specific adaptation.\n- First systematic study of inference-time scaling for mainstream time-series models (prior works seem to have only worked on LLMs for TSF).\n- Comprehensive evaluation across architectures and tasks."}, "weaknesses": {"value": "- The theoretical insight is naive and does not provide much insight. The method is not novel in other domains, so it is more like an engineering work with good empirical results.\n- The baselines are simple (see Figure 4). Although there are no prior works but there should be more sophisticated weighting methods they can compare with. This can demonstrate if the backward model actually provides effective guidance or if the effect simply comes from averaging.\n- The computation cost is somewhat high (1-15x slower with 2-12% improvement), and they did not mention or count the computation cost for training the backward model."}, "questions": {"value": "Can you try to justify the effectiveness of the backward model as a guidance more clearly? I believe comparing with more comprehensive baselines (for example entropy-guided or average) would also solve this.\nI will raise my score if you solve this well (since the score I would like to give now is somewhat between 4 and 6)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xtGfktfQm6", "forum": "xpxgepkpRO", "replyto": "xpxgepkpRO", "signatures": ["ICLR.cc/2026/Conference/Submission7281/Reviewer_tnTg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7281/Reviewer_tnTg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813186101, "cdate": 1761813186101, "tmdate": 1763739304879, "mdate": 1763739304879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an inference-time scaling method for time series data. The proposed method can be applied to both time series forecasting and time series imputation tasks. The core idea is to create multiple outputs from a pretrained model and then weight the outputs based on how well they reconstruct the input. The proposed method yields improved performance in the experiments, demonstrating the efficacy of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a robust empirical evaluation, utilising multiple datasets and methods. \n\nThe reweighting strategy is an interesting and effective method based on the reconstruction that improved performance across datasets."}, "weaknesses": {"value": "While the experiment section is strong, it only reports single-time statistics. To understand the full efficacy of methods, it would be better to have the experiments repeated over multiple runs and presented with the standard deviations. \n\nSee \"Questions*\" below for more"}, "questions": {"value": "1. Line 67-68: These line seems unnecessary given the same information appears in the contributions just below:\n\n2. The multi-candidate generation is done via Monte Carlo Dropout, which can provide very narrow sets of candidates. Furthermore, the probabilistic statements made from it are largely miscalibrated. It would be interesting to see how the results differ when using generative models such as flow-based models with or without calibrated probabilistic statements via Conformal Prediction.\n\n3. Line 160-161: A backward model needs to be trained for reconstruction. However, a strong backward model might be able to reconstruct from any of the given candidates. An interesting example could be how diffusion models work; even if there is complete noise, one might be able to reconstruct the true distribution, even if the score function is known.\n\n4. Line 204: X belong to R (F X C), does it need to only reconstruct S steps in the past? Also, why was it needed to divide the prediction horizon into segments? All of the prediction horizon could be predicted at once, no?\n\n5. The theoretical analysis seems restricted to MC dropout; are any other forms of probabilistic candidate generation supported?\n\n6. Line 341: \"the Exchange dataset, we follow the hyperparameter settings of the Weather dataset\": Why were the same hyperparameters used as in the Weather dataset? The datasets seem quite different. Isn't it better to perform a hyperparameter search for the Exchange dataset separately? \n\n7. How critical is the training of the backward model? How well-trained should it be?\n\n8. Line 419: \"Exchagne\" is written instead of \"Exchange\".\n\n9. Is there any rule of thumb to find the right number of candidates, K?\n\n10. Only reports single-time statistics. To understand the full efficacy of methods, it would be better to have the experiments repeated over multiple runs and presented with the standard deviations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b4zteAbm2o", "forum": "xpxgepkpRO", "replyto": "xpxgepkpRO", "signatures": ["ICLR.cc/2026/Conference/Submission7281/Reviewer_SLG7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7281/Reviewer_SLG7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762033493057, "cdate": 1762033493057, "tmdate": 1762919405673, "mdate": 1762919405673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}