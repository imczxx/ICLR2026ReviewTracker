{"id": "mDOZjDwDwQ", "number": 1562, "cdate": 1756892021379, "mdate": 1763759907491, "content": {"title": "Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning", "abstract": "As supervised fine-tuning (SFT) evolves from a lightweight post-training step into a compute-intensive phase rivaling mid-training in scale, data efficiency has become critical for aligning large language models (LLMs) under tight budgets. Existing data pruning methods suffer from a fragmented design: they operate either at the sample level or the token level in isolation, failing to jointly optimize both dimensions. This disconnect leads to significant inefficiencies—high-value samples may still contain redundant tokens, while token-level pruning often discards crucial instructional or corrective signals embedded in individual examples. To address this bottleneck, we introduce the *Error–Uncertainty (EU) Plane*, a diagnostic framework that jointly characterizes the heterogeneous utility of training data across samples and tokens. Guided by this insight, we propose *Quadrant-based Tuning (Q-Tuning)*, a unified framework that strategically coordinates sample pruning and token pruning. Q-Tuning employs a two-stage strategy: first, it performs sample-level triage to retain examples rich in informative misconceptions or calibration signals; second, it applies an asymmetric token-pruning policy, using a context-aware scoring mechanism to trim less salient tokens exclusively from misconception samples while preserving calibration samples in their entirety. Our method sets a new state of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B, Q-Tuning achieves a +38\\% average improvement over the full-data SFT baseline using only 12.5\\% of the original training data. As the first dynamic pruning approach to consistently outperform full-data training, Q-Tuning provides a practical and scalable blueprint for maximizing data utilization in budget-constrained LLM SFT. The code is attached in the supplementary materials.", "tldr": "We introduce the Error–Uncertainty (EU) Plane to capture data variation, and build on it to design Quadrant-based Tuning (Q-Tuning), the first joint sample and token pruning framework for efficient LLM fine-tuning that beats full-data training..", "keywords": ["Large Language Model", "Data Efficiency", "Data Pruning", "Token Pruning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1556bfb31f1dcf96b0a19bebdb6b8cc4352c0a8.pdf", "supplementary_material": "/attachment/3781df04b6c5ce48052b07a63c4402ed0c3499d8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Q-Tuning algorithm, which incorporates perplexity and predictive entropy to measure the error and uncertainty, and then make the sample-level and token-level pruning. The experiments across various settings validate the effectiveness of the algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The presentation is quite good. The illustration is quite helpful and the intuition behind algorithm is easy to understand.\n2. The experimental result is comprehensive."}, "weaknesses": {"value": "1. The algorithms depends the definition of $\\alpha$ and $\\beta$. The calculation that $0.5*(\\alpha_{low} + \\alpha_{high}) $ is quite heuristic. The author is encouraged to try other potential calculations. I wonder for different datasets, should we use different calculation methods since the error and uncertainty threshold might be quite different for math and creative writing. \n\n2. The author is encouraged to include more case study to understand how sample and token pruning works in practice, better across different domains. I am curious about which token will be dropped by token pruning.\n\n3. If possible, the author can provide some theoretical analysis for current algorithm. The current algorithm is quite experimental and highly require the setup of the hyper-parameters."}, "questions": {"value": "Please check the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FVeXxM7i2Z", "forum": "mDOZjDwDwQ", "replyto": "mDOZjDwDwQ", "signatures": ["ICLR.cc/2026/Conference/Submission1562/Reviewer_MSz7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1562/Reviewer_MSz7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546536133, "cdate": 1761546536133, "tmdate": 1762915813946, "mdate": 1762915813946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the critical challenge of data efficiency in supervised fine-tuning for large language models by introducing a unified approach to sample and token pruning. The authors present the Error-Uncertainty (EU) Plane, a diagnostic tool mapping training instances based on perplexity and entropy, which reveals data heterogeneity at both sample and token levels. Leveraging these insights, they propose Quadrant-based Tuning (Q-Tuning), a bi-level, context-aware pruning strategy that prunes at both sample and token levels depending on their EU quadrant classification. Evaluations across several models (LLaMA2-7B, LLaMA3-8B, Mistral-7B, SmolLM2-1.7B), benchmarks, and pruning budgets show Q-Tuning consistently outperforms both standard and recent pruning baselines, achieving results that sometimes surpass even full-data fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear Motivation & Unification**: The paper incisively diagnoses a gap in existing data pruning strategies, namely the isolated and uncoordinated treatment of sample-level and token-level pruning. The formalization of joint sample-token pruning (Generalized Dynamic Data Pruning) is thoughtfully constructed.\n\n- **Novel Framework**: The EU Plane is an intuitive and effective way to classify and differentiate data utility across both error (perplexity) and uncertainty (entropy), leading directly into a well-motivated Quadrant-based strategy.\nMethodological Rigor & Clarity: The algorithm is explicitly and thoroughly described (see Algorithm 1 and associated discussion), including how sample and token thresholds are efficiently estimated batchwise using bisection.\n\n- **Strong Empirical Results**: Q-Tuning consistently matches or surpasses full-data SFT performance despite using only a small fraction of the training data (e.g., +38% improvement using just 12.5% of data on SmolLM2-1.7B). Tables 1 and 2 show Q-Tuning outperforms strong and varied baselines across diverse data-token keep ratios, models, and benchmarks.\n\n- **Thorough Analysis and Fair Baselines**: The experiments include extensive comparisons with both older and state-of-the-art sample and token pruning methods (including InfoBatch, FastV, SparseVLM, Entropy, etc.), ablation studies of batch size and neighborhood smoothing parameter, and analysis of Q-Tuning's robustness."}, "weaknesses": {"value": "- **Limited Theoretical Analysis Beyond Heuristic Justification**: While the bi-level optimization formalism is clearly presented, there is little in terms of theoretical guarantees about optimality, generalization (beyond empirical validation), or the properties of the partitioning/procedures under varying dataset/model regimes. The EU Plane partitioning, though motivated with intuition, is not probed rigorously for, e.g., stability, sensitivity, or edge-case behavior.\n\n- **Token Pruning Specifics and Ambiguities**: The paper's description of token-level pruning is mostly empirical/heuristic, focused on high-PPL and neighborhood smoothing. There is lack of detail on scenarios where the context-aware scoring mechanism might fail or degenerate (e.g., when high PPL signals are not semantically aligned with true error). There should be more analysis on cases where token-level and sample-level criteria may conflict or interact in undesirable ways."}, "questions": {"value": "- Sensitivity to Quadrant Boundaries and Dynamic Partitioning: How robust is Q-Tuning to instability in the EU Plane thresholds, especially early in training or on unbalanced datasets? Are there pathological cases where the quadrant assignment is unstable or assigns virtually all data to one region?\n\n- Token Pruning Failure Modes: Can the authors provide adversarial or edge-case analysis of where neighborhood-smoothed token-level PPL might prune semantically valuable content, and how such cases would be detected or remedied in practice?\n\n- Computational Overhead: While the paper claims negligible overhead for the per-batch EU Plane computation and threshold search, is there any analysis or reporting of actual wall-clock timing or additional GPU consumption, especially for models closer to production scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AHpPIgY06U", "forum": "mDOZjDwDwQ", "replyto": "mDOZjDwDwQ", "signatures": ["ICLR.cc/2026/Conference/Submission1562/Reviewer_2BBK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1562/Reviewer_2BBK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956434806, "cdate": 1761956434806, "tmdate": 1762915813727, "mdate": 1762915813727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Q-Tuning, a unified framework for joint sample-level and token-level pruning in supervised fine-tuning (SFT) of large language models. It introduces an Error–Uncertainty (EU) Plane, where each training sample is represented by its perplexity (error) and predictive entropy (uncertainty). By partitioning samples into four quadrants, Q-Tuning dynamically prunes uninformative samples and redundant tokens. Experiments across LLaMA, Mistral, and SmolLM demonstrate its effectiveness and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The EU Plane connects sample pruning and token pruning under a common information-theoretic view. It offers an interpretable diagnostic space (error vs. uncertainty) to understand data quality.\n\n- Q-Tuning consistently improves or matches full-data SFT across multiple model sizes and datasets. Results demonstrate both efficiency and robust generalization across instruction-following and reasoning benchmarks.\n\n- The quadrant-based partitioning provides clear interpretability, helping analyze “overconfident wrong” and “uncertain correct” samples—useful for future dataset curation and alignment studies."}, "weaknesses": {"value": "- Incomplete efficiency analysis.  While data reduction is highlighted, actual compute savings (e.g., FLOPs, wall-clock time) as well as baselines' are not quantified. This makes the cost–benefit trade-off less clear.\n\n- The paper treats perplexity and entropy as independent axes for modeling error and uncertainty, but their relationship is not rigorously analyzed. Both are derived from the same model probability distribution—perplexity reflects the likelihood of the ground-truth token, while entropy captures the overall uncertainty—making them inherently correlated. Their connection can be written as\n $log(PPL) = H_{pred} + KL(p_{true} || p_{theta})$, where the KL term measures the divergence between the true and model distributions. This indicates that perplexity is not fully independent of entropy, but rather a function of entropy plus model–data mismatch, questioning whether the proposed Error–Uncertainty Plane truly represents two orthogonal dimensions of data quality.\n\n\n- The chosen baselines are somewhat mismatched to the paper’s setting. SparseVLM and FastV focus on visual token pruning, which is not directly comparable to the text-only tasks evaluated in this work. For such tasks, methods like Rho-1 and TokenCleaning [1] would serve as more appropriate token-level baselines. In addition, the selected sample-level baselines are overly simplistic. Recent and stronger approaches—such as LLM rating methods (e.g., Alpagasus [2], DELTA [3], DS2 [4]) or gradient-based methods like LESS [5]—would provide fairer and more competitive comparisons. Therefore, I remain somewhat skeptical about the reported improvements.\n\n[1] Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning, ICML 2025.\n\n[2] Alpagasus training a better alpaca model with fewer data, ICLR 2024.\n\n[3] Deita: Data-Efficient Instruction Tuning for Alignment, ICLR 2024.\n\n[4] Improving data efficiency via curating llm-driven rating systems, ICLR 2025.\n\n[5] Less: selecting influential data for targeted instruction tuning, ICML 2024."}, "questions": {"value": "1. Compared with sample-level methods such as DELTA, DS2, and LESS, which typically retain only 2–5% of the data, Q-Tuning’s use of 12.5% of the training set appears less data-efficient and may reduce its practical advantage.\n\n2. Could the thresholds (rsample, rtoken) adapt dynamically during training?\n\n3. How often is the EU Plane recomputed, and what’s the runtime overhead?\n\n4. An ablation on training efficiency (both token- and sample-level) compared to full-data SFT would be valuable to show whether Q-Tuning actually accelerates training in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lYfTHXtDEn", "forum": "mDOZjDwDwQ", "replyto": "mDOZjDwDwQ", "signatures": ["ICLR.cc/2026/Conference/Submission1562/Reviewer_ZLqF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1562/Reviewer_ZLqF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958527544, "cdate": 1761958527544, "tmdate": 1762915813534, "mdate": 1762915813534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper present a 2-stage data pruning framework for SFT. In the first stage, we will construct an error-uncertainty plane by computing the perplexity and entropy for every sample using a gradient-free forward pass. Then we apply a bisect search on identifying the proper quantiles for perplexity and entropy and pick out Q2 (high ppl, low ent) and Q4 (low ppl, high ent) samples. We discard all samples in Q1 and Q3. We then prune tokens in Q2 by identifying tokens that have both token-level PPL and neighbor token PPL. The experiments are conducted mainly on GSM8K, TriviaQA, SQuAD, Arc benchmarks with Llama2 and Mistral 7B, and show Q-Tuning can match full data SFT results, and demonstrate superiority over other baselines under the same budget across models and ratio."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper flow is quite clear and the motivation and specific pruning methodology is well-explained. \n\n- The empirical evaluation is comprehensive."}, "weaknesses": {"value": "- **critical** I understand the motivation of data pruning and adopting perplexity and entropy to produce a EU-plane, but a complete justification on this 2-stage framework is missing from the paper. In particular, the paper makes a few implicit assumptions without justification: \n  1. keeping Q1 will harm learning and therefore cannot be combined with either Q2 and Q4 (both high entropy and high ppl and their combination are worse than Q2 + Q4). \n  2. all tokens, especially more confident tokens, in Q4 are (still) important or should not be pruned. \n  3. the high PPL tokens in Q2 are indeed detrimental\n\nSuch justification is essential for supporting the validity of Q-Tuning and a quantitative ablation study should be added. \n\n- I wonder if it is computationally feasible to have at least 1 model evaluated on larger scale than 7B. The current model is all small-scaled (<= 8B).\n\n- I wonder if it is possible to compare with at least 1 or 2 conventional sample selection method in coreset selection community (sample level should be fine, not necessarily adapted to token-level).\n\nDisclaimer: I am less familiar with the data pruning approach in LLM community so I cannot accurately assess the technical novelty. This assessment is more targeted on the technical soundness.\n\nOverall, I think this is still a good paper but needs more grounded evidence to support the validity of this 2-stage framework. I am happy to raise score if the first weakness is properly addressed with more rigorous ablation studies."}, "questions": {"value": "All questions are listed in the weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "37ZBY39xW7", "forum": "mDOZjDwDwQ", "replyto": "mDOZjDwDwQ", "signatures": ["ICLR.cc/2026/Conference/Submission1562/Reviewer_Dzda"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1562/Reviewer_Dzda"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762799622579, "cdate": 1762799622579, "tmdate": 1762915813380, "mdate": 1762915813380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "(Part 1) General Response"}, "comment": {"value": "We sincerely thank the reviewers for their time and constructive feedback. We are encouraged by the reviewers’ shared recognition of the need for principled and scalable data pruning in SFT, and we appreciate the positive assessments of our proposed Q-Tuning framework, including its unified formulation, the clarity of its methodology, and the strength of its empirical results. To address reviewer concerns and strengthen the paper, we conducted extensive new experiments and analyses during the rebuttal period. All new results are included in the revised manuscript.\n\n**1. Extended Evaluation on Qwen3-8B**  \n   To further demonstrate generalization beyond LLaMA and Mistral, we extended our study to Qwen3-8B on OpenHermes under 12.5 percent and 25 percent sample ratios with 50 percent or 70 percent tokens. Q-Tuning consistently matches or surpasses full-data SFT; for example, with 12.5 percent samples and 50 percent tokens it reaches **60.19**, clearly higher than the full-data **55.45**. **These results are included in Appendix G.2 of the revised manuscript.**\n\n**2. Stronger Baselines for Sample-Level and Token-Level Pruning**  \n   To provide a more rigorous comparison, we incorporated recent sample pruning baselines (Alpagasus, Deita, DS2, Less) and token pruning or cleaning methods (Rho-1, TokenCleaning). Across identical budgets, Q-Tuning consistently delivers the best results and often exceeds full-data SFT; for instance, on LLaMA2-7B with 50 percent tokens it obtains **37.74** and **36.47** at 12.5 percent and 25 percent samples, both above the full-data **35.36**. **These results are included in Appendix G.3 of the revised manuscript.**\n\n**3. Extreme Low-Data Regimes on Mistral-7B**  \n   To examine data efficiency under severe constraints, we evaluated Q-Tuning when retaining only 3 to 9 percent of total tokens. Even in this extreme setting, Q-Tuning maintains averages of **46.23–46.92**, outperforming zero-shot (**36.98**) and surpassing full-data SFT (**45.84**). Notably, with 6.25 percent samples and 50 percent tokens—effectively **3.125 percent** data—Q-Tuning continues to achieve higher accuracy than full-data SFT. **These results are included in Appendix G.4 of the revised manuscript.**\n\n**4. Scalability Across Qwen3-8B, Qwen3-14B, and Qwen3-32B**  \n   To assess scaling trends, we evaluated Q-Tuning on Qwen3 models of 8B, 14B, and 32B parameters. Q-Tuning consistently outperforms zero-shot and full-data SFT, with the margin increasing for larger models; on GSM8K and ARC-C, Qwen3-32B with Q-Tuning clearly exceeds its full-data counterpart, especially with 70 percent tokens. **These results are included in Appendix G.5 of the revised manuscript.**\n\n**5. Quadrant-Level Ablation of the EU-Plane Design**  \n   To validate the Error–Uncertainty Plane, we selectively enabled or disabled pruning in Q1–Q4 at both sample and token levels. Across Wizard and OpenHermes, the best-performing configurations consistently match our intended strategy: pruning Q1/Q3 at the sample level and pruning only Q2 at the token level. **These results are included in Appendix H of the revised manuscript.**\n\n**6. Semantic Analysis of Pruned and Kept Tokens**  \n   To further understand Q-Tuning’s token selections, we analyzed over 44,000 annotated tokens from 300 Q2 samples. Q-Tuning reliably removes boilerplate and filler content while preserving gold answer spans, reasoning-critical steps, and key numerical tokens, confirming the effectiveness of neighbor-aware smoothing. **These results are included in Appendix I of the revised manuscript.**\n\n**7. Efficiency Analysis in Theory and Practice**  \n   To complement effectiveness results, we conducted a theoretical FLOPs analysis and empirical measurements. Q-Tuning reduces cost from three times the forward computation to roughly (1 plus 3r) times the forward computation, requiring r below two-thirds for efficiency, which holds in all settings. Empirically, on LLaMA2-7B with eight A100 GPUs, Q-Tuning reduces training time from 90 minutes to **65 minutes** and memory usage from **78 percent to 27 percent**, while maintaining or improving accuracy. **These results are included in Appendix J of the revised manuscript.**"}}, "id": "RL2OewmREC", "forum": "mDOZjDwDwQ", "replyto": "mDOZjDwDwQ", "signatures": ["ICLR.cc/2026/Conference/Submission1562/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1562/Authors"], "number": 23, "invitations": ["ICLR.cc/2026/Conference/Submission1562/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763762705938, "cdate": 1763762705938, "tmdate": 1763762938094, "mdate": 1763762938094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "(Part 2) General Response"}, "comment": {"value": "We sincerely thank the Area Chair and all reviewers for their thoughtful and constructive feedback. We are encouraged by the shared recognition of the value of principled, scalable data pruning for SFT, and we appreciate the reviewers’ clear acknowledgment of Q-Tuning’s motivation, formulation, and empirical strength.\n\n**Clarity of Motivation and Methodology**  \nMultiple reviewers noted that the motivation behind Q-Tuning is clearly presented and that the pruning methodology is well explained. Reviewer Dzda emphasized that the flow of the paper is clear and that both the motivation and the specific pruning strategy are well-articulated. Reviewer MSz7 similarly highlighted that the intuition behind the algorithm is easy to understand and that the illustrations effectively support comprehension.\n\n**Interpretability and the EU-Plane Formulation**  \nReviewers consistently recognized the interpretability of the Error–Uncertainty Plane. Reviewer ZLqF highlighted that the EU Plane provides an interpretable diagnostic space linking error and uncertainty, helping to understand data quality. The reviewer further noted that the quadrant-based structure clarifies patterns such as “overconfident wrong” and “uncertain correct,” which benefits dataset curation and alignment analysis. Reviewer 2BBK echoed this by pointing out that the EU Plane is an intuitive and effective way to differentiate data utility and motivates the quadrant-based pruning strategy.\n\n**Unified and Well-Formulated Framework**  \nQ-Tuning’s unification of sample-level and token-level pruning was positively received. Reviewer 2BBK emphasized that our work identifies a gap in existing approaches that treat these levels in an uncoordinated manner and commended the thoughtful formulation of the generalized sample–token pruning framework. Reviewer ZLqF also recognized that Q-Tuning provides a coherent and information-theoretic view connecting both pruning dimensions.\n\n**Methodological Rigor and Algorithmic Clarity**  \nReviewers appreciated the rigor and explicitness of the method. Reviewer 2BBK pointed out that the algorithm is thoroughly described, including details such as bisect-based threshold estimation. Reviewer MSz7 noted that the presentation is clear and supported by helpful illustrations.\n\n**Strong and Comprehensive Empirical Results**  \nReviewers highlighted that the empirical evaluation is broad and compelling. Reviewer Dzda noted that Q-Tuning matches full-data SFT results and demonstrates superiority over baselines across models and budget settings. Reviewer ZLqF emphasized that Q-Tuning consistently improves or matches full-data performance across model sizes and datasets. Reviewer 2BBK also commended the strong empirical results, noting that Q-Tuning sometimes even surpasses full-data SFT, and acknowledged the comprehensive comparisons with both older and recent pruning baselines."}}, "id": "NLxFmefuEi", "forum": "mDOZjDwDwQ", "replyto": "mDOZjDwDwQ", "signatures": ["ICLR.cc/2026/Conference/Submission1562/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1562/Authors"], "number": 24, "invitations": ["ICLR.cc/2026/Conference/Submission1562/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763763613491, "cdate": 1763763613491, "tmdate": 1763763613491, "mdate": 1763763613491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}