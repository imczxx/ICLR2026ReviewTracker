{"id": "mDOZjDwDwQ", "number": 1562, "cdate": 1756892021379, "mdate": 1759898201853, "content": {"title": "Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning", "abstract": "As supervised fine-tuning (SFT) evolves from a lightweight post-training step into a compute-intensive phase rivaling mid-training in scale, data efficiency has become critical for aligning large language models (LLMs) under tight budgets. Existing data pruning methods suffer from a fragmented design: they operate either at the sample level or the token level in isolation, failing to jointly optimize both dimensions. This disconnect leads to significant inefficiencies—high-value samples may still contain redundant tokens, while token-level pruning often discards crucial instructional or corrective signals embedded in individual examples. To address this bottleneck, we introduce the *Error–Uncertainty (EU) Plane*, a diagnostic framework that jointly characterizes the heterogeneous utility of training data across samples and tokens. Guided by this insight, we propose *Quadrant-based Tuning (Q-Tuning)*, a unified framework that strategically coordinates sample pruning and token pruning. Q-Tuning employs a two-stage strategy: first, it performs sample-level triage to retain examples rich in informative misconceptions or calibration signals; second, it applies an asymmetric token-pruning policy, using a context-aware scoring mechanism to trim less salient tokens exclusively from misconception samples while preserving calibration samples in their entirety. Our method sets a new state of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B, Q-Tuning achieves a +38\\% average improvement over the full-data SFT baseline using only 12.5\\% of the original training data. As the first dynamic pruning approach to consistently outperform full-data training, Q-Tuning provides a practical and scalable blueprint for maximizing data utilization in budget-constrained LLM SFT. The code is attached in the supplementary materials.", "tldr": "We introduce the Error–Uncertainty (EU) Plane to capture data variation, and build on it to design Quadrant-based Tuning (Q-Tuning), the first joint sample and token pruning framework for efficient LLM fine-tuning that beats full-data training..", "keywords": ["Large Language Model", "Data Efficiency", "Data Pruning", "Token Pruning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7892c031ca79467b878460e9e510c32786abd163.pdf", "supplementary_material": "/attachment/3781df04b6c5ce48052b07a63c4402ed0c3499d8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Q-Tuning algorithm, which incorporates perplexity and predictive entropy to measure the error and uncertainty, and then make the sample-level and token-level pruning. The experiments across various settings validate the effectiveness of the algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The presentation is quite good. The illustration is quite helpful and the intuition behind algorithm is easy to understand.\n2. The experimental result is comprehensive."}, "weaknesses": {"value": "1. The algorithms depends the definition of $\\alpha$ and $\\beta$. The calculation that $0.5*(\\alpha_{low} + \\alpha_{high}) $ is quite heuristic. The author is encouraged to try other potential calculations. I wonder for different datasets, should we use different calculation methods since the error and uncertainty threshold might be quite different for math and creative writing. \n\n2. The author is encouraged to include more case study to understand how sample and token pruning works in practice, better across different domains. I am curious about which token will be dropped by token pruning.\n\n3. If possible, the author can provide some theoretical analysis for current algorithm. The current algorithm is quite experimental and highly require the setup of the hyper-parameters."}, "questions": {"value": "Please check the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FVeXxM7i2Z", "forum": "mDOZjDwDwQ", "replyto": "mDOZjDwDwQ", "signatures": ["ICLR.cc/2026/Conference/Submission1562/Reviewer_MSz7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1562/Reviewer_MSz7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546536133, "cdate": 1761546536133, "tmdate": 1762915813946, "mdate": 1762915813946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the critical challenge of data efficiency in supervised fine-tuning for large language models by introducing a unified approach to sample and token pruning. The authors present the Error-Uncertainty (EU) Plane, a diagnostic tool mapping training instances based on perplexity and entropy, which reveals data heterogeneity at both sample and token levels. Leveraging these insights, they propose Quadrant-based Tuning (Q-Tuning), a bi-level, context-aware pruning strategy that prunes at both sample and token levels depending on their EU quadrant classification. Evaluations across several models (LLaMA2-7B, LLaMA3-8B, Mistral-7B, SmolLM2-1.7B), benchmarks, and pruning budgets show Q-Tuning consistently outperforms both standard and recent pruning baselines, achieving results that sometimes surpass even full-data fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear Motivation & Unification**: The paper incisively diagnoses a gap in existing data pruning strategies, namely the isolated and uncoordinated treatment of sample-level and token-level pruning. The formalization of joint sample-token pruning (Generalized Dynamic Data Pruning) is thoughtfully constructed.\n\n- **Novel Framework**: The EU Plane is an intuitive and effective way to classify and differentiate data utility across both error (perplexity) and uncertainty (entropy), leading directly into a well-motivated Quadrant-based strategy.\nMethodological Rigor & Clarity: The algorithm is explicitly and thoroughly described (see Algorithm 1 and associated discussion), including how sample and token thresholds are efficiently estimated batchwise using bisection.\n\n- **Strong Empirical Results**: Q-Tuning consistently matches or surpasses full-data SFT performance despite using only a small fraction of the training data (e.g., +38% improvement using just 12.5% of data on SmolLM2-1.7B). Tables 1 and 2 show Q-Tuning outperforms strong and varied baselines across diverse data-token keep ratios, models, and benchmarks.\n\n- **Thorough Analysis and Fair Baselines**: The experiments include extensive comparisons with both older and state-of-the-art sample and token pruning methods (including InfoBatch, FastV, SparseVLM, Entropy, etc.), ablation studies of batch size and neighborhood smoothing parameter, and analysis of Q-Tuning's robustness."}, "weaknesses": {"value": "- **Limited Theoretical Analysis Beyond Heuristic Justification**: While the bi-level optimization formalism is clearly presented, there is little in terms of theoretical guarantees about optimality, generalization (beyond empirical validation), or the properties of the partitioning/procedures under varying dataset/model regimes. The EU Plane partitioning, though motivated with intuition, is not probed rigorously for, e.g., stability, sensitivity, or edge-case behavior.\n\n- **Token Pruning Specifics and Ambiguities**: The paper's description of token-level pruning is mostly empirical/heuristic, focused on high-PPL and neighborhood smoothing. There is lack of detail on scenarios where the context-aware scoring mechanism might fail or degenerate (e.g., when high PPL signals are not semantically aligned with true error). There should be more analysis on cases where token-level and sample-level criteria may conflict or interact in undesirable ways."}, "questions": {"value": "- Sensitivity to Quadrant Boundaries and Dynamic Partitioning: How robust is Q-Tuning to instability in the EU Plane thresholds, especially early in training or on unbalanced datasets? Are there pathological cases where the quadrant assignment is unstable or assigns virtually all data to one region?\n\n- Token Pruning Failure Modes: Can the authors provide adversarial or edge-case analysis of where neighborhood-smoothed token-level PPL might prune semantically valuable content, and how such cases would be detected or remedied in practice?\n\n- Computational Overhead: While the paper claims negligible overhead for the per-batch EU Plane computation and threshold search, is there any analysis or reporting of actual wall-clock timing or additional GPU consumption, especially for models closer to production scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AHpPIgY06U", "forum": "mDOZjDwDwQ", "replyto": "mDOZjDwDwQ", "signatures": ["ICLR.cc/2026/Conference/Submission1562/Reviewer_2BBK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1562/Reviewer_2BBK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956434806, "cdate": 1761956434806, "tmdate": 1762915813727, "mdate": 1762915813727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Q-Tuning, a unified framework for joint sample-level and token-level pruning in supervised fine-tuning (SFT) of large language models. It introduces an Error–Uncertainty (EU) Plane, where each training sample is represented by its perplexity (error) and predictive entropy (uncertainty). By partitioning samples into four quadrants, Q-Tuning dynamically prunes uninformative samples and redundant tokens. Experiments across LLaMA, Mistral, and SmolLM demonstrate its effectiveness and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The EU Plane connects sample pruning and token pruning under a common information-theoretic view. It offers an interpretable diagnostic space (error vs. uncertainty) to understand data quality.\n\n- Q-Tuning consistently improves or matches full-data SFT across multiple model sizes and datasets. Results demonstrate both efficiency and robust generalization across instruction-following and reasoning benchmarks.\n\n- The quadrant-based partitioning provides clear interpretability, helping analyze “overconfident wrong” and “uncertain correct” samples—useful for future dataset curation and alignment studies."}, "weaknesses": {"value": "- Incomplete efficiency analysis.  While data reduction is highlighted, actual compute savings (e.g., FLOPs, wall-clock time) as well as baselines' are not quantified. This makes the cost–benefit trade-off less clear.\n\n- The paper treats perplexity and entropy as independent axes for modeling error and uncertainty, but their relationship is not rigorously analyzed. Both are derived from the same model probability distribution—perplexity reflects the likelihood of the ground-truth token, while entropy captures the overall uncertainty—making them inherently correlated. Their connection can be written as\n $log(PPL) = H_{pred} + KL(p_{true} || p_{theta})$, where the KL term measures the divergence between the true and model distributions. This indicates that perplexity is not fully independent of entropy, but rather a function of entropy plus model–data mismatch, questioning whether the proposed Error–Uncertainty Plane truly represents two orthogonal dimensions of data quality.\n\n\n- The chosen baselines are somewhat mismatched to the paper’s setting. SparseVLM and FastV focus on visual token pruning, which is not directly comparable to the text-only tasks evaluated in this work. For such tasks, methods like Rho-1 and TokenCleaning [1] would serve as more appropriate token-level baselines. In addition, the selected sample-level baselines are overly simplistic. Recent and stronger approaches—such as LLM rating methods (e.g., Alpagasus [2], DELTA [3], DS2 [4]) or gradient-based methods like LESS [5]—would provide fairer and more competitive comparisons. Therefore, I remain somewhat skeptical about the reported improvements.\n\n[1] Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning, ICML 2025.\n\n[2] Alpagasus training a better alpaca model with fewer data, ICLR 2024.\n\n[3] Deita: Data-Efficient Instruction Tuning for Alignment, ICLR 2024.\n\n[4] Improving data efficiency via curating llm-driven rating systems, ICLR 2025.\n\n[5] Less: selecting influential data for targeted instruction tuning, ICML 2024."}, "questions": {"value": "1. Compared with sample-level methods such as DELTA, DS2, and LESS, which typically retain only 2–5% of the data, Q-Tuning’s use of 12.5% of the training set appears less data-efficient and may reduce its practical advantage.\n\n2. Could the thresholds (rsample, rtoken) adapt dynamically during training?\n\n3. How often is the EU Plane recomputed, and what’s the runtime overhead?\n\n4. An ablation on training efficiency (both token- and sample-level) compared to full-data SFT would be valuable to show whether Q-Tuning actually accelerates training in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lYfTHXtDEn", "forum": "mDOZjDwDwQ", "replyto": "mDOZjDwDwQ", "signatures": ["ICLR.cc/2026/Conference/Submission1562/Reviewer_ZLqF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1562/Reviewer_ZLqF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958527544, "cdate": 1761958527544, "tmdate": 1762915813534, "mdate": 1762915813534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper present a 2-stage data pruning framework for SFT. In the first stage, we will construct an error-uncertainty plane by computing the perplexity and entropy for every sample using a gradient-free forward pass. Then we apply a bisect search on identifying the proper quantiles for perplexity and entropy and pick out Q2 (high ppl, low ent) and Q4 (low ppl, high ent) samples. We discard all samples in Q1 and Q3. We then prune tokens in Q2 by identifying tokens that have both token-level PPL and neighbor token PPL. The experiments are conducted mainly on GSM8K, TriviaQA, SQuAD, Arc benchmarks with Llama2 and Mistral 7B, and show Q-Tuning can match full data SFT results, and demonstrate superiority over other baselines under the same budget across models and ratio."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper flow is quite clear and the motivation and specific pruning methodology is well-explained. \n\n- The empirical evaluation is comprehensive."}, "weaknesses": {"value": "- **critical** I understand the motivation of data pruning and adopting perplexity and entropy to produce a EU-plane, but a complete justification on this 2-stage framework is missing from the paper. In particular, the paper makes a few implicit assumptions without justification: \n  1. keeping Q1 will harm learning and therefore cannot be combined with either Q2 and Q4 (both high entropy and high ppl and their combination are worse than Q2 + Q4). \n  2. all tokens, especially more confident tokens, in Q4 are (still) important or should not be pruned. \n  3. the high PPL tokens in Q2 are indeed detrimental\n\nSuch justification is essential for supporting the validity of Q-Tuning and a quantitative ablation study should be added. \n\n- I wonder if it is computationally feasible to have at least 1 model evaluated on larger scale than 7B. The current model is all small-scaled (<= 8B).\n\n- I wonder if it is possible to compare with at least 1 or 2 conventional sample selection method in coreset selection community (sample level should be fine, not necessarily adapted to token-level).\n\nDisclaimer: I am less familiar with the data pruning approach in LLM community so I cannot accurately assess the technical novelty. This assessment is more targeted on the technical soundness.\n\nOverall, I think this is still a good paper but needs more grounded evidence to support the validity of this 2-stage framework. I am happy to raise score if the first weakness is properly addressed with more rigorous ablation studies."}, "questions": {"value": "All questions are listed in the weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "37ZBY39xW7", "forum": "mDOZjDwDwQ", "replyto": "mDOZjDwDwQ", "signatures": ["ICLR.cc/2026/Conference/Submission1562/Reviewer_Dzda"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1562/Reviewer_Dzda"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762799622579, "cdate": 1762799622579, "tmdate": 1762915813380, "mdate": 1762915813380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}