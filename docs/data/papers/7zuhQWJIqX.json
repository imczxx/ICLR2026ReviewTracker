{"id": "7zuhQWJIqX", "number": 10489, "cdate": 1758173435320, "mdate": 1759897647714, "content": {"title": "Towards Policy-Compliant Agents: Learning Efficient Guardrails for Policy Violation Detection", "abstract": "Autonomous web agents need to operate under externally imposed or human-specified policies while generating long-horizon trajectories. \nHowever, little work has examined whether these trajectories comply with such policies, or whether policy violations persist across different contexts such as domains (e.g., shopping or coding websites) and subdomains (e.g., product search and order management in shopping). \nTo address this gap, we introduce PolicyGuardBench, a benchmark of about 60k examples for detecting policy violations in agent trajectories. From diverse agent runs, we generate a broad set of policies and create both within subdomain and cross subdomain pairings with violation labels. In addition to full-trajectory evaluation, PolicyGuardBench also includes a prefix-based violation detection task where models must anticipate policy violations from truncated trajectory prefixes rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a lightweight guardrail model that delivers strong detection accuracy across all tasks while keeping inference efficient.\nNotably, PolicyGuard-4B generalizes across domains and preserves high accuracy on unseen settings.\nTogether, PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework for studying policy compliance in web agent trajectories, and show that accurate and generalizable guardrails are feasible at small scales.", "tldr": "We introduce TrajGuardBench, a 60k-scale benchmark for detecting policy–trajectory violations in web agents, and train PolicyGuard-4B, a lightweight guardrail that outperforms larger baselines while generalizing across domains.", "keywords": ["Web Agents", "Guardrails", "Policy Compliance"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f7275afcb3d241847ec19f55642806eb6b74dc8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces POLICYGUARDBENCH, a 60k-scale benchmark for detecting policy violations in web agent trajectories, and proposes POLICYGUARD-4B, a 4B-parameter lightweight guardrail model trained on it. While most existing guardrails target safety (e.g., filtering harmful or insecure actions), the authors argue that policy compliance, adherence to domain- or institution-specific constraints, is an orthogonal dimension of agent reliability. They empirically show that safety-oriented guardrails (e.g., LlamaGuard, ShieldGemma) fail to detect policy violations even when safety risks are absent. To address this gap, the authors construct POLICYGUARDBENCH by systematically generating and annotating policy–trajectory pairs derived from 733 standardized agent trajectories across five domains (Reddit, Map, GitLab, Shopping, Shopping-Admin). Each trajectory–policy pair is labeled as “violation” or “no violation” under formalized rule types (obligation, prohibition, ordering, and conditional). The benchmark also supports prefix-based violation detection, where models must anticipate violations from truncated trajectories, probing early detection capabilities. POLICYGUARD-4B, fine-tuned from Qwen3-4B-Instruct, demonstrates cross-domain generalization and early-prediction accuracy on prefix tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper distinguishes policy compliance from traditional safety, introducing a new paradigm in agent evaluation. This conceptual clarity fills a gap between safety research and practical deployment constraints in real-world environments."}, "weaknesses": {"value": "1) While the dataset includes 2,195 policies, most are synthetically generated from existing trajectories. The degree of semantic and domain diversity (e.g., regulatory, ethical, or legal policies) is not fully quantified, leaving uncertainty about real-world coverage.\n\n2) The evaluation aggregates all violation types (obligation, prohibition, ordering, conditional) into a single binary metric. Disaggregated performance across these subtypes might provide deeper insight into where the model succeeds or struggles.\n\n3) While POLICYGUARD-4B performs well, its interpretability is limited to prediction-level metrics. A qualitative or attribution-based study could reveal which features (actions, policy clauses) drive violation detection."}, "questions": {"value": "Q1: How consistent are policy–trajectory pairings given the use of embedding-based retrieval and LLM scoring? What proportion of automatically matched pairs were manually corrected during annotation? Could spurious alignments (e.g., unrelated policies matched to trajectories) bias the model toward superficial lexical features?\n\nQ2: GPT-based annotation is calibrated via human seed data. What is the inter-annotator agreement (Cohen’s kappa) between human and model labels? How often did low-confidence labels trigger manual review, and were any systematic biases (e.g., over-flagging prohibitions) observed?\n\nQ3: The prefix-based task reveals highest accuracy at N = 1. Can the authors explain why shorter prefixes outperform longer ones? Does this reflect priors (most early actions are compliant) or detection bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n9DxZ8jthU", "forum": "7zuhQWJIqX", "replyto": "7zuhQWJIqX", "signatures": ["ICLR.cc/2026/Conference/Submission10489/Reviewer_onuQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10489/Reviewer_onuQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938837058, "cdate": 1761938837058, "tmdate": 1762921777690, "mdate": 1762921777690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new benchmark and model framework for ensuring policy compliance in autonomous web agents.\nThe results show the proposed 4B model achieves 90.1% accuracy and 87.6% F1, outperforming much larger models (e.g., Llama-3.3-70B, Qwen2.5-72B) and closed-source systems (Gemini, Claude) while maintaining low inference latency (22.5 ms/example). The model generalizes well across unseen domains and performs robustly under prefix-based (early detection) settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel focus on policy compliance.\nThis paper identifies a previously underexplored dimension of agent reliability: compliance with human or institutional policies, distinct from safety or task success. This conceptual contribution broadens the landscape of responsible AI and agentic system evaluation.\n2.The new benchmark is appreciated.\nThe dataset construction is meticulous—covering multiple domains, systematic policy synthesis using LLMs, and dual within- and cross-subdomain evaluations. The inclusion of prefix-based tasks allows studying early detection of violations, which is both original and practically relevant"}, "weaknesses": {"value": "1. Most data are generated and not validated by human.\n2. The coverage is limited in terms of context.\n\nMy major concern of this work is the generalization and how practical this work is.  Only a few domains are covered and I think it is better to include more. Additonally, I'm a little bit worried about the quality of the benchmark. I appreciate the efforts but there should be more detailed validation and analysis on the data.\n\nFor example, for the trajectories, maybe it is better to formulate the \"policy\" to formal specifications and then the violation can be checked by monitoring. For example, linear temporal or first-order logic. These monitoring and checks can provide rich feedback and formal guarantee on the results and label and I think this is missing in current version."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dWfmKTJBzs", "forum": "7zuhQWJIqX", "replyto": "7zuhQWJIqX", "signatures": ["ICLR.cc/2026/Conference/Submission10489/Reviewer_Z8ni"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10489/Reviewer_Z8ni"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972707163, "cdate": 1761972707163, "tmdate": 1762921777217, "mdate": 1762921777217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A challenge in practice is policy violation detection in long-horizon trajectories that are collected under externally imposed or human-defined policies in autonomous web agents, especially in the settings of multiple contexts and subdomains. This work introduces a benchmark called POLICYGUARDBENCH to provide 60k examples for this policy violation detection task, by providing both within subdomain and cross subdomain pairings with violation labels and policies. The raw data used in this work is from an existing benchmark SCRIBEAGENT. A benchmark algorithm called POLICYGUARD-4B is also introduced in this benchmark testbed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- From my perspective, this paper investigates an interesting practical challenge, policy violation detection, that can broadly exist in real-world scenarios but may be neglected by many policy learning approaches. \n- The introduced benchmark should be interesting and beneficial to related domains such as RL, safety, and system designs, etc.\n- The introduced algorithm POLICYGUARD-4B shows superior performance in terms of latency and EA-F1 in efficiency.\n- The introduced benchmark is examined via multiple evaluation metrics and case-studies."}, "weaknesses": {"value": "- The motivation of introduced algorithm is not very clear to me. Why POLICYGUARD-4B is in need and what guides the specific design of POLICYGUARD-4B? I assume latency is a part of motivation, but I would prefer to see a discussion from the authors.\n- Some details of benchmark design seem missing. Please refer to my questions in details."}, "questions": {"value": "Besides the weaknesses listed above, regarding benchmark design, I was also wondering: \n- The details of candidate policy generation seem missing. Could you provide further details on how you trained your policies and how you choose which algorithms (as you only synthesize 2-3 policies in total)?  \n- Do results in Table 2 averaged across all subdomains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5sxjHTODkU", "forum": "7zuhQWJIqX", "replyto": "7zuhQWJIqX", "signatures": ["ICLR.cc/2026/Conference/Submission10489/Reviewer_4BD3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10489/Reviewer_4BD3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150378456, "cdate": 1762150378456, "tmdate": 1762921776795, "mdate": 1762921776795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced a benchmark dataset claimed to be facilitating trajectory-level policy compliance, which started with raw trajectories from ScribeAgent and WebArena followed by data standardization (i.e., cleaning and assigning domains and sub-domains), synthesizing policies (i.e., getting the policies that the agent needed to follow) and labeling (i.e., trajectory-policy compliance check). Authors also fine-tuned a specific version of baseline LLM (Qwen3-4B-Instruct) as an additional baseline to the benchmark dataset. The benchmark itself not only introduced regular hold-out type binary classification tasks (i.e., complied or not), but prefix-based violation detection (i.e., when less information are available), and cross-domain generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The motivation of setting up this benchmark is clear and the reviewer has not found other peer-reviewed and published benchmark that specifically focuses on trajectory-level policy compliance.\n  * Note that there exists some highly related non-benchmark works (e.g., LlamaGuard 3 and 4) that were based off from a very similar setup, i.e., to classify if a series of questions and answers (i.e., essentially trajectories as considered in the paper) violates the policy/guidelines or not. And the authors found that LlamaGuard did not perform as well in this benchmark (which has multiple policies) than in the original setup (which only considered a single policy).\n* The benchmark provided 2 additional setups beyond the typical hold-out train/test split, i.e., prefix-based violation detection and domain generalization."}, "weaknesses": {"value": "* There are quite a few strong statements the authors made in the paper (mostly in the introduction and related work/background sections) that were left to be justified.\n > around lines 118, *These efforts are well motivated, since safety is a prerequisite for deployment, but they often result in\nover-defensiveness: models may over-refuse benign behaviors in order to minimize risks, thereby\nreducing usability in practice.*\n\n    * Is this a conclusion drawn by the authors following their benchmark, or a common ground that safety-oriented works including LlamaGuard has been widely recognized as often being *overly defensive* that limited their usability in practice?\n\n  > around lines 126, *Notably, some prior works have treated policy violations as equivalent to unsafe behaviors (Chen et al., 2025a;b; Vijayvargiya et al., 2025), an assumption that we argue is unfounded. Our own tests further corroborate this view: safety-oriented guardrails such as LLAMAGUARD3 (Meta Llama Team, 2024) and LLAMAGUARD4 (Meta Llama Team, 2025) often classify trajectories that clearly violate policies as safe.*\n\n    * The reviewer did not find any other supporting evidence or justification for the argument presented in the beginning of the quote above, i.e., the latter half of the quote itself arguable did not sufficiently justify. Specifically, the authors did show that under their benchmark setup, LlamaGuard did not perform as well as other baselines. However, could the deciding factor be that LlamaGuard only considered a single policy to comply with rather than multiple given their use case? Moreover, even one assumes that there are facts to support that the safety-oriented guardrail works do often violate policies, how's that connected to the statement that *treating  violations as equivalent to unsafe behaviors* was deemed unfounded by the authors? Besides, are there any other references implying or supporting this statement?\n\n* Following from the 2nd quote above, it was also unclear to the reviewer what were the formal definitions of *policy violations* and *unsafe behaviors*. Were the authors implying that one could be a subset of another, or there could be parts under each definition that are mutually exclusive? If the latter, how this benchmark reflected such nuances?\n\n* There are also unclarity in some of the design choices in data processing/policy synthesis.\n  > lines 170-172, *To transform these into a clean and uniform representation, we remove noise (e.g., empty or duplicate events, rendering artifacts), normalize actions into a controlled verb set (Click, Input, Scroll, Select, Navigate, Submit, etc.), and canonicalize objects (e.g., link ’My Account’ or button ’Search’). Redundant operations are merged, and excessively long trajectories may be truncated. Each trajectory is then assigned a domain and subdomain, which are propagated to all steps.*\n\n  * What were the raw actions and how were them being grouped into the controlled verb set? What was the coverage after grouping? What defines the redundant operations and how were they merged? How long a trajectory would be considered *excessively long* and how the truncation was done? \n    * Beyond these clarifications, the reviewer would also appreciate some stats showing the histogram on how often each element in the controlled verb set appeared.\n\n  > around lines 174, *Each trajectory is then assigned a domain and subdomain, which are propagated to all steps.*\n\n  * What were the criteria for labeling domains and sub-domains. For example, would a reddit thread on sharing cloth purchases opinions considered *Reddit* or *Shopping*? From Table 1 it looked like authors categorized all trajectories into five domains, i.e., reddit, map, gitlab, shopping_admin, shopping. How did the authors determine to classify into these five specific categories and what were the criteria used in the categorization? What was the difference between *shopping* and *shopping_admin*?\n\n  > lines 186, *Candidate policies are first synthesized by GPT-4o1 and subsequently curated to ensure quality and consistency.*\n  * The reviewer was wondering how to check the validity of these policies, as 2-3 were generated by GPT for every single trajectory where all needed to be human-reviewed. Are there any criteria to determine which GPT-generated policies needed to be modified, and in what ways? For example, the policies have to make sense but more importantly, do they have to reach some degree of complexity?\n    * The authors also claimed the following as one of the challenges in studying trajectory-level compliance around lines 52, *First, policies are inherently diverse and may originate from human instructions, institutional rules, or environmental constraints, which makes it non-trivial to align them with equally diverse trajectories.* \n    * The reviewer was wondering if the human-tuned on top of GPT-generated policies would naturally reflect these various types of constraints, i.e., the reviewer could see some human instructions being ported in during the manual correction process, how did the GPT was setup so the policies capture the institutional rules, environmental constraints, etc.?\n\n  > around lines 195, *First, candidate policies are retrieved for each trajectory using embedding-based similarity (Reimers & Gurevych, 2019) and keyword triggers (e.g., element names such as confirm or delete). These candidates are further refined using heuristic rules and LLM-based scoring to ensure that the policies are semantically relevant and checkable.*\n\n  * The reviewer was curious about to what extend the embedding-based similarity would ensure proper matching and what were the criteria used to determine if further refinement is needed or not. If so, what were the heuristic rules used for refinement. Moreover, how reliable the LLM-based scoring is to quantify the relevancy between the trajectory and policy? Are there any stats/analyses that could reflect these?\n\n  > around lines 209, *Each trajectory–policy pair is labeled as either violation or no violation. We define operational criteria covering common rule types: obligations (a required action is missing), prohibitions (a forbidden action is present), ordering constraints (steps occur in the wrong sequence), and conditional rules (a consequent is not satisfied when the antecedent holds).* \n\n  * The reviewer was curious how these 4 types of violations distribution across the train/test sets, and what was the thinking behind converting the task to a binary *violation/no_violation* one instead of a multi-class tasks pinpointing the type(s) of violations?\n\n  > around lines 249, *for violation cases, we truncate trajectories to the first N steps (N = 1. . . 5), re-match them with their corresponding policies, and re-label the resulting pairs.*\n\n  * If the purpose of the prefix-based splits was to study how the agent would perform with incomplete observations, the reviewer did not get why policy re-matching was needed. Couldn't the policy stays the same while just the trajectory got truncated?\n\n* The 3rd contribution of the paper seemed to be over claimed to some extent, i.e., as stated by the authors *We develop POLICYGUARD-4B, a lightweight and effective guardrail model that attains strong accuracy, cross-domain generalization, and state-of-the-art efficiency, demonstrating the practicality of small-scale guardrails in real-world deployment*. PolicyGuard-4B indeed performed better on top of its backbone after fine-tuning with the dataset curated by the authors. However, it does not facilitate an apple-to-apple comparison against other baseline models that are not fine-tuned. To the reviewer's understanding that the improvement was mostly attributed to the use of the benchmark dataset during fine-tuning, so it was not clear why a single model fine-tuned on the dataset could contribute? In contrast, the contribution brought in by the benchmark dataset could be more thoroughly justified by showing improvements for most baselines after fine-tuning. \n\n* Moreover, the reviewer thought it would be equally important to re-test models like LlamaGuard on their original tasks after fine-tuning with the benchmark dataset, ensuring that their performance on the original tasks were not significantly degraded. Otherwise, the performance gain from fine-tuning could mainly be attributed to data/topic biases, i.e., only train the models to do some tasks better than another. In the reviewer's opinion, this would also be the key showing the generalization ability of the agent, part of which was missing in the paper."}, "questions": {"value": "See the section above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8k8vZZM3ds", "forum": "7zuhQWJIqX", "replyto": "7zuhQWJIqX", "signatures": ["ICLR.cc/2026/Conference/Submission10489/Reviewer_fAdg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10489/Reviewer_fAdg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762237391313, "cdate": 1762237391313, "tmdate": 1762921776313, "mdate": 1762921776313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}