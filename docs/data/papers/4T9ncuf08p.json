{"id": "4T9ncuf08p", "number": 16305, "cdate": 1758262957349, "mdate": 1763549890746, "content": {"title": "Dataset Regeneration for Cross Domain Recommendation", "abstract": "Cross-domain recommendation (CDR) has emerged as an effective strategy to mitigate data sparsity and cold-start challenges by transferring knowledge from a source domain to a target domain. Despite recent progress, two key issues remain: (i) Sparse overlap. In real-world datasets such as Amazon, the proportion of users active in both domains is extremely low, significantly limiting the effectiveness of many state-of-the-art CDR approaches.  (ii) Negative transfer. Existing methods primarily address this problem at the model level, often assuming that logged interactions are unbiased and noise-free. In practice, however, recommender data contain numerous spurious correlations, and this issue is exacerbated in CDR due to domain heterogeneity.\nTo address these challenges, we propose a dataset regeneration framework. First, we leverage a prediction model to generate a pool of high-confidence candidate interactions to link non-overlapping target-domain users and source-domain items. Second, inspired by causal inference, we introduce a filtering process designed to prune spurious interactions. This process identifies and removes not only noisy edges created during generation but also those from the original dataset, retaining only the interactions that have a positive causal effect on the target-domain performance. Through these two processes, we can regenerate a source-domain dataset that exhibits a tighter coupling and a more explicit causal connection with the target domain.\nBy integrating our method with three representative recommendation backbones—LightGCN, BiTGCF, and CUT—we show that it significantly boosts their predictive accuracy on the target domain, achieving substantial gains of up to 23.81\\% in Recall@10 and 22.22\\% in NDCG@10.", "tldr": "", "keywords": ["Recommender System", "Cross-domain recommendation", "Dataset Regeneration"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a28591d67a70238bb70cb2485fcccf285f4283b2.pdf", "supplementary_material": "/attachment/a89479420fdb3ea9b785c1c11e69f799334ebf8a.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a data-centric framework, Generate-and-Filter (Gen/Del), for\ncross-domain recommendation (CDR). Instead of focusing on model-level transfer, the\nauthors address data sparsity and negative transfer by regenerating a causal and\ndenoised source-domain dataset. The framework consists of two stages:\n(1) Generation phase: A self-supervised model generates synthetic source-domain\ninteractions for users who exist only in the target domain, using masked-edge\nreconstruction and BPR loss.\n(2) Filtering phase: A counterfactual inference module assigns causal importance\nweights to each generated or existing edge and filters out non-causal or spurious ones.\nThe resulting regenerated dataset can be plugged into any backbone recommender\n(e.g., LightGCN, CUT, BiTGCF). Experiments on Douban and Amazon datasets show\nconsistent improvements across multiple backbones, with gains up to 23.8% in\nRecall@10."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tOriginality: Presents a fresh, data-centric perspective on CDR, shifting focus from model-level transfer to dataset regeneration. The integration of causal counterfactual filtering with GNN-based representation learning is particularly innovative.\n\t•\tQuality: Methodology is sound and well-formulated, with strong empirical results across multiple datasets and backbone models. Ablation studies effectively demonstrate the framework’s ability to mitigate negative transfer.\n\t•\tClarity: The paper is clearly written and well-structured, with intuitive explanations and informative figures.\n\t•\tSignificance: The framework is model-agnostic and has broad applicability, offering a principled foundation for future research on causal data manipulation and transfer learning.\n\nOverall, the work is conceptually original, empirically convincing, and highly relevant to data-centric and causal learning in recommender systems."}, "weaknesses": {"value": "(1) Limited Analysis of Computational Cost and Scalability\nWhile the proposed Generate-and-Filter framework is conceptually appealing, the paper lacks a systematic evaluation of its computational overhead. The counterfactual filtering stage requires training an additional GNN and repeatedly assessing target-domain performance, which could be computationally intensive for large-scale datasets. However, the paper provides no quantitative analysis of runtime, memory consumption, or scaling behavior with respect to dataset size, leaving the practicality of the approach for industrial-scale recommender systems uncertain.\n\n(2) Incomplete Symbol Definitions in the Counterfactual Interaction Filtering Section\nSeveral key symbols in Section 2.3—such as F_t^s, y_i, E_t^s, and the mapping l(E_t)—are introduced without explicit definitions or consistent explanations. This lack of clarity makes the mathematical formulation difficult to follow and reproduce. A concise summary table of notations or explicit variable definitions would greatly enhance readability and reproducibility.\n\n(3) Lack of Qualitative Analysis and Interpretability of Filtering Results\nAlthough the paper presents quantitative improvements in metrics such as Recall@10 and NDCG@10, it lacks qualitative analysis of the filtering process. There are no examples or visualizations illustrating which user–item edges are pruned or retained by the counterfactual filtering stage. Without such interpretability analysis, it is difficult to understand what types of interactions the model identifies as causal versus spurious.\n\n(4) Unclear Contribution of the Generation Phase\nAblation results suggest that most of the performance gains arise from the counterfactual filtering module rather than the data generation phase. However, the paper does not analyze the characteristics or quality of the generated interactions—such as their distribution, overlap with observed data, or effect on coverage. Consequently, the empirical contribution and necessity of the generation component remain ambiguous."}, "questions": {"value": "(1) Address scalability and efficiency concerns\nProviding details on the model’s runtime, computational cost, and resource usage\nduring experiments would help readers better understand the practical feasibility and\nefficiency of the proposed framework.\n(2) Deeper analysis of generation and filtering behavior\nAnalyzing how the generation phase adds synthetic edges and how the\ncounterfactual filtering module removes or retains interactions in practice would help\nreaders better understand the model’s decision behavior and its contribution to\nperformance improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "72YCS7Q83i", "forum": "4T9ncuf08p", "replyto": "4T9ncuf08p", "signatures": ["ICLR.cc/2026/Conference/Submission16305/Reviewer_MGmq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16305/Reviewer_MGmq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761198491407, "cdate": 1761198491407, "tmdate": 1762926445819, "mdate": 1762926445819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the cross-domain recommendation (CDR) task and addresses key challenges, including sparse user overlap across domains and negative transfer caused by spurious correlations in heterogeneous data. To tackle these issues, the authors propose a dataset regeneration framework that (1) generates high-confidence candidate interactions to link non-overlapping users and items, and (2) applies a causal-inference-inspired filtering process to remove spurious interactions from both the generated and original data. This approach enhances the causal connection between source and target domains. When integrated with recommendation models such as LightGCN, BiTGCF, and CUT, it substantially improves target-domain performance, achieving up to 23.81% gain in Recall@10 and 22.22% in NDCG@10."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper focuses on the cross-domain recommendation (CDR) task and addresses two major challenges: sparse user overlap across domains and negative transfer caused by spurious correlations in heterogeneous data.\n2. To tackle these challenges, the authors propose a dataset regeneration framework. This approach strengthens the causal connection between the source and target domains.\n3. The proposed framework, when integrated with recommendation models such as LightGCN, BiTGCF, and CUT, substantially improves target-domain performance."}, "weaknesses": {"value": "1. The core argument of this paper is that prior work primarily addresses sparse overlap and negative transfer at the model level, whereas this work tackles these challenges from a data-centric perspective. In fact, in the cross-domain recommendation (CDR) field, several studies have already explored data-centric solutions, such as [1][2][3]. The authors also provide a comparative analysis between their approach and these existing data-centric methods.\n\n[1]https://arxiv.org/pdf/2405.20710\n[2]https://arxiv.org/abs/2307.13910\n[3]https://dl.acm.org/doi/10.1145/3626772.3657902\n\n2. There is an inconsistency between the paper title in the main text and the title on OpenReview. The authors should ensure that the titles are consistent before submission.\n\n3. The proposed framework is divided into two stages: generation followed by filtering.\n- For the generation stage, the authors employ self-supervised pretraining, which is a common practice in graph learning, and therefore this stage lacks significant novelty.\n- For the filtering stage, the authors adopt counterfactual interaction filtering. It would be helpful to clarify the motivation for using this technique compared with existing filter-based methods. Are there unique challenges that the counterfactual approach specifically addresses?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vY2fbWxQW9", "forum": "4T9ncuf08p", "replyto": "4T9ncuf08p", "signatures": ["ICLR.cc/2026/Conference/Submission16305/Reviewer_eJuQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16305/Reviewer_eJuQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902201221, "cdate": 1761902201221, "tmdate": 1762926445508, "mdate": 1762926445508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a dataset enhancement strategy for cross-domain recommendation models. It aims to address the sparse cross-domain user overlap and noisy cross-domain signal (negative transfer) issues. The proposed strategy takes two stages. The first stage generates more user-item connections to address the sparse cross-domain user overlap issue, by learning a model that reconstructs edges in the source domain user-item graph. The second stage learns to identify spurious edges in the source domain user-item graph which should be removed to mitigate the negative transfer issue. Experimental results on two commonly used datasets, Amazon and Douban, showed the effectiveness of the proposed strategy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper is motivated well with a detailed example to illustrate issues of existing cross-domain recommendation solutions. \n\nS2. The proposed technique works on the dataset level and is orthogonal to cross-domain recommendation models, which has the potential to be applied to and strengthen different cross-domain recommendation models. \n\nS3. The proposed technique is shown to be effective on commonly used benchmark datasets.\n\nS4. Source code is available."}, "weaknesses": {"value": "W1. Technical details:\n\n- The synthetic edge set contains edges between every non-overlapping user and their top-$k$ relevant items in the source set. Even the top-$k$ items might not be very relevant for some of the users, and hence there may be false positives. Using a fix $k$ for all users might not be the most effective. How about using a score threshold to filter the items instead (or a combination of both)? Also, how is the value of $k$ chosen in the experiments, and how does its value impact overall accuracy? \n\n- The NP-hardness of Problem $\\overline{P}$ needs a proof. \n\n- How are the node embeddings in $\\mathcal{F}_\\theta^T$ initialized?\n\nW2. Experiments:\n\n- The performance gains obtained by using the proposed Gen/Del dataset preparation strategy is quite small as shown in Table 1 (noting the statistical significance test results). The second-best results in the two N columns of the Douban datasets didn't seem to be labeled correctly. \n\n- It would be interesting to see model running time results, model effectiveness results as $K$ (as in Recall/NDCG@$K$) varies, and model effectiveness results as the number of cross-domain overlapping users varies. \n\nW3. Presentation: \n\n- The preliminaries section should be moved to the main text to set up the context for the methodology section. Without it, the methodology section is difficult to follow.  \n\n- Even with the preliminaries section, the paper needs a notation table to explain what the many symbols mean in the paper. \n\n- The final sentence in Appendix A, \"The next section details the optimization techniques used to implement this filtering, integrating the pre-trained prediction model with edge weight adjustments to achieve the desired causal pruning.\", seems to be disconnected from the subsequent section. \n\n- Typo: \"”science fiction”\" => \"``science fiction”\"; \"in the Appendix B\" => \"in Appendix B\"; \"The single-domain baselines, trained exclusively on the target dataset\" => \"The single-domain baselines are trained exclusively on the target dataset\""}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LCnihkLKE7", "forum": "4T9ncuf08p", "replyto": "4T9ncuf08p", "signatures": ["ICLR.cc/2026/Conference/Submission16305/Reviewer_p3jn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16305/Reviewer_p3jn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071094919, "cdate": 1762071094919, "tmdate": 1762926444996, "mdate": 1762926444996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}