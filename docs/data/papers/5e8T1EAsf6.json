{"id": "5e8T1EAsf6", "number": 21250, "cdate": 1758315371342, "mdate": 1759896932224, "content": {"title": "A Spectral Characterization of Generalization in GCN: Escaping the Curse of Dimensionality", "abstract": "Empirically it is observed that Graph Convolution Networks (GCNs) often generalize better than \nfully connected neural networks (FCNNs) on graph-structured data. While this observation is often attributed to the ability of GCNs to exploit knowledge about the underlying graph structure, a rigorous theoretical explanation remains limited. In this work, we theoretically prove that one factor for the improved generalization of GCNs arises from the spectral representation of the filters or graph convolutional layers. Specifically, we derive generalization bounds that are independent of the number of parameters and instead scale nearly linearly with the number of graph nodes, offering a compelling explanation for their superior performance in over-parameterized regimes. Furthermore, in the limit of infinite number of nodes, we prove that under certain regularity conditions on the spectrum, GCNs escape the curse of dimensionality and continue to generalize well. We demonstrate our conclusions through numerical experiments.", "tldr": "", "keywords": ["Generalization theory", "Graph Neural Networks", "Spectral theory"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/85f3dc1534e67c3f33485b97431f0437e090dbea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a new theoretical framework for analyzing the generalization properties of GCNs by leveraging classical tools from signal processing and modern statistical learning techniques. The approach differs from prior work in that it leverages the graph spectral structure rather than relying purely on the parameter space. The authors argue that GCN filters, viewed in the graph Fourier domain, have lower intrinsic dimensionality, allowing the derivation of tighter generalization error bounds that are independent of the number of parameters. The paper further extends the analysis to graphons. Numerical experiments validate the scaling of the theoretical bounds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Sharp generalization bounds.\n2. The paper is generally well-written and clearly motivated.\n3. The paper provides some insights in Sec. 4.\n4. This seems to be the first theoretical results that analyze generalization properties in the spectral domain. The spectral viewpoint provides an elegant and under-explored angle for bounding generalization in GCNs.\n5. Tighter bounds: The derived bounds scale as $\\sqrt{n_x/N},$ independent of the total number of parameters, and improved over VC-dimension/PAC-Bayes/Rademacher complexity bounds."}, "weaknesses": {"value": "1. The paper claims sharp bounds but has not shown the actual gap between empirical and theoretical error bounds.\n2. The sub-Gaussian assumption may not hold for real-world graph data.\n3. The bounds involve constants such as $K, K'', L_\\mathcal{X},$ and $L_\\mathcal{H},$ which can benefit from better explanation and intuition of why they are defined in this way.\n4. Some typos/grammar issues, e.g., in lines 135-136 it should be \"an empirical\"; in lines 293-294 \"While\" should be removed."}, "questions": {"value": "1. How tight are your theoretical bounds to empirical values?\n2. How sensitive are your results to the Lipschitz or sub-Gaussian assumptions? Would small violations lead to large deviations in generalization behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FCKGqtgQYf", "forum": "5e8T1EAsf6", "replyto": "5e8T1EAsf6", "signatures": ["ICLR.cc/2026/Conference/Submission21250/Reviewer_ggKt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21250/Reviewer_ggKt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761592576784, "cdate": 1761592576784, "tmdate": 1762941656469, "mdate": 1762941656469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical study of generalization in Graph Convolutional Networks (GCNs) from a spectral perspective. It argues that the spectral representation of graph filters provides a natural space to analyze generalization, yielding bounds independent of the number of parameters. The authors derive sharp non-asymptotic generalization bounds that scale nearly linearly with the number of graph nodes and remain finite in the infinite-node (graphon) limit under mild regularity assumptions. Theoretical results are complemented by numerical simulations on both homophilous and heterophilous datasets, verifying the dependence of generalization on Lipschitz constants and spectral properties."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The spectral characterization of GCN generalization is interesting and deserves in-depth theoretical analysis;\n2. The paper includes comparisons with classical frameworks (VC dimension, PAC-Bayes, Rademacher complexity) and state-of-the-art works, highlighting the improvement in sample complexity."}, "weaknesses": {"value": "1. The readability is not high, as the notation is complex and difficult to follow.\n\n2. The link between the simulation studies and the theoretical insights is not clearly established.\n\n3. If the authors focus on the graph classification setup, what about the case of node classification?\n\n4. The current theoretical result does not consider the over-smoothing issue, why?"}, "questions": {"value": "1. How sensitive are the derived bounds to violations of the sub-Gaussian assumption?\n\n2. Can the spectral regularity (Lipschitz or low-pass) conditions be empirically estimated from real GCNs during training?\n\n3. Would incorporating stochasticity in the graph structure (e.g., random graph models) affect the generalization scaling behavior?\n\n4. Could the theoretical insights guide the design of regularization terms or spectral constraints to improve practical generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0hBYLv8RLW", "forum": "5e8T1EAsf6", "replyto": "5e8T1EAsf6", "signatures": ["ICLR.cc/2026/Conference/Submission21250/Reviewer_8PWR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21250/Reviewer_8PWR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893133987, "cdate": 1761893133987, "tmdate": 1762941656183, "mdate": 1762941656183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides new generalization bounds for GCNs. The bounds are derived by evaluating the covering number of the hypothesis space in the Fourier domain of convolution operators. Specifically, when the spectrum is bounded, the bound does not depend on the number of trainable parameters. When the spectrum decays rapidly, the bound is independent of the number of nodes on a graphon, which can be interpreted as a graph with an infinite number of nodes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. (Originality) To the best of my knowledge, this paper is the first to derive a generalization performance bound for GCNs utilizing spectral decay.\n2. (Quality) It provides a thorough review of previous statistical learning theory research on GCNs, clearly positioning this work within this line of research.\n3. (Clarity) The writing is clear. The paper's structure is appropriate, and the mathematical descriptions are accurate. I had no difficulties in understanding the paper's main claims.\n4. (Significance) For single-layer GCNs, the proposed method's performance bound achieves a better order with respect to node size than existing methods."}, "weaknesses": {"value": "1. The derivation of bounds employs evaluation using the covering number, which is a relatively classical statistical learning theory method. Therefore, its novelty from this perspective is limited.\n2. There is a discrepancy in the problem setting between the theoretical analysis and the numerical experiments. The theoretical analysis considers a problem setting where the graph signal and teacher signal are given in the I.I.D. setting. On the other hand, the numerical experiments consider a transductive node classification problem."}, "questions": {"value": "I would like the authors to address the concerns raised in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JsulWKELEl", "forum": "5e8T1EAsf6", "replyto": "5e8T1EAsf6", "signatures": ["ICLR.cc/2026/Conference/Submission21250/Reviewer_8YBy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21250/Reviewer_8YBy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898168015, "cdate": 1761898168015, "tmdate": 1762941655908, "mdate": 1762941655908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the generalization properties of Graph Convolutional Networks (GCNs) and attributes their empirical superiority over fully connected neural networks (FCNNs) to the spectral representation of graph convolution filters. The authors derive generalization bounds $O(\\sqrt{\\frac{n_x}{N}})$ that are independent of the number of parameters. They show that under mild spectral regularity conditions, GCNs can escape the curse of dimensionality. However, its immediate practical and theoretical claims are significantly weakened by several critical methodological flaws."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The core idea that GCN generalization is determined by spectral complexity (covering numbers) rather than the parameter count is novel and addresses an open problem in GNN theory.\n\n(2) If the results were fully valid, they would provide a rigorous theoretical explanation for a widely observed phenomenon: GCNs generalizing better than FCNNs on graph-structured data."}, "weaknesses": {"value": "(1) Unrealistic Assumptions: The author assumes sub-Gaussian for graph signals and convex, smooth loss. In practice, graph node features are often sparse, categorical, or heavy-tailed. They are **not sub-Gaussian**. Classification losses (e.g., cross-entropy in **multi-class classification**) are **non-convex**. This limits the applicability of the theoretical bounds. \n\n(2) Single-layer GCN analysis: All theoretical results in section 3.3 are derived for one layer $L=1$, whereas practical GCNs usually have 2 layers. **Multi-layer extensions are nontrivial**, and the bounds may degrade significantly due to Lipschitz composition and over-smoothing effects.\n\n(3) Mismatch between theory and experiments: Experiments are conducted on ChebNet rather than standard GCNs, introducing a gap between the theoretical model and empirical validation. The **polynomial order $K$** in ChebNet affects receptive fields and generalization, which is not addressed.\n\n(4) Negative generalization error in plots: Left one in Figure 4(a) shows **negative generalization errors** despite defining GE as absolute value in Eq. 4, suggesting either an inconsistency in the plot or a mismatch between theory and implementation."}, "questions": {"value": "(1) How sensitive are your bounds to the assumption of sub-Gaussian node features, and can they be relaxed for sparse or categorical inputs?\n\n(2) Can your $L=1$ layer analysis be generalized to multi-layer GCNs, and if so, how does the bound scale with $L$?\n\n(3) Why were ChebNet models used in experiments instead of the theoretical GCNs, and how does the polynomial order $K$ affect the generalization?\n\n(4) Explain the negative generalization error in left figure in Figure 4(a) when generalization error is defined as the absolute value in Eq. 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O1gUZvsrs4", "forum": "5e8T1EAsf6", "replyto": "5e8T1EAsf6", "signatures": ["ICLR.cc/2026/Conference/Submission21250/Reviewer_LuGU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21250/Reviewer_LuGU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981167173, "cdate": 1761981167173, "tmdate": 1762941655593, "mdate": 1762941655593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}