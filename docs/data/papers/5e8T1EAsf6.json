{"id": "5e8T1EAsf6", "number": 21250, "cdate": 1758315371342, "mdate": 1763692162551, "content": {"title": "A Spectral Characterization of Generalization in GCN: Escaping the Curse of Dimensionality", "abstract": "Empirically it is observed that Graph Convolution Networks (GCNs) often generalize better than \nfully connected neural networks (FCNNs) on graph-structured data. While this observation is often attributed to the ability of GCNs to exploit knowledge about the underlying graph structure, a rigorous theoretical explanation remains limited. In this work, we theoretically prove that one factor for the improved generalization of GCNs arises from the spectral representation of the filters or graph convolutional layers. Specifically, we derive generalization bounds that are independent of the number of parameters and instead scale nearly linearly with the number of graph nodes, offering a compelling explanation for their superior performance in over-parameterized regimes. Furthermore, in the limit of infinite number of nodes, we prove that under certain regularity conditions on the spectrum, GCNs escape the curse of dimensionality and continue to generalize well. We demonstrate our conclusions through numerical experiments.", "tldr": "", "keywords": ["Generalization theory", "Graph Neural Networks", "Spectral theory"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b146b8091a8f6376c82dfc1fe71647b278e4eb23.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a new theoretical framework for analyzing the generalization properties of GCNs by leveraging classical tools from signal processing and modern statistical learning techniques. The approach differs from prior work in that it leverages the graph spectral structure rather than relying purely on the parameter space. The authors argue that GCN filters, viewed in the graph Fourier domain, have lower intrinsic dimensionality, allowing the derivation of tighter generalization error bounds that are independent of the number of parameters. The paper further extends the analysis to graphons. Numerical experiments validate the scaling of the theoretical bounds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Sharp generalization bounds.\n2. The paper is generally well-written and clearly motivated.\n3. The paper provides some insights in Sec. 4.\n4. This seems to be the first theoretical results that analyze generalization properties in the spectral domain. The spectral viewpoint provides an elegant and under-explored angle for bounding generalization in GCNs.\n5. Tighter bounds: The derived bounds scale as $\\sqrt{n_x/N},$ independent of the total number of parameters, and improved over VC-dimension/PAC-Bayes/Rademacher complexity bounds."}, "weaknesses": {"value": "1. The paper claims sharp bounds but has not shown the actual gap between empirical and theoretical error bounds.\n2. The sub-Gaussian assumption may not hold for real-world graph data.\n3. The bounds involve constants such as $K, K'', L_\\mathcal{X},$ and $L_\\mathcal{H},$ which can benefit from better explanation and intuition of why they are defined in this way.\n4. Some typos/grammar issues, e.g., in lines 135-136 it should be \"an empirical\"; in lines 293-294 \"While\" should be removed."}, "questions": {"value": "1. How tight are your theoretical bounds to empirical values?\n2. How sensitive are your results to the Lipschitz or sub-Gaussian assumptions? Would small violations lead to large deviations in generalization behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FCKGqtgQYf", "forum": "5e8T1EAsf6", "replyto": "5e8T1EAsf6", "signatures": ["ICLR.cc/2026/Conference/Submission21250/Reviewer_ggKt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21250/Reviewer_ggKt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761592576784, "cdate": 1761592576784, "tmdate": 1762941656469, "mdate": 1762941656469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical study of generalization in Graph Convolutional Networks (GCNs) from a spectral perspective. It argues that the spectral representation of graph filters provides a natural space to analyze generalization, yielding bounds independent of the number of parameters. The authors derive sharp non-asymptotic generalization bounds that scale nearly linearly with the number of graph nodes and remain finite in the infinite-node (graphon) limit under mild regularity assumptions. Theoretical results are complemented by numerical simulations on both homophilous and heterophilous datasets, verifying the dependence of generalization on Lipschitz constants and spectral properties."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The spectral characterization of GCN generalization is interesting and deserves in-depth theoretical analysis;\n2. The paper includes comparisons with classical frameworks (VC dimension, PAC-Bayes, Rademacher complexity) and state-of-the-art works, highlighting the improvement in sample complexity."}, "weaknesses": {"value": "1. The readability is not high, as the notation is complex and difficult to follow.\n\n2. The link between the simulation studies and the theoretical insights is not clearly established.\n\n3. If the authors focus on the graph classification setup, what about the case of node classification?\n\n4. The current theoretical result does not consider the over-smoothing issue, why?"}, "questions": {"value": "1. How sensitive are the derived bounds to violations of the sub-Gaussian assumption?\n\n2. Can the spectral regularity (Lipschitz or low-pass) conditions be empirically estimated from real GCNs during training?\n\n3. Would incorporating stochasticity in the graph structure (e.g., random graph models) affect the generalization scaling behavior?\n\n4. Could the theoretical insights guide the design of regularization terms or spectral constraints to improve practical generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0hBYLv8RLW", "forum": "5e8T1EAsf6", "replyto": "5e8T1EAsf6", "signatures": ["ICLR.cc/2026/Conference/Submission21250/Reviewer_8PWR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21250/Reviewer_8PWR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893133987, "cdate": 1761893133987, "tmdate": 1762941656183, "mdate": 1762941656183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides new generalization bounds for GCNs. The bounds are derived by evaluating the covering number of the hypothesis space in the Fourier domain of convolution operators. Specifically, when the spectrum is bounded, the bound does not depend on the number of trainable parameters. When the spectrum decays rapidly, the bound is independent of the number of nodes on a graphon, which can be interpreted as a graph with an infinite number of nodes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. (Originality) To the best of my knowledge, this paper is the first to derive a generalization performance bound for GCNs utilizing spectral decay.\n2. (Quality) It provides a thorough review of previous statistical learning theory research on GCNs, clearly positioning this work within this line of research.\n3. (Clarity) The writing is clear. The paper's structure is appropriate, and the mathematical descriptions are accurate. I had no difficulties in understanding the paper's main claims.\n4. (Significance) For single-layer GCNs, the proposed method's performance bound achieves a better order with respect to node size than existing methods."}, "weaknesses": {"value": "1. The derivation of bounds employs evaluation using the covering number, which is a relatively classical statistical learning theory method. Therefore, its novelty from this perspective is limited.\n2. There is a discrepancy in the problem setting between the theoretical analysis and the numerical experiments. The theoretical analysis considers a problem setting where the graph signal and teacher signal are given in the I.I.D. setting. On the other hand, the numerical experiments consider a transductive node classification problem."}, "questions": {"value": "I would like the authors to address the concerns raised in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JsulWKELEl", "forum": "5e8T1EAsf6", "replyto": "5e8T1EAsf6", "signatures": ["ICLR.cc/2026/Conference/Submission21250/Reviewer_8YBy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21250/Reviewer_8YBy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898168015, "cdate": 1761898168015, "tmdate": 1762941655908, "mdate": 1762941655908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the generalization properties of Graph Convolutional Networks (GCNs) and attributes their empirical superiority over fully connected neural networks (FCNNs) to the spectral representation of graph convolution filters. The authors derive generalization bounds $O(\\sqrt{\\frac{n_x}{N}})$ that are independent of the number of parameters. They show that under mild spectral regularity conditions, GCNs can escape the curse of dimensionality. However, its immediate practical and theoretical claims are significantly weakened by several critical methodological flaws."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The core idea that GCN generalization is determined by spectral complexity (covering numbers) rather than the parameter count is novel and addresses an open problem in GNN theory.\n\n(2) If the results were fully valid, they would provide a rigorous theoretical explanation for a widely observed phenomenon: GCNs generalizing better than FCNNs on graph-structured data."}, "weaknesses": {"value": "(1) Unrealistic Assumptions: The author assumes sub-Gaussian for graph signals and convex, smooth loss. In practice, graph node features are often sparse, categorical, or heavy-tailed. They are **not sub-Gaussian**. Classification losses (e.g., cross-entropy in **multi-class classification**) are **non-convex**. This limits the applicability of the theoretical bounds. \n\n(2) Single-layer GCN analysis: All theoretical results in section 3.3 are derived for one layer $L=1$, whereas practical GCNs usually have 2 layers. **Multi-layer extensions are nontrivial**, and the bounds may degrade significantly due to Lipschitz composition and over-smoothing effects.\n\n(3) Mismatch between theory and experiments: Experiments are conducted on ChebNet rather than standard GCNs, introducing a gap between the theoretical model and empirical validation. The **polynomial order $K$** in ChebNet affects receptive fields and generalization, which is not addressed.\n\n(4) Negative generalization error in plots: Left one in Figure 4(a) shows **negative generalization errors** despite defining GE as absolute value in Eq. 4, suggesting either an inconsistency in the plot or a mismatch between theory and implementation."}, "questions": {"value": "(1) How sensitive are your bounds to the assumption of sub-Gaussian node features, and can they be relaxed for sparse or categorical inputs?\n\n(2) Can your $L=1$ layer analysis be generalized to multi-layer GCNs, and if so, how does the bound scale with $L$?\n\n(3) Why were ChebNet models used in experiments instead of the theoretical GCNs, and how does the polynomial order $K$ affect the generalization?\n\n(4) Explain the negative generalization error in left figure in Figure 4(a) when generalization error is defined as the absolute value in Eq. 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O1gUZvsrs4", "forum": "5e8T1EAsf6", "replyto": "5e8T1EAsf6", "signatures": ["ICLR.cc/2026/Conference/Submission21250/Reviewer_LuGU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21250/Reviewer_LuGU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981167173, "cdate": 1761981167173, "tmdate": 1762941655593, "mdate": 1762941655593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We thank all the reviewers and area chair for the smooth review process and constructive feedback. Generalization theory is a notoriously difficult problem and earliest known work dates back to 1960s by Vladimir Vapnik and Alexey Chervonenkis. Despite significant effort over 60 years, it remains an open problem to derive tight non-asymptotic generalization bounds for the deep neural networks. Our work makes significant progress in this direction for GCNs by leveraging the spectral properties of graphs and removing the parametric dependence in the bounds. We provide theoretical explanation for the empirical success of GCNs over FCNNs on graph-structured data, which is first of its kind result we are aware of. In particular, the reviewers acknowledged the elegance of our draft, novelty of our spectral perspective, and the tightness of our bounds; below are some representative quotes from the reviews:\n\n## Summarized Strengths\n\n1. \"*The paper is generally well-written and clearly motivated.*\" -- Reviewer ggKt\n2. \"*The writing is clear. The paper's structure is appropriate, and the mathematical descriptions are accurate. I had no difficulties in understanding the paper's main claims.*\" -- Reviewer 8YBy\n3. \"*The spectral viewpoint provides an elegant and under-explored angle for bounding generalization in GCNs.*\" -- Reviewer ggKt\n4. \"*...this paper is the first to derive a generalization performance bound for GCNs utilizing spectral decay.*\" -- Reviewer 8YBy\n5. \"*If the results were fully valid, they would provide a rigorous theoretical explanation for a widely observed phenomenon: GCNs generalizing better than FCNNs on graph-structured data.*\" -- Reviewer LuGU\n6. \"*Theoretical results are complemented by numerical simulations ... verifying the dependence of generalization on Lipschitz constants and spectral properties.*\" -- Reviewer 8PWR\n\nOur work is a significant departure from prior works that analyze generalization in the parameter space. Like in all theoretical works, assumptions are necessary to make the analysis tractable. Despite this, we ensured that our assumptions are mild and close to practice. We addressed all the concerns raised by the reviewers in our detailed individual responses. Nevertheless, we make a few global comments along these lines.\n\n1. **Sub-Gaussian data assumptions**:\n   We respectfully disagree with the reviewers that sub-Gaussian assumption is potentially restrictive for real-world graph data. We remind the reviewers that either of the following conditions is sufficient for sub-Gaussianity to hold true:\n   - Bounded random variables (common assumption in the literature).\n   - Tails of the distribution decay at least as fast as Gaussian.\n\n   All the real-world datasets used in our experiments consist of node features that fall under one of the above categories (see Table 2, Appendix A.2). We believe that most real-world datasets satisfy these conditions as well, and, in fact, we are unaware of real-world graph datasets with heavy-tailed distributions, except perhaps financial data being an exception (de Miranda Cardoso et. al., '21) due to time series nature we do not consider them here. Unlike previous works, which often assume variables are bounded, our bounds are valid for unbounded random variables drawn from sub-Gaussian distributions. Therefore, our assumptions are a strict relaxation of common prior assumptions, which we believe makes our theoretical framework much more general and closer to practice.\n\n2. **Motto of the experiments**:\n   Our experiments are designed to validate the trends in Section 3.3. Our theoretical framework suggests that controlling the spectral complexity of GCN filters can lead to improved generalization; we have demonstrated this empirically for various real-world datasets that are both heterophilous and homophilous. Furthermore, in the updated version we have compared the empirical generalization error with our theoretical bounds and validated the trends of our bounds.\n\nBased on the constructive feedback from the reviewers, we have improved our draft in the following ways:\n\n1. In section 3.3, our submitted version only contained discussion of our results for single-layer GCNs for the sake of brevity. We note that this was largely for simplicity of presentation, as our main result is also valid for multi-layer GCNs as well. Neverthessless, following the feedback from reviewers, in the updated version, we have generalized this discussion to the case of multi-layer GCNs.\n2. We have introduced a synthetic experiment to compare empirical generalization error with our theoretical bounds (Figure 3).\n3. We have clarified the definitions of constants such as $K, K'', L_{\\mathcal{X}}, L_{\\mathcal{H}}$ in the updated version.\n4. We have corrected all the typos and plotting issues pointed out by the reviewers."}}, "id": "V3tO2M3Byv", "forum": "5e8T1EAsf6", "replyto": "5e8T1EAsf6", "signatures": ["ICLR.cc/2026/Conference/Submission21250/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21250/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission21250/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763691535530, "cdate": 1763691535530, "tmdate": 1763693176828, "mdate": 1763693176828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}