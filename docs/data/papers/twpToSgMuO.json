{"id": "twpToSgMuO", "number": 9622, "cdate": 1758130862713, "mdate": 1759897708545, "content": {"title": "Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains", "abstract": "Time-Series Foundation Models (TSFMs), pretrained on large-scale, cross-domain data, enable zero-shot forecasting in new scenarios and are increasingly deployed in practice. \nDespite their role in decision-making, their security under adversarial conditions is underexplored, creating risks in settings vulnerable to man-in-the-middle manipulation or data poisoning. \nTo bridge this gap, we present a systematic study of TSFM adversarial robustness. \nUsing a unified evaluation that spans diverse threat models, attack strategies, and robustness metrics, we show that even minimal perturbations can induce significant and controllable forecast behaviors. Across representative TSFMs and multiple datasets, we find consistent vulnerabilities and limited cross-model transfer, indicating model-specific failure modes rather than generic distortions. We then assess two practical defenses and figure out that latent adversarial fine-tuning yields substantial worst-case gains and remains effective even when fine-tuned on out-of-domain data. Overall, this work provides a rigorous assessment of TSFM robustness and actionable guidance for building more resilient, deployment-ready time-series forecasting systems.", "tldr": "", "keywords": ["time series forecasting", "adversarial robustness"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/21793d32ab783ca3e1d1c17e3e36e9ccfa39146b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a comprehensive evaluation of the adversarial robustness of Time-Series Foundation Models (TSFMs). Despite their growing adoption in high-stakes applications, the security of TSFMs under adversarial conditions remains underexplored. This work addresses that gap through a unified framework that evaluates various threat models, attack methods, and robustness metrics."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses the underexplored adversarial robustness of TSFMs, which are increasingly used in critical real-world applications.\n- The study presents a unified framework covering diverse threat models, attack types, and metrics, offering a broad and systematic analysis of TSFM vulnerabilities.\n- The evaluation provides actionable insights for model selection and defense in adversarial settings."}, "weaknesses": {"value": "- **Outdated Attack and Defense Techniques**:\n  Although the evaluation spans different goals, capabilities, and knowledge levels, the choice of attack and defense methods appears outdated and insufficient. The paper still relies on traditional computer vision attacks, with the most recent being SimBA from 2019. Similarly, the defense strategies used are no longer state-of-the-art. I suggest incorporating more recent adversarial attacks and defenses specifically tailored to time-series data.\n  \n- **Lack of Time-Series–Specific Insights and Misleading Claims**:\n  The results echo patterns already well-documented in the computer vision literature, with few observations that highlight the unique characteristics of time-series forecasting. Some statements also appear overstated or even potentially incorrect:\n  \n  1. For instance, the claim “Failures are model-specific, with limited cross-model transfer” is not surprising, especially since the transfer attacks are seemingly conducted by directly applying white-box perturbations to other models, without any established transfer-enhancing strategies. The paper misses the opportunity to explore whether classic transferability techniques are effective for time-series models.\n  2. Additionally, using the same $\\epsilon$ across datasets is problematic because time-series data can have vastly different scales. This makes cross-dataset comparisons unfair and weakens the conclusion that certain datasets are more vulnerable. A per-dataset normalization or scale-aware perturbation constraint would be more appropriate.\n- **Ambiguous Writing and Visualization**:\n  Several parts of the paper are unclear or confusing, making it difficult to assess the actual threat. For example, in **Figure 1**, key parameters (like $\\epsilon$) are not specified. The perturbation visually appears to be much larger than $\\epsilon$ = 1 used in **Table 2**, suggesting inconsistencies between figures and main experiments. **Figure 3** is similarly vague—only a range of [0.25, 5] is mentioned for the budget, but no specific $\\epsilon$ values are plotted. I recommend the authors improve the clarity and specificity of all experimental settings and avoid vague or potentially misleading descriptions."}, "questions": {"value": "- What is q in Figures 1 and 3? This variable is never defined in the main text, which leaves the reader confused.\n  \n- It is unclear whether the values of $\\epsilon$ {0.25, 0.5, 0.75, 1} and r {0.25, 0.5, 0.75, 1} in Table 2 are paired (i.e., (0.25, 0.25), (0.5, 0.5), etc.) or fully cross-matched. If the latter, then averaging all results may obscure the effect of low perturbation regimes, since the strongest ($\\epsilon$ = 1, r = 1) combinations dominate the average. Please consider presenting disaggregated results for different perturbation levels to clarify how performance degrades under small perturbations.\n  \n- What is the perceptual or practical impact of different perturbation magnitudes? Since Figure 1 and Figure 3 lack explicit parameter annotations, the perturbations—especially in Figure 3(b)—appear visually large (perhaps >1000 in value). I suggest incorporating explicit metrics for perturbation imperceptibility to better support claims of realistic risk.\n  \n- The phrase “the maximum change per step” is likely incorrect, since the method appears to apply a single-step change. Please remove “per step” for accuracy.\n  \n- The current black-box attack is purely query-based. Have the authors considered traditional transfer-enhancing methods (e.g., input diversity, ensemble gradients, surrogate fine-tuning)? It would be valuable to assess whether such techniques, commonly used in vision, also work for time-series models.\n  \n- The true risk posed by these attacks remains unclear. While the RED score may indicate high vulnerability, the robustness curves suggest that small perturbations cause minimal absolute error. Would such small distortions result in meaningful or observable consequences in downstream applications? The paper would benefit from a clearer discussion of what constitutes a “successful” or threatening perturbation in real-world settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M8ii2YHZFP", "forum": "twpToSgMuO", "replyto": "twpToSgMuO", "signatures": ["ICLR.cc/2026/Conference/Submission9622/Reviewer_1xNi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9622/Reviewer_1xNi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761573487707, "cdate": 1761573487707, "tmdate": 1762921159327, "mdate": 1762921159327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an empirical study on the adversarial robustness of multiple Time-Series Foundation Models (TSFMs)—TimesFM, TimeMoE, UniTS, Moirai, Chronos, and TabPFN-TS—under white-/black-box and targeted/untargeted attacks.\nIt proposes a unified evaluation framework with the Relative Error Deviation (REDE) metric, a mixed-norm perturbation budget (ℓ₀ + ℓ∞), and two defenses (inference-time smoothing and latent adversarial training LAT).\nExperiments cover six models and eight datasets, showing that minor perturbations can cause large forecast errors and that LAT improves worst-case robustness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The unified threat model (goal / capability / knowledge) and hybrid-norm constraint are technically well-defined and well-motivated for time-series data.\n\n2. The work systematically examines model-specific failure modes, horizon sensitivity, and context-length effects.\n\n3. Defense results are quantitatively compelling. In-domain LAT improves worst-case NMAE up to 10× under PGD and generalizes well out-of-domain.\n\n4. Reproducibility statement and released code enhance reliability."}, "weaknesses": {"value": "**1. Over-claimed novelty and limited contribution boundary**\n\nThe paper repeatedly claims to be **“the first large-scale, systematic robustness evaluation of TSFMs.”**\nHowever, two peer-reviewed works have already addressed adversarial robustness of TSFMs directly:\n\n**Adversarial Vulnerabilities in Large Language Models for Time Series Forecasting - AISTATS 2025**\nPerforms a systematic, cross-model and cross-dataset robustness analysis including TSFM such as TimeGPT, demonstrating that small, structured perturbations cause significant and controllable forecast distortions.\n\n**Temporally Sparse Attack for Fooling LLMs in Time Series Forecasting - ICML 2025 workshop**\nIntroduces a cardinality-constrained optimization attack that manipulates only ≈ 10 % of time steps while severely degrading forecasts of LLM-based TSFMs (including TimeGPT), directly exposing their adversarial weaknesses.\n\nBecause both prior studies explicitly involve TSFMs and directly reveal their adversarial vulnerabilities, the core finding of this submission is no longer novel.\nThe main difference lies in the model family extension (from LLM-TSFMs to other pretrained forecasting models), which constitutes an incremental replication, not a conceptual breakthrough.\nConsequently, the repeated “first large-scale, systematic robustness evaluation of TSFMs” statements appear over-claimed and should be toned down to avoid misleading readers.\n\n**2. Limited methodological innovation**\n\nThe two proposed defenses—moving-average smoothing and LAT—are straightforward adaptations of known techniques with modest empirical tuning.\nNo comparison is provided against input-space adversarial training, noise-based defenses, or detection mechanisms, limiting methodological novelty.\n\n**3. Evaluation gaps**\n\nThe attack suite omits universal or adaptive baselines (e.g., AutoAttack-style ensembles).\n\nNo comparison to traditional forecasting models (LSTM, TCN, Informer), leaving unclear whether TSFMs are uniquely fragile or simply representative of general deep forecasting vulnerabilities.\n\nMechanistic insights remain descriptive: while horizon-boundary and context-length sensitivities are observed, there are no controlled ablations (e.g., disabling patchification or varying decoder type) to establish causality."}, "questions": {"value": "1. Compare LAT with input-space adversarial fine-tuning under equal training cost and report clean-accuracy trade-offs.\n\n2. Have you tested universal or adaptive attacks? It will be intresting to see these attacks' performance on TSFM.\n\n3. Can you conduct controlled ablations (e.g., without patchification or using alternative decoding heads) to validate causal explanations for observed vulnerabilities?\n\n4. Add non-foundation baselines to clarify whether fragility is specific to TSFMs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BdPq5chHfP", "forum": "twpToSgMuO", "replyto": "twpToSgMuO", "signatures": ["ICLR.cc/2026/Conference/Submission9622/Reviewer_iwVq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9622/Reviewer_iwVq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933356054, "cdate": 1761933356054, "tmdate": 1762921158882, "mdate": 1762921158882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission presents an adversarial attack framework for zero-shot time series forecasting models, designed to assess the robustness of time-series foundation models. Two types of attacks are introduced: a white-box attack based on the Fast Gradient Sign Method (FGSM) and a black-box attack employing zero-order optimization. To improve model robustness, the study further proposes two defense strategies: a filter-based preprocessing defense and an adversarial training–based defense."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The research topic is important and timely. The vulnerability and robustness of time series foundation models remain underexplored.\n\nThe experimental design is comprehensive. Six representative models are evaluated across eight diverse datasets, providing convincing evidence to support the study’s findings."}, "weaknesses": {"value": "The primary weakness of this submission lies in its limited technical novelty. The manuscript mainly applies existing adversarial attack and defense techniques to zero-shot time series forecasting models, without introducing fundamentally new methodologies. Consequently, the proposed attacks can largely be mitigated by existing defense mechanisms.\n\nMore specifically:\n\n1. **Relation to Prior Work**: The submission does not clearly articulate its relationship or distinction from prior studies. For example:\n  [1] introduced FGSM-based white-box attacks and proposed filter-based and adversarial fine-tuning defenses for time series forecasting;[2] presented a zero-order optimization (SPSA)-based black-box attack for forecasting models; and [3] developed targeted, gradient-free black-box attacks specifically for zero-shot, LLM-based time series forecasting models. \n\n2. **Novelty and Significance**: The reliance on established adversarial attack and defense methods reduces the overall contribution of the paper. The presented framework does not demonstrate a substantial methodological advancement beyond existing literature.\n\n3. **Insights on Foundation Models**: The work does not sufficiently uncover new challenges or unique vulnerability patterns specific to time series foundation models under adversarial conditions.\n\n4. **Comparative Robustness Analysis**: A direct robustness comparison between zero-shot time series foundation models and conventional forecasting models is missing. Including such baselines would strengthen the evaluation and clarify whether foundation models exhibit distinct robustness characteristics.\n\n**Reference**\n\n[1] Liu, Linbo, et al. \"Robust Multivariate Time-Series Forecasting: Adversarial Attacks and Defense Mechanisms.\" ICLR (2023).\n\n[2] Zhu, Lyuyi, et al. \"Adversarial diffusion attacks on graph-based traffic prediction models.\" IEEE Internet of Things Journal 11.1 (2023): 1481-1495.\n\n[3] Liu, Fuqiang, et al. \"Adversarial Vulnerabilities in Large Language Models for Time Series Forecasting.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2025."}, "questions": {"value": "This submission applies existing adversarial attack methods to evaluate the vulnerabilities of time series foundation models and employs established adversarial defenses to mitigate these attacks. Incorporating new insights or methodological innovations would substantially enhance the significance and contribution of the work. For example, it remains unclear what new attack designs are specifically tailored to the characteristics of time series foundation models, and what unique vulnerability patterns these models exhibit beyond those already observed in conventional time series forecasting models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FOinkbwcwU", "forum": "twpToSgMuO", "replyto": "twpToSgMuO", "signatures": ["ICLR.cc/2026/Conference/Submission9622/Reviewer_ib3J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9622/Reviewer_ib3J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995798188, "cdate": 1761995798188, "tmdate": 1762921158422, "mdate": 1762921158422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission presents an adversarial attack framework for zero-shot time series forecasting models, designed to assess the robustness of time-series foundation models. Two types of attacks are introduced: a white-box attack based on the Fast Gradient Sign Method (FGSM) and a black-box attack employing zero-order optimization. To improve model robustness, the study further proposes two defense strategies: a filter-based preprocessing defense and an adversarial training–based defense."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The research topic is important and timely. The vulnerability and robustness of time series foundation models remain underexplored.\n\nThe experimental design is comprehensive. Six representative models are evaluated across eight diverse datasets, providing convincing evidence to support the study’s findings."}, "weaknesses": {"value": "The primary weakness of this submission lies in its limited technical novelty. The manuscript mainly applies existing adversarial attack and defense techniques to zero-shot time series forecasting models, without introducing fundamentally new methodologies. Consequently, the proposed attacks can largely be mitigated by existing defense mechanisms.\n\nMore specifically:\n\n1. **Relation to Prior Work**: The submission does not clearly articulate its relationship or distinction from prior studies. For example:\n  [1] introduced FGSM-based white-box attacks and proposed filter-based and adversarial fine-tuning defenses for time series forecasting;[2] presented a zero-order optimization (SPSA)-based black-box attack for forecasting models; and [3] developed targeted, gradient-free black-box attacks specifically for zero-shot, LLM-based time series forecasting models. \n\n2. **Novelty and Significance**: The reliance on established adversarial attack and defense methods reduces the overall contribution of the paper. The presented framework does not demonstrate a substantial methodological advancement beyond existing literature.\n\n3. **Insights on Foundation Models**: The work does not sufficiently uncover new challenges or unique vulnerability patterns specific to time series foundation models under adversarial conditions.\n\n4. **Comparative Robustness Analysis**: A direct robustness comparison between zero-shot time series foundation models and conventional forecasting models is missing. Including such baselines would strengthen the evaluation and clarify whether foundation models exhibit distinct robustness characteristics.\n\n**Reference**\n\n[1] Liu, Linbo, et al. \"Robust Multivariate Time-Series Forecasting: Adversarial Attacks and Defense Mechanisms.\" ICLR (2023).\n\n[2] Zhu, Lyuyi, et al. \"Adversarial diffusion attacks on graph-based traffic prediction models.\" IEEE Internet of Things Journal 11.1 (2023): 1481-1495.\n\n[3] Liu, Fuqiang, et al. \"Adversarial Vulnerabilities in Large Language Models for Time Series Forecasting.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2025."}, "questions": {"value": "This submission applies existing adversarial attack methods to evaluate the vulnerabilities of time series foundation models and employs established adversarial defenses to mitigate these attacks. Incorporating new insights or methodological innovations would substantially enhance the significance and contribution of the work. For example, it remains unclear what new attack designs are specifically tailored to the characteristics of time series foundation models, and what unique vulnerability patterns these models exhibit beyond those already observed in conventional time series forecasting models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FOinkbwcwU", "forum": "twpToSgMuO", "replyto": "twpToSgMuO", "signatures": ["ICLR.cc/2026/Conference/Submission9622/Reviewer_ib3J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9622/Reviewer_ib3J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995798188, "cdate": 1761995798188, "tmdate": 1763102353220, "mdate": 1763102353220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper systematically evaluates TSFMs under a unified adversarial framework spanning goals (untargeted/targeted), capabilities (hybrid ℓ₀/ℓ∞ budget), and knowledge (white-box/black-box), showing that even small, structured perturbations can reliably steer forecasts (e.g., flips, drifts, scaling) across domains. TSFMs show pervasive vulnerabilities (e.g., TimesFM particularly sensitive under PGD); adversarial examples often don’t transfer well across models (model-specific failure modes); points near the forecast horizon are most vulnerable; longer contexts improve clean accuracy but amplify attack impact."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear, comprehensive threat modeling & eval setup: Covers white-box (PGD) and black-box (SimBA/ZOO), targeted and untargeted goals, with unified robustness metrics across six TSFMs and eight datasets.\n\n2. Finds pervasive but model-specific vulnerabilities and quantifies factors that modulate attack success (context length, attack location, model size)."}, "weaknesses": {"value": "1. Some robustness signals may reflect gradient obfuscation: MoE-style models appear PGD-resistant, but single-step and query-based attacks still work.\n\n2. Technical contribution seems to be limited. I don't like to use this argument for paper review but vulnerability to adversarial attacks are well-known in the entire ML community."}, "questions": {"value": "This is a pretty comprehensive study but the technical contribution seems to be limited. Defenses using smoothing to me is still vulnerable to adaptive attacks, and the LAT is also not new."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eRqaXHIdBP", "forum": "twpToSgMuO", "replyto": "twpToSgMuO", "signatures": ["ICLR.cc/2026/Conference/Submission9622/Reviewer_WKnA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9622/Reviewer_WKnA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762054858058, "cdate": 1762054858058, "tmdate": 1762921158031, "mdate": 1762921158031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}