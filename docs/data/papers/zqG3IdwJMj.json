{"id": "zqG3IdwJMj", "number": 5466, "cdate": 1757912538892, "mdate": 1759897972709, "content": {"title": "ModelPirate: Security Analysis of Partial Merging Against Model Stealing Attacks", "abstract": "Model merging is a promising technique to enhance the capabilities of neural networks (NNs) by integrating multiple downstream fine-tuned models without requiring access to clients' raw data or substantial computation resources. However, conventional model merging typically requires collecting the full set of fine-tuned model parameters from multiple clients, which may expose them to model-privacy risks. An emerging approach, known as partial model merging (PMM), mitigates this risk by splitting each model into private and shared parts, where only the shared part is merged while the private part remains local to each client. Despite its stricter parameter fusion, PMM can still achieve competitive performance compared to full-parameter sharing. However, the privacy properties of PMM remain underexplored. In this paper, we propose a novel model stealing attack and assess the risk of reconstructing the unshared private part of a partially merged model under eight attack scenarios with varying prior knowledge (i.e., partial training data, model parameters and/or model structure). Our comprehensive experiments reveal that merging NNs without adequate protection is highly vulnerable. Even when only a small fraction of training data, model parameters, or model structure is exposed, adversaries can still recover significant portions of the private model's performance.", "tldr": "", "keywords": ["model merging", "model stealing", "privacy", "security"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f53ce081a2c905a2fe78b4b21b23fe2b360f1b87.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates model stealing attacks in the setting of partial model merging. They investigate how the efficacy of model stealing attack changes with varying number of shared layers, training data availability and knowledge of model structure. Their experiments show that model merging without adequate protection is highly vulnerable to model stealing attacks with clone accuracies exceeding the accuracy of the merged models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The key ideas are presented well.\n2. The experimental methodology seems well thought out and clearly ablates the effects of different components of the attack.\n3. The deep-shallow design for the clone model is interesting."}, "weaknesses": {"value": "1. In the introduction (L052), authors claim the partial model merging emerged in response to the problem of model stealing attack. I’m not convinced of this premise.\nThis claim is not supported by the ZipIt! paper that proposed partial model merging. \n2. There are way too many notations, which make it hard to read the paper. A table which describes the notations might be useful.\n3. I did not find much novelty in the design of the model stealing attack itself (besides the deep-shallow design of the clone model). The paper simply applies existing model stealing methods to the setting of partial model merging and provides results.\n4. Experiments are limited to two datasets."}, "questions": {"value": "1. Can you provide citations to support the claim that partial model merging is supposed to help with model stealing.\n2. L261: How does the adversary have access to the classification head T_v? Isn’t this privately held by C_v?\n3. How does the efficacy of the attack change if you don’t adopt a deep-shallow design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MNwIYBPEON", "forum": "zqG3IdwJMj", "replyto": "zqG3IdwJMj", "signatures": ["ICLR.cc/2026/Conference/Submission5466/Reviewer_8tdM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5466/Reviewer_8tdM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761349759399, "cdate": 1761349759399, "tmdate": 1762918079839, "mdate": 1762918079839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the privacy vulnerabilities of Partial Model Merging (PMM), an emerging model merging paradigm that shares only a subset of parameters among clients to balance utility and privacy. It proposes ModelPirate, a novel model stealing attack specifically designed for PMM, which attempts to reconstruct the unshared private model part under eight attack scenarios with varying degrees of adversarial prior knowledge (training data, model structure, and shared parameters). Extensive experiments on both vision (ViT-based) and language (T5) models show that PMM is vulnerable to model reconstruction even with limited exposure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel problem setting**: As far as I know, it is the first systematic privacy analysis of PMM, a timely and underexplored topic given the increasing adoption of model merging methods.\n- **Comprehensive attack framework**: The formulation of eight attack scenarios with different knowledge configurations provides a detailed and rigorous threat model.\n- **Thorough experiments**: Evaluation across multiple datasets, model architectures (ViT-B/32, ViT-B/16, ViT-L/14, T5), and modalities (vision and NLP) demonstrates good generality."}, "weaknesses": {"value": "- **Lack of defense-side experiments.** The paper convincingly demonstrates the vulnerability of PMM but does not evaluate any defense baselines. It would be valuable to test whether common countermeasures, such as differential privacy (DP-SGD), weight quantization, or parameter noise injection, can mitigate the ModelPirate attack while maintaining model utility.\n- **Limited ablation on merging algorithms.** All experiments are conducted with layer-wise Task Arithmetic. To verify the generality of the attack, it is necessary to repeat key experiments using alternative model merging algorithms, such as Fisher-weighted averaging, Adamerging, or TIES-merging. This would reveal whether ModelPirate is robust across different merging schemes.\n- **Shallow evaluation on LLMs.** The extension to T5 is appreciated, but the analysis for NLP models is limited to a single pair of tasks (IMDB $\\rightarrow$ QASC) and lacks deeper investigation of linguistic behaviors or generalization differences.\n- **Insufficient analysis of loss function choices.**: The attack optimization is fixed to the MAE. Conducting ablation studies with alternative objectives such as cross-entropy, KL divergence, or knowledge-distillation loss, would clarify whether ModelPirate’s performance critically depends on this specific loss function and how robust it is across optimization objectives."}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dIBOUlRE62", "forum": "zqG3IdwJMj", "replyto": "zqG3IdwJMj", "signatures": ["ICLR.cc/2026/Conference/Submission5466/Reviewer_rMzP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5466/Reviewer_rMzP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761401272783, "cdate": 1761401272783, "tmdate": 1762918079517, "mdate": 1762918079517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the privacy risks of Partial Model Merging (PMM), a method where clients merge only part of their fine-tuned neural networks to form a shared model while keeping some layers private. The authors introduce ModelPirate, a model-stealing attack that reconstructs the private portion of a victim model under eight adversarial scenarios with varying levels of prior knowledge, including shared model parts, private structure, and partial training data. Experiments on vision and language models, such as ViT, CLIP, and T5, across several datasets demonstrate that an attacker can recover a significant portion of the victim’s performance even with limited information."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and well structured. It defines a systematic threat model that enumerates adversarial capabilities in a comprehensive way. The experimental section is extensive, spanning multiple architectures and modalities, and the results are reproducible and well documented."}, "weaknesses": {"value": "1. The conceptual scope of PMM needs clarification. PMM is not a well-established research paradigm, and the only cited related work, ZipIt! (Stoica et al., 2024), does not actually introduce PMM as a distinct framework or address privacy concerns. This makes the motivation for focusing on PMM somewhat speculative. Without broader evidence that PMM is being used or seriously considered in real systems, the practical relevance of the proposed analysis is limited.\n\n2. The setup presented in Figure 3 is essentially equivalent to one-shot federated learning or split learning. In these established paradigms, each client fine-tunes a model locally, shares a subset of layers for aggregation, and then combines the result with its private part. The paper does not clearly explain how PMM differs conceptually or operationally from these methods. As a result, the framing of PMM as a distinct problem domain appears overstated, and the novelty of the contribution becomes less convincing.\n\n3. The technical contribution of the ModelPirate attack is limited. The attack primarily applies standard model-stealing and knowledge-distillation principles under different assumptions about the adversary’s prior knowledge. The methodological novelty lies mainly in systematically enumerating these scenarios rather than introducing a new algorithmic mechanism. Consequently, the attack design feels more heuristic than innovative.\n\n4. The paper works on a strong assumption that an adversarial client can query the victim’s full model to obtain labels for clone training. This assumption is reasonable in machine-learning-as-a-service settings, where an external user interacts with a deployed model via an API, but it contradicts the principles of PMM, where private parts of the model are never shared or exposed. Allowing cross-client querying effectively converts the setup into a model-extraction attack against a public service rather than a PMM-specific vulnerability. Without this assumption, the proposed attack would not function as described, raising doubts about the realism of the threat model.\n\n5. The evaluation, while extensive, remains largely heuristic. It measures privacy leakage only through clone accuracy and does not provide a theoretical or information-theoretic interpretation of leakage. There is no analysis of how query budgets, output formats, or noise in victim responses would affect attack success. The experiments use small-scale datasets and fixed hyperparameters, with little justification for these design choices. While the results demonstrate that the attack can reproduce model behavior empirically, they do not provide insight into the mechanisms driving the leakage or its robustness under realistic constraints.\n\n6. The comparisons rely on classic model-stealing baselines such as Knockoff, JBDA, and Random; omitting more recent or adaptive extraction techniques limits the strength of the evaluation. The paper also does not propose or evaluate any defenses, leaving the analysis one-sided and incomplete."}, "questions": {"value": "1. How is the adversary realistically able to query another client’s full model within PMM?\n2. Would the ModelPirate attack still apply if such querying were disallowed, and how would labels be obtained in that case?\n3. How does PMM fundamentally differ from one-shot federated or split learning, which appear to follow the same workflow?\n4. What aspects of ModelPirate are technically novel compared with existing model-stealing and distillation attacks?\n5. Have you considered any concrete defense strategies, such as noisy merging, differential privacy, or secure aggregation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xYIANl6Vnb", "forum": "zqG3IdwJMj", "replyto": "zqG3IdwJMj", "signatures": ["ICLR.cc/2026/Conference/Submission5466/Reviewer_vKhS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5466/Reviewer_vKhS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930605145, "cdate": 1761930605145, "tmdate": 1762918079185, "mdate": 1762918079185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ModelPirate, a framework for analyzing model stealing attacks aimed at reconstructing the private component of a model in a partial model merging scenario. In particular, the authors investigate how different types of information leakage including training data, client architecture, and the shared model component, affect the accuracy of model stealing. Through empirical experiments, the authors demonstrate that adversaries can still recover a substantial portion of the private model component even when they have access to only a small subset of the training data, the client architecture, and the shared part model."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper provides insight into the trade-offs between different types of information leakage and their corresponding privacy risks in partial model merging.\n\n- ModelPirate demonstrates high attack accuracy across multiple attack scenarios."}, "weaknesses": {"value": "- The writing needs improvement and further proofreading. Some terms, such as local accuracy, are defined but not used throughout the paper.\n\n- Regarding the evaluation metric, it is unclear why Merged Accuracy is considered the lower bound accuracy of the cloned model. If it truly represents a lower bound, then Table 1 should not contain results below this value; however, several entries do fall below it. Furthermore, the defining Merged Accuracy as the model accuracy of $S_m + P_a$ requires clarification. Since  $P_a$ corresponds to a different task, its classification head may output a different set of classes from those in  $T_v$, resulting in a different number of output classes. Even when the number of classes coincides, changing the label order in $T_v$  could still alter the model accuracy of  $S_m + P_a$\n\n- The setup for training the client model using both the attack dataset  $D_a$ and the victim dataset $D_v$ is unclear. Because $D_a$ and \n$D_v$ are distinct datasets with different label spaces, how are the labels in $D_a$ redefined or aligned for joint training of the new client model?\n\n- I am curious about the role of the adversary in this model stealing setup.  In this setup, the adversary also acts as an ordinary participant, contributing its parameters to the merged model. What is the role or impact of this action? What would happen if the adversary did not participate in the merging process but still had access to certain information, such as part of the training data, the client model architecture, or the shared component of the client model?\n\n\n- In lines 361–364, the authors argue that it is more important for a client to protect its training data than its private model structure. However, the results of AS[X01] and AS[X10] do not support this claim. For instance, in AS[001] and AS[010], the results for ViT-L/14 are contradictory. Notably, in AS[010], where only the private model structure is known, the attack surprisingly achieves 93.81%, the highest performance among all attacks on the ViT-L/14 model.\n\n- The results in Table 1 also exhibit an abnormal trend: AS[110] often outperforms AS[111] by a large margin, implying that access to private training data may decrease attack performance. Moreover, AS[111] is not the best-performing setup, even though the adversary in this case knows the client model architecture, shared components, and training data. This raises concerns about whether the client model’s training process might limit its learning capability, thereby leading to misleading comparisons."}, "questions": {"value": "- Could the authors clarify the definition of merged accuracy and explain how the adversary’s dataset $D_a$ is used to train the cloned client model?\n\n- Could the authors elaborate on their argument that it is more important for a client to protect its training data than its private model structure?\n\n- Could the authors provide more details about the client model’s training process and explain why the AS[111] setup does not yield the best performance?\n\n- Could the authors to clarify the specific role of the adversary in the proposed model stealing setup when they also involve in merging process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LyyBD8BCQs", "forum": "zqG3IdwJMj", "replyto": "zqG3IdwJMj", "signatures": ["ICLR.cc/2026/Conference/Submission5466/Reviewer_Xx9S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5466/Reviewer_Xx9S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978566951, "cdate": 1761978566951, "tmdate": 1762918078570, "mdate": 1762918078570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}