{"id": "USMgLvZk1X", "number": 15268, "cdate": 1758249597865, "mdate": 1759897317033, "content": {"title": "A Multimodal LLM Approach for Visual Question Answering on Multiparametric 3D Brain MRI", "abstract": "We introduce mpLLM, a prompt-conditioned hierarchical mixture-of-experts (MoE) architecture for visual question answering over multi-parametric 3D brain MRI (mpMRI). mpLLM routes across modality-level and token-level projection experts to fuse multiple interrelated 3D modalities, enabling efficient training without image–report pretraining. To address limited image-text paired supervision, mpLLM integrates a synthetic visual question answering (VQA) protocol that generates medically relevant VQA from segmentation annotations, and we collaborate with medical experts for clinical validation. mpLLM outperforms strong medical VLM baselines by 5.3% on average across multiple mpMRI datasets. Our study features three main contributions: (1) the first clinically validated VQA dataset for 3D brain mpMRI, (2) a novel multimodal LLM that handles multiple interrelated 3D modalities, and (3) strong empirical results that demonstrate the medical utility of our methodology. Ablations highlight the importance of modality-level and token-level experts and prompt-conditioned routing. We have included our source code in the supplementary materials and will release our dataset upon publication.", "tldr": "We propose a clinically validated VQA dataset and novel multimodal LLM for multiparametric 3D brain MRI.", "keywords": ["Multimodal LLM", "VQA", "Medical Imaging"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35214c3e86f7b693c114395c452e93c3ee18ed05.pdf", "supplementary_material": "/attachment/c0f8ddbd22703b35aaf4ed6c5ca7971a0afce9d1.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents mpLLM, a multimodal large language model for visual question answering on multiparametric 3D brain MRI. The key contribution is a prompt-conditioned hierarchical mixture-of-experts (MoE) architecture that efficiently fuses multiple interrelated 3D modalities without requiring large-scale image-report pretraining. To address the lack of VQA data, the authors propose a synthetic data generation protocol that derives question-answer pairs from segmentation annotations. The model is evaluated on three BraTS datasets and shows 5.3% average improvement over strong baselines while using 50% less GPU memory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel hierarchical mixture-of-experts architecture that specifically addresses the challenge of fusing multiple interrelated 3D MRI modalities through prompt-conditioned routing.\n\n2. The work provides a VQA dataset specifically designed for 3D brain multiparametric MRI. \n\n3. The paper demonstrates consistent improvements over strong baselines across multiple datasets, with an average 5.3% performance gain."}, "weaknesses": {"value": "1. The ablation results show relatively modest improvements, with the Token-level MoE outperforming 1.7% and Modality-level MoE + Prompt-based MoE weights only increase additional 0.5% without significant analysis. \n\n2. The synthetic VQA dataset suffers from severe template-based question generation that does not reflect real clinical queries radiologists would ask in practice. The questions focus on low-level descriptive features (volume ranges, shape categories) rather than clinically meaningful diagnostic or treatment-related inquiries that would actually support medical decision-making.\n\n3. The clinical validation is extremely limited in scope, involving only one radiologist evaluating 10 cases with moderate inter-annotator agreement (Kappa score of 50.4), and the clinical sufficiency rate of only 37.3% actually reveals that most model responses are inadequate for real clinical use. \n\n4. The paper lacks analysis of when and why the MoE architecture provides benefits, missing opportunities to explain which types of questions benefit most from multi-modal fusion versus single modalities."}, "questions": {"value": "1. Can you provide a detailed analysis of what specific question types or clinical scenarios benefit from the token-level MoE versus modality-level experts?\n\n2. Given that only 37.3% of your model's responses were deemed clinically sufficient and the inter-annotator agreement for some tasks (particularly Region with Kappa 42.9) suggests unreliable ground truth, how do you justify claiming this is a \"clinically validated\" dataset?\n\n3. Can you provide a breakdown of model performance stratified by question type and modality combination to demonstrate when multi-modal reasoning actually matters? Since the shared expert baseline achieves 67.3% accuracy, suggesting single-modality or simple fusion may suffice for many questions, what percentage of questions in your dataset genuinely require sophisticated multi-modal analysis, and can you visualize the router's attention patterns to show how different modalities are weighted for different clinical queries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OytgF3UTsd", "forum": "USMgLvZk1X", "replyto": "USMgLvZk1X", "signatures": ["ICLR.cc/2026/Conference/Submission15268/Reviewer_fWj2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15268/Reviewer_fWj2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761609482620, "cdate": 1761609482620, "tmdate": 1762925569679, "mdate": 1762925569679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents mpLLM, a prompt-conditioned hierarchical MoE model for visual question answering on multiparametric 3D brain MRI (mpMRI). It fuses multiple MRI modalities through modality- and token-level experts, enabling efficient training without image–report pretraining. A synthetic VQA generation protocol and expert validation support dataset creation. Experiments show 5.3% average improvement over strong baselines, making mpLLM the first clinically validated VQA system for 3D brain mpMRI."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. A new VQA  benchmark"}, "weaknesses": {"value": "The paper’s contributions appear limited, and it is unclear whether the use of Mixture-of-Experts (MoE) is genuinely critical for mpMRI interpretation. The overall presentation and organization of the paper are weak, and the discussion fails to effectively address or connect with the key challenges and insights in current multimodal LLM research."}, "questions": {"value": "1. Why is the Hierarchical MoE Block architecture necessary in this context? If the model consistently handles three fixed input modalities, wouldn’t it be simpler and more transparent to use separate vision encoders for each input type? This design could also allow independent scaling of each encoder without adding the complexity of MoE routing.\n\n2. The layout in Page10 is wrong\n\n3. The Table 2 is out of boundaries\n\n4. As shown in Table 3, MoE brings limited improvement"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hUICj9nk9U", "forum": "USMgLvZk1X", "replyto": "USMgLvZk1X", "signatures": ["ICLR.cc/2026/Conference/Submission15268/Reviewer_v9ZF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15268/Reviewer_v9ZF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933185149, "cdate": 1761933185149, "tmdate": 1762925568867, "mdate": 1762925568867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents mpLLM, a prompt-conditioned hierarchical MoE model for visual question answering on multiparametric 3D brain MRI (mpMRI). It fuses multiple MRI modalities through modality- and token-level experts, enabling efficient training without image–report pretraining. A synthetic VQA generation protocol and expert validation support dataset creation. Experiments show 5.3% average improvement over strong baselines, making mpLLM the first clinically validated VQA system for 3D brain mpMRI."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. A new VQA  benchmark"}, "weaknesses": {"value": "The paper’s contributions appear limited, and it is unclear whether the use of Mixture-of-Experts (MoE) is genuinely critical for mpMRI interpretation. The overall presentation and organization of the paper are weak, and the discussion fails to effectively address or connect with the key challenges and insights in current multimodal LLM research."}, "questions": {"value": "1. Why is the Hierarchical MoE Block architecture necessary in this context? If the model consistently handles four fixed input modalities, wouldn’t it be simpler and more transparent to use separate vision encoders for each input type? This design could also allow independent scaling of each encoder without adding the complexity of MoE routing.\n\n2. The layout in Page10 is wrong\n\n3. The Table 2 is out of boundaries\n\n4. As shown in Table 3, MoE brings limited improvement"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hUICj9nk9U", "forum": "USMgLvZk1X", "replyto": "USMgLvZk1X", "signatures": ["ICLR.cc/2026/Conference/Submission15268/Reviewer_v9ZF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15268/Reviewer_v9ZF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933185149, "cdate": 1761933185149, "tmdate": 1763097156008, "mdate": 1763097156008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces mpLLM, a MoE-based multimodal LLM for visual question answering on 3D brain mpMRI, enabling efficient training without image-report pretraining. It includes a synthetic VQA pipeline generated from segmentation annotations, addressing the lack of paired medical VQA data. Experiments on 3 datasets show that mpLLM outperforms several medical VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces three synthetic 3D brain mpMRI VQA datasets, which facilitate future research and development in medical VQA.\n2. The incorporation of a Mixture-of-Experts (MoE) architecture to handle multiple 3D modalities is well-motivated. The proposed approach demonstrates strong performance across three benchmark datasets, surpassing several state-of-the-art models."}, "weaknesses": {"value": "1. One criticism of this type of work I have is how it will help the clinicians and human study. So, it would have been nicer if the authors can do a user study with radiologists to evaluate the findings and include it in the paper or in the future.\n\n2. The discussion of the model’s limitations is rather limited, particularly regarding hallucinations. Given that hallucinations can lead to serious consequences in medical AI applications, a more in-depth analysis and discussion of this issue would be valuable.\n\n3. A relevant prior work is missing from the related work section. Specifically, [1] introduces BrainGPT, a multimodal large language model for 3D brain CT report generation, which is closely related to the topic of this paper.\n\nReference:\n[1] Towards a Holistic Framework for Multimodal Large Language Models in Three-dimensional Brain CT Report Generation, Nature Communications 2025."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gnnDcM87fj", "forum": "USMgLvZk1X", "replyto": "USMgLvZk1X", "signatures": ["ICLR.cc/2026/Conference/Submission15268/Reviewer_kEGa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15268/Reviewer_kEGa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032520343, "cdate": 1762032520343, "tmdate": 1762925568535, "mdate": 1762925568535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces mpLLM, a multimodal LLM with a prompt-conditioned hierarchical mixture-of-experts for VQA on 3D multiparametric brain MRI. It includes a synthetic, clinician-validated VQA dataset generated from segmentation masks to address the lack of paired image–text data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces the first VQA dataset for 3D brain multiparametric MRI, filling a current gap in multimodal medical reasoning.\n- The authors propose a novel hierarchical architecture that shows better performance compared to existing methods on this type of data.\n- The use of intra-modality and token-level experts is an interesting idea."}, "weaknesses": {"value": "- It is difficult to determine the value of the contributions of the model and the dataset without extrinsic benchmarks / downstream tasks. While results on the presented test set seem strong, the synthetic questions may lack linguistic and semantic diversity, and limit generalization to out-of-distribution samples. Given the lack of 3D MRI VQA benchmarks, the authors could strengthen the work by including a small zero-shot evaluation subset to demonstrate that their contribution is valuable beyond the few categories of questions that they introduced with the dataset.\n\n- The method is a architectural adaptation rather than a fundamentally new paradigm.\n\n- The notation and terminology would benefit from polishing: There appear to be several symbols that are used inconsistently or are confusingly similar, for example $N$ defines the number of experts, while $N_{i}$ and $N_{m}$ are used for number of images and modalities. The low-level router is introduced briefly (L. 184) but later referred to as the image modality expert without clear distinction."}, "questions": {"value": "-  The routing weights over modalities and tokens are computed from the text query. This probably works because – as the authors mentioned – the task information is embedded within the text prompt. However, I wonder if expanding the text would improve the high level router performance. Have the authors tried other ways to enrich the query? E.g. by appending synthetic reasoning that makes more explicit which regions or modalities are relevant to answering the question.\n\n- To what extent is the routing mechanism novel compared to existing MoE-based VLM or LLM architectures? Which components are newly proposed versus simple adaptations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jYDRZDN9JL", "forum": "USMgLvZk1X", "replyto": "USMgLvZk1X", "signatures": ["ICLR.cc/2026/Conference/Submission15268/Reviewer_HLqR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15268/Reviewer_HLqR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134515928, "cdate": 1762134515928, "tmdate": 1762925568024, "mdate": 1762925568024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}