{"id": "IWvfQCkHRr", "number": 22274, "cdate": 1758328819207, "mdate": 1759896875490, "content": {"title": "Putting It All into Context: Simplifying Agents with LCLMs", "abstract": "Recent advances in language model (LM) agents have demonstrated significant potential for automating complex real-world tasks. \nTo make progress on these difficult tasks, LM agent architectures have become increasingly complex, often incorporating multi-step retrieval tools, multiple agents, and scaffolding adapted to the underlying LM.\nIn this work, we investigate whether all of this complexity is necessary, or if parts of these scaffolds can be removed on challenging tasks like SWE-bench.  We show that in the case of SWE-bench, simply putting the entire environment into the context of a long context language model (LCLM) and properly prompting the model makes it competitive with carefully tuned, complex agent scaffolds. \nWe show that a Gemini-1.5-Pro model without any scaffolding or tools achieves 38\\% on SWE-Bench-Verified, comparable with approaches using carefully tuned agent scaffolds (32\\%).\nWhile the unscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic architectures, we demonstrate that the more capable Gemini-2.5-Pro using the same unscaffolded approach directly attains a 50.8\\% solve rate. Additionally, a two-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a competitive 48.6\\% solve rate. \nThese results suggest that LCLMs can enable a more monolithic design, reducing reliance on exploration scaffolds in fully observable regimes.", "tldr": "This work proposes a scaffolding-free approach to LM agents by using long-context LMs to fully observe environments and generate actions directly, achieving strong results on SWE-Bench-Verified.", "keywords": ["Large language model; Agent; Coding; Long-context Language Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a3ba8470b4e4ee35eebacfb08b81a386121aa71.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper shows that dumping the entire codebase into a long-context LM (LCLM) and prompting it directly matches or beats heavily engineered agent pipelines on SWE-bench: Gemini-1.5-Pro scores 38 % and Gemini-2.5-Pro hits 50.8 % without any tools, retrieval, or multi-step scaffolding. The authors compress large repos to fit the context window, apply simple chain-of-thought prompts, and optionally let a stronger short-context model refine the patch. Results demonstrate that full observability plus in-context reasoning turns open-ended software-engineering tasks into closed-ended QA, questioning the need for complex agent frameworks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method transfers robustly across Gemini, Claude, and open-sourced models without task-specific tuning. \n- The work identifies simple techniques boost real-world system performance, offering practical value to applied practitioners.\n- The paper shows strong empirical results, rivaling or surpassing heavily engineered baselines."}, "weaknesses": {"value": "- Missing discussion of recent agentic framework. For instance, CodeAct and Agentess were preprinted in 2024, while there are other recent works in this area which address similar questions.\n- Lack of mathematical details for core steps. The key steps of file ranking, compression, and patch selection are described algorithmically but lack precise mathematical formulation or notation.\n- Insufficient analysis and experiments.The paper illustrate how method compression enables most SWE-Bench tasks to fit in a 2M-token context, but the approach may not generalize to even larger codebases in practice (most cases in practice). The scalability claims are not demonstrated with out-of-benchmark tasks."}, "questions": {"value": "- Could you please provide more comparison with more recent works?\n- It would be better if the author could conduct more experiments on other benchmarks to demonstrate the universality of the conclusion.\n- Please give more detailed mathematical formulation or notation about the framework, not just the description of the whole pipeline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "94weyOzP0J", "forum": "IWvfQCkHRr", "replyto": "IWvfQCkHRr", "signatures": ["ICLR.cc/2026/Conference/Submission22274/Reviewer_aF6w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22274/Reviewer_aF6w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807852914, "cdate": 1761807852914, "tmdate": 1762942147464, "mdate": 1762942147464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new paradigm for intelligent agent design: by directly inputting the entire environment state into a long-context language model (LCLM), it replaces the complex information-gathering scaffolding (such as tool calls and multi-round interactions) in traditional intelligent agents. Experiments on SWE-bench (a software engineering task benchmark) demonstrate that this \"state-in-context\" approach can match or even surpass the performance of traditional complex agent systems without the need for scaffolding. For example, Gemini-1.5-Pro achieved a 38% success rate on SWE-Bench-Verified, while Gemini-2.5-Pro reached 50.8%."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Highly Original: Proposed \"state-in-context\" agents, revolutionizing traditional multi-round interactive agent design.\n\n2.Simple Method: No complex toolchain required, relying solely on prompts and context to complete complex tasks.\n\n3.Excellent Performance: Gemini-2.5-Pro's DIRECTSOLVE method achieved a 50.8% success rate, surpassing most scaffolding systems.\n\n4.Scalability: Supports context compression and multi-model collaboration (e.g., SELECTSOLVE), adapting to diverse models and tasks.\n\n5.Extensive Ablation Experiments: Verified the impact of key factors such as prompting techniques, file ordering, and context length."}, "weaknesses": {"value": "1. According to public information, the SWE-BENCH-VERIFIED index is already very high (78.80%). You should put more experimental results in the experimental section to let everyone confirm the feasibility of your method.\n\n2. There are also some other mainstream benchmarks in the current code field or complex task field. I hope to provide some more experimental results and conclusions, which will make the method more convincing.\n\n3. Your references contain many methods from the past, but few new methods recently. I hope you can add some of the latest methods and provide certain indicator results, which will make your method more recognized.\n\n4 .In your SELECTSOLVE method, the surrogate model seems to play a very important role. I hope to provide more experimental results on the surrogate model to make the method more credible."}, "questions": {"value": "1. Your references only have two works published in 2025. Is there really no other work? In my opinion, solving complex problems has always been important and has attracted much attention.\n\n2.It seems to me that your method is very similar to Retrieval-Augmented Generation. Do you have any relevant literature cited? Or can you explain the difference between your method and Retrieval-Augmented Generation?\n\n3 .The appendix is ​​not well-structured, and it is difficult to give any suggestions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iXPuo7vbRy", "forum": "IWvfQCkHRr", "replyto": "IWvfQCkHRr", "signatures": ["ICLR.cc/2026/Conference/Submission22274/Reviewer_dXQ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22274/Reviewer_dXQ4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897219874, "cdate": 1761897219874, "tmdate": 1762942146949, "mdate": 1762942146949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper investigates whether complex agentic scaffolding (multi-step retrieval, tool usage, and iterative reasoning loops) is necessary for solving fully observable tasks such as SWE-Bench. The authors propose a simplified “state-in-context” agent design that places the entire environment state directly into the context window of a Long-Context Language Model (LCLM), allowing it to operate without external tools or structured scaffolding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear and well-motivated research question: The authors identify a meaningful gap—whether agentic complexity is necessary in fully observable environments."}, "weaknesses": {"value": "- Critical Methodological Contradiction Between Core Claims and Actual Implementation. The paper emphasizes that it explores \"simply putting the entire environment into the context of a long context language model (LCLM) and properly prompting the model\" (Abstract, line 17), while criticizing existing work for requiring \"a careful design of agentic scaffoldings tailored to specific tasks\" (line 192). However, the actual implementation still follows the agentic workflow approach:\n    - In \"3.2 State Compression\": The method uses \"a straightforward ranking-and-filtering approach”(line 262) to filter files, which is equivalent to code localization in Agentless\n    - In \"3.2 Patch format and validation\": The paper adopts identical operations from Agentless, including the Search/Replace edit format, reproduction tests, regression tests, and majority voting\n    - In \"4.3 ABLATION STUDY\" and \"A.1 ADDITIONAL RESULTS\": These sections explain the impacts of prompting techniques, compression, target file location, and other operations, which contradicts the claim of \"simply putting the entire environment\"\n  - Essentially, the authors still rely on human-engineered agentic workflows to simplify the environment, contradicting the claim that this approach is \"simpler to train end-to-end\" (line 77). If these operations are intended to mitigate the insufficient capabilities of current LCLMs, it would be more appropriate to validate the approach starting from a task with a smaller environment size."}, "questions": {"value": "- several key analyses are missing:\n    - Both LCLM and Agentless \"prompt the LM to perform localization based on the compressed repository.\" Does this mean that Agentless's additional \"embedding-based retrieval to localize\" actually produces worse localization results? This seems counterintuitive and requires explanation.\n    - Combined with the analysis at line 437, does the LCLM's advantage over Agentless stem solely from smaller file-level localization errors?\n    - Critical missing experiment: If we control for the same recall (i.e., ensure both methods retrieve the same set of relevant files), does LCLM still maintain its advantage? This would reveal whether the benefit comes from better localization or better utilization of the localized files.\n\n- Others\n  - The claim that \"multi-hop QA, software engineering do not inherently require active information gathering\" is inaccurate. However, many multi-hop QA tasks explicitly require search (e.g., BrowseComp, WebWalker, knowledge base reasoning).\n  - Line 177 states \"We instantiate state-in-context agents in two ways\" but only describes DIRECTSOLVE.\n  - Incomplete Prompt Documentation in Appendix C."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qIRKL5KhUK", "forum": "IWvfQCkHRr", "replyto": "IWvfQCkHRr", "signatures": ["ICLR.cc/2026/Conference/Submission22274/Reviewer_ee2y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22274/Reviewer_ee2y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904194677, "cdate": 1761904194677, "tmdate": 1762942146708, "mdate": 1762942146708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether the complex, multi-step agentic scaffolding currently used for software engineering tasks is necessary. The authors propose a simpler \"state-in-context\" approach, leveraging Long-Context Language Models to process the entire (or a compressed version of) a code repository in a single pass. They test this on the SWE-Bench-Verified benchmark and introduce two methods: DIRECTSOLVE, where a single LCLM call generates the solution patch , and SELECTSOLVE, a two-stage hybrid where an LCLM first localizes relevant files, which are then fed to a (potentially different) Short-Context LM for repair. The results demonstrate that these simplified, \"monolithic\" designs are highly competitive."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper's core research question is simple, timely, and contrarian. I think this state-in-context design is a useful study case for the agent design space.\n- The problem is well motivated. The paper spent quite some time and analysis to show that a lot of coding problems have context that can be reasonable reduced to below 2m. This helps build the case that we can have the proposed state in context.\n- The paper's findings are meaningful. It provides a powerful, simple baseline that, with a strong model like Gemini-2.5-Pro, achieves a state-of-the-art 50.8% pass@1 rate. Although I would love to see this compared to some of the more recent scaffolds like AIDE or R&D. But this is just a suggestion. Lack of those doesn't diminish this paper's contribution."}, "weaknesses": {"value": "```Long context performance is a bit contradictory```\n\nFigure 4 and Table 5 show that LCLM performance decreases as context length grows and is highly sensitive to the position of the target file (\"lost in the middle\"). This strongly suggests that current LCLMs are not effective at \"in-context retrieval\" over very long, noisy inputs. This finding, which is central to the paper's thesis, should be in the main body. It weakens the \"monolithic\" DIRECTSOLVE argument and implies that a smarter selection/retrieval step (like that in SELECTSOLVE or RAG) is still fundamentally necessary.\n\nIn addition, I foudn the scaffolding-free terminology inaccurate. The SELECTSOLVE is a scaffold abeit being simpler. \n\n```Putting entire environment into context is not entirely true```\n\nThe paper's premise of \"putting the entire environment into the context\"  is not fully met. The data shows that only 21.2% of instances fit this \"All Files\" definition. The authors get to 85.0% coverage only by aggressively pruning the state to Core Code Only. The compression step itself, which uses an LCLM to rank files based only on the repository structure and issue, is a form of light scaffolding and a potential weak point.\n\n```Cost and Practicality```\n\nThe authors rightly acknowledge the high financial cost of the LCLM approach (2.60 per instance vs. 0.25 for Agentless). Their counter-argument relies on KV caching for \"repeated queries\". However, this is a significant caveat. In a real-world CI/CD pipeline or for a developer working on a new bug, the first query on a given codebase (or a new version of it) will always be a \"cold start,\" incurring the full, high cost. Another counter arguments from the author is that cost will continue to go down. While this is true, and it might even go down in order of magnitudes, the inference-time may also scale in orders of magnitudes. It is always better to do a task with less tokens given that consideration."}, "questions": {"value": "- The cost counter-argument hinges on KV caching for \"repeated queries\". In a real-world software engineering context where codebases are constantly evolving, how often do you envision this cache being useful? Wouldn't a new commit or branch invalidate the entire cache?\n- Another question that's similar to the point above is when a long chain-of-thought or trial and error needs to be done (e.g., in a multi-turn data scientist env), what if what's generated so far exceeds the context length. How is that handled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NKUrmFBQ1q", "forum": "IWvfQCkHRr", "replyto": "IWvfQCkHRr", "signatures": ["ICLR.cc/2026/Conference/Submission22274/Reviewer_ckh8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22274/Reviewer_ckh8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965512005, "cdate": 1761965512005, "tmdate": 1762942145790, "mdate": 1762942145790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}