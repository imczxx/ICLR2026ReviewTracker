{"id": "nvNSyX5uV6", "number": 11394, "cdate": 1758198061002, "mdate": 1763105153135, "content": {"title": "OmniCustom: A Multimodal-Driven Architecture for Customized Video Generation", "abstract": "Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose OmniCustom, a multi-modal customized video generation model that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, OmniCustom introduces an identity-enhanced text-image conditioning module based on LLaVA for improved multi-modal understanding, and an image ID enhancement module that leverages temporal concatenation to reinforce identity features. To enable flexible audio- and video-driven customization, we further propose modality-specific injection modules. Our identity-disentangled AudioNet injects temporally aligned audio features into video latents via spatial cross-attention, enabling precise audio control. For video-driven generation, we design an identity-disentangled video injection module that projects conditional video into the latent space and efficiently aligns video features with latents for seamless integration. Extensive experiments on single- and multi-subject scenarios show that OmniCustom significantly outperforms state-of-the-art methods in ID consistency, realism, and text-video alignment. We further demonstrate its robustness on downstream tasks such as audio- and video-driven customized video generation, highlighting the effectiveness of our multi-modal conditioning and identity-preserving strategies for customized video generation.", "tldr": "", "keywords": ["Video Generation", "Video Customization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/abc757139fac1ec00323274d20b54904540e76c3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents OmniCustom, a multi-modal customized video generation model, designed to address two key limitations of existing methods: poor subject identity consistency and restricted input modalities. Built on the HunyuanVideo framework, OmniCustom supports flexible video generation conditioned on text, images, audio, and video inputs and maintains high subject consistency across single- and multi-subject scenarios. Key technical components include: An identity-enhanced text-image conditioning module (integrating LLaVA for multi-modal understanding and temporal concatenation to reinforce identity features). Identity-disentangled injection modules: AudioNet: Aligns audio features with video latents via spatial cross-attention for audio-driven generation.\nVideo injection module: Projects conditional video into the latent space and aligns features efficiently for video-driven editing.\nA rigorously constructed multi-modal dataset (filtered for quality, with subject segmentation and audio-video synchronization).\nExperimental results show OmniCustom outperforms state-of-the-art methods (e.g., Vidu, Pika, VACE) in identity consistency, realism, and text-video alignment. It also demonstrates robustness in downstream applications like virtual human advertising, virtual try-ons, and video editing."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strong multi-modal support: Unlike existing single-modality (image-driven) methods, it natively handles text, image, audio, and video inputs, enabling flexible customization (e.g., audio-driven speech animation, video-driven subject replacement).\n\nSuperior identity consistency: The identity-enhanced module and disentangled injection design preserve subject details (both human and non-human) across frames, outperforming baselines in metrics like Face-Sim and DINO-Sim.\n\nEfficient computation: The video injection module uses feature-alignment addition (instead of adapter-based conditioning or temporal concatenation) to avoid extra computational overhead during inference."}, "weaknesses": {"value": "1. No video demo is presented, the performance is hard to validate.\n\n2. Using rope to differentiate different identities sounds strange.\n\n3. The overall frameworks seems identical with existing works."}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YTf75OcHkB", "forum": "nvNSyX5uV6", "replyto": "nvNSyX5uV6", "signatures": ["ICLR.cc/2026/Conference/Submission11394/Reviewer_K8PH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11394/Reviewer_K8PH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760516884703, "cdate": 1760516884703, "tmdate": 1762922512817, "mdate": 1762922512817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "ImlplpJBlo", "forum": "nvNSyX5uV6", "replyto": "nvNSyX5uV6", "signatures": ["ICLR.cc/2026/Conference/Submission11394/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11394/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763105151310, "cdate": 1763105151310, "tmdate": 1763105151310, "mdate": 1763105151310, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose OmniCustom, a multi-modal driven video customization framework. The framework supports multi-modal conditions for generating customized videos, including images, audio, masks, video, and text. Specifically, the framework consists of an identity-enhanced textâ€“image condition module, an identity-disentangled AudioNet, and an identity-disentangled video injection module. The training datasets include Koala-36M and a private dataset. The base video generation model used is HunyuanVideo."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly structured and easy to follow.\n2. The input conditions supported are diverse and multi-modal.\n3. The proposed method appears straightforward and effective."}, "weaknesses": {"value": "1. No website or demo is provided in the supplementary material, making it difficult to assess the quality of the generated videos.\n2. The novelty of the method seems limited, as the framework appears to directly build upon the HunyuanVideo base model with several additional encoders/tokenizers.\n3. There are no quantitative comparisons with existing models for multi-condition video generation."}, "questions": {"value": "Could the authors provide the parameter counts and generation times for OmniCustom and each of the compared models? Without this information, it is difficult to assess the fairness of the comparisons. For example, comparing OmniCustom with VACE 1.3B is not equivalent to comparing it with VACE 14B, as the difference in model scale may significantly affect performance and efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "swkOmHABv5", "forum": "nvNSyX5uV6", "replyto": "nvNSyX5uV6", "signatures": ["ICLR.cc/2026/Conference/Submission11394/Reviewer_Yd2L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11394/Reviewer_Yd2L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760961190879, "cdate": 1760961190879, "tmdate": 1762922512351, "mdate": 1762922512351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniCustom, a framework that enables video customization using multi-modal inputs, including image, audio, mask, and video."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper explores a range of input modalities, demonstrating the capability to generate videos customized with various types of input data."}, "weaknesses": {"value": "- The claim of \"omni-modal\" support is somewhat overstated, as the different input modalities are handled separately rather than via a unified framework. Furthermore, previous works have already addressed similar types of inputs and tasks.\n- The introduced task closely resembles digital human video generation, which typically utilizes audio, facial images, and text prompts. The use of an identity-disentangled AudioNet for audio injection is similar to existing approaches, and audio is treated as optional. In addition, most tasks presented in the paper could be achieved through existing digital human video generation methods. For instance, multi-subject customization via continual tuning for image concept learning is a standard technique in the literature.\n- The evaluation is limited, with comparisons restricted to video foundation models. The paper should include comparisons with established methods in video personalization and talking face video generation to better situate its contributions."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YggyW2hxBe", "forum": "nvNSyX5uV6", "replyto": "nvNSyX5uV6", "signatures": ["ICLR.cc/2026/Conference/Submission11394/Reviewer_PTwo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11394/Reviewer_PTwo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934557185, "cdate": 1761934557185, "tmdate": 1762922511759, "mdate": 1762922511759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}