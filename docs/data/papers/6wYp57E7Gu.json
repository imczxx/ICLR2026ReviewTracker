{"id": "6wYp57E7Gu", "number": 15306, "cdate": 1758250105939, "mdate": 1759897314589, "content": {"title": "Asynchronous Decentralized SGD for Non-Convex Optimization via a Block-Coordinate Descent Lens", "abstract": "Decentralized optimization has become vital for leveraging distributed data without central control, enhancing scalability and privacy. However, practical deployments face fundamental challenges due to heterogeneous computation speeds, unpredictable communication delays, and diverse local data distributions. This paper introduces a refined model of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under practical assumptions of bounded computation and communication times. To analyze its convergence for non‑convex objectives, we first study Asynchronous Stochastic Block Coordinate Descent (ASBCD) as a theoretical tool, and employ a \\textit{double‑stepsize technique} to handle the interplay between stochasticity and asynchrony. This approach allows us to establish convergence of ADSGD under \\textit{computation‑delay‑independent} step sizes, without assuming bounded data heterogeneity. Empirical results show that ADSGD is practically robust even under extreme data heterogeneity and can be multiple times faster than existing methods in wall‑clock convergence. With its simplicity, efficiency in memory and communication, and resilience to delays, ADSGD is well‑suited for real‑world decentralized learning tasks.", "tldr": "", "keywords": ["Asynchronous Optimization", "Decentralized Optimization", "Block-Coordinate Descent", "SGD", "Non-Convex Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cb12b5a8b326e05046bf6442126ff770a082450.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies asynchronous decentralized optimization. The authors proposed a theoretical tool called Asynchronous Stochastic Block Coordinate Descent (ASBCD), and then developed the convergence analysis of a refined model of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD). Notably, they claimed that the convergence can be proven without any assumption on the bounded data heterogeneity. They also present numerical experiments for ADSGD on different datasets."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper investigates an important problem of asynchronous decentralized optimization, and studies the convergence rate of a fundamental algorithm called ADSGD. Notably, they claimed that the convergence analysis does not require any assumption on bounded data heterogeneity."}, "weaknesses": {"value": "The convergence rates developed in the paper are very slow! The authors assume that the number of steps between updates is bounded by $B$. Therefore, at the very least, the complexity of the method they proposed should be no worse than the synchronous version multiplied by $B$ (since one can always simulate the synchronous version between any $B$ steps of the asynchronous framework). \n\nHowever, the main theory in this paper (Theorem 3.9) shows that the optimization term of their method is $n$-times worse than the synchronous complexity multiplied by $B$. Regarding the synchronous complexity, one can for instance check the following gradient tracking paper: \nKoloskova, A., Lin, T., and Stich, S. U. An improved analysis of gradient tracking for decentralized machine learning. In Advances in Neural Information Processing Systems, 2021.\n\nJust to ensure I understand their theories correctly, I also looked at the proof in the Appendix. The derivations of many upper bounds are quite loose. I believe that the analysis should be significantly improved before this work is ready for publication."}, "questions": {"value": "The experiment plots also look a bit weird. Could you explain why in Figure 3(b) the ADSGD is even faster than SGD?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "o7AYdnyVcW", "forum": "6wYp57E7Gu", "replyto": "6wYp57E7Gu", "signatures": ["ICLR.cc/2026/Conference/Submission15306/Reviewer_GBmj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15306/Reviewer_GBmj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760640785018, "cdate": 1760640785018, "tmdate": 1762925604001, "mdate": 1762925604001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ADSGD, an Asynchronous Decentralized Stochastic Gradient Descent algorithm for non-convex optimization under bounded computation and communication delays. The main technical idea is to analyze ADSGD through a block-coordinate descent perspective. The authors develop a double-step-size technique that decouples the step size from delay-dependent Lipschitz constants, allowing delay-independent convergence guarantees. They prove convergence to stationary points for ADSGD under bounded computation and communication delays without assuming bounded data heterogeneity.\nThe analysis establishes an $O(K^{-1/3})$ convergence rate with step sizes independent of computation delays.\nEmpirically, ADSGD (logistic regression on MNIST; VGG11 on CIFAR-10) is robust to stragglers and shows faster wall-clock convergence than several baselines (ADPSGD, RFAST, DSGD, parallel SGD) across heterogeneous delay patterns and up to 128 agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The work extends asynchronous block-coordinate descent theory to the stochastic.\n\n- The analysis does not rely on bounded data heterogeneity assumption and decouples computation delays from step-size constraints.\n\n- The double-step-size technique is interesting.\n\n- Experiments cover heterogeneous data, different delay patterns, and scalability up to 128 agents. Results show performance gains.\n\n- The paper is generally quite well-written."}, "weaknesses": {"value": "1. The theoretical convergence rate is slower than that of standard SGD ($O(K^{-1/3})$ vs $O(K^{-1/2})$).\n\n2. Poor literature review\n\n    - The literature review is outdated. It includes only 1 citation from 2021, 3 from 2022, 4 from 2023, and none from 2024 or 2025. The past two years have seen a lot progress in asynchronous and/or decentralized optimization, yet these developments are entirely absent from the discussion. This omission weakens the positioning of the paper and gives the misleading impression that the proposed results are more novel than they actually are.\n\n    - line 57: The authors claim that \"most of the asynchronous algorithms make probabilistic assumptions regarding update patterns\". This statement is not very accurate and stems from a selective review of the literature. For instance, the recent works [1, 2, 3, 4] do not make any assumptions about update patterns, unlike this paper, which does impose such assumptions. These works should be acknowledged.\n\n    - line 70: The authors claim that the works on ABCD methods \"have not considered the stochastic gradient setting\". This is again incorrect. There exist studies, e.g., [5], which analyze stochastic asynchronous block-coordinate descent with variance reduction. The paper should cite and position itself relative to these earlier contributions.\n\n    - The authors of [6] (also not cited) study asynchronous decentralized SGD and obtain convergence guarantees matching those of standard SGD (up to a logarithmic factor), whereas the present work achieves a slower rate. The paper should explain this discrepancy and clarify what lead to the weaker theoretical guarantees. Similarly, [7] is another relevant and recent example that should be acknowledged.\n\n3. Missing or inconsistent notation, for example\n\n    - $\\mathcal{N}_i$ in line 127\n\n    - $i_k$ is used inconsistently: in eq. (4) it denotes the active block (scalar) but later the text uses notation $\\{i_k\\} = [n]$.\n\n4. [1] proposes nearly optimal decentralized stochastic asynchronous optimization methods. These should be included both in theoretical and empirical comparisons.\n\n[1] Tyurin A, Richtárik P. On the optimal time complexities in decentralized stochastic asynchronous optimization. Advances in Neural Information Processing Systems. 2024 Dec 16;37:122652-705.\n\n[2] Tyurin A, Gruntkowska K, Richtárik P. Freya page: First optimal time complexity for large-scale nonconvex finite-sum optimization with heterogeneous asynchronous computations. Advances in Neural Information Processing Systems. 2024 Dec 16;37:54239-87.\n\n[3] Tyurin A, Richtárik P. Optimal time complexities of parallel stochastic optimization methods under a fixed computation model. Advances in Neural Information Processing Systems. 2023 Dec 15;36:16515-77.\n\n[4] Maranjyan A, Tyurin A, Richtárik P. Ringmaster ASGD: The first Asynchronous SGD with optimal time complexity. arXiv preprint arXiv:2501.16168. 2025 Jan 27.\n\n[5] Gu B, Huo Z, Huang H. Asynchronous stochastic block coordinate descent with variance reduction. arXiv preprint arXiv:1610.09447. 2016 Oct 29.\n\n[6] Attiya H, Schiller N. Asynchronous fully-decentralized SGD in the cluster-based model. Theoretical Computer Science. 2025 Mar 21;1031:115073.\n\n[7] Nadiradze G, Sabour A, Davies P, Li S, Alistarh D. Asynchronous decentralized sgd with quantized and local updates. Advances in Neural Information Processing Systems. 2021 Dec 6;34:6829-42."}, "questions": {"value": "1. How does your method and analysis compare to [6, 7]? Why do you achieve worse convergence guarantees?\n\n2. Could the method benefit from variance reduction?\n\n3. line 98: \"ADSGD reduces per-iteration communication overhead by 50\\% and memory usage by 70\\%\". Could you clarify the reference point?\n\n4. Could you respond to the issues raised in previous sections?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DbECvsZ3pf", "forum": "6wYp57E7Gu", "replyto": "6wYp57E7Gu", "signatures": ["ICLR.cc/2026/Conference/Submission15306/Reviewer_zpCu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15306/Reviewer_zpCu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760967085778, "cdate": 1760967085778, "tmdate": 1762925602992, "mdate": 1762925602992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical analysis of asynchronous decentralized SGD (ADSGD) from a block-coordinate descent perspective. Convergence properties are examined via a double-step-size technique, yielding guarantees that do not rely on bounded data heterogeneity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a unified theoretical perspective linking asynchronous decentralized learning and block coordinate descent.\nThe experimental evaluation covers data heterogeneity and scalability under different cases."}, "weaknesses": {"value": "The paper lacks the inclusion of recent references.\nThe paper does not provide a scalability comparison with other methods."}, "questions": {"value": "The paper analyzes asynchronous decentralized SGD (ADSGD) from the perspective of block-coordinate descent, introducing a double-step-size technique to establish convergence guarantees under bounded delays.There are several questions as follows.\n1.The motivation for the key theoretical connection between ADSGD and ASBCD is unclear.The construction of the augmented function $L_{\\alpha}(x)$ seems to be presented as an a posteriori justification rather than a naturally motivated insight. Please explain the conceptual pathway that led to identifying this connection.\n\n2.The description of Algorithm 2 (ADSGD) is confusing regarding the timing of communication and computation. Line 8 suggests that each node keeps receiving models until its gradient is ready, which seems to imply model reception happens before gradient computation. However, the text later claims that ADSGD uses the global model after gradient estimation. Please clarify whether communication and computation occur sequentially or in parallel in Algorithm 2, as the current phrasing (“until $g^{f_i}(x_i)$ is available”) makes this unclear.\n\n3.Figure 4 only shows that ADSGD's performance improves with more agents, but it lacks a critical comparison with baseline methods (e.g., DSGD, ADPSGD) under the same scaling conditions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mnFBOK6u8a", "forum": "6wYp57E7Gu", "replyto": "6wYp57E7Gu", "signatures": ["ICLR.cc/2026/Conference/Submission15306/Reviewer_Lzeh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15306/Reviewer_Lzeh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761573070946, "cdate": 1761573070946, "tmdate": 1762925602466, "mdate": 1762925602466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an asynchronous decentralized SGD method. The authors analyze their approach for non-convex smooth functions under a bounded delay assumption.\nThe main novelty is that they view their method as a special case of block-coordinate descent and perform the analysis within that framework.\nThey also show that their method outperforms the baseline approaches that they consider."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of the paper is the analysis from the perspective of block-coordinate descent. I am not sure whether this viewpoint has been explored before (the authors should clarify this in the paper), but if not, it represents an interesting and novel way of analyzing asynchronous decentralized SGD. However, it is not entirely clear whether this perspective provides any concrete advantages over previous approaches (see Weakness 1)."}, "weaknesses": {"value": "1. **Unclear theoretical comparison to prior work**\nIt is not clear whether the proposed method achieves a better theoretical iteration complexity than previous methods, particularly those in [1, 2].\nEven more concerning, in [1, Corollary 1] the iteration complexity is $\\mathcal{O}\\left(\\tfrac{1}{K} + \\tfrac{\\sigma}{\\sqrt{K}}\\right)$, which is better than the $\\mathcal{O}\\left(\\tfrac{1}{K^{1/3}} + \\tfrac{\\sigma}{K^{1/3}}\\right)$ rate reported in Corollary 3.12 of this paper.\nAre the gains mainly in terms of memory or communication efficiency, as suggested in Table 2? The reported improvement (around a 4× reduction) appears relatively minor, and if this comes at the cost of requiring more iterations, the overall benefit is questionable.\n\n2. **Delay-independent stepsize** \nThe authors claim that one advantage of their method is that the stepsize does not depend on the delay. However, the paper does not explain why this is beneficial; in fact, this may be a disadvantage.\nIn [3, 4], delay-adaptive stepsizes lead to better iteration complexity and improved dependence on delay.\nSimilarly, the optimal asynchronous methods in the centralized setting [5, 6] explicitly use the delay bound in the stepsize.\nIntuitively, scaling the stepsize with the delay makes sense, since in the stochastic setting the delay bound acts similarly to an effective batch size [5].\n\n3. **Dependence on maximum delay**\nThe convergence rate depends on the maximum delay (and even its square), which is undesirable.\nIf a single iteration experiences a large delay while others are small, this should not dominate the overall complexity.\nRelated works in the centralized setting [3, 4] have addressed this issue using delay-adaptive stepsizes, which lead to improved dependence on the average delay rather than the worst-case delay.\n\n4. **No time complexity analysis**\nThe paper analyzes only iteration complexity, which is insufficient for evaluating efficiency in asynchronous optimization.\nIteration complexity alone does not reveal whether the method is actually faster than synchronous counterparts.\nIn fact, asynchronous methods often have worse iteration complexity, and to demonstrate their advantage, one must compare them in terms of time complexity instead.\nIt is also not guaranteed that asynchronous methods achieve better time complexity—sometimes they match the synchronous ones, as shown in Table 1 of [6].\nIn such cases, these asynchronous methods are not desirable, since they achieve the same time complexity while performing more computations.\nTime-complexity analyses for asynchronous methods can be found in [5, 6, 7] for the centralized case and in [8] for the decentralized setting.\n\n5. **Missing comparison to state-of-the-art** \nThere exists an optimal method in the same non-convex smooth decentralized regime, Amelie SGD [8], which achieves optimal time complexity guarantees.\nThe paper does not include any theoretical or empirical comparison with this method.\n\nI understand that the authors may be unaware of the recent line of work on time-complexity analysis, so it might be acceptable to overlook Weaknesses 4 and 5 to some extent.\nHowever, they should at least cite this line of research and provide a numerical comparison with the state-of-the-art method, since a thorough theoretical comparison may take additional effort.\n\n---\n\n[1] Zehan Zhu, Ye Tian, Yan Huang, Jinming Xu, and Shibo He. \"Robust fully-asynchronous methods for distributed training over general architecture\". arXiv preprint arXiv:2307.11617, 2023.\n\n[2] Vyacheslav Kungurtsev, Mahdi Morafah, Tara Javidi, and Gesualdo Scutari. \"Decentralized asynchronous non-convex stochastic optimization on directed graphs\". IEEE Transactions on Control of Network Systems, 2023.\n\n[3] Anastasiia Koloskova, Sebastian U Stich, and Martin Jaggi. \"Sharper convergence guarantees for asynchronous SGD for distributed and federated learning\". Advances in Neural Information Processing Systems, 35:17202–17215, 2022.\n\n[4] Konstantin Mishchenko, Francis Bach, Mathieu Even, and Blake E Woodworth. \"Asynchronous SGD beats minibatch SGD under arbitrary delays\". Advances in Neural Information Processing Systems, 35:420–433, 2022.\n\n[5] Artavazd Maranjyan, Alexander Tyurin, and Peter Richt´arik. \"Ringmaster ASGD: The first Asynchronous SGD with optimal time complexity\". In the International Conference on Machine Learning, 2025\n\n[6] Artavazd Maranjyan and Peter Richt´arik. \"Ringleader ASGD: The first Asynchronous SGD with optimal time complexity under data heterogeneity\". arXiv preprint arXiv:2509.22860, 2025\n\n[7] Alexander Tyurin and Peter Richt ´arik. \"Optimal time complexities of parallel stochastic optimization methods under a fixed computation model. Advances in Neural Information Processing Systems, 36, 2024.\n\n[8] Alexander Tyurin and Peter Richtárik. \"On the optimal time complexities in decentralized stochastic asynchronous optimization\". Advances in Neural Information Processing Systems 37 (2024): 122652-122705."}, "questions": {"value": "1. Can you provide a proper comparison with previous methods in terms of iteration complexity? Is your method theoretically better in that regard?\n2. Other than memory and communication reduction, what are the main theoretical benefits of your approach compared to existing methods?\n3. In Corollary C.4, the convergence rate appears worse, at $\\mathcal{O}(1/\\sqrt{K})$. Could the stepsize $\\gamma$ be chosen to achieve a faster $\\mathcal{O}(1/K)$ rate instead?\n4. Could you include a comparison with Amelie SGD [8], at least in the experimental section, to better position your method relative to the state of the art?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KBuvyMD3m1", "forum": "6wYp57E7Gu", "replyto": "6wYp57E7Gu", "signatures": ["ICLR.cc/2026/Conference/Submission15306/Reviewer_EcFE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15306/Reviewer_EcFE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838976405, "cdate": 1761838976405, "tmdate": 1762925602024, "mdate": 1762925602024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}