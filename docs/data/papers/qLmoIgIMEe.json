{"id": "qLmoIgIMEe", "number": 20484, "cdate": 1758306676048, "mdate": 1759896975341, "content": {"title": "Data, Not Model: Explaining Bias toward LLM Texts in Neural Retrievers", "abstract": "Recent studies show that neural retrievers often display source bias, favoring passages generated by LLMs over human-written ones, even when both are semantically similar. This bias has been considered an inherent flaw of retrievers, raising concerns about the fairness and reliability of modern information access systems. Our work challenges this view by showing that source bias stems from supervision in retrieval datasets rather than the models themselves. We found that non-semantic differences, like fluency and term specificity, exist between positive and negative documents, mirroring differences between LLM and human texts. In the embedding space, the bias direction from negatives to positives aligns with the direction from human-written to LLM-generated texts. We theoretically show that retrievers inevitably absorb the artifact imbalances in the training data during contrastive learning, which leads to their preferences over LLM texts. To mitigate the effect, we propose two approaches: 1) reducing artifact differences in training data and 2) adjusting LLM text vectors by removing their projection on the bias vector. Both methods substantially reduce source bias. We hope our study alleviates some concerns regarding LLM-generated texts in information access systems.", "tldr": "", "keywords": ["source bias", "dense retrieval", "contrastive learning", "representation learning", "large language models (LLMs)"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2e8391c46b215f04fdd3cfcfd5c705c78be06930.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the phenomenon of source bias, where neural retrievers preferentially rank text generated by LLMs over semantically similar human-written text. The authors challenge the prevailing view that this bias is an inherent flaw of retriever models. Instead, they posit that the bias originates from systematic, non-semantic artifacts within the supervised training data. They show that this bias is prominent only in models trained with relevance supervision, as the stylistic properties of positive documents mirror those of LLM-generated text, creating a learnable shortcut. This mechanism is supported by linguistic analysis, embedding-space geometry, and a formal theoretical framework. The paper validates this hypothesis by proposing two successful mitigation strategies: controlling artifact imbalance during training via negative sampling and removing the bias direction from embeddings at inference time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Strong contribution**: The central argument that source bias is learned from data artifacts is a compelling and important shift from previous explanations focused on model properties.\n* **Comprehensive evidentiary support**: The authors defend their thesis with a powerful combination of controlled experiments.\n* **Good structure and clarity**: The organization around three distinct research questions makes the paper clear and the narrative easy to follow.\n* **Practical mitigation strategies**: The proposed training- and inference-time techniques are not only practical solutions but also serve as an additional validation of the paper's core hypothesis."}, "weaknesses": {"value": "* **Unsupported assertions**: The paper makes several strong claims without sufficient evidence or citations. For instance, it asserts that Llama-3-8B provides a \"reliable proxy for human-perceived fluency\" and that the presence of false negatives in retrieval datasets is \"mostly irrelevant in practice\", yet no supporting references are provided.\nImpact of the relevance assessment method: Annotations highly dependent on how the candidate passages were retrieved (BM25 for MS MARCO vs. encoder-based for more recent datasets), more details should be given on that.\n* **Lack of systematic statistical significance assessment**: Some values in Table 1 are close to 0, but no statistical significance testing is reported.\n* **Limited analysis of linguistic features**: In Figure 1, while perplexity and IDF are informative, the examination of stylistic artifacts could be strengthened by incorporating additional metrics, such as passage length (in tokens or characters) or lexical diversity.\n* **Absence of non-English data**: The study would benefit from analyzing at least one language other than English to explore cross-linguistic lexical features.\n* **Typo**: Figure 3 is referenced in lines 319–320, but it does not appear in the paper (possibly referring to Figure 4)."}, "questions": {"value": "* **Unsupported assertions**: Can the authors provide empirical evidence or references to support the claims that Llama-3-8B serves as a \"reliable proxy for human-perceived fluency\" and that false negatives in retrieval datasets are \"mostly irrelevant in practice\"?\n* **Impact of relevance assessment method**: Annotating retrieval datasets typically involves using a pre-retrieval step to collect candidate documents. How does the choice of this retrieval method (e.g., BM25 vs. encoder-based retrieval) influence the resulting relevance annotations and reported performance? It would be helpful to provide more details on this dependency.\n* **Statistical significance assessment**: Were any statistical significance tests conducted for the values reported in Table 1, especially those close to 0? If not, can the authors clarify whether these differences are meaningful?\n* **Linguistic features**: Could the analysis of stylistic artifacts be extended to include additional linguistic metrics, such as passage length (in tokens or characters) or lexical diversity, to provide a more comprehensive view?\n* **Non-English data**: Would the authors consider including non-English data to assess whether the reported findings generalize across languages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NzBX0LJjQz", "forum": "qLmoIgIMEe", "replyto": "qLmoIgIMEe", "signatures": ["ICLR.cc/2026/Conference/Submission20484/Reviewer_HiGy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20484/Reviewer_HiGy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761648346945, "cdate": 1761648346945, "tmdate": 1762933921158, "mdate": 1762933921158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper evaluates the setup of whether neural retrievers prefer LLM-text over human, presenting source bias that stems from supervised training datasets (MS MARCO). They evaluate dense retrieval models on the Cocktail benchmark and further suggest two techniques (training-time and inference-time) to reduce the source bias."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper works on an interesting issue of the source bias towards LLM-generated texts over human texts, and ties the problem with training datasets instead of the model. The problem (source bias in retrieval models) has been studied previously.\n- The paper evaluates multiple dense retrieval models, spanning across various families.\n- The paper is well-written and overall easy to follow through."}, "weaknesses": {"value": "Here I've listed below the weaknesses associated with the paper:\n\n### **W1: Limited novelty in the work**\n\nI find limited novelty in this work; the experiments have been conducted with existing dense retrieval models, evaluating them on an already published Cocktail benchmark (Dai et al. 2024), plus further studies in (Dai et al. 2024c) already reveal a source bias of neural retrieval models. This paper only extends the artifact seen towards \"data\" and not models, as the title suggests.\n\n### **W2: The evaluation setup on Cocktail may not accurately reflect source bias towards LLM-generated texts.**\n\nThe evaluation results are solely based on the Cocktail benchmark and are not convincing. Here are three reasons why:\n\nThe cocktail benchmark (Dai et al. 2024) rewrites human-written passages from the BEIR benchmark (Thakur et al. 2021) using the Llama-2-7B model. The LLM-rewritten model is very similar to the original passage. However, there are two things which rewriting passage does: (a) removes grammatical mistakes, (b) removes unused words and bad punctuation present in text (e.g., [1], [2], present in Natural Questions), and most importantly (c) reduces the human passage content length on average (confirmed the lengths from Table 8).\n\nFirst, I suspect the preference comes from (c) and (a) of the neural retriever model potentially retrieving shorter texts as a bias seen in the following publications [1] [2], and since LLM-generated passages are shorter on average, they seem to prefer these over the human-generated passages. Second, the LLM-generated passage contains the same content without mistakes; therefore, even as a human assessor, I might prefer the LLM-generated passage over the human one.\n\nSo I have two suggestions: (a) what I would like to see, if rewriting the original human generated passage by a much longer rewritten by the LLM document, and repeat the experiments. (b) conduct a human validation study to assess whether they prefer the human or the LLM-generated document for a given query.\n\n- [1] Systematic Evaluation of Neural Retrieval Models on the Touché 2020 Argument Retrieval Subset of BEIR. Thakur et al. SIGIR 2024. https://dl.acm.org/doi/10.1145/3626772.3657861\n- [2] Collapse of Dense Retrievers: Short, Early, and Literal Biases Outranking Factual Evidence. Fayyaz et al. ACL 2024. https://aclanthology.org/2025.acl-long.447/\n\n### **W3: The source bias mitigation suggestions are not realistic**\n\nThe paper suggests two source bias mitigation techniques. \n\n- First, remove hard negatives during training: this is impractical and not used at all in practice. Mining hard negatives carefully and adding them during training is crucial for training dense retrieval models and achieving a rather high retrieval performance in contrast to training with in-batch negatives only.\n\n>\"We estimate n by averaging displacement vectors from 1000 randomly sampled human–LLM passage pairs per dataset\". \n- Now, at inference time for a realistic use case, when knowledge about the human-LLM passage pairs won't be available (assuming the corpus would be a mix of both human and LLM-generated and not artificially generated like in cocktail), the estimation of \"n\" won't be possible, and therefore not realistic."}, "questions": {"value": "One question I had was Where are you getting the hard negatives for the MS MARCO training? How many hard negatives do you consider during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b9O2eSFiCW", "forum": "qLmoIgIMEe", "replyto": "qLmoIgIMEe", "signatures": ["ICLR.cc/2026/Conference/Submission20484/Reviewer_LRJy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20484/Reviewer_LRJy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837746737, "cdate": 1761837746737, "tmdate": 1762933920849, "mdate": 1762933920849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why neural retrievers tend to favor LLM-generated passages over human-written ones. Through linguistic, embedding-space, and theoretical analyses, it concludes that source bias arises not from model architecture, but from artifact imbalances in supervised contrastive training data. This paper further proposes two effective mitigation strategies: training-time negative sampling control and inference-time embedding projection, both of which reduce bias without sacrificing retrieval performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Understanding the cause of source bias of neural retrievers in the LLM era is a timely and important research problem.  \n2. Through theoretical analysis and empirical analysis, this paper proposes a new and reasonable explanation for the root cause of source bias.\n3. This paper is overall well-written and easy to read."}, "weaknesses": {"value": "1. The current title “Data, not model, explains source bias” may be misleading, since the experiments show that the bias mainly stems from contrastive relevance-based supervision training, not merely “data” per se. The authors should unify terminology (e.g., “training supervision” vs. “data”) to avoid ambiguity.\n\n2. The PPL and IDF analyses (Figure 1) are weakly significant, and the observed effects are small.\n\n3. The paper lacks comparison with prior debiasing methods for source bias.\n\n4. Minor issue: Figure 3a and 3b mentioned in Line 319 actually correspond to Figure 4a and 4b."}, "questions": {"value": "1. Can the authors clarify whether the main causal factor is data composition, training objective, or negative sampling policy? How should we interpret “data not model” in this context?\n\n2. Did the authors test other stylistic features (e.g., syntactic complexity) beyond PPL and IDF to verify robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ViKVHOHqbM", "forum": "qLmoIgIMEe", "replyto": "qLmoIgIMEe", "signatures": ["ICLR.cc/2026/Conference/Submission20484/Reviewer_kz3D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20484/Reviewer_kz3D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903979264, "cdate": 1761903979264, "tmdate": 1762933920417, "mdate": 1762933920417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates to what degree neural embedding models are biased towards LLM generated text. To establish baselines, they evaluate various models against the Cocktail version of BEIR which contains human and LLM-generated chunks with semantically similar content. Many baselines are biased towards LLM-generated text. The authors indicate this bias could be influenced by changing how negatives are sampled during supervised embedding training.\n\nThere's other key results in the paper, including the following:\n- A de-biasing technique (sampling negatives only in-batch) is effective.\n- Training only on hard negatives slightly increases the bias towards LLM-generated text.\n- The de-biasing technique has a negligible impact on model quality.\n\nI would describe the main weaknesseses in this work as:\n- Overly reliant on MS MARCO, particularly for analysis and as a finetuning recipe. Although one can achieve decent BEIR results using MS MARCO alone, the dataset has particular quirks and it is more common to use extensive data beyond MS MARCO (see GECKO for an example training recipe https://arxiv.org/abs/2403.20327?). E5 is one paper that does go substantially beyond MS MARCO, and it is one embedding that is less based in the baselines table 1.\n- Not enough discussion about the different negative selection techniques used in various datasets and models. This seems essential given the debiasing technique is so focused on negative selection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper discusses an incredibly important question: How is the advent of AI-generated text impacting technology, namely embedding-based retrieval. Recent articles have shown that AI-generated text has grown rapidly, even in the largest newspaper publications (see Russell et al https://arxiv.org/abs/2510.18774). It's safe to say that the distribution of text on the web has changed in a major and unprecedented way, and the research community should spend more of its focus understanding the properties of this new world.\n\nS2. The paper presents analysis identifying that embedding models are biased towards LLM-generated text (via the existing Cocktail benchmark), and presents a simple method to debias based on negative selection (Table 3). And another method based on embedding projections (Tables 4 and 5).\n\nS3. There is discussion, analysis, and theoretical framework focused on the cause of bias in embeddings, centering around negatives as being a source of artifacts during embedding training."}, "weaknesses": {"value": "W1. The paper is very reliant on both MS MARCO for finetuning and for analysis of negatives. Yet, there is no discussion on how the negatives are selected for MS MARCO --- the negatives in MS MARCO are widely recognized to be highly limited in their semantic richness (see sec 2.2.2 in https://arxiv.org/abs/2405.17093). It would greatly strengthen the paper to (1) explain the negative selection method of MS MARCO and perhaps also how the different baseline models selected negatives during training, and (2) introduce a new finetuning baseline based on a known technique for negative selection different from what is done in MS MARCO. These are simply suggestions for how to help the reader understand whether the paper is making claims about embedding models in general, or simply the idiosyncrasies of MS MARCO.\n\nW2. The mitigation of only relying on in-batch positives as negatives is potentially too extreme. A simple alternative would be to retrieve negatives among positives as hard negatives. A more involved alternative might integrate AI detection techniques (see https://arxiv.org/abs/2509.19163).\n\nW3. Not enough focus on recent embeddings models. There has been a major shift in the last 1-2 years in how much data is used during training of embedding models, in particular how much synthetic LLM-generated data is used (usually for question generation rather than documents). It's plausible the findings in this paper will not hold for newer embedding models."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8CDOEt3gJp", "forum": "qLmoIgIMEe", "replyto": "qLmoIgIMEe", "signatures": ["ICLR.cc/2026/Conference/Submission20484/Reviewer_9jEq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20484/Reviewer_9jEq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044922320, "cdate": 1762044922320, "tmdate": 1762933920073, "mdate": 1762933920073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}