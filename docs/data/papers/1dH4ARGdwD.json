{"id": "1dH4ARGdwD", "number": 23612, "cdate": 1758346333880, "mdate": 1759896804681, "content": {"title": "Scaling up Memory for Robotic Control via Experience Retrieval", "abstract": "Humans rely on memory to perform tasks; our goal is to endow robot policies with the same ability. Naively conditioning on long observation histories is computationally expensive and brittle under covariate shift, while indiscriminate subsampling of history leads to irrelevant or redundant information. We propose a hierarchical policy framework, where the high-level policy is trained to select and track previous task-relevant keyframes from its experience. The high-level policy uses selected keyframes and the most recent frames when generating text instructions for a low-level policy to execute. This design is compatible with existing vision-language-action (VLA) models and enables the system to efficiently reason over long-horizon dependencies. In our experiments, we fine-tune Qwen2.5-VL-3B-Instruct and $\\pi_{0.5}$ as the high-level and low-level policies respectively, using demonstrations supplemented with minimal language annotations. Our approach, MemER, outperforms prior methods on three real-world long-horizon robotic manipulation tasks that require minutes of memory. Videos and code can be found at https://memer-policy.github.io.", "tldr": "We enable existing vision-language-action models (VLAs) to solve long-horizon tasks that require minutes of memory by finetuning a VLM to act like a high-level planner and select task-relevant keyframes.", "keywords": ["Robot Learning", "Memory", "Vision-Language-Action Models"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63a0f07acb1f7e04f904a57c74ae8ba6b8b59a51.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work addresses the problem that robotic policies often rely only on the current observation or a few recent frames and thus lack long-term memory. It proposes a hierarchical policy framework: the high-level policy selects keyframes or memories from past experiences and generates language-based subtasks or instructions based on both the current observation and the retrieved keyframes; the low-level policy then receives the current image, the robot’s current state, and the subtask produced by the high-level policy to execute the concrete actions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The system design is simple, effective, and scalable.\n\n2. The results show a significant improvement in long-horizon task success rates.\n\n3. The writing is clear and easy to follow."}, "weaknesses": {"value": "1. The evaluation of scalability aspects such as memory size and retrieval latency could be more detailed.\n\n2. Providing a conceptual comparison among different types of approaches addressing the long-horizon problem would offer readers deeper insights."}, "questions": {"value": "1. How does the high-level policy determine which frames are keyframes? When the task types or scenes vary significantly, is this keyframe selection mechanism still generalizable, or does it require task-specific tuning?\n\n2. The paper includes baselines such as “Short History (8 frames)” and “Long History (32 frames).” I’m curious why the long-history setup leads to such a large improvement. Moreover, since simply adding more historical frames performs worse than MemER, why does “more history” not yield the same gains as the keyframe retrieval mechanism?\n\n3. After the high-level module generates subtasks and keyframes, isn’t the way the low-level policy directly uses this information somewhat too simple?\n\n4. Although the paper proposes using memory to tackle long-horizon problems, it does not compare MemER conceptually with other VLA-based or related long-horizon approaches [1, 2, 3]. Even if experimental comparison is difficult, a conceptual discussion would provide readers with deeper insights.\n\n5. The paper mentions that keyframes are accumulated into memory but currently lacks a mechanism for “deletion” when the memory becomes too large. Are there any promising methods to address this issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dGMV3mp75D", "forum": "1dH4ARGdwD", "replyto": "1dH4ARGdwD", "signatures": ["ICLR.cc/2026/Conference/Submission23612/Reviewer_uYHS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23612/Reviewer_uYHS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657478477, "cdate": 1761657478477, "tmdate": 1762942735218, "mdate": 1762942735218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MemER, a hierarchical architecture for Vision-Language-Action (VLA) modeling. The low-level policy is a general VLA model, while the high-level policy employs a Vision-Language Model (VLM) to predict primitives and identify key frames. The core contribution is the proposed experience retrieval mechanism, which operates at the high-level policy, enabling the VLM to leverage critical historical information for more accurate primitive prediction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to follow, and the figures and tables are clear and easy to understand.\n- There is a strong motivation for this work. Memory is essential for real-world, long-horizon tasks, a need that is often overlooked in existing VLA literature.\n- The approach is simple and effective. The low-level VLA model requires minimal or even no training, with the bulk of the effort focused on fine-tuning the high-level VLM. This results in a low overall training cost.\n- The experimental validation provided is thorough and comprehensive."}, "weaknesses": {"value": "- The architecture of MemER appears overly simple. The core advancements appear to lie primarily in the training methodology and data preparation for the high-level VLM, and these specific improvements do not seem to be particularly novel. This raises concerns about the overall innovation of the paper.\n\n- The data preparation phase seems to be highly resource-intensive, relying on manual annotation of primitives and the segmentation of trajectories."}, "questions": {"value": "- Could the authors more clearly articulate the specific innovative contributions of this paper? Given that the architecture is simple and the high-level training methods appear familiar, a clearer distinction from existing work is necessary to establish the technical merit.\n\n- The authors discuss several related VLA models that utilize memory in the Related Works section. Why are these models not included as baselines for comparison in the experimental evaluation? Including these relevant memory-based approaches would provide a more rigorous validation of MemER's claimed state-of-the-art performance in memory-intensive tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YUu25l043m", "forum": "1dH4ARGdwD", "replyto": "1dH4ARGdwD", "signatures": ["ICLR.cc/2026/Conference/Submission23612/Reviewer_JibS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23612/Reviewer_JibS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806903921, "cdate": 1761806903921, "tmdate": 1762942733941, "mdate": 1762942733941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- A hierarchical VLA (Vision-Language-Action) framework where the high-level policy retrieves and tracks keyframes from past experience.\n\n- Efficient memory management via online keyframe selection and filtering, reducing redundancy and computational cost.\n\n- Real-world evaluation on three long-horizon tasks requiring minutes of memory: Object Search, Counting Scoops, and Dust & Replace."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Successfully tackles real-world robotic tasks that require reasoning over several minutes of past experience (hundreds of frames), a significant step beyond prior work limited to a few dozen frames.\n- The hierarchical design with intelligent keyframe selection avoids the high cost of processing long, raw video sequences, enabling low-latency inference (~1 Hz for high-level policy) suitable for closed-loop control."}, "weaknesses": {"value": "- The framework was evaluated on a single robot arm and on memory within a single task. Its scalability to mobile manipulation, multi-room navigation, and cross-task memory recall remains unexplored.\n- The approach is inherently limited to the specific task it was fine-tuned on and lacks the capacity for broader scaling."}, "questions": {"value": "- How does the keyframe extraction algorithm proposed in this work compare to frequency domain-based clustering methods (such as Fourier transform or wavelet transform followed by clustering), for example, with *UniDomain*?\n- As tasks become longer, does the computational overhead of maintaining an explicit visual memory buffer adversely impact the performance of the high-level policy by reducing its inference frequency?\n- Does the approach presented in this paper offer significant advantages compared to existing long-context strategies, such as introducing sink tokens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dGV8StheRj", "forum": "1dH4ARGdwD", "replyto": "1dH4ARGdwD", "signatures": ["ICLR.cc/2026/Conference/Submission23612/Reviewer_bnmy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23612/Reviewer_bnmy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996156437, "cdate": 1761996156437, "tmdate": 1762942733577, "mdate": 1762942733577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MemER, a hierarchical framework that endows VLAs with long-term memory via experience retrieval. A high-level VLM policy processes recent frames and retrieved keyframes to generate language primitives and nominate new keyframes for storage. A low-level VLA policy executes these primitives based on the current frame. This sparse retrieval mechanism allows the robot to successfully perform complex, multi-minute tasks that depend on recalling distant past events."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The hierarchical design, where a high-level policy learns to explicitly nominate salient keyframes for retrieval, is a novel and highly scalable architecture for managing truly long-horizon (multi-minute) dependencies.\n- The experiments are well-designed for long-horizon tasks, and the ablation comparing visual memory ($K_{img}$) to textual memory ($K_{text}$) provides a crucial insight into the limitations of using language as a lossy memory representation."}, "weaknesses": {"value": "- The method's generalizability is unproven, as it was only evaluated on three custom, in-domain tasks and lacks benchmarks on standard, multi-task datasets like RoboCasa or LIBERO.\n- The framework introduces significant system complexity and data annotation overhead, as it requires labeling both language primitives and ground-truth keyframes for training the high-level policy.\n- The paper provides no computational analysis, making it impossible to assess the inference latency or memory cost of this dual-policy system, which is a critical factor for real-world deployment.\n- The comparison to GPT-5 was only conducted offline (Table 2), and it's unclear if this setup accurately reflects the challenges of closed-loop control or simply highlights the need for finetuning."}, "questions": {"value": "- The most important concern is that the dual-policy architecture is computationally heavy and overly complex, requiring two separate, large models (a VLA and a VLM) to be run in parallel. The paper fails to justify this separation or explore a more efficient, unified architecture where a single VLM backbone could perform both high-level planning (keyframe nomination, primitive generation) and low-level feature extraction, which is a significant missed opportunity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bRchgY7BuV", "forum": "1dH4ARGdwD", "replyto": "1dH4ARGdwD", "signatures": ["ICLR.cc/2026/Conference/Submission23612/Reviewer_eE1T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23612/Reviewer_eE1T"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762085548875, "cdate": 1762085548875, "tmdate": 1762942733317, "mdate": 1762942733317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}