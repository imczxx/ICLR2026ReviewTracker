{"id": "qhLjdFHdXm", "number": 8296, "cdate": 1758077804788, "mdate": 1759897793723, "content": {"title": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference", "abstract": "Large Language Models (LLMs) demonstrate exceptional capabilities across various tasks, but their deployment is constrained by high computational and memory costs. Model pruning provides an effective means to alleviate these demands. However, existing methods often ignore the characteristics of prefill-decode (PD) disaggregation in practice. In this paper, we propose a novel pruning method for PD disaggregation inference, enabling more precise and efficient block and KV Cache pruning. Our approach constructs pruning and distillation sets to perform iterative block removal independently for the prefill and decode stages, obtaining better pruning solutions. Moreover, we introduce a cache pruning mechanism that selectively reuses entries corresponding to the first and last token sequences within designated layers, reducing communication costs while incurring only negligible computational overhead. Extensive experiments demonstrate that our approach consistently achieves strong performance in both PD disaggregation and PD unified settings without disaggregation. Under the same (default) settings, our method achieves improved performance and faster inference, along with a 4.95$\\times$ reduction in data transmission bandwidth consumption.", "tldr": "We propose a method that integrated with PD disaggregation.", "keywords": ["large language models", "disaggregation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a92d9eac1131d56f485cce072fda18cc195265eb.pdf", "supplementary_material": "/attachment/f00245f27b7e5d3f5707ad81640bdb4ba70a0dc2.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a pruning method tailored for prefill-decode (PD) disaggregated inference in large language models. It prunes model blocks and KV caches separately for prefill and decode stages, improving efficiency and accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Significance\n1. The paper conducts extensive experiments demonstrating that the proposed method can outperform state-of-the-art approaches."}, "weaknesses": {"value": "Originality\n\n1. The proposed approach is not methodologically novel. Iterative block pruning, cosine similarity scoring, and heuristic search strategies have already been explored in prior works.\n2. The paper fails to clearly justify why PD disaggregation requires a special pruning framework. Similar methods can be applied to both unified and disaggregated inference without modification.\n3. The KV cache pruning mechanism is largely heuristic and similar to the Sink method, offering limited conceptual advancement.\n\nQuality\n\n4. The method is highly heuristic, relying on many stochastic thresholds and hyperparameters, making results sensitive to tuning and hard to reproduce.\n5. The claim of global optimal block pruning is not supported. The paper merely proposes an approximate heuristic search method that lacks theoretical justification.\n6. The theoretical part is weak and lacks rigor, with unclear use of Lipschitz assumptions and non-standard theorem formatting. It appears poorly reasoned and seems to have been generated with the assistance of an LLM.\n7. Important KV cache pruning baselines such as Sink and H2O are missing, making the comparison incomplete and weakening the motivation for the proposed approach [1,2].\n8. The proposed method relies on distillation while several baselines do not, and important block pruning methods such as Wanda-SP are also omitted.\n9. The pruning ratios appear carefully tuned, suggesting possible cherry-picking that may bias results.\n\nClarity\n\n10. Key concepts are unclear or confusing, such as what is meant by the “first and last token sequences” and how they apply in the prefill stage.\n11. The paper does not describe how the overhead of KV cache pruning is measured or how attention heads with low scores are handled. The section on KV cache pruning is unclear and lacks sufficient explanation of key design choices.\n\nSignificance \n\n12. Despite extensive experimentation, the paper provides limited methodological insight and the gains are incremental and largely empirical.\n13. Reported latency improvements are negligible in practice, and the results do not convincingly demonstrate that PD-specific pruning is necessary.\n14. While the paper presents extensive experiments, it prioritizes quantity over conceptual clarity, offering very limited methodological insight beyond empirical validation.\n\n\n[1] Efficient Streaming Language Models with Attention Sinks  \n[2] H_2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"}, "questions": {"value": "1. While the paper presents extensive experiments, good research is not about stacking experiments but about providing clear methodological insight. The proposed method lacks a strong conceptual contribution beyond empirical validation, which limits its novelty and broader significance.\n2. The motivation for positioning this method specifically for PD disaggregation is not well justified. In principle, the same approach could be applied to both unified and disaggregated PD settings, and other existing methods can also handle such cases, suggesting that nothing in the proposed design is inherently PD-specific.\n3. The theoretical analysis lacks rigor and proper formatting, particularly in the theorem sections, which appear superficial and sometimes unsubstantiated, resembling LLM-generated text. A more careful and formal presentation is needed.\n4. Add ablation studies (e.g., without distillation, or isolating block vs. cache pruning) to ensure fair and transparent evaluation.\n5. Include missing baselines such as Wanda-SP, and ensure fair comparisons with calibration-only methods that do not rely on distillation.\n6. Reduce the number of hyperparameters and heuristic thresholds to improve reproducibility and reduce tuning bias.\n7. What exactly does “first and last token sequences” mean in the proposed pruning mechanism? The formulation in Equation (19) appears nearly identical to the Sink method [1], suggesting limited novelty.\n8.Please avoid claiming global optimal pruning, as the proposed approach is a heuristic approximation to a large-scale search problem rather than a proven optimal solution.\n9. How is the overhead of KV cache pruning measured in practice? Existing works such as Sink and H2O [2] provide clearer, more straightforward metrics for such evaluation.\n\n[1] Efficient Streaming Language Models with Attention Sinks  \n[2] H_2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "msbwboJrPj", "forum": "qhLjdFHdXm", "replyto": "qhLjdFHdXm", "signatures": ["ICLR.cc/2026/Conference/Submission8296/Reviewer_LQRH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8296/Reviewer_LQRH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761385423737, "cdate": 1761385423737, "tmdate": 1762920226592, "mdate": 1762920226592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PDTrim targets real-world prefill–decode (PD) disaggregation, where prefill and decode run on different resources. The method (1) builds separate pruning/distillation sets and performs iterative block pruning independently for prefill vs. decode; (2) introduces a KV-cache pruning rule that selectively keeps entries (e.g., first/last segments) for designated layers to cut bandwidth. Experiments show better accuracy/latency trade-offs in both disaggregated and unified settings, and up to $4.95\\times$ reduction in data transmission."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Treats bandwidth as a first‑class objective in pruning for PD.\n\n2. Demonstrates improvements across several backbones and settings.\n\n3. Compatible with quantization methods, suggesting composability."}, "weaknesses": {"value": "1. Heuristic criteria: attention‑score cutoffs and first/last‑segment retention may fail on datasets with mid‑context dependencies (e.g., retrieval‑augmented inputs).\n\n2. Limited scaling evidence: no systematic results for long contexts (e.g., $>$64k tokens) or for many concurrent decoders.\n\n3. Search overhead & stability: iterative pruning/annealing costs and variance across runs are insufficiently reported.\n\n4. Comparative breadth: needs head‑to‑head comparisons with recent KV compression, speculative decoding, and structured pruning under matched budgets."}, "questions": {"value": "1. How robust is the KV pruning policy for tasks with mid‑sequence salient tokens (long‑form QA, multi‑turn chat)?\n\n2. What is the wall‑clock cost and variance of the iterative block search for 13B/32B models?\n\n3. Can pruning decisions be made online using gradients or saliency estimates gathered during serving?\n\n4. How does PDTrim interact with speculative decoding and KV quantization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lD5kl0tCEA", "forum": "qhLjdFHdXm", "replyto": "qhLjdFHdXm", "signatures": ["ICLR.cc/2026/Conference/Submission8296/Reviewer_goEM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8296/Reviewer_goEM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972647145, "cdate": 1761972647145, "tmdate": 1762920226148, "mdate": 1762920226148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PDTrim, a pruning framework designed specifically for Prefill-Deocde disaggregation in large language model inference. While conventional pruning methods treat model layers uniformly, PDTrim recognizes that the prefill and decode stages have heterogeneous resource chracteristics and pruning sensitivities. The proposed framework combines two complementary mechanisms:\n\n\n(1) stage-aware block pruning: Iteratively removes redundant transformer blocks for each stage, guided by cosine similarity and probabilistic optimization.\n\n(2) Selective  KV cache pruning: retains cache entries for only the most attended tokens, reducing communication bandwith between prefill and decode nodes.\n\nExperiments across multiple LLMs dmonstrate competitive performance compared with baselines such as LLM-Pruner, ShortGPT, SLEB and Shorten LLaMA, achieving up to 4.95x reduction in bandwidth and faster inference with comparable accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The paper reports results on diverse model scales and benchmark suites, including MMLU, HellaSwag, ARC, BoolQ and others.\n\n(2) Demonstrates consistent gains over strong baselines, with signifcant latency and bandwidth improvements.\n\n(3) The work connects algorithmic pruning design with practical deployment efficiency."}, "weaknesses": {"value": "(1) Several tables omit key baselines or benchmarks without clear explanation. Formatting issues reduce credibility of reported outcomes.\n\n(2) Unclear technical formulation\n(2-1) The construction of pruning and distillation sets in Section 2.1.1 assigns every block to one of the two sets, yet introduces a threshold $d_{T}$ whos operational role is ambiguous.\n(2-2) It remains unclear how blocks below the threshold are handled and how these sets interact during iterative optimization.\n\n(3) The paper reports LLM-Pruner and Short GPT variants but lacks sufficient detail on how PDTrim was incorporated into those framework.\n\n(4) In some unified (non-PD) settings, PDTrim even outperforms the PD-Disaggregated case, which weakens the claim that its benefits stem from PD awareness itself.\n\n(5) Dense mathematical expressions and inconsistent figure-text correspondence make the method challenging to follow."}, "questions": {"value": "(1) In several cases, PDTrim performs better in unified (non-PD) inference than in PD-disaggregated setups. Could you clarify whether the performance gains are due to PD disaggregation or just from the pruning heuristics themselves?\n\n(2) Some benchmarks and baselines (e.g., ARC-E, HellaSwag, SLEB) are inconsistently included across tables. Is there a specific reason for their exclusion?\n\n(3) The pruning formulation combines cosine similarity, distillation pairing, and simulated annealing. Was this derived from prior theoretical work or developed empirically?\n\n(4) Can the authors provide ablation results isolating the contribution of Stage-Aware Pruning vs KV Cache Pruning? This would clarify which part contributes most to bandwidth reduction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zcfK0tdANo", "forum": "qhLjdFHdXm", "replyto": "qhLjdFHdXm", "signatures": ["ICLR.cc/2026/Conference/Submission8296/Reviewer_brgU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8296/Reviewer_brgU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979756825, "cdate": 1761979756825, "tmdate": 1762920225340, "mdate": 1762920225340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a pruning method PDTRIM, targeting for prefill–decode (PD) disaggregated inference. The framework learns separate block-removal plans for prefill vs. decode and prunes KV-cache transfers from prefill to decode. It builds pruning and distillation sets, defines block redundancy via cosine similarity between a block’s input/output, and searches the space of block removals with a simulated-annealing style iteration to find stage-specific solutions. To reduce inter-node bandwidth, it selects layers whose attention strongly concentrates on the first/last tokens and, for those layers, reuses only the first/last token KV entries from prefill while keeping decode-side KV intact. Experiments show faster inference and up to 4.95× less data transmitted, with accuracy competitive or better than strong pruning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method learns separate pruning plans for prefill and decode, which is different from prior pruning methods that mainly focus on the prefill stage.. The paper explicitly states this and analyzes why prefill is more sensitive (errors accumulate across generated tokens). Reported results show lower inference latency versus other baselines and better accuracy under PD disaggregation than unified one-stage pruning. \n\n2. Bandwidth-aware KV pruning matches the real bottleneck in PD process. The first/last-token reuse rule is simple, tunable, and empirically effective, with concrete bandwidth savings.\n\n3. The algorithm descriptions, practical hyperparameters are clearly shown.  And the paper reports broad evaluation across models and tasks, including direct comparisons to recent KV-pruning methods under a common token-pruning ratio.\n\n4. Ablation shows that the iterative search and the KV-layer selection both contribute to the framework."}, "weaknesses": {"value": "1. Per-task variability and KV-reuse generality remains unexplored. While accuracy averages outperforms other pruning baselines, several benchmarks show worse performance (WNLI, SST-2, RTE), and the first/last-token KV-reuse rule lacks per-task diagnostics for mid-sequence evidence tasks. \n\n2. The structural constraint may limit search space. The decode removal set must superset the prefill set to guarantee KV reuse. The trade-off versus potentially better non-subset solutions is not quantified."}, "questions": {"value": "1. What guides 0.95 as the default of the distillation threshold, and how does it transfer across models?\n\n2. Could you provide more task-specific analysis or adaptive policies would clarify when bandwidth cuts are safest other than first/last-token KV-reuse rule?\n\n3. As the paper uses 256 examples from PIQA and MMLU as the default calibration set. How sensitive are removal decisions and final accuracy to the domain of the calibration data when evaluating on tasks like SST-2, WNLI, and RTE?\n\n4. Bandwidth savings are now measured on a local network. Is it possible to consider application under different link speeds and latencies, including WAN-like RTTs, and with batch sizes beyond 1?\n\n5. The method requires the prefill pruning set to be a subset of the decode pruning set to enable KV reuse. Could you quantify accuracy, latency, and bandwidth if this constraint is relaxed? If infeasible, explain why and if there can be feasible variants?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rbFEeIOEo8", "forum": "qhLjdFHdXm", "replyto": "qhLjdFHdXm", "signatures": ["ICLR.cc/2026/Conference/Submission8296/Reviewer_3b4H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8296/Reviewer_3b4H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762182661033, "cdate": 1762182661033, "tmdate": 1762920224760, "mdate": 1762920224760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}