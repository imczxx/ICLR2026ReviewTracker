{"id": "kflYZjGumW", "number": 1604, "cdate": 1756896763796, "mdate": 1763524467680, "content": {"title": "DiCache: Let Diffusion Model Determine Its Own Cache", "abstract": "Recent years have witnessed the rapid development of acceleration techniques for diffusion models, especially caching-based acceleration methods. These studies seek to answer two fundamental questions: _\"When to cache\"_ and _\"How to use cache\"_, typically relying on predefined empirical laws or dataset-level priors to determine caching timings and adopting handcrafted rules for multi-step cache utilization. However, given the highly dynamic nature of the diffusion process, they often exhibit limited generalizability and fail to cope with diverse samples. In this paper, a strong sample-specific correlation is revealed between the variation patterns of the shallow-layer feature differences in the diffusion model and those of deep-layer features. Moreover, we have observed that the features from different model layers form similar trajectories. Based on these observations, we present **DiCache**, a novel training-free adaptive caching strategy for accelerating diffusion models at runtime, answering both when and how to cache within a unified framework. Specifically, DiCache is composed of two principal components: (1) _Online Probe Profiling Scheme_ leverages a shallow-layer online probe to obtain an on-the-fly indicator for the caching error in real time, enabling the model to dynamically customize the caching schedule for each sample. (2) _Dynamic Cache Trajectory Alignment_ adaptively approximates the deep-layer feature output from multi-step historical caches based on the shallow-layer feature trajectory, facilitating higher visual quality. Extensive experiments validate DiCache’s capability in achieving higher efficiency and improved fidelity over state-of-the-art approaches on various leading diffusion models including WAN 2.1, HunyuanVideo and Flux.", "tldr": "DiCache is a training-free adaptive caching strategy for accelerating diffusion models at runtime.", "keywords": ["diffusion model", "generative model", "inference acceleration"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f76ea56d2da0d9715c6df8f4ffc3afd040e874b.pdf", "supplementary_material": "/attachment/7d250b94c174f99be10918fe905636c7f2c248cd.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a caching mechanism for Diffusion Transformers (DiT). The core idea is to use shallow-layer residual probing to estimate deep-layer feature caching. The author verified DiCache on both text-to-image and text-to-video tasks and obtained superior results compared with the baseline method. In summary, I would say this work introduces a simple yet effective DiT caching method that is easy to use for the community. However, more experimental analysis is expected to make the work more solid and convincing."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this paper is clear. Empirical findings are quite convincing and interesting.\n2. This method is easy to use and can be integrated with other intra-step diffusion acceleration methods (such as the sparse-attention-based method). \n3. On one hand, the speedup is non-trivial which is especially useful for video diffusion model acceleration. On the other hand, the quality loss is marginal and it can maintain high consistency with the vanilla samples."}, "weaknesses": {"value": "The main weakness of this paper lies in the presentation and insufficient experimental analysis. Please see the questions below.\n\nSome advice on the presentation:\n1. I strongly recommend redesigning Figure 2 as it increases the understanding cost.\n2. Axes numbers in Figure 3 are too small.\n3. Abbreviations are better introduced at the first time they appear, such as DCTA."}, "questions": {"value": "1. Since Wan2.1 and HunyuanVideo are tested, I suggest that the authors provide VBench results on the t2v task. Evaluating generated video quality using image-oriented metrics is not convincing enough.\n2. Compared with baseline methods such as TaylorSeer, DiCache uses fewer text-to-image metrics, making the evaluation not comprehensive enough. For example, how does DiCache affect the text-image alignment?\n3. Since m is set to 1 for all experiments and the author claimed $m\\in [1,2,3] $ is sufficient, I suggest that the author show corresponding diagrams in Figures 3 and 4, instead of only showing m=5.\n4.  What is the recomputation rate under different parameter settings? In other words, is $\\delta$ hard to tune for different DiT models? It seems this value differs across the DiT models tested.\n5. For DCTA, does it mean the current residual is continuously computed from the two most recent recomputed timesteps? How much additional overhead (in terms of FLOPS and latency) will this cause?\n6. How does DiCache perform on distilled models?\n7. What's the source of text-to-video prompts?\n\nI will raise my score when concerns are mostly addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "otuCVk8VnC", "forum": "kflYZjGumW", "replyto": "kflYZjGumW", "signatures": ["ICLR.cc/2026/Conference/Submission1604/Reviewer_1sUf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1604/Reviewer_1sUf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761377097895, "cdate": 1761377097895, "tmdate": 1762915831620, "mdate": 1762915831620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DiCache, a unified probe-driven framework that adaptively schedules and utilizes cache during diffusion inference, achieving efficient acceleration without additional training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses the two fundamental challenges of cache-based acceleration through a unified probe-driven framework, which reduces reliance on empirical heuristics and offline calibration.\n\n2. DiCache can be further combined with Sparse VideoGen to achieve additional acceleration, demonstrating its complementarity with sparse attention techniques.\n\n3. The authors empirically observe a strong correlation between shallow-layer feature differences and deep-layer residuals, and find that features across different DiT blocks exhibit similar trajectories during the sampling process, providing the foundation for the probe-based error estimation and trajectory-based cache blending."}, "weaknesses": {"value": "1. The coverage of baselines is somewhat limited. Although the experimental tables include TeaCache, EasyCache, TaylorSeer, and ToCa, the Related Work section also discusses other comparable methods such as FasterCache, FORA, and Δ-DiT. Incorporating these methods into the quantitative comparison tables would make the empirical positioning of the proposed approach more complete.\n\n2. The paper primarily evaluates performance using similarity metrics (LPIPS, SSIM, and PSNR) with respect to the outputs of the original model, along with the inference speedup ratio. However, it provides limited analysis of broader perceptual quality or downstream task metrics. Since DiCache does not demonstrate particularly strong fidelity compared to the base model, including perceptual quality evaluations such as VBench scores and user studies would be necessary to provide a more comprehensive assessment."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z2QptvpaEj", "forum": "kflYZjGumW", "replyto": "kflYZjGumW", "signatures": ["ICLR.cc/2026/Conference/Submission1604/Reviewer_MEuZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1604/Reviewer_MEuZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462390584, "cdate": 1761462390584, "tmdate": 1762915831417, "mdate": 1762915831417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents DiCache, a training-free, runtime-adaptive caching framework for diffusion models.\nIn DiCache, the Online Probe Profiling Scheme runs the first few layers to extract shallow probe features and estimate a per-sample caching error, which serves as the threshold for cache reuse.\nDynamic Cache Trajectory Alignment then uses the probe’s estimated progress to select and align deep features from nearby cached steps, reconstructing the current step without recomputing the heavy layers.\nAs a result, DiCache achieves high speedups with stronger fidelity across WAN 2.1, HunyuanVideo, and Flux."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Novel idea on dynamic cache trajectory alignment that effectively and efficiently adapts the cached value to the current layer that reuses it.\n\ntraining-free and plug-and-play, requiring no model fine-tuning; it works at inference by wrapping around any DiT models.\n\nDiCache consistently achieves faster inference without sacrificing output quality, outperforming prior caching methods on both image and video diffusion models\n\nClear analysis and ablations with generally smooth writing and informative figures; easy to follow overall."}, "weaknesses": {"value": "1.\t**Reliance on Threshold Hyperparameter:** Although DiCache demonstrates effective runtime caching under the reported experimental settings, the chosen probe depth (m) and accumulated caching error threshold (δ) should ideally generalize across different models. Alternatively, the authors could justify that the method’s effectiveness is not sensitive to these hyperparameters to substantiate the “calibration-free” claim. However, the current analysis of both hyperparameters lacks evidence of such generalization. For example, Spearman correlation analyses (e.g., Fig. 3 (d)) across multiple architectures would strengthen this point.\n2.\t**Hyperparameter Trade-offs:** There exists a strong trade-off among probe depth, reuse threshold, and achieved speedup, as well as between the reuse threshold and output accuracy. This implies that achieving the optimal quality–efficiency balance may require manual hyperparameter tuning, making the approach functionally similar to other calibration-based caching methods rather than being fully self-adaptive.\n3.\t**Overlap with Prior Works on Adaptive Caching and Probing:** DiCache’s adaptive cache decision, which is based on accumulated probe scores and thresholding, follows the same high-level “accumulate + threshold” mechanism used in TeaCache, and its distance-based error proxy conceptually resembles AdaCache’s feature-change metric. Furthermore, shallow-layer probing and caching is a common technique in transformer models [1, 2]. The authors should clarify the DiT-specific novelty in their cache-layer determination, beyond the incremental combination or adaptation of these existing ideas.\n4.\tEffectiveness of DCTA: Although DCTA is presented as a major contribution of this work, the breakdown of DiCache’s accuracy in Table 2 raises concern about its actual impact, as the observed improvement appears marginal. It would be helpful to clarify whether the effectiveness of DCTA, while seemingly limited in magnitude, remains consistent across different models.\n5.\t**Memory Usage Analysis:** The paper lacks quantitative analysis of memory usage, even though higher-order or multi-step caching inevitably trades off between memory consumption and accuracy. Reporting VRAM or feature-map memory profiles would clarify the practical scalability of the proposed method.\n\n[1] LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding, Mostafa Elhoushi, Akshat Shrivastava, et al., ACL 2024.\n\n[2] Reducing Transformer Key-Value Cache Size with Cross-Layer Attention, William Brandon, Mayank Mishra et al., NeurIPS 2024."}, "questions": {"value": "Mainly listed in the weakness. Below are the additional questions.\n\n1.\t**Reasoning on probing:** Explaination on why the first few layers can be so informative for predicting global caching error—e.g., what structural or representational property of DiTs enables such strong shallow–deep feature correlation would strenghten the paper.\n\n2.\t**Minor:** (i) Typo: missing space in “…probe feature trajectory,which…” (line 108). (ii) Naming inconsistency between “Online Probe Profiling Scheme” (main text) and “Online Probe Profiling Strategy” (Fig. 2 caption)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9WsNjqDUVF", "forum": "kflYZjGumW", "replyto": "kflYZjGumW", "signatures": ["ICLR.cc/2026/Conference/Submission1604/Reviewer_HkJV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1604/Reviewer_HkJV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624152078, "cdate": 1761624152078, "tmdate": 1762915831248, "mdate": 1762915831248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DiCache, a training-free, adaptive caching strategy to accelerate diffusion models. DiCache addresses two core challenges in caching-based acceleration: \"when to cache\" and \"how to use cache.\" The method introduces two components: (1) an Online Probe Profiling Scheme to dynamically determine when to reuse cached outputs, and (2) Dynamic Cache Trajectory Alignment to improve multi-step feature reuse through shallow-to-deep trajectory consistency. Extensive experiments on WAN 2.1, HunyuanVideo, and Flux validate that DiCache achieves significant speedup while maintaining high visual fidelity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The motivation behind the proposed method is well articulated. The method's design is strongly supported by empirical evidence presented in the paper.\n\n* The method is completely training-free, making it highly practical and broadly applicable across different diffusion models."}, "weaknesses": {"value": "* The proposed reuse threshold δ appears to require manual, per-model tuning (δ = 0.2 for WAN 2.1, δ = 0.1 for HunyuanVideo, δ = 0.4 for Flux), which may reduce generality and increase tuning effort for new architectures.\n\n* While the probe is “shallow,” it is still computed at every single timestep to accumulate caching error. It remains unclear how much speedup is offset by repeated probing on large backbones."}, "questions": {"value": "1.Given the memory and dynamic variability challenges, is there a possibility to automatically determine δ or adaptively calibrate it without per-model tuning?\n\n2.The probe is shallow but still executed at every timestep. Could the authors provide a detailed comparison of the probe cost across different architectures.\n\n3.The probe uses L1 relative distance on shallow features. Have alternative feature distance metrics been considered, especially for semantic coherence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kKBVEUJ0N7", "forum": "kflYZjGumW", "replyto": "kflYZjGumW", "signatures": ["ICLR.cc/2026/Conference/Submission1604/Reviewer_bWWH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1604/Reviewer_bWWH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984261959, "cdate": 1761984261959, "tmdate": 1762915831038, "mdate": 1762915831038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}