{"id": "Q568txVIHt", "number": 13340, "cdate": 1758216760859, "mdate": 1763277660638, "content": {"title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions", "abstract": "In-context learning (ICL) has emerged as an effective approach to enhance the performance of large language models (LLMs). However, its effectiveness varies significantly across models and tasks, posing challenges for practitioners to determine when ICL reliably improves performance. Current evaluation approaches, reliant on performance change after applying ICL, suffer from low reliability, poor attribution, and impracticality in data-insufficient scenarios.\n\nWe propose the **Learning-to-Context Slope (LCS)**, a novel metric that quantifies ICL effectiveness by modeling the slope between *learning gain* (loss decrease from demonstrations) and *contextual relevance* (demonstration-input relevance).\n\nLCS addresses key limitations of performance-based metrics:\n\n1. it captures continuous loss changes even when outputs are incorrect, improving reliability;\n2. its formulation attributes ICL failures to weak contextual alignment (inability to adapt inputs to demonstrations) or strong output calibration (self-verification of correctness); and\n3. it minimizes reliance on labeled data via synthetic evaluation.\n\nExtensive experiments demonstrate that LCS strongly correlates with performance improvements in labeled settings and reliably reflects true effectiveness in biased or data-scarce scenarios. Further analysis reveals actionable thresholds for LCS and identifies model capabilities critical to ICL success.", "tldr": "", "keywords": ["In-Context Learning; ICL Effectiveness"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/674b28e541ae10a0c5963987800e0f79384f0ad8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Learning-to-Context Slope (LCS) to measure the in-context learning (ICL) effectiveness of the language models. Through empirical evaluations, authors show that LCS is more reliable compared to the existing evaluation method, namely model performance change with respect to demonstrations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Author provides mathematical formulation of LCS, and also provides intuitive analysis on its components. \n- LCS shows promising empirical results compared to the existing metrics."}, "weaknesses": {"value": "- Although authors provide mathematical formulation of LCS, the motivation of choosing LCS to overcome the problems of existing evaluation metric is not clear. \n- The purpose of 2.3 is not clear. Since they empirically show that LCS can be effective even with synthetic data, I do not think this section is necessary. To me discussing about real large language models, which they placed in the appendix might have been more informative. \n- Organization of Section 3 is not optimal. The limitation of the existing evaluation metric is (1) low reliability (2) poor attribution (3) requirement of labeled data. Then the experiments should first deal with these aspects of LCS.\n- The implications of Section 3.2.1 is not clear. \n- LCS cannot be applied to black box models."}, "questions": {"value": "Refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AGDzR3Tv0q", "forum": "Q568txVIHt", "replyto": "Q568txVIHt", "signatures": ["ICLR.cc/2026/Conference/Submission13340/Reviewer_yHFq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13340/Reviewer_yHFq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985741065, "cdate": 1761985741065, "tmdate": 1762923997016, "mdate": 1762923997016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper comes up Learning-to-Context Slope (LCS), a novel metric that quantifies the effectiveness of in-context learning (ICL). LCS is defined as the slope between **learning gain** $I_p(X \\to D\\mid Q)=p(D\\mid Q;X)-p(D\\mid Q)$ and **contextual relevance** $I_p(D \\to X\\mid Q)=p(X\\mid Q;D)-p(X\\mid Q)$ where $p$ is the predictive distribution, $Q$ is the user input, $D$ is the ICL demonstration (e.g., one-shot example), and $X$ is the labeled output. Such a metric seems the most useful when one does not have many labeled examples to examine the actual ICL accuracies (= few-shot - one-shot). Furthermore, the authors show that creating synthetic demonstrations and then applying LCS is well-correlated with ICL effectiveness.\n\nContributions:\n- Proposes LCS as a metric to measure ICL effectiveness and shows strong correlation with task performance improvement with ICL over 8 datasets with multiple models. Theorem 1 claims these are linearly related with slope $p(D\\mid Q)/p(X\\mid Q)$ and the paper provides Gives an empirical procedure to estimate the slope via least-squares on sampled triples of $(q_i, x_i, d_i)$.\n- Explores “label-free” estimation with synthetic demonstrations and argues such LCS is consistently smaller than from real data (Theorem 2).\n- Identifies two factors claimed to modulate LCS: contextual-alignment capability $\\hat{p}(D∣Q)$ and output-calibration capability $\\hat{p}(X∣Q)$.\n- Provides an empirical LCS threshold ≈ 0.2 and a heuristic for ICL demo selection based on learning gain, which achieves strong performance compared to other baseline selection methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: Framing ICL effectiveness as a slope between two “information-like” quantities is novel and produces a continuous signal where EM/Pass@1 are binary/noisy. \n- Clarity: The empirical estimator (Eq. 3) and plotting protocol are easy to reproduce conceptually, and the paper is upfront that $r_{\\hat{p}}= \\hat{p}(D∣Q) / \\hat{p}​(X∣Q)$ is errorful. (however, see weaknesses and questions for multiple points of unclear presentation.)\n- Quality: Broad evaluation across 8 datasets on several model families and choosing ICL examples based on their heuristic is particularly helpful.\n- Significance: If sound, LCS could help decide when to invest in demonstrations and guide demo selection when labeling comes at a high cost."}, "weaknesses": {"value": "1. **Foundational mismatch between Eq. (2) and autoregressive LLMs**: The proof underlying Eq. (2) treats conditionals like $p(D\\mid Q;X)$ and $p(D\\mid Q)$ as if the order of variables in the conditioning can be freely rearranged; this yields a neat decomposition of loss into a “zero-shot” term and a “demo-dependent” term (Eq. 2). Yet in an autoregressive LLM, $\\log p(x_t \\mid Q, D, x_{<t})$ depends on the exact **prompt order** $[Q, D, x_{<t}]$. Specifically, for an AR LM, $p(\\cdot\\mid\\cdot)$ is implemented as **conditional next-token probabilities over a specific prompt order**. Because the paper’s derivation ignores this fixed ordering, Eq. (2) does not work if one just plugs in an likelihood by LLM into $p$. The proof in Appendix C.1 must assume that an order-invariant joint over $(Q,D,X)$. So Eq. (2) does not hold for standard AR LLM $p$ that the paper later uses unless we assume order invariance, which is an extremely strong assumption on language modeling.\n\n2. **Strong assumptions needed for theoretical results to support LCS**:\n- **Pseudo-probabilities that do not align with theorems**: In practice the method forms a chat-template string, multiplies token probabilities to get a joint score, and then applies length normalization “to minimize the confounding effects of sequence length,” which is not a proper probability transformation and severs the direct connection to its mathematical justifications.\n- **Theorem 2 assumes a universally optimal demonstration $D^\\star$.**: Theorem 2 assumes a real demonstration $D^\\star$ such that for all $X, Q$ s.t. $\\hat p(D^\\star \\mid Q;X)$ dominates $\\hat p(\\hat D\\mid Q;X) $. In plain English this means that there exists a single demonstration that is uniformly best across questions and answers, which seems implausible. The assumption is doing heavy lifting for the conclusion, unless the authors can empirically demonstrate such an assumption is not strong.\n- **Mismatch between the loss motivation and the probability‑difference metric**: The loss motivation is in logarithms (Eq. 2), but the core quantities are later defined as differences of probabilities, and the slope result relates $I_p$ terms linearly. I'm confused what Section 2.1 is trying to show then.\n3. **LCS reflects ICL effectiveness are only partially substantiated**:\n- **Weak baselines for demo selection**: When proposing learning-gain–based selection, the comparison set is limited (BM25, GTR, IDS), omitting many modern ICL selection strategies (e.g., stronger LLM-scoring/uncertainty-aware retrievers). Stronger baselines, or discussions of existing baselines at least, would greatly support their experiments.\n- **Correlational studies (begging the question)**: My biggest confusion on LCS is how the paper grounds the notion of ICL effectiveness. It is unclear what the authors are definied as ICL effectiveness. At some points, ICL effectiveness is grounded by $\\Delta$, the performance change before and after ICL (e.g., Figure 1/2, Table 1, and the main argument of Section 3.2.1). However, Section 3.2.2 claims that performance change cannot reflect ICL effectiveness, which is measured by performance change just on a larger population. I find this reasoning circular and puts LCS on a weak foundation. For example, I have a hard time parsing this sentence in L414-416 without some implicit circular reasoning: \"However, LCS does not generally increase or decrease with the number of shots but rather exhibits some degree of fluctuation. This is because the value of LCS is related to the inherent ICL effectiveness on a given model and dataset, while increasing shot number cannot affect the ICL effectiveness.\""}, "questions": {"value": "1. In L49, the paper claims ICL performance-based metrics are not reliable. Can you provide practical examples where this is the case? Or cases where one has to perform ICL but obtaining labeled data is difficult?\n2. Theorem 2 proof error: L1062-1066 incorrectly uses the law of total probability or am I mistaken? Each equation should have a $\\hat{p}(X|Q)$ multiplied. The conclusion would still hold.\n3. Can you report $R^2$ values for Figure 10? It is hard to tell that contextual relevance is a better predictor than other metrics. The difference is small that I would like to see this plot with other datasets (MATH or GSM8K). Without convincing evidence, it is hard to see LCS as a useful predictive tool.\n4. “Bad cases” analysis (Fig. 3): while the stated conclusion \"Even on data where ICL does not improve performance, LCS still reveals the ICL effectiveness\"(L309) is true, this highlights one issue with LCS. LCS only provides a metric for (model, task). It is not predictive of ICL effectiveness of each demo (model, task, ICL demo), so even when LCS=1.06>threshold=0.2, there exist cases where ICL performance improvement $\\Delta$ is equal to zero.\n5. Table 2: This is a great result, but I would like to see the performances of these methods on the other tasks (GSM8K, ARC-C, etc.) to see how much LCS-based selection can ameliorate negative ICL effects. Furthermore, is IDS and GTR the state-of-the-art (training-free) ICL example selection method? I recall many works on this, and would like for the authors to compare against more diverse baselines, e.g., https://arxiv.org/pdf/2302.11042 or https://aclanthology.org/2023.emnlp-main.331.pdf. \n6. L427 \"multi-round iterative process is used to ensure the diversity and quality of the synthesized\": can you describe in detail what this process is? Is it manual or is there some heuristics for diversity?\n\nMisc.\n1. Regression slope inconsistency: Which slope is LCS? Theorem 1 claims\n$$\n  I(X \\to D\\mid Q)=\\frac{p(D\\mid Q)}{p(X\\mid Q)} I(D \\to X\\mid Q).\n$$\nThus, if you regress $t = I(X \\to D\\mid Q)$ on $s = I(D \\to X\\mid Q)$, the slope is $\\frac{p(D\\mid Q)}{p(X\\mid Q)}$. However, Eq. (3) defines $s_i:=I(d_i \\to x_i\\mid q_i)$ and $t_i:=I(x_i \\to d_i\\mid q_i)$ but then sets the slope as\n$$\n  r_{\\hat p}=\\frac{\\sum (t_i-\\bar t)(s_i-\\bar s)}{\\sum (t_i-\\bar t)^2},\n$$\nwhich is the slope of $s$ on $t$, if I am not mistaken. Meanwhile, plots in Figs. 7–9 show fitted lines with the slope of $t$ on $s$.\n2. In Section 2.2 the paper suddenly calls LCS the “Learning‑to‑Relevance Ratio”.\n3. Appendix C.1 proof introduces a mysterious variable $K$.\n4. L352: \"does\" -> \"do\". L42: \"cross\" -> \"across\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YCMqMeJ4j4", "forum": "Q568txVIHt", "replyto": "Q568txVIHt", "signatures": ["ICLR.cc/2026/Conference/Submission13340/Reviewer_bnZL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13340/Reviewer_bnZL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762048671531, "cdate": 1762048671531, "tmdate": 1762923996518, "mdate": 1762923996518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Learning to Context Slope, LCS, as a metric to evaluate the effectiveness of in context learning. The idea is to measure how much the loss decreases when we add demonstrations, as a function of how relevant those demonstrations are to the input. Formally, the authors define two quantities per triple Q as the input question, X as the output label, and D as the demonstrations. They define two Bayesian relationship terms:\n\nLearning gain $t = I(X \\rightarrow D \\mid Q)$\n\nContextual relevance $s = I(D \\rightarrow X \\mid Q).$\n\nOver many triples, they fit a line from t to s using least squares, and the slope is the definition of LCS. A larger slope means that increases in relevance translate into larger loss reductions. The paper argues that LCS is more reliable than simple performance deltas because it uses continuous loss even when the final answer is wrong. It also claims better attribution. Low LCS can arise from weak contextual alignment, meaning the model does not extract the right signals from demonstrations, or from strong output calibration, meaning the model already verifies answers without demonstrations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation. Performance based evaluation can be noisy and hard to attribute. LCS targets the underlying loss dynamics.\n- Simple mathematical core. The link between learning gain and contextual relevance is expressed as a slope that practitioners can estimate with standard scoring. Because it uses token level loss, LCS can detect progress that accuracy metrics miss.\n- Although interactions within D cannot be measured precisely and a microscopic view is not provided as noted in weakness one, when order and interactions between demonstration samples are treated as fixed, LCS appears to provide a useful global view."}, "weaknesses": {"value": "- LCS is a set level first order summary. It does not model higher order interactions between multiple demonstrations, such as redundancy, synergy, order, and position effects. The paper treats k shot by splitting into k points, which ignores interactions.\n- Assumptions in theory. Theorems rely on modeling choices and oracle versus empirical probabilities. They are valid under stated conditions, but the practical gap due to estimation error and prompt format choices remains.\n- Computational cost. Estimating s and t requires multiple scoring passes with consistent templates and length normalization. This can be heavy for large models and many demonstrations.\n- Interpretability of magnitude. The paper reports that LCS correlates with performance, but a universal threshold for large positive, medium, or small is not fully established in the main text."}, "questions": {"value": "- Naming inconsistency. (minor error I guess) The text uses Learning to Relevance Ratio (LCS) at line 162 while the main name is Learn to Context Slope (LCS).\n- When the base model already has strong output calibration, I guess LCS can be low and ICL may not help. The paper frames this as expected, but does this limit usefulness in easy tasks or with very strong models.\n- ICL can also learn well for adversarial or label flipped scenarios. If demonstrations are systematically misleading, I presume they can learn misbehavior quite well as well. It is unclear how stable LCS remains in practice and whether it can output robust and reliable conclusions. Robustness under adversarial or flipped labels. If demonstrations encode a wrong mapping that the model can learn, how do s, t, and LCS behave relative to accuracy and loss on the true label. Can the authors provide an experiment that stresses LCS in such cases.\n- Thresholds and portability. The paper mentions actionable thresholds. Can the authors provide numeric guidance that transfers across datasets and models, for example a range of LCS values that usually indicate effective ICL."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sz7tMRjoYv", "forum": "Q568txVIHt", "replyto": "Q568txVIHt", "signatures": ["ICLR.cc/2026/Conference/Submission13340/Reviewer_EHcL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13340/Reviewer_EHcL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762337352114, "cdate": 1762337352114, "tmdate": 1762923996168, "mdate": 1762923996168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a new metric called Learning-to-Context Slope (LCS) to measure how effectively large language models (LLMs) perform in-context learning (ICL). Compared to traditional evaluations that simply measure task performance, LCS quantifies the relationship between learning gain (loss reduction from demonstrations) and contextual relevance (alignment between demonstrations and input). This continuous, loss-based metric captures ICL effectiveness even when outputs are incorrect, provides clearer attribution to model limitations such as weak contextual alignment or excessive output calibration, and works without labeled data through synthetic evaluation. Experiments across eight datasets and multiple LLMs show that LCS correlates strongly with performance gains and reveals that ICL effectiveness depends mainly on a model’s ability to align contextually with demonstrations rather than raw performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed LCS metric is a novel and potentially useful idea. As this idea goes beyond performance-based evaluations and offers continuous, loss-based measure that remains informative even when model outputs are incorrect, the metric allows for a more refined analysis of ICL capabilities. Moreover, LCS works without labeled data through synthetic evaluation, making it applicable when data is limited. The experiments across multiple datasets and models validate its robust correlation with actual performance improvements, demonstrating both utility.\n\nAs far as I know, the idea and the metric presented in this work is novel, and I feel it is potentially very valuable for better quantifying the progress of ICL methodology."}, "weaknesses": {"value": "LCS primarily measures correlation, not causation as a high slope indicates association between loss reduction and context relevance, but does not directly prove that demonstrations cause better learning. Also, the theory and the metric's interpretability depends on strong modelling assumptions.\n\nAlso, the experiments could be expanded to a broader range of model families."}, "questions": {"value": "Can the authors clarify the intuition behind modeling ICL effectiveness as a slope? Why is a linear relationship assumed between loss decrease and contextual relevance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WiRBeYVfkU", "forum": "Q568txVIHt", "replyto": "Q568txVIHt", "signatures": ["ICLR.cc/2026/Conference/Submission13340/Reviewer_VfMS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13340/Reviewer_VfMS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762338380972, "cdate": 1762338380972, "tmdate": 1762923995791, "mdate": 1762923995791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}