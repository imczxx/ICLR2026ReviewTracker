{"id": "DtxerRpEu7", "number": 8164, "cdate": 1758071625047, "mdate": 1759897802594, "content": {"title": "ConfProBench: A Confidence Evaluation Benchmark for MLLM-based Process Judges", "abstract": "Reasoning is the critical capability of multimodal large language models (MLLMs) to solve complex multimodal tasks, and judging the correctness of reasoning steps is crucial to improving this capability. Recently, MLLM-based process judges (MPJs) have been widely used to judge the correctness of reasoning steps in multimodal reasoning tasks. Therefore, evaluating the capability of MPJs is crucial for identifying their limitations and guiding future improvements. However, existing benchmarks for MPJs primarily focus on evaluating capabilities such as step correctness classification and reasoning process search, while overlooking a critical dimension: whether the confidence scores produced by MPJs at the step level are reliable. To fill this gap, we propose ConfProBench, the first comprehensive benchmark designed to systematically evaluate the reliability of step-level confidence scores generated by MPJs. This benchmark constructs three types of adversarially perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and Image Perturbation, to evaluate the robustness of MPJs' confidence under perturbations. Furthermore, we propose three novel evaluation metrics: Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and Confidence Calibration Score (CCS), which are designed to capture three complementary aspects of MPJs' confidence—robustness, sensitivity, and calibration. We evaluate 14 state-of-the-art MLLMs, including both proprietary and open-source models. Through extensive experiments, we reveal limitations in existing MPJs’ confidence performance and provide competitive baselines, thereby paving the way for future research in this field. Our dataset is provided in the supplementary materials.", "tldr": "We propose ConfProBench, a benchmark for evaluating the reliability of MLLM-based process judges' confidence from three aspects: robustness, sensitivity, and calibration.", "keywords": ["Multimodal Large Language Models (MLLMs)", "Confidence Evaluation", "Reasoning Process Judging"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cc3e7138eb2440f30b31c7beda7103345e24942e.pdf", "supplementary_material": "/attachment/0403424a2637e572eec9b193596a92056e96d503.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ConfProBench, the first benchmark to evaluate the confidence reliability of MLLM-based Process Judges (MPJs). To fill the gap where existing benchmarks overlook confidence, it designs three types of adversarial perturbations—lexical, syntactic, and multimodal—and introduces three novel metrics: CRS, CSS, and CCS. Through experiments on 14 MLLMs, the paper reveals the limitations of current models and provides a baseline for future research."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's contribution lies in successfully shifting the academic focus from the \"classification performance\" of MPJs to their \"confidence reliability\". In certain domains, a model must not only make correct judgments but also have an accurate self-awareness of its confidence. ConfProBench is the first work to provide a systematic evaluation framework for this problem, making it a pioneering work.\n2. It proposes three novel evaluation metrics: CRS, CSS, and CCS. These metrics approach confidence reliability from three relatively orthogonal dimensions—robustness, sensitivity, and calibration—collectively creating a multi-dimensional profile that is more comprehensive and profound than single traditional metrics like ECE."}, "weaknesses": {"value": "1. The paper's core motivation is that the confidence reliability of MPJs is crucial for downstream tasks like reasoning chain optimization, automatic error correction. However, the entire evaluation is confined to intrinsic metrics. A more persuasive argument would involve extrinsic evaluation: demonstrating that an MPJ with higher scores on ConfProBench actually leads to better final performance when its confidence scores are used to guide a practical downstream task.\n2. ConfProBench is entirely sampled and constructed based on ProJudgeBench. This means that any inherent biases within ProJudgeBench such as the distribution of problem domains, style of reasoning chains, patterns of error types could be inherited and even amplified by ConfProBench.\n3. In addition to evaluating and analyzing the shortcomings of existing models, proposing even a simple method aimed at improving the proposed metrics would significantly enhance its contribution. This would elevate the paper from a \"problem finder\" to a \"solution explorer\". Even if the method's effectiveness maybe limited, it could provide a starting point and a reference for subsequent research."}, "questions": {"value": "Could you provide evidence showing how strong performance on ConfProBench translates to tangible benefits in downstream tasks? Additionally, I'm curious about the potential for inherited dataset bias from ProJudgeBench."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w8kdb3fqGK", "forum": "DtxerRpEu7", "replyto": "DtxerRpEu7", "signatures": ["ICLR.cc/2026/Conference/Submission8164/Reviewer_jaof"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8164/Reviewer_jaof"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630160545, "cdate": 1761630160545, "tmdate": 1762920127701, "mdate": 1762920127701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ConfProBench, a benchmark and a set of metrics evaluating the capabilities of LLMs and MLLMs used as a critic for the reasoning process of another MLLM. The paper introduces the concept of confidence as the uncertainty the LLM judges when predicting a reasoning step being correct or wrong. ConfProBench creates variants to the existing reasoning dataset by perturbing the thinking steps with non-critical changes. The confidence metrics then evaluate the variation in the judges' outputs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper looks at an interesting aspect of the variance when using LLMs as judges. This directly affects how good a judge LLM can be. \n2. The paper contains an extensive supplementary material, demonstrating that the authors have put work into the manuscript."}, "weaknesses": {"value": "1. The definition of MPJ output as written in ln.119-120 is very ambiguous. In particular, I'm skeptical whether asking an LLM to output a probability score has any meaning or consistency over different tries. Is there a standard implemented for the evaluation of correctness? Otherwise, I'm not convinced that LLM, or even humans are able to give consistent answers.  \n2. I feel the proposed metrics miss an important aspect: accuracy of the LLM's discrimination. It seems that the proposed metrics do not evaluate this aspect. Then I wonder if some models can achieve good scores without classifying correctly."}, "questions": {"value": "1. What are some insights we can obtain from each model's performance? Should we use one model over another for verification or judgment purposes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "w3yaeFJtYe", "forum": "DtxerRpEu7", "replyto": "DtxerRpEu7", "signatures": ["ICLR.cc/2026/Conference/Submission8164/Reviewer_pv9Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8164/Reviewer_pv9Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653269279, "cdate": 1761653269279, "tmdate": 1762920127318, "mdate": 1762920127318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ConfProBench, a benchmark designed to evaluate the step-level confidence reliability of multimodal process judges (MPJs). The benchmark perturbs reasoning steps at three levels—lexical, syntactic, and multimodal—to assess whether MPJs’ confidence estimates are robust, sensitive and well-calibrated. Corresponding metrics are introduced to quantify each aspect, offering a framework for confidence evaluation. Over ten multimodal large language models (MLLMs) are tested, revealing gaps in robustness and calibration for confidence performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an underexplored aspect of multimodal process judges (MPJs), i.e., their step-level confidence reliability. The proposed framework introduces three complementary metrics that assess robustness, sensitivity, and calibration.\n\n- The benchmark incorporates adversarial variants at the lexical, syntactic, and multimodal levels, which are constructed to preserve semantic meaning while effectively stressing the model’s confidence robustness. \n\n- The analysis provides valuable insights into the confidence behavior of current MLLMs,  such as revealing clear contrasts between thinking vs. non-thinking models and open-source vs. closed-source systems."}, "weaknesses": {"value": "- The lexical and syntactic perturbations are generated using GPT-4o, which may inadvertently advantage OpenAI models during evaluation. The paper should explicitly discuss this potential bias and clarify whether additional models or cross-validation methods were used to mitigate it.\n- The Data Quality Control section lacks essential information, such as the number and expertise of annotators, inter-annotator agreement scores, and rejection or revision rates. \n- The paper reports calibration metrics but does not analyze how confidence correlates with actual correctness before and after perturbations. \n- Although 14 MLLMs are evaluated, the benchmark omits the latest models such as Gemini-2.5-Pro and GPT-5."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R1MMUkRcA9", "forum": "DtxerRpEu7", "replyto": "DtxerRpEu7", "signatures": ["ICLR.cc/2026/Conference/Submission8164/Reviewer_Aftm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8164/Reviewer_Aftm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832378125, "cdate": 1761832378125, "tmdate": 1762920126656, "mdate": 1762920126656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this draft, the authors introduced ConfProBench, a benchmark designed to evaluate the reliability of step-level confidence scores generated by MLLM-based Process Judges (MPJs). Three metrics were proposed to capture difference aspects: robustness, sensitivity, and calibration. Experiments were conducted with both open-source MLLMs and proprietary MLLMs to show the results on ConfProBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The focus on step-level confidence is relatively new and could provide more information and supervision in building more advanced reasoning chains.\n2. The proposed three metrics cover a whole spectrum of aspects, from robustness, to sensitivity and calibration.\n3. Extensive experiments were conducted on both open-source and proprietary models to show how they perform on the proposed ConfProBench benchmark with the proposed metrics.\n4. Writing is good and easy to follow."}, "weaknesses": {"value": "1. The data construction process seems heavily rely on MLLMs themselves such as GPT-4o to generate different perturbations. While it automate the data curation process and make it more scalable, it is a bottleneck and capped by the capability of the models (i.e., GPT-4o). Should we get a new version of the dataset every time a more advanced model come out?\n2. The verbalized confidence used in the draft might be more prompt-dependent than more intrinsic methods (e.g., logit-based).\n3. For Table 2, it is clear that proprietary models are significantly better than open-sourced ones for CSS and CCS. However, for CRS, the open-sourced models are much better. Can we say that the proprietary models are worse than open-sourced ones for CRS or more investigations are needed to valid the data curation process and the metrics?"}, "questions": {"value": "Please refer to the paper weakness section and provide more justification on the proposed benchmark, metrics, and the evaluation results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xjXEeuucOW", "forum": "DtxerRpEu7", "replyto": "DtxerRpEu7", "signatures": ["ICLR.cc/2026/Conference/Submission8164/Reviewer_C61o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8164/Reviewer_C61o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979568473, "cdate": 1761979568473, "tmdate": 1762920126304, "mdate": 1762920126304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}