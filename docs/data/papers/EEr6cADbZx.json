{"id": "EEr6cADbZx", "number": 12186, "cdate": 1758206215081, "mdate": 1759897526485, "content": {"title": "Back to Square Roots: An Optimal Bound on the Matrix Factorization Error for Multi-Epoch Differentially Private SGD", "abstract": "Matrix factorization mechanisms for differentially private training have emerged as a promising approach to improve model utility under privacy constraints. In practical settings, models are typically trained over multiple epochs, requiring matrix factorizations that account for repeated participation. Existing theoretical upper and lower bounds on multi-epoch factorization error leave a significant gap. In this work, we introduce a new explicit factorization method, Banded Inverse Square Root (BISR), which imposes a banded structure on the inverse correlation matrix. This factorization enables us to derive an explicit and tight characterization of the multi-epoch error. We further prove that BISR achieves asymptotically optimal error by matching the upper and lower bounds. Empirically, BISR performs on par with the state of the art factorization methods, while being simpler to implement, computationally efficient, and easier to analyze.", "tldr": "", "keywords": ["Matrix Factorization", "Differential Privacy", "Machine Learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f92454d04f5a4cd1f2d68bbb0e6d47670496f706.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of adding correlated noise to elements in a data stream with a momentum structure in a differentially private manner under continuous observation, while maintaining low approximation error. The authors propose a novel factorization mechanism for injecting correlated noise at each iteration, providing formal differential privacy guarantees even against an adversary with access to all entries of the stream.\n\nTheir approach improves upon existing methods by factorizing the workload matrix, extending the square-root factorization framework studied in prior work. This refined factorization yields tighter approximation error bounds. Furthermore, the authors establish matching lower bounds on the achievable approximation error for general classes of stream structures, demonstrating the optimality of their approach.\n\nThe proposed mechanism has clear practical relevance, particularly for differentially private stochastic gradient descent (DP-SGD) with momentum. It enables more efficient and accurate private training of deep learning models, highlighting the work’s potential impact on privacy-preserving machine learning."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 4}, "strengths": {"value": "This paper is very strong in terms of results:\n1. The paper presents rigorous theoretical results based on novel techniques that effectively close a significant gap in the existing literature on matrix factorization mechanisms within the context of streaming differential privacy covering a large variety of tasks.\n\n2. The paper synthesizes concepts from several related domains to develop a generalized formulation of DP-SGD with momentum, extending beyond existing approaches in the literature.\n\n3. The paper proposes a computationally efficient method to compute correlated noises. It provides a practical technique to implement these correlated noise techniques for settings which are not simple SGD and attempt to break into the practical scenario in existing optimization techniques used for differentially private deep learning.\n\n4. Practically the algorithm performs very well compared to existing matrix factorization methods for DP-SGD on classification datasets."}, "weaknesses": {"value": "The main weakness of this paper lies in its presentation. While the technical results appear sound and potentially impactful, the exposition makes it difficult for readers to fully grasp the scope and implications of the work (especially from the lower bound perspective). Below are some specific comments and suggestions:\n1. *Dependence on Prior Work for Context*: The paper relies heavily on Kalinin & Lampert (2024) to set up the background for its lower bound results. For instance, the “multi-participation setting” introduced in Theorem 3 is not defined anywhere in the paper and an appropriate reference has not been given as well, which makes it hard to understand the exact conditions under which the stated bounds apply.\n\n*Suggestion*: It would be helpful if the authors could briefly explain or restate the definition of the multi-participation setting, even if it was introduced in prior work. A short contextual description would make the paper more self-contained and easier to follow.\n\n2. *Ambiguity in Notation*: Some notations are not clearly introduced. In particular, the terms $\\Omega_\\alpha$ and $\\Omega_{\\alpha, \\beta}$ in Theorem 3, along with their Big-O counterparts in Theorem 4 and Corollary 1, are not defined.\n\n*Suggestion*: Please clarify the meaning of these notations, either directly in the text or in a concise notation table.\n\nIncluding short background explanations—perhaps in an appendix if not in the main text—would greatly improve the paper’s readability. While it is understandable that some results depend on established frameworks, providing minimal context would help readers appreciate the contributions without needing to refer extensively to other works."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CLNPulQq0O", "forum": "EEr6cADbZx", "replyto": "EEr6cADbZx", "signatures": ["ICLR.cc/2026/Conference/Submission12186/Reviewer_xJi2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12186/Reviewer_xJi2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761522617335, "cdate": 1761522617335, "tmdate": 1762923136418, "mdate": 1762923136418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new algorithm banded inverse square root (BISR), which is an extension to BSR from (Kalinin & Lampert 2024) by imposing a banded structure on the inverse matrix $C^{-1}$ rather than $C$ itself.\n\nBy making such modification, the authors are allowed to give a more explicit expression error with respect to the bandwidth $p$ and reduce the overall computational complexity. The authors also show a tight error bound on the approximation error with matching lower bounds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I find the idea of bounding the bandwidth of matrix on matrix $C$ to reduce computational complexity interesting and the optimal error bound a solid contribution. Also, I appreciate the authors' efforts make to make a comprehensive discussion and comparison with prior work."}, "weaknesses": {"value": "1. The algorithmic modification are relatively minimal. The general structure of the algorithm directly follow that in (Kalinin & Lampert 2024) which slightly weaken the contribution of this work.\n2. More discussion is needed on the approximation error defined in equation (1) . In particular, how is this error related to the convergence error? Would achieving an optimal bound on this error also imply an optimal convergence rate? It would be great if the authors can provide explicit convergence rates for certain loss function classes (e.g., convex-Lipschitz loss) and make comparisons with existing DP algorithms like DP-SGD.\n3. The algorithm, claimed as computational efficient, may not be suitable for modern model training. To achieve optimal rate, $p$ needs to be set as $\\tilde{O}(b)$ where $b$, as far as I understand, can be approximated as the number of update steps per epoch. In large-scale training, this number can reach tens of thousands or even millions. Therefore, combining $p$ different $Z_i$'s in each update step could become prohibitively expensive in practice."}, "questions": {"value": "Questions are included in the \"Weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lGxMLMSvZw", "forum": "EEr6cADbZx", "replyto": "EEr6cADbZx", "signatures": ["ICLR.cc/2026/Conference/Submission12186/Reviewer_2RDV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12186/Reviewer_2RDV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772572282, "cdate": 1761772572282, "tmdate": 1762923135944, "mdate": 1762923135944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the matrix factorization mechanism for differentially private stochastic gradient descent (DP-SGD) under multi-epoch/multi-participation settings.\nThe matrix factorization mechanism is used to inject correlated noise into gradients during training by means of a correlation matrix.\nThe authors propose to enforce a banded structure on the _inverse_ of the correlation matrix, to derive explicit upper bounds on factorization errors and to prove asymptotic optimality.\nThe authors compare their approach with existing techniques, showing that their proposed approach achieves higher or comparable accuracy for large matrices.\nMoreover, the paper proposes an efficient, low-memory method which matches the performance of state-of-the-art approaches while being more efficient."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Theoretical contribution.**\nThe paper introduces and discusses a matrix factorization technique with provable optimality, and refines prior existing bound (Kalinin and Lampert, 2024).\nUnlike related work, you provide an explicit dependence on the bandwidth $p$ and on the participation $b$, which leads to more useful guarantees.\nThe idea to consider the inverse correlation matrix instead of the matrix itself is, as far as I can tell, novel and elegant.\nMoreover, the discussion on an efficient implementation reinforces the practical utility of your approach.\nThe problem discussed is very relevant and well placed within related literature.\n\n* **Empirical validation.**\nThe empirical validation you present is limited but consistent, and qualitatively supports your claims.\nIn particular in low-resource regimes, the presented approach performs better than existing ones."}, "weaknesses": {"value": "* **Clarity and accessibility.** The paper has dense notation and long proofs: intuition could be introduced earlier.\nFor instance, the benefits of inverse banding are not intuitively clear, and visualizations could help here.\n\n* **Low privacy regime.**\nIn your empirical evaluation, you only present results in a arguably low privacy regime $\\epsilon=9$.\nWhile this specific value for the privacy budget seems to be common in related literature, it is generally understood to be at the edge of what is considered to be differentially private at all.\nIf my understanding is correct, the benefits of one specific factorization technique against another, is particularly important when the amount of noise added is small.\nWhile this justifies the chosen privacy regime, it may diminish the contribution for more strict, and therefore relevant from a privacy perspective, privacy regimes ($\\epsilon <= 1$) where DP constraints are more meaningful.\n\n* **Empirical relevance.**\nFollowing from my previous point, I am not convinced of the practical relevance of the approach.\nWhile I understand that your contribution is, first of all, theoretical, a comparison with more recent DP-SGD variants would further justify its practical relevance.\nThe plots do not show any measure of dispersion (e.g., error bars).\nThe empirical setup does not seem to reflect a real-world application with realistic privacy requirements, and no investigation of the effectiveness of the approach with different privacy budgets is performed.\nFrom the plots, the correspondence between RMSE and accuracy is difficult to grasp.\n\n### Other remarks\n\n* The nomenclature is at times confusing.\nBoth $C$ and $C^{-1}$ are referred to as \"correlation matrix\" throughout the introduction (e.g., compare lines 45 and 59/61)."}, "questions": {"value": "* How practically (or theoretically) relevant is your approach for stricter privacy regimes, i.e., small values of $\\epsilon$?\n* What is the standard deviation/variability of the results reported in Figure 1/2? Are the results significantly different if you include error bands in the plots/results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KopaipFk6e", "forum": "EEr6cADbZx", "replyto": "EEr6cADbZx", "signatures": ["ICLR.cc/2026/Conference/Submission12186/Reviewer_53th"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12186/Reviewer_53th"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983098924, "cdate": 1761983098924, "tmdate": 1762923135546, "mdate": 1762923135546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and analyzes a novel matrix factorization scheme, called BISR, which can be applie to DP-SGD and relies on (i) computing the inverse square root of the workload's matrix, (ii) imposing a band structure on it, and (iii) inverting the resulting matrix again.\nA novel lower bound on factorization error for SGD's workload matrix is derived, with matching upper bound for the BISR method, which relies on band structure of the inverse square root.\nNumerical experiments show that the resulting factorizations are consistently on par than the existing BSR mechanism, and strongly outperform it for some choices of bandwidth."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The proposed method, relying on imposing structure on the inverse of SGD's workload matrix squared root, is original and very different from existing ideas.\n2. A refined theoretical lower bound on the factorization of SGD's workload matrix is provided.\n3. The BISR method is shown to match this upper bound.\n4. BISR is shown numerically to provide better factorization in many setting that existing methods, and this improved factorization precision results in improved accuracy over existing method in multiple private machine learning tasks.\n5. The paper is very well written and easy to read."}, "weaknesses": {"value": "The paper is very interesting, and I mostly remark strengths about the contributions. Nonetheless, there are some minor weaknessses:\n1. No theoretical guarantees for DP-SGD under the BISR matrix factorization are provided.\n2. While the theoretical claims hint for a large improvement over the BSR method, this does not always show in practice; studying more precisely (i.e., non-asymptotically) the respective behaviour of the two methods may reveal more subtle compromises.\n3. The experiments on CIFAR-10 and IMDB are performed in the small bandwidth and low privacy regime: it is not clear whether the same conclusions would hold in high-privacy/large bandwidth regimes."}, "questions": {"value": "1. Is it possible to derive theoretical convergence guarantees for DP-SGD in simple settings (e.g., strongly-convex functions)? In this setting, is there a chance to observe the true metrics on matrix factorization approximation that impact the final privacy-utility trade-off?\n2. Authors claim that the RMSE may not be a good proxy for approximation error. Are there other candidates for better proxies?\n3. Experiments showcase the low privacy regime: how would the result change in a high privacy regime? Would BISR still be better? Would the difference increase/decrease?\n4. What is the intuition why the inverse square root should be closer to a band structure than the square root itself?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6oVXv8gIbB", "forum": "EEr6cADbZx", "replyto": "EEr6cADbZx", "signatures": ["ICLR.cc/2026/Conference/Submission12186/Reviewer_7NNd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12186/Reviewer_7NNd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762366210719, "cdate": 1762366210719, "tmdate": 1762923135194, "mdate": 1762923135194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}