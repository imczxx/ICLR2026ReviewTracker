{"id": "wyIlqIMMnZ", "number": 4396, "cdate": 1757672183926, "mdate": 1759898034750, "content": {"title": "N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout", "abstract": "In mobile manipulation, the manipulation policy has strong preferences for initial poses where it is executed. However, the navigation module focuses solely on reaching the task area, without considering which initial pose is preferable for downstream manipulation. To address this misalignment, we introduce N2M, a transition module that guides the robot to a preferable initial pose after reaching the task area, thereby substantially improving task success rates. N2M features five key advantages: (1) reliance solely on ego-centric observation without requiring global or historical information; (2) real-time adaptation to environmental changes; (3) reliable prediction with high viewpoint robustness; (4) broad applicability across diverse tasks, manipulation policies, and robot hardware; and (5) remarkable data efficiency and generalizability. We demonstrate the effectiveness of N2M through extensive simulation and real-world experiments. In the PnPCounterToCab task, N2M improves the averaged success rate from 3% with the reachability-based baseline to 54%. Furthermore, in the Toybox Handover task, N2M provides reliable predictions even in unseen environments with only 15 data samples, showing remarkable data efficiency and generalizability.", "tldr": "N2M bridges the gap between navigation and manipulation by predicting preferred initial poses from ego-centric point clouds. It adapts in real time (>30 Hz), needs only a few rollouts for training, and generalizes well to unseen environments.", "keywords": ["mobile manipulation", "policy evaluation and deployment", "3D representation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b510030daada137601c1d168cadf1885a3fc40e2.pdf", "supplementary_material": "/attachment/e56816feadc6d62aafb7cc7aa269b6569421ec66.zip"}, "replies": [{"content": {"summary": {"value": "This paper identifies a critical misalignment in mobile manipulators: navigation modules deliver the robot to a task area, but not necessarily to a preferable initial pose for the manipulation policy. To bridge this gap, the authors introduce N2M, a lightweight transition module that uses only ego-centric observations to predict and guide the robot to a pose that maximizes manipulation success. Experiments in both simulation and real-world environment demonstrate the effectiveness of N2M."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Bridging off-the-shelf navigation and fixed-base manipulation policies with a separate fine-grained navigation model is of great application value.\n2. N2M can handle dynamic scene, being viewpoint-robust and data-efficient."}, "weaknesses": {"value": "1. More discussion is required with related works, such as MoManipVLA [1], Mobi-Pi and MoTo. All these methods empowers off-the-shelf fixed-base manipulation policy with mobile capability. A more comprehensive discussion is suggested to be placed in Introduction and Experiments.\n\n2. The above mentioned methods are training-free, but N2M requires rollout from the manipulation policy to train. The overall data collection and training time should be reported to make a fair comparison.\n\n\n\n\n[1] MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation, CVPR 2025."}, "questions": {"value": "After predicting the camera pose, how to efficiently optimize a path to navigation to this location?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GbboF4vPRB", "forum": "wyIlqIMMnZ", "replyto": "wyIlqIMMnZ", "signatures": ["ICLR.cc/2026/Conference/Submission4396/Reviewer_ycFo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4396/Reviewer_ycFo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461724226, "cdate": 1761461724226, "tmdate": 1762917337904, "mdate": 1762917337904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical misalignment in mobile manipulation pipelines, where navigation modules deliver the robot to a task area without considering the optimal initial pose for the subsequent manipulation policy. To bridge this gap, the authors introduce N2M, a transition module that re-positions the robot into a preferable initial pose after navigation. The core strength of N2M lies in its practical and efficient design, which operates on egocentric observations and demonstrates exceptional data efficiency and generalizability. Its key advantages are convincingly demonstrated through a dramatic performance improvement, elevating success rates from a baseline of 3% to 54% in one task, and showing reliable operation in unseen environments with only minimal data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The field of mobile manipulation, which this paper addresses, represents a relatively nascent yet highly challenging research area.\n2. The authors propose N2M, a transitional module designed to reposition the robot into a more favorable initial pose following the completion of navigation.\n3. Extensive experimental results are provided to validate the effectiveness of the proposed method and the rationality of its design."}, "weaknesses": {"value": "1. Does the pose in this paper refer to the position of the robot base or the position of the robot gripper?\n2. The generalizability of the method to novel viewpoints should be further evaluated.\n3. How would the system adjust the robot's position in case of partial occlusions?\n4. Could the authors elaborate on the training methodology for the policy illustrated in Figure 5(b)?"}, "questions": {"value": "please see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no ethics concerns"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9wRT8i5CQR", "forum": "wyIlqIMMnZ", "replyto": "wyIlqIMMnZ", "signatures": ["ICLR.cc/2026/Conference/Submission4396/Reviewer_tyZd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4396/Reviewer_tyZd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904051385, "cdate": 1761904051385, "tmdate": 1762917337553, "mdate": 1762917337553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In mobile manipulation, directly start manipulation from where the navigation ends may be ineffective, as the manipulation policy has preferred starting locations. To address this issue, this paper proposes to rollout the manipulation policy at different locations and fitting a distribution of successful locations, which could be used to generate navigation end points."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written, with clear description of its technical challenges and solutions. All figures are helpful for getting across the proposed method and the experiment setups.\n2. The authors conduct a large number of experiments to validate the proposed method, both in simulation and in the real world."}, "weaknesses": {"value": "My major concern is the novelty of the proposed method. Though I am not very familiar with mobile manipultion field, according to my limited understanding, finding an appropriate starting pose for manipulation policy is a classic problems in the filed, and there are already several prior papers focusing on this challenge [1][2]. It's unclear to me, compared to them, in which way the proposed method is more novel and contributes to solving this questions.\n\n[1] Iriondo, Ander, et al. \"Learning positioning policies for mobile manipulation operations with deep reinforcement learning.\" International journal of machine learning and cybernetics 14.9 (2023): 3003-3023.\n[2] Jauhri, Snehal, Jan Peters, and Georgia Chalvatzaki. \"Robot learning of mobile manipulation with reachability behavior priors.\" IEEE Robotics and Automation Letters 7.3 (2022): 8399-8406."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "bEwYPJg9HF", "forum": "wyIlqIMMnZ", "replyto": "wyIlqIMMnZ", "signatures": ["ICLR.cc/2026/Conference/Submission4396/Reviewer_RoPP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4396/Reviewer_RoPP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960491967, "cdate": 1761960491967, "tmdate": 1762917337134, "mdate": 1762917337134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the misalignment between navigation and manipulation in mobile robots, where navigation focuses solely on reaching task areas without considering which initial poses are preferable for executing manipulation policies. The authors propose N2M (Navigation-to-Manipulation), a transition module that predicts multi-modal distributions of preferable initial poses using Gaussian Mixture Models from ego-centric RGB point clouds. The key technical contributions include: (1) learning pose preferences directly from policy rollouts, treating policies as black boxes without requiring access to internals or training data, (2) a viewpoint augmentation strategy that renders observations from multiple angles during training, enabling viewpoint robustness and data efficiency with only 10-15 rollouts, and (3) real-time adaptation through single forward pass prediction from ego-centric observations without global scene reconstruction. Experimental validation includes simulation results across 4 tasks and 3 policy architectures in RoboCasa showing dramatic improvements (3% → 54% success rate on PnPCounterToCab), and real-world deployment on 5 tasks demonstrating 10 consecutive successes with randomized configurations and generalization to unseen environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**\n\nNovel problem formulation: The paper clearly identifies and formalizes the navigation-manipulation misalignment problem, which has been overlooked despite its criticality. The key innovation is **learning pose preferences directly from rollouts**, treating policies as black boxes—a creative departure from prior work requiring specific policy types (RL for value functions, IL for distributional similarity) or access to training data.\n\nTechnical innovation: The combination of GMM-based distribution modeling with viewpoint augmentation is elegant. Notably, viewpoint augmentation simultaneously addresses multiple challenges (robustness, data efficiency, generalization)—an emergent insight that represents a valuable contribution beyond the primary problem.\n\n**Quality**\n\nComprehensive experiments spanning 4 simulation tasks with 3 policy architectures, systematic data efficiency analysis, controlled generalization studies (texture vs. layout), and 5 real-world tasks. The ablation study confirms viewpoint augmentation's importance, and attention visualizations (Figure 12) provide interpretability. The paper also acknowledges N2M exceeding oracle performance and discusses policy preferences versus training distributions. However, real-world tasks (b)-(e) use manual rules rather than actual policies, slightly weakening end-to-end claims. More failure case analysis would strengthen the work.\n\n**Clarity**\n\nWell-structured: Clear progression from problem to solution to validation. The five-challenge framework effectively organizes the approach, and visualizations strongly support claims—particularly the multi-modality illustration (Figure 2), success heatmaps (Figure 9), and generalization demonstrations (Figure 10).\n\nMinor gaps: Some technical details need clarification—coordinate frame transformations, kernel number selection rationale (K=2 vs K=1), and the quantitative impact of augmentation ratio M=300 on final performance.\n\n**Significance**\n\nPractical impact: Addresses a fundamental bottleneck in mobile manipulation with dramatic improvements (3% → 54% success rates). The data efficiency (10-15 rollouts), broad applicability across tasks/policies/hardware, and ego-centric design lower adoption barriers significantly.\n\nResearch contributions: Opens research directions on viewpoint augmentation's effectiveness and rollout-based preference learning. The finding that policy preferences diverge from training distributions has broader implications for understanding learned behaviors.\n\nScope limitations: While valuable, the work improves existing modular pipelines rather than proposing transformative paradigms. End-to-end approaches may eventually supersede such bridging solutions. Real-world experiments remain in controlled lab settings. \n\nOverall, this is a solid, practical contribution that will benefit the community but may not be groundbreaking."}, "weaknesses": {"value": "**Problem Importance is Questionable**\n\nBase placement is already solved: Methods like B* [1] provide efficient and optimal base placement. The dramatic improvement over the \"reachability baseline\" (3% → 54%) likely reflects a poorly implemented baseline rather than fundamental limitations of geometric approaches. The paper must compare against state-of-the-art optimization methods (B*, modern inverse reachability maps) to justify that learning is necessary.\n\nEgo-centric observation is not an advantage: This is presented as a benefit but is actually a limitation: (1) Modern robots have mapping capabilities, (2) Methods like Mobi-π plan optimal docking from anywhere using scene reconstruction, while N2M requires navigating close first, (3) No evidence that ego-centric is faster or more robust than global methods.\n\n**Low Technical Novelty**\n\nStandard components: Point-BERT (pre-trained) + MLP + GMM prediction. GMM for robotic poses is established (Bahl et al. 2023), and learning from binary rollout labels is basic preference learning. Architecture (Figure 3) contains no innovation.\n\n\"Learning from rollouts\" overclaimed: This is standard reward-free imitation from success/failure labels. Predicting poses instead of actions (vs. Lee et al. 2019) is a simplification, not innovation.\n\nViewpoint augmentation under-analyzed: The only interesting aspect but: (1) No ablation on M=300, (2) No comparison with simpler augmentations, (3) No mechanistic explanation, (4) \"10-15 rollouts\" is misleading—actually 3000-4500 samples after M=300× augmentation.\n\n**Insufficient Empirical Validation**\n\nMissing critical baselines: No comparison with B* [1], Mobi-π, MoTo, or validation of Lee et al.'s \"1000+ rollouts\" claim.\n\nWeak real-world validation: Only 1 of 5 tasks uses actual learned policy—tasks (b)-(e) use manual rules. Generalization tested on only 5 similar scenes (Section 4.4) or qualitative only (Section 5.2).\n\nMisleading data efficiency: Claims \"10-15 rollouts\" but ignores M=300 multiplier. No human time comparison or empirical validation against alternatives.\n\nNo statistical rigor: Success rates lack error bars or significance tests despite high variance (Figure 6).\n\n\n**Missing Ablations**\n\nNo ablations on: (1) Encoder choice (Point-BERT vs. alternatives), (2) Loss regularization terms and hyperparameters, (3) GMM kernel number K, (4) Fine-tuning vs. frozen encoder."}, "questions": {"value": "**Questions**\n\n**Q1: Quantitative Comparison with Optimization Methods**\nHow does N2M compare with state-of-the-art optimization methods like B* [1]? The 3% → 54% improvement over your reachability baseline suggests a poorly implemented baseline rather than fundamental limitations of geometric approaches. Please provide: (a) success rates for B* or modern inverse reachability methods on your tasks, (b) computational cost comparison, (c) specific scenarios where learning demonstrably outperforms optimization, or (d) acknowledgment if the reachability baseline doesn't represent state-of-the-art geometric methods.\n\n**Q2: Viewpoint Augmentation Analysis and Data Requirements**\nWhy is M=300 necessary, and what are the true data requirements? Claiming \"10-15 rollouts\" when M=300 produces 3000-4500 training samples is misleading. Please provide: (a) ablation varying M ∈ {10, 50, 100, 300} showing performance vs. data trade-offs, (b) comparison with simpler augmentations (random jittering, noise), (c) total human time for data collection vs. alternatives, (d) mechanistic explanation of why geometric rendering improves generalization beyond just data quantity.\n\n**Q3: Real-World Validation with Learned Policies**\nWhy do only 1 of 5 real-world tasks use actual learned manipulation policies? Tasks (b)-(e) use manual rules, weakening end-to-end claims. Please provide: (a) quantitative success rates with learned policies for at least 2 additional tasks, (b) validation that manual rules accurately approximate policy preferences, or (c) explicit acknowledgment as a limitation with discussion of why full policy validation wasn't feasible.\n\n**Q4: Missing Comparisons and Ablations**\nWhat are the quantitative comparisons with related work and ablations on design choices? Please provide: (a) success rate and efficiency comparison with Mobi-π and MoTo on shared tasks, (b) encoder comparison (Point-BERT vs. PointNet++/PointTransformer), (c) loss function regularization ablation with hyperparameter values (αw, αdist, αmode), (d) empirical validation of data efficiency claims vs. Lee et al. 2019, or acknowledge which comparisons/ablations are infeasible.\n\n**Suggestions**\n\n**S1: Provide Statistical Rigor and Failure Analysis**\nAll success rates lack confidence intervals despite high variance. Include: (a) error bars and significance tests for all results, (b) failure mode breakdown (why 0.0 success in some Figure 9 cells?), (c) conditions where viewpoint robustness or generalization fails, (d) sensitivity to depth noise and partial occlusions. This analysis is critical for understanding practical limitations and deployment reliability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jc87gUasvs", "forum": "wyIlqIMMnZ", "replyto": "wyIlqIMMnZ", "signatures": ["ICLR.cc/2026/Conference/Submission4396/Reviewer_77AG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4396/Reviewer_77AG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995772683, "cdate": 1761995772683, "tmdate": 1762917336863, "mdate": 1762917336863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}