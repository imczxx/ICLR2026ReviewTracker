{"id": "xRjOrcj08o", "number": 22298, "cdate": 1758329218272, "mdate": 1759896874069, "content": {"title": "Conformalized Decision Risk Assessment", "abstract": "High-stakes decisions in healthcare, energy, and public policy have long depended on human expertise and heuristics, but are now increasingly supported by predictive and optimization-based tools. A prevailing paradigm in operations research is predict-then-optimize, where predictive models estimate uncertain inputs and optimization models recommend decisions. However, such approaches often sideline human judgment, creating a disconnect between algorithmic outputs and expert intuition that undermines trust and adoption in practice.\nTo bridge this gap, we propose CREDO, a framework that, for any candidate decision proposed by human experts, provides a distribution-free upper bound on the probability of suboptimality---informed by both the optimization structure and the data distribution. By combining inverse optimization geometry with conformal generative prediction, CREDO delivers statistically rigorous yet practically interpretable risk certificates. This framework allows human decision-makers to audit and validate their decisions under uncertainty, strengthening the alignment between algorithmic tools and human intuition.", "tldr": "A framework for quantifying the probability that a candidate decision is suboptimal in an optimization setting", "keywords": ["Conformal prediction", "inverse optimization", "risk assessment", "decision making under uncertainty"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bab71b5db9c76cfc31d3160137f63320645cafca.pdf", "supplementary_material": "/attachment/d58a7b132dbef1db24e728ab00ca55b369ed2bdf.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new framework that can evaluate the probability of suboptimality for any decision with strong statistical guarantees. The authors reformulate the probability of suboptimality as the probability of the outcome variable belonging to an inverse feasible region. This probability can then be estimated using conformal prediction sets of varying levels of marginal coverage produced from $K$ samples from a generative model. The method is validated on two synthetic datasets and a real-world infrastructure planning problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and well-organized. The theoretical contributions are meaningful in validating the proposed method. For example, Proposition 3 justified the use of generative models very well. This framework is a novel approach to handling risk in decision-making by quantifying it directly rather than being robust to it during optimization."}, "weaknesses": {"value": "The authors compare their method with other robust decision-making methods using empirical confidence ranking. However, I am not convinced that this is the best metric to evaluate decision quality. How is the predicted decision’s rank in terms of its frequency in the ground truth optimal decision set informative of decision quality? This metric is instance-independent, and so choosing an action $z$ that is optimal (i.e., $z \\in \\pi(Y; \\theta)$), but rare among the optimal set of actions in the test set, would be discouraged by this metric. That doesn’t seem like a fair assessment of decision quality. An experiment evaluating decision quality seems crucial in building a case for viewing robust decision-making through a different lens. If the authors can clearly and strongly justify this metric or reproduce this experiment with a metric more indicative of decision quality, I will reconsider my score. \n\nAdditionally, I believe that Kiyani et al. (2025) seems quite relevant to this line of work; however, it wasn’t included in the experiments. I believe adding this baseline can strengthen the paper.\n\n_References_\n* Kiyani et al. (2025), Decision theoretic foundations for conformal prediction: Optimal uncertainty quantification for risk-averse agents. https://arxiv.org/pdf/2502.02561."}, "questions": {"value": "* In Figure 5 Column 2, why isn’t the Point Model just a flat line? It shouldn’t be changing with $K$ increasing.\n* Why isn’t NS included in Columns 2 and 3 of Figure 5?\n* Column 3 of Figure 5 appears to be inaccurately interpreted (Lines 432-437). While high accuracy, in the typical sense, indicates better performance, “accuracy”, as defined by the authors, seems to be like a loss (absolute difference between true and estimated risk). So, shouldn’t the method with lower “accuracy” be better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rSflRQ7yme", "forum": "xRjOrcj08o", "replyto": "xRjOrcj08o", "signatures": ["ICLR.cc/2026/Conference/Submission22298/Reviewer_pzmF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22298/Reviewer_pzmF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876417037, "cdate": 1761876417037, "tmdate": 1762953942459, "mdate": 1762953942459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework that provides distribution-free upper bounds on the probability that a given decision is suboptimal. The authors use inverse optimization space of the outcome and then construct a conformal set that is contained in the inverse space to produce the upper bound on the probability of suboptimal decision. The authors give a computational efficient algorithm when the objective function is a linear combination of decision and outcome. The authors validate the framework by experiments on synthetic and power-grid planning datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's motivation is practical. The interpretable risk assessment in high-stakes domains is widely applicable and important.\n- The theorems on conservativeness and the Monte Carlo interpretation demonstrate sound reasoning and attention to statistical guarantees.\n- The idea of using inverse optimal space of outcome to find an upper bound on distribution-free probability is refreshing and can lead to potentially stronger results."}, "weaknesses": {"value": "- The computational efficiency of the framework is not discussed. Finding the inverse space of the outcome where a given decision is optimal can be NP-hard for any objective function. It is also NP-hard to check whether the conformal set is included by the inverse space. The radius assumption here is still not enough since the inverse space can be non-convex.\n- The upper bound that is found by the framework could be arbitrarily bad. That says there is no result on the lower bound of the probability of decision being suboptimal. I'm having this worry especially because the conformal set algorithm constructs the candidate space as a naive radius space. It is very easy to construct a case where the radius is arbitrarily small such that $\\alpha$ is arbitrarily large.\n- The linear form of objective function is not a very general form. A lot of utility functions in decision-making such as brier score cannot be converted to linear form.\n- I found the paper is a little hard to follow. Some properties are not discussed. See more details in questions."}, "questions": {"value": "- What are the intuitions on the generative model $\\hat{f}$? How does it impact the quality of $\\alpha$? Is there any guideline for choosing $\\hat{f}$?\n- Why the radius version of conformal set is taken instead of the more general one?\n- What does this repeat K times do? What is the random variable here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9JVD1ZAHFH", "forum": "xRjOrcj08o", "replyto": "xRjOrcj08o", "signatures": ["ICLR.cc/2026/Conference/Submission22298/Reviewer_TYnS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22298/Reviewer_TYnS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971098550, "cdate": 1761971098550, "tmdate": 1762942158317, "mdate": 1762942158317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CREDO provides a distribution-free upper bound on the probability that a candidate decision is suboptimal, using inverse optimization geometry and conformal prediction with generative models. It enables practitioners to audit both algorithmic and expert-proposed decisions, offering statistically valid risk certificates. Theoretical guarantees and empirical results on synthetic and real-world tasks demonstrate that CREDO delivers conservative, interpretable, and actionable risk estimates, improving trust and decision quality compared to standard PTO and robust optimization approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an under explored research area about how to provide rigorous, interpretable risk certificates for candidate decisions in high stakes, uncertain environments.\n2. The \"decide-then-assess\" paradigm is a reasonable variation from the standard \"predict-then-optimize\" pipeline, and is well-motivated by practical needs for human-AI collaboration.\n3. The use of inverse optimization geometry to characterize the optimality region for a decision is well done.\n4. The integration of conformal prediction with generative models for risk estimation and the corresponding theoretical guarantees are clearly stated and proved.\n5. The closed form solution for linear programs makes the method practical for large scale problems.\nThe experiments are well designed, covering both synthetic and real world settings (e.g., power grid planning)."}, "weaknesses": {"value": "1. While the method is general, the closed form efficiency is only for linear programs. For nonlinear or combinatorial problems, the computational cost of characterizing the inverse feasible region may be significant.\n2. The approach assumes access to a well calibrated conditional model for the uncertain parameters. While the paper uses generative models to estimate the conditional distribution, in practice, any model that can accurately capture and sample from P(Y∣X) would suffice, including parametric or non-parametric approaches. I think it is limiting to claim the importance of generative models in this use case.\n3. The method is conservative by design, but this can lead to loose risk estimates in some settings. The paper discusses this tradeoff, but more empirical analysis of the \"tightness\" of the certificates would strengthen the work.\n4. The paper is motivated by human AI collaboration, but there is little discussion or experimentation on how practitioners actually use or interpret the risk certificates. A user study or qualitative feedback would be valuable.\n5. The paper positions itself relative to robust optimization, DRO, and conformal prediction, but could more deeply discuss how CREDO compares to recent advances in human-in-the-loop optimization."}, "questions": {"value": "1. For general nonlinear or combinatorial optimization problems, how is the inverse feasible region (\\pi^{-1}(z;\\theta)) practically characterized? Are there efficient relaxations that maintain the validity of the risk certificate, or does the method require exact computation?\n2. The framework uses conformal prediction with generative models to construct inner approximations of the inverse feasible region. How sensitive is the risk estimate to the choice of conformal set (e.g., L2 balls vs. other shapes)?\n3. What are the theoretical or empirical sample complexity requirements for the calibration set to ensure valid and non-trivial risk certificates, especially as the dimension of Y increases? How does the method perform with limited calibration data?\n4. Can the CREDO framework be extended to settings where decisions are made sequentially or in multiple stages, with uncertainty revealed over time? What are the main challenges or limitations in such extensions?\n5. Beyond generative models, have the authors empirically compared CREDO with approaches using quantile regression, Bayesian models, or ensemble methods for conditional uncertainty estimation?\n6. Can the authors provide a more detailed analysis of the computational complexity of CREDO for both the linear and general cases, including the cost of generating samples, constructing conformal sets, and evaluating the inverse feasible region?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5ruzAQKd8z", "forum": "xRjOrcj08o", "replyto": "xRjOrcj08o", "signatures": ["ICLR.cc/2026/Conference/Submission22298/Reviewer_XFwE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22298/Reviewer_XFwE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983248653, "cdate": 1761983248653, "tmdate": 1762942157902, "mdate": 1762942157902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}