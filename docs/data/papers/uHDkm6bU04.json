{"id": "uHDkm6bU04", "number": 4888, "cdate": 1757784688462, "mdate": 1759898007141, "content": {"title": "Vec2Pix: Controllable Image Synthesis via Semantic-aligned Vector Graphics", "abstract": "Recent advances in image generation have achieved remarkable visual quality, while a fundamental challenge remains: Can image generation be controlled at the element level, enabling intuitive modifications such as adjusting shapes, altering colors, or adding and removing objects? In this work, we address this challenge by introducing layer-wise controllable generation through simplified vector graphics (VGs). Our approach first efficiently parses images into hierarchical VG representations that are highly semantic-aligned and structurally coherent. Building on this representation, we design a novel image synthesis framework guided by VGs, allowing users to freely modify elements and seamlessly translate these edits into photorealistic outputs. By leveraging the structural and semantic features of VGs in conjunction with noise prediction, our method provides precise control over geometry, color, and object semantics. Extensive experiments demonstrate the effectiveness of our approach in diverse applications, including image editing, object-level manipulation, and fine-grained content creation, establishing a new paradigm for controllable image generation.", "tldr": "", "keywords": ["Controllable Generation", "Diffusion Models", "Vector Graphics"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cb7f071201fbc8c714cf569edc23908ad54fea01.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper suggests a new controllable image editing pipeline - leveraging SVG to edit images using flow-matching based models.\nSVG enables a layerwise, easy to control domain for editability.\nThe paper details a complete framework, transforming images to SVG and back to images, enabling a regeneration loop for user-friendly editing."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is mostly well-written, polished, and easy to follow.\n- The qualitative results are impressive, and the suggested interface looks practical.\n- The authors are willing to publish their dataset and code for future research."}, "weaknesses": {"value": "- Limited Novelty - While the pipeline is effective, most of its components are already published, and the novelty of chaining them together is relatively modest\nSpecifically, the flexibility enabled by SVG is well known and already used to justify methods like DiffVG or LIVSS.\nAnd, the components used for the image-to-SVG are based on prior works with limited novelty.\nWhile the NPV module appears to suggest a novel alignment strategy, it's not well justified theoretically, and I'm unsure if this solution applies to any image-to-image frameworks or just SVG.\nThis also breaks the IID assumption of the Gaussian noise, as it might be spatially correlated (although supervised). A quantitative measure would be helpful. It might be that spatially correlated noise is beneficial for the editing task, but this would require more justification.\n\n- Limited quantitative experiments - \nSVG representation presents a lot more information than edges and depth.\nA naive baseline, close to the suggested framework, should be included in the quantitative evaluation.\nWhile acknowledging that the method is the first to suggest SVG-to-Image editing pipeline using diffusion, a baseline can be created to emphasize the novelty of the method.\nFor example, a non-layerwise, image-to-SVG method (like Adobe's or any other public method), and use an image-to-image editing framework?\nIn addition, it would be interesting to compare segmentation-guided diffusion editing to the suggested method, as it's a lot closer than edges or depth."}, "questions": {"value": "- According to Figure 2, the VAE encoder's input is a rasterized SVG (image domain), while this is not entirely clear to me from the text. Does the encoder use a rendered image or work directly on the SVG bezier parameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U96mZg5dgb", "forum": "uHDkm6bU04", "replyto": "uHDkm6bU04", "signatures": ["ICLR.cc/2026/Conference/Submission4888/Reviewer_yAJS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4888/Reviewer_yAJS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761203563309, "cdate": 1761203563309, "tmdate": 1762917740359, "mdate": 1762917740359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method that generate images with the condition of vector graphics. Meanwhile, the method perform vectorization of the generated images to enable further editings. The results demonstrated in the paper shows that the editing is effective for rough editing on the curve primitives. However, the undesired changes at the unedited regions make the proposed method suboptimal for image editing task. This issue makes me hesitate of giving higher ratings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The control through vector graphics primitive is straightforward and makes sense.\n- The closed loop of generation and vectorization is useful.\n- Multiple useful applications and scenarios are demonstrated."}, "weaknesses": {"value": "- The proposed method seems like a combination of many existing things. This makes the overall method less novel. For example, the vectorization part is mainly based on LIVSS. And I am not really sure whether the vector-guided image generation part is novel. The condition, i.e., the rough vectorized image, is similar to a segmentation map which has been used for guided generation using ControlNet like method. I am not satisfied with the experiment without the discussion related to this aspect.\n- It is very unclear what exactly the condition is and not clearly described in the paper. Is the rendered layer images? Or the curve parameters?\n- The demonstrated results often introduce undesired visual artifacts, e.g., additional blurs of the girl's hair (Fig. 3) and the change of identity of the house and the shape of the rocks at river shore in Fig. 3. These parts are not the edited region but still got affected, which is not desired. This is critical for image editing task."}, "questions": {"value": "- The overall implementation seems quite complicated because there are many steps. Do the authors consider to release the source code? Meanwhile, I think the Bezier splatting code is not released as well, therefore the implmenetation of the proposed method become more difficult. \n- Why not trying to introduce some preservation strategies for the regions outside the editing region during the image editing? I think this is critical for image editing. The lack of discussing about this matter makes me feel that the proposed method is not suitable for image editing yet."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GUUWol3cpq", "forum": "uHDkm6bU04", "replyto": "uHDkm6bU04", "signatures": ["ICLR.cc/2026/Conference/Submission4888/Reviewer_qeYi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4888/Reviewer_qeYi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462744668, "cdate": 1761462744668, "tmdate": 1762917739803, "mdate": 1762917739803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for element-level controllable image generation by leveraging simplified vector graphics (VGs) as the conditioning representation. Instead of traditional control signals such as sketches, layouts, or depth maps, the method converts images to hierarchical SVG-like vector structures and enables users to edit shapes, colors, and object components directly. A bidirectional loop is introduced, consisting of SVG-guided image synthesis and image-to-SVG parsing via differentiable vectorization and B-spline rendering. A noise-prediction module aligns vector structures with diffusion sampling. Experiments show effective object-level manipulation, layout adjustment, and fine-grained editing across diverse tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. New controllable generation paradigm using vector graphics for semantic and geometric control, beyond typical spatial conditioning (layout/depth/sketch).\n\n2. Closed SVGâ‡Œimage loop enabling iterative refinement and editable structure, bridging design workflows and generative models.\n\n3. Strong technical system: vector parsing, differentiable rasterization, noise alignment, LoRA adapters, and multimodal diffusion integration."}, "weaknesses": {"value": "1. Limited technical novelty. The core method primarily combines an existing hierarchical vector parsing pipeline LIVSS and O&R [2] with a ControlNet-style conditional guidance setup. The overall architecture closely parallels prior works such as Densediffusion [1], making the contribution appear incremental and largely engineering-oriented rather than introducing fundamentally new modeling innovations.\n\n2. Vectorization quality and practical usability remain weak. The hierarchical vector graphics obtained by the proposed pipeline are still coarse and far from production-ready. The editable control via point dragging is highly constrained in practice, offering only limited manipulation capability for complex shapes and semantic elements. As such, the real usability of the system does not fully match the claimed level of controllability.\n\n3. Efficiency and scalability concerns. The system depends on differentiable rendering and per-shape optimization, leading to slow inference and interaction cycles. This latency considerably limits scalability to high-resolution or complex scenes\n\n4. No clear advantage over existing image editing workflows. Compared to modern image editing pipelines (e.g., prompting-based iterative editing, sketch-guided refinement, or mask-based diffusion workflows), the proposed system does not show a clear qualitative advantage. In many cases, the generated results appear similar or even inferior to existing diffusion-based editing methods, weakening the practical motivation.\n\nReference:\n1. Dense Text-to-Image Generation with Attention Modulation\n2. Optimize and Reduce: A Top-Down Approach for Image Vectorization"}, "questions": {"value": "1. Figure 2 and the method description are confusing. Is the condition input to the diffusion model the SVG file itself (e.g., text, paths), or the rasterized pixel image derived from the SVG?\n\n2. What is the backbone of the Flow model shown in Figure 2? If it is DiT, the diagram should not depict a U-Net-style architecture, as this is misleading."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "24j8qzFOvq", "forum": "uHDkm6bU04", "replyto": "uHDkm6bU04", "signatures": ["ICLR.cc/2026/Conference/Submission4888/Reviewer_XUMh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4888/Reviewer_XUMh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946795897, "cdate": 1761946795897, "tmdate": 1762917739509, "mdate": 1762917739509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to tackle the core challenge in image generation: achieving fine-grained, intuitive control over image elements. It proposes a novel framework for layer-wise controllable generation using simplified, semantic-aligned vector graphics as an intermediate representation. By leveraging this intermediate representation within a diffusion-based generative pipeline, Vec2Pix enables element-level control over image synthesis. Experiments demonstrate its effectiveness across various applications, including image editing, object-level manipulation, and fine-grained content creation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of utilizing vector graphics as the control medium is novel and interesting. VGs are inherently object-based, hierarchical, and easily editable; the use of layer-wise initialization further facilitates this control paradigm.\n2. The proposed method enables intuitive manipulation operations that are highly relevant for creative workflows, interactive design tools, and content creation applications.\n3. The VG initialization achieves efficient and effective reconstructive results, and the method overall achieves good results for controllable image generation."}, "weaknesses": {"value": "1. Trade-off between VG complexity and fidelity: VGs have inherent limitations in representing photorealistic details. A highly complex scene (e.g., water, smoke) might require an extremely complex VG, which could be difficult to edit and inefficient to synthesize from. The paper needs to clarify how its \"simplified VG\" representation balances editability with the ability to guide high-fidelity image generation.\n2. The paper lacks clarity on how SVG vector graphics are actually represented and processed in the pipeline. Fig. 2 shows the VAE taking \"SVG\" as input, but it is unclear whether this refers to (a) tokenized SVG code/markup, or (b) rendered raster images of the SVG. If the method uses rendered images, then the \"SVG\" should be \"SVG's rendered image\".\n3. It seems that the method does not guarantee editing consistency, particularly with respect to background preservation. As shown in Fig. 3, when objects are added or removed, the background undergoes changes that are unrelated to the editing operation."}, "questions": {"value": "1. How sensitive is the method to the quality of the initial vector parsing? How is \"semantic alignment\" defined and evaluated?\n2. \"Seamlessness\" of Edits: When a user modifies a VG element, how does the model handle the complex interactions between that element and the rest of the image (e.g., lighting, reflections, shadows, occlusion)? The abstract claims this is \"seamless\" and \"photorealistic\" but this is one of the most difficult problems in controllable synthesis. If handled poorly, edited objects will look \"pasted on\" and inconsistent with the background (OmniPaint [1], Gen-omnimatte [2]).\n\n[1] OmniPaint: Mastering Object-Oriented Editing via Disentangled Insertion-Removal Inpainting ICCV'25\n\n[2] Generative Omnimatte: Learning to Decompose Video into Layers CVPR'25"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KIxZzK4kJ2", "forum": "uHDkm6bU04", "replyto": "uHDkm6bU04", "signatures": ["ICLR.cc/2026/Conference/Submission4888/Reviewer_GJ3f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4888/Reviewer_GJ3f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997732675, "cdate": 1761997732675, "tmdate": 1762917739164, "mdate": 1762917739164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}