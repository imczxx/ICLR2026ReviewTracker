{"id": "rfJ41gK9Ct", "number": 19141, "cdate": 1758293824098, "mdate": 1759897057151, "content": {"title": "PMDformer: Patch-Mean Decoupling Transformer for Long-term Forecasting", "abstract": "Long-term time series forecasting (LTSF) plays a crucial role in fields such as energy management, finance, and traffic prediction. Transformer-based models have adopted patch-based strategies to capture long-range dependencies, but accurately modeling shape similarities across patches and variables remains challenging due to scale differences. Traditional methods, like Patch Normalization, often distort shape information by removing amplitude variations. To address this, we introduce patch-mean decoupling (PMD), which separates the trend and residual shape information by subtracting the mean of each patch, preserving the original structure and ensuring that the model captures true shape similarities. Recognizing the importance of capturing shape relationships not just within patches but also across variables, we introduce Proximal Variable Attention (PVA), which focuses attention on the most relevant, recent time segments to avoid overfitting on outdated correlations. Finally, to restore global trend information without affecting shape similarity, we propose Trend Restoration Attention (TRA), which reintegrates the decoupled trend into the model’s attention mechanism. Combining these components, our model PMDformer outperforms existing state-of-the-art methods in stability and accuracy across multiple LTSF benchmarks.", "tldr": "", "keywords": ["Long-term Time Series Forecasting; Non-stationary; Patch-Mean Decoupled"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7918f796aba0d125717924b9763b07871b5291fa.pdf", "supplementary_material": "/attachment/b9d3b7154bcdca6c101ce24715a46d38ee9e67fd.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes PMDFormer, a novel Transformer-based model for Long-term Time Series Forecasting (LTSF). The key insight is that the attention mechanism in existing patch-based models is often biased by the scale (amplitude) of patches, hindering its ability to capture true shape similarities. To address this, the authors introduce three core components: 1) Patch-Mean Decoupling (PMD), which centers patches by subtracting their mean to separate trend from shape; 2) Proximal Variable Attention (PVA), which focuses cross-variable modeling only on the most recent patch to avoid noisy historical correlations; and 3) Trend Restoration Attention (TRA), which reintegrates the global trend back into the attention mechanism. Extensive experiments on eight benchmarks show that PMDFormer achieves state-of-the-art performance, outperforming recent strong baselines. Ablation studies and theoretical analysis validate the design of each component."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and well-motivated core idea. The identification of \"scale bias\" in patch-based attention is insightful. \n\n2. The model demonstrates compelling state-of-the-art performance, outperforming a wide range of recent and strong baselines across eight standard benchmarks.\n\n3. The paper provides thorough ablation studies and a theoretical analysis that convincingly justify the contribution of each proposed module and the underlying motivation."}, "weaknesses": {"value": "1. The paper positions PMDFormer as effectively modeling cross-variable dependencies, a domain where many previous \"Variable-Dependent\" (VD) models have struggled. However, the PVA module is applied only to the most recent patch. While the results are excellent, this design choice essentially limits cross-variable modeling to a very short, recent context. The paper could more explicitly discuss the implications of this: is the success of PMDFormer evidence that long-range cross-variable dependencies are generally not useful for LTSF, or that they are too noisy to model effectively? A comparison of PVA's performance when applied to the last $K$ patches (beyond just the ablation on $k$ in the sensitivity analysis) could have deepened this analysis.\n\n2. While the efficiency of PVA is mentioned, a more formal and overall complexity analysis of the full PMDFormer architecture compared to other leading models (e.g., PatchTST, iTransformer, TimeBase) is missing. Given the use of a Transformer encoder in the TRA module and the separate PVA module, a discussion of the total parameter count and FLOPs would be beneficial for a complete picture.\n\n3. The figure references \"Figure X\" in the Patch Size sensitivity analysis (Page 9), which appears to be a placeholder. This should be corrected to the appropriate figure number (likely 4b)."}, "questions": {"value": "1. The PVA module operates on the embedded tokens after PMD. Given that PMD centers the patches, does this mean PVA is exclusively modeling the shape similarities of variables at the most recent time segment, completely independent of their absolute levels? Could there be scenarios where the absolute values (or scales) of variables in the proximal patch are also critical for prediction?\n\n2. The TRA module reintegrates the trend via a simple broadcast addition to the Value projection (Eq. 8). Was there an exploration of more complex fusion mechanisms (e.g., gating, concatenation followed by a linear layer)? Why was additive reintegration chosen as the most effective method?\n\n3. For large multivariate datasets (e.g., Traffic with 862 variables), can the authors provide detailed training/inference time and memory usage comparisons between PMDformer and baselines (e.g., iTransformer, TimeBase)? How does PMDformer’s efficiency scale with the number of variables (C) or input sequence length (L), and is there potential to further optimize the TRA module’s computation (e.g., via parameter sharing)?\n\n4. The paper finds moderate patch sizes (24–72) are optimal, but what guidelines would the authors recommend for selecting patch size based on dataset properties (e.g., sampling interval, length of input sequence, seasonality period)? For example, should a dataset with 10-minute sampling intervals (e.g., Weather) use a smaller patch size than one with 1-hour intervals (e.g., ECL)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "N2DbGeeHha", "forum": "rfJ41gK9Ct", "replyto": "rfJ41gK9Ct", "signatures": ["ICLR.cc/2026/Conference/Submission19141/Reviewer_1ngW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19141/Reviewer_1ngW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761617521995, "cdate": 1761617521995, "tmdate": 1762931158155, "mdate": 1762931158155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PMDformer, targeting the core issue that shape matching is often dominated by scale. It adopts Patch-Mean Decoupling (PMD), which removes only the mean of each patch, applies Proximal Variable Attention (PVA) to perform cross-variable attention on the nearest patch to the forecasting window, and uses Trend Restoration Attention (TRA) to inject trend information back into the Value branch without disturbing the Q/K shape alignment, thereby achieving a unified design that separates and then fuses “shape” and “trend.” Across eight datasets and multiple forecasting horizons, PMDformer shows stable improvements over strong baselines, and ablation studies validate the necessity of the three modules and their ordering."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The problem is clearly defined, and each module is well-motivated by the design goals.\n\nS2. The paper includes a formal derivation of the “mean-dominance” condition and multi-module ablations; the experimental design is fairly complete, and PMDformer performs well.\n\nS3. The presentation is clear and the code structure is easy to follow."}, "weaknesses": {"value": "W1. PMD is conceptually close to RevIN (reversible instance normalization that removes mean/variance per series) and to decomposition-based approaches such as DLinear and Autoformer (alleviating scale/distribution shift or decoupling trend and seasonality). The paper could strengthen the analysis to emphasize the differences.\n\nW2. Restricting cross-variable modeling to only the nearest patch helps suppress noise and overfitting, but may lose interpretable dependencies when long-lag cross-variable effects exist.\n\nW3. The similarity of patch-level trends depends heavily on the patch segmentation scheme, yet the paper does not analyze this sensitivity."}, "questions": {"value": "Q1. After injecting trend into the Value branch, could residual pathways indirectly affect the attention output and thus interfere with shape alignment?\n\nQ2. Beyond a fixed k=1, can a gated/learned scheduler adaptively determine the size of the proximal window or whether to include cross-variable attention over earlier patches?\n\nQ3. How do you rule out—or control for—the confounding effects introduced by the patch segmentation choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zVtezARD0i", "forum": "rfJ41gK9Ct", "replyto": "rfJ41gK9Ct", "signatures": ["ICLR.cc/2026/Conference/Submission19141/Reviewer_rABD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19141/Reviewer_rABD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634536236, "cdate": 1761634536236, "tmdate": 1762931157552, "mdate": 1762931157552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a Transformer model named PMDformer for Long-Term Time Series Forecasting. The core idea is to decouple the trend and shape information of each time patch via Patch-Mean Decoupling (PMD), thereby preserving the original amplitude structure and avoiding the distortion of shape information caused by traditional normalization methods. Furthermore, the model introduces the Proximal Variable Attention (PVA) and Trend Restoration Attention (TRA) modules, which are designed to capture the most relevant short-term dependencies among variables and restore global trend information, respectively."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The ideas of this work are easy to understand. The description and presentation are clear.\n2. This work conducts ablation experiments for the three core modules—PMD, PVA, and TRA—on multiple datasets, validating the effectiveness of each module."}, "weaknesses": {"value": "1. The foundational premise of the PMD module is not fully convincing. According to Figure 1, the original patches (P1, P2), which have more similar means, receive a lower attention score than (P1, P3). This observation appears to contradict the authors' claim that \"the scale differences initially obscure true shape similarity\", as the patches with similar scales (P1, P2) are not assigned higher attention. This inconsistency raises questions about the necessity and motivation of the proposed decoupling.\n2. In the TRA module, the operation of adding μ to V is not sufficiently motivated. Directly adding the raw, unprojected trend mean μ to the projected value V forcibly mixes vectors from disparate spaces—the original data space and the projected feature space.\n3. The \"double addition\" of the trend in the architecture appears redundant. The trend term μ is added in the TRA module and again at the final projection layer. This design is not well-principled and risks over-emphasizing the trend component, potentially distorting the learned representations."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D5EkaY0JZP", "forum": "rfJ41gK9Ct", "replyto": "rfJ41gK9Ct", "signatures": ["ICLR.cc/2026/Conference/Submission19141/Reviewer_pzqK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19141/Reviewer_pzqK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717656124, "cdate": 1761717656124, "tmdate": 1762931157127, "mdate": 1762931157127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PMDformer, a Transformer-based model for long-term time series forecasting that preserves cross-patch and cross-variable shape similarities despite scale differences. It introduces three components: Patch-Mean Decoupling (PMD) to separate trend from residual shape by subtracting each patch’s mean; Proximal Variable Attention (PVA) to emphasize recent, relevant cross-variable relationships and mitigate overfitting to outdated correlations; and Trend Restoration Attention (TRA) to reintegrate global trend into the attention mechanism without distorting shape. The authors claim PMDformer achieves superior stability and accuracy over state-of-the-art baselines across multiple LTSF benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and easy to follow.\n\nThe proposed method demonstrates superior performance compared to several existing baselines."}, "weaknesses": {"value": "The paper’s motivation is unclear: there is no well-defined objective or problem statement, and the connection between the proposed method and the problem it aims to solve is not clearly articulated.\n\nThere is little theoretical or empirical justification for the design of the proposed PMDFormer; the choice of each component appears arbitrary and based on empirical intuition, with no systematic evaluation.\n\nEfficiency is not discussed: the paper lacks a complexity analysis (parameter count, runtime, and memory usage) relative to the baselines.\n\nThe LLM usage statement is not in the original manuscript."}, "questions": {"value": "please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "twXBXTyerF", "forum": "rfJ41gK9Ct", "replyto": "rfJ41gK9Ct", "signatures": ["ICLR.cc/2026/Conference/Submission19141/Reviewer_jQS8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19141/Reviewer_jQS8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945646112, "cdate": 1761945646112, "tmdate": 1762931156610, "mdate": 1762931156610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}