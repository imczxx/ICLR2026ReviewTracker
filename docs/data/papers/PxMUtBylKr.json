{"id": "PxMUtBylKr", "number": 19996, "cdate": 1758301286143, "mdate": 1759897007464, "content": {"title": "Strategic Self-Improvement for Competitive Agents in AI Labour Markets", "abstract": "As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. We investigate the dynamics of an AI labor market through a simulated gig economy where agents controlled by fixed policies or Large Language Models (LLMs) compete for jobs, develop skills, and adapt their strategies under competitive pressure. Our analysis identifies three core capabilities that successful LLM-agents develop organically: \\textbf{metacognition} (accurate self-assessment of skills), \\textbf{competitive awareness} (modeling rivals and market dynamics), and \\textbf{long-horizon strategic planning}. Moreover, we show that LLM agents explicitly prompted with these reasoning capabilities learn to strategically self-improve and demonstrate superior adaptability to changing market conditions. At the market level, our simulation reproduces classic macroeconomic phenomena found in human labor markets, while controlled experiments reveal potential AI-driven economic trends, such as rapid monopolization and systemic price deflation. This work provides a foundation to further explore the economic properties of AI-driven labour markets, and a conceptual framework to study the strategic reasoning capabilities in agents competing in the emerging economy.", "tldr": "We developed a simulated gig economy to study impact of AI to labour markets and identified strategic reasoning capabilities (metacognition, competitive awareness, long-term planning) that organically develop in successful agents", "keywords": ["multi-agents", "simulation", "economics", "self-improving", "llm agents"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6facb0ccfeef60f693303fb504374e256ece60df.pdf", "supplementary_material": "/attachment/449faf8d6724daaf404c51a98c37fc6a50199df1.zip"}, "replies": [{"content": {"summary": {"value": "The paper tackles how AI agents behave strategically under competitive economic pressure in an online gig-style labor market. It formalizes a Competitive Skill-Based Stochastic Game and instantiates it in a simulator (AI Work) where fixed-policy and LLM-driven agents choose between bidding and training, accumulate reputation, and interact via price-reputation-based matching. The authors report emergent macro regularities (e.g., Beveridge and Okun-like patterns), identify three reasoning domains (metacognition, competitive awareness, planning), and show that explicitly prompting these “Strategic Self-Improving Agents” (SSA) improves rewards, rank, and market share over CoT/ReAct-style baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper narrows from broad “LLM economies” to a controlled gig-market with explicit bidding–training trade-offs and partial observability. This scope makes it easier to connect agent reasoning to market outcomes than in full macro simulators. \n2. The market is specified as a multiplayer stochastic game with a clear state (skills, reputation), action (bid/train), and a matching mechanism coupled to prices and ratings. This yields an analyzable knob set (capacity ν, price–reputation weights, forgetting λ) that practitioners can tune. \n3. Reputation follows discounted Beta updates with a community base rate; matching uses a CES/Cobb–Douglas style score and Gale–Shapley allocation. Stating these pieces in equations (and an algorithmic step list) meaningfully improves reproducibility over purely narrative agent systems."}, "weaknesses": {"value": "1. Novelty is incremental and not crisply isolated. Prior LLM-agent simulators also study market behavior and “reasoning traces,” and this work’s distinct contribution—framing as a skill-based stochastic game with SSA prompts—lacks an ablation that shows which new component is necessary or sufficient for the headline effects. A component-by-outcome matrix (e.g., remove reputation dynamics, remove CES scoring, remove SSA prompts) is needed to establish a clear methodological delta.\n2. Statistical evidence for “emergent macro relationships” is weak. Reported R² values are based on short horizons and small economies; some claimed curves (e.g., Okun-like relationships) are presented mainly via plots without uncertainty quantification, seed aggregation, or sensitivity to windowing. For ICLR-level claims, longer runs, bootstrapped confidence bands, and formal tests across many seeds are required.\n3. Measurement validity hinges on an LLM-as-judge scoring the presence of capabilities. This introduces construct validity and evaluator-drift concerns that are not controlled with human annotation, inter-rater reliability, or blinded protocols. Without such controls, large reported correlations (e.g., r ≈ 0.74 for metacognition) risk reflecting judging prompts rather than genuine latent capabilities. \n4. The environment’s realism is limited by simplifying assumptions that drive outcomes. Matching is effectively price–reputation scoring plus stable matching with t=0 Gumbel noise, there is no moral-hazard contract within jobs, and “skills” evolve via stylized update rules. Wage deflation under open bidding may thus be an artifact of scoring and capacity constraints rather than a robust equilibrium feature; the paper does not test counterfactual mechanisms (e.g., noisy ranking, monitoring frictions, verification failures).\n5. Reproducibility and cost reporting are insufficient. The core experiments depend on proprietary models accessed via an aggregator, prompts are summarized but not fully released, and token/cost accounting is only approximate; some registry models even note “reasoning enabled (low)” without fixed revisions. For a simulation-heavy paper, releasing code, exact prompts, random seeds, and full configs is essential, and cost–variance analyses should be provided for independent verification. \n6. The title in the submission does not match the title in the paper."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kGEbwREfOO", "forum": "PxMUtBylKr", "replyto": "PxMUtBylKr", "signatures": ["ICLR.cc/2026/Conference/Submission19996/Reviewer_YmvH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19996/Reviewer_YmvH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837772677, "cdate": 1761837772677, "tmdate": 1762932899095, "mdate": 1762932899095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts simulations of a competitive labor market, in which multiple LLM agents compete for jobs with the goal of maximizing total earnings. In Section 3.1, the parameters are validated to check they are roughly realistic (however only in a cursory manner). In Section 3.2, the capabilities of LLM agents based on an array of different open- and closed-source LLMs are evaluated in a specific context (against certain static policies). In Section 3.3, two experiments are conducted to measure how the LLM agents respond to changes in marketplace incentives. In Section 4, additional analyses and experiments are presented, such as summary statistics (4.1), robustness checks (4.4) and ablations (4.5)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "S1. The authors construct an interesting market environment in which to test the economic reasoning capabilities of LLM agents. For example, it's interesting to give the LLM agent an explore-exploit style tradeoff between \"BID\" and \"TRAIN\" actions. \n\nS2. The authors test a wide array of LLMs and compare their system against sensible baselines CoT/ReAct (however, the details of these are insufficiently explained, see W2)."}, "weaknesses": {"value": "W1 (major). The writing quality of the paper is extremely low, making it difficult to read. \n* In terms of formatting, there are many newlines missing, and some punctuation marks like \"-\" replaced with \"/\" (e.g.: \"finite/horizon, discrete/time\" in Line 132-3).\n* In terms of spelling and grammar, there are numerous errors: the paragraph in Lines 204-210 has at least 3 such errors alone (\"We describe experiment setup in detail\" Line 206, \"aability\" Line 210, missing period at the end of the paragraph). \n\nW2 (major). Many key experimental details are not explained:\n* The precise details of the policy baselines GRDPL and FIXPL are not explained. Appendix H only describes how they work on a high level. \n* For the experiments in Section 3, it is not clearly described what LLM agent architecture is used. I first assumed SSA, since it was described in Section 2, but then from Section 4.2 I am led to infer that it is something simpler than SSA. \n* In general, insufficient detail is given about the LLM agent architectures. Only the prompt template for SSA is provided, and no information about CoT or ReAct is provided (beyond citations). \n* For the comparison of SSA and CoT/ReAct (Section 4.3), it is not explained which LLM is used (all of them? gpt-oss?). \n\nW3. I do not find the sanity checks in Section 3.1 convincing. It is not explained why the particular metrics that are reported are necessary or sufficient to establish the claim that the simulation is sufficiently accurate. For example, it is not clear that these metrics should behave the same in human-heavy versus AI-heavy economies. \n\nW4. I similarly do not find the capability evaluation in Section 3.2 particularly convincing. It is unclear why testing each LLM agent against two static policy agents (1 fixed, 1 greedy) is a  representative measure of capabilities. For example, are the results roughly the same if instead each LLM agent plays against 2 fixed, or 2 greedy agents? Also, while it serves as a helpful sanity check to verify whether the LLM agent outperforms the policy, a easier to interpret capability evaluation would be to benchmark its performance relative to the optimal best response. \n\nW5. The baselines CoT/ReAct seem to be relatively weak, it would be more natural to use as baselines prior work on self-improving and reflective agents (such as the agent architectures mentioned in Appendix A). \n\nW6 (minor). In Lines 859-863, you claim to report cost via aggregated token usage, but I don't see this information anywhere in the paper. \n\nW7 (minor). The LLM judge does not appear to be validated, for example by comparing with human labels."}, "questions": {"value": "Q1. There is already a rich body of prior work that studies LLM agent behavior in a simulated marketplace. Can the authors more precisely explain the novelty of their contribution relative to this prior work? (For example, the agent architecture SSA does not appear to be a novel contribution, because it appears to just be a specific instantiation of a prompt scaffold.) This is hinted at in Appendix A, however Li et al. (2024) is far from the only paper that takes a microeconomic perspective. For example, Qian et al. (2025) study bargaining and Fish et al. (2024) study pricing and auctions. \n\nQ2. Can the authors provide the missing experimental details?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sjPNmEUSbP", "forum": "PxMUtBylKr", "replyto": "PxMUtBylKr", "signatures": ["ICLR.cc/2026/Conference/Submission19996/Reviewer_JLJY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19996/Reviewer_JLJY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843901825, "cdate": 1761843901825, "tmdate": 1762932898634, "mdate": 1762932898634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a formal framework and a simulation platform — AI Work — to study labour-market dynamics when autonomous AI agents (LLM-based) compete for jobs, invest in skills, and adapt strategies over repeated rounds. The authors model the market as a competitive skill-based stochastic game, implement a gig-economy style simulator (with reputation, bidding, training, capacity constraints, etc.) and experiments on large-scale fixed-policy simulations to study emergent macroeconomic patterns (e.g., Beveridge-like curves, concentration), large-scale fixed-policy simulations to study emergent macroeconomic patterns (e.g., Beveridge-like curves, concentration)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The question of how LLM agents behave in economic settings is societally and scientifically relevant; the paper tackles a forward-looking problem with potential policy and ML implications.\n2. The market formalization (Competitive Skill-Based Stochastic Game) and the engineering of AI Work (price–reputation scoring, stochastic reranking, capacity constraints, skill/reputation dynamics) are well specified and grounded in economic primitives (Cobb–Douglas/CES scoring, Beta reputation aggregation)."}, "weaknesses": {"value": "1. Simplified proxy tasks and utility model. The simulated tasks are proxy tasks with stochastic scoring; how sensitive are conclusions (e.g., monopolization, wage deflation) to the choice of task scoring function $\\gamma$(·), client preference generation P(·), or to the Cobb–Douglas aggregator/parameterization (wq, wp)? The manuscript argues qualitative alignment with known macro facts, but it lacks sensitivity analyses showing results are robust across realistic alternative parameterizations.\n2. The environment hides latent skills and permits only price & reputation observations — a reasonable simplification — but many real markets expose additional signals (samples, portfolios, verification). It is unclear how adding richer observability or dispute/verification mechanics would change results. The limitations section mentions simplifications; more systematic exploration or discussion is needed.\n3. Ablation interpretability. The paper reports that metacognition is the dominant capability, planning less so. This is plausible, but further analysis—e.g., causal ablation, counterfactual prompting, or controlled micro-tasks isolating each capability—would strengthen the claim and assure it’s not an artifact of prompt phrasing."}, "questions": {"value": "1. Would the key macro and micro claims hold under variations of: (a) reputation update parameters (λ, W, H), (b) score aggregator weights (wq, wp) and CES parameter ρ, and (c) job/task diversity and concurrent capacity ν. Report whether monopolization and wage deflation persist qualitatively.\n2. How real-world platform features (verification, sample portfolios, dispute resolution) might change results and list concrete platform design levers that could mitigate harms (e.g., reputation caps, randomized matching, diversity quotas)"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The paper briefly mentions potential harms (monopolization, displacement), but policy implications and mitigation strategies (platform design interventions, regulation) are lightly sketched. Given the real-world stakes, the discussion could be expanded with more specific mechanism design recommendations and ethical considerations (e.g., human worker protection, transparency)."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s4hKeak1Wq", "forum": "PxMUtBylKr", "replyto": "PxMUtBylKr", "signatures": ["ICLR.cc/2026/Conference/Submission19996/Reviewer_LRqr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19996/Reviewer_LRqr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982412240, "cdate": 1761982412240, "tmdate": 1762932898221, "mdate": 1762932898221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}