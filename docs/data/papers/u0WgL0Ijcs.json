{"id": "u0WgL0Ijcs", "number": 23897, "cdate": 1758349989284, "mdate": 1763678341896, "content": {"title": "Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models", "abstract": "Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing \"a red cube and a blue sphere\" with \"a blue cube and a red sphere\". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., \"a monitor to the left of a bicycle on a white background\") and LLM-generated Contextual captions (e.g., \"In a brightly lit photography studio, a monitor is positioned to the left of a bicycle\"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel \"Confusion Benchmark\" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).", "tldr": "", "keywords": ["contrastive", "multimodal"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/30d62592af83f0119c9c392d1b50a3937edc5fde.pdf", "supplementary_material": "/attachment/659f018dbb7f1ba5e5b1412c0759531aad62fe9e.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents Auto-Comp, a fully automated, concept-driven pipeline for generating photorealistic synthetic benchmarks focused on evaluating compositional binding abilities in VLMs. The approach uses parallel data generation — Minimal images with template captions on white backgrounds versus Contextual images with realistic scenes and LLM-generated descriptions — to enable controlled A/B comparisons of visio-linguistic complexity. The authors release Auto-Comp-CP, a benchmark suite for color and spatial-relation binding (N = 1–3), including challenging Swap and Confusion hard-negative evaluation settings. Experiments show universal compositional failures across 20 contrastive VLMs (CLIP, SigLIP families), revealing that models often rely on brittle heuristics and struggle especially with complex multi-object reasoning. The paper additionally uncovers a trade-off: context improves spatial reasoning but harms attribute (color) binding, highlighting modality-interaction weaknesses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Addresses an evaluation gap in compositional reasoning for VLMs. The introduction highlights that existing real-image benchmarks are “inherently noisy,” preventing precise diagnosis of compositional failures, while synthetic benchmarks often lack photorealism or linguistic diversity.\n\nNovel benchmark design with hard negatives, including Swap and Confusion conditions that rigorously test binding capabilities and reveal brittleness to low-entropy distractors.\n\nEvaluation on 20+ VLMs demonstrates universal compositional failures and degradation with increasing object count, supporting claims with quantitative evidence across tasks."}, "weaknesses": {"value": "Current benchmark only evaluates Color (N=1–3) and Position (N=2–3) skills. Compositional semantics involving shape, action, affordance, numeracy, etc., remain untested.\n\nDependence on T2I models and LLMs may introduce generation artifacts not fully reported or characterized. The paper notes robust filtering but does not quantify latent biases introduced via the LLM captioning or T2I rendering process.\n\nLarge-scale generation and validation likely requires substantial compute — while some hardware details are noted elsewhere, the primary evaluation section omits efficiency metrics (user cost, inference latency)."}, "questions": {"value": "What is the generation + validation time/cost for producing one new benchmark configuration?\n\nDo results extend to recent vision-language generative models with token-level alignment (e.g., Llama-vision-style models)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w5wFrsBJoN", "forum": "u0WgL0Ijcs", "replyto": "u0WgL0Ijcs", "signatures": ["ICLR.cc/2026/Conference/Submission23897/Reviewer_RkbM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23897/Reviewer_RkbM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875383305, "cdate": 1761875383305, "tmdate": 1762942846944, "mdate": 1762942846944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Auto-Comp, a benchmark for evaluating compositionality in vision-language models, motivated by the goal of isolating the impact that visual or linguistic complexity has on the assessment of compositionality. \n\nAuto-Comp is a synthetic benchmark, produced through a pipeline that first samples \"concepts\" (sets of objects and associated attributes or relationships, restricted to color and spatial relationships respectively), constructs captions for these concepts (either through simple templates or LLM-generated text) and then generates images for them using StableDiffusion3.5-large. The generated captions and images are then automatically validated through a series of tests. The benchmark has two different settings: \"Minimal,\" which uses templated captions and a white background, and \"Contextual,\" which uses LLM-generated captions and a realistic background. Hard negatives are generated primarily through swapping attributes or objects. The authors also propose a setting called \"Confusion Benchmark,\" where hard negatives are constructed by instead sampling objects and attributes without replacement.\n\nThe authors evaluate VLMs, restricted to the CLIP and SigLIP families, on their benchmark. They find that the models consistently achieve poor results, particularly for a greater number of objects. They furthermore surprisingly find that realistic backgrounds help models determine the correct caption for hard-negatives featuring spatial relationships."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Evaluating models on images containing the same objects but with different backgrounds (achieved by the \"Minimal\" and \"Contextual\" conditions) is novel and leads to a fairly surprising result in the form of models improving in performance when a realistic background is used for hard negatives featuring spatial relations.\n- The automated pipeline makes clever use of various open-source resources for both the generation and the filtering components and achieves strong agreement with human judgements for validity."}, "weaknesses": {"value": "I would argue the paper is affected by two key limitations:\n- Firstly, I was surprised to see that the evaluation is restricted to VLMs of the CLIP and SigLIP families. In the context of contrastive vision-language models, I would have expected, for instance, to see NegCLIP, which is finetuned on hard-negatives. More importantly, however, the landscape of vision-language models today is not restricted to contrastive vision-language embedding models, but features numerous models, both open and proprietary, trained with a language modeling loss, including but not limited to: Qwen2.5-VL, InternVL3 for open-weight models and GPT-4o as a proprietary model. It would be valuable to similarly evaluate the performance of such models to determine whether this difficulty with compositional reasoning is restricted to the embeddings themselves, and whether the findings regarding the background generalize to other models. For the auto-regressive models, evaluation could occur either by checking whether the model assigns the highest probability to the correct caption or through framing as a multiple-choice question.\n- Secondly, the main conclusion that models such as CLIP and SigLIP struggle at compositionality is one that has been established in various papers since 2022, dating back to Winoground. Both Winoground and the various extensions of CREPE similarly use the strategy of generating hard-negatives through swapping operations (with the latter series of datasets featuring replacement and addition operations as well). If I were a practitioner wanting to evaluate my model in compositional reasoning, it is not clear what additional benefit evaluating on Auto-Comp presents. The main novelty comes from the paired subset featuring different backgrounds, but only the realistic background should be relevant for most applications.\n\nFor some lesser weaknesses:\n- While I do think the pipeline for assessing correctness is robust, I would have appreciated human accuracy being reported for a subset of questions, simply to verify that humans can solve the questions with near-perfect accuracy.\n- Generation artifacts from StableDiffusion3.5 could affect model performance."}, "questions": {"value": "- How is grammaticality maintained for hard-negatives in the \"Contextual\" setting, as these are not generated with templates?\n- When performing human validation, how many samples did each different subset contain (for different combinations of N, color/spatial reasoning question and background type)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k3oSnzchOL", "forum": "u0WgL0Ijcs", "replyto": "u0WgL0Ijcs", "signatures": ["ICLR.cc/2026/Conference/Submission23897/Reviewer_szMT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23897/Reviewer_szMT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965531348, "cdate": 1761965531348, "tmdate": 1762942846662, "mdate": 1762942846662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Auto-Comp, an automated pipeline for generating photorealistic, concept-driven benchmarks to evaluate compositional reasoning in vision-language models. This paper creates paired Minimal and Contextual image-caption sets, enabling controlled analysis of visual and linguistic factors. Using the resulting Auto-Comp-CP benchmark on color and spatial relations, the authors evaluate 20 CLIP and SigLIP models, revealing universal compositional failures and a key trade-off. This framework is scalable, reproducible, and validated with high human–model agreement."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The automated, concept-driven pipeline is well-structured.\n\nAuto-Comp can generate vast, high-quality benchmarks without manual labeling. The open-source data and code ensure reproducibility and community impact.\n\nThe paper evaluates a wide range of models, systematically analyzing error types, context effects, and model hierarchies."}, "weaknesses": {"value": "The benchmark currently focuses only on color binding and spatial relations. While sufficient for proof-of-concept, generalization to other compositional phenomena, such as actions and attributes, remains untested. Could your benchmark pipeline incorporate more aspects?\n\nSince Auto-Comp uses pretrained T2I models and LLM validators, biases in those systems propagate into the benchmark. Could you provide some insights or discussions on how to minimize the impact of external models on the benchmarks produced by this pipeline?"}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hzmtG08YlJ", "forum": "u0WgL0Ijcs", "replyto": "u0WgL0Ijcs", "signatures": ["ICLR.cc/2026/Conference/Submission23897/Reviewer_LjQ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23897/Reviewer_LjQ6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970817568, "cdate": 1761970817568, "tmdate": 1762942846296, "mdate": 1762942846296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. The evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: This paper is well-written and easy to understand\n\nS2: The investigated problem of benchmarking the compositional understanding of VLMs is important and interesting\n\nS3: The proposed pipeline for benchmark construction is well designed\n\nS4: The results and findings are interesting. In particular, models are highly susceptible to low-entropy distractors, showing their compositional failures extend beyond known bag-of-words limitations."}, "weaknesses": {"value": "W1: The benchmark generation relies on the capabilities of Gemma3-12b, StableDiffusion3.5-large, and GroundedSAM2. I am curious whether using other models could achieve similar (or even better) benchmark quality? In other words, does the automatic benchmark generation pipeline specifically work for this combination of models, or is it generalizable to stronger ones to be developed in the future? \n\nW2: A related concern is that the capabilities of each model in doing the corresponding tasks should be evaluated; otherwise, it’s hard to know whether the pipeline can be trusted, thus bringing more concern to the results obtained with this benchmark. In particular, the survival rates are also model-based, making it less trustworthy without (human) validation (at least in a subset of samples).\n\nW3: This work focuses only on contrastive VLMs, namely CLIP and SigLIP series. Can this benchmark be used for evaluating generative VLMs? Maybe with CoT and reasoning efforts, the compositional failures could be alleviated? Is it a problem specifically with contrastive VLMs? This makes the impact of this work relatively limited.\n\nW4: This work emphasizes the “automatic” pipeline a lot, which I don’t understand why it is so important. Many benchmarks are automatically synthesized and validated. What’s new here?"}, "questions": {"value": "Please refer to W1-W4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JwQTWPE3Ur", "forum": "u0WgL0Ijcs", "replyto": "u0WgL0Ijcs", "signatures": ["ICLR.cc/2026/Conference/Submission23897/Reviewer_YjoX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23897/Reviewer_YjoX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762383119847, "cdate": 1762383119847, "tmdate": 1762942845996, "mdate": 1762942845996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}