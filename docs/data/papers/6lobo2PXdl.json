{"id": "6lobo2PXdl", "number": 5985, "cdate": 1757949483799, "mdate": 1759897941059, "content": {"title": "The Power of Small Initialization in Noisy Low-Tubal-Rank Tensor Recovery", "abstract": "We study the problem of recovering a low-tubal-rank tensor $\\mathcal{X}\\_\\star\\in \\mathbb{R}^{n \\times n \\times k}$ from noisy linear measurements under the t-product framework. A widely adopted strategy involves factorizing the optimization variable as $\\mathcal{U} * \\mathcal{U}^\\top$, where $\\mathcal{U} \\in \\mathbb{R}^{n \\times R \\times k}$, followed by applying factorized gradient descent (FGD) to solve the resulting optimization problem. Since the tubal-rank $r$ of the underlying tensor $\\mathcal{X}_\\star$ is typically unknown, this method often assumes $r < R \\le n$, a regime known as over-parameterization. However, when the measurements are corrupted by some dense noise (e.g., sub-Gaussian noise), FGD with the commonly used spectral initialization yields a recovery error that grows linearly with the over-estimated tubal-rank $R$. To address this issue, we show that using a small initialization enables FGD to achieve a nearly minimax optimal recovery error, even when the tubal-rank $R$ is significantly overestimated. Using a four-stage analytic framework, we analyze this phenomenon and establish the sharpest known error bound to date, which is independent of the overestimated tubal-rank $R$. Furthermore, we provide a theoretical guarantee showing that an easy-to-use early stopping strategy can achieve the best known result in practice. All these theoretical findings are validated through a series of simulations and real-data experiments.", "tldr": "For the noisy low-tubal-rank tensor recovery problem, we show that factorized gradient descent with small initialization converges to nearly the minimax optimal error.", "keywords": ["low-tubal-rank tensor recovery; t-SVD; t-product; over-parameterization;non-convex"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/69ce48139ad3c94f113e888ef79c7ac1b2701f70.pdf", "supplementary_material": "/attachment/db2180321d29225b857da059b08c65bb9325c5f8.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the problem of recovering a noisy low-tubal-rank tensor model. Specifically, it studies the theoretical guarantees of the gradient descent method with small initialization for solving a low-rank recovery problem formulated via the Burer–Monteiro factorization under the t-product framework. The authors prove that when the dimensions of the optimization variables match the ground-truth rank, the algorithm enjoys a linear convergence rate. They further provide error estimates under different levels of over-parameterization, showing that the final recovery error depends only on the noise scale as the iteration proceeds. Furthermore, the paper proposes a practical stopping criterion and establishes the corresponding recovery error bound under this termination condition. Numerical experiments demonstrate that the small-initialization gradient descent method achieves superior recovery performance compared with those initialized randomly or by spectral methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well structured and clearly written, particularly on the mathematical side. The main results are rigorously proved and thoughtfully discussed. The authors also provide useful comparisons with existing works, which helps position their contributions within the current literature. The topic—low-tubal-rank tensor recovery—is timely and important, as tensor models have recently attracted significant attention due to their complex structures and broad applications. Moreover, the analysis of over-parameterization is of great interest, and the finding that the recovery error depends only on the noise scale is both elegant and encouraging."}, "weaknesses": {"value": "Although this paper studies the tensor case, the introduction and background on tensor are relatively limited, which reduces the readability for a broader audience. To improve accessibility, it would be helpful to include more definitions and explanations—such as the t-product and the corresponding notion of tensor rank—in the appendix, given the space limitations of the main text.\n\nAnother concern is that, as mentioned in Remark 7, the results of this paper can be viewed as a direct extension of the matrix case studied in [Ding et.al., 2025]. If one interprets the tensor in this paper as a matrix and the t-product as standard matrix multiplication, it is not immediately clear how the theoretical analysis differs from the matrix setting. The paper would benefit from a more explicit discussion highlighting the distinctive challenges or techniques specific to tensors, which would not only improve readability but also strengthen the contribution of the work.\n\nFinally, Remark 7 might be placed earlier in the text, since both Theorem 2 and its proof sketch are quite similar to those in [Ding et.al., 2025], not only the proposed stopping strategy."}, "questions": {"value": "- In the matrix case, the model obtained via the Burer–Monteiro (BM) factorization is nonconvex and may contain spurious local minima. For the tensor case studied in this paper, do similar spurious local minima exist? If so, what guarantees that the proposed gradient descent iterates will not converge to such undesirable local minima?\n\n- In the proof sketch of Theorem 2, the authors mention that the iterates undergo four different stages, during which the singular values of the iterates exhibit distinct behaviors. It would be interesting and instructive to design numerical experiments that illustrate these four phases and verify their correspondence with the theoretical analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wZHv4Qzf0y", "forum": "6lobo2PXdl", "replyto": "6lobo2PXdl", "signatures": ["ICLR.cc/2026/Conference/Submission5985/Reviewer_SVNR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5985/Reviewer_SVNR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729331741, "cdate": 1761729331741, "tmdate": 1762918393839, "mdate": 1762918393839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates low-tubal-rank tensor recovery from noisy linear measurements using the t-product framework and analyzes the behavior of factorized gradient descent (FGD) under over-parameterization. The authors show that small initialization and early stopping can significantly reduce recovery error independent of the over-estimated tubal rank, with both theoretical guarantees and empirical validation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main contribution of this paper lies in providing an error bound for low-tubal-rank tensor recovery when the tubal rank is overestimated in noisy settings, showing that FGD can still achieve reliable recovery."}, "weaknesses": {"value": "1. The contribution of this work is incremental. Factorized Gradient Descent (FGD) is not an original contribution of this work (see [1]). Similarly, the use of early stopping to avoid overfitting is a standard practice and cannot be regarded as an innovation here. \n[1] Z. Liu, Z. Han, Y. Tang, X. -L. Zhao and Y. Wang, \"Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent,\" in IEEE Transactions on Signal Processing, vol. 72, pp. 5470-5483, 2024\n\n2. The relationship with [1] should be discussed in detail rather than mentioned only briefly.\n\n3. Please provide a careful comparison with `Implicit Regularization for Tubal Tensors via GD' and A Validation Approach to Over-parameterized Matrix and Image Recovery'', particularly focusing on the technical tools employed in the proofs.\n\n4. The main motivation of this work is that traditional algorithms often use a higher rank than the true rank. However, in most practical applications (e.g., image and video recovery), the true rank is unknown. What really matters is which estimated rank leads to better recovery performance—and in many cases, such rank estimates are not unique. A thorough discussion and empirical comparison in real-world scenarios are therefore necessary. At present, the paper includes only one example on a single color image and lacks broader discussion of practical implications.\n\n5.Over the past five years, many tensor decomposition methods with rank estimation strategies have been proposed. Since this work is motivated by the problem of overestimated rank, it should include comparisons and discussion of rank-estimation-based approaches, such as the following:\n\n[2] Q. Shi, Y.-M. Cheung, and J. Lou, “Robust tensor svd and recovery with rank estimation,” IEEE Transactions on Cybernetics, 2021.\n\n[3] Zheng, J., Wang, W., Zhang, X., & Jiang, X. (2023). A Novel Tensor Factorization-Based Method with Robustness to Inaccurate Rank Estimation. arXiv preprint arXiv:2305.11458.\n\n[4] Zhu Q, Wu S, Fang S, et al. Fast tensor robust principal component analysis with estimated multi-rank and Riemannian optimization[J]. Applied Intelligence, 2025, 55(1): 52.\n\n6. Beyond Implicit Regularization for Tubal Tensors via GD, no other recent tensor decomposition methods (from the last five years) are compared."}, "questions": {"value": "See the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "my7BgLdhH2", "forum": "6lobo2PXdl", "replyto": "6lobo2PXdl", "signatures": ["ICLR.cc/2026/Conference/Submission5985/Reviewer_svxt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5985/Reviewer_svxt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771075610, "cdate": 1761771075610, "tmdate": 1762918393290, "mdate": 1762918393290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes the optimization dynamics of factorized gradient descent (FGD) for low tubal rank tensor recovery, which is based on the quite recent t-SVD for tensors.  The authors prove that using a small random initialization for FGD together with suitable early stopping of FGD leads to a minimax optimal estimator, under an RIP-type condition.  The authors include synthetic experiments that validate the theory.  Overall, this is a quality submission.  I would be open to *raising* my score if the authors addressed a couple of my concerns in the Weaknesses section below with their rebuttal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The optimization dynamics are interesting and not so intuitive.  In particular, the authors find that small random initialization is better than smart spectral initialization for this problem.  Further, FGD exhibits non-monotonicity with respect to error with the ground-truth; therefore, early stopping is needed.\n\n- The technical strength of analysis in this paper seems impressive. \n\n- The authors confirm their results also through numerical simulations.  Random small initialization and early stopping indeed improve the estimated tensor in experiments."}, "weaknesses": {"value": "- It is unclear whether the factored form of the tensor used by the authors imposes symmetry and/or psd constraints.  The authors use $\\mathcal{U} \\ast \\mathcal{U}^{\\top}$ as their ansatz for the low tubal rank tensor.\n\n- The relation to low rank matrix recovery (i.e., with matrices and matrix SVD, rather than with tensors and t-SVD) is not well-specified as far as I can tell.  The authors should comment on this.\n\n- There are no real data experiments.  Real data would be nice, because there are certain RIP assumptions in this work, and one wonders how well they capture real data situations."}, "questions": {"value": "Line 60-61: The authors write \"Under the t-SVD framework, since problem (2) is NP-hard, a common approach is to relax the tubal-rank constraint to the tensor nuclear norm.\"  Is it actually known that the low tubal rank recovery problem (2) is NP-hard?  What is a reference?  This wouldn't be in Hillar-Lim's paper, for instance.  \n\nLine 60-61: The authors should perhaps also say \"tubal tensor nuclear norm\", and provide a reference for its definition.  My understanding is that the authors mean the sum of singular values from the t-SVD, rather than the tensor nuclear norm as related to the CP decomposition.  That latter tensor nuclear norm is NP-hard to compute.\n\nLines 70-72: Would using the ansatz $\\mathcal{A} = \\mathcal{U} \\ast \\mathcal{U}^{\\top}$ imply symmetry and/or some sort of positive semi-definiteness in $\\mathcal{A}$?  If so, the authors should be explicit about these assumptions on $\\mathcal{A}$.  To me the natural low-rank ansatz would be $\\mathcal{A} = \\mathcal{U} \\ast \\mathcal{V}^{\\top}$ where $\\mathcal{U}$ need not equal $\\mathcal{V}$.\n\nLine 131: In the caption of Table 1, start the second sentence as \"The noise vector...\", i.e. please drop the leading \"And\".\n\nLine 202: Explain better what the block diagonal matrix $\\bar{Y}$ is.\n\nLines 240-269: In Theorem 2, item 1, is this bound true for all $t \\geq t_1$?  If not, and the error can increase later on, how about formulating item 1 with $\\hat{t}$ as you do with items 2 and 3 in Theorem 2?\n\nSection 4: It would be appreciated if the authors included a real data experiment.\n\nGeneral question: How do the results in this work relate to results for factorized gradient descent with just matrices and matrix SVD, rather than with tensors and t-SVD?  Are there parallel results in the matrix case?  How do the results here compare?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nzfH05c4Y3", "forum": "6lobo2PXdl", "replyto": "6lobo2PXdl", "signatures": ["ICLR.cc/2026/Conference/Submission5985/Reviewer_wQhQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5985/Reviewer_wQhQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051492348, "cdate": 1762051492348, "tmdate": 1762918393040, "mdate": 1762918393040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of recovering a low-tubal rank tensor $X_*$ from its noisy linear measurements, $y = M (X_*) + s$. Focusing on the commonly used Burer-Monteiro factorization framework, it assumes that $X_*$ can be decomposed as $U * U^\\top$, and aims to minimize the quadratic loss function to recover $U$. The authors consider the fractorized gradient descent (FGD) method and attempts to derive upper bounds for the estimator error. A key challenge here is that the true tubal rank of $X_*$, denoted as $r$, is typically unknown, and error bounds in past work often depends on the over-estimated tubal-rank $R$.\n\nThe main contribution of this paper is to show that, both theoretically and empirically, using a small random initialization for FGD achieves a nearly minimax optimal estimation error, which only relies on the true tubal rank $r$, not the over-estimated tubal-rank $R$. To be specific: (i) Theorem 2 establishes estimator error bounds for early stopped FGD, under three different regimes of $r$ and $R$; (ii) Theorem 3 shows that the error bound of Theorem 2 is minimax optimal in the case of Gaussian noise; (iii) Finally, Theorem 4 shows that the early stopping time $t$ can be reliably chosen using sample spitting, thus giving a practical algorithm based on the theoretical results in Theorem 2 and 3."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The key findings of this paper, that the estimation error of FGD only depends on the true tubal-rank $r$, and that small initialization overcomes the dependency on the estimated rank $R$, are definitely interesting. The authors also compare their work with earlier results on the same topic to highlight their contributions, and include a proof sketch section to illustrate why FGD with small initialization works, which I really appreciate."}, "weaknesses": {"value": "(i) The entire analysis replies on the T-PSD assumption on $X_*$, i.e., $X_*$ has the decomposition $X_* = U * U_*$. This is a significant simplification. The authors also acknowledge this limitation in Remark 5 and discussed a bit about extensions to general asymmetric case in Appendix I.\n(ii) Although the error bounds in Theorem 2 does not depend on $R$, the initialization scale $\\alpha$ and early stopping time $t$ depends on $R$. Further, in case 3 it seems to me that we can take $R$ to be so large that $\\alpha$ and $t$ are close to $0$, which is quite counterintuitive. Could the authors clarify more on this? This is very important since the error bounds in this paper improves on previous results only in the case $R \\geq 3r$."}, "questions": {"value": "A few minor comments/suggestions for the authors:\n(i) \"CONDECOMP\" should be \"CANDECOMP\" in line 051.\n(ii) What is $R_n$ in Theorem 2, is it just $R$? Is it possible to state the choices of $\\alpha$ and $t$ only in terms of $R$, since $r$ is unknown?\n(iii) In line 216, perhaps it is better to replace $s$ as some other notation since $s$ denotes the noise in $y$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KTtam0RrgL", "forum": "6lobo2PXdl", "replyto": "6lobo2PXdl", "signatures": ["ICLR.cc/2026/Conference/Submission5985/Reviewer_Zuao"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5985/Reviewer_Zuao"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053624253, "cdate": 1762053624253, "tmdate": 1762918392735, "mdate": 1762918392735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}