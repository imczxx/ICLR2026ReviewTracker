{"id": "tRRHVUwP2B", "number": 14317, "cdate": 1758232821362, "mdate": 1763512640288, "content": {"title": "VeriEquivBench: An Equivalence Score for Ground-Truth-Free Evaluation of Formally Verifiable Code", "abstract": "Formal verification is the next frontier for ensuring the correctness of code generated by Large Language Models (LLMs). \nWhile methods that co-generate code and formal specifications in formal languages, like Dafny, can, in principle, prove alignment with user intent, progress is bottlenecked by specification quality evaluation. \nCurrent benchmarks rely on matching against ground-truth specifications, a manual and expertise-intensive process that has limited existing datasets to a few hundred simple problems and also suffers from a reliability issue.\nTo address this, we introduce VeriEquivBench, a new benchmark with $2,389$ complex algorithmic problems that probe the limitations of current models in both code generation and formal reasoning. \nOur evaluation framework replaces ground-truth matching with a formally grounded metric, the equivalence score, and rigorously verifies the quality of generated specifications and code.\nOur results show that generating formally verifiable code remains a profound challenge for state-of-the-art LLMs. This underscores both the difficulty of the task and the need for benchmarks like VeriEquivBench to drive progress toward scalable and reliable coding agents.", "tldr": "", "keywords": ["Formal verification", "verifiable coding agents", "code generation", "large language models", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8930d926976198ffa3e90ba00ae9eaa4d9779c8.pdf", "supplementary_material": "/attachment/7c81a0cfa16ba2c8ffb08d541a4bc847da6798c3.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the formal verification of LLM-generated code, specifically within the Dafny language. The authors introduce two primary contributions: VeriEquivBench, a new large-scale benchmark of 2,389 algorithmic problems, which is significantly larger than prior datasets. The Equivalence Score, a novel, ground-truth-free metric. This metric uses the Dafny verifier to formally check for bidirectional implication between generated code and its specifications. The paper concludes with an evaluation of state-of-the-art LLMs on this new benchmark, highlighting the significant difficulty of the task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposed Equivalence Score is a significant conceptual contribution. By formally checking for bidirectional implication, it removes the dependency on manually-authored ground-truth specifications. This provides a more rigorous and automated method for evaluating the quality and completeness of the generated specifications themselves, which is a key bottleneck in this area.\n\nThe scale and complexity of VeriEquivBench are commendable. The dataset's 2,389 problems represent a more than 10x increase over existing benchmarks like DafnySynthesis and CloverBench combined. The higher average Cyclomatic Complexity (5.63 vs. 2.44) confirms that the problems are more challenging.\n\nThe inclusion of the TagComp subset, generated via a tag-composition pipeline, is a good step towards creating novel, contamination-free problems for evaluation."}, "weaknesses": {"value": "Misleading Explanation of the Equivalence Score: The paper's explanation of the equivalence score is inaccurate. The authors claim the metric verifies if the method's output is the \"unique value\" satisfying the specifications. A verifier like Dafny proves correctness (i.e., that the code's output is a valid value that satisfies the post-conditions), not uniqueness. While the goal is clearly to ensure the specification is \"tight\" (i.e., not under-specified), describing this as a check for a \"unique value\" misrepresents the verifier's function and the metric's actual technical contribution.\n\nFlawed Natural Language Alignment Check: The paper's second evaluation step, which translates formal specifications back to natural language (FL-to-NL) and uses an LLM-as-a-judge, has several issues. First, it re-introduces a non-formal, unreliable evaluation method, which undermines the paper's primary goal of achieving formal rigor. The reliability of this LLM judge is not validated against human assessments. Second, this FL-to-NL evaluation approach is not novel and is a known technique in related autoformalization communities (e.g., for Lean), which the paper fails to discuss [1].\n\nInsufficient Validation of Benchmark Data: The quality and correctness of the VeriEquivBench dataset are not adequately established. The authors report that the Python-to-Dafny code transformation was validated by running translated unit tests on only 20 random samples (out of 2,389). While 90% of these passed, this sample size (<1%) is far too small to provide statistical confidence in the correctness of the entire 2,389-problem benchmark. Citing \"long compilation times\" is not a sufficient justification for such a limited validation.\n\n[1] Ying H, Wu Z, Geng Y, et al. Lean workbook: A large-scale lean problem set formalized from natural language math problems[J]. Advances in Neural Information Processing Systems, 2024, 37: 105848-105863."}, "questions": {"value": "The paper presents the Equivalence Score as a rigorous check for code-specification alignment. If a pair passes this bidirectional implication check, the specification should, by definition, precisely and completely capture the behavior of the code. Why, then, is the secondary, unreliable Spec -> NL -> LLM-Judge step necessary? Does its inclusion imply that the Equivalence Score alone is insufficient to guarantee that the specification aligns with the original natural language query intent? \n\nThe TagComp pipeline is presented as a \"scalable method\" for generating novel problems. However, the paper's own data shows a very low yield: from ~1,900 initial problem descriptions, only 300 (~15.8%) had Python solutions that passed the unit tests, and only 215 (~11.3%) cleared the full pipeline. Given this low success rate, in what sense is this pipeline considered \"scalable\"? Furthermore, what analysis was done to determine the primary failure mode? Why did ~84% of the LLM-synthesized Python solutions fail to pass their own corresponding unit tests?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pE0CF8uZ34", "forum": "tRRHVUwP2B", "replyto": "tRRHVUwP2B", "signatures": ["ICLR.cc/2026/Conference/Submission14317/Reviewer_LnLN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14317/Reviewer_LnLN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896783479, "cdate": 1761896783479, "tmdate": 1762924751854, "mdate": 1762924751854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an alternative setting of verifiable code generation (jointly generating code, formal specifications, and proofs). To evaluate LLM-generated specs, prior work often checks their equivalence with human-annotated ground-truth specs, which are expensive to collect. This work tries to avoid that by using Dafny to prove the logical equivalence of the generated code and spec. The spec is considered correct if and only if it is equivalent to the code."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The authors collected a new benchmark for verifiable code generation, which can be useful."}, "weaknesses": {"value": "The paper's central contribution is a setup to evaluate specs by checking their logical equivalence with code, which is fundamentally flawed. A good spec often captures only a subset (rather than all) of the code's properties. Consider the task of writing a sorting algorithm, an LLM can easily generate a spec (the output list should be sorted, and it should contain exactly the same set of elements as the input list) and the code of a merge sort algorithm. However, under this paper's setup, this is incorrect because the spec fails to capture all properties of the code (e.g., merge sort is a stable sorting algorithm). This is not a contrived special case. In general, specs are at a higher level of abstraction than code and are expected to capture properties that are relevant. \n\nI'm open to the authors' clarification if I had misunderstood anything."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1xWKwx3EXA", "forum": "tRRHVUwP2B", "replyto": "tRRHVUwP2B", "signatures": ["ICLR.cc/2026/Conference/Submission14317/Reviewer_Bjj7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14317/Reviewer_Bjj7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141881118, "cdate": 1762141881118, "tmdate": 1762924751256, "mdate": 1762924751256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VeriEquivBench, a benchmark of 2K+ Dafny problems for evaluating formally verifiable code generation. The key innovation is an equivalence score that replaces ground-truth specification matching with bidirectional formal verification. Assuming the bidirectional formal verification is successful, this process eliminates the need for expensive manual specification annotation, which has limited prior benchmarks to much fewer problems.  The majority of the benchmark is from LeetCode, and a about 8% are synthetically generated. Empirical evaluations on three closed models show that there is a huge gap in performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The use of the equivalence score is clever, and as far as I know, quite novel.  The method is able to discover errors in previously manually specified ground true specs. The scale of the benchmark dwarfs previous benchmarks in this space, owing to its automated nature."}, "weaknesses": {"value": "The paper's most significant limitation is its fairly thin empirical evaluation. Only three proprietary models are tested on the main task, with no evaluation of open-source models like DeepSeek-Coder, Qwen, or CodeLlama.  The paper also doesn't do much analysis of the failure modes of the models, beyond report a very low success rate.  \n\nFinally, I wonder about the diversity of leetcode questions.  My understanding is that the focus is on very algorithmic coding problems.  So while the benchmark is large in size, it may not be as diverse as one would ideally want.  (To be clear, I don't think this is a serious weakness, but worth commenting on.)"}, "questions": {"value": "How were the coding agents set up and prompted?\n\nCan you comment on the types of coding problems that appear in LeetCode?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YE5flQNgYO", "forum": "tRRHVUwP2B", "replyto": "tRRHVUwP2B", "signatures": ["ICLR.cc/2026/Conference/Submission14317/Reviewer_dTTt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14317/Reviewer_dTTt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762241958095, "cdate": 1762241958095, "tmdate": 1762924750780, "mdate": 1762924750780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}