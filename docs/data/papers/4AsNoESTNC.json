{"id": "4AsNoESTNC", "number": 1005, "cdate": 1756827857368, "mdate": 1763050333744, "content": {"title": "VisionLogic: From Neuron Activations to Causally Grounded Concept Rules for Vision Models", "abstract": "While concept-based explanations improve interpretability over local attributions, they often rely on correlational signals and lack causal validation. We introduce VisionLogic, a novel neural–symbolic framework that produces faithful, hierarchical explanations as global logical rules over causally validated concepts. VisionLogic first learns activation thresholds to convert neuron activations into a reusable predicate vocabulary and induces class-level logical rules from these predicates. It then grounds predicates to visual concepts via ablation-based causal tests with iterative region refinement, ensuring that discovered concepts correspond to features that are causal for predicate activation. Across different vision architectures such as CNNs and ViTs, it produces interpretable concepts and compact rules that largely preserve the original model’s predictive performance. In our large-scale human evaluations, VisionLogic’s concept explanations significantly improve participants’ understanding of model behavior over prior concept-based methods. VisionLogic bridges neural representations and symbolic reasoning, providing more trustworthy explanations suited for safety-critical applications.", "tldr": "", "keywords": ["Interpretability", "Neural-symbolic", "Logical rules", "Convolutional neural network", "Vision transformer", "XAI"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1ec961e624a2254e8ded5c15f39a7b20692d670a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the problem of concept-based explanations for vision models. While existing concept-based methods offer semantic interpretability, they often rely on correlations rather than causal reasoning, which can lead to spurious or misleading explanations. To address these limitations, the paper introduces a neural–symbolic framework that converts neuron activations into interpretable predicates, extracts global logical rules over these predicates, and grounds them to visual concepts. This approach enables the discovery of causally validated and human-understandable concepts. Experiments on CNNs and ViTs demonstrate that the proposed method retains high predictive performance while producing compact and interpretable rules. Large-scale human evaluations are also provided to show the enhanced understanding of model behavior compared to prior concept-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and provides clear visualizations. \n2. The paper proposes a new perspective on concept-based explanations from a neural-symbolic reasoning approach, which can offer causally validated concepts for more trustworthy explanations. \n3. The paper conducts extensive human evaluation to validate the effectiveness of the generated concept explanations in enhancing human understanding and decision making."}, "weaknesses": {"value": "1. The paper appears to miss some recent baselines in concept-based explanations, such as [1, 2, 3], which makes the performance improvement achieved through neural symbolic reasoning less convincing.\n2. The methodology involves multiple stages, different components, and various hyperparameters; however, the paper lacks sufficient ablation studies to analyze the impact of each factor.\n3. The paper evaluates performance only on a single image dataset from general domains. However, it would be beneficial to include some domain-specific datasets, such as medical imaging datasets, to provide a more comprehensive evaluation and demonstrate the practicality of the proposed concept explanations for decision-making.\n\n[1] Explain Any Concept: Segment Anything Meets Concept-Based Explanation, NeurIPS 2023.\n\n[2] A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation, NeurIPS 2023.\n\n[3] Explainable Concept Generation through Vision-Language Preference Learning for Understanding Neural Networks’ Internal Representations, ICML 2025."}, "questions": {"value": "1. How does the performance of the proposed neural–symbolic explanation method compare with other baselines?\n2. How do the different components affect performance, such as the segmentation-based refinement step using Mask R-CNN and SAM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qU6Snb4Y51", "forum": "4AsNoESTNC", "replyto": "4AsNoESTNC", "signatures": ["ICLR.cc/2026/Conference/Submission1005/Reviewer_tFjU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1005/Reviewer_tFjU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791780424, "cdate": 1761791780424, "tmdate": 1762915654995, "mdate": 1762915654995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "FmN5y5mAQm", "forum": "4AsNoESTNC", "replyto": "4AsNoESTNC", "signatures": ["ICLR.cc/2026/Conference/Submission1005/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1005/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763050333096, "cdate": 1763050333096, "tmdate": 1763050333096, "mdate": 1763050333096, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VISIONLOGIC, a neuro-symbolic interpretability framework for vision models. It learns per-channel thresholds to convert last-layer activations into a binary predicate vocabulary, induces global, class-wise logical rules over these predicates with a ranking-based inference score, and causally grounds each predicate into a human-readable visual concept via ablation-based localization with iterative box refinement and segmentation masks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Predicates are localized by occlusion-driven search and sufficiency checks, then refined via segmentation; the pipeline operationalizes necessity/sufficiency rather than relying on correlation only.\n* The paper observes shorter rules for CNNs and longer, compositional clauses for Transformers, aligning with recent findings and offering a hypothesis for predicate usage differences."}, "weaknesses": {"value": "* The necessity/sufficiency status of a region depends on the occluder (noise/blur/mean/black/white) and crop-paste context. The paper notes other replacements can work and even that blurring often performs best, but it does not quantify agreement across intervention types.\n* Rule stability. Thresholds and learned rules are post-hoc; the paper does not report seed-to-seed or fine-tuning stability.\n* The final mask uses off-the-shelf segmenters and then re-validates causality. This is sensible, but the choice (SAM, ISNet, Mask R-CNN) may bias boundaries and occasionally produce over fragmented parts.\n* How do coverage and fidelity change on ImageNet-R or style-transferred images (backgrounds, textures)?"}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UPysPLViLC", "forum": "4AsNoESTNC", "replyto": "4AsNoESTNC", "signatures": ["ICLR.cc/2026/Conference/Submission1005/Reviewer_ZJEN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1005/Reviewer_ZJEN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818749809, "cdate": 1761818749809, "tmdate": 1762915654873, "mdate": 1762915654873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method that bridges concept-based explanations for computer vision tasks with predicate-/logic-based ones. The approach is architecture-agnostic and post-hoc. Training follows a two-stage pipeline:\n\nTrain a surrogate classifier on the base model predictions to estimate predicate thresholds and temperature parameters.\n\nTrain a predicate-to-visual-concept linker that maps visual patches (segmentation-based) to the learned predicates.\nExperiments include a large-scale user study showing the efficacy of the method."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is an interesting fusion of existing ideas. The combination of concept-based learning (supported with existing semantic segmentation methods) and logical frameworks is original and useful. The presentation is excellent - concise, well-organized, and easy to follow.\n\nAnother major strength is the extensive user study, which is designed around a solid XAI evaluation framework [1]. Overall, the method and results are convincing and well-presented, meeting ICLR standards.\n\n[1] Colin, Julien, et al. \"What I cannot predict, I do not understand: A human-centered evaluation framework for explainability methods.\" NeurIPS 35 (2022): 2832–2845."}, "weaknesses": {"value": "While I genuinely appreciate this work and believe it would make a good ICLR contribution, it has one major shortcomming: the causal framing is simply not substantiated.\n\nThe paper repeatedly claims to be \"causally grounded\" or \"causally validated.\" The title, abstract, and introduction all position this as a paper in causal inference. But there's no actual causal framework here: no structural causal model, no assumptions about confounding, no causal graph, no identifiability reasoning, nothing that grounds it in the causal inference literature.\n\nWhat the authors call \"causal validation\" is a sensitivity analysis: turning regions on and off, observing changes in predictions. It's useful, informative, and valid as a faithfulness test; and it is NOT causal. Calling it causal sets a dangerous precedent, because then any perturbation or ablation analysis could be sold as \"causal.\"\nIf the authors reframed their method honestly, e.g., \"perturbation-grounded concept validation\", \"necessary-region validation\", or simply \"ablation-based faithfulness tests\", it would stand on its own merits. It's already a strong contribution. It doesn't need causal marketing.\n\nAnother issue is that the evaluation focuses almost entirely on the visual side of explanations. The user study measures how interpretable the visual outputs are, but doesn't say much about the logical predicates or rule-based explanations themselves. Since the method is supposed to connect logic and vision, it's surprising that no human or quantitative evaluation targets the logical rules directly. Maybe the rules do not need to be human-interpretable, but the merit of these rules remains questionable then.\n\nAlso, the baselines could be more diverse. The paper includes concept-based and segmentation-based comparisons (and the saliency/control method), but lacks the standard pixel-level interpretability methods (GradCAM, Occlusion, etc.) that would situate results against the broader XAI landscape, especially given that [1] showed these methods often perform competitively in user studies.\n\n\nA more detailed ablation study would also help. For example:\n\n- How much do predicate thresholds and temperature affect concept stability?\n\n- How does varying the segmentation method influence the link between predicates and visual regions?\n\n- What happens if the predicates are fixed but visual linking changes (in the second training stage)?\n\n\nSuch analyses would clarify how robust each stage of the proposed pipeline really is.\n\nTo be clear: This is a good manuscript. It's technically sound, well-executed, and could be impactful if framed correctly. But the current causal language overreaches. As it stands, the paper is a strong contribution to faithfulness and interpretability, not to causal inference.\n\nThe reason I cannot vote for accept is that the framing is misleading. If the authors remove the causal rhetoric and stick to what they've actually shown, I will update my score accordingly.\n\nFor reference, the kind of \"causal\" deep learning work this paper gestures toward includes explicit causal assumptions, identifiable mechanisms, counterfactuals, or domain generalization arguments - see [2-5]. This paper doesn't engage with that level of rigor, which is fine. Just don't call it causal.\n\n[1] Colin, Julien, et al. \"What I cannot predict, I do not understand: A human-centered evaluation framework for explainability methods.\" NeurIPS 35 (2022): 2832–2845.\n[2] Schulte, Rickmer, David Rügamer, and Thomas Nagler. \"Adjustment for Confounding using Pre-Trained Representations.\" ICML 2025.\n[3] Madras, David, et al. \"Fairness through Causal Awareness.\" FAT* 2019.\n[4] Pettersson, Markus B., et al. \"Time Series of Satellite Imagery Improve Deep Learning Estimates of Neighborhood-Level Poverty in Africa.\" IJCAI 2023.\n[5] Louizos, Christos, et al. \"Causal Effect Inference with Deep Latent-Variable Models.\" NeurIPS 2017."}, "questions": {"value": "What motivated the causal framing? How do the authors see their work to be different from standard correlational approaches?\n\nWhy not include more classical pixel-level baselines (GradCAM, Saliency, Occlusion) for completeness?\n\nCan the authors provide their implementation? This would make the contribution more impactful for the community."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2xWD6MFK6c", "forum": "4AsNoESTNC", "replyto": "4AsNoESTNC", "signatures": ["ICLR.cc/2026/Conference/Submission1005/Reviewer_sGhN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1005/Reviewer_sGhN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891359368, "cdate": 1761891359368, "tmdate": 1762915654697, "mdate": 1762915654697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a causally validated explainability framework termed VisionLogic for visual interpretability. Specifically, VisionLogic has 3 stages: (i) activations are used to construct predicates, (ii) predicates construct inference rules and scores, and (iii) use occlusion ablation to causally validate. The authors address critical gap in concept-based interpretability through causal validation. Additionally, the paper is the first work providing explicit global logical rules at the full ImageNet scale for 4 architectures including transformers."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clarity**: The paper has a clear structure that introduces concepts to the readers in a readable manner. The notations are dense in some places, but not overtly so. The usage of figures and appendices is mostly excellent (except D.1 clarity which I question below). The paper is overall well-written.\n\n2. **Rigorous Human Evaluation**: 465 participants on 3 scenarios with proper statistical testing.  \n\n3. **Sound Technical Framework**: The 3 stages are together well-designed. The methods generalizes across CNNs and Transformers.\n\n4. **Results Scale and Performance**: As the authors state, VisionLogic is the first explainability framework to train and validate on the full ImageNet-1k dataset.\n\n5. **Comprehensive Evaluation**: The authors go above and beyond normal evaluation setups by including adversarial robustness analysis (D.2), multiple segmentation backends and detailed ablations. This is highly appreciated."}, "weaknesses": {"value": "**Technical soundness**:\n\n1. The following sentence is, in my opinion, concerning (Line 153): \"For clarity, all classwise statistics in Section 3 use only training examples correctly classified by the base model, preventing predicate learning from contamination by misclassified instances.\" In datasets like ImageNet, it can be argued that the networks have learned everything there is to learn. Hence, the proposed predicates are entirely dependent on a good model. How good will the VisionLogic Framework be in datasets and networks where this is not the case? In other words, how robust are the activation predicates? I believe this is an important question to address since it's the first step in the entire framework. \n\nPlease note: The robustness I am referring to is different from Section D.2. My concern is with learning the predicates themselves, rather than utilizing *well-learned* predicates for robust explanations.\n\n2. Additionally, utilizing predicates from a learned network based on correct predicates may lead to confirmation bias. The networks may have learned incorrect (correlation) rules which will just be confirmed later. Recent work [1] has shown that correct/incorrect results do not always provide same trends, specifically when evaluation uses occlusion. \n\n[1] Prabhushankar et al. \"Voice: Variance of induced contrastive explanations to quantify uncertainty in neural network interpretability.\" IEEE Journal of Selected Topics in Signal Processing 19.1 (2024): 19-31.\n\n**Result Discussion**\n\n1. I am unsure how to read the metrics in Table 2. App D.1 does not easily clear it up. For instance, in Table 2, ResNet Top-1 accuracy is 69%. Based on D.2, have the authors: (i) chosen N samples among the 50,000 validation images, (ii) among the N samples, M are covered (at least 1 concept activated), and (iii) 69% of M samples' prediction from the original classification layer match the predicate inference? I have similar questions about all other metrics as well.\n\n2. I feel the complexity is a major issue (even though the authors acknowledge this in Line 438). What does > 30 complexity mean? Qualitatively showcasing this is important. Is all the image covered? This goes back to my earlier question about the predicates themselves. Maybe other fine-grained classification datasets may tell the authors more.\n\n3. The paper can use more negative qualitative results. For instance, showcasing the images which are not covered (10-20% in Table 2) can be very informative.\n\n4. Other qualitative results include showcasing polysemanticilty. It will be interesting to see and analyze common features and common features between networks\n\n**Minor weaknesses**:\n\n1. In Eq.1, R^c needs to be defined as ordered rank. Currently the first definition seems to be in Line 240\n\n2. Line 362: identica -> identical"}, "questions": {"value": "Please see the weaknesses. I would be willing to increase my score if the following questions are answered:\n\n1. How robust are the predicates?\n\n2. How would the predicates and results change when including incorrect results (maybe hard to find on ImageNet training set)?\n\n3. Could you please clarify the metrics in Table 2\n\n4. Is there a focused human study where 50+ predicate explanations are shown to be interpretable?\n\n5. Are the uncovered images (i) concentrated in specific classes, or (ii) have lower prediction confidence, or (iii) represent out-of-distribution cases, or (iv) incorrect ground truths, or (v) just something fundamentally impossible to characterize in ImageNet? \n\n6. What percentage of predicates encode multiple distinct concepts, and how does this affect rule trustworthiness and interpretability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4bbq8MVVej", "forum": "4AsNoESTNC", "replyto": "4AsNoESTNC", "signatures": ["ICLR.cc/2026/Conference/Submission1005/Reviewer_yjw5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1005/Reviewer_yjw5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974241031, "cdate": 1761974241031, "tmdate": 1762915654272, "mdate": 1762915654272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}