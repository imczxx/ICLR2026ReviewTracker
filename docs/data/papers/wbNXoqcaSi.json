{"id": "wbNXoqcaSi", "number": 23123, "cdate": 1758339902253, "mdate": 1759896831380, "content": {"title": "Inference-Time Compute Scaling for Flow Matching", "abstract": "Allocating extra computation at inference time has recently improved sample quality in large language models and diffusion-based image generation. In parallel, Flow Matching (FM) has gained traction in language, vision, and scientific domains, but inference-time scaling methods for it remain under-explored. Kim et al., 2025 approach this problem but replace the linear interpolant with a variance-preserving (VP) interpolant at inference, sacrificing FM's efficient and straight sampling. Additionally, inference-time compute scaling for stochastic interpolant models has only been applied to visual tasks, like image generation. We introduce novel inference-time scaling procedures for FM that preserve the linear interpolant during sampling. Evaluations of our method on image generation, and for the first time (to the best of our knowledge), unconditional protein generation, show that I) sample quality consistently improves as inference compute increases, and II) stochastic interpolant inference-time scaling can be applied to scientific domains.", "tldr": "We introduce the first inference-time compute scaling methods for flow-matching that preserve the linear interpolant. We demonstrate this approach for protein generation and image generation.", "keywords": ["Inference-time scaling", "flow matching", "protein design", "generative models"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/62a1f448a17f9fc72c29b23dafbd9c2892e3e0ee.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces an inference-time compute scaling methods for flow matchingthat preserve the linear interpolant, avoiding the diffusion-style VP conversion of prior work. It proposes Noise Search and a two-stage RS+NS strategy, plus a DMFM-ODE noise schedule that injects time-decayed, score-orthogonal perturbations to expand the quality–diversity Pareto frontier."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The motivation of the paper was clear to me. The paper is the first to propose inference-time compute scaling tailored to flow matching while preserving the linear interpolant which keeps straight ODE paths and low step counts, retaining FM’s sampling efficiency. Additionally, I appreciated the results on unconditional protein design, showing that the approach is not limited to vision and can benefit scientific domains."}, "weaknesses": {"value": "A major concern is the claim of score-orthogonal perturbations. As stated, the notion of “orthogonality to the score” is underspecified statistically. What probabilistic meaning it carries for the flow generation? The paper should clarify motivation beyond intuition, and explain its theoretical implications. Additionally, while the authors propose different types of SDE/ODE variants, the paper does not explain why the proposed method should outperform the baselines. Theoretical derivations or at least high level intuitions would enhance readability. \n\nThe qualitative results are also unconvincing. Several examples (Figures 14–17) appear weak or inconsistent, raising doubts about whether the model is well fitted to the data and whether the proposed scaling actually improves perceptual fidelity rather than overfitting to the verifier. The model generates underfitted samples at 1x compute cost, which might indicate that the model has not been fully converged. \n\nLastly, the experiments focus on moderate compute scales, leaving open how well the methods scale to larger models or higher-dimensional domains. Can the authors test their method on a large scale generative models (FLUX: image generation, WAN: video generation) with a more practical reward function?"}, "questions": {"value": "Injecting stochasticity often necessitates using a smaller discretization step to maintain sample quality. Has the proposed method observed a similar effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O4IY8GOcX4", "forum": "wbNXoqcaSi", "replyto": "wbNXoqcaSi", "signatures": ["ICLR.cc/2026/Conference/Submission23123/Reviewer_AfW2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23123/Reviewer_AfW2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761266194888, "cdate": 1761266194888, "tmdate": 1762942521171, "mdate": 1762942521171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores inference-time compute scaling for flow matching (FM) models, aiming to improve sample quality by allocating additional test-time computation without retraining. Building on prior work on inference-time scaling in diffusion models, the authors propose an approach that preserves the linear interpolant characteristic of FM while introducing controlled stochasticity (DMFM-ODE) and a search mechanism. Experiments on ImageNet-256 and FoldFlow2 (for protein generation) demonstrate improvements in FID, Inception Score, and protein design metrics as the compute budget increases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper addresses the inference-time compute scaling for flow-matching models without converting them to diffusion-like samplers.\n- The proposed Noise Search and RS+NS algorithms are conceptually straightforward yet effective."}, "weaknesses": {"value": "- While the paper’s techniques are effective, many of the core ideas build upon existing strategies rather than inventing new paradigms. The paper's main distinction is well-motivated but represents an incremental methodological refinement.\n- The justification for the DMFM-ODE variant is relatively weak. The approach relies on empirically tuned heuristics, and the claim that linear interpolant scaling outperforms VP-trajectory in flow matching lacks sufficient theoretical or experimental support. Additional ablations, such as analyzing sample trajectories or verifier score distributions, would strengthen the argument and clarify whether the method generalizes beyond the current tuning regime.\n- This paper does not provide any direct head-to-head comparison to the concurrent methods despite heavily citing them. For the figures such as Figure 4, RS continues improving with additional compute, and the convergence behavior is also missing."}, "questions": {"value": "- In image generation, classifier guidance is commonly used to enhance sample fidelity. Did you employ classifier-free guidance (CFG) or any similar technique in your experiments and for example, Figure 1?\n- If not, can your approach be combined with CFG, and do you anticipate that such integration would further improve performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SGG8Ye9O1r", "forum": "wbNXoqcaSi", "replyto": "wbNXoqcaSi", "signatures": ["ICLR.cc/2026/Conference/Submission23123/Reviewer_gTSm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23123/Reviewer_gTSm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834276899, "cdate": 1761834276899, "tmdate": 1762942520754, "mdate": 1762942520754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies inference time compute scaling for flow matching while preserving the linear interpolant. The core methodolical procedures are a randomized ODE sampler that injects time decayed, score orthogonal noise with optional weak particle guidance, and a search procedure that branches and selects along the same deterministic linear path (Noise Search). The authors also propose a practical two stage variant that runs best of N over initial conditions and then applies Noise Search from saved intermediate states. Experiments are reported for ImageNet 256 with a pre-trained SiT XL2 model and, notably, unconditional protein backbone generation with FoldFlow2, using scTM and scRMSD via a ProteinMPNN plus ESMFold evaluation loop. The paper claims a consistent quality gains as test time compute grows and argues that keeping the linear interpolant is advantageous for straight trajectories in few steps generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- I believe this paper have a well-motivated setting. Inference time compute scaling for diffusion models via noise search and verifier guided selection has been established, with algorithms and framing similar to the Best of N and path search used here, but in the diffusion setting. \n- The randomized ODE that injects score orthogonal perturbations while staying on the linear FM path appears new relative to EDM-style SDE noise injection and to particle guidance, which previously targeted diffusion.\n- The paper isolates an inference time strategy that stays within the flow matching regime and empirically shows monotone improvements on ImageNet and on proteins, including a clear improvement in the percentage of structures with scRMSD below two angstroms at larger budget."}, "weaknesses": {"value": "- I have a minor concern about the novelty of the paper. The particle guidance repulsion [1]  and budget forcing [2] is exactly from respectively previous work (the authors did mention it).\n\n- The experimental results show monotone improvements with larger search. However, the compute accounting is not aligned across methods, which weakens empirical support.\n\n- Ablations on the noise injection are limited. The EDM and SDE ablations are useful but do not disentangle the roles of score orthogonality and particle coupling. \n\n[1] Gabriele Corso, Yilun Xu, Valentin de Bortoli, Regina Barzilay, and Tommi Jaakkola. Particle guidance: non-i.i.d. diverse sampling with diffusion models, 2023\n\n[2] Jaihoon Kim, Taehoon Yoon, et al. Inference-time scaling for flow models via stochastic generation and rollover budget forcing. 2025"}, "questions": {"value": "- For the orthogonal score, can you quantify the changes in $\\mathbb{E}[\\nabla \\cdot w_t]$ or bound it under a local smoothness model for the learned score? Currently, Theorem 2 removes the term $w_t s_t p_t$, but I think in order for the claim \"score-orthogonal perturbations minimize the probability-weighted divergence contribution\", you need to control $\\nabla \\cdot w_t$ as well.\n\n- How sensitive is Randon search + Noise Search to the 9 rounds schedule?\n\n- Could you add a matched compute comparison against the VP SDE approach of Kim et al. 2025 [1], both in image and in protein generation, and clarify whether the diversity increase from interpolant conversion helps or hurts at identical NFEs?\n\n[1] Jaihoon Kim, Taehoon Yoon, et al. Inference-time scaling for flow models via stochastic generation and rollover budget forcing. 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ndbjS3OIFr", "forum": "wbNXoqcaSi", "replyto": "wbNXoqcaSi", "signatures": ["ICLR.cc/2026/Conference/Submission23123/Reviewer_kwmg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23123/Reviewer_kwmg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965010303, "cdate": 1761965010303, "tmdate": 1762942520410, "mdate": 1762942520410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an inference-time scaling method for generative models (particularly diffusion or flow-based models) by introducing a particle-based sampling scheme with a \"noise search\" component to improve sample diversity. In practice, multiple latent trajectories (particles) are generated in parallel during the reverse diffusion process, and an additional step is taken to adjust or project the initial noise samples in an attempt to make them more diverse (the so-called noise search). The goal is to leverage extra computation at inference (multiple particles and possibly more integration steps) to achieve better output quality or alignment with desired objectives. The authors demonstrate the approach on several tasks including some novel application scenarios for diffusion models, and report improvements in metrics such as sample diversity and possibly downstream rewards (e.g. Inception Score or other task-specific measures). The main contributions, as outlined by the paper, appear to be:\n\n* Noise-space search: A procedure to modify or select initial Gaussian noise vectors for each particle, intended to encourage diversity among particles.\n\n* Inference-time particle sampling: Using multiple particles (stochastic trajectories) during generation to improve the chance of high-quality or reward-aligned outcomes, without retraining the model.\n\n* Applications to new tasks: Adapting the particle-based diffusion sampling to novel tasks (for example, reward-guided image generation or other conditional generation scenarios), showcasing the flexibility of the approach.\n\nWhile the idea of using extra computation and particles at inference is in line with recent trends in diffusion model research, the paper’s novelty largely lies in applying this concept to a broader set of tasks rather than introducing fundamentally new methodology."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper tackles the problem of inference-time optimization for generative models, which is timely and relevant. Improving sample quality or alignment without additional training is valuable for deploying diffusion models under strict computational budgets.\n\nBuilding on sequential Monte Carlo (SMC) style sampling for diffusion models, the use of multiple particles can in principle improve diversity and success rate. The idea of a \"noise search\" to initialize particles is intuitively plausible.\n\nThe authors evaluate the method on a range of tasks, including some novel settings."}, "weaknesses": {"value": "Unfortunately, the paper in its current form has significant weaknesses that undermine its contributions. The overall novelty and empirical support are not convincing, and several important baselines or design choices appear to have been overlooked.\n\nThe core idea of improving diversity by projecting or orthogonalizing initial Gaussian noise samples is not well justified. In high-dimensional latent spaces, random Gaussian vectors are already nearly orthogonal to each other due to concentration of measure. In other words, if one simply samples $N$ random noise vectors in a high dimension, the pairwise cosine similarities will with high probability be close to zero. This raises skepticism about how much the proposed noise projection actually increases diversity beyond what random sampling provides. The authors claim that Figure 2 demonstrates a meaningful effect of the method; however, it’s unclear if the plotted metric or visual quality genuinely improves with the noise search, since the baseline SDE already provides similar outcomes in the region that the baseline SDE gives similar performance as baseline deterministic ODE sampler.\n\nThe paper overlooks or downplays the findings of Kim et al. (2025), which is a significant recent work in inference-time scaling for generative models. Kim et al. introduce the idea of using a variance-preserving (VP) interpolant instead of a linear interpolant during the generative process, effectively allowing the sampling trajectory to stay closer to the noise distribution for longer and thus better maintain diversity. However, the submission under review continues to use a linear interpolation in the diffusion process and does not experiment with the VP approach. At minimum, some discussion or justification was needed as to why a linear schedule was used over a VP interpolant, given the known benefits of the latter.\n\nAnother related point from Kim et al. (2025) is the Rollover Budget Forcing (RBF) strategy, which adaptively allocates computation (function evaluations) across time steps. The current paper treats \"number of diffusion steps\" as the main handle for inference cost, but in reality, for particle-based methods, the total computation is roughly (#particles) x (#steps). Thus, RBF distributes more particles or function evaluations to the stages of sampling that matter most. The submission does not explore any adaptive allocation of its NFE (number of function evaluations) budget across the diffusion timeline. This omission is critical because the success of inference-time scaling often comes from using the budget wisely, not just brute-forcing more steps uniformly. By not comparing against or incorporating RBF-based step allocation, the paper’s approach may be suboptimal and it misses insights from prior art on how to best schedule multiple particles over time.\n\nIn the experimental evaluation, since Inception Score (IS) is used as a target for improvement, it is important to compare against methods that directly optimize this metric. Given that Inception Score is differentiable, one could use a method like Ψ-Sampler (Yoon et al., 2025) to directly maximize it, potentially achieving better results than heuristic noise search. Without this comparison, the paper fails to convince that its approach is competitive in scenarios where a known differentiable reward-based sampler exists. It also leaves a gap in positioning: is the proposed method offering any advantage (e.g., simplicity or speed) over such approaches? We cannot tell, because the authors did not include this baseline or discussion.\n\nThe only notable new aspect claimed is applying the particle inference to some \"novel tasks.\" While exploring new tasks can be valuable, it is not a strong technical contribution on its own. The method itself seems to be an incremental combination of existing ideas with a minor twist. There is no substantial theoretical insight or algorithmic breakthrough presented. For example, the paper doesn’t propose a new objective or a fundamentally new sampling algorithm; it mainly repurposes known techniques. Simply demonstrating those techniques on new tasks (e.g., perhaps optimizing a new type of reward, or applying diffusion to a new domain) is insufficient for a strong contribution. Overall, the submission lacks novelty in methodology and does not significantly advance the state-of-the-art beyond what prior work has already established."}, "questions": {"value": "Efficacy of Noise Search. High-dimensional random Gaussians are already nearly orthogonal, what specific benefit does the proposed noise search bring?\n\nDid you experiment with a VP interpolant in your framework, and if not, can you justify why the linear approach is still used? It would be useful to know if the proposed contributions are complementary to such an interpolant change.\n\nWhy did you set the timestamp to [0.0, 0.2, 0.4, 0.6, 0.75, 0.8, 0.85, 0.9, 0.95]?\n\nWas any strategy like Rollover Budget Forcing (RBF) considered to adaptively spend more computation on critical steps? If you simply fixed the number of particles and steps uniformly, there might be untapped efficiency. Please comment on whether an adaptive budget assignment could improve results, and why it was not explored.\n\nGiven that your work targets improvement in metrics like Inception Score and overall sample quality, methods that directly optimize these objectives via gradient-based sampling are highly relevant. Is there any comparisons?\n\nAside from applying the existing SMC diffusion framework to new tasks, what do you consider the key technical innovation of this work? The current components (particle sampling, noise orthogonalization, more diffusion steps) all seem borrowed or relatively minor modifications. Can you point to any aspect of the algorithm that is fundamentally new? This will help assess the contribution. If the novelty is primarily in the applications, how do you justify that as sufficient for publication?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hLGSXTiHxL", "forum": "wbNXoqcaSi", "replyto": "wbNXoqcaSi", "signatures": ["ICLR.cc/2026/Conference/Submission23123/Reviewer_WCen"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23123/Reviewer_WCen"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980984126, "cdate": 1761980984126, "tmdate": 1762942520049, "mdate": 1762942520049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}