{"id": "yFaIaj40Fr", "number": 5570, "cdate": 1757920521696, "mdate": 1759897967271, "content": {"title": "Sample Margin-Aware Recalibration of Temperature Scaling", "abstract": "Deep neural networks often exhibit overconfidence despite their high accuracy. Such miscalibration limits reliability in safety-critical domains where trustworthiness are crucial. Post-hoc calibration methods offer a practical solution where popular approaches like Temperature Scaling (TS) apply a single corrective parameter to all samples, failing to address the sample-dependent nature of miscalibration. While more advanced methods attempt to adapt to sample difficulty, they often rely on complex and indirectly learned proxies.\nIn this work, we first identify the \\emph{logit margin} as a direct, simple, and principled indicator of sample hardness. We provide substantial empirical and theoretical evidence that it serves as a more effective indicator of sample hardness than existing proxies. Meanwhile, we identify a fundamental flaw in current methods that optimizing Negative Log-Likelihood can paradoxically degrade calibration. To resolve this, we introduce Huber–SoftECE, a novel and theoretically guaranteed objective that directly minimizes calibration error.\nBuilding on these insights, we propose Sample Margin-Aware Recalibration of Temperature (SMART), a lightweight post-hoc method that learns a minimalistic sample-wise mapping from the logit margin to an optimal temperature, guided by our calibration-centric objective. Extensive experiments show state-of-the-art performance for calibration across diverse architectures and datasets with a minimal inference-time data consumption. The code is available at: \\url{https://anonymous.4open.science/r/SMART-8B11}.", "tldr": "We propose SMART, a lightweight post-hoc calibration method, that leverages margin to achieve SOTA calibration performance with minimal parameters and extremely limited validation data across diverse architectures and distribution shifts.", "keywords": ["Confidence Calibration", "Temperature Scaling", "Logit Margin", "Out-of-Distribution Detection", "Deep Neural Networks", "Deep Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf59ee1c00f1f84e6ac379d422e90756e8756e93.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposed a temperature scaling (TS) calibration method which utilizes the margin between the first and second largest logits as input to a per-sample parameterization of temperature. A soft ECE calibration objective is proposed."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The empirical results show that the proposed approach outperforms alternative methods in various settings."}, "weaknesses": {"value": "* \nThe novelty should be stated more clearly. There exist works, such as [Wei et al. 2022], that identified relation between dominant logits values and model calibration; there exist works, such as [Karandikar et al. 2021], that proposed and studied soft and differentiable versions of ECE; and there exist works, such as [Tomani et al., 2022], where the temperature is parameterized and gets as input the sorted logic vector (which includes the margin between the first and second largest entries).\n\n* \nThe statement: \"Current post-hoc calibration methods assume minimizing NLL improves calibration\" (Line 161) does not seem to be accurate, as it is common to optimize the temperature with objectives other than NLL, such as directly using ECE or its adaptive or soft versions ([Frenkel & Goldberger, 2021], [Karandikar et al. 2021], and many more).\n\n* \nIn line 158 you point to Figure 2, which shows that optimizing NLL may not improve test ECE. You should provide more details on the experimental setting associated with this figure.  \nFrom this figure it is also observed that optimizing the proposed softECE is not better than optimization of the plain ECE, which is practical when the number of variables is small enough (e.g., 1D scan for plain TS). Please elaborate on this.\n\n* \nThe presentation of the theoretical analysis and the proposed method should be significantly improved.\n\n* \nLemma 1 is not clear, and includes notations that are not defined.  \nSimilarly for Prop 3.1 and Lemma 2. For example, you discuss smCE and SMART without even defining them.\n\n* \nThe presentation in Section 3.3 also needs to be improved.  \nWhat do you mean by \"rho is reference density\"? What do you mean by \"Under mild regularity conditions\"? Your definitions for SoftECE and H_{\\lambda,\\delta} depend on the choice of kernel. How does the result in Theorem 3.2 relate to the actual ECE?\n\n* \nIn section 3.4, you should clearly describe the proposed method. Moreover, in the appendix there is a normalization step that has not been discussed.\n\n* \nRegarding the experiments, you should compare also to TS where the temperature is optimized using ECE and adaptive ECE."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C4jrXGZodL", "forum": "yFaIaj40Fr", "replyto": "yFaIaj40Fr", "signatures": ["ICLR.cc/2026/Conference/Submission5570/Reviewer_9SH3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5570/Reviewer_9SH3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865418253, "cdate": 1761865418253, "tmdate": 1762918142344, "mdate": 1762918142344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical miscalibration issue (overconfidence) of deep neural networks (DNNs) in safety-critical domains (e.g., autonomous driving, medical diagnosis) by proposing SMART (Sample Margin-Aware Recalibration of Temperature), a lightweight post-hoc calibration method. The key contributions are threefold: (1) Identifying logit margin (difference between the largest and second-largest logits) as a direct, principled indicator of sample hardness—supported by theoretical bounds (it tightly constrains optimal temperature) and empirical evidence (correlation of 0.756 with FGSM attack perturbation). (2) Proposing Huber–SoftECE, a differentiable objective function that directly minimizes calibration error and theoretically upper-bounds smooth calibration error (smCE), resolving the fundamental mismatch between Negative Log-Likelihood (NLL) optimization and calibration goals (where NLL reduction can paradoxically increase ECE). (3) Designing SMART, which uses a 2-layer MLP (only 49 parameters) to learn a sample-wise mapping from logit margin to optimal temperature, guided by Huber–SoftECE. Extensive experiments across diverse architectures (CNNs: ResNet, Wide-ResNet; Transformers: ViT, Swin-B) and datasets (CIFAR-10/100, ImageNet, ImageNet-C/LT/Sketch) demonstrate that SMART achieves state-of-the-art (SOTA) calibration performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Theoretically Grounded Objective Function: Huber–SoftECE addresses a longstanding flaw in NLL-based calibration by directly targeting calibration error. The theoretical guarantee that it upper-bounds smCE ensures alignment between optimization and calibration goals, a rare strength in post-hoc methods that often lack such rigor.\n\n2. Strong Practical Utility: SMART balances performance and efficiency: its lightweight MLP design avoids the computational burden of exisiting methods, making it feasible for large-scale datasets."}, "weaknesses": {"value": "The paper acknowledges that SMART may degrade in zero-shot scenarios but provides no further details—e.g., whether it can leverage cross-domain margin signals, or if pre-trained margin-temperature mappings transfer to new domains. This is a critical gap for safety-critical applications where validation data may be scarce."}, "questions": {"value": "For zero-shot or low-data calibration scenarios, have you explored strategies like transfer learning for the margin-temperature mapping? If so, what performance trends did you observe? If not, do you have theoretical or empirical insights into why such transfer might (or might not) work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F5js5gH6Ve", "forum": "yFaIaj40Fr", "replyto": "yFaIaj40Fr", "signatures": ["ICLR.cc/2026/Conference/Submission5570/Reviewer_XnHd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5570/Reviewer_XnHd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878951956, "cdate": 1761878951956, "tmdate": 1762918141950, "mdate": 1762918141950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new post-hoc calibration method, Sample Margin-Aware Recalibration of Temperature (SMART), which learns a sample-wise temperature scaling function. The core ideas are: first, to use the logit margin (the difference between the top two logits) as a simple proxy of sample hardness; and second, to introduce a novel loss function, Huber-SoftECE, which is designed to directly optimize calibration error and address a demonstrated mismatch between NLL optimization and calibration improvement. The method is shown to achieve state-of-the-art ECE on several standard benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper's motivation is clear and addresses important limitations of existing methods. The identification of the logit margin as a lightweight input for sample-wise calibration is interesting and supported by both theoretical arguments (Prop. 3.4) and empirical analysis.\n- The proposed method is lightweight and demonstrates good empirical performance on the reported datasets (CIFAR-10/100, ImageNet) and their variants, consistently outperforming a wide range of baselines."}, "weaknesses": {"value": "My main concerns are regarding missing baselines, the theoretical justification for the proposed objective, and the limited scope of the experimental evaluation.\n- Missing Baselines and modern models: A  highly relevant baseline is missing from the evaluation: Density Aware Calibration (Tomani et al., ICML 2023) is a recent, sample-adaptive method that also aims to provide robust calibration, particularly under distribution shift. Given that the authors make strong claims on robustess to such shifts, a comparison to this state-of-the-art method is essential. Similarly, please also report results on contemporary large-scale models such as EVA, BEIT and ConvNext.\n- Theoretical Concerns: I am not convinced by the theoretical framing around the Huber-SoftECE objective. While the authors correctly identify the problem with NLL, their proposed solution seems unnecessarily complex. As shown by Gruber & Buettner (NeurIPS 2022), every proper scoring rule induces a corresponding calibration error (e.g., the Brier score induces the canonical  calibration error). They also show that for injective transformations, such as the sample-wise temperature scaling, the difference in the root Brier score (ΔRBS) reliably estimates the improvement in calibration. Given this, the authors should  compare their Huber loss to a much simpler baseline of directly optimizing the Brier score. More importantly, they should quantify ΔRBS as a principled measure for calibration improvement, instead of relying on biased binned estimators like ECE. The ablation in Table 3 is insufficient as it only compares objectives, not the use of ΔRBS as a more fundamental evaluation metric. As a side note: I suggest to not report ECE as %, it's not bound between 0 and 1. \n- Limited Evaluation of Robustness: The investigation into calibration under distribution shift is only superficially investigated. While experiments on ImageNet-C, -LT, and -Sketch are included, it is unclear if the method's strong performance generalizes beyond these standard synthetic benchmarks to any real-world distribution shifts . To make stronger claims about robustness, an evaluation on a benchmark like WILDS would be necessary."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vFhLq2vYOd", "forum": "yFaIaj40Fr", "replyto": "yFaIaj40Fr", "signatures": ["ICLR.cc/2026/Conference/Submission5570/Reviewer_Bb97"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5570/Reviewer_Bb97"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944131412, "cdate": 1761944131412, "tmdate": 1762918141549, "mdate": 1762918141549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SMART, a post-hoc calibration method that:\n- uses the logit margin (the gap between the top two logits) as a proxy measure of sample hardness, serving as input to a lightweight 2-layer MLP;\n- outputs a sample-specific temperature for recalibration; and\n- introduces Huber–SoftECE, a smooth and differentiable calibration objective to train this mapping.\n\nThe contributions are twofold:\n1. Identifying logit margin as an effective and interpretable proxy for sample hardness;\n2. Demonstrating that minimizing Negative Log-Likelihood (NLL) can worsen calibration under confidence heterogeneity, and proposing Huber–SoftECE to align optimization with true calibration objectives, offering formal guarantees.\n\nExtensive experiments on CIFAR-10/100, ImageNet, and distribution-shift benchmarks (ImageNet-C, ImageNet-Sketch) show consistent and significant calibration improvements over prior post-hoc approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a rigorous and original theoretical explanation for why NLL optimization fails to align with calibration objectives (Proposition 3.1, Lemma 2). \n2. It (derivation in Appendix A.1) nicely shows that the feasible temperature range is linearly correlated with the logit margin, partially justifying using the margin as a key indicator of the optimal temperature.\n3. The proposed Huber–SoftECE objective inherits the differentiability of SoftECE while offering greater training stability and a theoretical upper-bound guarantee on the calibration error.\n4. The empirical evaluation is extensive and convincing, covering diverse architectures (CNNs and ViTs), as well as long-tailed and corrupted datasets. Comparisons against both post-hoc baselines (TS, PTS, CTS, Spline, GC, ProCal) and training-time baselines (Brier, MMCE, FL, LS) are thorough and well-executed.\n5. The method is simple yet elegant. Its minimal design—a two-layer MLP with only 49 parameters—and strong data efficiency (remaining robust with as few as 50 validation samples) make it highly practical for real-world deployment. In general, this is the most appealing feature in this paper."}, "weaknesses": {"value": "1. The paper combines two somewhat known ideas—logit margin as a hardness signal and soft-binned calibration loss. The theoretical insight about NLL–ECE misalignment is new, but intution is widely known in this community.  The the methodological innovation may be perceived as an incremental refinement rather than a paradigm shift.\n2. Limited intuition for Huber–SoftECE behavior. While the theorem provides an upper-bound guarantee, the paper does not include ablation or visualization showing how Huber–SoftECE gradients differ from NLL or SoftECE in practice (e.g., gradient field analysis, convergence plots)."}, "questions": {"value": "1. Figure 1 shows that in general the model is underconfident, which is not a common case in most neural networks. Have you conducted a larger analysis on more datasets and models?\n2. What kind of dataset and model do you use in Figure2d? Could you provide more concrete empirical evidence (or visualization) illustrating how optimizing NLL decreases ECE in the early epochs?\n3. How sensitive is Huber–SoftECE to the kernel bandwidth λ and smoothing δ? Are these parameters dataset-dependent?\n4. Is there any theoretical intuition on why the margin-to-temperature mapping is approximately monotonic? \n5. Would combining SMART with vector or class-wise temperature scaling bring further gains, or are these directions redundant given the per-sample mapping?\n6. How would SMART behave if the calibration set is heavily class-imbalanced—does the mapping generalize across rare vs. frequent classes?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "I find that some of the citations are not accurate and seem to be generated by AI. For example, \nI cannot find this paper reference: Mix and match: A strategy for training object detection models with noisy annotations.\nFor another paper: Miao Xiong, Chien-Yi Hsieh, Beier Yang, Serim Moon, Finale Doshi-Velez, and Krzysztof Z Gajos. Proximity-informed calibration for deep neural networks. arXiv preprint arXiv:2306.04590, 2023. The author list is not correct."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rs8nJ2hwQe", "forum": "yFaIaj40Fr", "replyto": "yFaIaj40Fr", "signatures": ["ICLR.cc/2026/Conference/Submission5570/Reviewer_ubNf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5570/Reviewer_ubNf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971751797, "cdate": 1761971751797, "tmdate": 1762918141165, "mdate": 1762918141165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}