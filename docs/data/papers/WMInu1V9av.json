{"id": "WMInu1V9av", "number": 20735, "cdate": 1758309483317, "mdate": 1759896961233, "content": {"title": "AURA: Visually Interpretable Affective Understanding via Robust Archetypes", "abstract": "Text-driven vision--language methods (e.g., CLIP variants) face three persistent hurdles in affective computing: (i) limited support for continuous regression (e.g., Valence--Arousal), (ii) brittle reliance on language prompts, and (iii) the absence of a unified paradigm across expression classification, action-unit detection, and affective regression. We introduce AURA, a prompt-free framework that operates directly in a frozen CLIP visual space via visual archetypes. AURA comprises two components: (1) self-organized archetype discovery, which adaptively allocates the number of archetypes per affective state, assigning denser archetype sets to complex or ambiguous states for fine-grained interpretability, and (2) archetype contextualization, which models interactions among the most relevant archetypes and semantic tokens to enhance structural consistency while suppressing redundancy. Inference reduces to cosine matching between projected features and fixed archetypes. Across six datasets, AURA consistently surpasses prior state-of-the-art while remaining highly efficient. Overall, AURA unifies classification, detection, and regression under a single visual-archetype paradigm, delivering strong accuracy, cognitively aligned interpretability, and excellent training/inference efficiency.", "tldr": "", "keywords": ["valence-arousal", "facial expressions", "action units"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11d80b26cfbba1b7249935b259293245fa795647.pdf", "supplementary_material": "/attachment/5f5b94eec5557f7c3a376d134a43f7d1482e2d33.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes AURA, a prompt-free framework for affective understanding that operates via self-organized visual archetypes in a frozen CLIP visual space. It aims to unify classification (FER), detection (AU), and regression (VA) tasks, claiming SOTA performance, improved interpretability, and high efficiency across six benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: Introduces a unified visual-archetype paradigm, moving away from brittle text prompts.\n\n2. Significance: A well-motivated approach with potential to influence affective computing and interpretable ML.\n\n3. Experiments: Comprehensive evaluation across three tasks; ablation studies support design choices."}, "weaknesses": {"value": "1.  **Misleading Claim of \"Direct\" Operation in CLIP Space:** The paper repeatedly states it operates \"directly in a frozen CLIP visual space.\" However, the core of the method relies on a **trainable Visual Archetype-Space Projector (VAS)**, `ℱ(·)`, which transforms the original CLIP features into a new, task-specific \"archetype space.\" This is not a direct operation but a learned adaptation. This phrasing is conceptually misleading and should be clarified to avoid confusion.\n\n2.  **Insufficient Detail on Adaptive Archetype Mechanism:** While the \"self-organized archetype discovery\" is a key contribution, the process for adaptively determining the final number of archetypes per state is underspecified. The text mentions it \"adaptively discovers around 100 archetypes\" but provides no concrete criteria, convergence conditions, or initialization strategy for this process. The lack of reproducibility details here is a significant methodological weakness.\n\n3.  **Inadequate Explanation for Feature Granularity Results:** The ablation study (Table 2) shows that patch-level features, intended for local AU detection, slightly outperform global features on RAF-DB (a global FER task). This counter-intuitive result is not discussed or analyzed. The authors should investigate whether this indicates that local features are unexpectedly beneficial for certain expression categories or if it reveals a more complex relationship between feature granularity and task performance that is not yet understood."}, "questions": {"value": "1.  Please clarify the methodological description: The framework learns a projection from the CLIP space to an archetype space. Revising the \"operates directly\" claim would improve conceptual accuracy.\n2.  What are the specific, concrete criteria or algorithm for determining the final number of archetypes `K` in the self-organizing process? How is it initialized and when does the adaptation converge?\n3.  How do you explain the superior performance of patch-level features on the global FER task (RAF-DB) in your ablation study? Please provide a hypothesis or further analysis.\n4.  Please provide all supplementary material (Appendices A, C) for a complete assessment of implementation details and reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3FOYJnK3fC", "forum": "WMInu1V9av", "replyto": "WMInu1V9av", "signatures": ["ICLR.cc/2026/Conference/Submission20735/Reviewer_EDGc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20735/Reviewer_EDGc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749276247, "cdate": 1761749276247, "tmdate": 1762934151515, "mdate": 1762934151515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AURA (Affective Understanding via Robust Archetypes), a prompt-free framework for affective computing that operates in the frozen visual embedding space of CLIP. AURA unifies facial expression recognition (FER), action unit detection (AUD), and valence-arousal estimation (VAE) through a shared paradigm based on learnable visual archetypes. These archetypes are discovered via a self-organized mechanism that adaptively allocates more prototypes to affective states with higher visual or semantic complexity. Predictions are made by cosine matching between projected features and fixed archetypes, enabling efficient inference. The method is evaluated across six benchmarks and reports state-of-the-art results with low computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A new framework is presented that integrates three different affective tasks into a unified architecture.\n2. The model is lightweight and computationally efficient, with a low number of parameters and floating-point operations (FLOPs), making it suitable for practical use.\n3. An adaptive prototype allocation mechanism helps determine the number of prototypes for each class or state, based on the data, reflecting the emotional complexity of the task.\n4. Task-aware regularization techniques are introduced for both classification and regression tasks, including score-based attraction and repulsion for continuous labels.\n5. Evaluation is conducted across multiple datasets and tasks, such as FER, AUD, and VAE, with a clear comparison to state-of-the-art (SOTA) methods."}, "weaknesses": {"value": "1. The use of only the visual branch of CLIP for purely visual tasks is a reasonable design choice, but this approach is widely adopted in recent literature and does not represent a conceptual novelty. The core mechanism relies on learnable prototypes, a well-established technique in metric and few-shot learning.\n2. While the introduction of the term \"archetype\" may create the impression of conceptual novelty, the main technical difference seems to be the adaptive density of prototypes based on label complexity - a useful but incremental extension.\n3. Interpretability is primarily demonstrated through post-hoc archetype visualizations and error diagnosis, but it is unclear how this approach can explain individual model decisions like attention-based or saliency-based methods. The current form of interpretability is more useful for dataset curation rather than understanding per-sample model behavior.\n4. While archetype visualizations highlight annotation errors, the paper does not analyze cases where the model’s prediction is wrong despite correct labels, which would better test the robustness of the archetype-based representation.\n5. The framework is applied separately to each task, using task-specific archetypes and heads. This limits the claim of \"unified modeling\", as there is no shared representation or joint optimization across FER, AUD, and VAE."}, "questions": {"value": "1. The authors could explain why they use the term \"archetype\" beyond stylistic or motivational reasons. Is there a specific reason why this term is necessary for their approach? Is there any formal distinction between their approach and other methods of adaptive prototype learning?\n2. How would AURA explain a correctly labeled but misclassified sample?\n3. It would be interesting to see if it is possible to train a shared set of archetypes for FER, AUD, and VAE using multi-task learning. If this is not possible, what are the fundamental reasons for this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uwUetCjKkw", "forum": "WMInu1V9av", "replyto": "WMInu1V9av", "signatures": ["ICLR.cc/2026/Conference/Submission20735/Reviewer_wT3U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20735/Reviewer_wT3U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759459400, "cdate": 1761759459400, "tmdate": 1762934150559, "mdate": 1762934150559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AURA, a prompt-free framework for affective understanding that operates directly on frozen CLIP visual embeddings. The core problem it addresses is the threefold limitation of existing VLM-based methods in affective computing: (i) incompatibility with continuous regression tasks (like Valence-Arousal), (ii) reliance on brittle and labor-intensive text prompts, and (iii) the lack of a unified paradigm across Facial Expression Recognition (FER), Action Unit Detection (AUD), and VA regression (VAE). AURA's main contribution is a \"Self-organized Archetype Discovery\" module, which learns a set of visual archetypes that serve as perceptual anchors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The shift from a text-dependent, \"text-image matching\" paradigm to a \"visual-archetype matching\" one is a significant and compelling contribution. This prompt-free approach directly solves a major, practical bottleneck in applying VLMs to specialized domains like affective computing.\n\n2. The ability of a single framework to successfully unify classification (FER), detection (AUD), and continuous regression (VAE) is a major strength. The paper's design choices, using global-level embeddings for FER/VAE and patch-level embeddings for AUD, combined with distinct regularization strategies, are well-motivated and empirically validated.\n\n3. The paper achieves SOTA performance across all three task categories on six different benchmarks. It achieves this with the lowest parameter count and FLOPs compared to other SOTA methods. The inference-time design (omitting the contextualization module and using only a projector and cosine matching) makes it extremely practical for deployment."}, "weaknesses": {"value": "1. While inference is exceptionally simple, the training process appears highly complex. The total loss is a weighted sum of three major components. $\\mathcal{L}^{Arc}$ is itself a composite of three other losses 23, one of which ($\\mathcal{L}^{Reg}$) has two entirely different formulations depending on the task. The paper provides little to no discussion on the sensitivity to the various weighting coefficients ($\\lambda_{Proj}$, $\\lambda_{Arc}$, $\\lambda_{Contx}$) or the margins ($m$) used in the regularization losses. This complexity could be a significant barrier to reproducibility.\n\n2. The paper claims to evaluate on a video benchmark (DISFA). However, the AURA architecture appears to be a frame-based processor. It uses patch-level features for AUD, but there is no mechanism described for modeling temporal dependencies across frames. Therefore, its excellent results on DISFA are for frame-level AU detection, not \"video-level\" analysis in a temporal sense."}, "questions": {"value": "The \"visual archetype\" paradigm is very promising. Do the authors believe this approach could be generalized to other domains beyond affective computing, where VLMs also struggle with prompt-brittleness, such as fine-grained visual categorization (FGVC) or other visual regression tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I9pZ6yxPZB", "forum": "WMInu1V9av", "replyto": "WMInu1V9av", "signatures": ["ICLR.cc/2026/Conference/Submission20735/Reviewer_GcZi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20735/Reviewer_GcZi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970452788, "cdate": 1761970452788, "tmdate": 1762934149390, "mdate": 1762934149390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AURA (Affective Understanding via Robust Archetypes), a visually interpretable framework for emotion understanding built on a frozen CLIP visual space. Instead of relying on textual prompts, AURA models emotions through adaptive visual archetypes that self-organize and contextualize within the embedding space, enabling unified handling of facial expression recognition, action unit detection, and valence-arousal regression. The approach combines efficiency and interpretability by matching features to archetypes through cosine similarity during inference, achieving competitive or superior results on six benchmarks with lower computational cost."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper proposes a clear and well-motivated framework that unifies multiple affective understanding tasks (FER, AU, VA) within a single, interpretable visual archetype space.\n2. AURA achieves strong empirical performance across six benchmarks while maintaining low computational cost and providing visual interpretability through archetype-based reasoning."}, "weaknesses": {"value": "The maximum number of archetypes ($K_{\\max}$) is fixed (e.g., 98) without any accompanying sensitivity or stability analysis. This omission leaves uncertainty regarding how different choices of $K_{\\max}$ affect model performance, convergence behavior, and the balance between representation granularity and computational efficiency."}, "questions": {"value": "Since the paper is developed within the CLIP-based visual space, it would be interesting to further explore how AURA might behave when paired with a non-CLIP encoder. Such an investigation could offer additional insight into whether the framework’s strengths arise primarily from the archetype design or from the representational characteristics of CLIP features."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5fqbOT6ZwZ", "forum": "WMInu1V9av", "replyto": "WMInu1V9av", "signatures": ["ICLR.cc/2026/Conference/Submission20735/Reviewer_ENGN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20735/Reviewer_ENGN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998318288, "cdate": 1761998318288, "tmdate": 1762934147746, "mdate": 1762934147746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}