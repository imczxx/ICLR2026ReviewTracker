{"id": "Jee2Q7qK0s", "number": 18245, "cdate": 1758285566147, "mdate": 1759897116929, "content": {"title": "LIMI: Less is More for Agency", "abstract": "We define **Agency** as the emergent capacity of AI systems to function as autonomous agents—actively discovering problems, formulating hypotheses, and executing solutions through self-directed engagement with environments and tools. This fundamental capability marks the dawn of **the Age of AI Agency**, driven by a critical industry shift: the urgent need for AI systems that **don't just think, but work**. While current AI excels at reasoning and generating responses, industries demand autonomous agents that can execute tasks, operate tools, and drive real-world outcomes. As agentic intelligence becomes the defining characteristic separating cognitive systems from productive workers, efficiently cultivating machine autonomy becomes paramount.\nCurrent approaches assume that more data yields better agency, following traditional scaling laws from language modeling. We fundamentally challenge this paradigm. **LIMI** (Less Is More for Intelligent Agency) demonstrates that agency follows radically different development principles. Through strategic focus on collaborative software development and scientific research workflows, we show that sophisticated agentic intelligence can emerge from minimal but strategically curated demonstrations of autonomous behavior.\nUsing only 78 carefully designed training samples, LIMI achieves 73.5\\% on AgencyBench, dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1\\%), DeepSeek-V3.1 (11.9\\%), Qwen3-235B-A22B-Instruct (27.5\\%), and GLM-4.5 (45.1\\%). Most strikingly, LIMI demonstrates 53.7\\% improvement over models trained on 10,000 samples—achieving superior agentic intelligence with 128 times fewer samples.\n\nOur findings establish the **Agency Efficiency Principle**: machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations. This discovery fundamentally reshapes how we develop autonomous AI systems, suggesting that **mastering agency requires understanding its essence, not scaling training data**. As industries transition from thinking AI to working AI, LIMI provides a paradigm for sustainable cultivation of truly agentic intelligence. Our data and code are available in an [anonymous repository](https://anonymous.4open.science/r/limi-2F47) and will be made publicly available upon acceptance.", "tldr": "", "keywords": ["agency", "agentic", "LLMs", "Agent"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61f6d4dd578e95ed0fc1c819e03a0f873f88ec3a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a concept called LIMI (Less Is More for Intelligent Agency) for training agentic large language models using a very small, high-quality dataset of only 78 curated trajectories. The authors argue that “agency” of AI, which is defined as the ability to set goals, plan, act, and reflect, can emerge from a handful of strategically selected examples rather than massive data accumulation. They claim that a small number of “archetypal” agentic examples encode sufficient behavioral structure to elicit autonomous behavior."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation: The claim that autonomy could emerge from a strategically curated dataset is intellectually provocative, which is a natural extension to previous papers such as LIMA.\n2. Interesting observation: The observation that LLMs can be improved significantly in vide coding and research workflows with a small amount of data is interesting, which can potentially benefit research in automating vibe coding and research workflows with LLMs."}, "weaknesses": {"value": "1. Conceptual Clarity:  The paper's definition of \"agency\", which is the capacity to set goals, plan, act, and adapt, is conceptually intuitive but not formally defined or measurable. The authors fail to provide any operational test distinguishing \"agentic\" from \"non-agentic\" behavior. The experimental dataset covers only two domains, vibe coding and research workflows, which significantly limits generality. Although vibe coding and research workflows are popular applications of LLMs, such definitions about \"agency\" is inherently subjective and ambiguous.\n2. Technical Novelty: The paper's tone is overstating its technical novelty. Similar data-efficiency ideas have been extensively explored by previous works in LIMA or LIMO. Only changing the contexts without discussing the fundamental difference provide very limited insights into LLMs post-training. In addition, while the experiments are only conducted with LLMs, the authors are claiming some findings around general AI, also without any clarification about the difference.\n3. Methodological Rigor: The authors claim that LLMs can be improved significantly through training with only 78 examples. However, the data-efficiency comparison (78 vs. 10k) is striking but potentially confounded by uncontrolled variables, for which the authors provide very limited explanations. While the curation process is described in detail, it is heavily subjective (three PhD annotators rating examples). There is also no ablation verifying whether LIMI’s gain stems from pure curation quality, domain specificity, or inherent properties of the data. Thus, I strongly feel the claim that “adding more data hurts agency” lacks systematic testing."}, "questions": {"value": "1. How was the 78-example subset chosen beyond human scoring, was there quantitative validation of “agency density”? Are there any ideas that people can actually use your concepts to do strategic data curation?\n1. How exactly is agency defined and measured? What is the fundamental difference between your \"agency\" and typical reasoning and alignment tasks? What is the connection of your \"agency\" with established theories or concepts (e.g., theory of agencies)? Why are vibe coding and research workflows sufficient to represent such tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y3uqGYZdKW", "forum": "Jee2Q7qK0s", "replyto": "Jee2Q7qK0s", "signatures": ["ICLR.cc/2026/Conference/Submission18245/Reviewer_dqPa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18245/Reviewer_dqPa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761424323345, "cdate": 1761424323345, "tmdate": 1762927976421, "mdate": 1762927976421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors provide a new finetuning dataset for improving multi-turn performance with a focus on quality over quantity. They compare models fine-tuned on this dataset to baseline models that are not finetuned, and to models that are fine-tuned on other datasets. Evaluating on AgencyBench and several other cross-domain datasets, they show that their new dataset provides significant gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The work compares reasonably well to baselines (i.e. ensuring that this dataset works better than other available datasets and works well across different base models. \n* It effectively supports the claim that targeted, high quality data collection will result in solid performance improvements for downstream tasks."}, "weaknesses": {"value": "The weaknesses of the paper I think center around two main objections:\n- The authors claim that \"Less is More\" for agency. However, this is not really supported by the evidence? I imagine if I could collect double the data the authors collected in the same way, I would continue to see gains in downstream tasks. The claim that the authors could support is that \"high quality\" data beats a lot of low quality or un-targeted data. However, if this is the claim the authors are making, the paper should be rewritten to present this more clearly. \n    - Additionally, if that's the claim, then this is not a very novel claim in general (it's fairly obvious that high quality data is better than low quality data). A different, more novel claim could be around how you can trade quality and quantity in making fine-tuning datasets for agency, but that requires a different set of experiments and I don't think is what the authors are aiming to show.\n- A major issue I see with the current results is that the authors collected data on a particular CLI tool's trajectory (SII CLI). If the base model was not trained to use this CLI, and the evaluations also use this CLI, then the value of the fine-tuning dataset might simply be that the model is fine-tuned to work better with this CLI and the format/tools available. This may not be the case, but some evidence here would help show that this is not all that's going on. This especially could explain differences if the other datasets compared to used different tool-call interfaces that weren't then used for evaluation. \n\nMore clarity here would help me provide a more accurate score."}, "questions": {"value": "All my questions are centered around the weaknesses, please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GEiq2ngVdv", "forum": "Jee2Q7qK0s", "replyto": "Jee2Q7qK0s", "signatures": ["ICLR.cc/2026/Conference/Submission18245/Reviewer_tvCG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18245/Reviewer_tvCG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929933372, "cdate": 1761929933372, "tmdate": 1762927976105, "mdate": 1762927976105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a dataset of 78 coding and coding-adjacent tasks, which are used to perform supervised fine-tuning on several open-source models. After fine-tuning, the models perform better on coding and coding-adjacent tasks. Smaller performance gains are also observed on several non-coding-related tasks. The paper compares the newly proposed training dataset to 3 preexisting training datasets and finds that the new dataset improves model performance more than the older datasets despite the new dataset containing significantly fewer data points. The paper argues that these results \"fundamentally challenge conventional scaling paradigms in agentic AI development\" and will bring the community into a new era of model development guided by the newly minted \"Agency Efficiency Principle.\""}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The new dataset proposed in the paper appears to be well-designed/curated and effective in eliciting improved real-world coding performance from open-source models. \n\n- A large amount of effort was clearly put into fine-tuning and testing a variety of models across several different benchmarks, with results being clearly and conveniently displayed in Tables 1, 2, and Figure 4.\n\n- The paper addresses an economically valuable topic and seeks to make an important observation about the optimal way to train models in the age of agents. \n\n- The primary claim argued by the paper (that data quality is more important than data quantity) is probably true and is an important observation (though see the weakness section for more on why this was not adequately demonstrated)."}, "weaknesses": {"value": "This would be better if the paper were simply about the methods of producing a high-quality coding dataset, which was then proven useful for improving the performance of open-source models on code development tasks. Instead, the paper focuses on the claim that \"less is more\" when training models to be effective agents. This is a much bigger claim, which would require a different experimental design to prove. Table 1 gives the performance of LIMI versus 3 other fine-tuning datasets in order to establish that the new 78-sample dataset is superior to the larger prior datasets. One problem with this analysis is that each individual sample within LIMI may be of a considerably different size than the samples in these prior datasets. Figure 3 does a good job of illustrating the potential issue here. If each of LIMI's 78 samples averages 42.4k tokens, then it could conceivably be larger than a 10,000 sample dataset where each sample response is small. Granted that it's likely the case that each sample in the '-Code' dataset is larger than 330 tokens, but if the paper is going to make this claim, it needs to include both measures of size. \n\nSupposing that the 'less is more' paradigm really is true, it raises a second question: why 78 samples? If less really is more, perhaps 1 or 2 samples would suffice? In order to persuasively make the case argued for in the conclusion of the paper, there would need to be a set of experiments showing that as the number of datapoints in LIMI increases from 1 to 78, model performance starts to plateau or even regress. Without a plateau, the conclusion becomes 'still get as much data as possible, but make sure that it is good'. There would also need to be an experiment showing that LIMI+(CC-Bench)+(AFM-Web)+(AFM-Code) is worse than pure LIMI. Without such a result, it could well be the case that 'more is more'.\n\nThe above are the two most fundamental issues with the paper, but there are a variety of smaller issues that should also be addressed:\n\n1) Line 82 suggests via bolding that the paper title should be LIMIA instead of LIMI. \n\n2) Line 85 makes the bold claim that coding and science research constitute the majority of knowledge work scenarios. I suspect that lawyers, engineers, accountants, radiologists, congressmen, architects, secretaries, etc., would object to this characterization. \n\n3) Section 2 gives a slightly different (more succinct) definition of 'agency' than appears in the abstract and introduction. It would probably be best to define this only once in the paper, but if it's going to be done multiple times, it should be the same definition each time. \n\n4) Lines 122-127 conflate properties about tasks with properties about the systems that accomplish tasks. It will be grammatically cleaner to pick one consistent point of view while enumerating the list. \n\n5) On Line 269, if there are going to be inline links to the Claude and Gemini CLIs (which were not used for the paper), then there should also be an inline link for the SII CLI (which was actually used)\n\n6) Regarding Line 344, identical training configurations do not necessarily imply a fair comparison when dataset sizes are different. Consider dataset A with 1 million data points and dataset B with 10 data points. If I use both datasets with 10 steps of training, I might conclude that A and B are equivalent, but this would obviously not be taking full advantage of A. If training parameters like batch size or step count were optimized for LIMI then other datasets may be at a disadvantage when all configurations are held constant.\n\n7) The Table 1 performance gains are impressive, but they don't seem to be translating as well out of distribution. The \"EvalPlus\" datasets in Table 2 are code generation tasks and therefore still in-distribution given the code-focused data used during training. When those are excluded, the Table 2 results show much smaller out-of-distribution gains. Figure 4 shows this well, with OOD gains being about 20x smaller than ID gains. In the Abstract, the paper argues for an 'Agency Efficiency Principle' in which 'machine autonomy emerges from strategic curation of high-quality demonstrations'. If it is true that LLMs are learning generalizable agenticness from these carefully curated data points, then why is performance not strongly generalizing?\n\nOverall, I think the paper needs a significant writing revision."}, "questions": {"value": "1) Regarding line 275, if GPT5 is being used to generate agentic model traces and then those traces are used to train open-source models, would that technically violate OpenAI's terms of service against using their outputs to develop competitor products?\n\n2) In Table 1, is GPT5 the 'auto' model, or is it the low-reasoning, high-reasoning, or codex-flavored version of GPT5?\n\n3) Was Appendix Section D supposed to have additional content? It sounded like it was going to provide explicit transcripts, but then appears to be giving high-level summaries of a few interactions instead. \n\n4) Given the overfitting concerns raised in weakness 7, I am curious what would happen if models were trained on a 'low quality' dataset that had formatting similar to the 'good' data but without many reasoning insights. Basically, is there some way to determine how much of the Table 1 performance gain is due to prepping the model to write output that looks like that particular exam, vs how much is teaching the model new agentic skills? This isn't a publication blocker, just a question for future consideration."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "af7VifA2AG", "forum": "Jee2Q7qK0s", "replyto": "Jee2Q7qK0s", "signatures": ["ICLR.cc/2026/Conference/Submission18245/Reviewer_t9f8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18245/Reviewer_t9f8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762091678901, "cdate": 1762091678901, "tmdate": 1762927975652, "mdate": 1762927975652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LIMI (Less Is More for Intelligent Agency), proposing that agentic intelligence (AI systems' capacity to act autonomously and collaboratively) emerges from strategically curated examples of agentic behavior. The authors introduce the Agency Efficiency Principle, showing that with only N=78 high-quality demonstrations, LIMI achieves very high performance on AgencyBench."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Conceptual framing of \"agency\" as distinct from reasoning or alignment, emphasizing autonomy and collaboration. Methods are very sophisticated (e.g., leveraging real-world human-AI workflows with GitHub PR-based synthetic queries). Evaluations across in-domain and out-of-domain benchmarks, demonstrating strong generalization."}, "weaknesses": {"value": "Overall, the paper is methodologically solid, clearly written, and empirically convincing. A few minor suggestions: \n\n1. 78 training tasks themselves are not described individually in the paper. A brief table or appendix summarizing their scope and domain coverage (e.g., “Gomoku, dataset retrieval, equation fitting, etc.”) could be helpful for readers to understand the diversity and representativeness of the dataset.\n\n2. The qualitative case studies (Appendix D) are very helpful, but they are mostly anecdotal. Could the authors summarize recurring behavioral patterns (e.g., error recovery, iterative planning, tool-use compositionality, proactive hypothesis reformulation)? A small taxonomy of such emergent agentic traits would make the contribution more theoretically grounded.\n\n3. The “Less-is-More” effect is impressive, but to reach certain level of performance, it seems that we still need large foundation model. Can you discuss, perhaps through a brief case comparison, which aspects of agency depend on model capacity (e.g., long-horizon planning, contextual memory) vs. those that purely arise from strategic fine-tuning."}, "questions": {"value": "1. Could you provide a concise description of the 78 curated tasks (perhaps in an appendix or table) to make the dataset’s coverage clearer?\n2. The case studies are insightful. Could you extend them into a small taxonomy of behaviors unique to LIMI—e.g., what specific reasoning or collaboration patterns emerge only after fine-tuning?\n3. The results suggest small, high-quality data suffice given a large base model. Could you illustrate, with one example or case study, how model scale concretely affects agency (e.g., trajectory length, tool-use diversity, or recovery behavior)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K4qrz0eOcv", "forum": "Jee2Q7qK0s", "replyto": "Jee2Q7qK0s", "signatures": ["ICLR.cc/2026/Conference/Submission18245/Reviewer_fSYt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18245/Reviewer_fSYt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179315647, "cdate": 1762179315647, "tmdate": 1762927974992, "mdate": 1762927974992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}