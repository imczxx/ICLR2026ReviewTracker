{"id": "RFp9s01xpT", "number": 11362, "cdate": 1758197466490, "mdate": 1763748571441, "content": {"title": "SARE: Semantic-Aware Reconstruction Error for Generalizable AI-Generated Image Detection", "abstract": "Recently, AI-generated image detection has gained increasing attention, as the rapid advancement of image generation technologies has raised serious concerns about their potential misuse. While existing detection methods have achieved promising results, their performance often degrades significantly when facing fake images from unseen, out-of-distribution (OOD) generative models, since they primarily rely on model-specific artifacts and thus overfit to the models used for training. To address this limitation, we propose a novel representation, namely Semantic-Aware Reconstruction Error (SARE), that measures the semantic difference between an image and its caption-guided reconstruction. The key hypothesis behind SARE is that real images, whose captions often fail to fully capture their complex visual content, may undergo noticeable semantic shifts during the caption-guided reconstruction process. In contrast, fake images, which closely align with their captions, show minimal semantic changes. By quantifying these semantic shifts, SARE provides a robust and discriminative feature for detecting fake images across diverse generative models. Additionally, we introduce a fusion module that integrates SARE into the backbone detector via a cross-attention mechanism. Image features attend to semantic representations extracted from SARE, enabling the model to adaptively leverage semantic information. Experimental results demonstrate that the proposed method achieves strong generalization, outperforming existing baselines on benchmarks including GenImage and ForenSynths. We further validate the effectiveness of caption guidance through a detailed analysis of semantic shifts, confirming its ability to enhance detection robustness.", "tldr": "We propose a novel representation for AI-generated image detection, namely Semantic-Aware Reconstruction Error (SARE), that measures the semantic difference between an image and its caption-guided reconstruction.", "keywords": ["AI-Generated Image Detection", "Diffusion Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d155be22b39797f3b24db6722cb9a0ed332734dd.pdf", "supplementary_material": "/attachment/2d763d81e387f816e4b80dd2a2350f96f6d5ce68.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a method Semantic-Aware Reconstruction Error to detect AI-generated images. The method measure the semantic discrepancy between the original image and its caption-guided reconstruction. A pretrained image-captioning model is used to produces the caption of the input image then a stable diffusion is used to reconstruct. The difference between the original image and the reconstructed image is used to classify whether the image is AI-generated."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and the author provides the source code which increase the soundness of the paper.\n2. The proposed method shows strong performance compared to the baseline methods.\n3. Although I have concerns about the cost and robustness of the proposed method, the authors have moved beyond the perspective of visual artifacts and demonstrated a new direction.\n4.Ablation study is comprehensive"}, "weaknesses": {"value": "1. The major concern is computational cost, the proposed method need one time caption generation and one time diffusion reconstruction. The cost is significantly larger than artifact-based or frequency-domain detectors, make it hard to real-time deployment. I also suggest the author add related experiment to show the time cost difference to boost the paper.\n2. It seems the method highly rely on the caption model and the reconstruction model. As showing in the experiment,  when using LLaVA-NeXT, the semantic gap between real and fake images becomes smaller. I am not sure the detail setting of caption. What is the length of the caption? A interesting question would be if use stronger caption model and longer sequence of caption, whether this would reduce the semantic difference between the fake and real?  If this conclusion is correct, then the effectiveness of the method seems to rely on the ambiguity of semantics.\n3.  Experiments are mainly conducted on GenImage and ForenSynths. I suggest that the authors include a small dataset generated by a VAR model[1], as this model exhibits a larger gap from the reconstruction-based SD. If the proposed method remains effective under this setting, it would greatly strengthen the validity of the proposed method.\n4. Once concern is stochasticity in diffusion reconstruction. Since Stable Diffusion sampling is inherently random, the reconstruction result can vary across seeds or sampling parameters. Related ablation is needed.\n\nI will revise my score according to author's reply and other reviewer's opinion.\n\n[1]Tian, Keyu, et al. \"Visual autoregressive modeling: Scalable image generation via next-scale prediction.\" Advances in neural information processing systems 37 (2024): 84839-84865."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4NLafozwiG", "forum": "RFp9s01xpT", "replyto": "RFp9s01xpT", "signatures": ["ICLR.cc/2026/Conference/Submission11362/Reviewer_yPPa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11362/Reviewer_yPPa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760620892325, "cdate": 1760620892325, "tmdate": 1762922493315, "mdate": 1762922493315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SARE (Semantic-Aware Reconstruction Error), a novel and generalizable method for detecting AI-generated images. The key idea is to measure the semantic difference between an image and its caption-guided reconstruction, leveraging the observation that real images often undergo larger semantic shifts than fake ones during reconstruction due to their richer, under-described details. Experimental results demonstrate that the proposed method achieves strong generalization and outperforms existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces SARE, a novel method that quantifies the semantic gap between an image and its reconstruction given a generated caption, rendering SARE a robust and broadly applicable feature for detection tasks.\n2. The paper presents the model architecture and training pipeline of SARE with exceptional clarity, ensuring high reproducibility."}, "weaknesses": {"value": "Major Weaknesses:\n\n1. Previous studies such as [1] have proven that JPEG compression significantly affects AI-generated image detection. The two early-stage datasets selected by the authors fall into this category. Therefore, robustness experiments against various perturbations are highly necessary; the absence of such experiments would greatly undermine the validity of the results presented in Tables 1–4.\n2. Previous studies [2] and empirical evidence have shown that classifiers trained with cross-entropy almost saturate on seen classes (e.g., Conv-B and DIRE on SD 1.4 in Table 1, and the same behavior is reported in Table 4 of [2]). By contrast, the proposed method does not reach such near-perfect accuracy on SD 1.4, while delivering much larger gains on the remaining classes. This discrepancy requires a convincing explanation from the authors.\n3. Figures 11 and 12 show that fake-image reconstructions stay visually close to the originals.\nIn Figure 4(a), however, the LPIPS score of fake images under caption guidance is much higher than that of real images reconstructed without caption. This seems to contradict the intuition that real photos should be harder to reconstruct even when no guidance. I recommend that the authors include the corresponding SARE visualizations in Figure 2 to clarify this issue. \n\nMinor Weakness:\n\na. The authors introduce a novel fusion module, yet no ablation is provided to verify its individual contribution (vs DIRE with semantic-aware reconstruction).\n\nb. In the tables, the authors prefix “+ SARE” to the results. Since SARE is a distinct method, for which DRCT is merely used as a backbone, the plus sign should be removed to avoid the impression that this is an incremental extension.\n\nc. Another limitation arises when the image content is overly complex, containing numerous objects or intricate relationships. In such cases, the caption guidance may struggle to fully capture the scene's nuances, leading to suboptimal reconstruction. \n\n\n[1] Grommelt et al. Fake or JPEG? Revealing Common Biases in Generated Image\nDetection Datasets. ECCV 2024. \n\n[2] Yan et al. A Sanity Check for AI-generated Image Detection. ICLR 2025."}, "questions": {"value": "As described in Weaknesses. My two main concerns are: 1) the JPEG compression experiments, and 2) explanation of the unexplained anomalous results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GKmGezTmAJ", "forum": "RFp9s01xpT", "replyto": "RFp9s01xpT", "signatures": ["ICLR.cc/2026/Conference/Submission11362/Reviewer_xzkZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11362/Reviewer_xzkZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761305000781, "cdate": 1761305000781, "tmdate": 1762922492979, "mdate": 1762922492979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a representation termed Semantic-Aware Reconstruction Error (SARE), which measures the semantic difference between an image and its caption-guided reconstruction. The key hypothesis is real images are harder to reconstruct by cpation-guided reconstruction process. Experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses the AI-generated image detection task from the semantic perspective, which is less explored for now.\n2. The experimental results support the effectiveness of the proposed method.\n3. This paper is well-organized and easy to follow."}, "weaknesses": {"value": "1. My major concern focuses on the hypothesis of this paper: although I believe the corresponding captions of the generated images are semantically close to the image contents, do the authors consider that there would be artifacts in generated images, such as distortion or blurry, which may have impact on the generated captions? And these artifacts have also been used in many previous work for detection. More directly, did the authors have any theoretical or empirical proof for their hypothesis?\n2. The authors state their motivation and make comparisons with DIRE in Section 3.1, which is good. But I still have concern on the novelty of this paper since it seems more like an incremental work based on DIRE, from both motivation and method design perspectives. More justifications are needed to address this.\n3. For the experiments part, I suggest the authors add more recent baselines.\n4. Can the authors present some visualization of their designed SARE representation? This will be beneficial for direct analysis.\n5. From the results, we can see the performance on GAN-generated images is clearly lower than diffusion-generated images. More explanations on this are needed. Does this mean the hypothesis works only for text-to-image diffusion models?"}, "questions": {"value": "Please refer to the weakness part. My major concerns are the hypothesis and method design parts. If the authors can address them properly, I will raise my ratings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Sv6iZwKKfr", "forum": "RFp9s01xpT", "replyto": "RFp9s01xpT", "signatures": ["ICLR.cc/2026/Conference/Submission11362/Reviewer_Se9Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11362/Reviewer_Se9Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879198742, "cdate": 1761879198742, "tmdate": 1762922492651, "mdate": 1762922492651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SARE (Semantic-Aware Reconstruction Error), a novel representation for detecting AI-generated images that aims to improve generalization to unseen generative models. The key idea is that real images typically contain more complex semantics than their captions can fully describe, leading to larger semantic shifts when reconstructed through caption-guided diffusion. In contrast, AI-generated images tend to align more closely with their captions, thus showing smaller reconstruction discrepancies. SARE measures this semantic difference and integrates it into the detection backbone through a cross-attention fusion mechanism. Experiments on GenImage and ForenSynths show consistent accuracy and AUC improvements over baselines such as DIRE, DRCT, and DE-FAKE, demonstrating good cross-model generalization."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Conceptual novelty and fresh perspective.** The work moves beyond artifact-based detection toward a semantic-consistency-based approach, which is an emerging and meaningful direction for robust AI-generated image detection.\n- **Empirical performance and generalization.** SARE exhibits strong cross-model robustness, achieving the best average AUC and accuracy across diverse unseen generators (ADM, GLIDE, VQDM, BigGAN).\n- **Comprehensive experiments.** The paper provides ablation studies on captioning models (BLIP vs. LLaVA-NeXT), parameter sensitivity (guidance scale, strength), and cross-dataset transfer, ensuring good reproducibility and coverage."}, "weaknesses": {"value": "- Limited validity of the core hypothesis and its dependence on caption quality.\n- Dependence on specific reconstruction and detection architectures.\n- High computational overhead and lack of efficiency analysis."}, "questions": {"value": "This paper presents an interesting and conceptually novel approach for AI-generated image detection by leveraging semantic-aware reconstruction errors (SARE). The idea of distinguishing real and synthetic images through caption-conditioned semantic shifts is innovative and well motivated from an interpretability standpoint. The experiments are comprehensive, and the visualization results help illustrate the mechanism clearly. However, I have several concerns that should be addressed to strengthen the paper:\n\n**Limited validity of the core hypothesis and its dependence on caption quality.**\n\nThe core assumption of SARE—that *real images* exhibit larger semantic shifts because their captions fail to capture full visual details, while *fake images* align closely with their captions—is insufficiently validated and may result from caption incompleteness rather than an inherent semantic property. Most qualitative examples compare complex, multi-object real images with simple synthetic ones, leaving it unclear whether the same trend holds for *simple real images* (e.g., single-object photos).\n\nMoreover, all “real” samples are from GenImage and ForenSynths, a narrow and curated dataset, so the method’s validity for diverse real-world imagery (e.g., faces, artistic photos, noisy web content) remains untested.\n\nAblation studies further show that SARE performs best with BLIP captions, which are typically shorter and coarser than those from LLaVA-NeXT. Such simplified captions may *artificially exaggerate* semantic gaps for real images, reinforcing the method’s assumption rather than validating it. More critically, this hypothesis could break down when using stronger vision-language models (VLLMs) capable of producing rich, detailed captions. If models like LLaVA, GPT-4V, or Gemini generate semantically complete descriptions that fully cover real image content, the reconstruction-guided semantic gap might vanish—contradicting the core premise of SARE. This suggests that the reported performance gains may primarily arise from *caption simplicity* or *incompleteness bias*, not from genuine semantic differences between real and synthetic images.\n\n**Dependence on specific reconstruction and detection architectures.**\n\nBeyond caption dependence, SARE is closely tied to its reconstruction and detection pipeline.\nAll reconstructions are performed using Stable Diffusion v1.4, and it remains unclear whether SARE maintains similar effectiveness with other diffusion backbones (e.g., SD v2, ADM, or Flux).\nThe design also couples a cross-attention fusion mechanism specifically tuned to this pipeline, making it uncertain how well the approach generalizes to other reconstruction paradigms or encoder structures.\nWithout cross-model validation, the claimed “semantic generalization” may partially reflect architectural bias rather than a universally applicable representation.\n\n**High computational overhead and lack of efficiency analysis.**\n\nThe SARE pipeline is computationally heavy, consisting of:\n (i) caption generation via BLIP,\n (ii) text-guided diffusion reconstruction requiring 50 denoising steps, and\n (iii) a cross-attention fusion module for classification.\nThe paper does not report FLOPs, inference time, or throughput, leaving its scalability uncertain.\nFor practical deployment—such as social media monitoring or large-scale image verification—this processing cost may severely limit feasibility, despite accuracy gains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Oy1A29fdKR", "forum": "RFp9s01xpT", "replyto": "RFp9s01xpT", "signatures": ["ICLR.cc/2026/Conference/Submission11362/Reviewer_CduT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11362/Reviewer_CduT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961232374, "cdate": 1761961232374, "tmdate": 1762922492330, "mdate": 1762922492330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}