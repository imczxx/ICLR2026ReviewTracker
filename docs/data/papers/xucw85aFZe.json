{"id": "xucw85aFZe", "number": 23261, "cdate": 1758341432835, "mdate": 1759896823847, "content": {"title": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models", "abstract": "Large language models (LLMs) with chain-of-thought reasoning have demonstrated remarkable problem-solving capabilities, but controlling their computational effort remains a significant challenge for practical deployment. Recent proprietary systems like OpenAI's gpt-oss series have introduced discrete operational modes for intuitive reasoning control, but the open-source community has largely failed to achieve such capabilities. In this paper, we introduce ThinkDial, the first open-recipe end-to-end framework that successfully implements gpt-oss-style controllable reasoning through discrete operational modes. Our system enables seamless switching between three distinct reasoning regimes: High mode (full reasoning capability), Medium mode (50% token reduction with <10% performance degradation), and Low mode (75% token reduction with <15% performance degradation). We achieve this through an end-to-end training paradigm that integrates budget-mode control throughout the entire pipeline: budget-mode supervised fine-tuning that embeds controllable reasoning capabilities directly into the learning process, and two-phase budget-aware reinforcement learning with adaptive reward shaping. Extensive experiments demonstrate that ThinkDial achieves target compression-performance trade-offs with clear response length reductions while maintaining performance thresholds. The framework also exhibits strong generalization capabilities on out-of-distribution tasks.", "tldr": "In this paper, we introduce ThinkDial, the first open-recipe end-to-end framework that successfully implements gpt-oss-style controllable reasoning through discrete operational modes.", "keywords": ["Reasoning Control", "Large Language Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e2950d8e96491453ef997d4cdb1b716821ae0053.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes THINKDIAL, an open-recipe, three-mode framework to control LLM reasoning effort end-to-end. The pipeline first performs Budget-Mode SFT to imprint mode awareness, then a two-phase RL: Phase-1 warm-up to reach peak performance, and Phase-2 budget-aware reward shaping with a Leak Penalty that discourages reasoning leakage from the <think> section into the answer. Experiments on AIME-24/25, GSM8K, and OOD GPQA claim step-wise token reductions while keeping accuracy within specified thresholds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper presents an end-to-end training framework that unifies budget-mode control across supervised fine-tuning and reinforcement learning through adaptive reward shaping. \n- The experiments conducted on multiple benchmark datasets demonstrate the effectiveness."}, "weaknesses": {"value": "- There have been efficiency-oriented baselines, e.g., [1-4] that explicitly consider budget constraints and token efficiency. These baselines both reduce token budgets and improve reasoning performance rather than merely \"maintaining accuracy\". What is new and the advantages of the proposed proprietary three distinct reasoning regimes. It is recommended to provide a more systematic comparison in terms of controllability and stability along the accuracy-length trade-off curve, or to include methodological analysis to clarify the distinction.\n\n[1] Towards Cost-Effective Reward Guided Text Generation. [2] Learning to think: Information-theoretic reinforcement fine-tuning for llms. [3] T-reg: Preference optimization with token-level reward regularization. [4] Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning.\n\n- The Leak Penalty appears overly rule-based by detecting leakage through keywords such as Wait and Actually, but may be easily circumvented by paraphrasing, raising concerns about robustness. Empirical evidence supporting its reliability would be helpful, as would clarification of its advantages over structure-based markers, hierarchical segmentation, or discriminator-based methods [1–3]. Also, the medium and low modes are generated by truncating high-mode reasoning chains, which may introduce logical discontinuities. The low mode also inserts fixed connective phrases that could encourage stylistic imitation rather than genuine reasoning ability. Further discussion of these issues is advised.\n- The training data consist mainly of mathematical problems, with GPQA as the only OOD benchmark. To substantiate claims of strong generalization, the evaluation should extend to code, multi-step reasoning, and scientific QA tasks, with reports on mode controllability retention across multiple OOD domains. In addition, as the ACT metric uses customized weighting and normalization, it would be preferable to also report raw accuracy, reasoning-token and total-token distributions, together with the Pareto frontier, confidence intervals, and significance tests to validate the method’s effectiveness."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sVSshRNrPr", "forum": "xucw85aFZe", "replyto": "xucw85aFZe", "signatures": ["ICLR.cc/2026/Conference/Submission23261/Reviewer_661r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23261/Reviewer_661r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761036570469, "cdate": 1761036570469, "tmdate": 1762942578108, "mdate": 1762942578108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to find the open recipe for training gpt-oss-like models that can control the high-level budget of inference time. By applying SFT with different conciseness levels and curriculum RL training, it enables controllable reasoning models using Qwen base models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper provides a guideline on how to reproduce models like gpt-oss, enabling controllability of token length and allowing users to choose between latency and correctness.\n- The idea is simple and works well on multiple benchmarks.\n- The paper is easy to follow and read, and the evaluation is reliable as results are averaged across multiple runs."}, "weaknesses": {"value": "- The paper’s novelty mainly lies in providing comparability beyond the binary mode, but I’m not sure whether adding just one more option (a medium-length response) is truly meaningful.\n- The ACT metric definition is heuristic, and I also wonder how robust the results are to different α values.\n- There are some missing details on SFT data: how is truncation performed for medium and lower modes? What does the data look like? There are some examples in the Appendix, but it’s hard to understand when only reading Section 3.1.\n- The training data used are not clearly described. It is called an “open recipe,” but the dataset itself is missing, which is critical.\n- It’s minor, but the AIME 2025 performance highlight in the main table seems incorrect.\n- Confidence interval should be in the results if the authors run the experiments multiple times."}, "questions": {"value": "It’s minor, but isn’t the core part of DAPO the different clip ratio and data filtering? Does this paper also include data filtering? Also, the explanation in Section 3.2.1 seems almost identical to GRPO rather than DAPO."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6MThjFgQZq", "forum": "xucw85aFZe", "replyto": "xucw85aFZe", "signatures": ["ICLR.cc/2026/Conference/Submission23261/Reviewer_HfmG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23261/Reviewer_HfmG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698872033, "cdate": 1761698872033, "tmdate": 1762942577851, "mdate": 1762942577851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces ThinkDial, the first open-recipe that allows GPT-oss-style controllable reasoning through discrete conditioning in the text space. The system enables the user to set between three conditioning regimes by trading off between reasoning quality and efficiency. The recipe includes data generation as well as SFT and RL training. Experiments show that ThinkDial achieves different tradeoffs with significant response length reductions while maintaining high reasoning quality and performance on common reasoning benchmarks, with strong generalization to out-of-distribution tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The work introduces a new framework for GPT-oss-style discrete conditioning in the text space. This is the first open reproduction of this idea.\n* The end-to-end training pipeline, based on SFT and RL, is compatible with common training pipelines for reasoning models, and can be easily integrated with existing recipes.\n* Despite being trained mainly on math reasoning data, it generalizes well to out-of-distribution tasks."}, "weaknesses": {"value": "* The SFT data is generated by truncating at $r_\\text{med}$ and $r_\\text{low}$ for medium and low conditioning regimes, respectively. Won't that lead to hallucination if the truncation happens when important steps have not yet finished?\n* DAPO, the framework uses by ThinkDial, normalizes the rewards with an std term. This is kept in L194 in the ThinkDial paper. However, this normalization will cause the length penalty to be amplified if the answer are all correct or incorrect in the sampling group. There is no mention or mitigation of this effect in the paper.\n* Only one model is used for training (Qwen-2.5-Instruct-32B). Newer models and models that are inherently reasoning-capable (e.g., Qwen3) are not evaluated. Whether ThinkDial is applicable on existing reasoning models is not investigated.\n* No comparisons with prior works that reduce overthinking by explicitly constraining the number of tokens in the output. Methods that lead to budget-aware LLMs, such as [1], are not compared.\n* No indication of averaging over multiple runs to make the results more stable. For benchmarks such as AIME 24/25, since each problem only has 30 samples, the results may be not stable if inference is not repeated. Avg@32 is a common practice to make the results more stable.\n\n[1] An Empirical Study of LLM Reasoning Ability Under Strict Output Length Constraint. Sun, et al. https://arxiv.org/abs/2504.14350"}, "questions": {"value": "1. DAPO’s reward normalization (L194) can amplify penalties when outputs are uniform. How is this mitigated?\n2. Since only Qwen-2.5-32B was tested, would ThinkDial work effectively on models that already perform reasoning (e.g., Qwen3)?\n3. Were multiple inference runs (e.g., Avg@32) done to ensure stable AIME 24/25 results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s2jFoIgtV5", "forum": "xucw85aFZe", "replyto": "xucw85aFZe", "signatures": ["ICLR.cc/2026/Conference/Submission23261/Reviewer_u9Gi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23261/Reviewer_u9Gi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027665140, "cdate": 1762027665140, "tmdate": 1762942577524, "mdate": 1762942577524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces thinkdail, a training recipe that gives open-weight LLMs a three-mode “reasoning dial” -- High, Medium, and Low -- to mimic proprietary systems’ mode switches. The training pipeline has two main parts:\n\n- Budget-Mode SFT, which teaches the model mode-conditioned behavior by constructing parallel solutions of the same problem with varying reasoning depth.\n- Two-phase RL, consisting of a (i) warm-up stage to reach a peak checkpoint, followed by (ii) budget-aware reward shaping. The latter adds a per-mode length reward and a Leak Penalty to stop reasoning-length hacking (moving chain-of-thought from <think> into the answer).\n\nExperiments on AIME-24/25, GSM8K, and GPQA show roughly 50% token reduction in Medium mode with ≤10% accuracy loss, and 75% reduction in Low mode with ≤15% loss, while preserving High-mode accuracy (Figure 1). The paper also defines an ACT score to compare each mode’s accuracy-compression trade-off against the no-length-controlled “peak” model and match its performance in high mode. Medium and Low modes trade accuracy for compression, while High mode retains full performance. Figures 2-4 support the necessity of each component -- Budget-Mode SFT, warm-up RL, and the Leak Penalty."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Timely and well-targeted.** The work reproduces a widely requested user affordance -- intuitive reasoning-effort control -- in an open stack. Figure 1 clearly illustrates this behavior.\n\n- **Strong observations and ablations.** The paper identifies a realistic failure mode -- reasoning leaking into the answer and quantifies it, and proposes a Leak Penalty that cuts total tokens while maintaining accuracy. Each component’s ablation shows measurable degradation, underscoring the necessity of the full recipe.\n\n- **Clarity.**  The paper effectively motivated the setting for readers unfamiliar with the topic. The methods were easy to understand, and the results were clearly presented."}, "weaknesses": {"value": "**[Major] Missing baselines.** The paper omits head-to-head comparisons with open controllability methods such as Shorter RL (e.g., L1, ThinkLess, CoT-Valve, TokenSkip, O1-Pruner, LightThinker) and binary gating approaches (AdaCoT, AdaptThink). Many are cited but not evaluated. It remains unclear whether these simpler methods can provide far better accuracy-vs-length curves than thinkdail and to understand whether the three-mode design incurs unnecessary trade-offs e.g. if LightThinker provides far better performance in a token budget equivalent to the low mode, then the three modes would be providing strictly worse tradeoffs.\n\n**[Major] Scale dependence.** Does the recipe work on smaller models, or is it effective only on 32B-scale systems? Results on smaller models would clarify on whether the recipe generalizes beyond the Qwen 2.5 Instruct 32B model.\n\n**[Curious] Domain Generalization.** Does controllability extend beyond math to tasks like MMLU or LiveCodeBench when trained only on math data?"}, "questions": {"value": "Please address major weaknesses.  I commit to increasing my score to 8 if Thinkdial recipe works on smaller models (and comparison with alternative methods becomes easier without reproducing them entirely).\n\nOverall, I liked the method -- it seems to be a solid, practical contribution and enjoyed reading the paper. The ablations (Budget-Mode SFT, warm-up RL, and Leak Penalty) address most of my concerns-- I believe the three-modes work, end-to-end recipe and the Leak Penalty are valuable necessities to make it work. If validated across scales and model families, thinkdail could become a reproducible open recipe for mode-based reasoning control."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Qxy3ZBN4ei", "forum": "xucw85aFZe", "replyto": "xucw85aFZe", "signatures": ["ICLR.cc/2026/Conference/Submission23261/Reviewer_C188"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23261/Reviewer_C188"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762158302837, "cdate": 1762158302837, "tmdate": 1762942577221, "mdate": 1762942577221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}