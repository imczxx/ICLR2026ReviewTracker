{"id": "JaK0MTiJd6", "number": 15045, "cdate": 1758247165517, "mdate": 1763457115439, "content": {"title": "Pixels Lie, Code Doesn't: Thinking with Visual Programming for ''Seemingly Impossible'' Multimodal Agentic Reasoning Tasks", "abstract": "To overcome the inherent limitations of Chain-of-Thought (CoT) and to further push the upper bound of multimodal reasoning capabilities, we introduce Thinking with Visual Programming (TVP), where models can iteratively interact with an external code executor to generate, run, and verify both visual and textual agentic operations as part of the reasoning loop. Motivated by the open question of how far Multimodal Large Language Models (MLLMs) still lag behind this paradigm, we introduce MMR-VIP, a MultiModal Agentic Reasoning benchmark built on Visual Impossible Problems. We design MMR-VIP with two key principles: (1) We construct a Difficulty Ladder grounded in computational complexity theory, structuring tasks from easy problems that can be solved with inherent perception and reasoning, through medium problems that require external computational tools, to hard problems that remain intractable even with programming assistance. (2) We decompose the paradigm of Thinking with Visual Programming into three Cognitive Skills, namely Perception, Abstraction, and Optimization, which correspond to perceiving visual inputs, abstracting them into problem formulations, and optimizing algorithms to obtain efficient solutions. Our experiments on MMR-VIP yield the following findings: (1) GPT-5, as a native TVP model, delivers the strongest overall results, yet its accuracy remains only 38.2%, underscoring substantial room for progress. (2) For commercial models, multi-turn code execution consistently surpasses direct CoT and single-turn execution, providing stable and significant improvements. (3) Across difficulty levels, performance follows a ladder-shaped trend, with negligible gains on easy tasks, the largest improvements on medium tasks, and steady advances on hard tasks. (4) From a cognitive perspective, TVP enhances optimization by offloading complex computation, search, and planning, but models still encounter bottlenecks in abstraction.", "tldr": "We introduce MMR-VIP, a MultiModal Agentic Reasoning benchmark that consists of Visual Impossible Problems.", "keywords": ["Multimodal Reasoning", "Thinking with Visual Programming"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/18763711bdb198c3b05f90391c0c0506761748cf.pdf", "supplementary_material": "/attachment/64c18a96ceab87cdd926a52de7c4e2c924d6b730.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates the limitations of Chain-of-Thought (CoT) and proposes methods to improve the upper bound of multimodal reasoning. They involve a external code executor to iteratively generate, run, and verify both visual and textual agentic operations. Comprehensive findings show the deficiency and performance pattern in current TVP models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The researched question of Multi-modal Reasoning plays a significant role in multimodal models, which endorses the model's thinking capabilities.\n\nExternal Code Executor is an efficient tool that has demonstrated efficiency in multiple settings.\n\nThe author conducted a comprehensive experiment that shows the deficiency of current TVP models."}, "weaknesses": {"value": "Some typos need to be revised, such as OpenAI’s o4 may likely be OpenAI’s 4o.\n\nThe difficult ladder is more likely designed manually, and lacks the theory to reflect the fundamental difference. This deserves thinking about the method for the difficulty ladder, which may start from basic cognitive capability.\n\nHow to ensure the quality and label correctness of the benchmark?\n\nAs shown in Table 2, the best MLLMs have a great gap from humans. What is the root cause behind this phenomenon? And what training or training-free method has the potential to improve the performance on these tasks?\n\nIt will be better if you provide some failure cases and visual examples across different baselines and proposed methods.\nIs the data curation possible to transfer to real CV or robot tasks?"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9J4cTxFFjM", "forum": "JaK0MTiJd6", "replyto": "JaK0MTiJd6", "signatures": ["ICLR.cc/2026/Conference/Submission15045/Reviewer_TRk7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15045/Reviewer_TRk7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915936593, "cdate": 1761915936593, "tmdate": 1762925368819, "mdate": 1762925368819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MMR-VIP, a benchmark for multimodal reasoning under the proposed paradigm of Thinking with Visual Programming (TVP). It evaluates how models use code execution to solve visual and symbolic reasoning problems across 28 task types, three difficulty levels, and three cognitive skills. The authors assess a range of models, showing that TVP improves performance over CoT reasoning, especially on medium-difficulty tasks. Overall, the paper aims to establish a new framework for studying executable multimodal reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **S1.** The benchmark provides a large dataset that tests visual reasoning with visual programming, providing a useful resource for future research.\n- **S2.** The benchmark covers 28 task types across three difficulty levels and three cognitive skills, could be very useful for the community to evaluate MLLMs. Several tasks are also quite interesting (e.g., physics, network).\n- **S3.** The paper is written well and very easy to follow."}, "weaknesses": {"value": "- **W1.** The proposed difficulty ladder is somehow problematic. Its definition depends on current model performance rather than on intrinsic task properties, making it unstable and model-dependent. For example, the 'hard' level is defined by whether existing models can solve it with programming assistance; as models improve, tasks once labeled 'hard' might shift to 'medium.'\n- **W2.** The three cognitive skill dimensions (Perception, Abstraction, Optimization) are intuitively reasonable but not theoretically or empirically justified. If these dimensions are claimed as a main contribution, they should be either theoretically grounded or empirically validated to demonstrate their distinctiveness and usefulness. It is fine to include such categorization for analytical purposes, but presenting it as a core contribution requires stronger justification.\n- **W3.** Section 3.3 lacks transparency in benchmark construction: the paper didn't provide much information on how annotators were recruited, what initial materials or code templates they were given, and whether they followed a unified guideline or independently designed tasks. It also remains unclear how the authors ensured diversity across annotators, how quality control and validation were conducted. I hope the author could provide more details about the benchmark construction.\n- **W4.** Although the paper claims to include 1680 tasks, these are derived from only 28 seed task types x 3 difficulty levels x 20 instances per combination, which indicates limited underlying diversity. This structure suggests that most tasks may share similar templates or reasoning structures, differing mainly by parameter changes. \n- **W5.** The related work section omits discussion of prior works on visual programming. For example, [1] is also a visual programming benchmark and uses similar code2Task method for data generation. [2] also considers the visual programming setting, in which they synthesize code for visual reasoning. These visual programming work are quite related but not discussed.\n\nReferences:\n\n[1] Program Synthesis Benchmark for Visual Programming in XLogoOnline Environment. ACL 2025.\n\n[2] Visual Programming: Compositional visual reasoning without training. CVPR 2023"}, "questions": {"value": "- **Q1.** In Table 2, why are some open-source models evaluated with T=1 while others include T=3?\n- **Q2.** What exactly defines a native TVP model? Why are such models not evaluated under CoT, T=1, or T=3?\n- **Q3.** How do the authors ensure that the dataset instances represent genuinely distinct reasoning challenges rather than minor parameter variations?\n- **Q4**. How did you come up with these 28 task types? Is it motivated by any existing benchmarks, designed from scratch, or other cases?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper uses human participants for dataset construction and evaluation but doesn’t explain their involvement procedure or compensation. It’s unclear whether they were paid, how much, or if consent was collected."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gnfH6WFhUf", "forum": "JaK0MTiJd6", "replyto": "JaK0MTiJd6", "signatures": ["ICLR.cc/2026/Conference/Submission15045/Reviewer_9r1P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15045/Reviewer_9r1P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945369970, "cdate": 1761945369970, "tmdate": 1762925368279, "mdate": 1762925368279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel paradigm termed **Thinking with Visual Programming (TVP)**. Unlike traditional *Chain-of-Thought (CoT)* or *Thinking with Images (TWI)* frameworks that rely on static visual tools (e.g., crop, zoom, rotate), TVP allows models to **generate, execute, and verify visual and textual code** during reasoning.\n\nThe authors introduce **MMR-VIP**, a comprehensive benchmark built on *Visual Impossible Problems* that require agentic reasoning with an external code executor. Tasks are structured along two orthogonal axes:\n\n1. **Difficulty Ladder**—from perception-based “easy” problems in P, to “medium” problems solvable with programming, to “hard” NP-hard-style tasks.\n2. **Cognitive Skills**—*Perception*, *Abstraction*, and *Optimization*.\n\nExperiments across commercial, open-source, and native TVP models show that **multi-turn visual programming markedly improves medium-difficulty performance**, with GPT-5 achieving the best overall accuracy (38.2%), while humans remain at 53.6%. The results reveal TVP’s promise in scaling multimodal reasoning depth and highlight abstraction as the main bottleneck."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Comprehensive benchmark.** MMR-VIP spans 1,680 instances and 28 task types with fine-grained coverage over cognitive skills and complexity tiers.\n* **Strong empirical evaluation.** Results across open-source and proprietary models clearly illustrate the scalability and difficulty ladder effects.\n* **Clear cognitive framing.** The decomposition into perception, abstraction, and optimization provides a cognitively interpretable lens for diagnosing MLLMs.\n* **Reproducibility and ethical standards.** All tasks are programmatically generated, with scripts and evaluation environments promised for release."}, "weaknesses": {"value": "* **Abstraction bottleneck insufficiently analyzed.** While results emphasize abstraction as the limiting factor, the paper provides limited qualitative insight into *why* models fail at symbolic or physical abstraction. Including more introspective analyses (e.g., code trace visualization, abstraction failure typology) would clarify the underlying cognitive gap.\n* **Interpretability of TVP reasoning remains underexplored.** Although multi-turn reasoning trajectories are central to the paper’s thesis, few qualitative examples are shown. Representative reasoning chains, success/failure visualizations, or interpretable program outputs would make the paradigm’s dynamics clearer.\n* **Limited discussion on computational cost and efficiency.** TVP introduces code execution into the reasoning loop, potentially increasing inference latency and token consumption (as suggested by Figure 4). A deeper discussion on runtime–accuracy trade-offs or efficiency optimizations would be valuable."}, "questions": {"value": "1. How robust is TVP to **execution noise or syntax errors**? Are erroneous interpreter outputs filtered or directly propagated into subsequent reasoning steps?\n2. The results indicate that **abstraction remains the major bottleneck**. Do the authors have additional insights on how this skill could be improved—e.g., through reinforcement learning over code-execution trajectories, hierarchical reasoning supervision, or curriculum-based abstraction tasks?\n3. **Figure 1(3)** shows substantial reported improvements under TVP, yet the magnitude of the plotted gains seems inconsistent with the y-axis scale."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TdYfUmRUVS", "forum": "JaK0MTiJd6", "replyto": "JaK0MTiJd6", "signatures": ["ICLR.cc/2026/Conference/Submission15045/Reviewer_GxU2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15045/Reviewer_GxU2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948903179, "cdate": 1761948903179, "tmdate": 1762925367743, "mdate": 1762925367743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Thinking with Visual Programming (TVP), where models generate and run code, including visual operations, as part of reasoning. It introduces MMR-VIP, a benchmark of “Visual Impossible Problems” across different difficulty levels and cognitive skills. Experiments show that iterative code execution improves medium-level performance, though models still struggle with abstraction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tpaper introduces a new benchmark to facilitate the development of VLMs.\n-\tIt reveals that existing VLMs struggle with visual programming tasks.\n-\tIt proposes a data generation framework for fine-tuning."}, "weaknesses": {"value": "-\tI think 28 task types are still somewhat limited. It would be better if the benchmark could be scaled up through more automated task generation methods to cover a broader and more diverse set of problems.\n-\tAlthough GPT-5 and o4 are labeled as native TVP models, I think it would be useful to also evaluate them under the same external-tool setup used for other models (e.g., through prompts that block internal executors). That would make the comparison fairer and help understand how much of the advantage comes from the model itself versus the interface.\n-\tThe “Human” results lack participant details such as background, age, or expertise. For a paper of this type, similar to what psychology or cognitive science works do, including such information would make the human baseline more credible.\n-\tThe paper could discuss more related benchmarks, for example Humaneval-V [1], to clarify how MMR-VIP differs from or complements existing multimodal reasoning datasets. It is recommended to add a discussion on visual reasoning benchmarks in the related work section.\n\n[1] https://arxiv.org/pdf/2410.12381"}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "86YjCTA5CT", "forum": "JaK0MTiJd6", "replyto": "JaK0MTiJd6", "signatures": ["ICLR.cc/2026/Conference/Submission15045/Reviewer_zm6s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15045/Reviewer_zm6s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992407322, "cdate": 1761992407322, "tmdate": 1762925367435, "mdate": 1762925367435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}