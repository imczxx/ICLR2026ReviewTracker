{"id": "I3dPEvbp8o", "number": 14525, "cdate": 1758237938991, "mdate": 1759897364799, "content": {"title": "Can Vision-Language Models Answer Face to Face Questions in the Real-World?", "abstract": "AI models have made significant strides in recent years in their ability to describe and answer questions about real-world images. They have also made progress in the ability to converse with users in real-time using audio input. This raises the question: have we reached the point where AI models, connected to a camera and microphone, can converse with users in real-time about scenes and events that are unfolding live in front of the camera? This has been a long-standing goal in AI and is a prerequisite for real-world AI assistants and humanoid robots to interact with humans in everyday situations. In this work, we introduce a new dataset and benchmark, the Interactive Video Dataset (IVD), which allows us to assess the extent to which existing models can support these abilities, and to what degree these capabilities can be instilled through fine-tuning. The dataset is based on a simple question-answering setup, where users ask questions that the system has to answer, in real-time, based on the camera and audio input. We show that existing models fall far behind human performance on this task, and we identify the main sources for the performance gap. However, we also show that for many of the required perceptual skills, fine-tuning on this form of data can significantly reduce this gap.", "tldr": "", "keywords": ["Situated Dataset", "Multi-Modal Dataset", "Vision Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/25afee6733128df7f6690fbb1ac7a5ccd647c919.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces IVD (Interactive Video Dataset) — a new benchmark designed to evaluate vision-language models (VLMs) in realistic face-to-face question answering scenarios. Unlike existing VideoQA datasets, IVD emphasizes “when-to-answer” (i.e., temporal readiness), audio-visual reasoning, and deictic references that commonly appear in human–AI interactive settings."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Novel and realistic task formulation\n- The paper highlights a crucial but underexplored problem — when and how VLMs should answer in interactive real-world scenarios.\n- This setup moves beyond static VideoQA and aligns better with embodied AI and multimodal assistant applications.\n\nHigh-quality dataset design\n- IVD provides rich annotations: question semantics, answer timestamps, and multimodal data (audio + video).\n- The inclusion of “best-answer time” is innovative, enabling quantitative evaluation of temporal readiness.\n\nComprehensive evaluation\n- The study benchmarks several leading LMMs (VideoLLaMA2.1, Qwen2.5-VL, GPT-4o) under both zero-shot and fine-tuned settings.\n- The analysis spans multiple aspects: when-to-answer accuracy, ASR quality, audio contribution, and fine-tuning gains.\n\nInsightful analysis\n- The t-SNE visualization of 1024-D embeddings effectively illustrates the domain shift between IVD and traditional VideoQA datasets.\n- The paper provides qualitative failure cases and detailed error categorization (e.g., deictic confusion, premature answering).\n\nRelevance and potential impact\n- The benchmark fills an important evaluation gap for real-world multimodal dialogue and will likely spur further research in streaming and interactive LMMs."}, "weaknesses": {"value": "Limited dataset scale and diversity\n- Only \\~2.9K samples with short video clips (\\~5 s).\n- The data collection is crowd-sourced and potentially biased toward controlled indoor scenes, limiting generalization."}, "questions": {"value": "- Is there any plan to open-source this dataset in the future? \n- Will there be more scenarios for real-time (face-to-face) question answering？\nGood job, no other questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "duELWPDTF8", "forum": "I3dPEvbp8o", "replyto": "I3dPEvbp8o", "signatures": ["ICLR.cc/2026/Conference/Submission14525/Reviewer_vNTF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14525/Reviewer_vNTF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761371628002, "cdate": 1761371628002, "tmdate": 1762924916910, "mdate": 1762924916910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new dataset, the Interactive Video Dataset (IVD), to evaluate the online responsiveness of LMMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important issue for AI systems: how to answer face-to-face questions. By providing a new dataset, it standardizes this problem, which helps inspire the research community to focus on the online responsiveness of LMMs."}, "weaknesses": {"value": "1. The paper solely uses the Streaming-Whisper model to determine \"when-to-answer.\" Whether this leads to insufficient accuracy in evaluating the performance of large models in answering face-to-face questions remains questionable. In particular, the paper notes that \"the end of a question does not necessarily capture the optimal moment for an answer.\" Is it necessary to introduce more suitable models for determining \"when-to-answer\"—for example, models trained specifically for question-answering timing, rather than simply detecting the end of a question?\n2. In face-to-face scenarios, a key consideration is that AI agents actually receive continuous video streams. These streams have no definite start time and can be regarded as infinitely long. The streaming approach proposed in the paper focuses on determining \"when-to-answer,\" but identifying \"when the current discussion begins\" is equally important (even without considering complex multi-turn dialogues). In the current dataset, it appears that multimodal information from the start of the video up to the \"when-to-answer\" timestamp is defaulted to input into the LMM. This is not feasible in real video streams.\n3. The scale of training data used for fine-tuning is relatively small."}, "questions": {"value": "1. For the Streaming-Whisper row in Table 4, when calculating METEOR, BLEU, and ROUGE-L for transcribed text, is the calculation based on the full transcribed text or the transcribed text up to the \"when-to-answer\" timestamp identified by the model?\n2. For VideoLLaMA2.1-7B-AV, after adding audio information, why did model performance conversely decrease? Are there any reasonable explanations for this phenomenon?\n3. Regarding the \"Impact of when-to-answer\" setup in Section 5.1, the study compares model performance when using ground-truth and ASR-derived when-to-answer timestamps. All models in Table 5 could participate in this comparison, so why is Qwen2.5-Omni specifically emphasized? With the same ASR transcription model used, even if the LMM itself does not support speech transcription, it should not affect the study on the \"Impact of when-to-answer.\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K149EGr6Ly", "forum": "I3dPEvbp8o", "replyto": "I3dPEvbp8o", "signatures": ["ICLR.cc/2026/Conference/Submission14525/Reviewer_2VbA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14525/Reviewer_2VbA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485165145, "cdate": 1761485165145, "tmdate": 1762924916509, "mdate": 1762924916509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces, IVD, a comprehensive benchmark, and dataset designed to assess and train LMMs on a wide variety of tasks requiring responding to humans in real time. Besides, the authors also propose a streaming baseline approach that couples a streaming Automatic Speech Recognition (ASR) model to detect the end of a question with a Video-LMM to generate an answer based on the preceding context. Experiments show that existing models fall far behind human performance on this task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The fundamental idea of this paper is technically correct.\n2. This work shifts focus from offline video analysis to the challenges of real-time, situated interaction, which is critical for the future of human-AI collaboration.\n3. The paper is easy to follow.\n4. The rationale behind each section and the overall motivation are clearly presented and easy to understand."}, "weaknesses": {"value": "1. While the authors introduce a valuable dataset and problem formulation, the technical contribution is limited in novelty. The core technical contribution is merely combining a streaming ASR system (Streaming-Whisper) with existing Video-LMMs. This approach lacks innovation, essentially representing a straightforward concatenation of existing components without novel architectural design or algorithmic improvements.\n3. The paper's core weakness is its reliance on the overly simplistic assumption that the optimal time to answer is the end of the spoken question. Although the authors acknowledge this limitation, basing their entire method on such a fundamentally flawed indicator introduces significant error and sidesteps the actual research challenge of learning when to respond.\n3. The dataset contains only 2,900 videos, which is relatively small.\n4. The experimental evaluation could be strengthened by including more recent and powerful models. The current comparisons feel incomplete as they omit several state-of-the-art models that have been released, such as Qwen3-VL and GPT-5.\n5. The paper lacks deep theoretical analysis of why existing models perform poorly on this task. While failure modes are identified, there is insufficient exploration of root causes and no targeted solutions are proposed for the fundamental issues.\n6. A very interesting and counter-intuitive finding is that the pre-trained VideoLLaMA model's performance degraded when audio was included. Could the authors provide a deeper analysis or hypothesis for this phenomenon?"}, "questions": {"value": "The work lacks technical novelty. The core flaw is assuming optimal response timing equals question end time, which avoids the actual research challenge. The dataset is small, evaluations miss recent models like Qwen3-VL and GPT-5, and the paper lacks theoretical depth in analyzing model failures."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NW0UktbGMw", "forum": "I3dPEvbp8o", "replyto": "I3dPEvbp8o", "signatures": ["ICLR.cc/2026/Conference/Submission14525/Reviewer_SQFL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14525/Reviewer_SQFL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579697104, "cdate": 1761579697104, "tmdate": 1762924916126, "mdate": 1762924916126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce the Interactive Video Dataset (IVD), a new video QA benchmark for evaluating the ability of multimodal large language models (MLLMs) to answer face-to-face questions in real-time. The authors provide a comprehensive evaluation of several baseline MLLMs on IVD, finding that current models significantly underperform compared to human-level performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The contribution of a real-time, interaction-focused video QA dataset is timely, addressing a growing interest in multimodal AI assistants and real-world interaction.\n- The collection of videos in \"in-the-wild,\" realistic settings is a strength, ensuring the dataset's quality, diversity, and practical relevance\n- The authors show that models fine-tuned on IVD achieve improved performance, confirming the dataset's value as a resource for advancing model capabilities in this domain."}, "weaknesses": {"value": "- The video clips are generally short (~5 seconds), limiting the dataset's demand on a model's contextual memory which is an important capability for real-world real-time assistants.\n- While the paper evaluates an extensive list of models, they are often generic MLLMs not designed specifically for real-time use cases. Evaluating baselines developed specifically for real-time QA, such as FlashVStream, would be helpful."}, "questions": {"value": "- Why is the streaming evaluation protocol (Sec 5.1) needed? It relies on Whisper for timestamps, which is not measuring the tested model's own capability to process the information and decide when to answer in real-time. This external dependency adds noise from Whisper's performance, and it seems like using the human annotated timestamps makes more sense. A more interesting setup would be asking the model itself to provide a timestamp of when-to-answer, and compare that with the annotated ground truth.\n- The key distinction of IVD compared to prior real-time QAs seems to be the face-to-face element (Table 1). Why is face-to-face important for real-time QA? Intuitively, a face-to-face setting implies a model should infer user intent from non-verbal cues like facial expressions or gestures, but this does not seem to be the focus of IVD. Could the authors elaborate on what makes the \"face-to-face\" aspect a unique and necessary contribution beyond what existing real-time datasets offer?\n- The authors claim that IVD poses a \"strong demand on situational context understanding\" compared to post-hoc created real-time datasets.  Could the authors expand on this claim, particularly in comparison to post-hoc created datasets like VStream-QA? One could argue that datasets like VStream-QA, with their longer context and less restrictive (face-to-face) settings, offer an equally or even more realistic real-time evaluation setup.\n- While not a weakness, the paper would be strengthened by including experiments on generalization of the IVD-finetuned model -- does a model fine-tuned on IVD show improved performance on other offline face-to-face datasets or other online streaming video benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VpbaAT89f8", "forum": "I3dPEvbp8o", "replyto": "I3dPEvbp8o", "signatures": ["ICLR.cc/2026/Conference/Submission14525/Reviewer_KsFd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14525/Reviewer_KsFd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854146395, "cdate": 1761854146395, "tmdate": 1762924915695, "mdate": 1762924915695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}