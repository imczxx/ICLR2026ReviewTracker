{"id": "OOTokVgBY6", "number": 9374, "cdate": 1758120548378, "mdate": 1763471358817, "content": {"title": "Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR", "abstract": "Large Language Models (LLMs) have recently improved mathematical reasoning through Reinforcement Learning with Verifiable Reward (RLVR). However, existing RLVR algorithms require large query budgets, making annotation costly. We investigate whether fewer but more informative queries can yield similar or superior performance, introducing active learning (AL) into RLVR. We identify that classic AL sampling strategies fail to outperform random selection in this setting,  due to ignoring \\textbf{objective uncertainty} when only selecting by subjective uncertainty. This work proposes an \\textbf{uncertainty consistency} metric to evaluate how well subjective uncertainty aligns with objective uncertainty. In the offline setting, this alignment is measured using the Point-Biserial Correlation Coefficient (PBC). For online training, because of limited sampling and dynamically shifting output distributions, PBC estimation is difficult. Therefore, we introduce a new online variant, computed from normalized advantage and subjective uncertainty.  Theoretically, we prove that the online variant is strictly negatively correlated with offline PBC and supports better sample selection. Experiments show our method consistently outperforms random and classic AL baselines, achieving full-dataset performance while training on only 30\\% of the data, effectively reducing the cost of RLVR for reasoning tasks.\\footnote{The code is available at \\hyperref[https://anonymous.4open.science/r/uncertainty-consistency-235C]{https://anonymous.4open.science/r/uncertainty-consistency-235C}.\n}", "tldr": "Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR", "keywords": ["Reinforcement learning; Large Language Model; Active Learning; Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f5db67daaacc4490af94bd3a7035464ebdc3a021.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors are proposing to use Active Learning (AL) to mitigate the large sample requirements of Reinforcement Learning with Verifiable Reward (RLVR) used in the LLM alignment process. \nThey find, that samples where the predicted model uncertainty diverges from the actual accuracy on the sample (measured by sampling K outputs and computing the reward) are detremental to model convergence in RLVR.\nThe authors propose an AL score to find samples with high alignment of uncertainty and accuracy and use this score to sample a subset of the full training data (offline setting).\nSince this proposed score is difficult to obtain in the online setting, the authors also propose an approximation of their score and demonstrate its theoretical properties.\nBoth proposed methods show strong empirical results on 3 models and 2 math datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Very streamlined description of research gap, related work and proposed method\n- Excellent line of reasoning:\n\t1. Preliminary experiment to show shortcomings of existing solutions to the problem\n\t2. Starting with a simplyfied setting (offline RL) and showing a principled advantage of the proposed method\n\t3. Generalizing to realistic settings (online RL) by finding an approximation of their method that has theoretical guarantees\n\t4. Demonstrating strong performance of the approximation on 3 models and 2 datasets\n\t5. Providing ablation studies on the additional properties of their method\n- Providing believable evidence for the non-standard assumption of \"Sample Gradient Orthogonality\"\n- No additional computational overhead, as sampling K inferences for each x is already part of RLVR"}, "weaknesses": {"value": "- the authors claim \"(non-)significant lifts\" multiple times (line 78,88,373,381), but do not provide hard evidence for this claim in the form of standard deviations of results, critical difference diagrams or p-tests. We urge the authors to either provide one of these metrics in the appendix, or use a less mathematically loaded term instead of \"significant\"."}, "questions": {"value": "- Impact of higher gammas (ablation study): Higher gamma values mean a stronger deterrence of predicted negative advantage in Eq. 5. Does this mean, we focus on samples the model knows to improve upon (positive advantage for some output y)? If so, does this not direcetly counteract exploration behaviours in the RL training? Would this be comparable to directly influencing exploration/exploitation ratios in the training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wzht3e2h5d", "forum": "OOTokVgBY6", "replyto": "OOTokVgBY6", "signatures": ["ICLR.cc/2026/Conference/Submission9374/Reviewer_KvRw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9374/Reviewer_KvRw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761223468118, "cdate": 1761223468118, "tmdate": 1762920989625, "mdate": 1762920989625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces the active learning (AL) process with a new acquisition metric into reinforcement learning with verifiable reward (RLVR) for large language models (LLMs). They first confirm the query selection's impact on the stability of the gradients during fine-tuning via RLVL and reveal that the classic AL cannot select the most informative queries as well as random sampling. After assessing the inconsistent between an LLM's output with the lowest probability and a reward model's evaluation of the samples' accuracy effects on the gradient norm, they propose the smallest inconsistent metric $r_{pb}$ by the Point-Biserial Correlation Coefficient (PBC). Moreover, they extend the idea to online training. The experimental results show that the AL with an inconsistent metric could enhance both offline and online training as well as different RL algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The story (writing) is good to easily follow the authors' idea and refresh the utilization of the AL for the emerging field.\n2. This paper highlights that the importance of the query selection metric should not only rely on the LLM itself but also require consistency with the reward model's evaluation."}, "weaknesses": {"value": "1. **My concern about using uncertainty.** After reviewing the Eqs. (2) and (3), IIUC, the definition of the subjective uncertainty is the low average probability of a policy model's responses $\\log \\pi_\\mathrm{ref}(y_{k, t}^{(i)} \\vert x^{(i)}, y_{k, <t}^{(i)})$ and the objective uncertainty is the low accuracy of a reward model's evaluation of a model's response, respectively. However, why can we call these two terms uncertainty? For example, if an LLM's response gives a response with lower probability but a 'consistent response' for the same (or similar) prompt $x^{(i)}$, could we still say the sample is uncertain?\n2. Follow 1., I feel that the metric of these terms is more like 'difficulty of reasoning the sample $x^{(i)}$', i.e., the degree of the probability that LLM can give the response (reasoning) and get the high reward (correct answer) for a sample. If so, what you check is the consistency between an LLM's output and the reward model's output.\n3. **My concern about the comparison with other AL methods.** Follow 2, if the core idea is evaluating a degree of the informative sample requires both LLMs and reward models, the proposed comparison with Entropy, K-center, K-means, and AskLLM might be insufficient, which only considers the LLMs' response and ignores the reward models' evaluation. To strengthen it, I suggest that the authors consider adding an alternative ablation study on the 'uncertainty' of the reward models.\n4. **Make a consistent symbol for equations.** For example, Eq. (1) uses $y_i$ but Eq. (3) uses $y_k^{(i)}$, the index of the sample and the index of generations should be differentiated.\n5. In Figure 1, the authors present that the inconsistent sample would give high gradient norm dynamics, i.e., these samples would cause gradient instability. However, the (degree of) impact of these gradient instabilities on the final performance is unclear. To highlight the gradient instability would be a significant issue for RLVR, I suggest that the authors provide some illustrations or examples of this issue."}, "questions": {"value": "1. While you mention that *Because the calculation of $r_{pb}$ relies on a large number of samples $K$, ...* in Sec 4.2, your experimental settings of $K = 8$ in Sec 5.1 seems not large. Could you give stronger motivations for using Online Query Selection? For example, the **Model update** is the key point to address.\n2. Following 1., what are the key components in Online Query Selection for addressing sampling distribution shift in **Model update**?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dla5DNNj85", "forum": "OOTokVgBY6", "replyto": "OOTokVgBY6", "signatures": ["ICLR.cc/2026/Conference/Submission9374/Reviewer_4djn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9374/Reviewer_4djn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942026773, "cdate": 1761942026773, "tmdate": 1762920988923, "mdate": 1762920988923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates query selection strategies for reinforcement learning from vision and reward (RLVR). The authors observe that standard active learning (AL) sampling methods often fail to outperform random selection in this context. To address this, they introduce an uncertainty consistency metric to guide sampling. In the offline setting, they use PBC (policy–behavior consistency) to measure alignment, while for online training—where estimating PBC directly is difficult—they propose a variant based on normalized advantage and subjective uncertainty. The paper also provides a theoretical analysis suggesting a negative correlation between offline and online PBC. Empirically, the proposed approach outperforms both random and classic AL baselines, reaching near–full-dataset performance using only 30% of the data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem is well-motivated and relevant to current challenges in RLVR.\n2. The paper offers an interesting empirical observation that inconsistent samples can lead to extreme gradients, which explains why standard AL can underperform random sampling.\n3. The introduction of two alignment metrics—one for offline and one for online settings—is insightful, and the accompanying theoretical analysis provides some grounding.\n4. Experiments are extensive and demonstrate strong results, achieving competitive performance with significantly fewer samples."}, "weaknesses": {"value": "While the paper is promising, several points could benefit from deeper clarification or justification:\n\n1. The link between sample inconsistency and extreme gradient behavior is intuitively explained but lacks theoretical support or formal analysis.\n2. It is unclear why the offline setting cannot also leverage the online metric $r_{pb}^{online}$, which appears to yield stronger performance in experiments.\n3. In some cases, training on the full dataset leads to worse results than using only 30% of the data; the paper should provide more discussion or intuition for why this happens.\n\nTypo: In line 451, “Table ??” is not rendering correctly."}, "questions": {"value": "Please comment/justify the weakness part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RrlALOmAQp", "forum": "OOTokVgBY6", "replyto": "OOTokVgBY6", "signatures": ["ICLR.cc/2026/Conference/Submission9374/Reviewer_ntf7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9374/Reviewer_ntf7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977061696, "cdate": 1761977061696, "tmdate": 1762920988504, "mdate": 1762920988504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new query selection strategy for Reinforcement Learning with Verifiable Rewards (RLVR) that allows training mathematical reasoning models using far fewer queries, without sacrificing performance. The key insight from the paper is that not all queries are equally informative. Standard active learning methods often pick samples with high subjective uncertainty (e.g., high perplexity), but this fails in RL reasoning because these samples frequently produce unstable or high-variance gradients, which hurts training stability. To address this problem, the paper proposes selecting samples where Subjective uncertainty (model confidence) and Objective uncertainty (whether the answer is correct) are consistent. They define it as “uncertainty-consistent” samples. The Experimental results show that using only 30% of the training data, the model reaches the same or better performance on the reasoning tasks compared to the full datasets RLVR training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies a practical limitation in current RL-based reasoning training pipelines: query selection methods that rely solely on subjective uncertainty (e.g., perplexity) often select examples that are uncertain but uninformative, leading to unstable gradients and inefficient learning. This motivation is clearly articulated and supported by empirical evidence. This insight is both intuitive and impactful—valuable training samples are those where uncertainty meaningfully reflects correctness, rather than those that are merely hard. The theoretical analysis establishing their negative correlation and training benefit is clearly developed and strengthens the contribution.\nThe method achieves full-dataset performance using only ~30% of the training data, while maintaining or improving generalization on standard math reasoning benchmarks. This is a practically meaningful result, especially given the rising cost of RL-based reasoning training."}, "weaknesses": {"value": "The consistency metric assumes that the examples used for selection reflect the distributions during RL optimization. If the underlying data distribution shifts over training (which is common in RLVR), the effectiveness of selection may degrade unless the scoring is frequently recomputed.\n\nThe evaluation is rather limited to only Math reasoning. For query selection methods, it would be great to draw broader insights on whether the methods can be generalized beyond Math reasoning tasks."}, "questions": {"value": "Do the uncertainty consistency proposed in this paper still hold in open-ended, multi-step reasoning tasks where correctness is subjective or less binary (e.g., instruction following, safety, dialogue)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PFz6TQAEnc", "forum": "OOTokVgBY6", "replyto": "OOTokVgBY6", "signatures": ["ICLR.cc/2026/Conference/Submission9374/Reviewer_eynX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9374/Reviewer_eynX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991168691, "cdate": 1761991168691, "tmdate": 1762920988190, "mdate": 1762920988190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}