{"id": "knc1y6jkTL", "number": 19093, "cdate": 1758293548616, "mdate": 1759897060313, "content": {"title": "Accelerating Newton-Schulz Iteration for Orthogonalization via Chebyshev-type Polynomials", "abstract": "The problem of computing optimal orthogonal approximation to a given matrix has attracted growing interest in machine learning. Notable applications include the recent Muon optimizer or Riemannian optimization on the Stiefel manifold. Among existing approaches, the Newton-Schulz iteration has emerged as a particularly effective solution, as it relies solely on matrix multiplications and thus achieves high computational efficiency on GPU hardware. Despite its efficiency, the method has inherent limitations—its coefficients are fixed and thus not optimized for a given matrix. In this paper we address this issue by proposing a Chebyshev-optimized version of Newton-Schulz (CANS). Based on the Chebyshev's alternance theorem, we theoretically derive optimal coefficients for the 3-rd order Newton-Schulz iteration and apply a Remez algorithm to compute optimal higher-degree polynomials. We leverage these polynomials to construct controlled approximate orthogonalization schemes, which is of interest in deep learning applications. Practically, we demonstrate the method's effectiveness in two key applications: orthogonalization in the Muon optimizer, and providing an efficient retraction alternative for Riemannian optimization on the Stiefel manifold.", "tldr": "A novel Chebyshev-optimized version of Newton-Schulz iteration for orthogonalization", "keywords": ["Newton-Schulz iteration", "orthogonalization", "polar decomposition", "Muon", "Riemannian optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2860116521e54ab3bd5300c39b1bfb5047eaa25f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the 3rd order Newton-Schulz method with optimal coefficients through Chebyshev’s alternance theorem, and higher order extension via Remez algorithm. The authors then apply it to Muon optimizer and to Riemannina optimization on the Stiefel manifold, and shows acceleration from the proposed algorithms on both cases."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper improves the classic Newton-Schultz algorithm and shows acceleration on Muon and Riemannian optimization settings both theoretically and empirically."}, "weaknesses": {"value": "The paper reads more like a mathematics paper than a machine learning paper. It lacks high-level intuition and is overly detailed, making it difficult to follow. The presentation is not very well-organized."}, "questions": {"value": "1.In the Muon application, only partial test loss results are shown. It would be helpful to also include training loss curves and full test loss trajectories to verify the theory. Also, please clarify what “optimal” means — is it optimal in the sense of training (optimization) performance?\n\n2.In section 4,\n\n2.1 Equations (5) and (6) are unclear and should be rewritten more rigorously.\n\n2.2 There are two “Algorithm 1” entries — please correct accordingly.\n\n2.3 Lines 276–285 are difficult to interpret. It seems you are discussing an extension that uses different polynomials at each iteration. This should either be formalized as a theorem, lemma, or corollary, or rewritten more intuitively as a high-level remark rather than a sequence of technical statements.\n\n2.4 How are d_i chosen at each step?\n\n3.What is the relationship between 1-\\delta and a? According to Proposition 4, a does not seem to be symmetric w.r.t. 1\n\n4.Please introduce Remez algorithm in the corresponding part. It’s unclear to me how the coefficients of higher-order (>3) polynomials are chosen (as well as the order in the above question).\n\n5.Please introduce the relationship Muon algorithm and Newton-Schultz more formally (at least you should state the whole algorithm of Muon), and then compare the corresponding part with your proposed optimal algorithm.\n\n6.In Figure 3 and 4, the authors show the comparison between their proposed polynomials and original Muon. It seems that whether the polynomial is closer to 1 is not important since Muon has smaller right end point and are closer to 1 in most of the points than the proposed polynomials, and only the derivative at zero matters. How should this be understood?\n\n\nMinor:\n\nLine184: “greater or equal than”\n\nLine 238: “On the other hand”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "y0LpAmVu5A", "forum": "knc1y6jkTL", "replyto": "knc1y6jkTL", "signatures": ["ICLR.cc/2026/Conference/Submission19093/Reviewer_6mHR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19093/Reviewer_6mHR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761164367081, "cdate": 1761164367081, "tmdate": 1762931116705, "mdate": 1762931116705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Chebyshev-Accelerated Newton-Schulz (CANS) framework to address the notable drawbacks of conventional matrix orthogonalization methods. The framework derives explicit formulas and analyzes the convergence of optimal odd polynomials, utilizing the Remez algorithm to compute their higher-order counterparts. Compared to classical methods like the Newton-Schulz iteration and Cayley retraction, the polynomials generated by CANS reduce computational complexity and accelerate convergence while maintaining accuracy. Furthermore, the authors validate the method's effectiveness through tasks such as accelerating parameter orthogonalization with the Muon optimizer, training NanoGPT, and performing fast retractions on the Stiefel manifold."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This paper systematically applies Chebyshev approximation theory to optimize the coefficients of the Newton-Schulz iteration, proposing the Chebyshev-Accelerated Newton-Schulz (CANS) framework for finding \"provably optimal\" odd polynomials.\n\n2.The paper is built upon a solid mathematical theory, with detailed and rigorous proofs for each proposition and corollary provided in the appendix. This offers robust mathematical support for the uniqueness and key properties of the optimal odd polynomials.\n\n3.The overall structure of the paper is clear. It begins by stating the problem and progressively develops its main theory, supplemented by figures and textual explanations to help readers understand the meaning behind each proposition."}, "weaknesses": {"value": "1.The typesetting for the proof of Proposition 1 in the appendix is slightly disorganized and could be improved. Additionally, the paper contains some errors; for instance, in the equation on line 50, the final exponent appears to be a transpose symbol 'T' when it should likely be 't'. The paper also mistakenly includes two distinct algorithms both labeled as \"Algorithm 1\".\n\n2.The experiments on the Stiefel manifold are conducted solely with a Wide ResNet-16-10 on the CIFAR-10 dataset. The evaluation does not cover other architectures  or larger datasets. This limited scope makes it difficult to assess the generalizability of the CANS retraction for deeper networks and larger-scale data, indicating a need for more extensive experiments."}, "questions": {"value": "Regarding the core parameter $\\delta$, the paper only sets specific values experimentally and does not provide a quantitative guide for its selection. It fails to clarify how one might determine an appropriate δ based on the singular value distribution of a matrix or the specific requirements of a task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4a6lXXku72", "forum": "knc1y6jkTL", "replyto": "knc1y6jkTL", "signatures": ["ICLR.cc/2026/Conference/Submission19093/Reviewer_xmCb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19093/Reviewer_xmCb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662201205, "cdate": 1761662201205, "tmdate": 1762931115818, "mdate": 1762931115818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of computing optimal orthogonal approximations to matrices, a fundamental operation in machine learning applications such as the Muon optimizer and Riemannian optimization on the Stiefel manifold. The authors propose CANS (Chebyshev-accelerated Newton-Schulz), which optimizes the coefficients of Newton-Schulz iterations using Chebyshev's alternance theorem. For degree-3 polynomials, they derive explicit optimal formulas, and for higher degrees, they apply the Remez algorithm. The method is demonstrated on three applications: matrix orthogonalization, the Muon optimizer for neural network training, and retraction on the Stiefel manifold for Riemannian optimization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work presents a novel theoretical framework for optimizing Newton-Schulz iteration coefficients. While polynomial approximation theory is classical, its systematic application to this problem through Chebyshev's alternance theorem is creative.\n\n2. The theoretical contributions are rigorous with complete proofs in the appendices. Proposition 2 provides closed-form solutions for degree-3 polynomials, and the convergence analysis establishing quadratic convergence is solid. The experimental validation spans multiple domains with appropriate baselines. \n\n3. The paper is generally well-written with clear motivation."}, "weaknesses": {"value": "1. The paper cites concurrent work by Amsel et al. (2025). A slightly more detailed comparison in the related work section could help readers more clearly understand the overlapping and distinct contributions of the two papers regarding the exact case."}, "questions": {"value": "1. Section 3.3 states that Gelfand's formula to estimate $\\sigma_1$ can be implemented \"without introducing any extra matmuls\". However, normalization of matrix is applized before NS iteration, does that mean $(A^T A)^k $ must be saved? Clarifying this is important for understanding the practical implementation overhead.\n\n2. Regarding $\\delta$-orthogonalization, \n\n(1) could you elaborate more on why \"If we use distinct polynomials on each iteration, we can achieve more rapid convergence\"? Say, given arbitrary $\\delta \\in (0,1)$, since optimal $a = a(d, \\delta) \\in (0,1-\\delta)$, could we instead do binary search for $a(d, \\delta)$ in region $(0, 1-\\delta)$ and iterates the process $p, \\epsilon = remez(a, 1+\\delta, 2d-1)$ until $|\\epsilon - \\delta| \\le 1e-7$ as follows:\n\n$L = 0; R = 1-\\delta$\nrepeat:\n  a = (L+R)/2\n\n  p, ε = remez(a, 1+$\\delta$, 2d-1)\n\n  if $|\\epsilon-\\delta| ≤ tol$: break\n\n  if $\\epsilon$ > $\\delta$: L = a       \n\n  else:      R = a    \n\n\n(2) Why we need rapid convergence for $\\delta$-orthogonalization? Fast convergence is desirable for exact orthogonalization through optimal odd polynomials. However, the goal of $\\delta$-orthogonalization is to a find a polynomial to push singular values into interval $[1-\\delta, 1+\\delta]$ while with large derivative at the origin $0$. However, the rapid convergence through distinct polynomials over iterations would easily make $\\epsilon$ approaching 0 (as stated in proposition 3), which could easily make $\\epsilon < \\delta$, especially for high tolerance.\n\n3. A high-level question is do we really need optimal/exact orthogonalization in real world applications like Muon optimizer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DmHwkQjvjB", "forum": "knc1y6jkTL", "replyto": "knc1y6jkTL", "signatures": ["ICLR.cc/2026/Conference/Submission19093/Reviewer_H3Bj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19093/Reviewer_H3Bj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833162561, "cdate": 1761833162561, "tmdate": 1762931115339, "mdate": 1762931115339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors look into finding optimal polynomials for Newton-Schulz iterations that make matrices approximately orthogonal, which is a key step in the Muon optimizer for LLMs and is useful in optimization over the Stiefel manifold. They also propose to use an alternative cheap pre-normalization for the algorithm to work differing from the Frobenius norm. Experimental results show an improvement over previous approaches"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Through rigorous theory, optimal polynomials are found for this task (sections 3 and 4)\n\nA trick is proposed for using Gelfand's formula with almost no computational overhead in order to get accurate upper bounds on the spectral norm."}, "weaknesses": {"value": "The theory seems to me to be more like a corollary of prior studies, but this does not necessarily undermine the value of this approach in this context.\n\nPolynomials better fit to the task\n\nSee the questions/suggestions section for more.\n\ntypo in appendix K, it references Figure 1 instead of Figure 2"}, "questions": {"value": "Experiments with and without the Gelfand bound were performed, but I am missing info on directly what the Gelfand bound was versus the Frobenius norm approach to see directly what advantage it is providing for the initial estimate. \n\nAlso, it would have been good to isolate the effect of your different polynomials vs Gelfand's formula (you run Muon with the Frobenius norm bound and your approach is run using Gelfand's so both effects are mixed)\n\nThere could be more explanations on Figure 5. Several plots are show by number of steps despite that each line curve used a different number of matrix multiplications per step. The only comparable ones are Muon vs Cans order=5 iter=4, (this was mentioned in the text) and also Jiacheng iter 6 mm=18 vs CANS order 3 iter=9. This ploto should have been drawn with number of matrix multiplications in the x axis directly, not number of steps.\n\nAlso, a few more comparisons would be needed with Jiacheng work in order to see that this comparison was not hand picked"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UFAkVWN7th", "forum": "knc1y6jkTL", "replyto": "knc1y6jkTL", "signatures": ["ICLR.cc/2026/Conference/Submission19093/Reviewer_6zcm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19093/Reviewer_6zcm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845173844, "cdate": 1761845173844, "tmdate": 1762931113709, "mdate": 1762931113709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}