{"id": "y7wAuwErpL", "number": 3623, "cdate": 1757488059818, "mdate": 1763747438151, "content": {"title": "GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning", "abstract": "Recent advances in diffusion models have significantly improved the quality of video generation. However, their real-world deployment often requires fine-tuning for physical constraints, a process that depends on human annotations and large-scale computational resources.\nIn this paper, we propose GigaVideo-1, an efficient fine-tuning framework that advances video generation without additional human supervision. Rather than injecting large volumes of high-quality data from external sources, GigaVideo-1 unlocks the latent potential of pre-trained video diffusion models through automatic feedback. \nGigaVideo-1 focuses on two key aspects: data and optimization. On the data side, we design a prompt-driven data engine that constructs diverse, weakness-oriented training samples. On the optimization side, we introduce a reward-guided training strategy, which adaptively weights samples using feedback from pre-trained vision-language models with a realism constraint.  \nGigaVideo-1 offers a flexible optimization framework adaptable to various capability dimensions. To demonstrate its versatility, we instantiate the framework on VBench-2.0's 17 evaluation dimensions as concrete application instances. Using Wan2.1 as the baseline, GigaVideo-1 yields consistent improvements, with an average gain of $\\sim$4\\% using only 4 GPU-hours. Requiring no manual annotations and minimal real data, GigaVideo-1 shows both effectiveness and efficiency. Code, model, and data will be publicly available.", "tldr": "", "keywords": ["video generation", "efficient fine-tuning", "automatic feedback"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0543dab98d555d4f81ad7e9be596fab82ba7607e.pdf", "supplementary_material": "/attachment/5227441ce9d1c1316c695b02c3e19a8367fdc7cc.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents an algorithm for the rapid scaling of video diffusion models, reporting up to a 4% performance gain with only 4 GPU hours. The approach involves two main components: a prompt-driven data engine to generate diverse synthetic training data and a VLM-based scoring mechanism to weight this data during training.\n\nHowever, I am puzzled by the formulation of the proposed KL divergence constraint. The authors frame the objective as minimizing the distance between $u(z_t,p,t;\\theta)+z_0$ and $z_1$. This is mathematically equivalent to minimizing the distance between the model's direct output, $u(z_t,p,t;\\theta)$ and the target $z_1-z_0$. Given this equivalence, the decision to add $z_0$ to the model's prediction before computing the loss appears superfluous."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. High Novelty: The core idea of actively identifying, synthesizing, and then reweighting challenging videos is highly innovative. This \"target-seeking\" optimization strategy, which explicitly pushes the model beyond its current capabilities, is a new direction for training diffusion models. To the best of my knowledge, no prior work has taken a similar approach.\n\n2. Thorough Experimental Validation: The experiments are comprehensive and well-designed. The ablation studies are particularly insightful, systematically justifying the components of the proposed method. The authors have diligently tested their reweighting strategy across various training paradigms (including SFT, RL, and gradient-based methods), concluding that the offline reweighting approach is most effective.\n\n3. Strong Empirical Results: The method achieves impressive results. Notably, the fact that this approach outperforms a strong baseline like Flow-GRPO provides compelling evidence for its effectiveness and practical value."}, "weaknesses": {"value": "1. Request for Intuition on Method's Effectiveness: I find the core mechanism of the paper's success to be somewhat perplexing, despite the strong empirical results. The methodology involves training on videos synthesized from challenging prompts. Intuitively, videos generated for highly challenging prompts would be of lower quality, receive a lower VLM score, and therefore contribute less to the training objective due to the reweighting. It is therefore surprising that this strategy yields such a significant (nearly 5-point) improvement over standard SFT. Could the authors provide a more detailed explanation or intuition for why this approach is so effective?\n\n2. Clarification on Data Blending in Table 2: Regarding the experiments in Table 2, when the datasets PsVs, PrVs, and PrVr are used concurrently, does this mean they are simply blended together for training? If so, what is the anticipated effect of also incorporating the PsVr dataset into this mixture?\n\n3. Suggestion for Showcasing Qualitative Results: The static figures presented in the paper are not sufficient to fully demonstrate the superiority of the proposed method's video generation quality. While the supplementary videos are helpful, I would strongly suggest that the authors create an anonymous GitHub Pages site or a similar web-based platform. This would allow reviewers to more easily and directly compare the results and appreciate the qualitative improvements"}, "questions": {"value": "No"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "71RLynuzKj", "forum": "y7wAuwErpL", "replyto": "y7wAuwErpL", "signatures": ["ICLR.cc/2026/Conference/Submission3623/Reviewer_YYeF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3623/Reviewer_YYeF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760461892185, "cdate": 1760461892185, "tmdate": 1762916878613, "mdate": 1762916878613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an automated pipeline for fine-tuning video diffusion models. Its core idea is a variant of DPO, first use LLM to generate polished prompts, then use the polished prompts to generate videos to performe DPO. During the fine-tuning, use the VLM to score the specific generated video and performe RL. \n\nIts core concept is a straightforward combination of existing ideas, lacking genuine conceptual innovation or technical breakthrough. It resembles more of a robust engineering frame work than a pioneering research method. I think the authors need to clarify the contributions and significance of this paper, and provide more insights behind this method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is complete, constructing an end-to-end automated fine-tuning system that includes all stages from data generation and evaluation to optimization. The experiment results on VBench 2.0 seems good."}, "weaknesses": {"value": "1. Lack of core innovation. This is the most critical flaw, and the authors need to clarify their contributions and significances. From my perspective, the method proposed in this paper is essentially \"Automated DPO/RWR\". The entire pipeline can be summarized as: LLM generates targeted prompts -> base model generate the videos -> MLLM scores -> score-weighted loss training. Every module in this paradigm is off-the-shelf, and the combination method is straight forward.\n\n2. Lack of in-depth experiments. All experiments in this paper are conducted in VBench 2.0. The pipeline heavily relies on the dimensions defined by VBench 2.0. Does this suggest limited generalizability of the framework?\n\n3. The paper claims to address deep semantic issues like \"physical consistency\". However, the MLLM itself, trained on large-scale web data, possesses \"physical knowledge\" and \"common sense\" that are similarly superficial and biased. Using a biased judge to correct a biased generative model might only be optimizing a model consensus rather than truly approximating physical consistency."}, "questions": {"value": "No."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6bFr9tAONH", "forum": "y7wAuwErpL", "replyto": "y7wAuwErpL", "signatures": ["ICLR.cc/2026/Conference/Submission3623/Reviewer_K5pd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3623/Reviewer_K5pd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726643223, "cdate": 1761726643223, "tmdate": 1762916878458, "mdate": 1762916878458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GigaVideo-1, a lightweight and data-efficient fine-tuning framework designed to enhance video diffusion models (e.g., Wan2.1-T2V-1.3B). The core of the paper consists of two modules: the Prompt-Driven Data Engine and the Reward-Guided Optimization module. According to the paper, their method can automatically improve the performance of video generation models on challenging dimensions (such as physical dimensions like thermodynamics) while requiring minimal computational resources (4 GPU hours)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is presented with clarity and is easy to understand. The contributions are clearly articulated (1. the data engine; 2. the optimization method).\n\n2. The framework demonstrates strong generalizability. As validated in the appendix, it brings consistent performance gains when applied to various video model backbones (e.g., CogVideoX, HunyuanVideo), proving it is a versatile and portable solution rather than a model-specific trick.\n\n3. A major contribution of this paper lies in its well-chosen research issue — addressing the poor performance of general video models in physical dimensions — and its proposed automated mechanism for rapid targeted fine-tuning, which has achieved substantial performance gains."}, "weaknesses": {"value": "1. I'm curious about the statement on **line 314**: \"the synthetic dataset is generated by different pre-trained T2V models.\" Specifically, which T2V models were used for this purpose? I mean, if you're fine-tuning a Wan2.1-1.3B model but the synthetic data is generated using Wan2.1-14B, wouldn't the time required for synthetic data generation be excessively long?\n\n2. Another concern centers on the unvalidated effectiveness of the MLLM-based evaluation. Without a correlation analysis between MLLM scores and human judgment, the entire optimization process risks \"reward hacking\"—improving automated metrics at the cost of human perception, potentially resulting in high-scoring but perceptually poor videos. \n\n3. While the paper avoids the massive human annotation required by methods like DPO, it introduces a different bottleneck: the manual selection of seed prompts to generate videos exhibiting specific flawed dimensions.  You must first manually identify the model's weak dimensions and then design corresponding seed prompts, which is inherently subjective."}, "questions": {"value": "1. The entire optimization process relies on the MLLM providing accurate and meaningful scores. Could the authors provide a correlation analysis between the MLLM's dimension-specific scores and human subjective judgments?\n\n2. The fine-tuning is highly targeted on specific weak dimensions. Did the authors evaluate whether this targeted improvement comes at the cost of performance on other, non-optimized dimensions? For example, after fine-tuning on \"Human Interaction,\" did the model's performance on \"Aesthetic Quality\" or \"Background Consistency\" degrade?\n\n3. Could an automated system use a large set of diverse prompts, generate videos, and use the MLLM's own failure signals (low scores on certain prompt types) to automatically cluster and identify new weak dimensions without human pre-definition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d7rc00ONvb", "forum": "y7wAuwErpL", "replyto": "y7wAuwErpL", "signatures": ["ICLR.cc/2026/Conference/Submission3623/Reviewer_khhf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3623/Reviewer_khhf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809217342, "cdate": 1761809217342, "tmdate": 1762916878221, "mdate": 1762916878221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents GigaVideo-1, a lightweight post-training framework that boosts pre-trained text-to-video diffusion models without any human annotations or extra real videos. Key technical components are Prompt-Driven Data Engine and an LLM-based generator. With only 4 GPU-hours of full-parameter fine-tuning on Wan2.1-1.3B, the system raises the average VBench-2.0 score by ~4 percentage points, outperforming several larger-scale competitors. Extensive ablations, user studies, and cross-backbone transfers are provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- 4 GPU-hours is orders of magnitude cheaper than prior SFT/RL works, making the method attractive for practitioners.\n- The prompt engine explicitly amplifies failure modes, leading to a stronger training signal than random web videos.\n- 17 dimensions, 5 strong baselines, user study, ablation of data source & reward strategy, and tests on four different architectures (2B–13B)."}, "weaknesses": {"value": "- Sec. 4.3 shows that mixing synthetic prompts with synthetic videos ($P_sV_s$+$P_rV_s$) actually hurts accuracy, hinting that some LLM-generated captions are too exotic and push the model away from realism.\n- How do you filter or validate the LLM-generated captions to prevent physically impossible or nonsensical queries (e.g., “a person with three elbows”)?  Could such cases bias the model toward hallucination?\n- Have you tried a single, unified reward model (e.g., training a small diffusion critic on MLLM pseudo-labels) instead of switching between MLLM and specialist models?\n- What is the expected GPU-hour scaling for larger models?  Does the cost grow linearly with parameter count, or does the targeted small-data regime keep it sub-linear?\n- In joint training, did you explore dynamic loss-balancing techniques (grad-norm, uncertainty weighting, or Pareto optimisation) to mitigate the observed interference between dimensions?\n- Some related work discussed is helpful for improving manuscript quality, like InstructVideo and Lumos-1 etc.\n\n[1] InstructVideo: Instructing Video Diffusion Models with Human Feedback, CVPR.\n\n[2] Lumos-1: On autoregressive video generation from a unified model perspective, Arxiv."}, "questions": {"value": "Please see WEAKNESS."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7n8P7Pn5pf", "forum": "y7wAuwErpL", "replyto": "y7wAuwErpL", "signatures": ["ICLR.cc/2026/Conference/Submission3623/Reviewer_YMbE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3623/Reviewer_YMbE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983039660, "cdate": 1761983039660, "tmdate": 1762916876099, "mdate": 1762916876099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}