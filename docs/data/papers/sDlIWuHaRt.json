{"id": "sDlIWuHaRt", "number": 6801, "cdate": 1757996286237, "mdate": 1763640465666, "content": {"title": "Impact of Regularization on Calibration and Robustness: From the Representation Space Perspective", "abstract": "Recent studies have shown that regularization techniques using soft labels, e.g., label smoothing, Mixup, and CutMix, not only enhance image classification accuracy but also mitigate miscalibration due to overconfident predictions, and improve robustness against adversarial attacks. However, the underlying mechanisms of such improvements remain underexplored. In this paper, we offer a novel explanation from the perspective of the representation space (i.e., the space of the features obtained at the penultimate layer). Based on examination of decision boundaries and structure of features (or representation vectors), our study investigates confidence contours and gradient directions within the representation space. Furthermore, we analyze the adjustments in feature distributions due to regularization in relation to these contours and directions, from which we uncover central mechanisms inducing improved calibration and robustness. Our findings provide new insights into the characteristics of the high-dimensional representation space in relation to training and regularization using soft labels.", "tldr": "", "keywords": ["Soft labels", "regularization", "representation space", "image classification"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3dc5f3f98a63c162e080e795a03fb0986554154d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the underlying mechanisms of how regularization techniques using soft labels simultaneously improve model calibration and adversarial robustness. The authors provide a novel explanation from the perspective of the feature representation space (the penultimate layer). They find that regularization reduces the magnitude of feature vectors (bringing them closer to the origin) and increases their alignment (cosine similarity) with their respective class centers. This dual effect explains the seemingly contradictory improvements: the reduced magnitude acts like temperature scaling to improve calibration by mitigating overconfidence, while the increased alignment makes features more robust to gradient-based attacks by directing perturbations toward the origin rather than toward lateral decision boundaries."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1). Novel and Intuitive Explanation: The paper offers a clear, insightful, and geometrically intuitive explanation for a widely observed but not fully understood phenomenon. By decomposing the effect of regularization into changes in feature magnitude and alignment, it provides a strong conceptual framework.\n2). Resolves a Key Contradiction: It successfully addresses the apparent paradox where models become less overconfident (features closer to decision boundaries) yet more robust. The distinction between the radial boundary near the origin and the angular boundaries between classes is a key insight.\n3). Comprehensive Empirical Validation: The claims are backed by extensive experiments across multiple modern architectures (ResNet50, Swin-T, MobileNetV2, ConvNeXt-T) and datasets. The combination of 2D visualizations for intuition and quantitative analysis in the original high-dimensional space makes the evidence highly convincing."}, "weaknesses": {"value": "1)Focus on Gradient-Based Attacks: The core explanation for enhanced robustness hinges on the direction of loss gradients. The implications for non-gradient-based or black-box attacks are not discussed, which could limit the generality of the conclusions regarding adversarial robustness.\n2)Generalizability of Geometric Assumptions: The analysis is based on the \"cone-shaped\" structure of decision regions, which is demonstrated for image classification tasks with cross-entropy loss. The extent to which these geometric insights generalize to other domains (e.g., NLP, time-series), model types, or loss functions remains an open question that could be mentioned as a direction for future work.\n3)Narrow Scope of Regularization Techniques: The paper's title, \"Impact of Regularization...\", suggests a broad analysis, but the study exclusively focuses on soft-label-based methods (Label Smoothing, Mixup, CutMix). It does not investigate or contrast its findings with other prominent regularization techniques that are also known to affect robustness and feature norms, such as weight decay or dropout. A discussion on whether the proposed geometric mechanism (reduced magnitude and improved alignment) also explains the effects of these other regularizers would have significantly strengthened the paper's generality.\n4)Reliance on Simplified Assumptions for Theoretical Proof: The theoretical justification for why regularization reduces feature magnitude (Theorem 1 and its proof in Appendix H) rests on simplifying assumptions, namely that bias terms are negligible bc≈0 and weight vectors for all classes have similar norms ||wc||≈||w||. While the authors provide empirical evidence that these conditions often hold in practice for high-dimensional models, the theoretical claim itself is conditional. The paper would be more rigorous if it included a discussion on the behavior of the loss landscape and the validity of the theorem when these assumptions are not fully met."}, "questions": {"value": "Regarding the limited effectiveness against strong attacks (e.g., PGD and AutoAttack with large epsilon), could you elaborate on why the geometric advantage of better alignment might break down? Does the perturbation in the representation space become large enough to cross the origin into another cone, or does the local gradient direction for a well-aligned feature change significantly under strong, iterative attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RFoavSpMLS", "forum": "sDlIWuHaRt", "replyto": "sDlIWuHaRt", "signatures": ["ICLR.cc/2026/Conference/Submission6801/Reviewer_guCA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6801/Reviewer_guCA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530512057, "cdate": 1761530512057, "tmdate": 1762919072769, "mdate": 1762919072769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines how soft-label regularization methods, including label smoothing, Mixup, and CutMix, enhance accuracy, calibration, and robustness in image classification models. In particular, the paper investigates the representation space (i.e., the features in the penultimate layer of the network) and observes that the decision regions often form cone-shaped clusters around the origin, while using soft-label regularization tends to reduce the norm of the feature vectors and increase the cosine similarity between them and their class centers. Empirical visualization figures and experimental tables demonstrate that such observations may be key mechanisms explaining why soft-label regularization enhances the calibration and robustness of neural networks."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The perspective of studying the geometry of the feature space (norm, cosine similarities, decision-region shape) to more fundamentally explain why soft-label regularizations help is interesting.\n\n+ Overall, the paper is well-written and has a nice structure for readers to follow."}, "weaknesses": {"value": "- Many of the summarized key takeaways are toward a causal connection between soft-label regularizations, the change of the geometry of feature representation space, and the generalization/calibration/robustness measures of the neural network. However, only qualitative visualization figures or limited empirical analyses are provided as support.\n\n- Lack of mathematical rigor regarding the key concepts and theoretical analysis to back up the generalizability of the concluded findings.\n\n- The very low robustness performance against PGD and AutoAttack suggests the ineffectiveness of soft-label regularization, which contradicts the motivation of the study.\n\n- The insights might be trivial within the broader machine learning literature. There is a lack of practical implications of the work."}, "questions": {"value": "While understanding why utilizing soft labels helps improve generalization, confidence calibration, and robustness is an important research question, I believe the paper falls short in the following critical aspects: \n\n1.  My biggest concern is the lack of mathematical rigor regarding the key takeaways highlighted throughout the paper. The core concepts proposed or investigated are often intuitive, without a clear definition.  \n\n    For instance, the main findings highlighted in Section 3.1 are: (a) decision regions of classification models are cone-like shapes around the origin, and (b) without the bias term, the balance across different classes somewhat reduces. However, one would expect a rigorous definition of how the degree of cone-like shapes is measured for any given classification model, along with concrete quantitative results to support the main findings and their generalizability. Based solely on the qualitative figures converted into 2D for visualization purposes, I’m not convinced that the concluded findings are well-supported. The same concern holds for findings highlighted in other sections, such as the non-rigorous definition of minimum loss points and feature alignment with class centers. \n\n2. In Table 2, the model robustness performance is mostly very low against PGD and AutoAttack despite variations in terms of other metrics (e.g., RMS, Cosine Similarity, and ECE). This seems contradictory to the hypothesis, “soft-label regularization improves adversarial robustness”, which casts doubt on the initial motivation of the work. It is unclear whether regularization can truly enhance model robustness against worst-case perturbations. In addition, one key takeaway argues that regularization improves robustness to gradient-based adversarial attacks by enhancing feature alignment with class centers; however, this is hardly supported by the empirical results reported in Table 2. The correlation between feature-alignment-relevant metrics and adversarial robustness appears to be fairly weak. The setup of weak versions of PGD and AutoAttack also looks unusual to me, which seems to be a cherry-picked setup. Why evaluate the model robustness against a 7-step PGD attack with step size 0.2/255 and perturbation limit 0.4/255 in particular? \n\n3. While the paper summarizes a couple of key takeaways, it is unclear whether these key messages are insightful or trivial, given the broader machine learning literature on interpretability, calibration, and adversarial robustness. More importantly, the practical implications of the summarized findings are limited. For example, how can practitioners utilize the insights that regularization reduces feature norms and improves alignment?\n\n4. I believe many of the findings are based on empirically observed correlations or qualitative visualization figures. It would be more interesting and solid if they could be justified through theoretical analyses and/or intervention-type experimental designs. For example, can we design experiments that show reducing feature norms or improving feature alignment with the class center improves calibration and/or robustness, or even prove their theoretical connection (under certain conditions)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xkX9UvWuIe", "forum": "sDlIWuHaRt", "replyto": "sDlIWuHaRt", "signatures": ["ICLR.cc/2026/Conference/Submission6801/Reviewer_CRdf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6801/Reviewer_CRdf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745872948, "cdate": 1761745872948, "tmdate": 1762919072353, "mdate": 1762919072353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response by Authors"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback. Several reviewers highlighted the strengths of our work, including the clarity of the geometric perspective connecting soft-label regularization to representation-space behavior (Mpdz, uj9a, CRdf), the combination of intuitive 2D/3D visualizations with quantitative high-dimensional analyses (Mpdz, guCA), the comprehensive experimental evaluation (Mpdz, guCA, uj9a), and the novelty of resolving the contradiction between reduced confidence and improved robustness (guCA). We are grateful for these encouraging assessments.\nIn response to reviewer suggestions, we have expanded the manuscript to include several new analyses and clarifications:\n\n- Robustness to natural corruptions (ImageNet-C) (Appendix B.1)\n- Black-box adversarial transferability analysis (Appendix B)\n- Hyperparameter sensitivity of minimum-loss center estimation (Appendix I)\n- Strengthening the proof of Theorem 1 to make it fully agnostic to the presence of bias terms (Appendix J)\n- Discussion on the generalizability of geometric insights and future work (Section 8)\n\nWe have uploaded the first revision of the manuscript (changes are highlighted by the red color).\nWe hope our response and revision sincerely address all the reviewers’ concerns."}}, "id": "KBOVgh9RUr", "forum": "sDlIWuHaRt", "replyto": "sDlIWuHaRt", "signatures": ["ICLR.cc/2026/Conference/Submission6801/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6801/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6801/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763639023477, "cdate": 1763639023477, "tmdate": 1763639023477, "mdate": 1763639023477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how regularization techniques using soft labels (like label smoothing, Mixup, and CutMix) effect the features space of image classification models. The key finding is that in standard image classifiers (both CNNs and Transformers) trained using hard labels, the decision regions in the representation space (features from the penultimate layer) form cone-like shapes around the origin. The features in these models often have high magnitude, which results in over-confident predictions. Regularization reduces the magnitude (RMS) of feature vectors and features become more tightly clustered around their class centers, resulting in higher cosine similarity between features and class centers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It quantifies the correlation between the minimum loss point and confidence using Pearson correlation coefficients (Table 1) and intuitively illustrates \"cone-shaped decision regions\" and \"gradient direction changes\" via 2D visualizations (Figs. 1, 5). This makes abstract analysis of the high-dimensional representation space more interpretable.\n2. The principal concern this manuscript seeks to address relates to its significance in the broader context of machine learning research. While the authors present valuable insights into the mechanics of soft labels as a training technique, it is important to acknowledge that soft labels, despite being recognized as a general training trick, remain under-explored. This work opens up avenues for further investigation, specifically into the fundamental reasons behind their effectiveness. Expanding on this point could enhance the importance of their findings within the literature.\n3. The topic and goal of the paper is laudable, improved understanding of fundamental aspects of training pipelines such as regularization is very important to making scientific progress"}, "weaknesses": {"value": "1. The number of experiments is insufficient, the authors discuss the impact of regularization on robustness, but only one set of experiments is carried out. Prior works study robustness much more thoroughly and also study different aspects of it, such as robustness to natural corruptions, robustness to black box attacks, etc.\n2. Although it is stated that \"biases do not affect cone-shaped structures in high-dimensional spaces,\" the specific impact of bias magnitude on feature distribution is not quantified (e.g., whether the minimum loss point remains a stable class center when biases are large). Discussion of the \"boundary conditions for bias terms\" is inadequate.\n3. Failure to isolate single variables: For example, when verifying the role of \"feature magnitude,\" it does not design experimental groups where \"alignment is fixed while magnitude is adjusted independently\" (feature magnitude and alignment usually change synchronously in existing experiments). This makes it impossible to quantify the respective contribution ratios of \"reduced magnitude\" and \"improved alignment\" to performance.\n4. Lack of explicit ablation logic labeling: It does not organize the correspondence between variables and results in a dedicated section (e.g., \"Ablation Study\") or table (e.g., \"Ablation Results\"). Readers must extract logic from multiple tables and figures independently, reducing the intuitiveness of conclusions."}, "questions": {"value": "1. Are the \"feature magnitude reduction\" mechanisms of manual feature scaling and soft-label regularization completely equivalent? Is there a difference in their impacts on robustness? \n2. Does the effect of soft-label regularization on feature distribution depend on the ratio of the number of dataset classes to feature dimensionality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "i7qgO1dJ5c", "forum": "sDlIWuHaRt", "replyto": "sDlIWuHaRt", "signatures": ["ICLR.cc/2026/Conference/Submission6801/Reviewer_uj9a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6801/Reviewer_uj9a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904476283, "cdate": 1761904476283, "tmdate": 1762919071988, "mdate": 1762919071988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes why soft-label regularizers—Label Smoothing, Mixup, CutMix—improve calibration and gradient-based adversarial robustness by examining geometry in the representation space (penultimate-layer features). It argues that (i) decision regions are cone-like around the origin; (ii) confidence contours and loss gradients radiate from minimal-loss points; and (iii) soft-label regularization reduces feature norms (RMS) while increasing alignment to class “centers,” jointly explaining better calibration and robustness. Evidence spans CIFAR-10/100 and ImageNet with models including ResNet-50, Swin-T, MobileNetV2, ConvNeXt-T, and some pretrained ViT-B/16 variants (Tabs. 1–6; Figs. 1–7, 13–22)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clear geometric story (cone-shaped decision regions; outward confidence contours; gradient directions) supported by intuitive 2D/3D visualizations and high-dimensional checks (Fig. 1–2; Tabs. 3–4).\n- Consistent empirical patterns across backbones: with soft labels, RMS ↓, cos-sim to class center ↑, ECE improves, and attack success ↓ on ImageNet (Table 2; Figs. 3, 13–21).\n- Theoretical lens (Theorem 1) linking soft labels to finite-norm features, connecting feature-scaling with temperature scaling for calibration; simple post-hoc feature scaling sanity check (Fig. 22)."}, "weaknesses": {"value": "- Slightly dated framing / ConvNet-centric emphasis. While the experiments do include Swin-T and ViT-B/16, the motivation, related-work framing, and chosen regularizers are largely those historically developed for ConvNets; modern Transformer-first training regimes (large-scale weak supervision, diffusion-style objectives, masked modeling, data mixing specific to ViTs) and Transformer-tailored regularizers are not deeply discussed. This makes the narrative feel a bit ConvNet-era despite the ViT results.\n- Limited downstream perspective. The study focuses on classification (calibration/robustness) and does not evaluate fine-tuning to downstream tasks such as detection/segmentation/transfer, where representation-space changes could impact task loss landscapes and robustness differently. (No such finetuning sections or tables are provided.)\n- Comparison to Transformer literature could be sharper. The Discussion acknowledges that prior claims of ViT robustness/calibration can confound pretraining/regularization differences, but a systematic head-to-head under contemporary ViT training recipes (e.g., stronger data mixing, augment stacks specific to ViTs) is limited (Table 2 contrasts are helpful but narrow)."}, "questions": {"value": "- Transformer-first training: Can you add an experiment suite where ViT models are trained under modern ViT recipes (stronger data mixing/augment stacks used in ViT papers) to verify whether the RMS↓ / cos-sim↑ / ECE↓ patterns—and the robustness story—still hold, and how they compare against ConvNets at matched accuracy and compute? (Extend Table 2 with ViT-B/L under such settings.)\n- Downstream fine-tuning: Please report whether the softer, more compact features help or hurt fine-tuned downstream tasks (e.g., COCO detection/segmentation or VTAB-transfer). Even a linear-probe vs fine-tune summary table would clarify practical impact beyond classification.\n- Ablations on representation “centers”: Your results favor minimum-loss points over class means/weights (Table 1). Could you quantify how stable these centers are across seeds and training schedules, and whether center estimation noise affects the reported correlations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XcGAWTOZqz", "forum": "sDlIWuHaRt", "replyto": "sDlIWuHaRt", "signatures": ["ICLR.cc/2026/Conference/Submission6801/Reviewer_Mpdz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6801/Reviewer_Mpdz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932619530, "cdate": 1761932619530, "tmdate": 1762919071433, "mdate": 1762919071433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}