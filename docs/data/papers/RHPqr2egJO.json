{"id": "RHPqr2egJO", "number": 16765, "cdate": 1758268475239, "mdate": 1763722346264, "content": {"title": "Closing the Modality Gap Aligns Group-Wise Semantics", "abstract": "In multimodal learning, CLIP has been recognized as the \\textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is more pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we prove our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.", "tldr": "We prove that closing the modality gap, while irrelevant for instance-wise tasks, significantly enhances performance in group-wise tasks and we propose a combination of novel losses to do so.", "keywords": ["Multimodal Learning", "Representation Alignment", "Modality Gap"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9de9b039072433125d15aa80dacccbe3e35fdeb8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an approach to address the modality gap by introducing two complementary loss functions. The first, $\\mathcal{L}\\_{\\text{ATP}}$ directly aligns modalities by minimizing the Euclidean distance between true pairs. The second,$\\mathcal{L}\\_{\\text{CU}}$ makes semantic concepts (represented by their centroids) sparse and uniformly spread throughout the latent space.\n\nThese two losses are combined as $\\mathcal{L}\\_{\\text{gap}}$ and added to the standard contrastive loss. This method functions by pulling embeddings of the same concept from different modalities together ($\\mathcal{L}\\_{\\text{ATP}}$) while simultaneously pushing the centroids of different concepts apart ($\\mathcal{L}\\_{\\text{CU}}$).\n\nThen, this leads to reducing modality gap, not only with 2 modality like CLIP, but additional modalities within 3 or 4 modalities, which extend into general settings, improves performance in downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Recent studies have sought to interpret why CLIP-based models inherently exhibit a gap between modalities (e.g., intra-modal misalignment [1]). This paper clearly defines this problem and proposes an approach to mitigate the modality gap by grouping clusters with similar concepts to enhance intra-modal alignment and disentangle their representations.\n\n- In addition, the paper provides both qualitative and quantitative evidence supporting the effectiveness of the proposed method (e.g., qualitative results in Fig. 2 and Fig. 6; quantitative results in Fig. 3 and Fig. 4). The demonstrated reduction of the modality gap also leads to improved performance on downstream tasks, consistent with the paper’s main objectives.\n\n&nbsp;\n\n[1] Mistretta, Marco, et al. \"Cross the gap: Exposing the intra-modal misalignment in clip via modality inversion.\" ICLR 2025\\"}, "weaknesses": {"value": "**Major** \n\nIn my understanding, there remains a fundamental concern about whether reducing the modality gap is always beneficial [1][2]. For instance, in an image–text pair where the image contains additional contextual elements (e.g., trees) not mentioned in the text, the discrepancy reflects complementary rather than misaligned information. Specifically, let consider an image–text pair where the image depicts a dog playing with a frisbee under trees (Image Modality), while the text annotation reads *“The dog is playing with a frisbee.”* (Text Modality). In this case, the image contains additional, unique information (e.g., the presence of trees) that the text does not. This suggests that a certain degree of modality discrepancy can be natural or even desirable, as each modality may capture complementary aspects of the scene [3]. However, the paper assumes that reducing this gap is inherently optimal for multimodal learning but does not provide some justifications for this claim, just relying mainly on partial empirical observations (e.g., Fig. 6; Tables 6 and 7).\n\n[1] Yaras, Can, et al. \"Explaining and mitigating the modality gap in contrastive multimodal learning.\" CPAL 2025\\\n[2] Schrodi, Simon, et al. \"Two effects, one trigger: On the modality gap, object bias, and information imbalance in contrastive vision-language representation learning.\" ICLR 2025\\\n[3] Sammani, Fawaz, and Nikos Deligiannis. \"Interpreting and analysing CLIP's zero-shot image classification via mutual knowledge.\" NeurIPS 2024\n\n**Minor** \n\n- The paper appears to address a slightly trivial problem — primarily focusing on reducing the modality gap — without presenting a sufficiently distinctive or theoretically grounded contribution. It seems to be incremental contribution.\n- Limited comparison: The paper lacks comprehensive comparisons with other relevant approaches, such as  OTI/OVI [1] and CoMM [2].\n\n[1] Mistretta, Marco, et al. \"Cross the gap: Exposing the intra-modal misalignment in clip via modality inversion.\" ICLR 2025\\\n[2] Dufumier, Benoit, et al. \"What to align in multimodal contrastive learning?.\" ICLR 2025\n\n\n(**Note**: *The authors do not need to respond to these weaknesses, but to the questions listed below.*)"}, "questions": {"value": "1. (Related to the major weakness) What are the benefits of reducing the modality gap beyond merely improving model performance? While prior works have suggested that narrowing this gap leads to better VLMs or multimodal representations based on empirical results, a deeper discussion is needed. The authors should clarify why reducing the gap is inherently advantageous, rather than preserving some degree of modality-specific uniqueness.\n2. The relative strength of the proposed $\\mathcal{L}_{\\text{gap}}$ compared to existing methods, such as OTI, OVI, or CoMM, remains unclear. Therefore, additional experiments are likely required to demonstrate whether this approach provides a novel or complementary benefit when applied alongside other methods. Comparisons do not need to be limited to OTI, OVI, or CoMM; any reasonable alternative methods may be used.\n3. As mentioned in the Limitations (Appendix A.3), the method may struggle when datasets are highly imbalanced across modalities. As a minor concern, it would be valuable to include additional experiments or analyses evaluating performance under varying degrees of modality imbalance (e.g., via controlled imbalance scenarios or dataset subsampling). The choice of datasets for such experiments can be flexible.\n\n=======================================================\n\n**Note**: I acknowledge that I may have partially misunderstood certain aspects of the paper. Therefore, I am willing to raise my rating score if these questions and concerns are adequately addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A3zrtSwoxB", "forum": "RHPqr2egJO", "replyto": "RHPqr2egJO", "signatures": ["ICLR.cc/2026/Conference/Submission16765/Reviewer_pS85"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16765/Reviewer_pS85"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761399379160, "cdate": 1761399379160, "tmdate": 1762926810894, "mdate": 1762926810894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the *modality gap* in multimodal contrastive learning — the separation that naturally arises between different modalities (e.g., image and text) in shared embedding spaces. The authors argue that this gap, while having little effect on instance-wise retrieval, negatively impacts *group-wise* semantic alignment such as clustering or class-level organization. To address this, they propose two additional objectives: **Align-True-Pairs** ($L_{\\text{ATP}}$) to directly align paired samples, and **Centroid-Uniformity** ($L_{\\text{CU}}$) to promote balanced structure across modalities. When combined with InfoNCE, these losses explicitly reduce the modality gap. Experiments on several bi- and tri-modal datasets demonstrate improved clustering performance and maintained retrieval accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a timely and underexplored problem in multimodal contrastive learning — the **modality gap** — and offers an insightful analysis of its effects.  \n2. The distinction between *instance-wise* and *group-wise* consequences of the modality gap is both novel and intuitively well-motivated.  \n3. The proposed loss functions are simple, differentiable, and easily integrated into existing CLIP-style frameworks.  \n4. The experimental validation is extensive, spanning multiple bi- and tri-modal datasets and evaluating with diverse metrics such as $V$-Measure, cosine similarity, and retrieval accuracy.  \n5. The paper is **clearly presented** and well organized, with intuitive figures and a coherent narrative connecting the modality gap, angular separation, and semantic alignment."}, "weaknesses": {"value": "## Major  \n1. **Limited theoretical grounding.**  \n   - The main theoretical contribution — that the modality gap influences group-wise semantics — is persuasive but remains largely qualitative. A more formal treatment (e.g., linking it to the alignment–uniformity trade-offs in contrastive theory) would strengthen the argument.  \n   - The principal practical novelty appears to be Equation (8), as Equation (7) corresponds to the standard “alignment” term introduced by Wang & Isola (2020). Although the proposed term performs well empirically, the paper would benefit from a clearer intuition or justification for it — ideally from an information-theoretic or probabilistic perspective, as is common in recent contrastive learning research.  \n\n2. **Incremental methodological contribution.**  \n   Previous works (e.g., Liang et al., 2022; Schrodi et al., 2025) have already analyzed or attempted to reduce the modality gap. The proposed losses can thus be seen as practical refinements rather than fundamentally new formulations. Nonetheless, emphasizing *group-wise* effects remains an interesting and original perspective.  \n\n---\n\n## Minor  \n1. In Equation (3), the summation index should start at 1, not 0.  \n2. The meaning of the “semantic class” $c$ in Equation (3) is unclear — the notation suggests a single sample, which makes the formulation ambiguous. It would also be better to avoid reusing the symbol $c$, given that it is later used to denote centroids.  \n3. Equation (6) should be accompanied by a proof or derivation, ideally included in the appendix."}, "questions": {"value": "1. I would like to clarify a conceptual point: the paper states that the canonical goal of representation learning is to organize the representation space according to semantic structure. I would instead define the canonical goal as *learning representations that capture the task-relevant structure of the data to enable effective downstream learning*. While this may often result in semantic organization, it is not necessarily the goal itself. I’m curious to hear the authors’ perspective on this distinction.  \n2. Have you analyzed the sensitivity of the results to the weighting of $L_{\\text{ATP}}$ and $L_{\\text{CU}}$?  \n3. Could the proposed objectives interact with the temperature $\\tau$ in the contrastive loss, or could temperature scaling alone reduce the modality gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uIsvP4IIJH", "forum": "RHPqr2egJO", "replyto": "RHPqr2egJO", "signatures": ["ICLR.cc/2026/Conference/Submission16765/Reviewer_d752"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16765/Reviewer_d752"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762385892, "cdate": 1761762385892, "tmdate": 1762926810308, "mdate": 1762926810308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reviews the modality gap in multimodal representation learning, providing insights into the impact of this phenomenon on downstream tasks. By a series of experiments, this paper observes that the modality gap has a limited influence on instance-level tasks such as retrieval. However, it would significantly affect group-wise tasks like clustering. To this end, this paper proposes a new objective function to explicitly align true pairs while promoting latent space uniformity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly revisited an important distinction between instance-wise and group-wise tasks in multimodal learning, providing a new perspective on the modality gap phenomenon.\n2. The theoretical analysis offers a reasonable explanation for why the modality gap affects clustering performance but not retrieval, with the derivation showing how the gap inflates within-class scatter.\n3. The paper provides thorough experimental validation across diverse datasets and modalities, with clear visualizations of the latent space before and after closing the gap."}, "weaknesses": {"value": "1. The core idea of combining alignment and uniformity losses is not fundamentally new. Some methods previously established that contrastive learning involves alignment and uniformity, and explored closing the modality gap with similar objectives. The specific combination of LATP and LCU doesn't represent a significant methodological advance.\n2. The paper states that \"closing the modality gap\" is necessary for group-wise tasks, but the marginal improvements on some datasets (like CIFAR-10) suggest the gap may not be the primary limiting factor in many cases. \n3. While the paper claims to achieve \"nearly zero gap,\" it doesn't address whether completely closing the gap might lead to other issues like reduced discriminative power between different concepts or increased vulnerability to modality-specific noise.\n4. This paper mentions that \"the entire latent space collapses into small portions\" without the LCU loss, but the ablation studies don't clearly quantify this collapse. The authors should provide visualization or metrics showing the extent of collapse when using only LATP versus the full approach.\n5. The paper claims that \"closing the modality gap\" is the key to improving group-wise tasks. However, could the observed improvements be due to other factors, such as the increased expressiveness of the latent space from your loss formulation rather than specifically from closing the gap?"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zyumtfntqy", "forum": "RHPqr2egJO", "replyto": "RHPqr2egJO", "signatures": ["ICLR.cc/2026/Conference/Submission16765/Reviewer_h2Ko"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16765/Reviewer_h2Ko"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996192037, "cdate": 1761996192037, "tmdate": 1762926809344, "mdate": 1762926809344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors observe that modality gap does not necessarily affect fine-grained tasks like retrieval in some cases but could affect group-level tasks like clustering in multimodal learning. Based on this, they design a loss function which encourages groups of positive samples to be compactly embedded, while uniformly dispersing negative samples on the hypersphere. They evaluated their approach on several instance-wise and group-wise tasks using standard datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The observation that modality gap does not necessarily affect fine-grained tasks like retrieval in some cases but could affect group-level tasks like clustering is a novel contribution.\n\n2. The idea of applying cluster-based contrastive losses to close the modality gap in multimodal models is an interesting.\n\n3. The motivation of the paper is clear and the authors do a good job conveying intuitions for some of their claims.\n\n4. Indeed as the authors claim, their empirical results confirm that reducing the modality gap improves group-level tasks like clustering, while leaving tasks like retrieval relatively unaffected in the cases considered."}, "weaknesses": {"value": "1. In Section 3.2, the authors mention that one of the reasons contributing to modality gap is the use of networks with different random initialization for different modalities? How about the scenario where weights are shared across modalities (all modalities are encoded via the same network), since that is the way several widely used multimodal models are trained? Furthermore, although the authors claim this to be one of the sources of modality gap, their solution to resolving it only revolves around modified loss functions, and no attempt to align initialization weights across encoders.\n\n2. The way Section 3.2 is currently organized, it seems as if different random initializations for different modalities is the only source of modality gap. The point about the gap arising from the limitations of the CLIP loss does seems to be made suddenly in the second paragraph and does not follow naturally from the first. I would suggest reorganizing it so as to first enumerate the sources of modality gap, followed by descriptions of how they might be related / affect each other etc., if such relationships exist.\n\n3. In the second paragraph of Section 3.2, authors claim the existence of what they call \"semantic stripes\" in the representation space of CLIP. Although I can intuitively understand what they are, I would still like to know if the authors have any concrete evidence, theoretical or empirical, to establish their existence. Without any such evidence, such claims seem rather arbitrary.\n\n4. The idea of performing contrastive learning by contrasting cluster centroids is not particularly novel, as research along similar lines is being conducted for several years in the self-supervised learning literature [d, e].\n\n5. It is necessary to investigate further the purpose and effect of ATP. If I understand correctly, ATP is meant to encourage the formation of clusters and prevent positive pairs from being far apart. However, this is not stated explicitly, and there is no term in ATP, unlike the CU loss, involving cluster centroids.\n\n6. It is not entirely true that eliminating modality specific features is guaranteed to leave instance-level tasks like retrieval unaffected. There are several works in the literature [f, g, h] that point to the contrary based on the fact that in many retrieval tasks, there are modality-specific features which could be informative about the downstream task and the instances being compared. The assumption may hold for the experimental settings considered here, but is certainly not generally true.\n\nReferences:\\\n[a] Ngiam et al., \"Multimodal Deep Learning\", ICML 2011.\\\n[b] Hu et al., \"Towards Unsupervised Sketch-based Image Retrieval\", BMVC 2022.\\\n[c] Rastegar et al., \"MDL-CW: A Multimodal Deep Learning Framework with Cross Weights\", CVPR 2016.\\\n[d] Caron et al., \"Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\", NeurIPS 2020.\\\n[e] Caron et al., \"Deep Clustering for Unsupervised Learning of Visual Features\", ECCV 2018.\\\n[f] Jing et al., \"Cross-Modal Center Loss for 3D Cross-Modal Retrieval\", CVPR 2021.\\\n[g] Chaudhuri et al., \"Cross-Modal Fusion Distillation for Fine-Grained Sketch-Based Image Retrieval\", BMVC 2022.\\\n[h] Ren et al. \"Cross-modal retrieval based on multi-dimensional feature fusion hashing\", Frontiers in Physics 2024.\n\nMinors:\\\nLine 142: \"in narrow different cones\" -> \"in different narrow cones\"\\\nLine 207: \"suppose to retrieve a cat image caption. The sufficient condition...\" -> \"suppose to retrieve a cat image caption, the sufficient condition...\""}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Uh9GkCjUFx", "forum": "RHPqr2egJO", "replyto": "RHPqr2egJO", "signatures": ["ICLR.cc/2026/Conference/Submission16765/Reviewer_4NhU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16765/Reviewer_4NhU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997601708, "cdate": 1761997601708, "tmdate": 1762926808654, "mdate": 1762926808654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}