{"id": "Ok9uHVtBHQ", "number": 4000, "cdate": 1757581812065, "mdate": 1759898059064, "content": {"title": "BLIPs: Bayesian Learned Interatomic Potentials", "abstract": "Machine Learning Interatomic Potentials (MLIPs) are becoming a central tool in simulation-based chemistry. However, like most deep learning models, MLIPs struggle to make accurate predictions on out-of-distribution data or when trained in a data-scarce regime, both common scenarios in simulation-based chemistry. Moreover, MLIPs do not provide uncertainty estimates by construction, which are fundamental to guide active learning pipelines and to ensure the accuracy of simulation results compared to quantum calculations. To address this shortcoming, we propose BLIPs: Bayesian Learned Interatomic Potentials. BLIP is a scalable, architecture-agnostic variational Bayesian framework for training or fine-tuning MLIPs, built on an adaptive version of Variational Dropout. BLIP delivers well-calibrated uncertainty estimates and minimal computational overhead for energy and forces prediction at inference time, while integrating seamlessly with (equivariant) message-passing architectures. Empirical results on simulation-based computational chemistry tasks demonstrate improved predictive accuracy with respect to standard MLIPs, and trustworthy uncertainty estimates, especially in data-scarse or heavy out-of-distribution regimes. Moreover, fine-tuning pretrained MLIPs with BLIP yields consistent performance gains and calibrated uncertainties.", "tldr": "BLIPs is a Bayesian framework that converts any machine-learned interatomic potential into a Bayesian version, improving predictive accuracy and uncertainty quantification with minimal overhead.", "keywords": ["Machine Learning Force Fields", "Machine Learning Potentials", "Uncertainty Quantification", "DFT", "Molecular Dynamics", "Bayesian Methods"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7367c0b306592d1916d5389cf7caf6b627f18bd0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a method for uncertainty quantification based on Bayesian inference that can be used with existing MLIP architectures. They evaluate the method on synthetic data, NH3, and a silica glass system. They evaluate UQ performance and accuracy on energy and force metrics."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Uncertainty quantification is an important aspect not yet fully addressed by the MLIP community. \n- The proposed method is motivated well and grounded in existing work.\n- In general, I agree with the authors that tackling UQ / generalization with MLIPs is an important area, especially since many important scientific applications deal explicitly with extrapolation."}, "weaknesses": {"value": "While I think the method makes sense and is well presented, the evaluation makes it hard to justify the benefit of the proposed approach:\n- The models considered are not the best models. Although the authors mention that the method is not currently amenable to new models like UMA, if the authors are claiming to evaluate generalization capability, then it is important to consider models explicitly trained with broad coverage. While the authors look at Orb, it is hard to evaluate how meaningful any E/F improvements are. The difference seems to go away as data increases and the relative performance benefit it quite small compared to the difference between the PaiNN model trained from scratch and the pre-trained Orb. The UQ performance seems to be worse here than an ensemble.\n- In such data-limited regimes, it feels like a deep learning model is not the best choice. A simpler model like sGDML [1] often performs quite well with limited data. How does sGDML perform in terms of UQ and errors?     \n- The authors only evaluate F/E regression metrics without evaluating downstream performance (like MD simulations).\n- It would strengthen the paper to more clearly address the computational costs during both training and inference. The authors claim multiple times that these models are cheaper than ensembles but don't explicitly quantify the differences. If the cost of having multiple models is an issue, then different readout heads could be evaluated as an alternative. In addition, there are many other cheap ways of evaluating uncertainty, including training a smaller ML model [1]. Bayesian neural networks are also known to be hard to train at scale so it is hard to evaluate how generally applicable this method would be (or if the authors envision this as just a fine-tuning strategy).\n- There are a number of existing works that discuss out-of-distribution performance and benchmarks for MLIPs and might be relevant for this work [2,3,4].  \n\n\n[1] Chmiela, S., Sauceda, H., Poltavsky, I., Müller, K.R., & Tkatchenko, A. (2019). sGDML: Constructing accurate and data efficient molecular force fields using machine learning. Computer Physics Communications, 240, 38–45.\n\n[2] Bowen Deng, Yunyeong Choi, Peichen Zhong, Janosh Riebesell, Shashwat Anand, Zhuohan Li, KyuJung Jun, Kristin A. Persson, & Gerbrand Ceder. (2024). Overcoming systematic softening in universal machine learning interatomic potentials by fine-tuning.\n\n[3] Tobias Kreiman, & Aditi S. Krishnapriyan. (2025). Understanding and Mitigating Distribution Shifts For Machine Learning Force Fields.\n\n[4] Chanussot, L., Das, A., Goyal, S., Lavril, T., Shuaibi, M., Riviere, M., Tran, K., Heras-Domingo, J., Ho, C., Hu, W., Palizhati, A., Sriram, A., Wood, B., Yoon, J., Parikh, D., Zitnick, C., & Ulissi, Z. (2021). Open Catalyst 2020 (OC20) Dataset and Community Challenges. ACS Catalysis, 11(10), 6059–6072."}, "questions": {"value": "- What are the energy errors in Fig. 3?\n- Why is performance worse with 128 samples vs. 32 samples in Fig. 3?\n- What is the inference slowdown from using this type of model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6GsU7wlRTl", "forum": "Ok9uHVtBHQ", "replyto": "Ok9uHVtBHQ", "signatures": ["ICLR.cc/2026/Conference/Submission4000/Reviewer_3VJ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4000/Reviewer_3VJ2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761443383858, "cdate": 1761443383858, "tmdate": 1762917131363, "mdate": 1762917131363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes how a particular formulation of variational inference can be applied to the setting of a machine learning interatomic potential. The method uses local reparametrization and involves input dependent variance scaling factors: the variance of each weight is specified as the squared mean value multiplied by an input dependent scale (adaptive variance term). It is discussed how the method retains equivariance (in distribution) when used with an equivariant GNN architecture. The method is demonstrated on a simple N-body system, a small ammonia dataset with training data near equilibrium an out of equilibrium test data, and finetuning a pretrained ORB model on a SiO2 dataset. The results show favorable peformance in comparison with deep ensembles."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and easy to follow.\n\nThe proposed modeling approach is reasonable and seems to work well. It is very general and can be applied to a long range of problems, also outside MLIPs."}, "weaknesses": {"value": "Deep ensembles are commonly used for UQ in MLIPs, and there are several papers that focus on different ways to do this, however this literature is not cited sufficiently. In particular it would be beneficial with a discussion on how posthoc calibration is beneficial. \n\nAll examples are in the small data regime and for the most part concerned with out of distribution uncertainty. Typically this means high sensitivity to hyperparameter choices. Often a slightly over-regularized model can trade in distribution performance for better out-of-distribution performance. However, in-distribution performance is not reported, and it is not described in sufficient detail how baselines including deep ensembles were tuned. \n\nThe paper is positioned (and titled) as a Bayesian method, however when using a data dependent approximate posterior, the Bayesian interpretation is a stretch.\n\nThe discussion around equivariance and invariance in distribution in section 3.3 is very interesting and important in my opinion. I would have liked to see this extended to a more formal analysis, showing exactly when and how invariant coefficients (alpha and beta) is enough to guarantee equivariance in distribution for an equivariant base architecture. Furthermore, the argument that this does not hold for architectures based on irreducible representations of SO(3) would be very interesting to unfold more."}, "questions": {"value": "I did not find a reference to Figure 1 in the text - is it missing?\n\nIf I understand correctly, eq. (8) is not actual objective used? There is a scaling factor on the KL term as described in A.1. This should be stated more clearly in the main text. \n\nWhy is the prior in eq. (9) written in terms of it sampling procedure rather than directly as its distribution?\n\nEq. (9): I assume this variance matches the variance under weight dropout with the weights rescaled by 1/(1-p)? If I have understood correctly, consider writing this more clearly in the text.\n\nWhat exactly were hyperparameters for the Ammonia data? Did you use the same hyperparameters for fitting the ensemble? If so, this would likely not be optimal.\n\nHow exactly are the ORB models without BLIP finetuned? This could be done in many different ways (full finetuning, last layer retraining, last layer finetuning, adapters, etc.) Are you certain a well chosen finetuning strategy would not be competitive?\n\nIn C.1 what exactly does this mean: \"Dropout probabilities p are predicted and (...)\"? I am missing some detail regarding this.\n\nIn C.3 it is mentioned how the KL scale and prior (dropout probability), but details are missing regarding how these were chosen. \n\nHow exactly were the train/validation splits created? \n\nHow exactly were validation data used for BLIP, for training ensembles and for finetuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NCeNV9RDqc", "forum": "Ok9uHVtBHQ", "replyto": "Ok9uHVtBHQ", "signatures": ["ICLR.cc/2026/Conference/Submission4000/Reviewer_EYRq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4000/Reviewer_EYRq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571259276, "cdate": 1761571259276, "tmdate": 1762917131189, "mdate": 1762917131189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Bayesian framework for learning potential energy surfaces (PESs) via equivariant message passing neural networks. Compared to the non-Bayesian setting, this approach in principle allows for uncertainty quantification. It is also claimed that predictive accuracy is improved through the Bayesian approach. \n\nIn contrast to previous approaches which rely on (compute and memory intensive) ensemble methods, the presented method only introduced a light computational overhead for a given model. It is also flexible and can be incorporated into existing neural network potentials. \n\nThis seems like a valuable contribution to the field."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper significantly contributes to an important problem and provides a novel approach to Bayesian modeling in computational chemistry."}, "weaknesses": {"value": "The paper claims that the Bayesian approach improves predictive accuracy for out of distribution configurations, specifically out of equilibrium geometries. For out of equilibrium geometries (in case they have multireference character) even DFT will not provide accurate energies or forces, which makes this a quite fundamental issue. I have a difficult time believing that a Bayesian model should overcome this and would appreciate a more detailed elaboration, as well as an explanation of what precisely causes the gain in accuracy."}, "questions": {"value": "I would appreciate a more detailed explanation of Variational Adaptive Dropout (maybe in the Appendix)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0UjTnPD954", "forum": "Ok9uHVtBHQ", "replyto": "Ok9uHVtBHQ", "signatures": ["ICLR.cc/2026/Conference/Submission4000/Reviewer_cqQ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4000/Reviewer_cqQ8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918544375, "cdate": 1761918544375, "tmdate": 1762917131001, "mdate": 1762917131001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors apply a variational Bayesian framework for machine learning interatomic potentials, used for molecular modeling and simulation. The goal of this framework is to better calibrate the model and quantify uncertainty when encountering new atomistic structures. The authors apply this method to training a model from scratch on ammonia data, and fine-tuning the Orbv3 model on a new dataset. The error is quantified based on the energy and force MAE."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- There is a need for MLIP uncertainty quantification methods, and the variational framework applied here has been less common for MLIPs."}, "weaknesses": {"value": "A number of my concerns around the paper are understanding what utility this approach has given the progress in the field. \n\n- The MLIP field now has models trained on large, broad datasets. The experiment of training a network from scratch feels unrealistic now, such as the first example the authors look at. They are also using an architecture (PaiNN) that is outdated, with many architectures having improved on this architecture. It’s not clear if this method can actually help the best models trained on large datasets, which is where the utility of the method would be.\n\n- Error is only quantified by energy and force MAE, vs. other evaluation tasks. The BLIP error in Table 3 and Figure 3 is not very statistically significant to show that the method actually improves, even in the settings that the authors picked which don’t seem very relevant to where modern-day MLIPs are at now. The bar chart in Figure 3 is also highly misleading in the y-axis units.\n\n- The models used and comparisons are also not the best ones. It is unclear if the method can actually make the best models better.\n\n- This paper generally needs a lot of work, and it is very unclear if this method would actually be useful to use for MLIPs. This includes using the best models trained on a lot of data, more rigorously exploring out-of-distribution cases, and more rigorously quantifying the utility of the uncertainty estimates and performance improvements on downstream tasks. An example place to start would be models trained on the Open Molecules 2025 dataset."}, "questions": {"value": "- What does it look like if the method is used with an MLIP trained on larger datasets, such as eSEN or UMA? \n\n- Building on the above, can you show examples where the uncertainty quantification provided by this method is noticeably beneficial in some way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YGwuuLDfaL", "forum": "Ok9uHVtBHQ", "replyto": "Ok9uHVtBHQ", "signatures": ["ICLR.cc/2026/Conference/Submission4000/Reviewer_BtQJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4000/Reviewer_BtQJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762747950003, "cdate": 1762747950003, "tmdate": 1762917130668, "mdate": 1762917130668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}