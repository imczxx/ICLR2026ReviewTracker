{"id": "5OXvNX9LWh", "number": 15436, "cdate": 1758251322335, "mdate": 1759897307292, "content": {"title": "CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation", "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in generating highly structured texts. However, while exhibiting a high degree of structural organization, movie scripts demand an additional layer of nuanced storytelling and emotional depth—the 'soul' of compelling cinema—that LLMs often fail to capture. To investigate this deficiency, we first curated CML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup Language (CML), where 'content' consists of segments from esteemed, high-quality movie scripts and 'summary' is a concise description of the content. Through an in-depth analysis of the intrinsic multi-shot continuity and narrative structures within these authentic scripts, we identified three pivotal dimensions for quality assessment: Dialogue Coherence (DC), Character Consistency (CC), and Plot Reasonableness (PR). Informed by these findings, we propose the CML-Bench, featuring quantitative metrics across these dimensions. CML-Bench effectively assigns high scores to well-crafted, human-written scripts while concurrently pinpointing the weaknesses in screenplays generated by LLMs. To further validate our benchmark, we introduce CML-Instruction, a prompting strategy that provides detailed instructions on character dialogue and event logic to guide LLMs in generating more structured and cinematically sound scripts. Extensive experiments validate the effectiveness of our benchmark and demonstrate that LLMs guided by CML-Instruction generate higher-quality screenplays, with results aligned with human preferences. Our work offers a comprehensive framework for both evaluating and guiding LLMs in screenplay authoring.", "tldr": "We introduce CML-Bench, a new framework with a benchmark to evaluate and an intermediate syntax CML to help Large Language Models write much better, more structured, and coherent movie scripts.", "keywords": ["Creative Text Benchmarking", "AI in Filmmaking", "Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d79d776071560f5778670f4462369a801452f2e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to address the common failure of LLMs to generate movie scripts with sufficient emotional depth and narrative coherence. To do this, the authors first introduce the CML-Dataset, a collection of 100 script segments from highly-rated films paired with LLM-generated summaries. Based on an analysis of these scripts, they identify three core dimensions for quality assessment: DC, CC and PR. They then build upon these dimensions to create CML-Bench, a quantitative evaluation benchmark, and propose CML-Instruction, a structured prompting strategy using XML-like tags to improve script generation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- This paper is easy to follow.\n- The paper tackles an important problem. Evaluating the quality of long-form creative text like screenplays is far more complex than standard NLP tasks."}, "weaknesses": {"value": "**1. Internal Inconsistencies and Lack of Polish**\n\nThe manuscript contains contradictory statements and appears unfinished. Throughout the paper, it sometimes states that the benchmark has 8 metrics, and at other times it says 9. This caused significant confusion for the reader. For example, L88 mentions that CML-bench consists of 8 interpretable metrics, but L251 says that CML-bench has 9 interpretable metrics. \n\nFurthermore, the manuscript contains numerous unresolved references, which appear as double question marks. For example, L95, L106 and L238. This suggests a lack of careful proofreading before submission and significantly undermines the paper's professionalism.\n\nTable 1 presents numerical results with inconsistent precision, mixing values with two and three decimal places without clear justification. This suggests a lack of attention to detail in the presentation of results.\n\n**2. CML-Instruction is insufficient to be considered a key contribution**\n\nThe paper presents CML-Instruction as a key contribution. However, its core mechanism is a form of in-context learning, providing detailed instructions and examples in the prompt, which is a well-established capability of basically all modern LLMs. Claiming this widely supported prompting technique as a novel contribution overstates its originality.\n\n**3. Experimental Evaluation Confounds Formatting with Quality**\n\nAnother main flaw of the paper lies in its experimental design. The results in Table 1 show that base LLMs score very poorly, while instruction models score close to human level. However, the appendix candidly admits that the primary reason for the low scores is the failure of base models to generate the specific XML-like format expected by the CML-Bench parser. Therefore, the experiment does not robustly demonstrate that CML-Instruction improves the creative quality of the script, but rather that it improves the model's adherence to a specific parser format.\n\n**4. Confusing Metrics**\n\nSome of the metric definitions appear very confusing. For example, the descriptions for PR1 (Event Sequence Semantic Coherence) and PR2 (Event Coherence) make the me feel they are nearly identical. And there is no formulation for PR2 to help readers better understanding it. Furthermore, the authors admit in the appendix L982 that \"PR2 is not a good indicator of discrimination\".\nThis admission is confusing and directly contradicts the PR2's inclusion, which severely undermines the credibility of CML-Bench as a carefully designed benchmark.\n\n**5. Not Enough Samples**\n\nThe CML-Dataset contains only 100 samples. This is a very small number for a benchmark intended to evaluate a complex, high-variance task like creative script generation."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1di3eSpaeB", "forum": "5OXvNX9LWh", "replyto": "5OXvNX9LWh", "signatures": ["ICLR.cc/2026/Conference/Submission15436/Reviewer_DnJr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15436/Reviewer_DnJr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761041520411, "cdate": 1761041520411, "tmdate": 1762925720994, "mdate": 1762925720994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper contributes CML-Dataset, CML-Bench, and CML-Instruction. The CML-Dataset consists of script-summary pairs where the script is a 15-20 scene segment of a popular movie script and the summary is generated by an LLM describing this script. Using this dataset, they then design 9 metrics which can be used to judge dialogue coherence, character consistency, and plot reasonableness of scripts. These metrics in combination with their dataset form CML-Bench, a benchmark on which to judge LLM abilities to generate high quality movie scripts. Finally, they propose CML-Instruction, a method of prompting LLMs which improves the quality of their generated movie scripts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\n- the proposed metrics seem to distinguish between high quality and low quality scripts well and provide a nice combination of structured and LLM-based components\n\nQuality:\n- a human study is included to validate results\n\nClarity:\n- the visual depictions of results help to clearly communicate performance details\n- contributions are clear\n\nSignificance:\n- the proposed metrics seem like they would be useful to help evaluate future movie script generation methods"}, "weaknesses": {"value": "1. The presentation of the paper lacks polish. For example, the intro has a broken figure reference, there are several missing periods throughout, the wrong \\cite command is frequently used (citations should be in parentheticals in most places), figure captions lack important details (are the numbers in figure 2 counts or percents? what is the x axis in figure 3 and are the values averages or singular examples?), and the font in many plots is too small to be legible (e.g., figure 5).\n\n2. There are not enough details of the human study to determine its validity. Most importantly, who are the human annotators and how were they recruited? If applicable, was IRB approval obtained? What is the level of agreement between annotators?\n\n3. There is not enough detail on the metric implementation to fully understand them. Most importantly, what is the creativity analysis noted in line 281? Also, how are emotions classified? Finally, there are many points where an LLM is used to extract some detail (e.g., keywords, distinctive speech features, etc.) and we need more details on how this is done to understand the output of these extractions.\n\n4. I am skeptical of the usefulness of CML-Instruction and CML-Bench. Figure 4 shows that CML-Instruction brings LLMs mostly up to ground-truth performance across almost all metrics. If this is the case, is performance on this benchmark already saturated? Also, why does CML-Instruction improve results so much across all metrics? Without more qualitative discussion of these results, I'm skeptical that the methodology is not somehow gaming the metrics. In general, the results section needs more discussion of the results as it is currently just one short paragraph of text."}, "questions": {"value": "Suggestion: The intro includes a lot of detail on the dataset collection and methods that could be left for the later section. The intro would be stronger if it focused more on contextualizing the importance of the work and key takeaways."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ggJLE64ags", "forum": "5OXvNX9LWh", "replyto": "5OXvNX9LWh", "signatures": ["ICLR.cc/2026/Conference/Submission15436/Reviewer_gRNh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15436/Reviewer_gRNh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683179413, "cdate": 1761683179413, "tmdate": 1762925720408, "mdate": 1762925720408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CML-Bench, a comprehensive framework for evaluating and enhancing movie script generation by large language models (LLMs). Although LLMs are proficient at producing structurally coherent text, they often fall short in capturing the narrative depth and emotional nuance essential to cinematic storytelling. To bridge this gap, the authors introduce a pipeline comprising three key components: CML-Dataset, CML-Bench, and CML-Instruction. Experimental results show that while LLMs struggle across all screenplay quality dimensions, applying CML-Instruction leads to significant improvements, yielding scripts with coherence and logic comparable to human-written ones. Human evaluations further demonstrate strong alignment with CML-Bench scores, validating the benchmark’s robustness and reliability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work introduces a well-structured, domain-specific benchmark (CML-Bench) with interpretable metrics and a curated dataset (CML-Dataset), filling a clear gap in evaluating LLMs for creative, long-form movie script writing.\n\n2. The CML-Instruction prompting strategy is practical and demonstrates strong, quantitative improvements on various LLMs, as shown in Figure 4 and Table 1.\n\n3. Main results on CML-Bench are comprehensive, covering a wide range of mainstream LLMs (e.g., Qwen, LLaMA, Gemini, DeepSeek), which ensures the conclusions are broadly representative and robust across model architectures.\n\n4. The paper is clearly written and well-organized."}, "weaknesses": {"value": "1. The CML-Dataset remains relatively small (100 films) and limited to English-language, high-rated classic films, which may restrict its generalizability.\n\n2. The evaluation still partially depends on LLM-based feature extraction, which may introduce bias.\n\n3. The comparison and discussion with existing benchmarks (e.g., MovieBench, Movie101) is missing, making it difficult to assess CML-Bench's contribution to the community.\n\n4. The caption in Figure 5 is incomplete, and the illustration is too blurry to read.\n\n5. L107: Missing figure reference number."}, "questions": {"value": "The performance improvements achieved with CML-Instruction are substantial across all evaluated models, in some cases approaching human-level results, but the performance on character originality remains consistently low across models. Could the authors discuss what factors might contribute to this discrepancy?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "details_of_ethics_concerns": {"value": "The paper may raise fairness and bias concerns since the CML-Dataset is composed exclusively of English-language, high-rated classic films, which could reinforce cultural and stylistic biases in both evaluation and generation."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8VTxpfjl2g", "forum": "5OXvNX9LWh", "replyto": "5OXvNX9LWh", "signatures": ["ICLR.cc/2026/Conference/Submission15436/Reviewer_yFrW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15436/Reviewer_yFrW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913432128, "cdate": 1761913432128, "tmdate": 1762925717445, "mdate": 1762925717445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CML-Bench (Cinematic Markup Language Benchmark) — a new benchmark framework for evaluating large language models (LLMs) on screenplay or movie script generation. Unlike traditional text generation benchmarks, CML-Bench focuses on cinematic storytelling quality, aiming to measure whether LLMs can produce coherent, character-consistent, and narratively reasonable scripts. The benchmark design is logically organized into three principal evaluation dimensions—Dialogue Coherence (DC), Character Consistency (CC), and Plot Reasonableness (PR)—each further decomposed into mathematically defined sub-metrics. As a benchmark work, this paper design comprehensive evaluation metrics and comparative baselines to give a solid results. However, there is room to improve on presentation. Here are some detailed comments:\n1. Lack of clear definition of \"Cinematic Markup Language”\nThe concept of Cinematic Markup Language—which appears to be central to this paper—is not clearly defined or contextualized in the introduction or related work sections. Since this is not a widely recognized or established task in the broader multimodal learning community, readers unfamiliar with the term will find it difficult to understand the problem formulation and motivation. I strongly recommend that the authors provide a precise and intuitive definition of CML early in the paper (e.g., in the introduction or Section 2), clarify its distinction from related multimodal representation or video annotation tasks, and perhaps add one or two concrete examples for better accessibility.\n2. Formatting and figure quality issues.\nThe manuscript contains several writing and formatting errors that significantly affect readability. For example: There are multiple broken or missing figure references on page 2 and page 5 (e.g., \"Figure ??\"), suggesting incorrect LaTeX linking. Several figures (e.g., Figures 1, 2, and 4) have font sizes that are too small to read clearly, especially in axis labels and legends.\nThese presentation issues make it difficult for readers to interpret the experimental setup and key results. The authors should carefully proofread the paper, fix the figure reference errors, and increase the resolution and font size of all plots and diagrams in the future version."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tAddresses an underexplored but valuable task. The paper tackles an emerging and underrepresented domain — screenplay generation and cinematic storytelling, which indeed deserves attention within the multimodal and creative AI community.\n2.\tConceptual originality in metric design. The decomposition of screenplay quality into nine measurable components is creative and well-aligned with the narrative theory of scriptwriting, offering a potentially reusable evaluation framework for creative generation tasks."}, "weaknesses": {"value": "1.\tQuestionable validity of benchmark effectiveness. A major concern is that several LLM baselines outperform human-written scripts (the supposed “ground truth”) on many of the proposed metrics. The authors may could provide a qualitative or human-evaluation-based sanity check to justify this outcome.\n2.\tInsufficient evidence of real-world applicability. It remains unclear whether the proposed benchmark will generalize beyond this dataset or meaningfully correlate with human judgments. \n3.\tWriting quality. I recommend authors polish the paper for better understanding."}, "questions": {"value": "The questions are similar with what I mentioned in the weakness part. There are mainly two questions:\n1.\tIs the usage of MovieSum’s training set reasonable? These movies are classic and famous movies, there might be some data leak for those LLMs.\n2.\tThe effectiveness of benchmark. The score of many LLMs are better than GT."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LSoGozHNmB", "forum": "5OXvNX9LWh", "replyto": "5OXvNX9LWh", "signatures": ["ICLR.cc/2026/Conference/Submission15436/Reviewer_AWtP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15436/Reviewer_AWtP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088099456, "cdate": 1762088099456, "tmdate": 1762925715559, "mdate": 1762925715559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}