{"id": "tJgVC9UyeW", "number": 695, "cdate": 1756773258975, "mdate": 1763619550832, "content": {"title": "CACD-SEG: Contrastive Alignment Consistent Distillation for All Day Semantic Segmentation", "abstract": "Existing semantic segmentation methods based on traditional frame cameras often encounter issues in complex lighting scenes, such as low-light nighttime or overexposed scenes, and boundary ambiguities caused by motion blur in high-speed scenarios. Event cameras, with their high dynamic range and high temporal resolution, can effectively alleviate these issues and have consequently attracted increasing attention. However, most existing event-based semantic segmentation methods employ straightforward concatenation feature fusion, overlooking the heterogeneity of features between the two modalities. To address these issues, we propose an event-frame alignment-distillation semantic segmentation method. Specifically, we design a heterogeneous feature contrastive alignment module that projects both modalities into a common space to bridge the representation gap. Furthermore, we present a joint boundary-content knowledge distillation module to transfer the clear region and edge information captured by event camera to frame domain, effectively enhancing the robustness of segmentation results. Besides, we construct the first real-world pixel-aligned event-frame semantic segmentation dataset to enable comprehensive training and evaluation, which will be publicly available online. Extensive experiments demonstrate the effectiveness of our method.", "tldr": "event-frame method to tackle all-day semantic segmentation", "keywords": ["semantic segmentation", "multi-modal", "dataset"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/155e9cea91b67d4580e95a5d5b9138a7ee126fa3.pdf", "supplementary_material": "/attachment/8f0727ba7c1a8a5408d40c6751e670299ca2d7e1.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a dual-modality semantic segmentation framework leveraging event and frame data through contrastive alignment and knowledge distillation, along with a new real-world dataset. Their method shows promising results. However, several aspects require clarification, including comparisons with existing fusion methods, latency metrics, and some counter-intuitive experimental results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tThe authors provide a large scale real-world event-frame paired semantic segmentation dataset for driving scenarios RPEA.\n* Authors conduct extensive experiments to compare their scheme with SOTA methods."}, "weaknesses": {"value": "•\tThere exist mechanisms for event-frame fusion so authors need to clarify weaknesses of existing event-frame fusion methods and clearly state how their method works better than existing fusion schemes\n•\tLook at CMDA to see why their latency is 43.3ms while their model’s latency is smaller despite using Mit-B5+SAM+EvLight?\n•\tThe authors include event-frame reconstruction before they extract features for feature alignment – if they include this step in existing models, then may be the performance for existing models will improve as well. Did they do ablation study on this part (can remove this step for their method and evaluate)?\n•\tIn Table 1, why CMDA performance for DSEC-N is better than DSEC-S? why SegFormer performance merely using RGB is better than CMDA on DSEC-S?"}, "questions": {"value": "•\tExplain better the programmable circult – by is it programmable? Do you have to send a trigger signal for capturing every frame??\n•\tIn Line 358, it says event selects the same spatial coordinates as RGB since event has higher temporal resolutions, pixels in both images may not be well aligned – can authors explain better why they make this decision?\n•\tThe efficiency of their model is questionable for they use SAM+EvLight\n•\tShould compare other existing segmentation knowledge distillation methods with their own.\n•\tShould elaborate on how they generate Table 4 –  did they do each expt multiple times (e.g. purposely misalign one pixel along a random direction multiple times) and get the average mIOU scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6Q2lYzyiZN", "forum": "tJgVC9UyeW", "replyto": "tJgVC9UyeW", "signatures": ["ICLR.cc/2026/Conference/Submission695/Reviewer_T5DB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission695/Reviewer_T5DB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761134353260, "cdate": 1761134353260, "tmdate": 1762915583381, "mdate": 1762915583381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses all day video semantic segmentation, indicating there are underexposed, overexposed, or other extreme scenes that often occur in the real world.  The authors propose a new dataset with paired event frames and RGB frames to explore and study the issues. To tackle those issues, the authors propose Heterogeneous Feature Contrastive Alignment to learn distinct feature representations for instances in the same domain and reduce the gap between the instances of two modalities. The authors additionally propose joint boundary-content distillation to distill boundary and content in event frames into RGB frames when motion blur/underexposed/overexposed scenes occur."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors have created a paired and pixel-aligned dataset which is both impactful and valuable for the community to explore and handle more extreme cases in real-world situations for semantic segmentation. \n- The proposed method reasonably utilizes both modalities to tackle the challenges. \n- The experimental results show that the proposed method is effective and the pixel-level alignment in the dataset is crucial in this task."}, "weaknesses": {"value": "- In line 318-321, the authors describe the process of using gradient intensity maps with features of images to get the boundary features.  Can the authors elaborate more on it? How can the boundary features/information be obtained by computing gradient intensity maps? \n\n- In equation 7, did the authors apply it only to specific areas? What would happen if it were applied to all areas? Could this loss negatively influence the performance, especially when the RGB frames have already provided enough information for semantic segmentation? \n\n- The authors did not provide the ablation study for comparing different losses. I wonder how boundary and content losses influence the performance.\n\n- Although the authors showed some qualitative results for different lighting conditions, they are not sufficient to convince reviewers that the proposed method is robust and reliable. It would be better if the authors provided output videos in their supplementary materials, which clearly show the robustness of the method."}, "questions": {"value": "- How can the boundary features/information be obtained by computing gradient intensity maps? \n\n- In equation 7, did the authors apply it only to specific areas? What would happen if it were applied to all areas? Could this loss negatively influence the performance, especially when the RGB frames have already provided enough information for semantic segmentation?\n\n- Could the authors provide ablation study for comparing w/ and w/o boundary and content losses?\n\nFor some details, please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tnu4cDXGYF", "forum": "tJgVC9UyeW", "replyto": "tJgVC9UyeW", "signatures": ["ICLR.cc/2026/Conference/Submission695/Reviewer_ZCEa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission695/Reviewer_ZCEa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753060937, "cdate": 1761753060937, "tmdate": 1762915583225, "mdate": 1762915583225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses semantic segmentation failure under all-weather conditions (overexposure, underexposure, motion blur). The authors propose CACD, a framework fusing event cameras with RGB images. The method contains two modules: HFCA leverages SAM-generated instance masks for cross-modal contrastive alignment in a shared semantic space; JBCD distills boundary and extreme-lighting content information from event data to the RGB branch to enhance robustness and detail. The authors construct the RPEA dataset (real capture, coaxial imaging, pixel-level alignment, day-night coverage) to support training and evaluation. Experiments on RPEA and DSEC-Night show significant gains over SOTA, demonstrating effectiveness and generalization under extreme imaging conditions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Dataset contribution is solid. RPEA provides real capture, coaxial imaging, pixel-level alignment, and day-night coverage, filling a critical gap in all-weather RGB-event segmentation. Ablations show alignment quality substantially impacts performance.\n\nMethod design is well-targeted. HFCA performs cross-modal alignment at instance-level semantic space (using SAM masks) to mitigate heterogeneous gaps. JBCD separately distills boundary and content, effectively transferring event advantages in boundary clarity and extreme lighting to the RGB branch.\n\nExperimental support is sufficient. Results on RPEA and DSEC-Night outperform strong baselines. Clear component ablations and efficiency comparisons show performance sources and overhead trade-offs."}, "weaknesses": {"value": "Heavy reliance on external components obscures innovation boundary. The pipeline heavily depends on (Grounded) SAM for instance masks and EvLight for reconstruction to generate distillation signals. Missing: (i) sensitivity curves showing how mask/reconstruction quality degradation affects final mIoU; (ii) ablations without SAM/EvLight or with weak substitutes (simple segmenters, direct gradient maps); (iii) variance analysis across external model versions. Current results cannot disentangle gains from the proposed framework versus external model capability transfer.\n\nDataset incremental value and analysis are insufficient. RPEA shares coverage of extreme lighting and high-speed motion with existing DDD and DSEC. Incremental novelty lies mainly in pixel-level alignment and day-night coverage, but missing: (i) quantified difficulty spectrum/distribution differences versus existing datasets (exposure/blur stratification, class long-tail, occlusion intensity); (ii) alignment error distribution and its stratified performance impact; (iii) systematic cross-dataset transferability evaluation (RPEA→DSEC/DDD). This limits the persuasiveness of the dataset contribution.\n\nEvaluation and baseline coverage are incomplete, fairness is questionable. Main tables lack unified-protocol comparisons with recent open-vocabulary/large-model segmentation baselines (SAM/SAM2/SAM3, Open-World SAM) on public datasets. True marginal contribution of the event modality is not rigorously verified through \"RGB-only strong baseline + event augmentation\" aligned experiments. Metrics focus on mIoU, lacking Boundary F1/Trimap IoU and robustness stratification (alignment error, exposure level, blur intensity), making it hard to assess boundary quality and degradation resistance.\n\nMethod assumption boundaries are unverified. HFCA's instance-level aggregation and contrastive learning are susceptible to mask noise from small objects, adhesion, and occlusion. JBCD's V-channel thresholds (α, β) are heuristic, lacking adaptive or automatic selection strategies across datasets/time periods. Systematic ablations on noise sensitivity, occlusion scenarios, and threshold transferability are missing.\n\nPerformance ceiling and negative case analysis are insufficient. Results on DSEC-Semantic not reaching SOTA are dismissed as \"annotation noise\" without unified-protocol comparison and error attribution (per-class, boundary bandwidth, hard-case slices). Comparisons with stronger fusion/prior-guided methods (e.g., CLIP-guided OpenESS) under the same setting and failure case analysis are absent. The motivation-method-evidence chain remains weak, falling short of top-tier conference standards for rigor and reproducibility."}, "questions": {"value": "Reduce external dependency, establish attribution and robustness. Add ablations without/with weak SAM and EvLight. Provide quality degradation vs. performance curves (mIoU, Boundary F1). Report mean±variance across different versions (SAM/SAM2/SAM3, reconstructors) to clarify net gains from the framework rather than external models.\n\nComplete strong baselines and fair evaluation. Under unified train/test protocols, add RGB-only strong baseline + event augmentation. \n\nValidate method assumption transferability. Assess HFCA sensitivity to small objects, occlusion, mask noise. Convert JBCD's V-channel thresholds to adaptive or learned gating. Conduct cross-dataset/time-period transfer experiments.\n\nSubstantiate dataset incremental value. Provide difference profiles versus DDD/DSEC (class long-tail, exposure/blur distribution, alignment error histogram). Report zero/few-shot transfer from RPEA→DSEC/DDD. Extend misalignment ablations to per-class/boundary-bandwidth stratification to strengthen the true gains from coaxial alignment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dSIjnecCqK", "forum": "tJgVC9UyeW", "replyto": "tJgVC9UyeW", "signatures": ["ICLR.cc/2026/Conference/Submission695/Reviewer_HUsp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission695/Reviewer_HUsp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809937386, "cdate": 1761809937386, "tmdate": 1762915582994, "mdate": 1762915582994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses semantic segmentation failure under all-weather conditions (overexposure, underexposure, motion blur). The authors propose CACD, a framework fusing event cameras with RGB images. The method contains two modules: HFCA leverages SAM-generated instance masks for cross-modal contrastive alignment in a shared semantic space; JBCD distills boundary and extreme-lighting content information from event data to the RGB branch to enhance robustness and detail. The authors construct the RPEA dataset (real capture, coaxial imaging, pixel-level alignment, day-night coverage) to support training and evaluation. Experiments on RPEA and DSEC-Night show significant gains over SOTA, demonstrating effectiveness and generalization under extreme imaging conditions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Dataset contribution is solid. RPEA provides real capture, coaxial imaging, pixel-level alignment, and day-night coverage, filling a critical gap in all-weather RGB-event segmentation. Ablations show alignment quality substantially impacts performance.\n\nMethod design is well-targeted. HFCA performs cross-modal alignment at instance-level semantic space (using SAM masks) to mitigate heterogeneous gaps. JBCD separately distills boundary and content, effectively transferring event advantages in boundary clarity and extreme lighting to the RGB branch.\n\nExperimental support is sufficient. Results on RPEA and DSEC-Night outperform strong baselines. Clear component ablations and efficiency comparisons show performance sources and overhead trade-offs."}, "weaknesses": {"value": "Heavy reliance on external components obscures innovation boundary. The pipeline heavily depends on (Grounded) SAM for instance masks and EvLight for reconstruction to generate distillation signals. Missing: (i) sensitivity curves showing how mask/reconstruction quality degradation affects final mIoU; (ii) ablations without SAM/EvLight or with weak substitutes (simple segmenters, direct gradient maps); (iii) variance analysis across external model versions. Current results cannot disentangle gains from the proposed framework versus external model capability transfer.\n\nDataset incremental value and analysis are insufficient. RPEA shares coverage of extreme lighting and high-speed motion with existing DDD and DSEC. Incremental novelty lies mainly in pixel-level alignment and day-night coverage, but missing: (i) quantified difficulty spectrum/distribution differences versus existing datasets (exposure/blur stratification, class long-tail, occlusion intensity); (ii) alignment error distribution and its stratified performance impact; (iii) systematic cross-dataset transferability evaluation (RPEA→DSEC/DDD). This limits the persuasiveness of the dataset contribution.\n\nEvaluation and baseline coverage are incomplete, fairness is questionable. Main tables lack unified-protocol comparisons with recent open-vocabulary/large-model segmentation baselines (SAM/SAM2/SAM3, Open-World SAM) on public datasets. True marginal contribution of the event modality is not rigorously verified through \"RGB-only strong baseline + event augmentation\" aligned experiments. Metrics focus on mIoU, lacking Boundary F1/Trimap IoU and robustness stratification (alignment error, exposure level, blur intensity), making it hard to assess boundary quality and degradation resistance.\n\nMethod assumption boundaries are unverified. HFCA's instance-level aggregation and contrastive learning are susceptible to mask noise from small objects, adhesion, and occlusion. JBCD's V-channel thresholds (α, β) are heuristic, lacking adaptive or automatic selection strategies across datasets/time periods. Systematic ablations on noise sensitivity, occlusion scenarios, and threshold transferability are missing.\n\nPerformance ceiling and negative case analysis are insufficient. Results on DSEC-Semantic not reaching SOTA are dismissed as \"annotation noise\" without unified-protocol comparison and error attribution (per-class, boundary bandwidth, hard-case slices). Comparisons with stronger fusion/prior-guided methods (e.g., CLIP-guided OpenESS) under the same setting and failure case analysis are absent. The motivation-method-evidence chain remains weak, falling short of top-tier conference standards for rigor and reproducibility."}, "questions": {"value": "Reduce external dependency, establish attribution and robustness. Add ablations without/with weak SAM and EvLight. Provide quality degradation vs. performance curves (mIoU, Boundary F1). Report mean±variance across different versions (SAM/SAM2/SAM3, reconstructors) to clarify net gains from the framework rather than external models.\n\nComplete strong baselines and fair evaluation. Under unified train/test protocols, add RGB-only strong baseline + event augmentation. \n\nValidate method assumption transferability. Assess HFCA sensitivity to small objects, occlusion, mask noise. Convert JBCD's V-channel thresholds to adaptive or learned gating. Conduct cross-dataset/time-period transfer experiments.\n\nSubstantiate dataset incremental value. Provide difference profiles versus DDD/DSEC (class long-tail, exposure/blur distribution, alignment error histogram). Report zero/few-shot transfer from RPEA→DSEC/DDD. Extend misalignment ablations to per-class/boundary-bandwidth stratification to strengthen the true gains from coaxial alignment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dSIjnecCqK", "forum": "tJgVC9UyeW", "replyto": "tJgVC9UyeW", "signatures": ["ICLR.cc/2026/Conference/Submission695/Reviewer_HUsp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission695/Reviewer_HUsp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809937386, "cdate": 1761809937386, "tmdate": 1763694165346, "mdate": 1763694165346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenging problem of all-day semantic segmentation, where traditional frame-based cameras struggle under extreme lighting (over/under-exposure) and motion blur. The authors propose a two-pronged contribution: (1) RPEA, a new real-world pixel-aligned event-frame semantic segmentation dataset with all-day scenarios, and (2) CACD, a novel framework that first aligns heterogeneous event and frame features in a common semantic space using a contrastive learning module (HFCA), and then distills boundary and content knowledge from the event modality to the frame modality via a joint distillation module (JBCD). Extensive experiments show that CACD outperforms existing state-of-the-art methods on multiple datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The construction of the RPEA dataset is a significant contribution. It is the first real-world pixel-aligned dataset for event-frame semantic segmentation, filling a critical gap in the field and enabling more reliable training and evaluation. The coaxial optical system for hardware-level alignment is well-motivated.\n2. The core idea of first aligning representations before distillation is sound and addresses a genuine challenge in multi-modal fusion—the heterogeneity between event and frame data. The separation of concerns into boundary and content distillation is also intuitive.\n3. The paper includes extensive experiments on multiple datasets (RPEA, Cityscapes, DSEC), ablation studies, and efficiency comparisons, which provide strong empirical support for the effectiveness of the proposed CACD framework."}, "weaknesses": {"value": "1. Despite being a core contribution, the paper lacks critical details about the RPEA dataset curation pipeline. Key missing information includes: the data collection process, specific event representation format (e.g., voxel grid parameters), detailed hardware calibration and alignment error metrics, the annotation protocol (e.g., inter-annotator agreement, handling of ambiguous regions), and a more thorough statistical analysis (e.g., instance counts, motion statistics). This hinders reproducibility and trust in the dataset's quality.\n2. The core modules of CACD, contrastive alignment and knowledge distillation, build upon well-established techniques from multimodal learning literature without sufficient adaptation or justification for the event-frame domain. The work lacks comparison to recent contrastive or distillation-based segmentation frameworks, making it difficult to assess whether the performance gain stems from algorithmic insight or simply from using a stronger backbone and high-quality data. \n3. Although the inference pipeline is lightweight, the training process critically relies on GroundedSAM and EvLight—large, external, frozen models. This creates a “black-box” dependency that: (1) makes performance contingent on the quality of SAM masks generated from EvLight-reconstructed images; (2) complicates the training pipeline; and (3) obscures the true source of performance gains. The paper provides no ablation on the sensitivity to these components. \n4. The SOTA comparison lacks uniform re-implementation of recent event-segmentation methods (e.g., ESEG, AAAI’25) on RPEA under identical training protocols. Given that CACD uses a larger MiT-B5 backbone while some baselines use smaller variants (e.g., MiT-B2/B4), the reported gains may be partially attributable to model capacity rather than the proposed design. The claim of outperforming SOTA is thus not fully substantiated. \n5. While the paper emphasizes that JBCD improves performance in “over/under-exposed regions,” this claim is supported only by qualitative visualizations (Fig. 6). There is no quantitative, exposure-stratified evaluation. For instance, mIoU per illumination bin (e.g., splitting test images by mean HSV V-channel intensity into 5 quantiles). Similarly, performance on motion-blurred vs. static regions is not isolated, despite the boundary enhancement claim. \n6. While RPEA is a valuable academic contribution, its real-world applicability is questionable. The requirement of a coaxial beam-splitter setup with synchronized event and frame cameras imposes significant hardware and calibration overhead, which is rarely feasible in production systems (e.g., autonomous vehicles or surveillance). Moreover, recent advances in HDR imaging and temporal modeling may reduce the necessity of event cameras for extreme lighting robustness. The paper should discuss these practical trade-offs and clarify target deployment scenarios where the added complexity is justified. Without such discussion, the practical impact of the work remains unclear."}, "questions": {"value": "My main concerns are outlined in the \"Weaknesses\" part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ELQEnrYWVx", "forum": "tJgVC9UyeW", "replyto": "tJgVC9UyeW", "signatures": ["ICLR.cc/2026/Conference/Submission695/Reviewer_uz38"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission695/Reviewer_uz38"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986745600, "cdate": 1761986745600, "tmdate": 1762915582776, "mdate": 1762915582776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}