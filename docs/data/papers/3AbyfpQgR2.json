{"id": "3AbyfpQgR2", "number": 12760, "cdate": 1758210115590, "mdate": 1759897488869, "content": {"title": "AEMP: Autoregressive-Enhanced Masked Pre-training for Robust Indoor Localization", "abstract": "Indoor localization with WiFi Channel State Information (CSI) requires models that can generalize across diverse deployment conditions, yet collecting large amounts of high-quality labeled data is costly and often impractical. Pre-training offers a promising solution, but conventional masked modeling is not directly suitable for CSI signals. It tends to produce unstable representations in unmasked regions, fails to preserve long-range channel correlations, and remains highly sensitive to variations in access point layouts and propagation environments. To address these issues, we propose an autoregressive-enhanced masked pre-training (AEMP) framework. AEMP employs a hierarchical Transformer architecture where spatial subnetworks perform masked reconstruction to capture local channel features, while a temporal network enforces consistency through autoregressive prediction. In addition, multi-view fusion and span masking improve robustness under dynamic deployment conditions. Extensive experiments demonstrate that AEMP yields stable and transferable representations, achieving superior performance and strong generalization on downstream indoor localization tasks. To the best of our knowledge, this is the first pre-training framework for wireless sensing that integrates temporal prediction to complement masked reconstruction.", "tldr": "", "keywords": ["Indoor localization", "Self-supervised", "Pre-training", "Channel State Information"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/023072aee3387d02305383c60761594ceb44ca29.pdf", "supplementary_material": "/attachment/a396cc246e157fc5151be02d75f853ea5d89eec6.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the issue that traditional masked pre-training methods are unsuitable for WiFi CSI indoor localization signals, pointing out that these methods produce unstable representations and fail to capture global spatio-temporal correlations, making models highly sensitive to real-world deployment changes . To solve this, the paper proposes a novel hybrid framework called AEMP, which utilizes a spatial subnetwork for Masked Reconstruction while using a temporal subnetwork for Autoregressive Prediction. AEMP achieves state-of-the-art localization performance  and demonstrates strong generalization, especially in low-data scenarios. Additionally, the authors contribute a new large-scale real-world dataset called ISACLoc for this research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses the issue that traditional masked pre-training methods are unsuitable for WiFi CSI indoor localization signals, pointing out that these methods produce unstable representations and fail to capture global spatio-temporal correlations, making models highly sensitive to real-world deployment changes . To solve this, the paper proposes a novel hybrid framework called AEMP, which utilizes a spatial subnetwork for Masked Reconstruction while using a temporal subnetwork for Autoregressive Prediction  . AEMP achieves state-of-the-art localization performance and demonstrates strong generalization, especially in low-data scenarios. Additionally, the authors contribute a new large-scale real-world dataset called ISACLoc for this research."}, "weaknesses": {"value": "“We employ a multi-view fusion strategy to reduce the reliance on specific AP combinations. In addition, we introduce a span masking mechanism (Joshi et al., 2020) to simulate dynamic deployment conditions in real-world scenarios.”\nI dont see the ablation study  on these two.\nBesides, I'm not sure the novelty of the MR and AP combination."}, "questions": {"value": "Is the MR+AP combination really that useful? In other words, in your specific scenario, I feel the distinction between the two is not that significant. Also, I suggest experimenting on one or two public datasets to demonstrate the generalization performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2RvTTsDmBD", "forum": "3AbyfpQgR2", "replyto": "3AbyfpQgR2", "signatures": ["ICLR.cc/2026/Conference/Submission12760/Reviewer_3fbY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12760/Reviewer_3fbY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808643208, "cdate": 1761808643208, "tmdate": 1762923573838, "mdate": 1762923573838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a self-supervised pre-training mechanism for CSI-based data-driven indoor localization systems that aims to capture the spatiotemporal dependencies of wireless signals, providing robust representations that enhance the robustness and generalization capabilities of downstream indoor localization systems. The Spatial component is learned by training the model to perform 2D mask reconstruction as well as the model's confidence in the mask prediction, represented by the predicted variance, while temporal consistency is taught to the model by auto-regressively predicting the future frames. Moreover, the authors collect a large-scale dataset of CSI measurements from a large number of APs coming from a diverse set of mobile phones, and from different geographical locations over several days. Several studies and evaluations are performed to quantify the impact of the two proposed sub-components, as well as some experiments to quantify the proposed approach's capability to make the downstream systems robust to varying deployment settings. Moreover, quantitative analysis shows that the proposed approach outperforms other pretraining methods in terms of median and tail localization error."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed work maintains both spatial and temporal properties of wireless signal propagation\n- The proposed methodology is well motivated and decently evaluated."}, "weaknesses": {"value": "- The work does not fully evaluate the impact of the different proposed components on the mentioned challenges. For example, it is not clear the contribution of the Masked Reconstruction (MR) on generalizing to new areas or across different devices\n- The work is only evaluated on the constructed dataset, and it is not clear how well the performance gains from the proposed pre-training methods carry over to other well-established datasets.\n\nOverall, this proposed work can be a significant contribution in enhancing indoor localization systems, but would benefit from some additional clarifications and experiments. Based on these clarifications and experiments from the authors, I would be willing to revise this score.\n\nClarifying the following points in writing or by performing minor additional experiments would help quantify the effects and impacts of several design choices:\n- Which part of the dataset was used as a validation split?\n- What is the impact of multi-view fusion in the MR component on the generalization capability of downstream models? What is the impact of an increasing number of combinations by dropping more than 1 AP?\n- I think the fact that almost all the baselines are underperforming compared to systems without pretraining is counterintuitive and warrants discussing and possibly evaluating.\n- Since the position of the APs are encoded in the input, can a model trained on one test area be directly evaluated on another? I believe including the performance without additional fine-tuning in Table 4 would be a good addition.\n- It would be interesting to see how reproducible the results are on other existing and well-established datasets, and to show that the proposed approach is not over-fitting on some nuanced features of the environments where the data was collected. This would help strengthen the proposed approach, as well as validate the proposed dataset.\n- What is the structure of the MLP used as a task-specific head?\n\n\nMinor comments:\n- In lines 216 and 217, the N-1 and N should be swapped."}, "questions": {"value": "Please focus on the weakenesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z5iBpfCbyD", "forum": "3AbyfpQgR2", "replyto": "3AbyfpQgR2", "signatures": ["ICLR.cc/2026/Conference/Submission12760/Reviewer_d1Ze"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12760/Reviewer_d1Ze"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896079193, "cdate": 1761896079193, "tmdate": 1762923573530, "mdate": 1762923573530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AEMP (Autoregressive-Enhanced Masked Pre-training), a self-supervised framework designed to improve robustness and generalization in indoor localization using WiFi Channel State Information (CSI). Traditional masked modeling fails to capture the spatiotemporal dependencies of CSI signals and is sensitive to environmental variations. To address these issues, AEMP integrates a hierarchical Transformer architecture with two complementary pretext tasks: masked reconstruction for learning spatial features and autoregressive prediction for temporal consistency. The authors also introduce span masking and multi-view fusion strategies to enhance resilience to deployment changes. Extensive experiments on the proposed ISACLoc dataset show that AEMP achieves superior localization accuracy and generalization compared with state-of-the-art baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The first pre-training framework for wireless sensing that combines masked modeling with autoregressive prediction to improve temporal coherence in CSI representation learning.\n\n(2) A well-designed hierarchical Transformer architecture that separates spatial and temporal modeling via parameter-shared subnetworks.\n\n(3) Introduction of span masking and multi-view fusion to simulate real-world dynamics and improve robustness to varying access point configurations.\n\n(4) Comprehensive experiments, including cross-region, cross-device, and low-label scenarios, showing consistent improvements in accuracy and stability. Clear ablation and fine-tuning analyses demonstrating the role of each module in performance gains."}, "weaknesses": {"value": "(1) The abstract does not effectively highlight the research gap or the motivation. Readers must infer the challenges being addressed from the introduction.\n\n(2) Figure 2 is difficult to interpret due to small and cluttered text; visualization clarity is limited.\n\n(3) While the gap is well-defined, the proposed solution lacks strong novelty, primarily integrating known ideas (masked modeling and autoregression) with moderate architectural adjustments.\n\n(4) Although the method section is detailed, the flow is dense, and the conceptual link between the two tasks (masked reconstruction and autoregression) could be better articulated. \n\n(5) while thorough, results mostly compare against conventional pretraining baselines. Broader comparisons (e.g., contrastive or diffusion-based pretraining methods) would better support novelty claims.\n\n(6) The ISACLoc dataset setup is interesting, but reproducibility could be improved by including open-source plans or quantitative data statistics."}, "questions": {"value": "(1) The combination of Gaussian NLL loss for masking and weighted MSE for prediction is sensible, but how is the stability of training? \n\n(2) How sensitive is AEMP to the weighting and scheduling parameters (e.g., λ and η) in Equation (9)? Were these tuned heuristically or via validation?\n\n(3) Since span masking and multi-view fusion both modify spatial inputs, how do they interact? Could one suffice without the other?\n\n(4) How scalable is AEMP to different wireless protocols or hardware with different CSI formats—does the pretraining transfer effectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FAwu3aO26C", "forum": "3AbyfpQgR2", "replyto": "3AbyfpQgR2", "signatures": ["ICLR.cc/2026/Conference/Submission12760/Reviewer_H4cu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12760/Reviewer_H4cu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762217496972, "cdate": 1762217496972, "tmdate": 1762923573066, "mdate": 1762923573066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}