{"id": "rsEhpeY6HE", "number": 18797, "cdate": 1758290956905, "mdate": 1759897080942, "content": {"title": "From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs", "abstract": "Large Vision-Language Models (LVLMs) have achieved remarkable success across a wide range of multimodal tasks, yet their robustness to spatial variations remains insufficiently understood. In this work, we present a systematic study of the spatial bias of LVLMs, focusing on how models respond when identical key visual information is placed at different locations within an image. Through a carefully designed probing dataset, we demonstrate that current LVLMs often produce inconsistent outputs under such spatial shifts, revealing a fundamental limitation in their spatial-semantic understanding. Further analysis shows that this phenomenon originates not from the vision encoder, which reliably perceives and interprets visual content across positions, but from the unbalanced design of position embeddings in the language model component. In particular, the widely adopted position embedding strategies, such as RoPE, introduce imbalance during cross-modal interaction, leading image tokens at different positions to exert unequal influence on semantic understanding. To mitigate this issue, we introduce **Balanced Position Assignment (BaPA)**, a simple yet effective mechanism that assigns identical position embeddings to all image tokens, promoting a more balanced integration of visual information. Extensive experiments show that BaPA enhances the spatial robustness of LVLMs without retraining and further boosts their performance across diverse multimodal benchmarks when combined with lightweight fine-tuning. Further analysis of information flow reveals that BaPA yields balanced attention, enabling more holistic visual understanding.", "tldr": "", "keywords": ["Large Vision-Language Models", "Spatial Bias", "Position Embedding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72ff01bfabaad5a3a9b5e4f9d316968a2c75014c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates the spatial bias of Large Vision-Language Models (LVLMs), showing that models often produce inconsistent predictions when identical visual content appears in different locations. The authors attribute this bias to imbalanced positional embeddings in the LLM component and propose Balanced Position Assignment (BaPA), which assigns identical positional IDs to all image tokens. Applied during inference without retraining, BaPA improves spatial robustness and slightly enhances downstream performance across several LVLMs and multimodal benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper follows a commendable research style of identifying a specific problem, systematically probing its underlying causes, and designing a targeted solution. The overall organization—from constructing a probing dataset, analyzing the vision and language components separately, to proposing a mitigation strategy—is clear and methodical. The inclusion of multiple LVLM backbones (e.g., Qwen2.5-VL, LLaVA-v1.6, Gemma3) makes the study comprehensive."}, "weaknesses": {"value": "1. Some of the conclusions are not rigorously supported by the provided probing experiments. For instance, Qwen2.5-VL-7B already exhibits relatively stable spatial robustness on the proposed probe task, and applying BaPA to it even slightly worsens its performance. Moreover, the claim that “the vision encoder understanding is spatially robust” is not directly validated with the proposed task, despite being plausible. The conclusion that “the root of spatial bias lies in the imbalance of positional embeddings within the LLM” also appears oversimplified, as other contributing factors, such as the attention-sink effect, are not examined.\n\n2. The proposed positional encoding modification, Balanced Position Assignment (BaPA), is conceptually straightforward and not novel—it has already been used or discussed in prior works such as CogVLM, which similarly assigns a shared positional ID for all image tokens. In addition, the paper lacks comparisons with other recent RoPE variants designed for LVLMs, including V2PE, MRoPE, and CircleRoPE, making it difficult to assess the relative merit of BaPA.\n\n3. The proposed positional encoding scheme may also be suboptimal given the design of current top-performing LVLMs. Modern architectures such as Qwen2.5/3-VL, KiMi-VL, and LLaVA-OV-1.5 adopt resolution-flexible schemes where 2D RoPE is applied in the vision encoder with minimal positional information propagated to the language component. Since these models benefit from letting the LLM also perceive coarse spatial structures, simply removing positional differentiation among image tokens—as BaPA does—might not align with this evolving design trend.\n\n4. The experimental evaluation should be broader. The paper mainly includes general multimodal benchmarks like ScienceQA, MME, and MMMU-Pro, but misses fine-grained grounding and structural visual understanding benchmarks such as ChartVQA and DocVQA, which are more sensitive to positional or structural reasoning. Including such benchmarks would provide a fuller validation of spatial robustness improvements."}, "questions": {"value": "Why is the reported score of Qwen2.5-VL on ScienceQA-Img only 0.78? My evaluation using lmms-eval is 0.87."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1fLTJYVA2R", "forum": "rsEhpeY6HE", "replyto": "rsEhpeY6HE", "signatures": ["ICLR.cc/2026/Conference/Submission18797/Reviewer_MZF9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18797/Reviewer_MZF9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726369105, "cdate": 1761726369105, "tmdate": 1762999982835, "mdate": 1762999982835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focus on benchmarking and mitigating of spatial bias. For benchmarking, they design a **probing** dataset, and find that current LVLMs is sensitive to spatial location shifts of objects. To this end, they propose Balanced Position Assignment (BaPA), a method that applies identical position embeddings to all image tokens. Such approach enhances the spatial robustness of LVLMs without retraining and boosts their performance on multiple multimodal benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Topic. The study of spatial robustness is interesting.\n+ Presentation. The presentation and core idea is overall easy to follow.\n+ Experiment. Experiments are sufficient to support claims."}, "weaknesses": {"value": "+ Novelty. The novelty concerns are from two aspects, including, \n    - Benchmarking. CCA [Xing et al. 2024] already proposed a spatial probing data for evaluating spatial robustness, which authors ignored to mention.\n    - Approach. It should be pointed out that assigning the same positions to visual tokens is not a new approach, while this paper ignores to discuss. [A]\n\n+ Relationship between better spatial robustness and LVLM benchmark accuracy. Authors are suggested to include more deeper insights, especially how better spatial robustness benefits existing LVLMs, from what aspects LVLMs are benefited. For example, spatial relationship, referential, in what cases such correlations are strong, and in what cases such correlations are weak? \n\n[A] Vista-LLaMA: Reducing Hallucination in Video Language Models via Equal Distance to Visual Tokens. CVPR 2024."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZNF61dzzLD", "forum": "rsEhpeY6HE", "replyto": "rsEhpeY6HE", "signatures": ["ICLR.cc/2026/Conference/Submission18797/Reviewer_2vXx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18797/Reviewer_2vXx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796336393, "cdate": 1761796336393, "tmdate": 1762999980430, "mdate": 1762999980430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to reduce the spatial bias in VLMs by introducing same position embedding values for image tokens. The authors analyses the phenomena of position bias in VLM semantic understanding by varying position of key visual information in an image, revealing that VLM  performances change as visual information shifts to different positions. Based on these findings, the authors offer a different view contrary to previous researchers that spatial bias is not a result of long-term dependency of RoPE. Following this finding, the authors proposed a position encoding modification strategy that sets the position embedding to be the same for all visual tokens. Experimental results on multiple image QA benchmarks show improved performance after applying BaPA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, the manuscript is clearly written. The issue of VLM spatial bias is clearly presented through a series of extensive analysis experiments, showcasing the effect of changing key visual information positions on model performance using multiple baseline models. The visualisations provided in figures 1,2 and 3 also offer a clear view of the spatial bias phenomena which greatly assists reader in understanding the concept."}, "weaknesses": {"value": "This work challenges previous findings that VLM positional bias may not arise due to the long-term decay effect of RoPE. The authors further backup this claim with experiments, showing that model results are not directly correlated with distance as key visual feature shifts position. However, the authors do not specify the token length for this probing experiment. The claim can be further validated if the same experimental results are observed on high resolution data, where the input tokens are longer and long-term decay would be potentially more significant. \n\nThough the manuscript is clearly written and easy to follow, there appear to be typos in important result sections of the paper (see questions for more detail), which may confuse the reader and undermine credibility of this work."}, "questions": {"value": "1. In section 4.1 the authors derive the conclusion that \"vision encoder’s perception is robust to spatial variation\"(line 241). However, the experimental result presented in figure 2 is obtained from \"logits of generated token\"(line 233) for each LVLM. How does this result support the claim if it is under the influence of both vision encoder and attention layers of VLM?\n2. How does BaPA affect temporal dependency on video understanding tasks when applied to video models such as Qwen 2.5 VL? \n3. In table 1 under line 330, the average result for Gamma3 is reported as 0.82. Is it a typo?\n4. In table 1 line 333-334, LLaVA-v1.6-BaPA is listed twice but with different results. Is it also a typo? \n5. In table 1, the result of LLaVA-NeXT is significantly lower compared to other models and the LLaVA-v1.6 variant. However, in some conventions, they are the different names for the same model. Could you please specify the exact model used to avoid confusion? Additional evidence such as evaluation logs would also be welcomed to increase the credibility of this result. \n6. The authors evaluated BaPA on general image QA benchmarks such as MMMU-Pro and ScienceQA. How does BaPA perform on hallucination benchmarks such as POPE and CHAIR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yhrLo62O80", "forum": "rsEhpeY6HE", "replyto": "rsEhpeY6HE", "signatures": ["ICLR.cc/2026/Conference/Submission18797/Reviewer_p9cf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18797/Reviewer_p9cf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884333733, "cdate": 1761884333733, "tmdate": 1762999980348, "mdate": 1762999980348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates spatial bias in LVLMs—prediction inconsistency when key visual evidence is moved across image regions.  The authors trace the root cause to RoPE-induced relative-position imbalance in the LLM rather than the vision encoder.  They propose Balanced Position Assignment (BaPA), which assigns the same position id to all image tokens while preserving sequence continuity, thereby equalizing their influence on text tokens; BaPA is applied at inference without retraining."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear causal diagnosis with a principled fix. The paper pinpoints RoPE-induced relative-position imbalance in the LLM, which is unsuited for cross-modal interaction. They proposes BaPA, assigning an identical position id to all image tokens while preserving sequence continuity; this is motivated by the vision encoder already modeling spatial structure. \n\n- Rigorous and convincing evidence. A 90k-sample probe quantifies spatial robustness; BaPA consistently boosts accuracy across multiple LVLMs and sharply reduces variance."}, "weaknesses": {"value": "- Limited novelty. The core technique (assigning one shared position id to all image tokens) was already used in CogVLM (arXiv 2023; NeurIPS 2024), which explicitly states that all visual tokens “share a single position id” under RoPE. MammothModa (2024) further applies the same principle at the frame level (“Shared Frame Position ID”) for video inputs.\n\n- Inconsistent effects on strong base models (Qwen2.5-VL). On the probe dataset (Table 1), BaPA yields negligible or negative changes for Qwen: 7B Avg 82.49 → 81.28 (variance 1.74 → 2.06), 32B 81.96 → 82.48 (variance 0.28 → 0.80). Yet on downstream tasks (Table 2) results are mixed: ScienceQA rises 0.7898 → 0.8909 while HallusionBench drops 0.7066 → 0.6909; MMMU-Pro shows small gains. This divergence questions how reliably BaPA translates probe-level “debiasing” into broad task improvements.\n\n1. CogVLM: Visual Expert for Pretrained Language Models，2311.03079\n2. MammothModa: Multi-Modal Large Language Model， 2406.18193"}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qx4CJcXfln", "forum": "rsEhpeY6HE", "replyto": "rsEhpeY6HE", "signatures": ["ICLR.cc/2026/Conference/Submission18797/Reviewer_UrNU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18797/Reviewer_UrNU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903308415, "cdate": 1761903308415, "tmdate": 1762999981210, "mdate": 1762999981210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}