{"id": "0sXt3wT22y", "number": 741, "cdate": 1756786141976, "mdate": 1759898244371, "content": {"title": "DNON: A Brain-Inspired Architecture for Multi-Domain Reasoning with Specialized Neural Modules", "abstract": "Dynamic Neural Orchestration Networks (DNON) is introduced as a brain-inspired architecture that composes specialized language models via information-theoretic routing. DNON comprises four modules — Perception, Memory (short-term, long-term, deep subconscious), Reasoning, and Executive — whose interactions are dynamically regulated by mutual-information-based routing on information manifolds. The framework provides a principled path for modular cognition and offers Lyapunov-style convergence guarantees under reasonable assumptions. The implementation leverages frozen foundation models (Claude for closed-source experiments and Mistral Large for open-source deployments) while training the routing mechanisms, information-geometric parameters, and memory dynamics through gradient-based optimization of mutual information objectives. Empirically, DNON is evaluated across four reasoning benchmarks using stratified 5-fold validation (seed 50) and demonstrates strong accuracy while reducing end-to-end inference cost compared to baselines and retrieval-augmented methods. Ablation studies confirm the importance of the three-tier memory: removing STM, LTM, or DSM degrades performance substantially. DNON thus combines theoretical rigor and practical gains for multi-domain reasoning with modular LLMs.", "tldr": "DNON orchestrates specialized language models through information-theoretic routing on Riemannian manifolds, providing both theoretical convergence guarantees and improved performance on reasoning benchmarks.", "keywords": ["Neural orchestration", "Information geometry", "Brain-inspired architecture", "Multi-tier memory", "Lyapunov stability", "Modular language models"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ca52f8638c5a5fa87f3380af29ab243c4713043.pdf", "supplementary_material": "/attachment/84c3dc4798100c28f71deebdff01301bae60d02f.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Dynamic Neural Orchestration Networks (DNON), a modular architecture inspired by cognitive neuroscience. The system consists of four specialized modules Perception, Memory, Reasoning, and Executive and employs a three-tier memory mechanism combined with information-theoretic dynamic routing. The authors evaluate the model on reasoning-oriented benchmarks such as StrategyQA and MultiArith, claiming improvements through adaptive routing and convergence analysis."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper proposes a modular architecture with explicit separation between perception, memory, reasoning, and executive control, which is interesting."}, "weaknesses": {"value": "1. Clarity and documentation issues regarding LLM usage and references.\nAccording to the ICLR 2026 Author Guide (https://iclr.cc/Conferences/2026/AuthorGuide), all submissions that make significant use of Large Language Models (LLMs) are expected to include a dedicated section titled “The Use of Large Language Models (LLMs)” describing how such models were used. There are factual and bibliographic inconsistencies: the citations “Shazeer et al., 2017a” and “Shazeer et al., 2017b” refer to the same paper (Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer), and the model “Mistral Pixtral Large (25.02)” does not appear to exist publicly. These should be corrected or properly referenced for accuracy.\n\n2. The scientific contribution of the paper remains ambiguous.\nAfter reviewing the implementation provided in the Supplementary Material, the core algorithm appears to perform rule-based routing and prompt orchestration, rather than genuine representational learning or trainable model composition.\nWhile the paper describes the system as “brain-inspired,” the implementation relies on hand-coded logic and heuristic mutual-information estimates, rather than biologically motivated or learnable mechanisms. The gap between the description and the implemented system should be clarified.\n\n3. Benchmarks such as MultiArith are trivially solvable by modern LLMs and insufficient to convincingly demonstrate reasoning capability or the effectiveness of the proposed dynamic routing mechanism.\nOverall, the chosen evaluation tasks are too simple.\nFor instance, reasoning ability would be better demonstrated on more challenging benchmarks such as AIME25, HLE.\nThe current datasets SQuAD, StrategyQA, and HotpotQA are insufficient to validate deeper reasoning capability.\n\n4. Presentation quality issues. Figures (especially Figure 2–4) are too small and difficult to read."}, "questions": {"value": "See Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7dD5zIpJF9", "forum": "0sXt3wT22y", "replyto": "0sXt3wT22y", "signatures": ["ICLR.cc/2026/Conference/Submission741/Reviewer_QsCt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission741/Reviewer_QsCt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400510029, "cdate": 1761400510029, "tmdate": 1762915594794, "mdate": 1762915594794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a brain-inspired modular architecture (DNON) that integrates multiple specialized neural modules under an information-theoretic routing mechanism. DNON uses mutual-information-based routing on information manifolds to dynamically allocate computation across modules, aiming to emulate biological specialization and cognitive flexibility. The architecture is evaluated on four reasoning benchmarks, showing performance improvements over prompting and retrieval-based baselines, but at the cost of higher inference time."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper proposed an information-theoretic routing mechanism for dynamically orchestrating specialized LLM modules, bridging ideas from information geometry, neuroscience, and AI.\n2. The paper is well-grounded in a broad and rich set of references, spanning neuroscience, cognitive psychology, and modern deep learning."}, "weaknesses": {"value": "1. The major weakness of this paper is that the main text provides almost no detailed explanation of the DNON architecture or its implementation. Section 3 is highly abstract and lacks concrete methodological descriptions. Figure 1 is barely explained, and readers cannot understand how the four modules: Perception, Reasoning, Executive, and Memory, are instantiated or integrated within both the open-source and closed-source model settings.\n\n2. In lines 207–208, the authors state that ``Complete prompt templates, key implementation components, and high-resolution figures are provided in the supplementary material to facilitate reproducibility of the core results reported in this work''  However, no specific appendix sections are cited, and this claim appears inaccurate, as the supplementary material does not include any details regarding prompt templates.\n\n3. The experimental descriptions are somewhat arbitrary and incomplete. The paper does not specify the exact model versions or provide appropriate citations for the underlying foundation models ( `Claude` and `Mistral Large` ) used in the experiments.\n\n4. The authors claim that the architecture can improve interpretability, yet there is no explanation or evidence showing how the structure actually enhances interpretability in practice."}, "questions": {"value": "The claim that DNON enhances interpretability is compelling. Could the authors provide visualizations of routing dynamics or module activations over time to substantiate this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UNMofLK8Yq", "forum": "0sXt3wT22y", "replyto": "0sXt3wT22y", "signatures": ["ICLR.cc/2026/Conference/Submission741/Reviewer_6Sun"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission741/Reviewer_6Sun"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819918534, "cdate": 1761819918534, "tmdate": 1762915594671, "mdate": 1762915594671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DNON (Differentiable Neural Organization Network), a brain-inspired modular architecture that composes a frozen foundation LLM with a set of specialized neural modules (Perception, Memory, Reasoning, Executive) and a differentiable router trained to maximize conditional mutual information for routing. A three-tier memory system (STM/LTM/DSM) supports short-term working memory, consolidates long-term knowledge, and provides a “deep subconscious” store with affective and procedural cues. Training alternates between routing/memory dynamics and module specialization while keeping the base LLM fixed. Experiments on MultiArith, SQuAD, StrategyQA, and HotpotQA show gains vs. prompting/RAG baselines, and the authors provide proofs of convergence for the dynamical system under stated conditions"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality: MI-based differentiable routing + explicit three-tier memory goes beyond standard MoE/RAG hybrids; framing as a dynamical system with a Lyapunov argument is a nice touch.\n\nClarity: Appendix including implementation details, hyperparameters, and algorithmic components, isolating the routing/memory’s effect."}, "weaknesses": {"value": "Construct validity of DSM/emotion: DSM includes an “emotional salience” term with a learned vector; conceptual grounding and ablations are thin. Provide targeted stress tests where affect/procedural memory should (or should not) help, and show failure cases when ϵ is randomized or zeroed.\n\nEfficiency: The paper acknowledges that DNON is slower than RAG for several settings (and much slower on SQuAD). Include budgeted settings (fixed latency/compute) to show when DNON still wins—or propose hybrid routing that falls back to RAG when MI signals are low.\n\nPoor readability: the text of the chart is too small, making details difficult to see. Also, the main text reports Claude's results while relegating Mistral to the appendix, obscuring cross-backbone consistency."}, "questions": {"value": "When to prefer RAG? Given RAG’s speed advantage (notably on SQuAD), can the router pre-screen tasks and route to RAG for extraction-dominant queries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5KSvmAte31", "forum": "0sXt3wT22y", "replyto": "0sXt3wT22y", "signatures": ["ICLR.cc/2026/Conference/Submission741/Reviewer_MnnS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission741/Reviewer_MnnS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066530269, "cdate": 1762066530269, "tmdate": 1762915594436, "mdate": 1762915594436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dynamic Neural Orchestration Networks (DNON), a brain-inspired framework that composes specialized language models through information-theoretic routing. The system integrates four cognitive-style modules, Perception, Memory (short-term, long-term, deep subconscious), Reasoning, and Executive, regulated dynamically based on mutual-information-guided routing on information manifolds. Empirical evaluation spans four reasoning benchmarks with stratified 5-fold validation, showing higher accuracy and reduced inference cost relative to retrieval-augmented and monolithic baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a well-motivated approach to modular cognition in large language models, grounded in information theory. The analogy to brain modularity and dynamic orchestration is compelling.\n2. The use of mutual information as a routing and training criterion is appealing.\n3. The experiments show promise in both reasoning accuracy and computational efficiency. \n4. The inclusion of detailed ablation on the STM/LTM/DSM memory components strengthens causal claims about DNON’s design choices. \n5. The combination of frozen LLMs with learnable information-geometric routing could make this architecture potentially practically attractive for low-cost, multi-domain reasoning."}, "weaknesses": {"value": "1. While the paper references Lyapunov-style guarantees, the exact assumptions (on manifold smoothness, bounded divergence, etc.) and proofs are only briefly outlined. A more rigorous derivation in an appendix would increase confidence. The word ‘optimal’ is often used in these Theorem statements without any formal definition of it. The theoretical analysis are added without any meaningful integration in the main text. \n2. It is not fully clear how the mutual-information-based routing manifests in practice. Visualization or qualitative analysis of routing decisions would be valuable.\n3. Since the core foundation models are not trained jointly, it remains ambiguous how much of the observed improvement stems from routing versus intrinsic model strength.\n4. Baselines include retrieval-augmented and monolithic systems, but not recent modular architectures such as Liquid LLMs, Mixture-of-Experts routing, or Self-Discovering Agents. This omission weakens the empirical positioning.\n5. The names “deep subconscious memory” and “information manifolds” could benefit from more precise definitions to improve reproducibility.\n6. Although DNON is framed as “brain-inspired,” the paper does not situate it within the broader lineage of cognitive architectures such as ACT-R, SOAR, or Global Workspace Theory. Without this context, it is difficult to assess how DNON extends, diverges from, or formalizes existing models of modular cognition. A brief comparative discussion would significantly strengthen the conceptual grounding."}, "questions": {"value": "1. Can the authors provide all the assumptions needed in the theoretical derivations and formally define all the technical terms like optimality? The proves currently are meaningless without appropriate definitions and assumptions.\n2. How sensitive is DNON to the choice of information-geometric metric (e.g., Fisher-Rao vs. α-divergence)?\n3. What is the computational overhead of computing MI-based routing in real time?\n4. Have the authors tested whether DNON generalizes beyond language reasoning (e.g., vision-language tasks)?\n5. How is this different or like the cognitive architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wQXhU9Fc3l", "forum": "0sXt3wT22y", "replyto": "0sXt3wT22y", "signatures": ["ICLR.cc/2026/Conference/Submission741/Reviewer_XKKu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission741/Reviewer_XKKu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762221550189, "cdate": 1762221550189, "tmdate": 1762915594295, "mdate": 1762915594295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}