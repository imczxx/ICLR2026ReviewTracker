{"id": "Biz1vpQeLI", "number": 23945, "cdate": 1758350644384, "mdate": 1759896789445, "content": {"title": "STAIRS-Former: Spatio-Temporal Attention with Interleaved Recursive Structure TransFormer for Offline Mulit-task Multi-agent Reinforcement Learning", "abstract": "Offline multi-agent reinforcement learning (MARL) with multi-task (MT) datasets poses unique challenges, as input structures vary across tasks due to the varying number of agents. Prior works have adopted transformers and hierarchical skill learning to facilitate coordination, but these methods underutilize the transformer’s attention mechanism, focusing instead on extracting transferable skills. Moreover, existing transformer-based approaches compress the entire history into a single token and input this token at next time step, forming simple recursive neural network (RNN) processing on history tokens. As a result, models rely primarily on current and near-past observations while neglecting long historical information, even though the partially observable nature of MARL makes history information critical. In this paper, we propose STAIRS-Former, a transformer architecture augmented with spatial and temporal hierarchies that enables the model to properly  attend to critical tokens while effectively leveraging long history. To further enhance robustness across varying token counts, we incorporate token dropout, which improves generalization to diverse agent populations. Experiments on the StarCraft Multi-Agent Challenge (SMAC) benchmark with diverse multi-task datasets show that STAIRS-Former consistently outperforms prior algorithms, achieving new state-of-the-art performance.", "tldr": "", "keywords": ["Reinforcement Learning", "multi-agent", "multi-task", "transformer", "offline learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3daf566ef83f4e225c042b2aed60d63ce90b0d2a.pdf", "supplementary_material": "/attachment/90f4e28c6e1bcd421990ec320afb6b198b085c3d.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces STAIRS-Former, a transformer-based architecture for offline multi-task multi-agent reinforcement learning (MT-MARL). It addresses two main limitations in prior transformer-based MARL models (like UPDeT, ODIS, and HiSSD): poor handling of long-term temporal dependencies and limited relational reasoning among entities. STAIRS-Former introduces three modules — (1) a spatial recursive transformer for deeper inter-agent correlation modeling, (2) a dual-scale temporal module that maintains short- and long-term histories, and (3) a token-dropout mechanism to improve robustness across varying numbers of agents. Extensive experiments on SMAC benchmarks (Marine-Easy, Marine-Hard, and Stalker-Zealot) show consistent and significant improvements over state-of-the-art baselines, with ablation studies and interpretability analyses (e.g., attention and dormant neuron studies) supporting the architectural design"}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper makes a clear and well-motivated contribution to the field of offline multi-task multi-agent reinforcement learning. The authors identify real and important limitations in existing transformer-based MARL models—specifically their difficulty in capturing long-term temporal relations and rich inter-agent dependencies—and address them with a framework that feels both technically sound and intuitively designed. The proposed STAIRS-Former architecture is an elegant combination of spatial recursion and dual-scale temporal modeling, allowing the system to reason jointly over history and agent relations in a way that existing models cannot.\n\nThe experimental section is particularly strong: the evaluation on multiple SMAC benchmarks is thorough, ablation studies are comprehensive, and the visualizations provide genuine insight into how the model learns. The paper is also very well written and organized, making it easy to follow both the motivation and the technical details. Finally, the work feels significant because it pushes transformer-based MARL toward more scalable and generalizable architectures, offering a practical foundation for future research in multi-agent decision-making systems."}, "weaknesses": {"value": "While the framework is strong and the results are convincing, the paper would benefit from a clearer discussion of generalization beyond the SMAC environment. SMAC’s discrete and tokenizable observation space makes it naturally suited to transformer architectures, so it’s uncertain whether STAIRS-Former would maintain its advantages in less structured or multi-modal domains (e.g., visual-linguistic inputs or real-world sensor data). A brief evaluation or qualitative analysis in such settings would significantly strengthen the paper’s claim to generality."}, "questions": {"value": "Generality beyond SMAC: Have you tested or considered applying STAIRS-Former to environments with more complex or unstructured observations, such as multi-modal inputs (e.g., visual or continuous sensor data)? If not, how do you anticipate the model’s spatial recursion and tokenization scheme would adapt in such cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SIhOLtHjqE", "forum": "Biz1vpQeLI", "replyto": "Biz1vpQeLI", "signatures": ["ICLR.cc/2026/Conference/Submission23945/Reviewer_PLFk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23945/Reviewer_PLFk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760939762379, "cdate": 1760939762379, "tmdate": 1762942866002, "mdate": 1762942866002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces STAIRS-Former, a spatio-temporal transformer architecture for offline multi-task multi-agent reinforcement learning (MT-MARL). The proposed method includes (1) a novel spatial recursive model to extract correlations among local observations of different entities, and (2) a novel temporal module that helps mitigating partial-observability in MARL settings and allows for capturing long-term dependencies.\nThe authors carry out experiments on offline SMAC v1 datasets in multi-task fashion, showing improved performance over previous baselines. Particularly, STAIRS-Former displays impressive generalization over unseen tasks, as well as varying number of agents, likely due to the token dropout mechanism."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors tackle a relevant and underexplored setting (offline MT-MARL) which is likely of interest to the community and opens important directions of application.\n- The paper shows clear improvements and strong empirical performance over relevant baselines in the field (UPDeT-m, ODIS, HiSSD), specifically over unseen tasks."}, "weaknesses": {"value": "- Limited and outdated benchmark tasks: the authors solely present their comparison in the context of the SMAC benchmark. The authors neglected experimentation on more recent benchmarks such as the improved benchmark SMACv2 [1], as well as the MaMuJoCo benchmark [2]. Considering the nature of this work is mostly empirical, and that SOTA online MARL methods notoriously test on these benchmark, it is unclear why the authors only provide experimentations on SMAC v1. In turn, it's unclear how the architectural contributions in STAIRS-Former really compare against relevant benchmark tasks in the field.\n\n[1] Ellis, Benjamin, et al. \"Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 37567-37593.\n\n[2] Peng, Bei, et al. \"Facmac: Factored multi-agent centralised policy gradients.\" Advances in Neural Information Processing Systems 34 (2021): 12208-12221."}, "questions": {"value": "- Why was SMACv1 chosen by the authors against the improved SMACv2 benchmark?\n- Please clarify how the tasks are divided into training and tests, with respect to number of agents and different goals/rewards. It appears to me that the authors do not rely on explicit task-conditioning information at training time, so I'm assuming agents must implicitly infer the task from observations. If so, how can they generalize to a task with a different objective? Or do tasks only differ by the amount of agents?\n- The authors claim in the abstract that a Transformer module is not able to capture long-range dependencies  because it compresses the entire history into a single token. However, this is in general a false claim, because that's exactly what transformers claim to do over RNNs. Could you please clarify what is the main drawback of previous methods and whether the limitation over long-horizons is a fundamental consequence of the architecture itself or rather a resulting effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oa6Tdz9Pc5", "forum": "Biz1vpQeLI", "replyto": "Biz1vpQeLI", "signatures": ["ICLR.cc/2026/Conference/Submission23945/Reviewer_7cme"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23945/Reviewer_7cme"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928921965, "cdate": 1761928921965, "tmdate": 1762942865794, "mdate": 1762942865794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets offline multi-task multi-agent RL (MT-MARL) with varying agent counts, inputs and actions across tasks. It builds on UPDeT/transformer-style architectures and proposes (i) a “spatial recursive”/deeper transformer to get less uniform attention, (ii) a dual-timescale temporal/history module (short- and long-term), and (iii) token dropout to generalize across different token/entity counts. Experiments on SMAC multi-task offline datasets show improvements over UPDeT-m, ODIS, and HiSSD."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Problem setting (offline + multi-task + variable agents) is relevant for MT-MARL and aligns with recent transformer-based MARL lines.\n2. Paper is clearly written and well-situated w.r.t. UPDeT/ODIS/HiSSD."}, "weaknesses": {"value": "1. Overall, the paper integrates known ingredients rather than introducing a genuinely new architectural principle for offline MT-MARL.\n2. If the problem is “uniform attention” in UPDeT/HiSSD, there are other mechanisms: attention sharpening, entropy regularization, auxiliary supervision on heads, or stronger positional/task conditioning. The paper should justify why a relatively heavy spatial–temporal–recursive stack is preferable to these lighter alternatives.\n3. Baselines may be underpowered: The main UPDeT-style baselines in the paper use very shallow transformers (as the authors themselves note “one-layer transformer cannot capture diverse relations”). A fairer test is: what happens if we (i) increase the number of transformer layers, (ii) add a simple recurrent/history token with longer horizon. Right now, the improvement could just be due to “more depth + a GRUi.e., model capacity, not the specific STAIRS interleaving.\n4. Experiments on other benchmarks such as MAMuJoCo, WareHouse, etc. can also be presented to enhance the experimental evaluations to compare how this method compares against other offline MARL baselines."}, "questions": {"value": "1. Offline pretrained transformer-based MARL (MADT) show that one big sequence model can handle multiple SMAC tasks and benefit from offline pretraining. How does this method improve upon MADT and similar baselines?\n2. For offline MARL, optimizing just the TD3-loss with BC regulation has shown to yield poor results because of very weak regularizations on the exploding joint action spaces. How has that been tackled here? Why did the authors use this method of training over existing offline MARL framework? \n3. For the comparisons with other methods, how many layers were used for the baselines vs the STAIRS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7EwT0cx7pb", "forum": "Biz1vpQeLI", "replyto": "Biz1vpQeLI", "signatures": ["ICLR.cc/2026/Conference/Submission23945/Reviewer_7K4n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23945/Reviewer_7K4n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974008162, "cdate": 1761974008162, "tmdate": 1762942865592, "mdate": 1762942865592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main challenge in offline multi-agent reinforcement learning (MARL) with multi-task (MT) datasets is the varying number of agents across tasks, which changes the input structure. Prior transformer and hierarchical skill methods underutilized the transformer's attention mechanism, focusing instead on transferable skills, and crucially, they suffered from poor historical context: they compressed the entire history into a single token at each step, making them function like a basic recurrent neural network that largely ignores long-term historical information despite its criticality in partially observable MARL. The proposed STAIRS-Former addresses this by augmenting the transformer with spatial and temporal hierarchies to effectively leverage long history and properly attend to critical tokens, while a new token dropout technique is incorporated to improve generalization to diverse agent populations; experiments on the StarCraft Multi-Agent Challenge (SMAC) benchmark confirm that STAIRS-Former achieves new state-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- I believe this paper is well-structured and written.\n- The central problem of this work is well-motivated and does sound.\n- I think this work offers a good solution for the realization of MT-MARL problem with the corresponding challenges. \n- Another strength point is the ablation on the algorithmic decision by the authors and the informative discussion."}, "weaknesses": {"value": "- A crucial weakness of this work is not stating the limitations.\n- I believe a limitation of this work could be the potential overhead and memory footprint due to the introduced components. Although, the overall training time is highlighted in the appendix, a deeper analysis would be appreciated where the training time or process time for each introduced component. This can be done by reporting the ablated training time if available or simply the overhead proccessing time compared to normal training step.\n- In the experimental section, there is no highlighting for the model sizes used across methods. This could result in an unfair comparison to the baselines.\n- Figure 5 is unclear. A more informative caption would be appreciated."}, "questions": {"value": "- What are the limitations of this work?\n- What is the memory footprint of the model and the introduced overhead compared to the other baselines?\n- In Figure 6, what is \"(\"wo RT\" excludes repeat & TSFFN)\"?\n- I did not understand well the analysis in Figure 5. Would you mind elaborating more and clarify the heatmaps in the figure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TkilLhRTlv", "forum": "Biz1vpQeLI", "replyto": "Biz1vpQeLI", "signatures": ["ICLR.cc/2026/Conference/Submission23945/Reviewer_u6gB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23945/Reviewer_u6gB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976657120, "cdate": 1761976657120, "tmdate": 1762942865284, "mdate": 1762942865284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}