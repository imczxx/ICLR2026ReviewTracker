{"id": "1o0H00pH0y", "number": 24562, "cdate": 1758357993712, "mdate": 1759896760669, "content": {"title": "I Am Aligned, But With Whom? MENA Values Benchmark for Evaluating Cultural Alignment and Multilingual Bias in LLMs", "abstract": "We introduce MENAValues, a novel benchmark designed to evaluate the cultural alignment and multilingual biases of large language models (LLMs) with respect to the beliefs and values of the Middle East and North Africa (MENA) region, an underrepresented area in current AI evaluation efforts. Drawing from large-scale, authoritative human surveys, we curate a structured dataset that captures the sociocultural landscape of MENA with population-level response distributions from 16 countries. To probe LLM behavior, we evaluate diverse models across multiple conditions formed by crossing three perspective framings (neutral, personalized, and third-person/cultural observer) with two language modes (English and localized native languages: Arabic, Persian, Turkish). Our analysis reveals three critical phenomena: \"Cross-Lingual Value Shifts\" where identical questions yield drastically different responses based on language, \"Reasoning-Induced Degradation\" where prompting models to explain their reasoning worsens cultural alignment, and \"Logit Leakage\" where models refuse sensitive questions while internal probabilities reveal strong hidden preferences. We further demonstrate that models collapse into simplistic linguistic categories when operating in native languages, treating diverse nations as monolithic entities. MENAValues offers a scalable framework for diagnosing cultural misalignment, providing both empirical insights and methodological tools for developing more culturally inclusive AI.", "tldr": "We evaluate how LLMs align with real human values in the MENA region and uncover major cultural and linguistic misalignments through a novel, survey-based benchmark.", "keywords": ["Cultural Alignment", "LLM Evaluation", "Multilingual Benchmarking"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b803b8c60ebf03f800677445713b9f4a89a1cddf.pdf", "supplementary_material": "/attachment/1fc53e14bd89cd7d79f192b6ffdb6c5de3519a7e.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a new benchmark, MENAValues, to measure the cultural alignment of LLMs with the MENA region. They use the WVS 7 survey questions to create prompts and administer the questionnaire to 8 LLMs to measure alignment."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The ensuing dataset of prompts might be useful to the community."}, "weaknesses": {"value": "1. The novelty of the paper is weak. They take the WVS questions and design prompts with it - an approach which is already used by several other papers (such as WorldValuesBench https://arxiv.org/pdf/2404.16308) which study cultural alignment of LLMs. Also, using socio-demographic prompting to measure model alignment is heavily studied and has lots of critiques (https://aclanthology.org/2025.naacl-long.408.pdf, https://aclanthology.org/2025.acl-long.1256.pdf) in being a good way of measuring cultural alignment.\n2. The paper positions itself as a novel benchmark but does not discuss the statistics (size, complexity, etc) of the benchmark succinctly. Also, it does not specify how this benchmark compares to existing Arabic/MENA region benchmarks such as CamelBench (https://arxiv.org/pdf/2410.18976), CamelEval (https://arxiv.org/pdf/2409.12623), Aradice (https://arxiv.org/pdf/2409.11404), etc.\n3. The paper lacks crucial details in several sections (listed below). Furthermore, it does not mention how the experiments were carried out, the model hyperparameter settings, such as temperature, number of times each experiment is repeated, etc, which hurts reproducibility and reliability of the results. List of missing details:\n4. The NVAS and all other metrics (equations 1,2,3,4) are defined at a model and country level. However, Table 1 contains scores only at a model level. How were the scores aggregated? Also, what are the scores at a country level?\n5. The intuition behind equations 2, 3, and 4 is not explained. What do they capture, and what does a low or high score indicate? Also, the variables in equations 2, 3, and 4 are not defined.\n6. How is the normalized distance function \"D\" (lines 335-336) defined?\n7. Section 5.2.1 is lacking crucial details: How were the normalized log probabilities calculated? What \"maximum\" are the authors referring to in lines 365-366? How was the KLD computed? It would be beneficial to specify the equation that was used.\n8. Section 5.2.3 is not explained at all. What was the motivation of the PCA? How was it computed? Although there is a section in the Appendix, it doesn't seem to be referred to in Section 5.2.3\n9. Lines 151-152 mention that the dataset is grounded on 864 total questions. However, lines 384-385 mention that the results are computed over an \"immense dataset of over 820,000 data points\". It would be beneficial to discuss how the dataset size increased 1000-fold.\n10. In Section 6, the authors report \"pervasive failures\" (lines 384-386) in the cultural alignment of LLM. However, we do observe models achieving high scores in Table 1 (~ 90% scores for GPT-4o-mini), which might increase with larger models such as GPT-4o and GPT-5. Hence, it is not clear on what basis the authors claim failure. Without explicitly specifying the criteria for success, it is hard to understand if the models have failed or passed the tests."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "f50Ocnscyj", "forum": "1o0H00pH0y", "replyto": "1o0H00pH0y", "signatures": ["ICLR.cc/2026/Conference/Submission24562/Reviewer_FeAR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24562/Reviewer_FeAR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828208946, "cdate": 1761828208946, "tmdate": 1762943122943, "mdate": 1762943122943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MENAValues, a benchmark for evaluating the cultural alignment and multilingual biases of large language models (LLMs) concerning the beliefs and values of the Middle East and North Africa (MENA) region. Built from large-scale human survey data across 16 countries, it assesses models under different perspective framings and languages (English and local languages such as Arabic, Persian, and Turkish). The analysis uncovers key issues—including Cross-Lingual Value Shifts, Reasoning-Induced Degradation, and Logit Leakage—highlighting the need for culturally inclusive AI evaluation and development."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. I like the evaluation metrics proposed in Sec 5.1."}, "weaknesses": {"value": "1. I think some findings in the paper is not novel, e.g., Cross-Lingual Value Shift and Prompt-Sensitive Misalignment. Lots of paper have mentioned that.\n[1] Cao Y, Zhou L, Lee S, et al. Assessing cross-cultural alignment between ChatGPT and human societies: An empirical study[J]. arXiv preprint arXiv:2303.17466, 2023.\n\n2. For the benchmark, it utilized two human surveys, covering 864 questions. I think the contributions are not enough to support ICLR publication."}, "questions": {"value": "1. For the metrics in Sec 5.1, how to prove those metrics are plausible? Have you compare with human evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yvhSmLUY2K", "forum": "1o0H00pH0y", "replyto": "1o0H00pH0y", "signatures": ["ICLR.cc/2026/Conference/Submission24562/Reviewer_fnXw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24562/Reviewer_fnXw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962019011, "cdate": 1761962019011, "tmdate": 1762943122662, "mdate": 1762943122662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present MENAValues, a new composite evaluation benchmark built from the World Values Survey and the Arab Opinion Index to evaluate cultural alignment and multilingual bias of LLMs in some Middle-East and North African countries. The central framing is around contrasting viewpoints: different system prompts to elicit claims along value dimensions (e.g. a first-person persona vs a third-party observer), language (same questions in english vs arabic) and w.r.t. prompted reasoning. A new metric for Normalized Value Alignment Score (NVAS) is proposed which measures the models accuracy at predicting survey questions answers. In addition to this multiple consistency scores are defined to measure the difference between the prompting approaches (persona vs observer). Finally the authors analyze token-level probabilities for multiple choice answers vs refusals. After analyzing 7 different models, including 2 region-specific models, the authors find that all models demonstrate varying alignment and consistency, with larger models leading on cross-lingual consistency, and find the prompted reasoning reduces measured alignment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. One of the strengths of this work is the clear, region-specific focus and use of high-quality sources. The benchmark grounds items in two well-established surveys, covers 864 questions spanning 16 countries, and applies post-stratification weights. \n\nS2. The approach to experimentation here is systematic with the explicit multiple prompt framing in the form of the persona vs observer, native vs english language, and prompted-reasoning vs none. THis allows for clean attribution of effects like framing sensitivity and cross-lingual drift. \n\nS3. Transparent, model-diverse evaluation. It was great to see the comparison across seven diverse models (general, multilingual, regional, and frontier) being compared with CIs. The results for region-specific models like Allam underperforming GPT-4o-mini and Gemini was a surprising and interesting result.\n\nS4. The token-probability analysis to quantify refusal vs “internal conviction” was another interesting direction. Further investigation of this by measuring logit leakage as a causal effect instead of just a static threshold could be interesting. The example PCA to visualize representational structure helps communicate the ideas effectively."}, "weaknesses": {"value": "W1. One of the most serious weaknesses with the work as it is now is that the ground-truth reduction may oversimplify the idea of ‘cultural values’. The benchmark collapses the broad survey results into an average as the primary reference for NVAS. That choice can erase multimodality and within-country heterogeneity presented in the original surveys and may bias NVAS toward central tendencies. A distributional metric for each dimension of consistency would better respect the surveys. \n\nW2. The grounding of the evaluation framework axes is quite weak. Although they make some intuitive sense it would be more helpful to the research if the choice of framing (neutral vs persona vs observer) could be further established based on research of how users ask questions of LLMs and when/where cultural values may have some impact on the resulting answers. For example, it might be obvious that cultural values should have little impact on asking an LLM to solve a math problem like ‘2+2=x. Solve for x’ but have a more significant impact on the example question in Figure 1. These topic areas are currently determined from the upstream value surveys, but have no LLM user-based validation. \n\nW3. The selection of the survey datasets and countries. There is not a 1:1 mapping of the selected countries to having results from both surveys. It is also not clear why these twos surveys in particular were selected. Working off the WVS it seems the approach could easily be scaled beyond MENA-only if AOI is excluded as a source and it is not clear if the additional questions are that complementary given that 80% of the AOI questions are governance-related which is already represented in WVS. Finally the reasoning of the selection of countries themselves which belong to MENA is not given.\n\nW4. There is limited validation against prior cultural-opinion frameworks. Existing work already measures country-conditioned opinion similarity and language effects (e.g. GlobalOpinionQA where cross-national and linguistic prompting against WVS questions are already defined). The novelty over those baselines is partly incremental (regional scope, added observer framing, token-probability probing). A side-by-side replication on overlapping items would position novelty more convincingly. \n\nW5. There are some manual validation steps that are not well defined. For example, there appear to be some arbitrary filtering criteria for survey questions applied in order to “ensure each question was value-centric”. This leads to concerns about bias introduced to surveys that otherwise have been previously externally validated and well reviewed. For native language prompting it is mentioned that in-house human annotators validated the prompts but intern-annotator agreement rates are provided or any notes on if prompts were removed by annotators due to being incorrectly translated."}, "questions": {"value": "Q1. The comparison of alignment when asking the models to reason is interesting. Did the authors consider any studies on purpose-built reasoning models like DeepSeek-R1 or Gemini 2.5 “Thinking” instead of just prompting the current models? This could help better ablate the impact of reasoning on alignment, particularly through the introduction of a fixed reasoning budget.\n\nQ2. How were translations completed for native language prompting? Where the translations done manually by the human annotators or only validated by them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bGZ758dxYQ", "forum": "1o0H00pH0y", "replyto": "1o0H00pH0y", "signatures": ["ICLR.cc/2026/Conference/Submission24562/Reviewer_DXeF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24562/Reviewer_DXeF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141369821, "cdate": 1762141369821, "tmdate": 1762943122437, "mdate": 1762943122437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper evaluates cultural alignment of popular LLMs with Middle Eastern and North African (MENA) survey response distributions. The work looks at different experimental settings for assessing alignment - using English vs native language questions, vanilla vs two forms of persona based prompting, and with and without chain-of-throught/reasoning. The results are mixed, no model consistently performs the best. The results additionally show that eliciting reasoning reduces alignment scores and inspecting logits shows a strong preference towards a certain option even when the model refuses to answer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The writing is easy to follow\n- The study uses both large multilingual models and regional models\n- MENA values and cultural alignment is an important topic, with little prior work focusing on the topic"}, "weaknesses": {"value": "- Overall, I’m failing to see clear contributions over prior work. Yes, MENA is perhaps not the focus in previous studies but the analysis done is not culture-specific, one could replace it with any other region and it would look the same. WVS is already accommodated as part of previous benchmarks like GlobalOpinionQA, so I don’t clearly see the utility of converting the same questions and response distributions into a separate benchmark. \n- Most of the findings have been previously well established - safety being superficial, lack of consistency, Several previous studies have shown token-level probabilities to be an unreliable signal when doing MCQA tasks, so here them"}, "questions": {"value": "- The perspective axis is more about frame of reference, whether or not persona based prompting is used, so it would be good to rephrase it to connect with the persona literature. Framing also corresponds to linguistic or communicative framing, for instance with lexical choices in the survey questions, which would also have an impact on the responses, so it would be good to not confuse the reader with that.\n- The new addition seems to be the AOI survey, the authors could hone in on that resource, highlight how the questions and the response distributions there differ from the WVS and then comparatively analyse the model’s answers.\n- A potentially novel and interesting finding is the language based clustering overriding cultural distinctions, however it isn’t explored in sufficient depth in the main paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5zzoWgy9Y5", "forum": "1o0H00pH0y", "replyto": "1o0H00pH0y", "signatures": ["ICLR.cc/2026/Conference/Submission24562/Reviewer_mXaa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24562/Reviewer_mXaa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762251994448, "cdate": 1762251994448, "tmdate": 1762943122198, "mdate": 1762943122198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}