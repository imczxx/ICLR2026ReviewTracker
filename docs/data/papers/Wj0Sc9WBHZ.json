{"id": "Wj0Sc9WBHZ", "number": 17334, "cdate": 1758274792076, "mdate": 1759897181658, "content": {"title": "InfoDet: A Dataset for Infographic Element Detection", "abstract": "Given the central role of charts in scientific, business, and communication contexts, enhancing the chart understanding capabilities of vision-language models (VLMs) has become increasingly critical. A key limitation of existing VLMs lies in their inaccurate visual grounding of infographic elements, including charts and human-recognizable objects (HROs) such as icons and images. However, chart understanding often requires identifying relevant elements and reasoning over them. To address this limitation, we introduce InfoDet, a dataset designed to support the development of accurate object detection models for charts and HROs in infographics. It contains 11,264 real and 90,000 synthetic infographics, with over 14 million bounding box annotations. These annotations are created by combining the model-in-the-loop and programmatic methods. We demonstrate the usefulness of InfoDet through three applications: 1) constructing a Thinking-with-Boxes scheme to boost the chart understanding performance of VLMs, 2) comparing existing object detection models, and 3) applying the developed detection model to document layout and UI element detection.", "tldr": "We build InfoDet, a dataset for infographic element detection", "keywords": ["Infographics", "visual reasoning", "grounded chain-of-thought", "object detection", "dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/59fe89131cec5026cd933f6c79534c499433241b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces InfoDet, a large-scale dataset for detecting infographic elements， including charts, text, and human-recognizable objects (HROs), to improve visual grounding for chart understanding. InfoDet combines 11,264 real and 90,000 synthetic infographics with over 14 million bounding-box annotations, produced via a hybrid programmatic + model-in-the-loop pipeline. The paper demonstrates InfoDet's value via three applications: (1) enabling a \"Thinking-with-Boxes\" (Grounded CoT) scheme that significantly improves VLM performance on the ChartQAPro benchmark (+49.6 Enhanced Relaxed Acc. for O4-mini, Fig 1); (2) benchmarking object detection models, showing InfoDet-trained models outperform SOTA foundation models ; and (3) showing the InfoDet-trained detector generalizes to UI and document layout tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work introduces InfoDet, a large-scale dataset featuring 101k images and 14M annotations, specifically designed to address the complex layouts of infographics that integrate charts and Human-Related Objects (HROs).\n2. The usefulness of the dataset is validated across diverse downstream tasks (Section 4). The authors provide compelling quantitative evidence for its impact on VLM grounding (+49.6 gain on ChartQAPro, Sec 4.1), its utility as a challenging benchmark for object detectors (Table 4), and the robust generalizability of its features to out-of-domain tasks (Table 5).."}, "weaknesses": {"value": "1. The model-in-the-loop method (Sec 3.2) may introduce systematic biases from the initial model (trained on synthetic data) into the real-image annotations. Please discuss this limitation and ideally quantify it, perhaps by reporting Inter-Annotator Agreement (IAA) among experts or by comparing against a small, fully-manual gold-standard set.\n2. The method layers visual overlays and textual descriptors, but it’s unclear how much gain comes from (i) two-layer overlays, (ii) textual element lists, or (iii) better detections/OCR. Please include controlled ablations that independently remove the text lists, the two-layer separation, and vary detector/OCR quality.\n3. The paper does not include a dedicated limitations discussion. A concise reflection on dataset scope and failure modes would improve transparency and guide responsible use."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iqg3QKiYy3", "forum": "Wj0Sc9WBHZ", "replyto": "Wj0Sc9WBHZ", "signatures": ["ICLR.cc/2026/Conference/Submission17334/Reviewer_x562"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17334/Reviewer_x562"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944357524, "cdate": 1761944357524, "tmdate": 1762927258161, "mdate": 1762927258161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InfoDet, a large-scale dataset for infographic element detection containing 11,264 real and 90,000 synthetic infographics with over 14 million bounding box annotations (texts, charts, HROs, sub-elements). Synthetic infographics are generated using 1,072 template-based methods, while real infographics are annotated via model-in-the-loop approach achieving 93.9% precision and 96.7% recall. The paper demonstrates usefulness through three applications: (1) Thinking-with-Boxes scheme improving o4-mini on ChartQAPro by 1.7pp, (2) comparing 11 object detection models showing foundation models struggle (DINO-X: 14.0 chart AP) while Co-DETR achieves 88.2 mAP after InfoDet fine-tuning, and (3) transfer learning on Rico and DocGenome showing InfoDet pre-training improves performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**1. Comprehensive and Well-Designed Dataset**\n- First large-scale infographic dataset (101,264 samples vs. prior Borkin et al. 393 samples) strategically combining real and synthetic data for authenticity and scalability\n- Efficient model-in-the-loop annotation achieving quality comparable to COCO (precision 93.9%, recall 96.7% vs. COCO's 71.9%/83.0%)\n- Multi-level annotations: element-level (charts, HROs) and mark-level (26 sub-element categories) providing fine-grained labels\n- Verified diversity across 67 chart types, 96 topics, 15 visual styles (Appendix F)\n\n**2. Important Empirical Findings on Model Capabilities**\n- **Foundation model limitations**: State-of-the-art models like DINO-X achieve only 14.0 chart AP and 15.0 HRO AP in zero-shot, with few-shot prompting showing minimal improvement (MQ-GLIP 4-shot: 16.2 chart AP). This reveals natural scene pre-training is insufficient for graphics domain.\n- **Fine-tuning effectiveness**: Co-DETR improves dramatically from 27.6 AP (4-shot) to 88.2 AP (InfoDet training), demonstrating value of large-scale quality data\n- **Generalization capability**: Cross-dataset evaluation shows InfoDet -> VG-DCU transfer drops only 17.1% mAP vs. 53.7% drop in reverse direction, indicating infographic data learns more general representations than plain charts\n\n**3. Thorough Experimental Protocol and Transparent Reporting**\n- Standardized training protocol ensuring fair comparison across 11 models\n- Ablation studies demonstrating complementary roles of visual and textual prompts (64.1 combined vs. 62.8 visual-only, 61.6 textual-only)\n- Transfer learning validated on Rico (42.1 -> 50.6 -> 53.6) and DocGenome (69.0 -> 74.4 -> 80.0)\n- Honest reporting of limitations (minimal improvement on plain single charts, o3's instruction-following issues)"}, "weaknesses": {"value": "**1. Dataset Construction Issues: Representativeness and Transparency**\n- **Fine-grained annotation imbalance**: 75 chart types exist only for synthetic infographics. Authors mention GPT-4o achieved only 61.49% accuracy on real infographics but provide no alternative approach (human annotation? better models?), leaving dataset incomplete.\n- **Annotation process opaque**: No information on expert demographics (number? background: medical imaging experts? graphic designers? CV researchers?), training process, or inter-annotator agreement. \"Multiple rounds of refinement\" lacks specifics: how many rounds? How did precision/recall improve each round? What was initial auto-annotation quality? These details are critical for reproducibility and trustworthiness.\n\n**2. Limited Application Improvements with Insufficient Analysis**\n- **Grounded CoT modest gains**: Overall 1.7pp improvement is limited, especially negligible on plain single charts (58.1→60.6) with clear improvement only on infographic multiple (70.6→72.5), suggesting method effectiveness is confined to complex scenarios.\n- **Detection evaluation lacks depth**: No analysis of post-fine-tuning regression on other scenarios (natural images). Mark-level performance drops 6.5pp from element-level (76.1% -> 69.6%) without investigating causes (more classes? smaller objects? ambiguous boundaries?). Chart detection improves 3.2× (27.6 -> 88.2) while HRO only 2.5× (25.5 -> 64.0) - why is HRO harder? Which HRO types (data-related vs. theme-related) are particularly challenging?\n\n**3. Insufficient Motivation and Missing Broader Context**\n- **Limited practical justification**: Introduction only states \"charts are fundamental\" without explaining why infographic understanding matters or concrete use cases (e.g., social media analysis, news understanding, educational accessibility, automated fact-checking for visually impaired users). Lack of real-world deployment scenarios weakens motivation.\n- **Limited generalization evaluation**: Grounded CoT only tested on ChartQAPro - does it work on other chart QA benchmarks (ChartQA, PlotQA)? Transfer learning only on Rico/DocGenome - what about other layout detection datasets?"}, "questions": {"value": "**1. Dataset Quality: Synthetic Data and Annotation Process**\n- How does the 8:1 synthetic-to-real ratio with observed biases (hand-drawn style overrepresentation, missing composite charts) affect learned model behavior?\n- Why is fine-grained chart type annotation impossible for real infographics? Were better models (Gemini 2.5 pro, GPT5, Claude Sonnet 4) or human annotation attempted beyond GPT-4o's 61.49%?\n- How many expert annotators participated with what backgrounds and training? How many refinement rounds occurred with what precision/recall progression? What was initial auto-annotation quality and inter-annotator agreement, especially for ambiguous chart-HRO boundaries?\n\n**2. Application Mechanisms and Effectiveness**\n- For grounded CoT: Aren't bbox coordinates sufficient instead of full content text? Does current approach constitute \"answer leakage\" via OCR? How does bbox detection error (~12% given 88.2 AP) impact final QA performance - correlation analysis available?\n- Why minimal improvement on plain single charts - is native VLM visual reasoning already sufficient there?\n- For object detection: Does InfoDet fine-tuning cause regression on other domains (natural images)? What causes 6.5pp mark-level drop - class count, object size, or boundary ambiguity? Root cause of foundation model failure - simply pre-training data mismatch or intrinsic task difficulty?\n\n**3. Evaluation Rigor and Generalization**\n- Does grounded CoT generalize to other chart QA benchmarks (ChartQA, PlotQA)?\n- Does transfer learning generalize to other layout detection datasets beyond Rico/DocGenome?\n- Which specific component types in Rico/DocGenome benefit most from InfoDet pre-training, and why larger improvement on DocGenome?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "57JjfiYfTO", "forum": "Wj0Sc9WBHZ", "replyto": "Wj0Sc9WBHZ", "signatures": ["ICLR.cc/2026/Conference/Submission17334/Reviewer_Jyhz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17334/Reviewer_Jyhz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991735712, "cdate": 1761991735712, "tmdate": 1762927257816, "mdate": 1762927257816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents InfoDet, a large-scale dataset for infographic-style element detection comprising about 101K images and 14M bounding boxes, annotated over text, charts, human-recognizable objects, and fine-grained chart components (26 mark types, 75 chart types). The authors argue that many failures of current multimodal/chart QA systems arise from inadequate grounding in cluttered infographic layouts, and they show that supplying detected elements to recent vision–language models improves performance on challenging multi-chart benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper targets a real and under-served pain point in multimodal/chart understanding—current VLMs and chart/infographic QA systems often fail not because they cannot “reason,” but because they cannot reliably ground the relevant regions in cluttered infographic-style inputs.\n\n2.The proposed InfoDet dataset is both large (≈101K images, ≈14M boxes) and unusually fine-grained, covering text, charts, and human-recognizable objects (HROs), as well as 26 chart-level marks and 75 chart types. This level of structural annotation is rare in existing public datasets and is directly useful for detection-then-reasoning pipelines.\n\n3.The “thinking-with-boxes” usage demonstrates that plugging the detected elements into strong VLMs yields consistent gains on challenging, multi-chart settings (e.g., ChartQAPro), which shows the dataset is not only a collection effort but can translate into practical improvements in multimodal QA."}, "weaknesses": {"value": "1.The core novelty lies in building a large, high-quality dataset and a reasonable model-in-the-loop pipeline, plus a demonstrative prompting scheme. Compared to typical ICLR work, the methodological/learning novelty is modest.\n\n2. Given that the dataset provides structured and layered infographic elements, the paper could reasonably be expected to propose a model or training scheme that explicitly exploits this structure (for example, through element-level selection, layout-aware fusion, or hierarchical grounding). The current usage mainly demonstrates utility through prompting, rather than through a dedicated model design that fully leverages the annotations."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2CTd9miOGf", "forum": "Wj0Sc9WBHZ", "replyto": "Wj0Sc9WBHZ", "signatures": ["ICLR.cc/2026/Conference/Submission17334/Reviewer_WrQm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17334/Reviewer_WrQm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088576208, "cdate": 1762088576208, "tmdate": 1762927257377, "mdate": 1762927257377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InfoDet, a large-scale dataset for infographic element detection, designed to improve visual grounding and chart understanding in vision-language models (VLMs). The dataset consists of 11,264 real and 90,000 synthetic infographics, totaling over 14 million bounding box annotations covering texts, charts, human-recognizable objects (HROs), and sub-elements like bars and legends. The authors develop a hybrid annotation pipeline combining model-in-the-loop and programmatic methods to efficiently produce high-quality labels. Three key applications are demonstrated: (1) a Thinking-with-Boxes scheme that enhances grounded reasoning in VLMs , (2) benchmarking 11 object detection models on infographic data, and (3) transferring InfoDet-trained models to graphic layout detection in documents and UI design."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s methodological rigor and practical significance are outstanding. The hybrid annotation strategy—combining synthetic and real data under a model-in-the-loop refinement—balances scalability and accuracy, achieving dataset quality comparable to COCO. The scale (over 100k infographics) and fine-grained annotations (charts, HROs, sub-elements) fill a clear research gap. The grounded CoT prompting demonstrates genuine insight into how structured visual grounding improves reasoning in modern VLMs. Empirical evaluations are extensive and carefully analyzed, revealing key insights (e.g., why zero-/few-shot detection fails on infographics). Finally, ethical considerations and licensing transparency are meticulously handled."}, "weaknesses": {"value": "First, the paper lacks quantitative validation for synthetic data fidelity and bias, which is a critical limitation given that nearly 90% of InfoDet consists of synthetic infographics. Without quantitative analysis or cross-domain alignment metrics—such as style distribution, semantic bias, or embedding similarity—the fidelity of synthetic data relative to real samples remains uncertain, casting doubt on downstream generalization and fairness.\nSecond, there is insufficient analysis of annotation tradeoffs and reproducibility. Although the model-in-the-loop annotation pipeline is innovative, the authors do not quantify annotation cost, human effort, or refinement rounds, nor do they analyze how annotation quality affects detection accuracy, which weakens the dataset’s reproducibility and practical replicability.\nThird, the evaluation of the Thinking-with-Boxes method is limited in scope and validation. The experiments are restricted to a single benchmark (ChartQAPro) with relatively small gains (around 3–5%) and no statistical or qualitative error analysis, suggesting that the improvements may stem from prompt design rather than genuine reasoning enhancement.\nFinally, there is a lack of formal evaluation of annotation consistency and labeling noise. The reported annotation precision and recall are derived from only 1,250 samples and omit inter-annotator agreement or systematic error analysis. This absence of consistency validation leaves the internal reliability of the dataset uncertain and may affect the credibility of subsequent benchmarking results."}, "questions": {"value": "How does annotation quality vary across infographic types (e.g., dense textual vs. image-heavy layouts)?\nCould the Thinking-with-Boxes approach extend to document reasoning tasks beyond charts (e.g., scientific figures, tables)?\nHave the authors considered releasing segmentation masks or relation graphs between detected elements to support compositional reasoning?\nCould the authors quantify the annotation process in terms of human effort and computational\nWhat are the computational costs of fine-tuning large detection models (e.g., Co-DETR) on InfoDet relative to standard datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6yhC3kAHp2", "forum": "Wj0Sc9WBHZ", "replyto": "Wj0Sc9WBHZ", "signatures": ["ICLR.cc/2026/Conference/Submission17334/Reviewer_fMwe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17334/Reviewer_fMwe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762304730064, "cdate": 1762304730064, "tmdate": 1762927256938, "mdate": 1762927256938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}