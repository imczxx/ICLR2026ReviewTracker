{"id": "ksnVJfhj73", "number": 18287, "cdate": 1758286068946, "mdate": 1759897114036, "content": {"title": "Hierarchical Decision Making with Structured Policies: A Principled Design via Inverse Optimization", "abstract": "Hierarchical decision-making frameworks are pivotal for addressing complex control tasks  enabling agents to decompose intricate problems into manageable subgoals. Despite their promise, existing hierarchical policies face critical limitations: (i) reinforcement learning (RL)-based methods struggle to guarantee strict constraint satisfaction, and (ii) optimization-based approaches often rely on myopic and computationally prohibitive formulations. In this work, we propose a bi-level reinforcement learning and optimization framework that systematically integrates high-level goal abstraction with structured lower-level decision making. We adopt an inverse optimization approach to inform the structure of the lower-level problem from expert demonstrations, ensuring that the objective of lower-level policy remains aligned with the overall long-term task goal. To validate the approach, our framework is evaluated on three real-world scenarios, where it outperforms baseline methods in both efficiency and decision quality, demonstrating the benefits of learning structured optimization policies within a hierarchical RL architecture.", "tldr": "", "keywords": ["reinforcement learning", "optimization", "hierarchical policy"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a2a16ebd17de0c92dbd0cd2592db4e935bf6a56d.pdf", "supplementary_material": "/attachment/520ce7d498f43971214d8a69cf5edc06b48b77f4.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a bi-level hierarchical framework combining reinforcement learning (RL) for upper-level subgoal generation with optimization-based control (OC) for lower-level execution in complex decision-making tasks. It addresses limitations in existing hierarchical policies by using inverse optimization to learn the structure of the lower-level cost function from expert demonstrations, ensuring alignment with long-term goals while maintaining computational efficiency and constraint satisfaction. The approach is theoretically analyzed for linear cost cases, guaranteeing inverse feasibility and forward stability, and empirically validated on autonomous vehicle rebalancing, supply chain inventory management, and mobile robot navigation, showing improved performance over baselines like end-to-end RL and unchanged bi-level methods.​"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The approach innovatively uses inverse optimization to derive interpretable, structured lower-level policies from sparse expert data, ensuring alignment with global objectives while preserving real-time solvability via convex formulations. The theoretical contributions, including non-empty inverse feasible sets (Proposition 2) and bounded forward instability (Proposition 4), provide rigorous justification for the ReLU-augmented linear cost structure, enhancing reliability in safety-critical applications. The framework achieves near-MPC performance with drastically reduced computation as shown in Table 3."}, "weaknesses": {"value": "1. While the inverse optimization ensures feasibility for linear costs, the reliance on offline expert demonstrations assumes their near-optimality, which could propagate biases if data is sub-optimal\n\n2. The paper lacks ablation on noisy or incomplete datasets beyond basic sensitivity. \n\n3. The experiments are limited to simulated environments without real-world deployment, and baselines like end-to-end RL could be strengthened with hyperparameter tuning or larger scales to better highlight advantages."}, "questions": {"value": "1. How does the method handle cases where subgoals cannot be directly retrieved from expert data, as mentioned in Remark 3? Could joint estimation of subgoals and θ introduce additional instability, and what regularization would you recommend?​\n\n2. The sensitivity analysis shows minor degradation with noise (Figure 5); can you provide theoretical bounds or comment on the performance loss under bounded noise in expert actions, perhaps extending Proposition 4?\n\n3. For scalability, how would the mixed-integer reformulation of perform in high-dimensional action spaces (e.g., >100 dims in robotics)? Have you done any experiments to test this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "d01dsxpkpS", "forum": "ksnVJfhj73", "replyto": "ksnVJfhj73", "signatures": ["ICLR.cc/2026/Conference/Submission18287/Reviewer_btwC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18287/Reviewer_btwC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826696944, "cdate": 1761826696944, "tmdate": 1762928008567, "mdate": 1762928008567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a hierarchical decision making framework that combines a learned high-level policy for goal abstraction with a structured low-level optimization module. The distinctive idea is to design the low-level objective via inverse optimization from a small set of expert demonstrations so that the executor aligns with long-horizon task goals while remaining computationally tractable. For a special class with linear costs, the authors analyze inverse feasibility and improve forward stability by augmenting the cost with piecewise-linear terms and a small quadratic regularizer, then solve the inverse problem with a mixed-integer formulation. Experiments on autonomous vehicle rebalancing, supply chain inventory management, and mobile robot navigation show better decision quality and lower run time than several baselines, though still behind full-model MPC in some metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper tackles an important pain point in hierarchical RL and learning-based control: how to choose the single-step low-level objective so that it is fast yet not myopic. Framing this as inverse optimization from expert traces is principled and makes the lower-level policy interpretable. The bi-level formalization and the role of subgoal representations are clearly stated, and the inverse-optimization guided design is motivated by explicit requirements on feasibility and stability.\n   \n   \n* Theoretical development for the linear-cost case is easy to follow and highlights why naive formulations can make the inverse problem infeasible. The choice of ReLU-style terms for inverse feasibility, plus a small quadratic term for stability with an explicit distance bound between expert and forward solutions, provides a concrete recipe that others can replicate.\n   \n   \n   \n* Empirically, the cross-domain evaluation is appreciated. The bi-level learned variant improves over a bi-level baseline that keeps an ad hoc low-level cost and outperforms end-to-end RL on multiple metrics. The paper reports tangible run time gains relative to long-horizon MPC while preserving a large fraction of its performance."}, "weaknesses": {"value": "* The theory is largely confined to a linear-cost setting with control-affine dynamics and then loosely extended to quadratic costs. It is unclear how much of the inverse feasibility and forward stability story carries to more general nonlinear objectives or constraints that commonly arise in robotics and operations. The current scope limits confidence that the approach will generalize without significant engineering.\n   \n   \n* The inverse problem is eventually solved as a mixed-integer program with branch-and-bound. Although the paper argues the bilinear count is reduced, there is little empirical evidence on scaling in the number of demonstrations, decision variables, or ReLU terms. A complexity study and wall-clock profiling for the inverse phase would strengthen the practicality claim.\n   \n* The size and diversity of the expert dataset are said to be small, yet quantitative counts, coverage across operating conditions, and noise or suboptimality levels are not clearly reported. The fairness and tuning protocol for competing differentiable optimization baselines are not detailed, and it is hard to tell whether stronger value-function or differentiable layer baselines could close the gap. The ablations on the number of ReLU terms and the regularization weight in the low-level cost are not surfaced in the main text.\n   \n   \n* Some notation and assumptions appear first in the appendices but are used in the main text, and there are occasional grammar issues. The paper states that an LLM was used for editing, which is fine, yet a careful pass would help reduce ambiguity and tighten exposition."}, "questions": {"value": "* How sensitive is performance to the choice of subgoal mapping matrix and to the dimensionality of the subgoal space. A short study that varies these design choices would help assess robustness.\n   \n* What are the sample complexity and identifiability considerations of the inverse problem in practice. For example, how many and what type of demonstrations are needed to recover a useful low-level cost when experts are imperfect or noisy.\n\n* Can the authors report detailed solve times and scaling curves for the inverse optimization as a function of problem size, number of ReLU terms, and demonstrations. This would clarify practicality beyond the forward-time savings.\n\n* In the mobile robot setting, to what extent do the safety constraints and collision margins interact with the learned penalties. If subgoals are infeasible, how often does the penalty-only approach suffice, and when would constraint learning be necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pvctatLh1m", "forum": "ksnVJfhj73", "replyto": "ksnVJfhj73", "signatures": ["ICLR.cc/2026/Conference/Submission18287/Reviewer_Vxfa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18287/Reviewer_Vxfa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918175721, "cdate": 1761918175721, "tmdate": 1762928008122, "mdate": 1762928008122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers a hierarchical approach to solving complex problems where reinforcement learning (RL) is used at the higher level to generate subgoals for some overarching task and an optimization-based approach is used at the lower level to achieve those subgoals in an optimal manner. The paper proposes to use a dataset of expert demonstrations to obtain a lower-level cost function via inverse optimization that captures the overarching task while maintaining important properties like stability and safety. Theoretical results describing properties of the proposed optimization problems are derived and experiments illustrating performance of the approach are provided."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Bi-level formulations of hierarchical reinforcement learning methods have seen significant interest in recent years, and this work considers a variation of this problem that may be of interest to the community. Using expert demonstrations to anchor the bi-level formulation to the specific overarching task at hand is an interesting idea and merits exploration."}, "weaknesses": {"value": "The paper has the following weaknesses:\n1. The core approach proposed in the paper lacks intuitive motivation and technical justification. Specifically, it is technically unclear how the inverse optimization approach applied to the dataset of expert demonstrations successfully aligns the resulting lower-level cost function with the overarching, higher-level objective. The intuitive motivation for why we should expect the approach to achieve this is unclear, as well. See the **Questions** below for specific points related to these issues.\n2. The paper is not clearly situated within the context of the related literature. In particular, it is not clear what the main drawbacks of existing approaches are and how the proposed approach addresses them. Though it is stated around lines 73-75 that existing approaches \"rely on myopic objectives\" that cause \"solutions to overlook longer-term impacts\" and can lead to suboptimal trajectories, no evidence (e.g., relevant references or supporting experimental results) is provided to support this claim. Similarly, in the related works it is stated that previous methods \"[don't] relate to the final cumulative rewards directly\" (lines 131-132) and corresponding \"sub-optimality issues\" are alluded to (lines 139-140), but no concrete support to these claims is given. It thus remains unclear what the concrete drawbacks are of existing approaches, making it difficult to judge how or whether the proposed approach addresses them.\n3. The experimental results do not sufficiently support the need for the proposed method within the context of existing methods. Specifically, the summaries provided in Tables 1 and 2 indicate that standard model predictive control (MPC) outperforms the proposed approach on both problems considered, while the \"Bi-level-cvxpy\" baseline has comparable performance to the proposed approach. In addition, the proposed method is not compared with any of the hierarchical baselines mentioned in the introduction and related works, leaving unanswered the question of how the proposed method performs against direct competitors from the recent literature."}, "questions": {"value": "1. Where is the claim on lines 73-75 that existing \"formulations typically rely on myopic objectives ... causing solutions to overlook longer-term impacts and potentially leading to suboptimal trajectories\" substantiated? If it is not, can you provide justification for this claim?\n2. In standard hierarchical settings, the higher-level policies generates subgoals, while the lower-level policy solves an optimization problem that is specific to achieving that particular subgoal. This raises several questions for your setting:\n    \n    (a) Is each lower-level optimization problem in your approach designed to achieve a specific subgoal, or is it aligned with the overarching, higher-level goal?\n\n    (b) Is the static dataset of expert demonstrations related in any way to the specific subgoals or to the higher-level goal?\n\n    (c) What is the intuitive reason why we should expect that we can learn a useful lower-level cost function from a static dataset of expert demonstrations?\n\n    (d) What is the technical reason why we can learn a useful lower-level cost function from a static dataset of expert demonstrations?\n3. It is stated lines 185-186 that \"a commonly used form of subgoals is a linear transformation\" using a known matrix, $C$. Can you provide references or justification supporting this?\n4. What is the precise relationship between $d_{\\theta}, h(\\theta)$, the objective of the high-level problem, and the expert demonstration dataset in the context of Section 3.3.1?\n5. How is $d_{\\theta}$ \"properly designed to align the lower-level optimization problem to the objective of the overarching decision-making problem\", as stated in lines 233-234? How does solving equation (6) achieve this alignment?\n6. It is stated lines 254-255 that you \"focus on common cases where subgoal $\\hat{\\mathbf{x}}_t$ can be retrieved from expert data\", but in continuous or large spaces this may almost never hold. Can you elaborate on why this case is common, ideally providing references?\n7. In the experiments, MPC outperforms the proposed Bi-level-learned method in both problem settings, while Bi-level-cvxpy performs comparably to Bi-level-learned. Can you discuss the implications of this for the significance of the proposed approach?\n8. Can you elaborate on why you did not compare with existing hierarchical methods from the literature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QbUt6SXbG9", "forum": "ksnVJfhj73", "replyto": "ksnVJfhj73", "signatures": ["ICLR.cc/2026/Conference/Submission18287/Reviewer_t9Sj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18287/Reviewer_t9Sj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762202525079, "cdate": 1762202525079, "tmdate": 1762928007618, "mdate": 1762928007618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work uses Hierarchical Reinforcement Learning framework towards optical control problems, where the lower-level cost is learned from expert demonstrations via inverse optimization. The upper level outputs subgoal in a compressed space, while the lower level solves a single-step convex program FOP(·) that enforces hard constraints and is designed to approximate the behavior of expert controller. This work test their framework on 3 benchmarks and the results shows improvement over (i) a bi-level scheme with a hand-designed one-step cost, (ii) a differentiable-optimization baseline learned end-to-end via cvxpy layers, and (iii) end-to-end RL, while approaching the performance of a long-horizon MPC expert at significantly lower per-step compute."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: Most prior hierarchical RL–OC work concentrates on the upper level (learning high-level actions or subgoals) and treats the lower-level as fixed instead.  This a concrete gap in the literature. This paper improves upon it and focuses on learning the lower-level cost structure. Given expert trajectories from a strong OC controller, this paper explores ways to create a single-step lower-level optimization while satisfying two objectives: speed and alignment with long-term performance. \n\n2. Significance: In this paper, their inverse-optimization formulation is not just a simple quadratic with least-squares fitting, but rather a ReLU-based cost structure to guarantee the existence of parameters that can make any given expert decisions optimal (inverse feasibility).\n \n3. Quality: The propositions and proofs (inverse feasibility, bounded distance between learned and expert decisions) are sound and valid with respect to the expert demonstration. The formulation of the bi-level framework (HRL+OC) is generally mathematically clean and aligned with known RL–MPC hybrids: The upper-level objective being the standard discounted return and the lower level being the explicitly constrained by physical limits and dynamics."}, "weaknesses": {"value": "1. Presentation: The abstract is too vague. The abstract states \"our framework is evaluated on three real-world scenarios, where it outperforms baseline methods\" without furthering clarifying the nature/type of benchmark (e.g. is it a control or navigation? is it 2D or 3D? Is it discrete or continuous). Also what is the baseline? Are they HRL? are they basic RL? are they SOTA? After reading the abstract, the reader is not able to pin point and narrow down on the narrative of this paper. Although the related work section does cite representative HRL and learning-based OC works, the abstract/introduction language (“we present a general decision-making framework that integrates RL with OC through a hierarchical policy…”) reads as if this paper is novel in proposing the integration itself. \n\n2. Presentation: Introduction lacks citation (see questions) \n\n3. Benchmarks and baselines: The benchmarks are well-matched to the framework’s assumptions but are not standard HRL benchmarks; the baselines are decent within those domains, but not clearly SOTA in the broader HRL literature. \nThe baselines are reasonable within the chosen setting but not include more recent RL–OC baselines that are conceptually close, like Graph-RL (Gammelli 2023 https://arxiv.org/abs/2305.09129) on the AV task, or OHIO (Schmidt 2024 https://arxiv.org/pdf/2410.07933) on hierarchical control. \n\n4. Inverse optimization step may be a computational bottleneck in larger systems. The paper argues that the IO formulation can be solved “within a reasonable time (line 379)” via MIP and branch-and-bound, and they do reduce the number of bilinear terms. But there is a lack of detailed analysis on runtime of scaling of the IO stage itself, as opposed to the forward control at test time. For larger network control or higher dimensional robotics tasks, MIPs can become intractable. Although it is acknowledged in the future work section, it raises the concern to the framework's applicability to large-scale systems. \n\n5. Lack of detail on the upper-level RL. \nThe paper could 1. specify on the RL algorithm used and 2. analyze the RL algorithm's sensitivity to hyperparameters or reward shaping. These additional information would help explain and explore whether the RL policy exploit the learned lower-level FOP (e.g., pathological subgoals that are technically feasible but undesirable over long horizons). \n\n6. No comparison to “direct” inverse MPC baselines. \nThis paper did not include baselines that more closely resemble modern inverse MPC methods (e.g., Zhang et al., 2024, Learning Optimal Control Cost Functions for MPC) which also aim to learn cost structures from data. It would be informative to see how much of the improvements in result comes from the ReLU-based structure + forward-stability regularization vs just “learning a quadratic/terminal cost” with a strong IO solver."}, "questions": {"value": "1. Missing citations: line 052-071: \"Existing approaches have several limitations. First, most hierarchical methods adopt long-horizon OC formulations at the lower level to preserve stability and feasibility guarantees, which can introduce prohibitive computational complexity for real-time applications.\" No citations for the methods outlined in this sentence. 073-074 \"causing solutions to overlook longer-term impacts and potentially leading to suboptimal trajectories.\" no citation to support this. \n\n2. (comment) Overall, the optimal control portion of this paper is detailed and well structured, but there is a lack in the discussion of the RL component, making the paper's presentation slightly unbalanced."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gdPWvPq4gn", "forum": "ksnVJfhj73", "replyto": "ksnVJfhj73", "signatures": ["ICLR.cc/2026/Conference/Submission18287/Reviewer_UqDh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18287/Reviewer_UqDh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762463455215, "cdate": 1762463455215, "tmdate": 1762928007190, "mdate": 1762928007190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a general decision-making framework that integrates reinforcement learning (RL) with optimal control (OC) through a hierarchical policy. Its main contribution lies in an inverse optimization-based approach to inform the lower-level control policy. The paper provides theoretical analysis and quantitative evaluations across several scenarios, demonstrating its potential impact in real-world applications."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper formulates the lower-level control problem as an inverse optimization problem to enable real-time decision-making and alleviate myopic issues, which is crucial for real-world applications.\n- The paper provides sufficient theoretical analysis to ensure the interpretability of the proposed method.\n- The paper evaluates the proposed Bi-level-learned framework across several domains.\n- The paper is well written and clearly presented."}, "weaknesses": {"value": "- The proposed approach relies on expert demonstrations to guide the lower-level optimization and is claimed to be suitable for scenarios with scarce data, where imitation learning may fail. However, the paper lacks  a direct comparison with an imitation learning baseline and an analysis of sample efficiency, which weakens its convincingness.\n- The proposed approach and its theoretical analysis heavily rely on the assumptions of strong convexity and affine constraints in the lower-level optimization, which limits its applicability. In practice, many control systems involve nonlinear dynamics or nonconvex feasible regions, particularly in complex tasks where hierarchical frameworks are really needed.\n- As a hierarchical framework, the paper does not provide details on the method and experimental setup of the upper-level reinforcement learning."}, "questions": {"value": "- How does the proposed approach compare with imitation learning baselines? How sample-efficient is it?\n- Can the proposed approach handle deviations from the strong convexity and affine constraint assumptions in practical control tasks?\n- What methodology and experimental setup are used for the upper-level reinforcement learning in the proposed hierarchical framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "68etRJjfdQ", "forum": "ksnVJfhj73", "replyto": "ksnVJfhj73", "signatures": ["ICLR.cc/2026/Conference/Submission18287/Reviewer_4BTY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18287/Reviewer_4BTY"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762540475155, "cdate": 1762540475155, "tmdate": 1762928006752, "mdate": 1762928006752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}