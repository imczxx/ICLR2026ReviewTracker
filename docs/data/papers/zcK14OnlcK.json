{"id": "zcK14OnlcK", "number": 17167, "cdate": 1758272972050, "mdate": 1759897192939, "content": {"title": "Compensate, Don't Reconstruct: Parameter- and Data-Efficient 2-bit LLM Quantization", "abstract": "The substantial memory footprint of large language models (LLMs) remains a key barrier to their on-device deployment. 2-bit quantization is a promising solution; however, current methods impose a difficult trade-off between the high accuracy of training-intensive Quantization-Aware Training (QAT) and the efficiency of lower-performing Quantization Error Compensation (QEC). Our analysis of QEC reveals a critical insight: its effectiveness is more dependent on minimizing activation discrepancy than weight discrepancy alone. Building on this, we introduce LG-QEC, a framework that significantly enhances the compensation process. LG-QEC combines a hybrid adapter and a local-global optimization strategy to directly align activations and suppress quantization errors. Experiments show LG-QEC achieves accuracy comparable to state-of-the-art QAT methods while using only a fraction of the training token budget and trainable parameters. This work successfully bridges the gap between efficiency and performance, enabling accurate and practical 2-bit LLMs.", "tldr": "", "keywords": ["LLM", "Quantization", "PEFT"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff4b7f893b6b46892061f5f451994383a98d6da3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an in-depth empirical study of Quantization Error Compensation (QEC) and, based on the findings, introduces LG-QEC, an efficient and effective recipe for performing QEC in low-bit (e.g., 2-bit) large language models."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* This paper presents extensive and well-designed empirical studies on QEC methods, which offer valuable insights for researchers and practitioners in this field. In particular, the findings that hybrid approaches achieve impressive performance, and the analysis of how weight and activation discrepancies evolve with an increasing number of training tokens, are especially noteworthy.\n\n* The proposed QEC recipe, LG-QEC, demonstrates strong empirical performance.\n\n* The paper is very well-written and easy to follow."}, "weaknesses": {"value": "* Some of the analyses behind the empirical results are not entirely persuasive to me.\n\nThe first concerns the concept of weight stability. The authors seem to regard weight stability as something essential for high-quality adaptation, mentioning that vector quantization performs better with adaptation than uniform quantization. However, I find this interpretation somewhat overstated. The superior results of vector quantization could simply stem from its inherent advantages over uniform quantization, rather than from better weight stability being a more suitable fit for QEC. The authors’ attempt to attribute vector quantization’s strength to weight stability does not sound convincing. In my view, to substantiate the unique role of weight stability in QEC, an experiment should compare two quantization methods where one shows poorer base quantization results but higher weight stability, and yet performs better under QEC than the other with the opposite characteristics.\n\nSecond, the rationale behind the design of LG-QEC is somewhat unclear.\nThe authors argue that end-to-end fine-tuning can be suboptimal because it must simultaneously optimize two objectives, weight and activation discrepancies, and therefore propose to decouple them. However, their actual approach, which first optimizes only activation discrepancy and then proceeds with end-to-end fine-tuning, does not appear to implement such decoupling. A true decomposition would be something like a first stage for weight discrepancy and a second stage for activation discrepancy. While I acknowledge that LG-QEC achieves strong empirical results, the justification and explanation of why it works appear somewhat speculative.\n\n* The analysis and evaluations are performed exclusively on a single model, Llama-3-8B, leaving questions about generalizability to other architectures, particularly MoE models.\n\n* Defining the budget solely in terms of parameter size reduces the paper’s practical relevance. To make the discussion more useful for practitioners, inference efficiency should also be considered. For instance, sparse adaptation methods may degrade inference throughput compared to LoRA even assuming the same parameter size, and vector quantization often leads to slower inference than scalar quantization. Incorporating such efficiency considerations would make the paper’s insights substantially more valuable."}, "questions": {"value": "Directly related to the weaknesses I mentioned above.\nI would like the authors to clarify whether weight stability is indeed an important factor for QEC, and how LG-QEC achieves a decoupling of the optimization processes for weight and activation discrepancies. \nIn addition, I would appreciate any discussion or speculation on how the presented findings might generalize to other model types, especially architectures beyond Llama-style dense transformers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SIuL3LdXp5", "forum": "zcK14OnlcK", "replyto": "zcK14OnlcK", "signatures": ["ICLR.cc/2026/Conference/Submission17167/Reviewer_guY5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17167/Reviewer_guY5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547328500, "cdate": 1761547328500, "tmdate": 1762927150353, "mdate": 1762927150353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a detailed analysis of the Quantization Error Compensation (QEC) quantization method. The paper posits that traditional Quantization-Aware Training (QAT) performs \"Reconstruction,\" whereas QEC performs \"Compensation.\" Through observations from QEC experiments analyzing different adapter structures and initialization methods, the scalability of training parameters, and the selection of quantizers, the authors identify the key factors for improving the accuracy of the QEC quantization method. Finally, the authors propose LQ-QEC, employing a two-stage QEC approach to compensate for 2-bit LLM quantization, claiming to achieve high 2-bit quantization accuracy using minimal trainable parameters and a limited training token budget."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Novel Research Problem:** The focus is on the Quantization Error Compensation (QEC) problem. This strategy effectively addresses the issue of traditional Quantization-Aware Training (QAT) frameworks requiring substantial computational resources for extremely low-bit quantization, saving overhead and possessing practical value.\n2.  **Rich Theoretical Analysis:** The paper provides a very detailed overview of different methods currently employed for Quantization Error Compensation (QEC). It analyzes various choices, such as different adapters (LoRA/Top-k), different initialization methods (L/S/LS), and different quantizers (uniform/vector). The experimental scope is extensive, yielding numerous valuable observations and insights."}, "weaknesses": {"value": "1.  **Lacks Clear Writing Logic:** The core of the paper is the LQ-QEC quantization method (Section 5), but the exposition on LQ-QEC itself is relatively brief (e.g., how the budget is allocated between the first and second stages, more ablation study results, visualization of discrepancy values after adopting the two-stage scheme). Instead, the analysis in Section 4 takes up the majority of the content. Furthermore, within Section 4, Sections 4.1-4.2, which discuss the selection of different adapters and initialization methods, are not closely related to the core insights presented later. The authors should allocate more space to the core LQ-QEC method. Some introductory experiments and observations are overly lengthy, hindering in-depth reading.\n2.  **Core Contribution is Unclear:** Logically, the paper seems to focus on improving the QEC method. However, the abstract or introduction emphasizes more that their method requires significantly fewer trainable parameters and training token budget compared to traditional QAT methods. This advantage should primarily be attributed to QEC itself, rather than being a specific contribution of this paper.\n3.  **Lacks Deeper Investigation Across Bit-widths:** Although the paper's title is \"2-bit LLM quantization,\" the insights observed in this paper, such as the increase in weight discrepancy and the decrease in activation during QEC training, are not analyzed across other bit-width scenarios."}, "questions": {"value": "1.  **Regarding the Core Argument of the Paper:** The paper devotes significant space to discussing the trend of weight and activation discrepancy changes during the QEC fine-tuning process. Sections 4.3 and 4.4 visualize these trends, but the connection between the defined discrepancy values and the final end-to-end performance is not well established, making the argument less convincing. For example, in Figure 4(C), the magnitude of the increase in weight discrepancy is notably smaller than the decrease in activation discrepancy. What impact does this difference in magnitude have on the final performance? Furthermore, in Section 5, when proposing LQ-QEC, why is there no quantitative comparison of discrepancy changes to get related to the content presented in Section 4?\n2.  **Regarding Experimental Setup:** \n\n    (1) Why is there a lack of comparison with more training tokens for local optimization under the \"Uniform quantizer\" in Table 1? Why aren't experiments with local optimization conducted under the Uniform quantizer setting? \n\n    (2) The number representing the QAT token consumption in the \"ParetoQ\" row of Table 2 is somewhat misleading. Since that setting and the paper's QEC method are entirely different (QEC requires a pretrained checkpoint as a base), directly listing \"30B vs 16M tokens\" could be slightly misleading. \n\n    (3) Why is there no entry for the vector method *without* local optimization in this table? It seems the vector method alone is not proposed in this paper but originates from QuIP# (Tseng et al., 2024). (4) For Table 2, it is recommended to also provide downstream task metrics (e.g., CSQA and MMLU) as done in other tables."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hH2pqvlEQ1", "forum": "zcK14OnlcK", "replyto": "zcK14OnlcK", "signatures": ["ICLR.cc/2026/Conference/Submission17167/Reviewer_T5JN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17167/Reviewer_T5JN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708498297, "cdate": 1761708498297, "tmdate": 1762927149777, "mdate": 1762927149777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LG-QEC, an efficient framework for 2-bit LLM quantization. The authors show that model performance depends more on activation discrepancy than on weight discrepancy, and design a compensation-based approach combining hybrid adapters (low-rank + sparse), vector quantization, and a local-global optimization scheme. Experiments on Llama-3-8B demonstrate that LG-QEC achieves ParetoQ-level performance (PPL ≈ 8.1 on WikiText-2) using only 16 M training tokens, drastically reducing both computation and parameter cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper has a clear motivation and identifies a genuine gap between QAT and QEC for 2-bit LLMs.\n2. LG-QEC is conceptually clean. It combines hybrid adapters + vector quantization + local–global optimization and fit together nicely.\n3. The method matches QAT performance with ~1/1800 of the training tokens. Ablations are thorough and convincing."}, "weaknesses": {"value": "1. Experiments feel a bit narrow. Results are mainly WikiText-2, C4, and CSQA; I’d like to see broader coverage (reasoning, code, multilingual).\n2. No hardware results. Everything is perplexity/accuracy, so the “on-device” efficiency claim isn’t fully demonstrated."}, "questions": {"value": "1. The experiments mainly focus on WikiText-2, C4, and a few commonsense QA benchmarks. Have you tested LG-QEC on other domains (e.g., reasoning, coding, or multilingual datasets)? If not, do you expect the same activation-discrepancy insights to hold there?\n2. Since the paper emphasizes on-device efficiency, could you provide empirical results on real hardware (e.g., inference latency, memory usage, or throughput) to substantiate the claimed deployment benefits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Up0etSFKsa", "forum": "zcK14OnlcK", "replyto": "zcK14OnlcK", "signatures": ["ICLR.cc/2026/Conference/Submission17167/Reviewer_EnF3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17167/Reviewer_EnF3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762169778871, "cdate": 1762169778871, "tmdate": 1762927148622, "mdate": 1762927148622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose LG-QEC that combines a hybrid adapter (low-rank and sparse matrix) and a local-global optimization strategy to align activations and suppress quantization errors. In 2-bit quantization. Experiments show LG-QEC achieves accuracy comparable to state-of-the-art QAT methods while using only a fraction of the training token budget and trainable parameters."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is easy to follow\n2. The insight \"effectiveness is more dependent on minimizing activation than minimizing weight discrepancy alone\" is well-motivated and validated through systematic experiments.\n3. Both Uniform quantization and Vector quantization are applied in the experiments"}, "weaknesses": {"value": "1. **Inefficient and Potentially Misleading Sparsity**: The proposed method relies on unstructured sparsity within its hybrid adapters. This design choice is problematic for two reasons. First, _unstructured sparsity does not typically yield practical speedups or memory savings during training, as the computational graphs and gradient updates remain dense._ Second, the paper uses the number of elements (non-zero) as budget, which is an unrealistic metric for hardware efficiency. True memory benefits only materialize when sparsity exceeds a high threshold and supported by specialized hardware, which is not the case here. In fact, training two large, unstructured sparse matrices likely incurs a computational overhead exceeding that of standard QAT, as the dense representation of these sparse structures must be maintained and processed. This fundamentally undermines the paper's claims of training efficiency.\n2. **Unfair Comparison and Overlooked Deployment Costs**: The experimental comparison is skewed. LG-QEC is evaluated while retaining its FP adapters (1GB in a 3.8GB model), whereas QAT methods are full quantized. A fair comparison requires merging the FP adapters back into the quantized weights and then re-quantizing the entire model to a true 2-bit state, ensuring all methods have identical computational footprints. The current setup masks potential \"merge errors\" and inflates the perceived performance of LG-QEC. The limited gains reported in Table 2 are questionable given the significant 1GB of FP parameters, raising concerns about the method's utility in a realistic, efficiency-focused deployment scenario.\n3. **Lack of Efficiency Evaluation**: Given the substantial overhead introduced by the hybrid adapter structure (both sparse and low-rank), the claims of parameter and training efficiency are not substantiated by relevant metrics. The paper lacks critical experiments measuring actual training memory footprint, training latency, and—most importantly—inference latency and memory usage. Without benchmarking these practical hardware metrics against QAT and PTQ baselines, the proposed method's real-world viability remains unproven.\n4. **Limited Model Scale and Task Diversity:** The evaluation is conducted primarily on Llama-3-8B. It is crucial to demonstrate the effectiveness of LG-QEC across a wider range of model scales, including both smaller (e.g., 1B, 3B) and, more importantly, larger models (e.g., 70B).\n5. **Questionable Citation and Related Work**:  This paper states \"... either low-rank or sparse adapters Li et al (a),; Guo et al; Zhang et al;\". However, I read these references none of them do not use sparse adapters, are their correct?"}, "questions": {"value": "1. How do you expect LG-QEC to scale to much larger models (e.g., 70B)? Do you foresee new challenges, or will the same principles hold? Conversely, have you tested on smaller models (e.g., 1B-3B) where the parameter overhead of your adapters is relatively larger?\n2. In the local optimization stage, you use only half the parameter budget. What is the intuition behind this?\n3. The performance of your hybrid adapter seems highly dependent on the use of a high-quality vector quantizer. Does this mean LG-QEC is less effective or even fails with a standard uniform quantizer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "srqFzC6B68", "forum": "zcK14OnlcK", "replyto": "zcK14OnlcK", "signatures": ["ICLR.cc/2026/Conference/Submission17167/Reviewer_SrtC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17167/Reviewer_SrtC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184314431, "cdate": 1762184314431, "tmdate": 1762927148403, "mdate": 1762927148403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues the \"reconstruction\" paradigm of Quantization-Aware Training (QAT) for 2-bit LLM quantization, advocating for an efficient \"compensation\" approach (QEC) . The paper proposes: minimizing activation discrepancy is a more reliable predictor of final accuracy than minimizing weight discrepancy alone. Based on this, the paper proposes LG-QEC, a framework that decouples these objectives. It first uses a refined vector quantizer to suppress initial weight errors. Then, it employs a two-stage local-global optimization strategy to efficiently align activation distributions. The LG-QEC achieves accuracy comparable to state-of-the-art QAT while using 1875x less training data and a fraction of the trainable parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's performance in llama3-8B is superior in data and parameter efficiency.\n2. The paper has clear presenation and writing, good motivation.\n3. Rigorous Empirical Analysis: The paper is grounded in a rigorous empirical study (Sec. 4) . It methodically investigates the interplay between adapter architecture , initialization strategies , and quantizer design, building a foundation for the method.\n4. Effective Decoupled Framework: The LG-QEC framework offers an effective solution by \"decoupling\" the quantization problem . It uses a vector quantizer to suppress weight discrepancies and a two-stage (local-global) optimization to align activations, providing a stable and efficient pathway to high accuracy ."}, "weaknesses": {"value": "1. Limited Empirical Validation: The framework's generalizability is unsubstantiated, as validation is confined to a single model (Llama-3-8B).\n2. Critical Dependency on Vector Quantization: The method's success is critically contingent on a sophisticated vector quantizer (VQ). Performance collapses with a standard uniform quantizer (PPL > 11 vs. 8.1) , indicating the framework is only effective after VQ has suppressed initial weight errors.\n3. Methodological Complexity: The complete LG-QEC pipeline—integrating a specialized VQ, a hybrid adapter, and a two-stage optimization procedure—is significantly more complex than standard QAT, posing a barrier to practical implementation.\n4. For low-bit quantization of large language models, employing vector quantization appears to be a straightforward approach. Looks like this method is the combination of vector quantization and QEC-like method (e.g., LoftQ)."}, "questions": {"value": "1. What's the LG-QEC performance on other different scale models, such as llama3-1B, 3B, 70B. and multi zero-shot tasks (like lm-evaluation-harness).\n2. What is the meaning of \"LG\" in LG-QEC?\n3. Marginal Efficacy of L-G Optimization: The core two-stage (L-G) optimization strategy yields negligible empirical gains. Data shows this complex procedure only improved C4 PPL from 8.15 to 8.11, suggesting the performance benefits stem almost entirely from the VQ choice, not the L-G strategy?\n4. Increased Inference Memory Footprint: The reliance on adapters results in a larger final model size (3.8GB) compared to the QAT baseline (2.8GB), conflicting with the primary goal of edge deployment.\n5. Large language models are sensitive to activation quantization. Therefore, the first insight—that optimizing the discrepancy of activation values would be effective—does not appear to yield meaningful insights. Moreover, directly correlating output-side activation discrepancies has already been discussed in some prior quantization works (e.g., PD-Quant, CVPR 2023, LiDAR-PTQ, ICLR 2024). The authors should address this discrepancy in their discussion.\n6. what is the training/finetuing time for LG-QEC?\n7. Looks like the LG-QEC is weight-only quantization, what is the final memory saving ratio?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uuls4l1ILh", "forum": "zcK14OnlcK", "replyto": "zcK14OnlcK", "signatures": ["ICLR.cc/2026/Conference/Submission17167/Reviewer_GXWA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17167/Reviewer_GXWA"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762349726465, "cdate": 1762349726465, "tmdate": 1762927148176, "mdate": 1762927148176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LG-QEC, a framework for efficient 2-bit quantization of LLMs using Quantization Error Compensation. It achieves perplexity of 8.1 on WikiText-2 with only 16M training tokens—a 1,875× reduction compared to  state-of-the-art methods requiring 30B tokens. The key insight is that minimizing activation discrepancy (rather than weight discrepancy) is crucial for effective quantization error compensation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Achieving comparable performance to methods using 30B tokens with only 16M tokens represents major data efficiency gains.\n\nThe hybrid adapter structure and two-stage local-global optimization are clearly justified through ablations, showing complementary benefits and principled division of responsibilities."}, "weaknesses": {"value": "The paper provides rigorous analysis showing that activation discrepancy correlates more strongly with final performance than weight discrepancy. However, this obervervation has already been found by many previous papers. It is far away from a new finding. \n\nOnly compares against RILQ from the QEC family, missing critical recent methods like PiSSA, LQ-LoRA, and LQER. No head-to-head comparisons under identical settings."}, "questions": {"value": "refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y1B3ubVW8r", "forum": "zcK14OnlcK", "replyto": "zcK14OnlcK", "signatures": ["ICLR.cc/2026/Conference/Submission17167/Reviewer_8Yj5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17167/Reviewer_8Yj5"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission17167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762435473223, "cdate": 1762435473223, "tmdate": 1762927147898, "mdate": 1762927147898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}