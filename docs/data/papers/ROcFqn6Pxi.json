{"id": "ROcFqn6Pxi", "number": 18145, "cdate": 1758284340166, "mdate": 1759897127705, "content": {"title": "Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking", "abstract": "Large Language Models (LLMs) are increasingly fine-tuned on smaller, domain-specific datasets to improve downstream performance. These datasets often contain proprietary or copyrighted material, raising the need for reliable safeguards against unauthorized use. Existing membership inference attacks (MIAs) and dataset-inference methods typically require access to internal signals such as logits, while current black-box approaches often rely on handcrafted prompts or a clean reference dataset for calibration, both of which limit practical applicability. Watermarking is a promising alternative, but prior techniques can degrade text quality or reduce task performance. We propose TRACE, a practical framework for fully black-box detection of copyrighted dataset usage in LLM fine-tuning. TRACE rewrites datasets with distortion-free watermarks guided by a private key, ensuring both text quality and downstream utility. At detection time, we exploit the radioactivity effect of fine-tuning on watermarked data and introduce an entropy-gated procedure that selectively scores high-uncertainty tokens, substantially amplifying detection power. Across diverse datasets and model families, TRACE consistently achieves significant detections ($p < 0.05$), often with extremely strong statistical evidence. Furthermore, it supports multi-dataset attribution and remains robust even after continued pretraining on large non-watermarked corpora. These results establish TRACE as a practical route to reliable black-box verification of copyrighted dataset usage.", "tldr": "", "keywords": ["large language models", "Copyrighted dataset usage detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f82053c4466b72750b4701cff1459f19462ef2d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper works on the challenge of detecting unauthorized use of copyrighted datasets in the fine-tuning of LLMs under a black-box setting, where only model inputs and outputs are accessible. The authors propose TRACE, a framework that enables dataset owners to verify usage.  Experimental results across multiple model families and diverse datasets demonstrate that TRACE achieves statistically significant detections with extremely low p-values, substantially outperforming existing grey-box and black-box methods like Membership Inference Attacks (MIAs)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper introduces an entropy-gated detection procedure. This method selectively scores high-uncertainty tokens, which effectively amplifies the watermark signal from fine-tuning, as shown in Figure 2.\n2.  The evaluation is extensive, testing on three recent model families and seven diverse datasets. This demonstrates the method's general applicability beyond a single model or task type."}, "weaknesses": {"value": "1.  The paper's core innovation, \"entropy-gated detection,\" relies on an auxiliary model to estimate token entropy. However, the paper fails to adequately explore and experimentally validate the mechanism's robustness when significant discrepancies (e.g., in architecture or scale) exist between the auxiliary and suspect models. It is recommended to add experiments, for instance, using auxiliary models of different sizes or families, to quantify the impact of this model mismatch on detection performance.\n2.  The paper claims its watermarking method is \"distortion-free,\" primarily based on the fact that the adopted SynthID-Text algorithm preserves the original distribution in expectation. However, could a specific watermarked dataset generated with a single key introduce subtle yet perceptible biases for downstream tasks? A more nuanced discussion of the \"distortion-free\" property is advised, along with considering evaluations of the rewritten dataset on a broader range of distribution-sensitive tasks.\n3.  The experimental section compares TRACE with DE-COP as a black-box baseline, but the setups for these two methods are fundamentally different: TRACE actively modifies the dataset to embed a signal, whereas DE-COP performs passive detection on original data. This comparison may not fully and fairly reflect the respective strengths and weaknesses of each method in its intended scenario. It is suggested to clarify this setup difference more explicitly in the discussion and to emphasize that TRACE is designed for \"proactive\" copyright protection scenarios."}, "questions": {"value": "1.  Regarding the entropy-gating mechanism: Could you please detail the selection criteria for the auxiliary model? In your experiments, did the auxiliary model share the same architecture as the suspect model, or was it different? Have you assessed how much TRACE's detection power degrades when there is a significant mismatch in scale (e.g., 3B vs. 70B) or family (e.g., Llama vs. Qwen) between the auxiliary and suspect models?\n2.  Regarding the watermarked rewriting process: Table 3 shows that the PPL of the rewritten text is sometimes even lower than the original, which seems counter-intuitive. Could you explain why this might be the case? Does this suggest that the rewriting process systematically simplifies the text, or could it be related to the base model used for calculating PPL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QJP58ZeOn3", "forum": "ROcFqn6Pxi", "replyto": "ROcFqn6Pxi", "signatures": ["ICLR.cc/2026/Conference/Submission18145/Reviewer_Q6UN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18145/Reviewer_Q6UN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804429703, "cdate": 1761804429703, "tmdate": 1762927903804, "mdate": 1762927903804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TRACE, a method that detects dataset usage in finetuning of LLMs. They test their methods against other membership inference methods, on a model that they finetuned themselves. Results show that TRACE works, and a “unique” signal can be embedded in the text (multi-dataset attribution). TRACE is better than DE-COP and has some other empirical properties as well."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "S1. Has many results and shows several properties of TRACE watermarking. TRACE is better than DE-COP."}, "weaknesses": {"value": "W1. I’m not really convinced by the test setting. I’ve unfortunately reviewed several of these kinds of membership inference papers, and they conduct membership inference on a model that they trained themselves. The results in the paper can be made arbitrarily stronger or weaker by choosing a different training setting (e.g. if I finetune for 100 epochs instead of 2, any membership inference method will be able to do well). \n\nW2. If the setting were to be believed, I don’t feel that ideas in the paper are novel. Gu et al., 2024 (https://arxiv.org/abs/2312.04469) have already conducted very similar experiments, and with a motivation that makes way more sense. They are thinking from the perspective of open-weight model developers, who may want to mark their models before release, so they can train their models as much as they want on watermarked text.\n\nW3. I feel like that TRACE is robust to continued pretraining is sort of interesting. Again, this result is very selectively presented. Even though there is 2.94M examples in OpenOrca, how many steps of continued pretraining is it? How does it compare to the number of steps taken in finetuning?"}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ceEGUH7xh4", "forum": "ROcFqn6Pxi", "replyto": "ROcFqn6Pxi", "signatures": ["ICLR.cc/2026/Conference/Submission18145/Reviewer_BSae"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18145/Reviewer_BSae"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964861573, "cdate": 1761964861573, "tmdate": 1762927903283, "mdate": 1762927903283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TRACE, a black-box methodology designed to determine if proprietary datasets were used for fine-tuning Large Language Models (LLMs). The core mechanism involves subtly watermarking the original dataset, specifically using SynthID [1] before its release. Detection is subsequently carried out via a hypothesis test that leverages the radioactivity of these watermarks within outputs generated by suspected LLMs trained on the watermarked data. Experimental results confirm the effectiveness of the detection performance. The authors also demonstrate that the rephrased datasets retain their utility for fine-tuning purposes. Furthermore, the study shows that TRACE achieves superior performance compared to several existing gray-box approaches for membership detection.\n\nReferences: \n\n[1] Scalable watermarking for identifying large language model outputs, Nature 2024"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-written and easy to follow.\n* The experiments are comprehensive covering a range of datasets (there is a slight overemphasis on benchmarks) and model families demonstrating the generalizability of the algorithm.\n* The method clearly works and demonstrates strong detection performance with significantly low p-values.\n* TRACE is robust against false positives and rephrased datasets also preserve their utility."}, "weaknesses": {"value": "* The contribution lacks novelty and the key idea of repurposing watermarks to detect dataset membership has already been explored in prior works [2,3], with [3] also leveraging radioactivity of watermarks. While the authors cite [2], the work is missing discussion or comparison explaining how TRACE differs from or improves upon these existing works.\n\n\n* The authors claim that using  SynthID [1] leads to distortion-free rewrites [line 75..], which is inaccurate. SynthID is distortion-free in the sense that over a large set of random keys, the output distribution is preserved in expectation. However, this does not imply that rewrites of a specific dataset generated with a fixed key are distortion-free: the watermarked outputs will systematically differ from unwatermarked outputs. The authors should clarify this distinction to avoid confusion.\n\n* The authors propose an \"entropy-gated\" detection procedure that scores only high-entropy tokens. However, this idea has been previously explored in the watermarking literature [4], and the authors should cite this work and discuss how their application differs, if at all.\n\n\n* The paper lacks implementation details, specifically the exact hyperparameters used for SynthID when generating the rephrased datasets. Additionally, the authors should include representative examples of original, watermarked/rephrased text pairs along with completions from the suspect model in the appendix to aid understanding and reproducibility. There’s also missing details around sample sizes of the benchmarks used in the paper.\n\n\n* The current experimental setup centers on a supervised fine-tuning (SFT) paradigm that intentionally utilizes watermarked data. Given this focus, the authors must also assess their proposed method's robustness in adversarial contexts, particularly those where model owners might actively attempt to evade the watermark detection process. Note that this setup differs from [2,3] in the sense that they focus on a scenario when datasets are accidentally leaked in a \"pretraining\" dataset.\n\n\nReferences: \n\n[1] Scalable watermarking for identifying large language model outputs, Nature 2024\n\n[2] STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings. ICML 25\n\n[3] Detecting Benchmark Contamination Through Watermarking, ICLR 25 workshop on Watermarking\n\n[4] An Entropy-based Text Watermarking Detection Method, ACL 24"}, "questions": {"value": "* The p-values for detecting copyrighted dataset usage are extremely low (e.g., around 10^-172). For context and to better understand the detection mechanism, can the authors report the watermark strength (e.g., z-score) or p-values when directly testing the original rephrased dataset before it is used for training?\n\n\n* Can the authors provide more details about the training procedure? In a standard SFT setup, the loss is computed only on the completions, meaning the model is trained exclusively on watermarked responses whereas in the paper for ARC and MMLU the detection is performed on the completions of the “questions”? [acc to prompt templates on page 16.]\n\n\n* Since watermarking operates at the token level and depends on the previous token context, does using a different tokenizer in the suspected model affect detection performance? Different tokenizers will segment text differently, potentially disrupting the token-level watermark pattern and the contextual dependencies it relies on.\n\n\n* Will the code be made publicly available upon the paper's publication?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6VgQooZzW5", "forum": "ROcFqn6Pxi", "replyto": "ROcFqn6Pxi", "signatures": ["ICLR.cc/2026/Conference/Submission18145/Reviewer_ym9J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18145/Reviewer_ym9J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973186589, "cdate": 1761973186589, "tmdate": 1762927902653, "mdate": 1762927902653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a black box detection method for telling if copyrighted content was trained on. They call it TRACE. The idea is to use watermarked rewrite of the dataset (using SynthID-Text) and then later query a suspect model to tell if it has been effective by the radioactivity of the watermark. Or in technical terms,  run an entropy-gated, one-sided Z-test over token-level watermark scores. Did the model learn this bias during fine-tuning?\n\nThe method reports strong results across QA, MCQ, and text-only corpora and multiple families (Llama-3B, Phi-3, Qwen-7B), along with multi-dataset attribution and robustness to continued pretraining. \n\nSince the method is based on rewrites, the paper also evaluates quality preservation and task utility of the rewrites."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The black-box threat model is very important for the community to focus on, as majority of llms are served via APIs. The work addresses dataset-use verification for closed models, avoiding logits/reference models that many MIAs and dataset-inference methods require. \n\n2. The keyed rewrite followed by entropy-gated radioactivity test is simple to implement and statistically interpretable.\n\n3. The watermarked rewrites maintain semantic similarity and fluency and largely preserve task performance similar to the original data"}, "weaknesses": {"value": "## “Distributional neutrality” vs. a fixed watermark key.\nAveraging over random keys recovers the base next-token distribution. But TRACE fixes one key per dataset. With a fixed key, SynthID-Text must introduce small, systematic token-frequency biases (that’s the watermark). Even if semantics and fluency are preserved (Tables 3–4), a large keyed corpus can shift n-gram statistics and create a key-specific “micro-dialect.”\nWhy it matters: It complicates dataset inference assumptions (IID / matched reference) and may leak stylistic signals unrelated to membership. The paper should quantify this with KL divergences over n-grams, perplexity shifts under a base LM, and a classifier test (watermarked vs. original) to show the shift is negligible at the corpus scale. Look at this work on Blind Baselines for MIAs that work by detecting such statistical skews between two distributions: https://arxiv.org/html/2406.16201v1\n\n## Multi-key interference / key collisions.\nThe paper shows multi-dataset attribution with two keys (Table 6), but not the more realistic case where different owners watermark their respective datasets, and they both go into the training mix. Mixed-key training can dilute radioactivity for each key, or create ambiguous partial signals. Please add a stress test with overlapping keyed corpora and report both power and false positives.\n\n## Missing head-to-head with the closest contemporaries.\nBaselines include DE-COP and gray-box MIAs (Table 1), which is useful, but omits the most pertinent modern comparators: paraphrase-based watermark attribution with paired testing (e.g., STAMP-style) and provenance frameworks like Waterfall. A matched-setting comparison (black-box budget, same prompts, same token budgets) would clarify how much lift comes from entropy-gated radioactivity versus the watermark backend."}, "questions": {"value": "1. What is the measured distribution shift (KL over n-grams, LM PPL deltas) between original vs. SynthID-Text rewrites? or any other ways to measure this (look at blind MIAs for inspiration: https://arxiv.org/html/2406.16201v1)\n2. How does detection behave under overlapping keys (two owners watermarking respective data)?\n3. Do people like the rewrites? Is this something authors/creators will be okay with?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xqmOZN8rdX", "forum": "ROcFqn6Pxi", "replyto": "ROcFqn6Pxi", "signatures": ["ICLR.cc/2026/Conference/Submission18145/Reviewer_QTko"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18145/Reviewer_QTko"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762414429034, "cdate": 1762414429034, "tmdate": 1762927901991, "mdate": 1762927901991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}