{"id": "EvigHapfYj", "number": 13987, "cdate": 1758226519729, "mdate": 1759897398106, "content": {"title": "KPI-Chain: Multi-Agent Planning with Entity-Based Task Chaining for Reliable Recovery", "abstract": "Large language model (LLM) agents and multi-agent systems promise flexible\nproblem-solving, but they remain brittle: plans often fail silently, and\nexisting approaches lack mechanisms for reliable recovery. We propose a\ngeneric multi-agent planning framework called KPI-Chain with a novel plan\ndesign that integrates per-task key performance indicators (KPIs) based on\nentity extraction. In our formulation, each task—whether a tool call or a\nreasoning step—is associated with a set of expected entities extracted from\nits output. These entities are typed, and they serve both to determine task\nsuccess and to populate the input parameters of subsequent dependent tasks\nretrieved from a task registry. If the KPI is not met, the system automatically\ntriggers replanning with failure feedback, enabling reliable recovery\nfrom failure. To support this design, we introduce a JSON-path memory\nrepresentation for structured, queryable, and type-aware state tracking. We\nintegrate with Model Context Protocol (MCP) servers for standardized tool\naccess and use chain-of-thought prompting for reasoning tasks. Across 5\nchallenging benchmarks, our KPI-Chain framework achieves higher success\nrates compared to existing agent architectures including ReAct and Planand-\nExecute. These results suggest that KPI-driven planning with typed,\nentity-based task chaining provides a foundation for building more reliable\nand adaptive multi-agent systems.", "tldr": "", "keywords": ["multi-agent systems", "large language models", "automated planning", "failure recovery", "entity extraction", "key performance indicators", "task chaining", "JSON-path memory", "Model Context Protocol", "agent reliability"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48cfe3e48aa692ab7a7dfb4f46ed31733ddab358.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identifies a key problem with current LLM agents: they fail silently and lack robust recovery mechanisms. To address this, it introduces KPI-Chain, a multi-agent framework where every task in a plan has defined Key Performance Indicators (KPIs) based on extracting typed entities (like names and times). It suggests some components including KPIs, memory, and adaptive planning.  On five benchmarks, they show the proposed KPI-Chains to be effective."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work points out the weaknesses of LLMs when applied to long-horizon planning tasks. \n\n2. The authors proposed a KPI-chain framework to provide  a clear, objective standard for success/failure. \n\n3. Its continuation-based replanning is a recovery mechanism, which current LLMs cannot natively support."}, "weaknesses": {"value": "Unfortunately, the proposed solutions are heuristic-based and ad-hoc. The authors have neglected the rich history of work in the database community, particularly transaction processing research from the 1980s onwards. This paper identifies real LLM shortcomings, but its solutions need significant improvement. Specifically:\n\n**Very High Computational Cost:** The system uses way too many LLM calls. An average of 2,447 calls per question is extremely expensive and not practical for real applications. This causes 4x higher latency than ReAct, showing a brute-force approach rather than an efficient solution. The paper is honest about this cost, but being honest does not fix the problem.\nLimited Scope: The framework only works for information extraction and multi-hop QA tasks. The authors admit it does not work well for creative or subjective tasks. This limits its usefulness significantly. The field needs general-purpose agent systems, but KPI-Chain only solves a narrow set of problems.\n\n**Context Chunking Problem:** Over 50% of LLM calls are for entity extraction from chunked contexts. This shows the main innovation is just a workaround for LLMs' difficulty with long documents, not a real advance in planning or state management. The system's performance depends heavily on this pre-processing step, which is fragile.\n\n**Limited Novelty:** The core ideas of entity-based KPIs and continuation-based replanning are heuristic approaches to problems already solved more formally in prior work. Compared to frameworks like SagaLLM (VLDB 2025), which provides proper transactional guarantees based on database theory, KPI-Chain's contribution seems incremental and ad-hoc.\n\n**Weak Baseline Comparisons:** The paper only compares against ReAct and Plan-and-Execute, which are now basic baselines. It does not compare against more advanced recent frameworks that also handle reliability and state management. This makes it hard to judge the real contribution."}, "questions": {"value": "**Q1. Scalability and Cost Mitigation:** Given over 2,400 LLM calls per question is computationally unsustainable, what specific architectural changes or optimizations could reduce this cost by an order of magnitude without sacrificing the core performance benefits? Is the entity-chunking approach fundamentally incompatible with efficiency?\n\n**2. Generalization Beyond QA:** The framework is explicitly noted as being unsuitable for creative or subjective tasks. How could the principle of entity-based KPIs be extended to more open-ended domains like design, strategy, or negotiation, where the *entities* are not well-defined? Does this limitation suggest a fundamental constraint of the approach?\n\n**Q3. Theoretical Foundations:** How does your *continuation-based replanning* formally differ from and improve upon established transaction rollback mechanisms, such as the compensation actions in the Saga pattern (Salem 1987)? What are the specific theoretical guarantees of your recovery mechanism regarding state consistency and convergence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JDx1TtobLh", "forum": "EvigHapfYj", "replyto": "EvigHapfYj", "signatures": ["ICLR.cc/2026/Conference/Submission13987/Reviewer_YLFn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13987/Reviewer_YLFn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761078728790, "cdate": 1761078728790, "tmdate": 1762924484667, "mdate": 1762924484667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes KPI-Chain, a generic multi-agent planning framework. Its core idea is to integrate per-task KPI based on entity extraction, a JSON-Path structured memory system, and a continuation-based replanning mechanism. It judges task success by verifying typed entities with sufficient confidence in task outputs, and meanwhile reuses valid entities to achieve recovery from failure points. Experiments show that its performance on LOFT-series benchmarks and HotpotQA outperforms the two baseline frameworks, ReAct and Plan-and-Execute."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "A multi-agent framework is proposed, and experimental results show that this framework outperforms the ReAct and Plan-and-Execute frameworks on the LOFT and HotpotQA benchmarks."}, "weaknesses": {"value": "- The technical details of the paper’s core method, entity-based KPI evaluation, are vaguely described, and the calculation method of confidence is not clarified. Additionally, JSON-structured representation, continuation-based replanning at failure points, and MCP are all basic technologies widely used in the multi-agent field. \n    \n- The baseline only compares two early methods, ReAct and Plan-and-Execute, and does not benchmark against current state-of-the-art multi-agent frameworks, making it impossible to prove the effectiveness of this method.\n\n- The five benchmarks claimed in the paper essentially cover only two types of scenarios, and no ablation experiments are conducted.\n\n- The paper’s writing lacks clarity, and the presentation of figures is rough."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2a1zTzIArl", "forum": "EvigHapfYj", "replyto": "EvigHapfYj", "signatures": ["ICLR.cc/2026/Conference/Submission13987/Reviewer_NSN3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13987/Reviewer_NSN3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823244345, "cdate": 1761823244345, "tmdate": 1762924483982, "mdate": 1762924483982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents KPI-Chain, a multi-agent planning framework that focuses on structuring reasoning into a pre-defined YAML format. The system consists of a Planner to generate the YAML sent to an MCP server, an Entity Extractor Agent that validates the output of the server, a Reasoner to improve entity extraction, a Replanner to handle failures, and a JSON-Path Global memory for storing and retrieving past responses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The system is easy to understand, achieves an improvement in performance and token efficiency over comparable baselines."}, "weaknesses": {"value": "- No prompts in the Appendix\n- The system focuses around a structured YAML format which isn’t necessarily novel [1]\n- No failure analysis or ablations over the components of the system\n\n[1] Executable Code Actions Elicit Better LLM Agents"}, "questions": {"value": "My main concerns are (1) the lack of a failure analysis specifically for the KPI-Chain framework and (2) the lack of ablations over the system. While there is notable performance improvement and token efficiency, there is no insight to a reader on where to focus to improve the system except for the following note\n“The high number of LLM calls (2,447 average) primarily stems from our large context chunking approach for entity extraction, suggesting that optimized entity extraction algorithms capable of handling longer contexts without splitting represent a critical research priority.”\nwhich suggests that the efficiency of the system should be improved while there is still a large performance gap to close.\n- What are the main failure modes of KPI-Chain?\n- How does the system perform without the Reasoning agent? Without the Re-planner agent? Without the Entity Extractor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0PgeNvNxPo", "forum": "EvigHapfYj", "replyto": "EvigHapfYj", "signatures": ["ICLR.cc/2026/Conference/Submission13987/Reviewer_71u3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13987/Reviewer_71u3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991826875, "cdate": 1761991826875, "tmdate": 1762924483322, "mdate": 1762924483322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}