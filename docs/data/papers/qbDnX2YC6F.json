{"id": "qbDnX2YC6F", "number": 21533, "cdate": 1758318606965, "mdate": 1759896917236, "content": {"title": "Open-Set Recognition Interaction Effects: Modular Gains and Where to Find Them", "abstract": "Open-set recognition (OSR) requires neural networks to classify known classes while rejecting unknown samples, which is critical for real-world deployment. So far, OSR research studied and developed representation learning and postprocessing methods independently and their interaction effects remain unexplored, leaving potential performance gains untapped. In this paper, we present the first systematic study of these interactions across dataset scales and auxiliary data usage. First, we discover a failure mode we term magnitude collapse, where representation learning methods that utilize auxiliary data can suffer performance degradation at large scale and irreversibly destroy discriminative information, despite excelling at small scale. Second, we study the interaction effects between representation learning and postprocessing methods, and reveal when they can be leveraged for modular performance gains via two-stage processing.\nWe also show where interaction effects amplify performance degradation due to magnitude collapse. Third, we show how these findings can be used to achieve state-of-the-art performance with a simple baseline and two-stage processing of OSR techniques. Finally, our results demonstrate that small-scale evaluations with auxiliary data are not predictive of large-scale performance, invalidating current best practices in OSR research.", "tldr": "", "keywords": ["open-set recognition", "interaction effects", "auxiliary data", "feature magnitude", "large-scale evaluation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/95f1cd8cbf97b02009616e49b1f950a005663dc2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies interaction effects between representation learning (RL) and post-processing (PP) in open-set recognition (OSR) using a modular two-stage framework (RL+PP). Across small-scale (CIFAR+N) and large-scale (ImageNet P1–P3) protocols, it shows that auxiliary-data RL methods, which manipulate feature magnitudes (e.g., OE, ObjectoSphere), can degrade at scale due to a newly identified failure mode, magnitude collapse, feature norms of some known classes shrink toward the origin when auxiliary and known classes are semantically similar, yielding imbalanced class-wise CCR and poorer OSR despite gains on small datasets. Conversely, non-magnitude-manipulating RL (notably AddON, i.e., a K+1 background class) synergizes with magnitude-aware PP (e.g., PostMax, GHOST) to produce additive gains. \n\nThe key contributions are: (i) the first systematic analysis of RL-PP modular interactions, (ii) discovery and analysis of magnitude collapse, and (iii) practical guidance showing AddON + MA-PP as a robust recipe and that small-scale auxiliary-based evaluations are not predictive of large-scale performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Regarding originality, this is the first systematic analysis of interaction effects between RL and PP for OSR, framing OSR as a modular two-stage pipeline and introducing the magnitude-collapse failure mode, which explains why magnitude-manipulating RL degrades at scale when auxiliary and known classes are semantically similar.\n\nIn terms of quality, the study design is meticulous: five RL × five PP methods are combined, trained (mostly) from scratch to prevent leakage, and evaluated across both small-scale CIFAR+N and large-scale ImageNet P1–P3 protocols, using appropriate OSR metrics and a clear decomposition of RL versus PP gains, enabling fair attribution of effects.\n\nOn clarity, the paper clearly formalizes the RL+PP decomposition, defines decision rules, and provides intuitive visual and quantitative evidence (feature-magnitude distributions, regression linking class-wise CCR to feature norms, and heatmaps of AUOSCR/OOSA) that make the interaction story understandable. On significance, the results provide actionable guidance: non-MM RL, such as AddON combined with magnitude-aware PP (PostMax, GHOST), gives additive gains, whereas MM RL paired with MA PP should be avoided at high similarity; moreover, small-scale auxiliary-based wins do not predict large-scale behavior, which has immediate implications for benchmarking and deployment practices in OSR."}, "weaknesses": {"value": "Although the authors provide a thorough modular evaluation, all large-scale experiments utilize a single ResNet backbone, leaving uncertainty about whether the identified interaction effects and magnitudes persist across other architectures, such as transformers, ConvNeXt, or contrastive self-supervised encoders. Incorporating these architectures would clarify if the observed norm-related behavior stems from the backbone’s feature geometry or from the learning principle itself. \n\n Moreover, statistical rigor is lacking: most large-scale results appear single-seeded without confidence intervals or significance tests, and several reported gains fall within plausible noise margins. Multi-seed averages, confidence intervals, and effect-size reporting would strengthen reliability. \n\nThe analysis of magnitude collapse, while intuitively presented, remains purely correlational; providing a geometric or probabilistic explanation of how auxiliary similarity drives norm shrinkage would deepen insight. Additionally, the work could explore hyperparameter sensitivity and mitigation strategies for the collapse phenomenon by systematically varying λ and ξ in OE/OS and visualizing stability regions. Baseline fairness could be improved through compute-normalized comparisons and consistent tuning across RL/PP methods. \n\nFinally, the practical impact would benefit from an automatic diagnostic that detects early signs of magnitude collapse or misaligned RL–PP pairings during training."}, "questions": {"value": "1. Do the RL–PP interaction patterns and magnitude behavior persist with modern backbones beyond ResNet (e.g., ViT/DeiT, ConvNeXt, Swin) and with contrastive self-supervised encoders (e.g., MoCo-v3, DINOv2)?\n   \n2. Are the reported improvements stable across random seeds and training noise, and which results remain significant after controlling for multiple comparisons across the RL×PP grid?\n\n3. What concrete mechanism links auxiliary-known semantic similarity to feature-norm shrinkage and class-wise CCR imbalance, beyond observed correlations?\n   \n4. Can collapse be prevented or reduced by tuning OE/OS hyperparameters (λ, ξ) or by simple regularizers (norm floors, margin constraints, temperature scaling)?\n   \n5. Are baseline methods trained and tuned under comparable compute, data budgets, and augmentation/search spaces, and could budget asymmetries explain small gains?\n   \n6.  Can practitioners detect early during training when a given RL–PP pairing is at risk of magnitude collapse or harmful interaction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vJqo5BZpv1", "forum": "qbDnX2YC6F", "replyto": "qbDnX2YC6F", "signatures": ["ICLR.cc/2026/Conference/Submission21533/Reviewer_rzub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21533/Reviewer_rzub"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761195526969, "cdate": 1761195526969, "tmdate": 1762941822819, "mdate": 1762941822819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose analyzing the interactions between representation\nlearning (RL) and post processing (PP, post-hoc methods to add\nopen-set capabilities) in Open Set Recognition (OSR) in a modular\n2-component structure.  Particularly they analyze (feature) magnitude\nmanipulation (MM) in RL and magnitude-aware (MA) methods in PP.  Some\nRL methods use auxiliary data/classes to representation samples of\nunknown classes.\n\nFor the analysis, they use 5 existing RL (3 used auxiliary data) and 5\nexisting PP methods (3 are MA) over 2 datasets.  With large-scale\ndata, they find using auxiliary data does not improve performance.\nHowever, with small-scale data, using auxiliary data generally improves\nperformance.\n\nTo understand why MM methods with auxiliary data degrade in\nperformance, they analyze magnitude vs performance and find positive\ncorrelation.  Also, increasing similarity (via the P1 to P3 protocols)\nbetween known and auxiliary samples, MM methods learn stronger\nrelationships.  They find that high similarity between auxiliary and\nknown classes can degrade the performance of MM methods.  They call\nthe phenomenon magnitude collapse.  To reduce magnitude collapse, they\nfind Additional Output Node (AddON) for the auxiliary data is\nbeneficial.\n\nWithout using auxiliary data, they find that RL and PP are independent\nand hence any methods from RL and PP can be paired without the\ncontributions from one being degraded by another."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  Investigating the interactions between representation learning\n(RL) and post processing (PP) in Open Set Recognition (OSR) in a\nmodular 2-component structure is interesting.\n\n2.  The analysis indicates that with auxiliary data, similarity between\nauxiliary and known classes can degrade Magnitude Manipulation (MM)\nmethods.  Using AddON with MM can reduce the issue.\n\n3.  Also, the analysis indicates that without auxiliary data, RP and PP\nare independent and can be paired without interference."}, "weaknesses": {"value": "1.  The auxiliary data are intended to represent the unknown classes,\nso high similarity to known classes is generally not desirable.\nConsequently, the findings are not surprising.\n\n2.  While MM methods degrade when auxiliary data are similar to known\nclasses, how AddON can reduce the issue is not clear.\n\n3.  Existing methods are analyzed, but new methods are not introduced."}, "questions": {"value": "1.  Why does AddON reduce magnitude collapse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EaJ5huDrCK", "forum": "qbDnX2YC6F", "replyto": "qbDnX2YC6F", "signatures": ["ICLR.cc/2026/Conference/Submission21533/Reviewer_bxry"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21533/Reviewer_bxry"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848006576, "cdate": 1761848006576, "tmdate": 1762941822575, "mdate": 1762941822575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first systematic study of the interaction effects between Representation Learning (RL) and Post-Processing (PP) methods in Open-Set Recognition (OSR). The authors introduce a modular, two-stage framework to analyze these combinations, identifying a key failure mode termed \"magnitude collapse\" that affects certain RL methods at large scale. They propose a simple yet effective baseline (AddON) to mitigate this issue and provide actionable guidelines for combining RL and PP methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core contribution—systematically studying the interaction between RL and PP—is highly novel and impactful for the OSR field.\n2. The experimental setup is rigorous and thorough. \n3. The (re-)introduction and thorough evaluation of AddON as a powerful and simple baseline is a significant contribution."}, "weaknesses": {"value": "1. While the paper's title and thesis revolve around \"modular gains,\" the quantitative evidence for the practical significance of these gains is somewhat lacking. \n2. While the paper compares to canonical RL/PP methods (e.g., OE, OpenMax), it omits recent state-of-the-art OSR approaches that may interact differently with PP methods."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YnWbr29E0a", "forum": "qbDnX2YC6F", "replyto": "qbDnX2YC6F", "signatures": ["ICLR.cc/2026/Conference/Submission21533/Reviewer_zUuP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21533/Reviewer_zUuP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903858564, "cdate": 1761903858564, "tmdate": 1762941822331, "mdate": 1762941822331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically analyzes Open-Set Recognition (OSR) as a modular, two-stage framework combining Representation Learning (RL) and Postprocessing (PP). Its central contribution is the discovery of \"magnitude collapse,\" a failure mode where popular magnitude-manipulating (MM) methods like Outlier Exposure fail at scale. The authors show this occurs when high similarity between known and auxiliary data causes the model to irreversibly destroy feature magnitude information. They contrast this with the simple, non-MM AddON ($K+1$ classifier), which remains robust. The paper's method is to study the \"interaction effects\" between different RL and PP components, concluding that small-scale benchmarks are misleading and that robust performance comes from the correct combination of methods (e.g., AddON + PostMax), not a single \"best\" component."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Key Insight on Feature Magnitude: The paper's primary strength is its insightful diagnosis of the \"magnitude collapse\" failure mode. This provides a clear, testable explanation for why popular methods that excel on small-scale benchmarks (like OE) fail on large-scale ones, linking the failure to high semantic similarity between known and auxiliary data.\n\n - Marginal SOTA Improvement: While the paper successfully identifies a robust combination (AddON + PostMax), the resulting performance gains over other strong, existing combinations (e.g., CE + GHOST or ARPL + GHOST) are often marginal. For instance, on the $P_3$ benchmark (Figure 2, AUOSCR), the proposed AddON+PostMax (79.7) is only a minor improvement over ARPL+GHOST (79.2), suggesting the baseline was already very strong."}, "weaknesses": {"value": "- Limited Compatibility of AddON: The AddON method's objective creates a representation that is incompatible with many existing OOD detectors. AddON trains the model to produce high-magnitude signals for unknowns (at its $K+1$ node), which directly contradicts the core assumption of many feature-norm-based detectors that expect low-magnitude signals for unknowns. This limits the \"modular\" combinations to only those PP methods that can be adapted to AddON's specific logic.\n\n - Oversimplification of the Unknown Space: The AddON method relies on a $K+1$ classifier, which fundamentally models the entire, infinitely diverse \"unknown\" space using a single prototype vector (the weights for that node). This is a significant oversimplification that likely only works on benchmarks where the unknown classes have limited diversity or happen to be well-represented by the specific auxiliary data used.\n\n- Entangled Evaluation Metrics: The paper's main metrics for OSR performance (AUOSCR and OOSA) entangle unknown detection performance and closed-set classification accuracy into a single score. This can be misleading, as a method could improve in one aspect while regressing in the other. For example, the paper's results in Figure 8 show that AddON has a slightly worse closed-set accuracy than ARPL on ImageNet $P_3$, but its detection (AUROC) is stronger. The final AUOSCR score obscures this trade-off."}, "questions": {"value": "Please refer to the above weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LPq3thK6Yi", "forum": "qbDnX2YC6F", "replyto": "qbDnX2YC6F", "signatures": ["ICLR.cc/2026/Conference/Submission21533/Reviewer_YC14"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21533/Reviewer_YC14"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762080381127, "cdate": 1762080381127, "tmdate": 1762941821983, "mdate": 1762941821983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}