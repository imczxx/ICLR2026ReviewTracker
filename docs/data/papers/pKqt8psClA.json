{"id": "pKqt8psClA", "number": 3448, "cdate": 1757429496298, "mdate": 1763664169287, "content": {"title": "HAI-Eval: Measuring Human-AI Synergy in Collaborative Coding", "abstract": "LLM-powered coding agents are reshaping the development paradigm. However, existing evaluation systems, neither traditional tests for humans nor benchmarks for LLMs, fail to capture this shift. They remain focused on well-defined algorithmic problems, which excludes problems where success depends on human-AI collaboration. Such collaborative problems not only require human reasoning to interpret complex contexts and guide solution strategies, but also demand AI efficiency for implementation. To bridge this gap, we introduce HAI-Eval, a unified benchmark designed to measure the synergy of human-AI partnership in coding. HAI-Eval's core innovation is its \"Collaboration-Necessary\" problem templates, which are intractable for both standalone LLMs and unaided humans, but solvable through effective collaboration. Specifically, HAI-Eval uses 45 templates to dynamically create tasks. It also provides a standardized IDE for human participants and a reproducible toolkit with 450 task instances for LLMs, ensuring an ecologically valid evaluation. We conduct a within-subject study with 45 participants and benchmark their performance against 5 state-of-the-art LLMs under 4 different levels of human intervention. Results show that while standalone LLMs and unaided participants achieve poor pass rates (0.67% and 18.89%), human–AI collaboration significantly improves performance to 31.11%. Our analysis reveals an emerging co-reasoning partnership. This finding challenges the traditional human-tool hierarchy by showing that strategic breakthroughs can originate from either humans or AI. HAI-Eval establishes not only a challenging benchmark for next-generation coding agents but also a grounded, scalable framework for assessing core developer competencies in the AI era. Our benchmark and interactive demo are openly accessible.", "tldr": "", "keywords": ["Coding Benchmark", "Developer Evaluation", "Human-AI Collaboration", "Coding Agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ddca2124a922a896e6127d18d1977ae426ab17a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper creates a method to create tasks for human-AI collaboration, an interface, and conducts a user study evaluating the performance of collaboration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "I find the user study and interface good contributions, but the most relevant contribution the approach the paper takes to the creation of tasks that are ecologically relevant but neither are solvable by humans nor LLMs alone. For me, this is the main contribution, and the rest is an evaluation of this method of creating tasks."}, "weaknesses": {"value": "Some of the evaluations seem as if they are satisfied by construction. E.g., the fact that SOTA LLMs can't solve the tasks is in the specification of the task creation algorithm."}, "questions": {"value": "- What part of the evaluations is \"forced\" by how you design the tasks?\n- How similar to \"real use\" of LLMs are the tasks you are creating.\n- [RealHumanEval](https://arxiv.org/abs/2404.02806) and the papers listed in [Centaur Evals](https://openreview.net/forum?id=LkdH35003E) could be additional helpful related literature."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jGyLcBZWEq", "forum": "pKqt8psClA", "replyto": "pKqt8psClA", "signatures": ["ICLR.cc/2026/Conference/Submission3448/Reviewer_jwbk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3448/Reviewer_jwbk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850488110, "cdate": 1761850488110, "tmdate": 1762916728692, "mdate": 1762916728692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HAI-Eval, a benchmark for human-AI collaborative coding. They contribute a framework for generating items that are meant to be difficult for humans and AI in isolation, but that together, humans and AI are able to do better on the tasks. They contribute a standardized human eval protocol as well as an automated evaluation toolkit for LLMs, and perform experiments comparing LLM performance, human performance, and LLM + human performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This is a really important direction. Regarding originality, I am familiar with some work evaluating how AI might enhance human performance, but not a standardized benchmark for comparing human-AI teaming with just one of these factors. Trying to construct problems that expose the value of human-AI partnership is a unique approach that I think is conceptually very interesting. Regarding significance, the domain should broadly be of interest to many, as increasingly, human-AI teaming is the norm for software development. The paper is generally easy to understand, and considerable thought was put into evaluation construction as well as the human experiments."}, "weaknesses": {"value": "While I think the approach of finding problems that are AI-incomplete but that are amenable to human reliance on AI for parts is a really interesting one, I’m left with a question: is the way that this is done meant to capture the real ways that humans and AI complement each other, and if so, does it accomplish this? The construction of this benchmark seems to rely on some intuitive building blocks for how humans and AI complement each other (e.g. humans providing clarification and decomposition, helping with diagrams), but also, could the way these items are constructed be a bit artificial? I have a hard time deciding, here, and would (ideally) want to understand what more in-the-wild data says about what’s effective in teaming.\n\nRelated, aspects of how the evaluation captures good partnership in a controlled way seems a bit opaque. Two related questions that seem quite light on important details in this paper. (1) How do you measure success when the task is ambiguous and requires human clarification? Might precisely how the human clarifies the task affect the difficulty of the task itself, if it’s left ambiguous ahead of time? And (2) how does the static LLM eval toolkit accomplish simulating interaction with a human user? I’m very confused by what that static evaluation is meant to measure, as 4.4 seems to say that it’s meant to simulate interaction with a human via HAI-EC (if so, how is HAI-EC engineered?), whereas in 5.1, HAI-EC is said to be used in C0. It would be really helpful to clarify these points.\n\nThe interpretations of Finding 1 in results seem unsupported. There is a claim that “(LLMs) are unable to perform higher-order reasoning tasks”, but the ways the problems have been constructed to be AI-incomplete (e.g. with incomplete instructions) don’t seem to support making such a claim — what’s higher-order reasoning, and what are the other ways (e.g. interpreting diagrams, clarifying) that lead to errors?"}, "questions": {"value": "Aside from the ones I couldn’t disentangle from the above weaknesses:\n\n1. How are the C1 conditions chosen? How is this standardized? The presentation in the appendix makes it seem like there might be some variation in precisely how this is done.\n2. How are the difficulty levels determined? How are they calibrated?\n3. How do you establish when humans take a “fundamentally different approach”, as described in Finding 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yGvG5oWKSU", "forum": "pKqt8psClA", "replyto": "pKqt8psClA", "signatures": ["ICLR.cc/2026/Conference/Submission3448/Reviewer_NyJz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3448/Reviewer_NyJz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964014799, "cdate": 1761964014799, "tmdate": 1762916728499, "mdate": 1762916728499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new benchmark system that evaluates AI coding assistants in vivo. They validate this benchmark by showing that the results on this benchmark differ dramatically from those of a human coder alone or an AI coder alone."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- I like the idea. It does indeed identify a key gap in existing benchmarks. It also constructs the benchmark in a way that fits a lot with my own experiences on where Agentic AI coding systems are most useful."}, "weaknesses": {"value": "- I think my biggest criticism (which is a relatively small one) is that the sample of participants is very biased. This should be mentioned in the main text. I think it would be sufficient to note the biggest biases in the sample in the main text, namely that all the participants identified as East Asian and that all of the participants regularly use AI coding assistants (this would just imply to the reader that some care needs to be taken when drawing conclusions). It should also be noted (though in the appendix is fine) whether participants were recruited among personal acquaintances of the authors. This isn't a dealbreaker, but should be noted.\n- It's not completely true that other assessments assume a perfectly defined problem. I would add the clause \"most\" into that. This is false in two ways: (1) LLM-based evaluations (e.g., LLM-as-a-Judge) allow more soft definitions of a task, and (2) advanced versions of that (e.g., Agent-as-a-Judge) allow one to evaluate on much higher-level definitions of the problem, rendering the \"perfectly defined problem\" not a limitation. It would be worth mentioning these kinds of evaluation strategies here.\n- Figure 2 could be cleaned up a bit. I think you could simplify this quite a bit. I find in-figure sentences are useful in annotating what's going on there.\n- What is this Anti-LLM? It's only mentioned in the conclusion and a figure. Maybe avoid the new notation if it's not really used?\n- I'm not a fan of this \"commitment to maintain\" that's being done a lot nowadays. This is near impossible to guarantee in academia (and will be embarrassing for the authors when someone reads the paper 10 years from now and this benchmark has reached its end-of-life). A more useful commitment would be to ensure that others can easily compute the metrics themselves. Maybe just mention that you are releasing the source code instead?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KgAAppjDoT", "forum": "pKqt8psClA", "replyto": "pKqt8psClA", "signatures": ["ICLR.cc/2026/Conference/Submission3448/Reviewer_eTTX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3448/Reviewer_eTTX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966080842, "cdate": 1761966080842, "tmdate": 1762916728309, "mdate": 1762916728309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}