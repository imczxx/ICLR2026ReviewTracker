{"id": "ngAdlt5n0q", "number": 20453, "cdate": 1758306325598, "mdate": 1759896977035, "content": {"title": "BigO(Bench) - Can LLMs Generate Code with Controlled Time and Space Complexity?", "abstract": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the capabilities of generative language models in understanding and generating code with specified time and space complexities. This benchmark addresses the gap in current evaluations that often overlook the ability of models to comprehend and produce code constrained by computational complexity. BigO(Bench) includes tooling to infer the algorithmic complexity of any Python function from profiling measurements, including human- or LLM-generated solutions. BigO(Bench) also includes a set of 3,105 coding problems and 1,190,250 solutions from Code Contests annotated with inferred (synthetic) time and space complexity labels from the complexity framework, as well as corresponding runtime and memory footprint values for a large set of input sizes. We present results from evaluating multiple state-of-the-art language models on this benchmark, highlighting their strengths and weaknesses in handling complexity requirements. In particular, token-space reasoning models are unrivaled in code generation but not in complexity understanding, hinting that they may not generalize well to tasks for which no reward was given at training time.", "tldr": "BigO(Bench) assesses the capacity of Large Language Models (LLMs) to comprehend time-space computational complexity of input or generated code.", "keywords": ["Computational Complexity", "Program Synthesis", "Code Generation", "Competitive Programming", "LLMs", "Large Language Model", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b9f6a349296252bc47f5def7f2153abd83e7d60.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces BIGO(BENCH), a benchmark designed to evaluate Large Language Models (LLMs) on their ability to understand and generate Python code with specific time and space complexity constraints. The contributions include: (1) A dataset derived from CODE CONTESTS, annotated with complexity labels using a custom dynamic inference framework (3,105 problems, ~1.2M solutions). (2) The complexity inference framework itself, which uses profiling, fuzzing, and curve fitting to estimate complexities. (3) An evaluation of 14 state-of-the-art LLMs on three tasks: complexity prediction, complexity-constrained code generation, and ranking generated solutions against human ones based on complexity coefficients. The results show that current LLMs, even reasoning-focused ones, struggle significantly with complexity generation tasks, despite high performance on standard code synthesis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a critical and often overlooked aspect of code generation by LLMs – their understanding and control of computational complexity, which is crucial for real-world software development.\n\n2. The authors have annotated a large dataset (3,105 problems, ~1.2M solutions) from competitive programming platforms with inferred complexity labels, providing a substantial resource if the labels were reliable.\n\n3. The evaluation covers multiple facets of complexity handling (prediction, generation, ranking) and includes a wide range of recent LLMs.\n\n4. The release of the complexity inference framework code allows for reproducibility and potential community improvement."}, "weaknesses": {"value": "1. The reported 82-84% accuracy against human labels is insufficient for generating trustworthy ground truth. Reliance on empirical profiling makes the framework susceptible to noise, hardware variations, and specific runtime environments, potentially failing to capture true asymptotic complexity. The paper itself notes it can fall upon edge cases. This lack of robustness undermines the entire benchmark's validity.\n\n2. The extensive, framework-dependent filtering applied during dataset creation may introduce significant bias, potentially selecting only problems/solutions where the framework performs well. The resulting dataset's representativeness is questionable. The complexity distribution is highly imbalanced, potentially skewing results.\n\n3. The All@1 scores for complexity generation are exceptionally low (often <5% for time, <3% for space) across all models, including powerful reasoning models. While interpreted as LLM failure, these near-zero scores could equally indicate issues with the benchmark itself: unreliable ground truth, an overly harsh metric (All@k), or ill-posed tasks. It's hard to draw meaningful conclusions or measure progress when performance is near the floor.\n\n4. Fine-tuning Llama 3.1 70B specifically on the benchmark tasks yielded negligible or even negative impacts on complexity generation performance (Table 4). This suggests the benchmark data and tasks, as currently formulated, may not provide a useful signal for improving LLM capabilities in this area, questioning their utility.\n\n5. The framework relies on dynamic analysis (profiling runs). It's unclear how well this empirical approach approximates theoretical worst-case complexity, especially given Python's dynamic nature and CPython optimizations. The benchmark might be evaluating the ability to generate code that performs well empirically under the framework's specific testing conditions, rather than code with a provably correct asymptotic complexity.\n\n6. Decisions like using Big-O for worst-case, the specific parameters for fuzzing/curve fitting, and the `simplicity bias`  need stronger justification and sensitivity analysis."}, "questions": {"value": "1. Could the authors provide a more detailed error analysis for the complexity framework? What types of code or complexity classes does it struggle with most? How was the 125-sample human validation set selected, and what was the inter-annotator agreement if multiple humans were involved? Given the 16-18% error rate, how confident can we be in the benchmark labels, especially for evaluating subtle differences between models or measuring fine-tuning progress?\n\n2. How sensitive are the framework's complexity estimations to the specific hardware, Python version, background processes, and profiling tool versions? Were experiments run to quantify this variability? Could this noise explain some of the poor LLM generation results?\n\n3. Can the framework distinguish between empirical performance fitting a curve (e.g., $O(n)$ up to $n=10000$) and true theoretical complexity (e.g., an underlying $O(n \\log n)$ algorithm that looks linear in the tested range)? How does it handle amortized analysis?\n\n4. Could the extensive filtering applied during test set creation (e.g., removing outliers, unstable predictions, unlikely ASTs) have biased the benchmark towards problems/solutions the framework can easily analyze, potentially masking harder cases?\n\n5. Given the near-zero All@k generation scores, how can the authors be sure this reflects inherent LLM limitations versus issues with the benchmark's noisy labels, task formulation, or the stringency of the All@k metric?\n\n6. Why do the authors believe fine-tuning failed to improve (and sometimes worsened) complexity generation performance? Does this suggest the benchmark data lacks a useful signal, or that standard fine-tuning is simply inadequate for this type of reasoning?\n\n7. Why was Big-O chosen for the main benchmark despite the potential ambiguity noted, especially when the Big-Theta prompt seemed to improve generation for the reasoning model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dAnitzf1BQ", "forum": "ngAdlt5n0q", "replyto": "ngAdlt5n0q", "signatures": ["ICLR.cc/2026/Conference/Submission20453/Reviewer_vpRc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20453/Reviewer_vpRc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631086917, "cdate": 1761631086917, "tmdate": 1762933894085, "mdate": 1762933894085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "BIGO(BENCH) introduces a benchmark for testing whether LLMs can understand and produce code that meets stated time and space complexity. It covers three tasks: predicting the complexity of a given solution, generating a solution that satisfies a target complexity, and ranking solutions within the same class by constant factors. The labels come from profiling code across input scales rather than purely theoretical analysis.\n\nExperiments show that today’s strong models often achieve functional correctness but still miss explicit complexity targets. Performance on complexity-constrained generation is low, and complexity prediction is only modest. The paper argues that current training does not teach models to control algorithmic complexity and calls for methods that align generation with resource goals."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark targets controlled time and space complexity rather than only functional correctness, and it instantiates this through three complementary tasks. The core idea feels novel, and the contribution is substantial and well scoped.\n\n2. The authors annotate 3,105 problems and 1,190,250 Python solutions from CODE CONTESTS, provide per-input runtime and memory traces, and release problem-specific dataclasses so solutions can be profiled end to end. The test sets explicitly retain problems with multiple complexity classes, improving diagnostic value.\n\n3. Single-sample performance on complexity-constrained generation remains low across models, and while larger sampling budgets help, the task stays difficult. Fine-tuning brings only targeted, partial gains, with All@1 often in the single digits, suggesting a genuinely hard benchmark that is likely to remain relevant over time.\n\n4. The paper introduces a practical framework to infer time and space complexity from empirical profiling via fuzzing and curve fitting, and indicates an open release. It reports 84% agreement for time and 82% for space with human-theoretical labels, along with high self-consistency across runs."}, "weaknesses": {"value": "1. Robustness of the inferred complexity labels is under-specified. While the authors report ≈90% self-consistency across multiple runs, it remains unclear how stable the labels are under runtime noise or varying sampling resolutions. For example, would the predicted class remain unchanged if input sizes were halved, or if 5–10% random noise were added to the timing measurements? A quantitative sensitivity analysis along these lines would strengthen confidence in the framework’s robustness.\n2. The distribution is heavily concentrated in O(1) and O(n), which can inflate averages and conceal weaknesses on rarer classes like O(n log n) or O(n²). Class-balanced/re-weighted metrics, stratified subsamples, and per-class All@k would test whether gains persist beyond majority-class guessing and keep both tasks informative on the tails."}, "questions": {"value": "1. You mention 37 algorithmic notions of  problems in your dataset. Could you provide the full list and their distributions?\n\n2. In Table 2 several evaluated models are already outdated. Given newer releases such as o4-mini and GPT-5, how do you justify the current model selection? If immediate re-runs are not feasible, is there a plan for an updated leaderboard, submission interface, or fixed-budget evaluation protocol that would allow the community to add recent models while preserving comparability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XIqkuqGHmm", "forum": "ngAdlt5n0q", "replyto": "ngAdlt5n0q", "signatures": ["ICLR.cc/2026/Conference/Submission20453/Reviewer_Ka3Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20453/Reviewer_Ka3Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759619582, "cdate": 1761759619582, "tmdate": 1762933892906, "mdate": 1762933892906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper attempts to test how well code LMs understand the concept of code complexity by gauging how well they adhere to complexity-constrained code generation and code complexity prediction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper uncovers an interesting setting in which Code LMs show a non-trivial lack of understanding.\n2. The paper moves to improve the tooling and evaluation suite for open-ended code complexity evaluation by allowing arbitrary executable code to be profiled for runtime and memory usage over a large variety of inputs and then inferring its complexity via curve fitting."}, "weaknesses": {"value": "1. The paper introduces a very specific task without really doing a good job of motivating its need. The need for generating the most efficient code is clear to all, but why does a model need to controllably generate code of a certain complexity is not very clear at all.\n2. I would be sympathetic to a code understanding angle in terms of why controllable complexity is important, but I also feel there are better ways to disentangle that, for e.g. by asking the model to rewrite an existing code solution of a problem into another one that follows a specific complexity class.\n\nOverall, my rating of 4 is harsh, and I would prefer to give a 5 if the option were present. I would also be open to be persuaded to move the score to a 6 if the work were better motivated."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "96ueLgIWB1", "forum": "ngAdlt5n0q", "replyto": "ngAdlt5n0q", "signatures": ["ICLR.cc/2026/Conference/Submission20453/Reviewer_EQiS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20453/Reviewer_EQiS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762182502887, "cdate": 1762182502887, "tmdate": 1762933892157, "mdate": 1762933892157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}