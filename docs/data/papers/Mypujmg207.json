{"id": "Mypujmg207", "number": 23431, "cdate": 1758343663498, "mdate": 1759896815130, "content": {"title": "Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment", "abstract": "We present Federated Timeline Synthesis (FTS), a novel framework for training generative foundation models across distributed timeseries data applied to electronic health records (EHR). At its core, FTS represents patient history as tokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding temporal, categorical, and continuous clinical information. Each institution trains an autoregressive transformer on its local PHTs and transmits only model weights to a central server. The server uses the generators to synthesize a large corpus of trajectories and train a Global Generator (GG), enabling zero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS on five clinically meaningful prediction tasks using MIMIC-IV data, showing that models trained on synthetic data generated by GG perform comparably to those trained on real data. FTS has the potential to offer strong privacy guarantees, scalability across institutions, and extensibility to diverse prediction and simulation tasks especially in healthcare, including counterfactual inference, early warning detection, and synthetic trial design. We publish the code at https://anonymous.4open.science/r/fts-paper.", "tldr": "", "keywords": ["foundation models", "ehr", "generative ai", "healthcare", "patient health trajectories"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36b13b6a46c80483fe9bd2097f9183d2db89049b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes Federated Timeline Synthesis (FTS), a new framework that uses a federated learning (FL) for training autoregressive transformer models on distributed electronic health record (EHR) data. Each site trains a local generator on tokenized Patient Health Timelines (PHTs) and transmits model weights to a central server. The server then uses these generators to synthesize synthetic timelines and trains a Global Generator (GG) that can generate synthetic patient data for downstream tasks. The authors evaluate the method on five clinical prediction tasks using the MIMIC-IV dataset."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper combines FL with transformer models by training local transformer generators and aggregating for synthetic data generation.\n\n2. This work proposes a multi-model integration that combines doctor notes, structured codes, images, measurements, and genomics. \n\n3. It proposes a Zero-Shot Probabilistic Inference approach to generate a sequential synethic data. It claims this method can generate a sequence in an irreguarly-sampled way."}, "weaknesses": {"value": "1. Unclear motivation and objective: It is difficult to discern whether the main contribution is FLframework with transformers or timeline synthesis as a generative objective. \n\n2. Unclear core concept: The term “timeline synthesis” could be clarified. This paper does not explicility define it or properly cites it in previous works. It seems like it is a complete new concept but it lacks clear definition. \n\n3. Inadequate dataset: This work essentailly proposes a framework which is working on cross-site datasets. However, it only tests on MIMIC4 which is sampled paients from a singel site. This paper may need more suitable dataset such as eICU, which has over 200 hospitals. \n\n4. No privacy analysis: It does not have privacy analysis, while it claims it in the motivation.\n\n5. Limited experiment: It has limited experiments and lack the visualization, ablation studies, and more analysis about the irregularly-sampled data generation (I assume it is core contribution). The experiments it has barely support the claim."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "owziUe1PHG", "forum": "Mypujmg207", "replyto": "Mypujmg207", "signatures": ["ICLR.cc/2026/Conference/Submission23431/Reviewer_pSTX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23431/Reviewer_pSTX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687139376, "cdate": 1761687139376, "tmdate": 1762942658531, "mdate": 1762942658531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "* The paper proposes **Federated Timeline Synthesis (FTS)**: silos train local AR transformers on tokenized Patient Health Timelines (PHTs) and transmit either model weights or synthetic timelines to a server. The server then trains a Global Generator from synthetic sequences and performs zero-shot downstream inference by sampling future timelines.\n* Experiments on **MIMIC-IV** cover DRG multi-class, SOFA regression, readmission, ICU admission, and mortality, with ablations on data size and sampling temperatures. Results show mixed real + synthetic can approach real-only baselines, while fully synthetic lags."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Frame FL of time series generators as **federated synthesis** by sharing compact trained synthesizers instead of gradient checkpoints or synthesized data, distinct from prior works\n* Provides a clear pipeline, reasonable ablations (temperature, data regimes), and fidelity checks (unigram/DimWise) that support fidelity preservation.\n* Lowered communication cost of aggregating synthetic data across silos."}, "weaknesses": {"value": "* Privacy evidence: No formal privacy guarantees or empirical audits are presented across the paper and appendix, despite privacy being the central motivation. Note that is well acknowledged that even sharing trained generator model produces privacy risks due to model inversion attack / data reconstruction attack / membership inference attack[1]. \n* Prior art contrast: While the paper recognized prior work on federated learning for synthesizers[2] / aggregating synthetic data rather than gradient checkpoint[3], there is limited discussion and comparison with those works , and none of them included as baseline.\n* Limited evaluation: Evaluation is on Mimic-IV database and largely homogeneous; robustness to cross-institution heterogeneity (coding skews, missingness) is untested.\n* Unclear inference cost: Zero-shot via Monte-Carlo sampling is computationally heavy, and the paper lacks any analysis on run time. \n\n[1] Hayes, Jamie, et al. \"Logan: Membership inference attacks against generative models.\" arXiv preprint arXiv:1705.07663 (2017).\n[2] Rasouli, Mohammad, Tao Sun, and Ram Rajagopal. \"Fedgan: Federated generative adversarial networks for distributed data.\" arXiv preprint arXiv:2006.07228 (2020).\n[3] Goetz, Jack, and Ambuj Tewari. \"Federated learning via synthetic data.\" arXiv preprint arXiv:2008.04489 (2020)."}, "questions": {"value": "* What attacker model do you assume, and can you provide empirical privacy audits (membership inference/model inversion/data reconstruction) or any formal guarantees?\n* How does your method compare empirically to prior “federated synthesizer” (e.g., FedGAN) and “upload synthetic data” baselines (e.g., Goetz & Tewari), and why were these not included / compared?\n* What is the end-to-end runtime and sampling budget for zero-shot Monte-Carlo inference, and how does performance scale with the number of samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VjQZUQVmrm", "forum": "Mypujmg207", "replyto": "Mypujmg207", "signatures": ["ICLR.cc/2026/Conference/Submission23431/Reviewer_yGqp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23431/Reviewer_yGqp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963606173, "cdate": 1761963606173, "tmdate": 1762942658201, "mdate": 1762942658201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Federated Timeline Synthesis (FTS), a novel framework for training generative transformer models across distributed, electronic health records (EHRs). Each institution trains an autoregressive model locally on Patient Health Timelines (PHTs) — tokenized representations of longitudinal patient data — and sends only model weights to a central server. The server then synthesizes data from these local generators to train a Global Generator (GG), which can be used for zero-shot inference or downstream predictive modeling. Using MIMIC-IV, the authors show that models trained on synthetic PHTs perform comparably to those trained on real data across five clinical prediction tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The approach could meaningfully reduce privacy risks and regulatory barriers in federated clinical modeling, which is a major obstacle in real-world EHR applications.\n\n- Evaluation across five clinically meaningful tasks (DRG, SOFA, readmission, ICU admission, mortality) demonstrates reasonable robustness. The inclusion of calibration and fidelity metrics (e.g., Unigram and DimWise R²) strengthens the empirical credibility."}, "weaknesses": {"value": "- The paper does not compare with any other federated learning method as baselines. It's unclear whether the proposed method is optimal.\n\n- The experiment is only conducted on one dataset, MIMIC. This limits generalizability to more heterogeneous or real-world multi-institutional setups. The “federated” simulation is synthetic rather than operationally federated and from different institutes.\n\n- The experiment settings are not comprehensive - the impact of number of clients and different amount of data are not discussed. Ablation studies on different settings are not complete."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6vBkHO8Ya6", "forum": "Mypujmg207", "replyto": "Mypujmg207", "signatures": ["ICLR.cc/2026/Conference/Submission23431/Reviewer_xCSB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23431/Reviewer_xCSB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044815651, "cdate": 1762044815651, "tmdate": 1762942657967, "mdate": 1762942657967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Federated Timeline Synthesis (FTS), a framework for training generative foundation models on distributed Electronic Health Record (EHR) data. The core problem it addresses is the difficulty of training large models on clinical data, which is siloed across institutions due to privacy constraints and data heterogeneity.\n\nFirst, each participating institution locally converts its EHR data into \"Patient Health Timelines\" (PHTs), which are language-agnostic token sequences representing temporal, categorical, and continuous clinical events. Then, each institution trains a local autoregressive transformer model on its own PHTs. Each institution sends its trained model weights (not data or gradients) to a central server in a one-shot transfer. The server uses the collection of local generators to synthesize a large corpus of \"pseudo-PHTs\". Finally, a single \"Global Generator\" (GG) model is then trained from scratch on this centralized synthetic corpus."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles a critical and high-impact problem: enabling collaborative, large-scale model training for healthcare while respecting the severe privacy and data-siloing constraints of the field.\n- The proposed two-stage synthesis framework (local generators -> central synthetic corpus -> global generator) is an interesting alternative to traditional federated learning"}, "weaknesses": {"value": "- The paper repeatedly claims \"strong privacy guarantees\" in the abstract , introduction , and discussion, positioning this as a primary benefit. However, the authors explicitly contradict this in their contributions, stating, \"we do not provide formal privacy guarantees\" , and again in the limitations: \"it does not guarantee protection against potential attacks such as membership inference or model inversion\". Sending trained generator weights is not formally private; these weights can contain memorized information from the local dataset. This seems misleading.\n- The model trained on the little (10%) dataset \"overfitted... and failed to produce sensible patient timelines\". This is a gap, as \"low-resource settings\" are a key use case for FL. \n- Table 7 reveals \"glitches in the synthetic data,\" such as demographic tokens (GENDER, RACE) appearing multiple times. This indicates the underlying generative models still have a ways to go. Would be good for the authors to comment on this."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "L800QiHLRb", "forum": "Mypujmg207", "replyto": "Mypujmg207", "signatures": ["ICLR.cc/2026/Conference/Submission23431/Reviewer_iMxW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23431/Reviewer_iMxW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160932094, "cdate": 1762160932094, "tmdate": 1762942657688, "mdate": 1762942657688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}