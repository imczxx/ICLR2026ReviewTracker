{"id": "9I1G8PjEnA", "number": 23511, "cdate": 1758344776486, "mdate": 1763645824182, "content": {"title": "Representation Drift Compensation: A Zero-Cost Enhancement for LLM Decomposition", "abstract": "While low-rank decomposition offers potential for LLM size reduction, its application is limited by considerable performance degradation. In this work, we identify and formalize a key overlooked issue in LLM decomposition: \\textit{representation drift}. We show that approximation errors introduced by decomposition propagate and amplify non-linearly through the deep layers of the transformer architecture, progressively distorting internal representations and degrading downstream performance. To mitigate this, we introduce a conceptually simple but principled compensation mechanism, named ``\\our'', that operates by suppressing error at its source. By learning to align the output distribution of decomposed transformer blocks with their original counterparts, our method effectively counteracts representation drift, achieving notable performance recovery with zero inference overhead. Extensive experiments across OPT, LLaMA-2, LLaMA-3, and QWen exhibit remarkable improvements in language modeling, common-sense reasoning, knowledge-based reasoning, and vision-language tasks. For instance, on LLaMA-3-8B and OPT-13B at 40\\% compression, perplexity is reduced by more than 70\\% while reasoning task accuracy improves by over 10\\%. Our code is available at this anonymous URL.", "tldr": "", "keywords": ["Large Language Model", "Model Compression", "Large Model Efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38331b075733a7510c0913f4bf6e1c2b7ea05039.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Decomper to align decomposed block outputs with original ones. It tests on OPT, LLaMA-2/3, QWen, showing over 70% perplexity reduction and 10% accuracy gain at 40% compression."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper is well organized and well written.\n\n2.The theoretical foundation is sound and presented with clarity, providing solid support for the proposed approach.\n\n3.The experimental setup is rigorous and comprehensive, effectively validating the theoretical claims."}, "weaknesses": {"value": "1. **In Table 1, the color blocks used in the “ratio” section are visually confusing.**\n   The inconsistent coloring makes it difficult for readers to interpret the results clearly, and the authors are encouraged to unify or clarify the color scheme for better readability.\n\n2. **From Table 1, it can be observed that while the proposed method performs well on simpler tasks such as CommonsenseQA, it suffers larger performance drops on more challenging benchmarks like MMLU.**\n   Moreover, the paper lacks evaluation on difficult text generation or reasoning tasks (e.g., GSM8K, HumanEval), which are crucial for demonstrating the generality and robustness of the method. Without such experiments, it is hard to justify that the proposed approach has broad applicability.\n\n3. **The practical value of the proposed method remains unclear.**\n   As shown in Table 1, *Decomper* exhibits substantial performance degradation compared to the original model, especially on more complex tasks such as MMLU. This raises the question of whether the method offers any real advantage over simpler quantization techniques. Although the “Compatibility with Quantization” section claims that *Decomper* can be combined with quantization, the paper does not provide comparisons with standalone quantization methods. More detailed experimental analyses should be included to validate the method’s practical value, including but not limited to comparisons in effectiveness, inference speed, and computational cost."}, "questions": {"value": "As shown in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0YxXJZkEoz", "forum": "9I1G8PjEnA", "replyto": "9I1G8PjEnA", "signatures": ["ICLR.cc/2026/Conference/Submission23511/Reviewer_cKn2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23511/Reviewer_cKn2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761296412827, "cdate": 1761296412827, "tmdate": 1762942691072, "mdate": 1762942691072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies “representation drift” — the propagation and amplification of approximation errors in low-rank decomposition of LLMs — as a key cause of performance degradation. The authors propose Decomper, a zero-overhead compensation method that learns bias corrections to align the outputs of decomposed layers with the original model. Experiments across multiple LLMs and benchmarks show the method effectively mitigates performance drops while adding no inference cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The paper is well-structured with logical flow, facilitating reader comprehension. Section 3 presents a step-by-step, tightly connected process from error quantification to propagation analysis, enhancing readability and reinforcing technical rigor.\nS2: The diagnosis of “representation drift” combines theoretical analysis with empirical validation. The proposed Decomper mechanism features zero deployment overhead and demonstrates strong generalizability across SVD/PCA-based decomposition, quantization, and vision-language models.\nS3: Experiments cover diverse models and benchmarks, with in-depth analysis validating robustness and efficiency compared to fine-tuning/matrix updates, ensuring thorough method evaluation."}, "weaknesses": {"value": "W1: Novelty: Decomposition-based compensation has been extensively explored in this field, as evidenced by prior works such as FLAP and AFM cited in the paper. This study primarily tests similar approaches on low-rank decompositions without substantial conceptual advancement.\nW2: Although the paper conducts numerous comparative experiments, it fails to adequately contrast its approach with several outstanding related studies it cites.\nW3: Section 3 presents critical formulas but omits intermediate derivation steps, and the Appendix does not supplement them. The paper directly states the linear layer reconstruction loss expectation without showing how to expand the squared norm expectation into mean and covariance terms.\nW4: Equation 3 contains critical issues: the left-hand side is defined as the drift of the L-th block, but the right-hand side sums drift from l=1 to L, creating a logical contradiction. The formula also fails to retain the expectation term or connect to the squared L2 norm, breaking the link between theoretical setup and propagation analysis.\nW5: The paper compares Decomper with recovery fine-tuning and least-squares matrix updates but omits critical experimental details. It remains unclear which dataset (WikiText-2 or C4) was used for fine-tuning/calibration, which benchmark the “Avg. Acc.” corresponds to, and how Decomper performs in domain-specific scenarios where fine-tuning typically excels."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tG2yHPlz7F", "forum": "9I1G8PjEnA", "replyto": "9I1G8PjEnA", "signatures": ["ICLR.cc/2026/Conference/Submission23511/Reviewer_irUD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23511/Reviewer_irUD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657687742, "cdate": 1761657687742, "tmdate": 1762942690847, "mdate": 1762942690847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the low-rank decomposition of LLMs. The authors discuss the layerwise decomposition and show that the error from layerwise decomposition can accumulate over different layers. They propose to add a bias term to compensate for the additional error. The resulting optimization is solved using gradient descent. The authors also provide numerical experiments over a wide range of baselines and benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper studies an important problem\n- The numerical experiments are extensive"}, "weaknesses": {"value": "I think the writing can be improved. Some examples:\n\n- Proposition 1 is quite handwavy. What is random? What is variance? With respect to what distribution? The results should be written properly.\n\n- Why should one care about (3)? We only care about the final error, not the average layerwise error.\n\n- It is not immediately clear how problem (6) is solved. One has to look into the appendix to find the algorithm, which I don't think it is referred to in the main text.\n\n- Table 4 is missing dense baselines. I'm not sure how accurate the comparisons in the section named \"Contribution of Compensation Strategy\" are."}, "questions": {"value": "Some of the models used in the experiments are rather old (OPT, Llama 2). Can the authors please present more benchmarks for newer models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eidLHoHRVO", "forum": "9I1G8PjEnA", "replyto": "9I1G8PjEnA", "signatures": ["ICLR.cc/2026/Conference/Submission23511/Reviewer_9JKN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23511/Reviewer_9JKN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761943387, "cdate": 1761761943387, "tmdate": 1762942690660, "mdate": 1762942690660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies \"representation drift\" as a key problem in low-rank decomposition of LLMs, where approximation errors accumulate and amplify non-linearly through deep transformer layers, progressively distorting internal representations. The authors propose \"Decomper,\" a compensation mechanism that learns bias vectors for each decomposed linear layer to align decomposed block outputs with original counterparts. The method is optimized on a small calibration set and adds zero inference overhead by fusing learned compensation into existing bias terms. Extensive experiments on OPT, LLaMA-2, LLaMA-3, and QWen2.5-VL etc, are shown."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The core insight about error propagation through Equation 3 is well-motivated and provides theoretical grounding for the empirical phenomenon\n- Proposition 1 offers a clear bound on the expected local error after compensation\n- Experimental validation is comprehensive across multiple model families, scales, and benchmarks and the ablation studies convincingly isolate the contribution of the compensation mechanism"}, "weaknesses": {"value": "- The core idea of learning bias corrections is incremental—FLAP (An et al., 2024) already uses bias correction for pruning, and this work essentially adapts it for decomposition\n- No comparison with other alignment strategies (e.g., feature matching, KL divergence minimization between distributions)\n- The theoretical analysis in Section 3.1 uses first-order Taylor expansions without discussing when this approximation is valid or quantifying higher-order terms\n- Proposition 1's proof assumes convergence to c* = Eμ but doesn't address the non-convex optimization landscape mentioned in Section 3.2\n- The claim that Equation 6 \"consistently converges to a strong local optimum\" lacks empirical evidence (e.g., convergence curves, sensitivity to initialization)\n- The theoretical closed-form solution c* = Eμ is dismissed as insufficient, but no rigorous analysis explains why the learned bias systematically outperforms this baseline\n- Missing analysis of how the compensation mechanism interacts with different compression ratios per layer (mentioned but not studied)\n\n\nMinor:\n- Figure 2's caption could better explain what \"23rd Transformer block\" represents (out of how many?)\n- The connection between the local error term (Equation 1) and the block-level propagation (Equation 3) could be made more explicit\n- Section 3.2 introduces multiple ideas (theoretical c*, averaging trick, learned bias) that feel somewhat disconnected\n- The paper claims \"zero-cost deployment\" but doesn't discuss memory overhead of storing compensation vectors during training or calibration time costs\n- Some experimental details are unclear: what is \"data-whitening SVD\" as distinct from vanilla SVD?"}, "questions": {"value": "- Neat but not very well explained:why does the learned bias outperform the closed-form solution? \n- Which layers contribute most to drift? Are all layers equally important to compensate? How does compensation allocation across layers affect results?\n- What is the wall-clock time and memory cost of the compensation optimization phase? How does this scale with model size?\n- Figure 2 shows drift recovery, but can you quantify the alignment more rigorously (e.g., KL divergence, Wasserstein distance between original and compensated distributions)?\n- Can you provide error bars or confidence intervals for the main results (Table 1-3) to assess statistical significance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YAqwvf4QS4", "forum": "9I1G8PjEnA", "replyto": "9I1G8PjEnA", "signatures": ["ICLR.cc/2026/Conference/Submission23511/Reviewer_a5jo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23511/Reviewer_a5jo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762322625269, "cdate": 1762322625269, "tmdate": 1762942690489, "mdate": 1762942690489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Reviewers,\n\nWe sincerely thank all reviewers (with IDs: a5jo, 9JKN, irUD, cKn2) for your valuable time and insightful comments aimed at improving our manuscript. We are pleased that the reviewers recognized the core motivation of our work (R-a5jo), the importance of the problem (R-9JKN), the clear logical flow (R-irUD), and the solid theoretical foundation (R-cKn2).\n\nThe reviewers' feedback was helpful for us to identify three key areas for strengthening: (1) the argument regarding novelty (particularly the comparison with methods like FLAP); (2) the rigor of the theoretical presentation (especially concerning Eq. 3 and Proposition 1); and (3) the experimental gap in comparisons on more challenging benchmarks (e.g., GSM8K).\n\nIn response to this feedback, **we have uploaded a revised manuscript**. To facilitate your review, **all major changes are highlighted in blue**. Specifically, we have made the following revisions:\n- [Novelty Clarification] We have substantially revised Section 2 (Related Work) to clearly and technically differentiate Decomper—which learns to mitigate the complex, input-dependent variance and non-linear drift introduced by SVD—from heuristics like FLAP, which only address the static mean shift introduced by pruning.\n- [Theoretical Rigor] We are: (a) corrected a significant typesetting error in Eq. 3 (thanks R-irUD) to accurately reflect the recursive propagation of error; (b) provided a detailed derivation of the expectation expansion for Eq. 1 in the appendix (addressing R-irUD); and (c) formalized the assumptions for Proposition 1.\n- [Complex Tasks] To address the reasonable concerns raised by R-cKn2, we have added new experiments on GSM8K. The results (detailed in our response to R-cKn2) indicate that while absolute drops are a field-wide challenge, Decomper provides performance recovery and achieve relatively better results, validating our drift mitigation thesis even in brittle scenarios.\n- [Relationship with Quantization ] To validate the practical value of Decomper (R-cKn2 Q3), we have included a new analysis demonstrating that Decomper is orthogonal and synergistic with quantization. Specifically, Decomper provides hardware-agnostic parameter reduction, which is complementary to the numerical memory reduction achieved by quantization. This synergy enables a superior compression-performance trade-off when combined.\n\nWe believe these revisions will significantly strengthen our paper and address the reviewers' concerns.\n\nSincerely,\n\nAuthors of submission #23511"}, "title": {"value": "Global Response"}}, "id": "Zs7Lxk9Cl0", "forum": "9I1G8PjEnA", "replyto": "9I1G8PjEnA", "signatures": ["ICLR.cc/2026/Conference/Submission23511/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23511/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission23511/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763543390801, "cdate": 1763543390801, "tmdate": 1763643214973, "mdate": 1763643214973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}