{"id": "6hnjZMWICE", "number": 20597, "cdate": 1758308100867, "mdate": 1759896968765, "content": {"title": "RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs", "abstract": "Reinforcement learning based post-training of large language models (LLMs) has recently gained attention, particularly following the release of DeepSeek R1, which applied GRPO for fine-tuning. Amid the growing hype around improved reasoning abilities attributed to RL post-training, we critically examine the formulation and assumptions underlying these methods. We start by highlighting popular structural assumptions made in modeling LLM training as an MDP, and show how they lead to a degenerate MDP that doesn’t quite need the RL/GRPO apparatus. The two critical structural assumptions include (1) making the MDP states be just a concatenation of the actions with states becoming the context window and the actions becoming the tokens in LLMs and (2) splitting the reward of a state-action trajectory uniformly across the trajectory. Our comprehensive analysis demonstrates that, due to these simplifying assumptions, the standard approach is effectively equivalent to outcome-driven supervised learning. Our experiments on benchmarks including GSM8K and Countdown using Qwen-2.5 base models and Llama-3.2 instruct models show that Filtered Iterative SFT, incorporating both positive and negative samples, achieves performance comparable to GRPO-based training. We also show that these structural assumptions indirectly incentivize RL to generate longer sequences of intermediate tokens which in turn feeds into the narrative of “RL incentivizing thinking because it generates longer thinking traces.” We continue to believe that RL writ large can be a great tool for post-training LLMs, and hope that our analysis of the limiting assumptions in the currently popular RL framework encourages work that goes beyond patching the symptoms of faulty assumptions and towards improving the underlying formulation.", "tldr": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Post training", "Degenerate MDP"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22ef0476bb5bfd479affbcb4cdeb0947e28f2340.pdf", "supplementary_material": "/attachment/5b8358eea921a0878b519b969e255037de891ae0.zip"}, "replies": [{"content": {"summary": {"value": "This paper critically analyzes the structural assumptions of reinforcement learning (RL)-based post-training for large language models (LLMs), especially focusing on Group Relative Policy Optimization (GRPO) popularized by DeepSeek R1. It identifies two key flawed assumptions in the dominant LLM-Markov Decision Process (MDP) formulation: states defined as concatenations of past tokens and uniform allocation of terminal rewards across all tokens. Theoretically, the paper shows the standard approach\nis effectively equivalent to outcome-driven supervised learning; empirically, using datasets like GSM8K and Countdown with Qwen-2.5 and Llama-3.2 models, this SFT variant achieves performance comparable to GRPO. It also reveals that longer outputs from RL-trained LLMs are biases from the structural assumptions rather than improved reasoning, and calls for better MDP formulations to fully unlock RL’s potential for LLM post-training"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper addresses a fundamental and timely question about the validity of current RL-based post-training formulations for large language models. By dissecting the structural assumptions that underlie widely used frameworks such as GRPO, the work provides valuable conceptual clarity that is highly relevant to the ongoing debate around “RL for reasoning.”\n\n2. The analysis is thorough and well-grounded, connecting theoretical insights with empirical evidence on standard benchmarks (e.g., GSM8K, Countdown) and multiple base models."}, "weaknesses": {"value": "1. It is unclear what is meant by ``RL in Name Only''. The term needs clearer definition or justification.\n2. Questionable claim that RL-based training is equivalent to outcome-driven supervised learning.\n\n\nWhile the paper argues that the MDP formulation degenerates under common assumptions, this does not fully justify equating RL with supervised learning. The critical distinction lies in on-policy versus off-policy optimization — which directly affects importance sampling in gradient estimation. A contemporaneous work by XiongW et al. reaches a more nuanced conclusion: outcome-driven supervised learning performs slightly worse than GRPO but remains competitive due to its simplicity. This aligns with the authors’ own training curves in arxiv version, which also show a modest but consistent performance gap favoring GRPO.\n\n3. The discussion about sequence length lacks causal evidence.\n\nThe paper suggests that RL encourages longer reasoning traces because of the advantage. In GRPO-like algorithms, the loss is distributed across tokens proportionally to their advantages (e.g., averaged over sequence length) as shown in eq. 8, rather than being solely related to \\frac{A_i}{L_x}. Therefore, changes in output length cannot be causally linked to the claimed mechanism without further controlled analysis.\n\n[1]. Xiong W, Yao J, Xu Y, et al. A minimalist approach to llm reasoning: from rejection sampling to reinforce[J]. arXiv preprint arXiv:2504.11343, 2025."}, "questions": {"value": "Please see question in section Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EgSZjiWYNs", "forum": "6hnjZMWICE", "replyto": "6hnjZMWICE", "signatures": ["ICLR.cc/2026/Conference/Submission20597/Reviewer_WZko"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20597/Reviewer_WZko"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554381206, "cdate": 1761554381206, "tmdate": 1762934006564, "mdate": 1762934006564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work examines the structural assumptions underlying reinforcement learning methods for LLM post-training, specifically focusing on GRPO as used in DeepSeek-R1. The authors argue that two key assumptions in the LLM-MDP formulation (representing states as token sequences and uniformly distributing terminal rewards) reduce GRPO to filtered iterative supervised fine-tuning. The contributions are as follows. First, the authors present a theoretical analysis showing how GRPO simplifies to weighted supervised learning under these assumptions. Second, they provide empirical evidence through experiments on GSM8K and Countdown datasets using Qwen-2.5 and Llama-3.2 models, demonstrating that F-ISFT with positive and negative samples achieves comparable performance to GRPO. Third, they show that increased response length during RL training stems from structural assumptions rather than improved reasoning.\n\nThe strengths of the paper are as follows. First, the paper addresses a timely concern about what RL accomplishes. Second, the mathematical derivation effectively demonstrates how structural assumptions lead to equivalence with F-ISFT. The step-by-step simplification is easy to follow. Third, there is a comprehensive experimental setup, across several model families and sizes. Finally, the paper tackles the root cause, rather than proposing another patch like length penalties, the paper identifies fundamental formulation issues, which is  valuable long-term.\n\nThe weaknesses of the paper are as follows. First, the largest model size explored was 3B, and findings may not hold for larger model sizes. Second, there is no comparison between RL methods using proper credit assignment e.g. MTCS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of the paper are as follows.\n- First, the paper addresses a timely concern about what RL accomplishes.\n- Second, the mathematical derivation effectively demonstrates how structural assumptions lead to equivalence with F-ISFT. The step-by-step simplification is easy to follow.\n- Third, there is a comprehensive experimental setup, across several model families and sizes.\n- Finally, the paper tackles the root cause, rather than proposing another patch like length penalties, the paper identifies fundamental formulation issues, which is intellectually honest and valuable long-term."}, "weaknesses": {"value": "The weaknesses of the paper are as follows.\n- First, the largest model size explored was 3B, and findings may not hold for larger model sizes.\n- Second, there is no comparison between RL methods using proper credit assignment e.g. MTCS."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cm7vc5D3Un", "forum": "6hnjZMWICE", "replyto": "6hnjZMWICE", "signatures": ["ICLR.cc/2026/Conference/Submission20597/Reviewer_wmmp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20597/Reviewer_wmmp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868747923, "cdate": 1761868747923, "tmdate": 1762934006075, "mdate": 1762934006075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the two critical structural assumptions in current RL for LLMs. Authors's analysis demonstrates that, due to these simplifying assumptions, the standard approach is effectively equivalent to outcome-driven supervised learning. Authors also show that these structural assumptions indirectly incentivize RL to generate longer sequences of intermediate tokens. To support their claims, authors conducted experiments on GSM8K and Countdown using Qwen-2.5 base models and Llama-3.2 instruct models."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. First of all, the authors claim that \"Our comprehensive analysis demonstrates that, due to these simplifying assumptions, the standard approach is effectively equivalent to outcome-driven supervised learning\". However, the validity of this claim requires further consideration. Although the derived Equation (8) looks like SFT, the expectation is taken over the distribution of the current policy, whereas SFT is done over a static dataset. RL learns over its own rollouts!\n2. Limited technical novelty. The authors attribute the length bias of GRPO to its \"average loss over the entire sequences\", which has already been investigated in Section 3.3 of DAPO, the ByteDance's work about 5 months ago. Moreover, the authors claim that \"we show that the primary driver of response elongation is this uniform credit distribution\". However, in Figure 7(a) of DAPO's paper, we can still see that the response length increases during training, even though DAPO fixes the length bias in GRPO by Token-Level Policy Gradient Loss."}, "questions": {"value": "Please refer to Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GFaaWDKesy", "forum": "6hnjZMWICE", "replyto": "6hnjZMWICE", "signatures": ["ICLR.cc/2026/Conference/Submission20597/Reviewer_SgUC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20597/Reviewer_SgUC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898902432, "cdate": 1761898902432, "tmdate": 1762934005500, "mdate": 1762934005500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper breaks down structural assumptions in RL post-training for LLMs and GRPO and shows that under these assumptions, the GRPO objective can be simplified to a filtered iterative SFT objective. To prove this, the authors relax the KL term in the GRPO objective and then decompose it by positive and negative responses which results in a F-ISFT+- objective with both positive and negative responses incorporated. Empirical evidence shows that the F-ISFT+- trained models are comparable in performance to GRPO trained models on reasoning benchmarks. \n\nIt also provides an alternate explanation for the length bias in GRPO which suggests that the uniformly distributed terminal reward incentivizes the model to produce longer sequences to reduce the per-token penalty for incorrect responses. This is further supported by F-ISFT+- producing longer responses like GRPO but F-ISFT+ does not."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The analysis and breakdown of the MDP assumptions and terminal reward assignment in RL LLM post-training was meticulous and led to very interesting conclusions\n- The explanation for increasing response lengths in GRPO is compelling and opens up a new avenue for exploration to fix the issue with more sophisticated solutions than naive length penalties.\n - Quantitative results on Qwen and Llama models and GSM8K, Countdown and MATH datasets validate the theoretical claims in the paper and providing strong evidence for the central arguments."}, "weaknesses": {"value": "- Empirical results are only shown on smaller (0.5B to 3B) models. Adding additional results with larger models would add further support to claims of the paper.\n- A discussion on the implementation complexity and training dynamics of F-ISFT+- would improve the paper.\n - The removal of the KL penalty could be an over-simplification. Similarly, the assumption of binary rewards also might not hold true for all tasks, and F-ISFT is not necessarily comparable to other RL algorithms like PPO. \n- While revisiting the MDP formultion is proposed as a direction to leverage the full capacity of RL, no setup is explored. This leaves the paper as critique with no proposed solutions."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Gl3cGRjO6G", "forum": "6hnjZMWICE", "replyto": "6hnjZMWICE", "signatures": ["ICLR.cc/2026/Conference/Submission20597/Reviewer_UE3A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20597/Reviewer_UE3A"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134352782, "cdate": 1762134352782, "tmdate": 1762934004607, "mdate": 1762934004607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}