{"id": "tTZGP1d1A8", "number": 12230, "cdate": 1758206477147, "mdate": 1759897523819, "content": {"title": "Adversarial Test Case Generation via Reinforcement Learning Extends Scaling Laws", "abstract": "Rule-based reinforcement learning (RL) has greatly advanced the coding capabilities of large language models (LLMs). However, existing RL methods remain largely confined to code generation, relying on fixed test cases for evaluation and leaving the problem of test case generation underexplored. Generating diverse and adversarial test cases is critical, as it not only enriches coding knowledge but also enables effective self-verification during inference.\nRecent supervised learning approaches attempt to jointly train code and test case generation during the post-training stage. Yet, these methods fall short: supervised learning inherently lags behind RL in coding performance, and the resulting test cases often lack diversity and adversarial quality, limiting their ability to identify erroneous code.\nTo address these limitations while retaining the advantages of RL, we propose Test Cases Scaling (TCS), a two-stage reinforcement learning framework for learning to generate high-quality adversarial test cases. TCS employs stage-specific reward functions and a policy-aligned training buffer to progressively enhance test case quality and alignment with the evolving model.\nExperimental results on TACO and LiveCodeBench show that TCS significantly outperforms supervised baselines in both code and test case generation during both training and inference. Furthermore, adversarial test cases generated by our trained TCS-7B model improve the inference-time performance of leading proprietary LLMs.", "tldr": "", "keywords": ["Reinforcement Learning", "Large Language Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/041a313ebe76fa08b9d6845f959fb20fd9ae3df0.pdf", "supplementary_material": "/attachment/55e6cb2afae9e17da2373249e4062fcea418f2ae.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a two-stage reinforcement learning framework for code and test cases generation. Their main contribution is the integration of RL into the test case generation, which is utilized to generate \"adversarial\" test cases and improve code quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The application of RL to the problem of adversarial test case generation is novel in the context of LLM coding.\n\nWhen done properly, an adversarial test generator can provide a continuous mechanism for self-correction and iterative improvement.\n\nThe presented framework is well described, and the flow of the methodology is clear."}, "weaknesses": {"value": "**Fundamental Flaws in RL Reward Function Design**\n\nThe methodological execution of the RL framework appears fundamentally flawed due to the definition of the primary reward signals.\n\nReward $R_1$ Issue: The reward $R_1$ is defined as $1$ if the execution of the target code solution $C*$ with the generated input $I_g$ matches the generated output $O_g$. Since $C^*$ is the correct solution, any valid input $I_g$ will produce a valid output $O_g$. This means a randomly generated input $I_g$ would also achieve $R_1=1$ with high probability. This definition fails to reward intelligent search or adversarial quality.\n\nAdversarial Reward Issue: The adversarial reward, which utilizes the incorrect code $C_{wrong}$, suffers from the same problem. A random search over the input space that simply evaluates inputs on $C^*$ and then checks $C_{wrong}$ would likely achieve a positive reward frequently, making the learned RL policy potentially no better than random exploration.\n\nMissing Justification: The paper fails to provide a compelling argument for the advantage of a learned RL policy over a simple random search over the input space combined with a standard pass/fail evaluation. This lack of a random baseline comparison undermines the entire RL methodology.\n\n**Insufficient and Unclear Evaluation**\n\nThe experimental design and reporting lack the necessary rigor to support the paper's claims:\n\nMissing Definition of Key Metrics: The paper must explicitly define how key evaluation metrics are calculated:\n\n* The definition of the pass@1 score is missing.\n\n* The calculation method for the Best-of-N (BoN) score (as noted in the second and third rows of tables) is not provided.\n\nAmbiguity in Results Presentation: For Figure 3(a) (\"Pass@1 and BoN (N=8) performance\"), it is ambiguous whether the single bar presented for each model/baseline is the average over both metrics, which would be an inappropriate aggregation. The authors need to clarify this presentation.\n\nUnclear Accuracy Calculation: The method for calculating the \"accuracy\" for the results presented in Figure 3(b) is not defined and requires detailed expansion.\n\nStatistically Insignificant Sample Size: The use of only 16 samples for TACO and 32 for LiveCodeBench is severely limited and insufficient to derive robust conclusions about model performance or the generalization of the test case generator.\n\nLack of Statistical Rigor: None of the results presented include standard deviations ($\\pm$ std) or confidence intervals. This omission prevents any assessment of the statistical relevance of the findings and suggests the experiments may have been conducted with a single trial, rendering the claims of performance improvement tenuous."}, "questions": {"value": "Please see weaknesses. \n\nCan you add clarity to the score/accuracy definitions?\n\nWhere the evaluation done only on 1 trail?\n\nWhy only 16 samples for TACO and 32 for LiveCodeBench where used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MjxCRvQ7Hp", "forum": "tTZGP1d1A8", "replyto": "tTZGP1d1A8", "signatures": ["ICLR.cc/2026/Conference/Submission12230/Reviewer_j3Nq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12230/Reviewer_j3Nq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761592825150, "cdate": 1761592825150, "tmdate": 1762923174659, "mdate": 1762923174659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the under-explored problem of learning to generate adversarial test cases for code evaluation. The authors propose a two-stage RL framework (TCS): Stage I learns to produce correct and non-duplicative tests; Stage II learns to produce adversarial tests that distinguish incorrect solutions. A policy-aligned buffer keeps the test generator focused on the solver’s current error modes. At inference time, the model generates tests for each candidate program and selects the candidate that passes the most tests (self-generated tests, majority vote). Experiments on TACO and LiveCodeBench with 1.5B/7B backbones show improvements over SFT and simple self-verification baselines and illustrate positive scaling with more trajectories."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Timely focus: Training a model to author adversarial tests addresses a real bottleneck in sample-and-verify pipelines for code generation.  \n2) Simple and practical design: The two-stage reward and rolling buffer are easy to implement and reason about.  \n3) Clear deployment story: The inference-time selection via self-generated tests is straightforward and actionable.  \n4) Empirical signal: Consistent gains over SFT/simple self-verification across two model sizes; reasonable data-scaling trends."}, "weaknesses": {"value": "1) Missing head-to-head comparison with verifier-based methods \n   The paper’s inference-time scheme:“generate tests → execute → vote”—aligns closely with the verifier paradigm. Strong baselines in this space are independent verifier models (e.g., CodeRM-8B) that are trained specifically to write high-quality tests and discriminate with small test budgets. \n2) Data and comparability details  \n   Training/evaluation are focused on Python/contest-style tasks. Generalization to other languages and more heterogeneous settings is not validated. If benchmarks are patched or randomized, a standardized evaluation harness (fixed temperatures, retries, timeouts, tool stacks) is needed to ensure cross-paper comparability.\n3) Leakage and template-dependence checks  \n   If public exemplars/writeups or replays are used, stronger evidence of near-duplicate filtering, time splits, and template obfuscation is needed to ensure adversarial tests are not memorized patterns."}, "questions": {"value": "1) Can you provide same-candidate-pool, same N×M budget comparisons against CodeRM-8B , and report the minimum M required to reach a target accuracy?  \n2) Will you release a standardized evaluation harness with fixed decoding settings to ensure cross-paper comparability?  \n3) Do you have near-duplicate/time-split/template-obfuscation analyses to rule out leakage or pattern memorization in adversarial tests?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vBdIHMdAxH", "forum": "tTZGP1d1A8", "replyto": "tTZGP1d1A8", "signatures": ["ICLR.cc/2026/Conference/Submission12230/Reviewer_B6xj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12230/Reviewer_B6xj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603395419, "cdate": 1761603395419, "tmdate": 1762923174113, "mdate": 1762923174113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Test Cases Scaling (TCS), a two-stage reinforcement learning framework that trains an LLM not only to solve programming problems but also to generate adversarial test cases that expose errors in incorrect code. Stage-1 uses a “valid-but-non-copied” reward $R_{t1}$ to teach the model to produce correct, non-example test cases; Stage-2 switches to an adversarial reward $R_{t2}$ that grants a reward when the ground-truth solution passes but an incorrect solution fails on the generated case. The framework also maintains a policy-aligned buffer of recent code outputs to construct dynamic test-case prompts. At inference time, the model can generate test cases to self-verify and select among multiple code candidates using a pass-count criterion. On TACO and LiveCodeBench, TCS improves both pass@1 and Best-of-N performance over base and SFT baselines, with further gains when using self-generated test cases for selection. The paper releases filtered training data, model weights, and implementation details to aid reproducibility."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "First, it elevates test-case generation to a first-class RL target and specifies concrete rewards for code $R_c$ and tests $R_{t1}$ and $R_{t2}$, with GRPO used for optimization and an explicit pseudocode loop. This makes the approach testable and implementable.     Second, the paper motivates a two-stage schedule by observing that $R_{t1}$ is lenient and $R_{t2}$ is stricter, then shows that $R_{t1}$ alone under the same budget gives weak scaling while adding $R_{t2}$ yields gains as $N$ grows.   Third, the inference-time selection rule is simple and auditable, defined as a pass-count objective over $N \\times M$ tests, and the results indicate that self-generated test selection scales more reliably than a reward model.   Fourth, the experiments show consistent gains across two model sizes and both benchmarks, with tables and figures that break out the effect of public tests and of the two-stage reward.   Finally, the paper includes useful details for reproduction, including data filtering to a $6{,}318$-problem training set and the LiveCodeBench window, hyperparameters, and released checkpoints."}, "weaknesses": {"value": "The Stage 1 to Stage 2 switch is described as happening after a preset test-case accuracy is reached, yet the metric, split, threshold value, and patience are not given in the main text or appendix. This limits repeatability and could affect results. \n\nThe inference-time section defines $M$ test cases per prompt, but the limitations state that the system currently produces only a single test per inference. It is not clear whether the main tables used $M=1$ or $M>1$ and how sensitive results are to $M$.   \n\nThe policy-aligned buffer is a key mechanism, yet the exact buffer admission rule is given only at a high level, for example excluding timeouts and syntax errors in Stage 1 and retaining only incorrect code in Stage 2, with Algorithm 1 referring to items that satisfy the buffer policy without a precise definition or diversity constraint. A detailed admission rule and $T_b$ sensitivity would strengthen reproducibility.    For fairness across budgets, the appendix reports GPU hours and step counts yet the main results do not normalize by total RL rollouts or training tokens; a matched-budget comparison would clarify how much of the gain comes from the reward design itself.   \n\nThe experimental setup uses public-case filtering in some settings and designates the first TACO evaluation case as public, which can interact with selection methods. A control without public filtering or with alternative public choices would reduce this confound.   \n\nThere is at least one typo, in Algorithm 1 where “generatation” appears in the comment."}, "questions": {"value": "Stage transition rule. You state that Stage 2 begins “upon reaching a predefined test case generation accuracy” in Stage 1. Please give the exact metric, the split used to measure it, the numerical threshold for each model size, and any patience or smoothing. If a fixed step schedule was used instead, please reconcile this with the stated accuracy trigger by citing the exact step at which you switched for each model size.   \n\nDefinition and computation of $R_{t2}$. In $R_{t2}(I_g, O_g, C^{*}, C_{\\text{wrong}})$ please specify how $C_{\\text{wrong}}$ is chosen from the buffer $B$, how many incorrect programs are considered per update, and how you aggregate outcomes when GRPO samples a group of $G$ responses. Please also state how exceptions or timeouts during $Exec(C_{\\text{wrong}}, I_g)$ affect the reward, since Stage 1 mentions excluding timeouts and syntax errors when populating $B$ but Stage 2 does not detail exception handling. A precise sampling rule and error policy are needed to reproduce $R_{t2}$.    \n\nInference time test count $M$. Section 3.3 defines generating $M$ test cases per prompt and selecting with $N \\times M$ tests using Eq. (8). The limitations section states that the current system produces only a single test case per inference. Please state the exact $M$ used in Table 1 and in the Section 4.4 runs, and if $M>1$ explain how multiple tests were produced per inference, or otherwise reconcile with the limitation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8nboMtIuTN", "forum": "tTZGP1d1A8", "replyto": "tTZGP1d1A8", "signatures": ["ICLR.cc/2026/Conference/Submission12230/Reviewer_ChCt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12230/Reviewer_ChCt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981285241, "cdate": 1761981285241, "tmdate": 1762923173765, "mdate": 1762923173765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Test Cases Scaling (TCS), a two-stage reinforcement learning framework designed to jointly improve a large language model's capabilities in both code generation and adversarial test case generation. The core idea is to move beyond static, predefined test suites used in typical RL-for-code setups. In Stage 1, the model is trained with RL to generate valid test cases that pass a ground-truth solution, using a reward function $R_t^1$. Once proficient, it moves to Stage 2, where it learns to generate adversarial test cases that specifically cause incorrect code solutions to fail while still passing the ground-truth solution, using a stricter reward $R_t^2$. The training process is dynamic, using a \"policy-aligned buffer\" of recently generated code to construct prompts for test case generation. The authors show that this method improves code generation performance (pass@1) on the TACO and LiveCodeBench datasets and, notably, enables an effective inference-time selection mechanism where the model uses its own generated test cases to select the best among multiple candidate solutions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The methodology is sound and the technical details are well-executed. The experiments effectively demonstrate the necessity of the two-stage approach (Figure 5) and that using self generated UT at test time is better than reward models. It also shows that the quality of the self generated test can be improved with the proposed method.\n* The paper is well-written and easy to follow. The reward functions are precisely defined and their motivations are clearly explained."}, "weaknesses": {"value": "* Reliance on Ground-Truth Solutions: The entire training framework is fundamentally dependent on the availability of a correct, executable ground-truth solution ($C^*$) to compute the rewards for test case generation (Eq. 6 and 7). This is a very strong and unrealistic assumption for a general code generation setting, where the main goal is to find solutions to problems that are not yet solved. This constraint limits the method's applicability to a form of \"re-distillation\" of knowledge from existing solutions, rather than enabling the discovery of new ones. Consequently, the claim that this method \"extends scaling laws\" is undermined, as it cannot be applied to the open-ended problems where such scaling is most needed.\n\n*  Unclear Source of Training-Time Improvement due to Missing Baselines: The experiments do not provide a clear picture of *why* the training-time code generation performance (pass@1) improves. The comparison in Table 1 is primarily between the proposed `RL using TCS Training` and an `SFT Model`. However, two crucial baselines are missing:\n    1.  **RL for Code Generation Only:** A standard RL baseline trained solely on the code generation task, using the provided ground-truth test cases as the reward signal. Without this, it is impossible to disentangle the gains from using RL in general versus the gains from the novel TCS framework. It is plausible that much of the observed improvement over SFT simply comes from the RL optimization process itself.\n    2.  **Decoupled Training of Policies:** An ablation where the code generation policy and the test case generation policy are trained independently. This would test the implicit claim that joint training provides a synergistic benefit. It is unclear if the two tasks must be trained together within a single model or if one could simply train a strong, separate \"verifier\" model.\n\n* The proposed adversarial framework was proposed before in other similar scenario where an external interpreter is available (e.g. for Lean with STP [1]) so the idea is not entirely novel.\n\n[1] Dong, Kefan, and Tengyu Ma. \"Stp: Self-play llm theorem provers with iterative conjecturing and proving.\" arXiv preprint arXiv:2502.00212 (2025)."}, "questions": {"value": "1.  The reliance on an executable ground-truth solution $C^*$ is the most significant limitation. Could you elaborate on how this method could be adapted for a more realistic setting where such solutions are not available? Have you considered alternative approaches?\n\n2.  To better isolate the contribution of the TCS framework, could you please comment on the expected performance of the two baselines I mentioned earlier?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YokZepbusC", "forum": "tTZGP1d1A8", "replyto": "tTZGP1d1A8", "signatures": ["ICLR.cc/2026/Conference/Submission12230/Reviewer_V83i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12230/Reviewer_V83i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990593854, "cdate": 1761990593854, "tmdate": 1762923173092, "mdate": 1762923173092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}