{"id": "kVaE2kYjtV", "number": 19192, "cdate": 1758294275235, "mdate": 1759897052954, "content": {"title": "Unpacking Human Preference for LLMs: Demographically Aware Evaluation with the Diverse Framework", "abstract": "Current evaluation of large language models relies predominantly on technical benchmarks that fail to capture how users actually experience these systems in practice. Even the most notable human preference evaluation approaches suffer from methodological limitations including unrepresentative sampling, superficial assessment depth, and single-metric reductionism that obscures the multidimensional nature of human-AI interaction quality. We introduce DIVERSE, a rigorous evaluation framework that addresses these limitations through demographically stratified sampling, multi-turn naturalistic conversations, and assessment across five human-centric dimensions. We collected conversations from 21,352 participants stratified across 22 demographic groups in the US and UK, evaluating 27 state-of-the-art language models through pairwise comparisons. Using a robust hierarchical Bradley-Terry-Davidson model alongside post-stratified demographic adjustments to census weights, we reveal insights unavailable within existing approaches: (1) clear performance hierarchies with Gemini-2.5-Pro achieving 97% probability of ranking first for overall preference, (2) quantification of significant preference heterogeneity, identifying user age as the primary factor, revealing failures in model generalization across populations, and (3) differential discriminative power across human-centric evaluation dimensions, with Trust, Ethics & Safety showing significantly higher tie rates than task performance metrics. Our framework demonstrates that meaningful evaluation requires moving beyond aggregate preference scores to understand the complex, demographic-specific patterns that determine real-world model preference. We release our complete dataset, interactive leaderboard, and evaluation framework to catalyse further research into more rigorous and equitable evaluation of language models.", "tldr": "We introduce DIVERSE, a new evaluation framework for LLMs that uses demographically stratified sampling and multi-turn conversations to reveal significant performance differences across user demographics.", "keywords": ["Leaderboards", "LLM", "Evaluation", "Benchmarking"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b8d7a3848f0242b6133204f1555ba74f871373a.pdf", "supplementary_material": "/attachment/9f8a0dc1105c69b6e55795192310706baeb52085.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes DIVERSE, a demographically aware, multidimensional framework for human evaluation of LLMs. The authors collect large-scale, multi-turn pairwise comparisons from stratified US/UK participants and analyze results with a hierarchical Bayesian Bradley–Terry–Davidson model combined with census post-stratification. They report a clear performance hierarchy among contemporary models, with one model consistently leading, but show that preferences vary meaningfully across demographic groups—especially by age—altering perceived ranks. They also find that evaluation dimensions differ markedly in how decisively users can discriminate between models, with holistic judgments being more decisive than safety-oriented ones. The work advocates moving beyond single-metric leaderboards toward nuanced, population-aware assessment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper surveys a large, demographically stratified US/UK population across many groups, collects extensive multi-turn data, and commits to releasing it—providing a high-quality community resource.\n\n2. The authors use parallel pairwise dialogues and a hierarchical Bayesian BTD model with tie handling and post-stratification, explicitly modeling demographic heterogeneity to produce uncertainty-aware, multidimensional rankings.\n\n3. The paper also provides actionable, empirically grounded insights. It shows that “overall” judgments align closely with core task/reasoning while rankings shift on communication and safety; reveals systematic age-driven preference differences with older users more tie-prone; and demonstrates large disparities in metric discriminability. These discoveries can guide audience- and task-specific model selection and alignment."}, "weaknesses": {"value": "1. The participant pool, though large, is limited to US/UK and underrepresents broader cultural and socioeconomic contexts (e.g., non‑Anglophone regions, Global South populations, rural communities, low digital‑literacy users, non‑binary identities, and diverse education/income strata).\n\n2. The paper reports five dimensions but offers little analysis linking the four sub-dimensions to the overall score or unpacking how conversational features (topic, complexity, safety triggers) map to those judgments; the chosen sub-dimensions may also miss facets like creativity, empathy, humor, and long-horizon reliability.\n\n3. Data arise from a study interface rather than organic, in-the-wild use, so prompts and stakes may diverge from real workflows, creating ecological bias; in addition, compared with standard automated benchmarks, this framework is harder to deploy and slower to collect, and the paper could better justify the return on that extra effort.\n\n4. The related work section could be broadened to cover more papers on human preference variability and pitfalls in human feedback (e.g., “Human Feedback is Not a Gold Standard,” ICLR 2024; “Towards Understanding Sycophancy in Language Models,” ICLR 2024; “Dissecting Human and LLM Preferences,” ACL 2024)."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k7twdbpaJE", "forum": "kVaE2kYjtV", "replyto": "kVaE2kYjtV", "signatures": ["ICLR.cc/2026/Conference/Submission19192/Reviewer_3vQb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19192/Reviewer_3vQb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817850494, "cdate": 1761817850494, "tmdate": 1762931190747, "mdate": 1762931190747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an exciting ongoing effort to collect real-world preference data from people across diverse demographics. Much of this information was not available in other public datasets. Having a dataset with detailed breakdowns of diverse human preferences will be very valuable to the scientific community in the long run."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* The pairwise preference dataset collection is very well executed with adaptive sampling to select the most uncertain pair and quality controls with GPT-4o-mini as the LLM judge.\n\n* The dataset is collected in large-scale with 106K pairwise data from 21K participants across 27 LLMs and will be made publicly available.\n\n* This paper also introduces a novel evaluation framework based on the hierarchical BTD model. This methodology contribution is just as important as the dataset contribution."}, "weaknesses": {"value": "* It seems that participants are instructed to have at least 3 turns in the study. It will be great to have a deeper analysis/discussion on how the depth of a conversation impact quality/model performance.\n\n* Another dimension that authors could consider is break down by tasks. One may hypothesize that preferences may vary across both tasks and demographics.\n\n* I highly suggest the authors to make this living benchmark and ongoing dataset release."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wib9POXNRB", "forum": "kVaE2kYjtV", "replyto": "kVaE2kYjtV", "signatures": ["ICLR.cc/2026/Conference/Submission19192/Reviewer_HzhH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19192/Reviewer_HzhH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932462369, "cdate": 1761932462369, "tmdate": 1762931190114, "mdate": 1762931190114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a large-scale dataset capturing human preferences over pairs of language model outputs along five evaluation dimensions, stratified by three demographic attributes (age, ethnicity, and politics), covering 27 models. The authors analyze these preferences using a hierarchical Bradley–Terry–Davidson model to learn demographic-specific adjustments and quantify preference variation. Their analysis reveals (1) the overall ranking of models, (2) age as the most divergent demographic axis, (3) variation in rankings across evaluation dimensions, and (4) differences in human tie rates across dimensions, indicating varying decisiveness among annotators."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Well-motivated and timely research direction. The paper tackles an increasingly critical issue in LLM evaluation: how human preferences differ across demographic groups and qualitative dimensions. It contributes a more nuanced understanding of model performance evaluation.\n- Valuable dataset contribution. The dataset could be a useful addition to the community: it is large-scale, multi-turn, demographically stratified, and spans a broad set of 27 LLMs. Such a dataset can serve as an important benchmark for studying model preference heterogeneity and subjective quality dimensions beyond conventional accuracy-based benchmark metrics. \n- Methodological rigor in modeling human preferences: The paper goes beyond simple aggregation of crowd judgments by employing a hierarchical Bradley–Terry–Davidson model. This allows the authors to systematically estimate demographic-specific adjustments and quantify preference variation."}, "weaknesses": {"value": "- Insufficient alignment between the stated problem and the proposed solution. The paper’s introduction highlights two major issues: (1) the dominance of single-metric evaluation and (2) the neglect of subjectivity. However, the proposed solution, evaluating models on five dimensions across demographic strata, resembles running multiple benchmarks, each focusing on a different aspect. While this is a meaningful improvement, it does not fully capture the “subjectivity” aspect claimed in the introduction, since subjectivity extends beyond demographic factors and fixed evaluation axes.\n- Brief and unclear data description. Section 3.2 should provide a more comprehensive account of the dataset. For example, it mentions that “each message sent by the participant was delivered to both models simultaneously” and that “a minimum of 3 conversational turns were required,” but it is unclear how the same message could be reused across multiple turns, given the dependency on previous model responses. The paper also states that participants were free to select topics—what kinds of topics did they typically choose? Furthermore, while gpt-4o-mini was used to flag “low-effort inputs,” the definition and examples of such inputs are not provided. Given that the dataset is the paper’s primary contribution, the absence of even a single example conversation or basic statistics (e.g., number of evaluations per stratum, average conversation length) makes it difficult to grasp the dataset’s structure and richness.\n- Overgeneralization and overstatement of findings. The paper emphasizes that “age is the most significant demographic factor driving preference heterogeneity,” but this conclusion holds only among the three studied axes (age, ethnicity, politics). The claim should be scoped accordingly. Similarly, while the paper asserts that “a model’s competitive standing can change dramatically depending on the evaluation lens,” Figure 3 shows high correlation across dimensions, implying that rankings are fairly consistent rather than dramatically shifting. These inconsistencies weaken the strength of the argument.\n- Weak overall contribution and presentation quality. The paper feels stretched for a full-length submission. For example, Section 3.4.2 describes an “LLM judge for conversational analysis,” but this analysis is absent from the results. The discussion section repeats much of the results content rather than extending it, which makes it redundant. Some metrics, such as “average rank shifts” (Section 4.2) and “tie percentage” (Section 4.4), are insufficiently defined. These issues collectively reduce the paper’s overall impact and quality."}, "questions": {"value": "- The compensation rate of £9/hr is described as \"recommended\" (line 157), yet this falls below the UK minimum wage of £12.21/hr for workers aged 21 and over. Could you clarify how this rate was determined and whether it meets ethical guidelines for research compensation?\n- The paper repeatedly refers to DIVERSE as a \"living benchmark\" with \"continuous evaluation\" and claims to \"continuously add new models and update rankings.\" Is there an active platform where ongoing evaluation is happening? If not, these claims seem misleading and should be clarified or removed.\n- The four evaluation dimensions are stated to derive from a pilot study using factor analysis (Section 3.3), but no details about this study are provided. Given these dimensions are central to your analysis, could you describe the pilot study methodology, sample size, and how factor analysis led to these specific dimensions?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper reports that they compensated US and UK annotators at the rate of £9/hr (line 157) which falls below the UK minimum wage of £12.21/hr for workers aged 21 and over. Would benefit to check with authors to make sure it meets ethical guidelines for research compensation."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4tmwaJvfxk", "forum": "kVaE2kYjtV", "replyto": "kVaE2kYjtV", "signatures": ["ICLR.cc/2026/Conference/Submission19192/Reviewer_Ba5U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19192/Reviewer_Ba5U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762300110273, "cdate": 1762300110273, "tmdate": 1762931189761, "mdate": 1762931189761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DIVERSE, a large-scale framework for LLM evaluation that is demographically aware, multi-dimensional, and human-AI interaction focused. The study collects ~110K pairwise comparisons of 27 models from ~21K participants across 22 different demographic groups in the US/UK. Using a hierarchical bayesian bradley-terry model with tie handling and demographic effect adjustments, they found that Google's gemini-2.5-pro achieved the top performance in most dimensional analyses. Lastly the authors emphasized that the context aware design of LLM evaluation is highly essential to accurately measure a model's capacity with user needs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The presentation of all methodology and results is very clear. \n- A large-scale data collection with a thoroughly curated design of DIVERSE (many participants, many data points, and multi-turn interaction logs)."}, "weaknesses": {"value": "- Allowing participants to choose their own topic of conversation can enhance validity of experiment setup (data collection especially), but this is likely to inject some heterogeneity in the task type and difficulty that can affect the pairwise evaluation settings. Although the paper collects LLM-as-judge annotations over several aspects, these variables are not put to the hierarchical TBD model. This can risk that the model assumes all conversations are equally treated, even though some task contexts may inherently require multi-step reasoning of problem solving (than just freely writing creative text). Ignoring this may not reveal quality differences between LLMs in a pairwise setup. If a LLM happens to be matched frequently on tasks that the model is good at, the estimated skill parameter of this LLM may be inflated independent of true capacity it has. The current manuscript does not provide those level of analysis. \n\n- The hierarchical BTD model treats age, ethnicity, and political affiliation as additive effects (scaled by 1/ $\\sqrt{3}$). This assumes that each demographic axe contributes independently to preference. However, human preferennce often involves interaction effects. For example, political tone or language can differ between younger and older generations (e.g., younger conservatives vs. older conservatives). The political identity of an individual may differ across racial and ethnic groups. The current manuscript does not consider this interactions of several demographic dimensions in the analysis."}, "questions": {"value": "- Could you provide a breakdown of task types and complexity (from the collected LLM-judge annotations) that each model was exposed to? Do win-rates change substantially when comparing models within the same task type (e.g., reasoning tasks like math, or causal conversation?) Is the overall leaderboard remaining stable when grouping by task complexity or task topic? Some stratification analysis would be highly valuable to strengthen the paper. \n\n- Did you check whether preference patterns differ across combinations of demographics, such as age x politics? A simple heatmap or table showing win/tie rates for age x politics groups can help assess whether those interaction effects are present."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pxehZL9bDj", "forum": "kVaE2kYjtV", "replyto": "kVaE2kYjtV", "signatures": ["ICLR.cc/2026/Conference/Submission19192/Reviewer_n89e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19192/Reviewer_n89e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762308827900, "cdate": 1762308827900, "tmdate": 1762931189337, "mdate": 1762931189337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DIVERSE, a large-scale, demographically aware framework for evaluating large language models through human preference data rather than technical benchmarks.\n\nStrengths\n\n- Dataset scale:\n\nThe study is based on 106,760 pairwise comparisons from 21,352 participants across 27 language models, which provides substantial empirical depth.\n\n- Methodological rigor:\n\nThe hierarchical Bayesian BTD model is statistically sound and appropriate for modeling heterogeneous human preferences.\n\n- Insightful analysis:\n\nThe examination of demographic heterogeneity and metric discriminability yields novel and meaningful findings for human-centered LLM evaluation.\n\nWeakness\n\n- Data Availability\n\nThe paper briefly mentions data and framework availability in the conclusion but does not provide access at review time. Given the paper’s emphasis on dataset, it would be important to release at least a partial dataset or representative samples during the review process. If the paper is accepted and made public, the authors should clearly commit to releasing the full dataset for research use.\n\n\n- Representativeness\n\nWhile the paper makes a valuable contribution, its claims about representativeness and the mitigation of sampling bias are overstated. The participant pool includes only users from the US and UK—two English-speaking, Western countries, failing to capture the broader global, multilingual, and cultural diversity of LLM users. The claim of being “stratified across 22 demographic groups” creates an impression of global inclusiveness, though the scope is in fact regionally constrained. The authors should moderate such claims and clearly state that their findings are representative only within certain contexts. A more cautious framing would enhance the paper."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Dataset scale:\n\nThe study is based on 106,760 pairwise comparisons from 21,352 participants across 27 language models, which provides substantial empirical depth.\n\n- Methodological rigor:\n\nThe hierarchical Bayesian BTD model is statistically sound and appropriate for modeling heterogeneous human preferences.\n\n- Insightful analysis:\n\nThe examination of demographic heterogeneity and metric discriminability yields novel and meaningful findings for human-centered LLM evaluation."}, "weaknesses": {"value": "- Data Availability\n\nThe paper briefly mentions data and framework availability in the conclusion but does not provide access at review time. Given the paper’s emphasis on dataset, it would be important to release at least a partial dataset or representative samples during the review process. If the paper is accepted and made public, the authors should clearly commit to releasing the full dataset for research use.\n\n\n- Representativeness\n\nWhile the paper makes a valuable contribution, its claims about representativeness and the mitigation of sampling bias are overstated. The participant pool includes only users from the US and UK—two English-speaking, Western countries, failing to capture the broader global, multilingual, and cultural diversity of LLM users. The claim of being “stratified across 22 demographic groups” creates an impression of global inclusiveness, though the scope is in fact regionally constrained. The authors should moderate such claims and clearly state that their findings are representative only within certain contexts. A more cautious framing would enhance the paper."}, "questions": {"value": "1. Dataset release: Can the authors provide example cases or partial data during review to improve transparency? Will the complete dataset be released for research use if the paper is accepted?\n2. Demographic scope: Do the authors plan to extend data collection beyond the US and UK to include more diverse populations in future iterations of DIVERSE? Otherwise, it would be better to limit the contexts in the introduction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vc5OtgyxuL", "forum": "kVaE2kYjtV", "replyto": "kVaE2kYjtV", "signatures": ["ICLR.cc/2026/Conference/Submission19192/Reviewer_Kbk1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19192/Reviewer_Kbk1"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762485949760, "cdate": 1762485949760, "tmdate": 1762931188873, "mdate": 1762931188873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}