{"id": "dwbrZtYP04", "number": 2226, "cdate": 1757034813267, "mdate": 1759898161625, "content": {"title": "SparseD: Sparse Attention for Diffusion Language Models", "abstract": "While diffusion language models (DLMs) offer a promising alternative to autoregressive models (ARs), existing open-source DLMs suffer from high inference latency. This bottleneck is mainly due to the attention’s quadratic complexity with respect to context length in computing all query–key pairs. Intuitively, to reduce this complexity, restricting computation to sparse attention patterns that retain only the most important ones offers an effective solution. This type of method is widely used in ARs, where the attention mechanism exhibits clear and fixed sparse patterns. In DLMs, our analysis also reveals the presence of sparse patterns and further highlights three unique observations: (1) attention patterns vary across heads, (2) attention patterns in each head remain highly similar across denoising steps, and (3) early denoising steps are critical for generation. These unique findings render well-studied fixed sparse attention methods in ARs largely incompatible with DLMs, as their fixed patterns fail to capture head-specific patterns in DLMs, and sparse attention applied in the early steps can lead to degradation in generation. To address these challenges, we propose **SparseD**, a novel sparse attention method for DLMs. \nLeveraging the observations in DLMs, SparseD only pre-computes and selects the most important query–key pairs once as head-specific sparse patterns for reusing across denoising steps. This manner can handle head-specific patterns without incurring the high latency associated with recomputing sparse patterns at each denoising step. Meanwhile, SparseD skips sparse attention and uses full attention in the early steps to preserve generation quality. Together, these establish SparseD as a practical and efficient solution for deploying DLMs in long-context applications. Experimental results demonstrate that SparseD achieves lossless acceleration, delivering up to $1.50\\times$ speedup over FlashAttention at a 64k context length with 1,024 denoising steps. Anonymous code is available at https://anonymous.4open.science/r/SparseD-8C76/.", "tldr": "", "keywords": ["Diffusion Language Models", "Sparse Attention"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/185b46ee1d8c6dfb78413935d0ad71fa14b48b5a.pdf", "supplementary_material": "/attachment/67bfe963b7607c5d73da33b54eeb6e00dfd61440.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the high inference latency of diffusion language models (DLMs), which stems from the quadratic complexity of the attention mechanism applied over all tokens at each denoising step. The authors propose SparseD, a sparse attention method tailored for DLMs. The method is motivated by three key empirical observations: (1) DLM attention patterns are \"head-specific,\" meaning they vary significantly across different attention heads; (2) patterns *within* a given head are \"highly similar\" across all denoising steps; and (3) the early denoising steps are \"critical\" for generation quality, and applying sparsity too early causes performance degradation.\n\nBased on these findings, SparseD's design is threefold:\n1.  **Skipping Sparse:** It applies full (dense) attention for an initial percentage of denoising steps (e.g., first 20%) to preserve generation quality.\n2.  **Isolated Selection:** At the end of this initial phase, it computes the full attention matrix *once* and selects a head-specific, block-wise top-$\\rho\\%$ sparse pattern. This selection is done \"isolating\" prefill and generation tokens to ensure both are adequately represented.\n3.  **Sparse Reusing:** This single, pre-computed head-specific sparse pattern is then cached and reused for all remaining denoising steps, leveraging the observed temporal stability.\n\nExperiments on models like Dream-7B and LLaDA-1.5 show that SparseD achieves \"lossless\" accuracy on various benchmarks (MMLU, RULER) while delivering significant speedups over standard FlashAttention (e.g., up to 1.50x at 64k context with 1024 steps), as the one-time pre-computation cost is amortized."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Novel Empirical Insights:** The paper's main strength is its clear identification and empirical validation of three specific attention behaviors in DLMs: (1) head-specific patterns, (2) temporal stability of patterns, and (3) high sensitivity of early denoising steps. This analysis is novel and provides a solid foundation for future work on DLM efficiency.\n2.  **Well-Motivated Design:** The SparseD method is a strong example of principled system design. Each of its three components (Skipping Sparse, Isolated Selection, Sparse Reusing) is a direct and logical solution to one of the three identified empirical observations.\n3.  **Strong Experimental Validation:** The evaluation is comprehensive. The authors demonstrate that SparseD achieves its \"lossless\" claim by benchmarking against the original models. Crucially, they also show its superiority to two distinct and relevant classes of baselines: (a) AR-based sparse attention (which fails on accuracy) and (b) other DLM acceleration methods (which degrade on long-context tasks).\n4.  **Good Ablation Study:** The ablation in Table 2 is a model of clarity. It individually removes each component of SparseD and shows the precise, and severe, negative impact: removing \"Skipping Sparse\" destroys accuracy (Observation 3 validated), removing \"Sparse Reusing\" explodes latency (Observation 2 validated), and removing \"Isolated Selection\" hurts accuracy (heuristic validated).\n5.  **Practicality and Scalability:** The method provides a practical speedup (up to 1.5x) that compellingly scales with the number of denoising steps $T$ (Figure 5). This makes it particularly attractive for high-quality generation, which often requires many steps."}, "weaknesses": {"value": "1.  **Lack of Deeper Analysis:** The paper is purely empirical. It does not offer any hypothesis or theoretical exploration for *why* these attention patterns are temporally stable. The denoising process is defined by $T$ steps of iterative refinement, so the $Q$, $K$, and $V$ representations are constantly changing. It is non-obvious and fascinating why the resulting $QK^T$ patterns should remain static. While a full theoretical proof is not required, some discussion or preliminary analysis (e.g., measuring the cosine similarity of $Q$/$K$ vectors over time) would have strengthened the paper.\n2.  **Scalability of the $O(N^2)$ Pre-computation Step:** The method requires a single, full $O(N^2)$ attention computation at step $T \\times \\text{skip}\\%$ to build the sparse pattern. While the authors use a memory-efficient block-wise implementation, the *computational cost* of this step is still $O(N^2)$. The experiments stop at 64k context. For truly massive contexts (e.g., 256k, 1M), this single pre-computation step could become the new latency bottleneck, potentially dominating the cost of all subsequent sparse steps. The paper does not analyze the scaling of this specific step or discuss this potential limitation."}, "questions": {"value": "1.  Could the authors provide a more quantitative analysis of \"Attention Similarity Across Time\" (Observation 2)? For example, by plotting the Jaccard similarity of the selected top-$\\rho\\%$ blocks between the pattern computed at step $T \\times \\text{skip}\\%$ and the \"ground truth\" top-$\\rho\\%$ blocks at each subsequent step $t$? This would be much stronger evidence than the current heatmaps.\n2.  How robust is the \"lossless\" claim to the temporal similarity assumption? What is the performance impact (e.g., on RULER) if the sparse pattern from one prompt is (incorrectly) reused for a different, unseen prompt? This would stress-test the model's reliance on the exact pattern.\n3.  Regarding the $O(N^2)$ pre-computation step: How does the wall-clock latency of this *single step* scale with context length (e.g., from 4k to 64k)? At what context length $N$ (assuming $T=1024$) would the authors project this one-time $O(N^2)$ cost to become larger than the cumulative $O(T \\cdot N \\cdot \\rho N)$ cost of the remaining sparse steps?\n4.  The \"Isolated Selection\" heuristic uses the same selection ratio $\\rho\\%$ for both prefill and generation tokens. Is this an optimal choice? Have the authors experimented with different ratios (e.g., $\\rho_{\\text{prefill}}$ and $\\rho_{\\text{gen}}$)? One might hypothesize that static prefill tokens and dynamic generation tokens could benefit from different sparsity levels."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HVHQqj4Qnz", "forum": "dwbrZtYP04", "replyto": "dwbrZtYP04", "signatures": ["ICLR.cc/2026/Conference/Submission2226/Reviewer_Q4L3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2226/Reviewer_Q4L3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761291027252, "cdate": 1761291027252, "tmdate": 1762916153574, "mdate": 1762916153574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a sparse attention mechanism designed to accelerate Diffusion Language Model (DLM) inference, particularly in long-context, high-step settings. While it is the first or one of the first approaches to apply sparse attention for DLMs, it is based on mainly *empirical* observations and shows only incremental improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Empirical Validation of DLM Dynamics: The analysis highlighting the temporal consistency and the importance of early denoising steps is highly insightful and provides foundation for a DLM-specific acceleration technique. The ablation study confirming the necessity of the \"Skipping Sparse\" component is well-executed and validates the central hypothesis regarding diffusion step sensitivity. \n\n- State-of-the-Art Accuracy: the method achieves an almost lossless performance, competitive with other SOTA methods."}, "weaknesses": {"value": "- *(General)* The paper is based mainly on very empirical observations concerning the distribution of attention scores. There is no validation of these properties through a quantitative analysis. \n- *(General)* The method per-se is not very novel, as it applies principles that are well known already in ARMs (top-k selection based on attention scores in blocks) to DLMs.\n- *(General)* The observation that attention patterns are head-specific is not a new empirical finding in the literature, and was already noticed in ARMs (cf. \"Duo Attention\", ICLR 2025 for KV Cache compression ARMs). \n- *(Results)* The comparison against StreamingLLM looks ill-posed. Streaming LLM was developed for ARMs and is based on attention sinks in ARMs, but other work shows sinks in DLMs behave very differently (e.g. \"Attention Sinks in Diffusion Language Models\" Arxiv Preprint) so it is not surprising that it underperforms greatly in this scenario.\n- *(Results)* The comparison with other baselines does not look very clear. Sparse methods are strongly dependent on the sparsity threshold, but the main results are displayed with a fixed chosen threshold for each method, making it difficult to understand the effective trade-off between accuracy and sparsity. A proper comparison should include different sparsity levels, which imply different FLOPs and memory footprint tradeoffs.\n- *(Results)* Accuracy and Latency are evaluated against different sets of baselines which makes the results part a bit confusing."}, "questions": {"value": "-  I think the choice of the baselines is not clear. This is a sparse attention method but the chosen baselines for efficiency include KVCaching methods for DLMs. However, KVCaching methods mainly aim at storing pre-computed values, while sparse attention aims at reducing computations. I think the paper would benefit from a clearer discussion of these two different aspects. Are they complementary, orthogonal ? How do they relate ? \n-  For the same reason, it is not clear to me if this method is reducing memory footprint other than improving latency ? In line 294 the authors claim that the method reduced memory footprint but not experiments or comparisons are shown in this direction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GR4dk3F824", "forum": "dwbrZtYP04", "replyto": "dwbrZtYP04", "signatures": ["ICLR.cc/2026/Conference/Submission2226/Reviewer_tBmW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2226/Reviewer_tBmW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658844873, "cdate": 1761658844873, "tmdate": 1762916153226, "mdate": 1762916153226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SparseD, a sparse-attention framework specifically designed for diffusion language models (DLMs).  \nThe authors first present three empirical observations:  \n1. Attention patterns are head-specific rather than uniform across heads.  \n2. Within each head, the attention pattern is stable across denoising steps.  \n3. Early denoising steps are critical, and applying sparsity too early degrades quality.  \n\nBased on these insights, SparseD adopts a simple and practical design:  \n- Skip sparsity during the early diffusion steps.  \n- At a representative step, compute full attention once, select block-wise Top-ρ attention regions per head, and store these as reusable sparse masks.  \n- Reuse the same mask across later steps, with isolated selection for prefill and generation tokens to better handle different distributions.  \n\nExperiments on Dream-7B-Instruct and LLaDA-1.5 show that SparseD maintains almost identical accuracy  while achieving significant speedup over FlashAttention at 64 k context and up to 1024 diffusion steps.  \nAblation studies confirm that skipping early steps and reusing fixed patterns are both necessary for preserving model quality.  \n\nOverall, SparseD offers a simple yet effective mechanism to accelerate DLM inference while retaining quality, contributing a valuable empirical and practical perspective to efficient diffusion-based text generation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Novel DLM-specific insights: Identifies and validates three key empirical properties (head-specific, step-stable, early-step-sensitive) that distinguish DLM attention from AR models.  \n- Simplicity and practicality: The “skip-early + reuse mask + isolated selection” design is lightweight, easily implementable on top of Flash/FlexAttention, and hardware-friendly.  \n- Strong empirical results: Demonstrates consistent latency improvements with negligible performance drop across multiple DLMs and tasks (MMLU, GSM8K, HumanEval, RULER).  \n- Comprehensive ablations: Validates each component’s necessity and explores sensitivity to skip ratio and sparsity rate ρ.  \n- Clear presentation: Includes concise pseudo-code and diagrams explaining mask generation and reuse; writing is crisp and reproducible."}, "weaknesses": {"value": "1. Overhead accounting clarity.  The paper amortizes sparse-pattern pre-computation but does not quantify the actual wall-clock fraction of that cost across steps.  \n\n2. Storage footprint not quantified.  The memory cost for storing per-head, per-layer sparse masks (after block-wise Top-ρ selection) is not reported, leaving uncertainty about scalability under 64 k contexts and many layers.  \n\n3. Comparison to dynamic sparsity baselines.  The paper argues autoregressive (AR) sparsity patterns do not transfer to diffusion models, but does not include an adapted dynamic head-wise Top-ρ baseline for DLMs to confirm the advantage of static reuse."}, "questions": {"value": "1. What is the per-layer/head memory footprint of the stored sparse masks under your standard setting?  \n2. Have you tried computing masks at multiple checkpoints (e.g., twice in the diffusion process) and reusing them thereafter?  \n3. If the prompt distribution shifts (e.g., code → narrative), do the pre-computed masks still perform well?  \n4. How do latency and accuracy trade off with smaller or larger block sizes for a fixed ρ?  \n5. Can heads with low cross-step similarity be selectively excluded from mask reuse to improve robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "L0zJmyRaXi", "forum": "dwbrZtYP04", "replyto": "dwbrZtYP04", "signatures": ["ICLR.cc/2026/Conference/Submission2226/Reviewer_C5KP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2226/Reviewer_C5KP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832969443, "cdate": 1761832969443, "tmdate": 1762916152974, "mdate": 1762916152974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SparseD, a sparse-attention framework specifically designed for DLMs.   \n\nThe author design the efficient algorithm based on following observations. 1 Attention pattern within each attention head. 2 attention pattern is same across different denoise step. \n\nThen, author propose the SparseD algorithm. They compute attention pattern at early denoising steps and resuse them in the latter steps. The author conduct experiments on Dear 7B Instruct and Llada models, showing that SpraseD maintain accuracy while increase the efficiency. \n\nOverall, SparseD is an algorithm designed from similar attention pattern among different head and different deniose steps, which is a quite interesting paper."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1 The paper identify the head specific and step consistent attention pattern.\n\n2 The overall idea of the algorithm is simple, clean, and straight forward. \n\n3 The empirical results show that the proposed SparseD algorithm indeed increase the efficiency of DLMs."}, "weaknesses": {"value": "1 Running time is not reported. The SparseD introduce additional precomputation, but how much compute does these precomputation takes does not reported in the paper. \n\n2 Lack of memory usage report. The paper lack of report for how much memory does it need to store the footprint for the sparse attention mask.\n\n3 Need comparison with dynamic sparsity baselines. The paper argues that AR based sparsity pattern does not generalize to DLM. But there is not experiment showing that static reuse is better than dynamic sparsity in DLM."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "L0zJmyRaXi", "forum": "dwbrZtYP04", "replyto": "dwbrZtYP04", "signatures": ["ICLR.cc/2026/Conference/Submission2226/Reviewer_C5KP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2226/Reviewer_C5KP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832969443, "cdate": 1761832969443, "tmdate": 1763679648251, "mdate": 1763679648251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **SparseD**, a **training-free, inference-time** sparse attention method for **Diffusion Language Models (DLMs)**. The key observations are: (1) **head-specific** attention patterns, (2) **high step-wise consistency** within a head, and (3) **early steps are sensitive** to sparsification. SparseD thus applies **full attention in early steps**, then **pre-computes head-specific sparse patterns once** using **block-wise top-ρ% selection with isolated selection for prefill vs. generation tokens**, and **reuses** the pattern thereafter. Experiments on Dream-7B-Instruct and LLaDA-1.5 show **up to 1.50× speedup** over FlashAttention at 64k context and 1,024 steps with negligible accuracy loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Practical & training-free:** No retraining; one-shot sparse mask + reuse; early-step full attention preserves quality.  \n- **Simple, hardware-friendly design:** Block-wise avg-pool → top-ρ% selection; **isolated selection** for prefill/generation.  \n- **Consistent wins at scale:** Clear latency gains at long contexts; speedups increase with more steps due to reuse."}, "weaknesses": {"value": "## Major\n- **Theoretical under-pinning is light.** No bounds/guarantees on error from mask reuse or head-specific sparsity; argument is largely empirical.  \n- **Model diversity is limited.** Only two DLMs (Dream-7B, LLaDA-1.5); generality to other DLM families or hybrid architectures is unproven.  \n- **Early-step sensitivity.** Skip ratio is tuned (20–30%) rather than principled; transferability across schedules/steps remains unclear.\n\n## Minor\n- **Amortized precompute cost** is not fully quantified in wall-clock breakdowns, though reuse intuition is sound.  \n- **Context vs. cache baselines.** A deeper head-to-head with cache-based methods across sequence lengths would help.  \n- **AR-sparse comparisons.** Positioning vs. structured AR patterns (e.g., Longformer/BigBird) could be elaborated."}, "questions": {"value": "1. **Skip ratio sensitivity:** How does accuracy/speed trade-off vary beyond 20–30% and across different total steps (e.g., 256 vs. 2048)? Any adaptive criterion?  \n2. **Dynamic prompts/streaming:** If prefill/generation boundaries change mid-run, can the mask be updated without losing reuse benefits?  \n3. **Precompute overhead:** What is the exact one-time cost for mask construction at 64k and how does it scale with heads/layers? (A timing table would help.)  \n4. **Composability with caches:** Any synergy or redundancy when combining SparseD with dKV-Cache/Fast-dLLM for short vs. long contexts?  \n5. **Very long contexts (128k+):** Does block granularity dominate selection quality; how does isolated selection behave there?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NGB4JtZDAM", "forum": "dwbrZtYP04", "replyto": "dwbrZtYP04", "signatures": ["ICLR.cc/2026/Conference/Submission2226/Reviewer_ibCB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2226/Reviewer_ibCB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762084900961, "cdate": 1762084900961, "tmdate": 1762916152731, "mdate": 1762916152731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}