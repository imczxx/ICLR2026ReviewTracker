{"id": "RIdmav4MZ7", "number": 21218, "cdate": 1758315042920, "mdate": 1759896934017, "content": {"title": "Freeze, Prompt, and Adapt: A Framework for Source-free Unsupervised GNN Prompting", "abstract": "Prompt tuning has become a key mechanism for adapting pre-trained Graph Neural Networks (GNNs) to new downstream tasks. However, existing approaches are predominantly supervised, relying on labeled data to optimize the prompting\nparameters and typically fine-tuning a task-specific prediction head—practices that undermine the promise of parameter-efficient adaptation. We propose Unsupervised Graph Prompting Problem (UGPP), a challenging new setting where\nthe pre-trained GNN is kept entirely frozen, labels on the target domain are unavailable, the source data is inaccessible, and the target distribution exhibits co-variate shift. To address this, we propose UGPROMPT, the first fully unsupervised GNN prompting framework. UGPROMPT leverages consistency regularization and pseudo-labeling to train a prompting function, complemented with\ndiversity and domain regularization to mitigate class imbalance and distribution mismatch. Our extensive experiments demonstrate that UGPROMPT consistently outperforms state-of-the-art supervised prompting methods with access to labeled\ndata, demonstrating the viability of unsupervised prompting as a practical adaptation paradigm for GNNs.", "tldr": "", "keywords": ["Graph Neural Networks", "Prompting", "Unsupervised"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/164a5052c0c8124f9adb281a23704f889138552b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Unsupervised Graph Prompting Problem (UGPP). To handle this problem, the authors propose UGPROMPT, a fully unsupervised prompting framework based on consistency regularization and pseudo-labeling.\nExtensive experiments across both graph- and node-level classification tasks demonstrate that UGPROMPT outperforms graph prompting baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is clearly presented.\n2. Extensive experiments demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The most critic weakness of this paper is the problem formulation. The studied problem UGPP in this paper is more like domain adaptation instead of graph prompting. As the comparison between this study and source-free domain adaptation (SFDA) in Introduction, the only difference is the adopted techniques: previous SFDA studies achieve this by learning GNN parameters while this work achieves this through learnable prompts. However, UGPP is essentially different from graph prompting in terms of accessible data (with/withour label information) and learning paradigms (unsupervised/supervised) for pre-training and adaptation. Hence, I encourage the authors to rewrite this paper as a study of SFDA through learnable prompts. \n2. Theoretical analysis is encouraged to be provided in the paper."}, "questions": {"value": "See the above Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cF82IEau7C", "forum": "RIdmav4MZ7", "replyto": "RIdmav4MZ7", "signatures": ["ICLR.cc/2026/Conference/Submission21218/Reviewer_BK6j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21218/Reviewer_BK6j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760757610148, "cdate": 1760757610148, "tmdate": 1762941628479, "mdate": 1762941628479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new problem setting called Unsupervised Graph Prompting Problem (UGPP), where the goal is to adapt a frozen pre-trained GNN to  inaccessible source data and unavailable target domain with covariate shift. To address this scenario, this paper proposes UGPROMPT, the first fully unsupervised GNN prompting framework. UGPROMPT leverages consistency regularization and pseudo-labeling to train a learnable prompting function, complemented with diversity and domain regularization to mitigate class imbalance and distribution mismatch. Extensive experiments show that UGPROMPT consistently outperforms state-of-the-art supervised prompting methods—even those using 25% to 100% labeled data—demonstrating the feasibility and effectiveness of unsupervised adaptation in the context of GNNs."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* Novel Problem Formulation: The UGPP setting is well-motivated and defined. This aligns with the parameter-efficient adaptation paradigm of LLM prompting but addresses unique challenges in graphs.\n* Technical Innovation: The proposed UGPROMPT framework creatively combines consistency regularization, pseudo-labeling, and novel regularization techniques to enable effective unsupervised prompting.\n* Comprehensive Evaluation: The paper provides thorough experiments across multiple datasets, tasks (node and graph classification), and base GNN architectures, demonstrating consistent improvements over supervised baselines.\n* Reproducibility: Code is provided, hyperparameters are detailed, and experimental protocols (e.g., 50 runs per setting) ensure statistical reliability."}, "weaknesses": {"value": "* Assumption Limitation: The assumption $P_T(Y \\mid X)=P_S(Y \\mid X)$,  (i.e., label-conditional distribution remains unchanged) limits applicability. In practice, label distribution shift may occur, which could severely degrade performance. This constraint needs more discussion as a key limitation.\n* Augmentation Sensitivity: Results in Table 3 suggest that high feature masking rates can hurt performance on datasets with continuous features (e.g., ENZYMES, DHFR). This indicates that the choice of augmentation must align carefully with data characteristics, potentially limiting generalizability. More discussion on how to choose or design augmentations would strengthen the paper.\n* Computational Overhead: Training involves multiple components (discriminator, augmentations), leading to higher training time than some baselines (though inference is efficient). This trade-off is noted but not thoroughly analyzed for resource-constrained settings."}, "questions": {"value": "* How does UGPROMPT's performance scale with the degree of distribution shift between source and target domains? Are there theoretical guarantees on when the method would fail?\n* UGPROMPT relies heavily on the pre-trained GNN’s ability to generalize knowledge from the source domain. If the source and target domains differ significantly in semantics (e.g., molecular graphs vs. social networks), do you expect the method to still work? Would stronger pre-training objectives or architectures improve robustness in such extreme transfer scenarios?\n* How is UGPROMPT's scalability to larger graphs (e.g., billions of nodes)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QrBkRkhlXj", "forum": "RIdmav4MZ7", "replyto": "RIdmav4MZ7", "signatures": ["ICLR.cc/2026/Conference/Submission21218/Reviewer_YXgK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21218/Reviewer_YXgK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876934014, "cdate": 1761876934014, "tmdate": 1762941628117, "mdate": 1762941628117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a challenging new problem setting called the Unsupervised Graph Prompting Problem (UGPP), which requires adapting a completely frozen, pre-trained GNN to new, unlabeled, and distribution-shifted data without access to the original source data. To solve this, the authors develop UGPROMPT, the first fully unsupervised GNN prompting framework, which uniquely utilizes a combination of consistency regularization, pseudo-labeling, and domain regularization. Experiments are conducted to verify its effectiveness."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Extensive experiments regarding both node classification and graph classification are conducted.\n- It’s novel trial for trying to establish a label-free graph prompting paradigm.\n- Code and data is provided for reproducibility."}, "weaknesses": {"value": "**Major Concerns:**\n\n1. **Clarity and Significance of Motivations:** The motivation for the proposed UGPP setting could be strengthened.\n    - The introduction (Line 47) frames the problem by contrasting it with methods that \"rely heavily on labeled data.\" This characterization may not fully capture the current state of graph prompting, as many state-of-the-art methods [1-3] or other models benchmarked in [4],  are already designed for label-scarce (e.g., 1-shot or 5-shot) scenarios. The authors are encouraged to clarify how UGPP's \"zero-label\" requirement offers a significant practical advantage over these \"few-label\" settings.\n    - The paper's second motivation (Lines 48-49), regarding the use of \"truly frozen\" models, also requires further clarification. The argument that projection heads prevent a GNN from being \"truly frozen\" is not immediately clear. The authors should elaborate on this distinction and, more importantly, substantiate the tangible benefits of a \"truly frozen\" GNN over other parameter-efficient adaptation approaches.\n2. **Related Work:** The literature review appears to conclude around early 2024. The authors should consider including and discussing several more recent and highly relevant contributions, for example [1–3], to situate the proposed framework within the most current research landscape.\n3. **Experimental Comparisons:**\n    - **Baselines:** To fully validate the \"state-of-the-art\" claims, the experimental comparison would benefit from the inclusion of several stronger, more recent baselines.\n    - **Result Discrepancies:** There appear to be discrepancies in the reported baseline results. The paper states it uses the code from [4], but the results in Table 2 differ notably from those in the original [4] paper. Furthermore, the performance reported here (with a 25% label ratio) seems to underperform the 1-shot or 3-shot results from [4]. The authors are requested to clarify the reasons for this difference to ensure a fair and reliable comparison.\n4. **Reliance on pseudo-labeling:**\n    - The method's reliance on pseudo-labeling (e.g., `FixMatch`) acts as a \"semantic anchor.\" This mechanism conceptually requires the target graph to share the identical class space as the source graph, tethering the solution to the specific pre-trained task.\n    - While the goal of prompting is often to adapt a model to diverse downstream tasks of various domains, this reliance on shared semantics seems to limit the method to adapting the model to relevant semantic domains and tasks. It seems that the proposed method does not transfer general structural comprehension;\n    - Further, the authors claim other graph prompting methods that rely on few downstream labels and projection heads are sub-optimal. While those methods may have costs, their application is still practical. In contrast, this model requires the match of the label **space. Is this rigid requirement not significantly less practical?\n5. **Conceptual Framing (Prompting vs. Domain Adaptation):** A conceptual concern arises regarding the framework's classification as a new \"prompting\" paradigm, distinct from Unsupervised Source-Free Domain Adaptation (SFDA).\n    - The authors argue that their method \"fundamentally differs\" from SFDA because the GNN remains frozen. This distinction, however, appears to be more of an *implementation* (parameter-efficient) choice rather than a *conceptual* one. Both approaches solve the same problem (adapting a fixed task) using the same core constraint (a shared class space, enforced by pseudo-labels).\n    - Therefore, the claim that this setting \"firmly places\" the work in a new paradigm may be overstated. It might be more accurate to position this as a novel, parameter-efficient *variant* of SFDA. Clarifying this positioning would strengthen the paper's claims.\n\n**Minor Concerns:**\n\n- Please review the citation formatting; parenthetical citations should generally use `\\citep`, reserving `\\citet` for in-text references.\n\n[1] All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining, KDD 2024.\n\n[2] Non-homophilic graph pre-training and prompt learning, KDD 2025.\n\n[3] DAGPrompT: Pushing the limits of graph prompting with a distribution-aware graph prompt tuning approach, WWW 2025.\n\n[4] ProG: A Graph Prompt Learning Benchmark, NeurIPS 2024."}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VSIaMhdZ27", "forum": "RIdmav4MZ7", "replyto": "RIdmav4MZ7", "signatures": ["ICLR.cc/2026/Conference/Submission21218/Reviewer_T4hS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21218/Reviewer_T4hS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890809338, "cdate": 1761890809338, "tmdate": 1762941627726, "mdate": 1762941627726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a parameter-efficient graph adaptation method that freezes the pre-trained GNN and learns only small prompts without labeled data to adapt to new graph datasets/tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It's impressive UGPrompt outperform competitive baselines without leveraging labeled data for adaptation.\n2. Prompts are small and modular, easy to plug into different backbones.\n3. Clear and well-defined setting, stricter than standard source-free adaptation, which is well-motivated.\n4. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. The proposed method heavily depends on pseudo-label confidence; when the backbone is poorly calibrated under distribution shift, performance may decline.\n2. Node classification requires ego-subgraph extraction, adding preprocessing overhead.\n3. The proposed method cannot handle label distribution shift, since the classifier head remains frozen."}, "questions": {"value": "It would be beneficial if other important graph learning tasks (regression, generation) could also be evaluated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZhBQGkTing", "forum": "RIdmav4MZ7", "replyto": "RIdmav4MZ7", "signatures": ["ICLR.cc/2026/Conference/Submission21218/Reviewer_LB4N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21218/Reviewer_LB4N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968437993, "cdate": 1761968437993, "tmdate": 1762941626952, "mdate": 1762941626952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}