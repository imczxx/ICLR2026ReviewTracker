{"id": "FQ8KX1pxeo", "number": 22729, "cdate": 1758334829477, "mdate": 1759896850158, "content": {"title": "Progressive Binarization with Semi-Structured Pruning for LLMs", "abstract": "Large language models (LLMs) have achieved remarkable progress in natural language processing, but their high computational and memory costs hinder deployment on resource-constrained devices. Binarization represents the most extreme form of quantization, yet binarized models still contain redundancy that can be further removed. Pruning provides a natural way to eliminate such redundancy, but na√Øve combination with binarization often results in severe performance degradation. In this paper, we propose Progressive Binarization with Semi-Structured Pruning (PBS$^2$P), a novel post-training framework that seamlessly integrates binarization and semi-structured pruning. We first propose Stepwise semi-structured Pruning with Binarization Optimization (SPBO), which progressively introduces sparsity while optimizing binarization parameters to jointly reduce pruning and quantization error, yielding more stable and accurate compression. Additionally, we propose a Coarse-to-Fine Search (CFS) that first allocates pruning ratios and then refines element selection, further enhancing overall performance. Extensive experiments across multiple LLM families show that PBS$^2$P consistently outperforms state-of-the-art (SOTA) binary post-training quantization methods in both perplexity and downstream accuracy. We will release all the code and models.", "tldr": "PBS$^2$P enables accurate sub-1-bit LLMs via seamless integration of semi-structured pruning and binarization.", "keywords": ["LLM", "Binarization", "Pruning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/810d21fed7548a13bf0d91fe0266c54c1cfb9dec.pdf", "supplementary_material": "/attachment/0d3e3b2f2041fd163a4d44c1eae745cc42103bc8.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces a pipeline for jointly applying 1-bit quantization and semi-structured pruning to Large Language Models (LLMs). The method is built upon a layer-wise closed-form solution for 1-bit quantization, governed by two control parameters, which minimizes the reconstruction error under the Frobenius norm. For pruning, a global strategy assigns layer-specific sparsity ratios based on inter-layer similarity, and the Optimal Brain Surgeon (OBS) framework is used to select the weights for removal. A key aspect of the pipeline is the iterative update of the quantization parameters after each pruning step to maintain accuracy. The constituent techniques are established, the key contribution lies in the integration of these methods into an end-to-end pipeline.  However, given that SparseGPT also supports joint quantization and semi-structured pruning using the OBS framework, the similarities between the two methods should be explicitly clarified. Experimental results demonstrate that the proposed method outperforms compared methods. However, to strengthen the validation, evaluations on more recent model families like Qwen and comparisons against state-of-the-art baselines such as OmniQuant and  ParetoQ should be included."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The constituent techniques are established, the key contribution lies in the integration of these methods into an end-to-end pipeline. Experimental results demonstrate that the proposed method outperforms compared methods. The presentation is easy to follow."}, "weaknesses": {"value": "This work presents a pipeline for joint quantization and semi-structure pruning, but it has several weaknesses that limit its current impact. The methodological description lacks clarity in key areas, such as the handling of activation flow during compression (independent vs. sequential) and the specific role of hyperparameters like the block size. The technical foundation is questioned, particularly the choice of the cosine similarity metric without a clear strategy for handling negative values or empirical validation of its distribution. The experimental validation is limited in scope, relying heavily on WikiText2 and older model families like LLaMA, while omitting stronger recent baselines and a thorough comparison to the closely related SparseGPT. Finally, the paper would benefit from a discussion of the method's applicability to broader architectures like MoE and diffusion models."}, "questions": {"value": "1. Regarding the formulation in Equation 2, it appears the compression objective for a linear layer uses the original, unmodified activations. Could you clarify the following? (a) Is each linear layer's compression treated as an independent objective, using the original network's activations? (b) Or is the compression applied sequentially, where the input to a layer comes from the already compressedprevious layer?\n\n2. You use cosine similarity, which ranges from [-1, 1], to gauge layer importance. Could you please clarify how your algorithm handles cases where the cosine similarity is zero or negative? The subsequent use of a reciprocal (i.e., 1/similarity) in your global pruning ratio assignment would be undefined or invert the intended importance ranking in such scenarios. Was this encountered in practice, and if so, how was it addressed?\n\n3. Could you show some empirical results of the cosine similarity values computed for the layers of a model (e.g., Llama-2 7B)?\n\n4. The results (Tables 1 and 2) indicate the use of a block size of 128. Could you please clarify the role of this parameter in your method? Specifically, is this block size exclusively for the 1-bit quantization process?\n\n5. The experimental evaluation currently reports perplexity results primarily on WikiText2. To more thoroughly and fairly assess the generalizability of the proposed method, it would be beneficial to follow the common practice established by your cited baselines (e.g., GPTQ, BiLLM). Could you please include perplexity results on additional standard datasets, such as PTB and C4?\n\n6. The experimental validation is conducted on established model families like LLaMA (1-3) and OPT. To further demonstrate the relevance and effectiveness of the method, it would be valuable to include results on more recent and widely-used models, such as the Qwen series.\n\n7. The method is presented in the context of standard dense transformer-based LLMs. Could you comment on its potential adaptability to other important model classes (e.g., MoE models, diffusion models)?\n\n8. From a general perspective, the goal of jointly performing quantization and pruning is also a key feature of the SparseGPT [1] framework. Specifically, SparseGPT supports various quantization bit-widths alongside semi-structured pruning, and similarly utilizes the OBS framework for weight selection and error minimization. Given these high-level similarities, could you please provide a more detailed discussion of the fundamental differences between your method and SparseGPT?\n\n9. The experimental comparisons would be strengthened by including recent state-of-the-art methods that support extreme low-bit quantization of LLMs, such as ‚Äã‚ÄãOmniQuant [2]‚Äã‚Äã and ‚Äã‚ÄãParetoQ [3]‚Äã‚Äã.\n\n10. The paper reports computational savings based on the latency of a single matrix multiplication operation. However, in real-world deployment, end-to-end inference time, which includes I/O overhead, memory access patterns, and other system-level bottlenecks, is a more meaningful metric for evaluating efficiency. Could you please provide measurements of the end-to-end inference latency (e.g., tokens/second) for a complete forward pass on a standard benchmark?\n\n[1] Frantar, Elias, and Dan Alistarh. \"Sparsegpt: Massive language models can be accurately pruned in one-shot.\" International conference on machine learning. PMLR, 2023.\n\n[2] Shao, Wenqi, et al. \"Omniquant: Omnidirectionally calibrated quantization for large language models.\" arXiv preprint arXiv:2308.13137 (2023).\n\n[3] Liu, Zechun, et al. \"Paretoq: Scaling laws in extremely low-bit llm quantization.\" arXiv preprint arXiv:2502.02631 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3AtzcGIipq", "forum": "FQ8KX1pxeo", "replyto": "FQ8KX1pxeo", "signatures": ["ICLR.cc/2026/Conference/Submission22729/Reviewer_zycs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22729/Reviewer_zycs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553435110, "cdate": 1761553435110, "tmdate": 1762942361835, "mdate": 1762942361835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PBS¬≤P (Progressive Binarization with Semi-Structured Pruning), a pure post-training framework that pushes LLMs down to 0.55‚Äì0.8 bit weight precision while retaining SOTA perplexity and zero-shot accuracy on LLaMA/OPT families. \nThe method alternates two components:\nSPBO ‚Äì step-wise N:M pruning followed by on-the-fly re-optimisation of binarisation scalars Œ±/Œº to reduce compound error.\nCFS ‚Äì a coarse-to-fine search that first allocates layer-wise pruning rates via cosine-similarity importance and then picks elements to prune by a Hessian-based second-order criterion."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written.\n2. The paper introduces PBS2P, a novel post-training framework that seamlessly integrates binarization (1-bit quantization) and semi-structured pruning (N:M sparsity), effectively reduces combined errors from pruning and quantization\n3. Ablation tests validate each component (e.g., SPBO, CFS metrics, pruning types), highlighting their necessity and superiority, which strengthens the method's credibility."}, "weaknesses": {"value": "1. The proposed method involves some predefined constants, such as N_high and N_low in CFS, and hyperparameters like Optimization Steps. It is unclear how to set the values of these predefined constants whether the settings of these constants affect the final compression effectiveness. (I am concerned that there may be difficulties or troubles in setting these constants during practical applications.)\n2. The paper only tested zero-shot tasks on relatively old models, such as the Llama1 and Llama2 series. If applied to stronger models (e.g., Llama3 or Qwen3 series) after quantization and pruning, how would it perform on zero-shot tasks?"}, "questions": {"value": "1. How sensitive is the method in the paper to calibration data? Does the distribution of calibration data have an impact? How should calibration data be selected for training?\n2. The paper demonstrates efficiency advantages in matrix multiplication. How much efficiency improvement can it bring in normal inference tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "duYW0li0eL", "forum": "FQ8KX1pxeo", "replyto": "FQ8KX1pxeo", "signatures": ["ICLR.cc/2026/Conference/Submission22729/Reviewer_hctq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22729/Reviewer_hctq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750963448, "cdate": 1761750963448, "tmdate": 1762942361433, "mdate": 1762942361433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PBS2P, a post-training framework that combines binarization with semi-structured (N:M) pruning for LLM compression. The method consists of two key components: (1) Stepwise Pruning with Binarization Optimization (SPBO), which jointly optimizes weight pruning and binarization parameters, and (2) Coarse-to-Fine Search (CFS), a two-stage strategy that first allocates pruning ratios based on layer importance and then selects specific elements using Hessian-based metrics. Experimental results on LLaMA and OPT model families demonstrate improvements over STBLLM and other binary post-training quantization methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tWell-motivated problem: Combining binarization with pruning to reduce redundancy and overcome performance degradation is a valuable research direction.\n2.\tComprehensive experiments: Extensive evaluation across multiple model families (LLaMA-1/2/3, OPT), datasets (perplexity and zero-shot), and model sizes demonstrates broad applicability.\n3.\tThorough ablations: Section 4.4 provides a good analysis of design choices (SPBO, search metrics, group size, etc.)."}, "weaknesses": {"value": "1.\tCertain techniques are not well explained, which may cause confusion and make reproduction difficult. See specific concerns in the Questions section below.\n2.\tComputational cost: Inverting block wise covariances even at size 128 is not cheap; the fine stage dominates runtime (109 min on 7B). Complexity and wall-time scaling to 65B/70B should be analyzed more carefully (per-layer cost, number of SPBO alternations œÑ, M‚àíN steps)."}, "questions": {"value": "1.\tThe notation in Equation 4 is confusing: what is the shape of 1? If 1 is just a column unit vector, Œº should be a scalar. However, the binarization center for each row should be different.\n2.\tFor the Coarse Stage, why not use gradient-based importance (e.g., Fisher information) or loss sensitivity?\n3.\tFor Equation 7, the \"+1/2\" term for rounding is not explained. Additionally, the concrete choices of N_high and N_low are not presented in the paper.\n4.\tTheorem 3.1 (Equation 8) is essentially a restatement of classical results from Optimal Brain Surgeon (Hassibi et al., 1993). The \"proof in supplementary\" claim doesn't add novelty‚Äîthis is a well-known second-order approximation. What is the difference between Theorem 3.1 and the results from OBS?\n5.\tComputational cost: 111 minutes for LLaMA-7B is 2.5√ó slower than ARB-LLM. For larger models (65B), this could be prohibitive. Is there any study on the computation time for large models?\n6.\tTable 4(c) only analyzes the LLaMA-7B model. While RI causes a degradation, the LI metric seems to provide only a small improvement. There should be more justification on more models for importance selection in the coarse stage and the necessity of adaptive assignment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4kvnboIiF4", "forum": "FQ8KX1pxeo", "replyto": "FQ8KX1pxeo", "signatures": ["ICLR.cc/2026/Conference/Submission22729/Reviewer_CQVV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22729/Reviewer_CQVV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820345225, "cdate": 1761820345225, "tmdate": 1762942361108, "mdate": 1762942361108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Progressive Binarization with Semi-Structured Pruning (PBS¬≤P) for compressing large language models (LLMs). The core component is SPBO (Stepwise Semi-Structured Pruning with Binarization Optimization), which progressively prunes a subset of elements at each step while jointly optimizing the binarized parameters, effectively reducing the combined error from pruning and binarization. In addition, the authors introduce a Coarse-to-Fine Search strategy to improve the accuracy of pruning element selection, further enhancing compression efficiency. Extensive experiments show that PBS¬≤P outperforms existing post-training quantization methods across various LLM families and evaluation metric."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Method design shows some innovation: The paper jointly optimizes pruning and binarization, using a stepwise strategy to reduce the error accumulation from single-step pruning.\n2. Comprehensive ablation studies: Experiments validate the contributions of the SPBO strategy as well as different metrics and pruning types to performance.\n3. Clear presentation: The writing is well-structured, and the workflow and formulas are described in detail, making the approach easy to understand."}, "weaknesses": {"value": "1. Limited innovation: Although the combination of stepwise pruning and quantization is experimentally validated, it essentially remains a combination of pruning and quantization, resulting in moderate to low novelty.\n2. Hardware support limitations: The paper adopts 5:8 and 6:8 N:M sparsity configurations, but public documentation shows that NVIDIA GPUs only natively support 2:4 sparsity. Therefore, higher-ratio sparsity may not achieve hardware acceleration in practice.\n3. Unclear hyperparameter selection: The method for setting ùëÅhigh and ùëÅlow is not specified, lacking theoretical justification or search strategy, which reduces reproducibility and interpretability.\n4. Optimality of mask decomposition not demonstrated: The stepwise progressive mask decomposition is not proven to be optimal, and there may exist schemes that achieve higher accuracy at the cost of longer runtime. The paper does not explore this trade-off.\n5. Method limitations: The SPBO‚Äôs stepwise updates rely on calibration data and multiple iterations, increasing computational cost. The impact on efficiency for large-scale model deployment is not thoroughly discussed."}, "questions": {"value": "1. How are ùëÅhigh and ùëÅlow selected? Is there a transferable principle or tuning strategy?\n2. For the 5:8 and 6:8 configurations, is hardware acceleration actually achieved, or are they only used for experimental comparison?\n3. Have other mask decomposition schemes been tried? Is there a better accuracy-runtime trade-off?\n4. What are the computational overhead and practical deployment costs of SPBO on large models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jhgkaen2Z3", "forum": "FQ8KX1pxeo", "replyto": "FQ8KX1pxeo", "signatures": ["ICLR.cc/2026/Conference/Submission22729/Reviewer_6aYr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22729/Reviewer_6aYr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856670814, "cdate": 1761856670814, "tmdate": 1762942360799, "mdate": 1762942360799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}