{"id": "FvMyAMbbX0", "number": 19583, "cdate": 1758297418248, "mdate": 1759897031786, "content": {"title": "ReactDance: Hierarchical Representation for High-Fidelity and Coherent Long-Form Reactive Dance Generation", "abstract": "Reactive dance generation (RDG), the task of generating a dance conditioned on a lead dancer's motion, holds significant promise for enhancing human-robot interaction and immersive digital entertainment. Despite progress in duet synchronization and motion-music alignment, two key challenges remain: generating fine-grained spatial interactions and ensuring long-term temporal coherence. In this work, we introduce $\\textbf{ReactDance}$, a diffusion framework that operates on a novel hierarchical latent space to address these spatiotemporal challenges in RDG. First, for fine-grained spatial control and artistic expression, we propose Hierarchical Finite Scalar Quantization ($\\textbf{HFSQ}$). This multi-scale motion representation effectively disentangles coarse body posture from subtle limb dynamics, enabling independent and detailed control over both aspects through a layered guidance mechanism. Second, to efficiently generate long sequences with high temporal coherence, we propose Blockwise Local Context ($\\textbf{BLC}$), a non-autoregressive sampling strategy. Departing from slow, frame-by-frame generation, BLC partitions the sequence into blocks and synthesizes them in parallel via periodic causal masking and positional encodings. Coherence across these blocks is ensured by a dense sliding-window training approach that enriches the representation with local temporal context. Extensive experiments show that ReactDance substantially outperforms state-of-the-art methods in motion quality, long-term coherence, and sampling efficiency.", "tldr": "ReactDance: A diffusion framework for reactive dance generation using hierarchical latent representation and achieving efficient long-term coherence through inference-training temporal alignment.", "keywords": ["diffusion", "dance generation", "motion generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cd5e243306433fe103cff6098964211f56e387ef.pdf", "supplementary_material": "/attachment/8646ae0a879e1b398d10b00dc9d6ccf3b30950f7.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the task of Reactive Dance Generation (RDG), with the specific goal of synthesizing high-fidelity, coherent, and long-form dance sequences that react to a lead dancer's motion and accompanying music. The authors identify two primary challenges in prior work: (1) modeling fine-grained spatial interactions between dancers, and (2) maintaining temporal coherence over long durations.\nTo tackle these issues, the paper introduces ReactDance, a diffusion-based framework built upon a novel hierarchical latent space. The core contributions of the work are twofold:\nHierarchical Finite Scalar Quantization (HFSQ): The authors propose a new multi-scale motion representation. This autoencoder-based module is designed to disentangle coarse-scale body posture from fine-grained limb dynamics, creating a structured latent space that facilitates hierarchical modeling.\nBlockwise Local Context (BLC): For long sequence generation, the paper puts forward a non-autoregressive sampling strategy. BLC partitions a long sequence into blocks and synthesizes them in parallel. It employs periodic causal masking and positional encodings to ensure local consistency within each block, aiming to mitigate the error accumulation and inefficiency associated with traditional autoregressive methods.\nThe authors evaluate their method on the DD100 paired dance dataset, comparing it against a suite of state-of-the-art baselines. The experiments cover metrics for motion quality, interaction quality, music-dance alignment, and generation efficiency. The reported results indicate that ReactDance achieves superior performance across the majority of these metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper demonstrate originality by combining and adapting ideas for the challenging task of long-form reactive dance generation. The application of a hierarchical quantizer (HFSQ), inspired by audio codecs, to motion representation is novel. The proposed Blockwise Local Context (BLC) sampling strategy is an innovative, non-autoregressive approach to generating long sequences, distinct from common iterative refinement or autoregressive methods.\n(2) The proposed ReactDance framework is a complete system that addresses the problem end-to-end. The quantitative results presented in Table 1 shows that the method outperforms state-of-the-art baselines across numerous metrics on the DD100 dataset. \n(3)The paper is well-written, clearly structured, and easy to follow. The authors effectively articulate the two main challenges in the field (fine-grained interaction and long-term coherence) and systematically introduce their proposed solutions (HFSQ and BLC). The diagrams, particularly the pipeline overview in Figure 4, are helpful in understanding the overall architecture."}, "weaknesses": {"value": "(1) Unsubstantiated Semantic Claims of HFSQ Representation: The paper's core claim that HFSQ \"explicitly disentangles\" motion semantics is a strong assertion that is not only unsubstantiated by evidence but also potentially disconnected from the method's actual implementation.\n    (a) In the Abstract (Lines 020-022), they state it \"...effectively disentangles coarse body posture from subtle limb dynamics...,\" and in the Introduction (Lines 089-090), they claim it \"...explicitly disentangles coarse global movements from fine-grained local details.\" However, the mathematical reality of HFSQ (Eq. 1) is that of a generic cascaded residual quantizer. It is optimized to minimize reconstruction error layer by layer, with no inherent mechanism to ensure that this mathematical decomposition aligns with a human-perceived semantic hierarchy (i.e., posture → limb details). The paper operates on the unproven assumption that low-frequency/high-amplitude errors correspond to \"coarse\" semantics, which may not hold true for complex dance motions.\n    (b) To bridge this gap and validate their claims, a layer-wise reconstruction visualization or valid proof of HFSQ for semantic disentanglement is needed. Without an experiment showing the decoded motion from each progressive layer of the hierarchy, the claims about semantic disentanglement and the resulting \"fine-grained control\" remain speculative assumptions.\n\n(2) The paper's central claim of achieving \"coherent long-form\" generation seems like a major overstatement, as it rests on an overly narrow definition of \"coherence\" and a method (BLC) that is fundamentally incapable of learning global choreographic structure.\n     (a): Loss of Global Context and Narrative: A complete dance performance possesses a clear structure (e.g., introduction, development, climax, conclusion). The BLC mechanism, by design, induces a form of \"global amnesia,\" resetting the temporal context for each block. This prevents the model from perceiving the absolute position of any given segment within the larger performance. Consequently, it is theoretically impossible for the model to generate a dance with a narrative arc. What it produces is a chain of locally-coherent but globally disconnected motion snippets, stitched together. The \"coherence\" achieved is merely low-level kinematic continuity (no drifting or freezing), not the high-level artistic and structural coherence.\n    (b): Unreliable \"Implicit\" Transition Learning: The authors' claim that dense sliding-window training enables the model to \"implicitly\" learn smooth transitions is an overly optimistic and unreliable assumption. The method lacks any explicit mechanism to ensure that the end of Block N is semantically and dynamically compatible with the start of Block N+1. While the model may learn generic start/end poses, this provides no guarantee of a meaningful transition at every arbitrary splice point. It is highly likely that many transitions, while kinematically smooth, are logically broken.\n    (c): Missing Global Structure Analysis: check for pattern repetition or mode collapse in long dance generation is required. The long-term sequence produced by BLC is very likely to be repetitive actions.\n\n(3) The paper's apparent success in long-form coherence might be a byproduct of the highly constraining nature of the conditioning signals (the leader's dense motion), which could, in turn, severely limit the model's generative diversity at a semantic level.\nIn a reactive dance task, the follower's motion is tightly coupled with the leader's on a frame-by-frame basis. This strong constraint simplifies the problem of maintaining kinematic continuity; the model's primary task becomes finding a valid \"following\" state for each frame, which naturally links the sequence together. The impressive coherence might therefore stem more from the problem's inherent constraints than from the architectural sophistication of BLC alone. This leads to a critical concern about diversity. The paper reports Divk and Divg metrics, but I still want to know: given the exact same leader motion, can the model generate fundamentally different but equally plausible reactions? Using a concrete example: given the same leader, can the model generate one sequence where the follower places a hand on the leader's shoulder, and another, distinct sequence where they place the hand on the waist? \n\n(4) The Training Paradigm Severely Limits Generalization: The model is trained on a small and specific dataset (1.95 hours, 10 genres). This raises serious concerns about its robustness and generalization ability. The paper provides no experiments on out-of-distribution data (e.g., different dance styles, non-dance interactions) or cross-dataset validation.\n\n(5) Contradictory Claims and Critical Omissions in Evaluation:\n    (a) The claim of modeling \"fine-grained limb movements\" is directly contradicted by the admission that all hand and finger motions are omitted. In partner dance, hands are the primary medium for physical communication and interaction, making their omission a critical failure for a model focused on fine-grained interaction.\n    (b) For a multi-person motion generation task, interpenetration is a primary failure mode. The paper completely lacks any quantitative metric to evaluate collision or penetration, which is a major oversight in assessing the physical plausibility of the generated interactions.\n    (c) The paper fails to ablate the influence of the two control signals (audio vs. leader motion) and does not test the model's understanding of interaction roles (e.g., by swapping the leader and follower in a pair).\n\n(6) The paper claims that LDCFG enables fine-grained control over different body parts like the upper and lower body. However, the mechanism for this control is explained ambiguously. The ability to control body parts does not stem from an intrinsic property of the HFSQ representation itself, which is a general-purpose quantizer. Instead, it originates from a crucial, explicit design choice in the input representation (Section 3.1), where the reactor's motion is manually split into separate streams (upper-body, lower-body, etc.), each processed by a dedicated HFSQ module. The paper fails to clearly attribute the control capability to this architectural choice, potentially misleading readers into believing that the HFSQ representation magically learns this spatial disentanglement. This lack of clarity should be rectified."}, "questions": {"value": "(1) Could you please provide a layer-wise reconstruction visualization? Specifically, what does the output motion look like when decoded from only the first HFSQ layer, and how does it progressively refine as subsequent layers are added? This is crucial to substantiate your claim of coarse-to-fine semantic disentanglement.\n(2) Given that the model is only trained on 8s clips, how do you argue it can learn long-term choreographic structure beyond just kinematic continuity? Could you provide an analysis of a very long generated sequence (e.g., >2 minutes) to demonstrate its semantic diversity and structural integrity, for instance, by showing it does not fall into repetitive loops? if it works, a convincing explanation is also needed.\n(3) On Generalization and Robustness: How does your model perform on out-of-distribution data? For example, what is the generated reaction if the leader performs a dance style not present in DD100 (e.g., popping) or a non-dance interaction (e.g., a handshake)?\nOn Evaluation and Missing Details:\n(5) Could you provide a quantitative evaluation of interpenetration between the dancers?\n(6) How do you reconcile the \"fine-grained\" claim with the complete omission of hand interactions, which are arguably the most fine-grained and crucial part of partner dance?\n(7) Could you provide an ablation study that disentangles the contributions of the audio and leader motion conditions to the final output?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3u5U2BAsHY", "forum": "FvMyAMbbX0", "replyto": "FvMyAMbbX0", "signatures": ["ICLR.cc/2026/Conference/Submission19583/Reviewer_dk6L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19583/Reviewer_dk6L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760880479340, "cdate": 1760880479340, "tmdate": 1762931454101, "mdate": 1762931454101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a HFSQ+diffusion framework for long-term reactive dance generation and achieves superior performance. Specifically, the paper first proposes a Hierarchical Finite Scalar Quantization, which combines FSQ with a hierarchical structure, to generate the complex motion in a coarse-to-fine manner. Furthermore, to enable the long-term dance generation, the paper proposes a Blockwise Local Context. The BLC generates each subsequence in parallel, while the inter-block continuity is ensured by dense sliding window training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed HFSQ is technically sound for fine-grained motion generation.\n2. The proposed BLC seems to effectively solve the long-term generation issue with improved sampling efficiency.\n3. The method shows an impressive result according to the quantitative metrics and the video provided in the supplementary."}, "weaknesses": {"value": "1. It is still unclear to me why the continuity is ensured in parallel inference. Why the model can \" implicitly learns how to naturally begin and end motion phrases from any temporal point\"？ How does this related to the dense sliding window？  Could the author further explain it?\n\n2. To provide a more robust assessment and validate the effectiveness of proposed kinematic loss, the physical plausibility metrics should be added, such as PFC [1].\n3. I think HFSQ is proved to be effective for generating more realistic dance in a corse-to-fine manner, yet the claim that this training manner can \"disentangles coarse body posture from subtle limb dynamics\" has not been fully verified. The HFSQ can certainly produce a more accurate reconstruction motion, but it is not necessarily to be limb dynamics. Experiments has not validated this claim either.\n4. In line 126-127, the author claim that DuetDance's [2] coupled representation limits motion diversity. However, this does not seem to hold true based on the metrics in the original paper. Could the author provide more explanation?\n   \n[1] Jonathan Tseng, Rodrigo Castellon, and C. Karen Liu. EDGE: Editable dance generation from music. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 448–458. IEEE, 2023.  \n[2]  Anindita Ghosh, Bing Zhou, Rishabh Dabral, Jian Wang, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek, and Chuan Guo. Duetgen: Music driven two-person dance generation via hierarchical masked modeling. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pp. 1–11, 2025."}, "questions": {"value": "1. The clarification of how the BLC can solve the inter-block continuity in parallel.\n2. The experiment should validate the claim of the fine-grained generation of \"subtle limb dynamics\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hWnxP7iSDw", "forum": "FvMyAMbbX0", "replyto": "FvMyAMbbX0", "signatures": ["ICLR.cc/2026/Conference/Submission19583/Reviewer_5N4J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19583/Reviewer_5N4J"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937544466, "cdate": 1761937544466, "tmdate": 1762931453590, "mdate": 1762931453590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses the dance accompaniment problem (or, as the authors term it, reactive dance generation) where the input consists of a lead motion and corresponding music, and the goal is to generate the follower motion.\n\nThe main challenge of this task lies in its spatio-temporal complexity: how to efficiently and accurately represent motion in space while maintaining temporal coherence and preventing error accumulation over time.\n\nTo tackle this, the authors propose a hierarchical Finite Scalar Quantization (FSQ) model combined with a diffusion-based generation framework. The method builds upon FSQ with residual encoding and integrates it into the diffusion process to produce high-quality accompanying motions.\n\nThe paper explores a range of implementation details. For example, it introduces Blockwise Local Context, which investigates how to make diffusion more context-aware, and Layer-Decoupled Classifier-Free Guidance, designed to apply classifier-free guidance to hierarchical SQ targets.\n\nThe proposed approach achieves promissing results. Both quantitatively and qualitatively, as demonstrated in the accompanying videos. The work is also thorough, with extensive discussions, detailed methodology, and comprehensive ablation studies.\n\nIn addition, I feel some aspects of this work are particularly insightful and valuable. The hierarchical representation of motion using FSQ, its impact on motion quality, and the overall FSQ+diffusion generation process provide meaningful contributions that could inspire future research in the community."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Technical Comments:**\n\nOverall, this paper is quite novel in its design and methodology.\n\n\n1. The paper identifies the limitations of FSQ in motion representation space and explores hierarchical and residual variants of FSQ for richer motion encoding.\n\n\n2. Instead of following the conventional Quantize → GPT / autoregressive pipeline, the paper investigates a diffusion model targeting the dequantized FSQ features. This is particularly interesting because it implicitly raises an important question: Can the dequantized FSQ serve as an effective tokenizer for DiT?\nThe authors compare their approach with a 2-layer RVQ baseline, but it would also be valuable to explore other tokenization or encoding schemes (e.g., VAE) to understand their influence on the subsequent diffusion process.\n\n3. The proposed Blockwise Local Context mechanism allows causal modeling within each block while maintaining synchronized blockwise generation. This design is elegant and effectively mitigates the error accumulation problem typically seen in long autoregressive inference.\n\n**Experiments:**\nQuantitatively, the results consistently outperform state-of-the-art baselines. The visual quality is also impressive, and the ablation studies are comprehensive.\n\n**Writing and Presentation:**\nThe overall writing is clear. Although the demo video contains some background noise, it still effectively demonstrates the model’s performance and validity."}, "weaknesses": {"value": "1. Although the Discussion section provides some interpretation of LDCFG, the explanation remains insufficiently clear. For example, how is  {s_coarse,s_fine} sampled during training? What exactly is defined as the coarse part and what as the fine part?\n\n2. As mentioned in the Strengths section, the authors found that both HFSQ and PM contribute positively to the subsequent diffusion process. However, no further exploration was conducted. For instance, how would a VAE perform as the diffusion encoder/tokenizer? Or how would RVQ + PM function as a tokenizer?\n\n3. The video presentation is acceptable overall but suffers from recording issues, such as noticeable background noise and the absence of background music in some examples.\n\nMinor issue:\n\nLine 49: quotation mark formatting problem."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ziCxZeUgPv", "forum": "FvMyAMbbX0", "replyto": "FvMyAMbbX0", "signatures": ["ICLR.cc/2026/Conference/Submission19583/Reviewer_5QeS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19583/Reviewer_5QeS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974778504, "cdate": 1761974778504, "tmdate": 1762931453251, "mdate": 1762931453251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ReactDance proposes a two-stage framework based on diffusion for long-duration reactive dance generation. The paper designs a hierarchical latent representation called Hierarchical Finite Scalar Quantization (HFSQ), which decouples coarse body posture from fine-grained body dynamics, enhancing spatial detail and controllability. To efficiently and coherently generate ultra-long sequences, the authors introduce Blockwise Local Context (BLC) combined with dense sliding-window training, enabling parallel block sampling to avoid error accumulation. Additionally, independent guidance weights are applied at each layer through Layer-Decoupled Classifier-Free Guidance (LDCFG), offering fine-grained control over different semantic scales."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Hierarchical Finite Scalar Quantization (HFSQ) provides a stable, continuous multi-scale latent that disentangles coarse global posture from fine local articulations, enabling high-fidelity, layerwise control of motion detail.\n\n2. Blockwise Local Context (BLC) provides a stable, continuous multi-scale latent that disentangles coarse global posture from fine local articulations, enabling high-fidelity, layerwise control of motion detail.\n\n3. The paper is well-organized with a clear structure and lucid exposition that make the motivation, method, and technical details easy to follow.  \n\n4. The experiments and comparisons are comprehensive and rigorous, covering multiple baselines, ablations, quantitative metrics, qualitative visualizations, and a user study."}, "weaknesses": {"value": "1. The HFSQ design does not specify the number of residual stages R, leaving unclear the representational capacity and the coarse–fine trade-off, which hinders reproducibility and complicates tuning of per-stage guidance weights s_r.  \n\n2. The BLC training/inference protocol is underspecified (e.g., whether sliding-window stride m can cross T), so it is unclear how periodic causal masking affects cross‑block continuity."}, "questions": {"value": "1. From the appendix, does the LDCFG setup imply the HFSQ has only two residual stages (i.e., R=2)? If so, please clarify the generality of your design and report results for other values of R to show scalability.  \n\n2. The reported LDCFG experiments use equal guidance weights (s_1=s_2 = 1.2); how does this demonstrate the benefit of layer‑decoupling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8kK0WzPPW7", "forum": "FvMyAMbbX0", "replyto": "FvMyAMbbX0", "signatures": ["ICLR.cc/2026/Conference/Submission19583/Reviewer_X7nk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19583/Reviewer_X7nk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985830713, "cdate": 1761985830713, "tmdate": 1762931452628, "mdate": 1762931452628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}