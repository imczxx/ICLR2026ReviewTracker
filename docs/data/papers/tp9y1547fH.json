{"id": "tp9y1547fH", "number": 23558, "cdate": 1758345430621, "mdate": 1759896807981, "content": {"title": "$\\alpha$-GFN: Generalized Mixing in GFlowNets for Better Exploration-Exploitation Trade-off", "abstract": "Standard Generative Flow Network (GFlowNet) training implicitly assigns equal weights to the forward and backward policies, a consequence of the flow-matching view that constrains the exploration–exploitation dynamics.\nExtending the connection between GFlowNets and Markov chains, we show that this equal weighting arises from a theoretical equivalence between GFlowNet objectives and Markov chain reversibility.\nBuilding on this, we introduce $\\boldsymbol{\\alpha}\\textbf{-GFNs}$, which generalize standard GFlowNet training from strictly balanced flows to imbalanced flows by mixing the forward and backward policies with a hyperparameter $\\alpha$ in the training objectives. Through the link to reversibility, we further establish that such objectives converge to unique flows. This generalization provides a richer exploration–exploitation trade-off and, in some settings, coarse control over trajectory lengths. We also propose a simple scheduling algorithm to combine the strengths of different $\\alpha$ values. Experiments on Set Generation, Bit Sequence Generation, and Molecule Generation demonstrate consistent performance gains and highlight the benefits of $\\alpha$ tuning.", "tldr": "We propose α-GFNs: flexible forward–backward policy mixing with a parameter α that controls exploration–exploitation, grounded in an extended GFlowNet–Markov chain connection.", "keywords": ["GFlowNet", "Markov chain", "Generative Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5536b692f15431b7f1596a573ae8f08bdfc94a50.pdf", "supplementary_material": "/attachment/134caf03cff78bd788cffbd8f4dcba0145ef85b0.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces α-GFlowNets (α-GFNs), a generalization of Generative Flow Networks (GFlowNets) that allows unequal mixing of forward and backward policies using a tunable hyperparameter α ∈ (0, 1). This breaks the traditional assumption of equal weighting and provides explicit control over exploration vs. exploitation during training. The theoretical ground is mostly solid and the method is easy to understand."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Strong Theoretical Innovation: \nThis paper proposes a generalized framework (α-GFN) that extends traditional “balanced flow” GFlowNets to imbalanced flows via a tunable α parameter. This paper establishes a formal equivalence between standard GFlowNet objectives (DB, SubTB, TB) and the reversibility condition of Markov chains.\n\n- Simple yet Effective Design\nThe method requires minimal modification—just introducing a scalar α into existing losses (DB, SubTB, TB). The two-stage scheduling algorithm (annealing α from off-center to 0.5) is conceptually simple, easy to implement, and compatible with existing training setups.\n\n- Interesting Secondary Finding\nDiscovers that α indirectly controls trajectory length, offering new insight for length-controllable GFlowNet sampling, a potential future research direction."}, "weaknesses": {"value": "There are several substantial weaknesses in this paper:\n\n* **W1. Missing baselines.** Although I do not usually emphasize the need for additional baselines, some critical ones are missing here.\n\n  * *Kim et al. (ICLR 2025), \"Adaptive Teachers for Amortized Samplers\"*\n  * *Lau et al. (NeurIPS 2024), \"QGFN: Controllable Greediness with Action Values\"*\n\nBoth methods focus on improving the sampling efficiency and performance of GFlowNets. For instance, the adaptive teacher approach leverages a teacher model to guide exploration more effectively. Since this paper also evaluates mode discovery and average reward as its primary metrics, omitting these stronger baselines (and relying only on TB, DB, and FL-TB) weakens the empirical comparison.\n\n* **W2. Novelty concerns.** The paper presents limited novelty. The main contribution:unequal mixing of forward and backward policies, requires only minimal modifications to existing GFlowNet implementations. Even though a scheduling algorithm is proposed to adjust the value of $\\alpha$, this addition does not appear to offer substantial conceptual advancement over prior work.\n\n* **W3. Reproducibility concerns.** The authors do not provide code, and the described method is simple enough to reimplement. However, when I attempted to reproduce the results using the unequal mixing strategy, the performance was significantly worse than the results reported in the paper. While I do not intend to suggest any data fabrication, this discrepancy indicates possible implementation or reporting inconsistencies.\n\n* **W4. Clarity issues.** Several parts of the paper lack clarity. For example, the authors state that “Corresponding to imbalanced flows, unequal weights can enhance the performance of GFlowNets, as illustrated in Fig. 1,” but *Fig. 1* only depicts schematic arrows between forward and backward policies, without any actual illustration of performance enhancement. Moreover, the term **“unique flows”** appears multiple times, yet it is not defined anywhere. Since this terminology is uncommon in the GFlowNet literature, its undefined usage hinders readability and understanding, making the paper hard to follow.\n\n* **W5. Exaggerated results.** The second subfigure of *Fig. 1* claims a **270× improvement** over TB, but neither the task setup nor the precise definition of the “average reward” metric in this context is provided. This lack of detail makes the claimed improvement difficult to verify and potentially misleading.\n\n- **W6. Concerns about performance.** The performance improvements achieved by the proposed method appear to be marginal on both the bit sequence generation and molecule generation tasks. For instance, the results of TB with $\\alpha = 0.9$ are only about 1.5× higher than the baseline (TB with $\\alpha = 0.5$), whereas methods such as QGFN report nearly 4× improvements over TB under similar settings. Furthermore, the Spearman correlation is lower in several cases, suggesting that the proposed approach may not consistently enhance learning stability.\n\n- **W7. Violation of the flow structure.** Does Equation (3) violate the fundamental principle of GFlowNets? By definition, a GFlowNet performs probabilistic inference—its goal is to learn to sample from a target distribution induced by a given reward function, ensuring that the total inflow to each state equals its total outflow. However, the proposed method introduces an imbalance through the $\\alpha$-weighted mixing, which appears to disrupt this flow condition. This raises serious concerns about the theoretical soundness of the approach, as this property is the core foundation of the GFlowNet framework. If the flow structure is indeed preserved, please prove that the global minimization of your objective implies that the learned action policy samples proportionally to $R$. \n\n- **W8. More solid experimental results.** Beyond the number of modes and Spearman correlation, there exist additional quantitative metrics to assess the performance of GFlowNets, such as ELBO and EUBO, as reported in “Adaptive Teachers for Amortized Samplers.” Including these metrics would make the evaluation more thorough and convincing. Moreover, the current experimental setups appear relatively simple and may not fully demonstrate the proposed method’s effectiveness. The paper would be significantly strengthened by scaling the experiments to more realistic and complex scenarios, such as language model fine-tuning tasks (“Amortizing Intractable Inference in Large Language Models”) or listwise recommendation tasks (“Generative Flow Network for Listwise Recommendation”).\n\n\n- **W9. Inconsistent experimental setup.** In set generation task, three baselines are reported: DB, FL-DB and TB. In bit sequence generation tasl, three different baselines are reported: DB, SubTB and TB. As for molecule generation task, five baselines are reported: DB, FL-DB, SubTB, FL-SubTB and TB. Such inconsistencies make it difficult to perform a fair comparison and to clearly assess the effectiveness of the proposed method. The authors are encouraged to align the experimental setups across tasks to ensure consistency and fairness in evaluation. \n\nMinor mistakes:\n-  **Mistakes in formulations.** The equation on *line 964* contains an error in the term $$\\frac{1}{2}P_B(s_{k+i-1}\\mid s_{k+i}).$$ Additionally, there is a typographical error in *Formulation (21)* on *line 1039*, where $P_{(\\alpha)}$  should be corrected to $P_{\\alpha}$.\n\n- **Mismatch with previous works.** The number of modes reported for the baseline methods in the molecule generation task is notably lower than that reported in previous studies. While I understand that differences in experimental setups can lead to variations in results, could you further clarify why your reported results are significantly worse?"}, "questions": {"value": "I think I've said all that needs to be said elsewhere."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "umI0EFgqF1", "forum": "tp9y1547fH", "replyto": "tp9y1547fH", "signatures": ["ICLR.cc/2026/Conference/Submission23558/Reviewer_Cy2F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23558/Reviewer_Cy2F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760938838384, "cdate": 1760938838384, "tmdate": 1762942711720, "mdate": 1762942711720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes $\\alpha$-GFlowNets, which generalizes standard GFlowNets by introducing a mixing coefficient $\\alpha$ to balance exploration and exploitation. By replacing the fixed equally mixed policy ($\\alpha=0.5$) with a flexible $\\alpha$-weighted mixture of forward and backward policies, the method offers a flexible way to control exploration-exploitation in online training of GFlowNets. The approach shows consistent performance gains across various generative tasks, such as set, sequence, and molecule generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to follow, with clear motivation and intuitive explanations.\n\n- The proposed method is easy and straightforward to implement for exploration–exploitation control. \n\n- It is insightful and interesting that the mixing coefficient $\\alpha$ is introduced from the connection between Markov chain reversibility and the GFlowNet formulation.\n\n- The proposed method demonstrates strong empirical performance across diverse benchmarks, showing improved exploration and sample diversity."}, "weaknesses": {"value": "- My first concern stems from the ambiguity in proving equally mixed policy:\n$P_{\\text{0.5}} = \\frac{1}{2} P_F + \\frac{1}{2} P_B$.\nThe authors state that if $P_F(s'|s) > 0$, then $P_F(s|s') = 0$ in Line 967, as $P_F$ is one-directional. Here, it seems that the authors interpret $P_F(s|s')$ as $P_F(s_{t+1} = s ,|, s_t = s')$. However, to me, it seems that the equation should instead be written as $P_F(s|s') = P_F(s_t = s ,|, s_{t+1} = s')$, which is non-zero, since\n$P_F(s_t = s ,|, s_{t+1} = s') = P_F(s_{t+1} = s' ,|, s_t = s) \\frac{P_F(s_t = s)}{P_F(s_{t+1} = s')}$\n(I think this misunderstanding might stem from time conditioning). Could the authors further clarify this point?\n\n- How does the role of $\\alpha$ differ from temperature-scaling of rewards in terms of exploration–exploitation? Furthermore, why do the authors not compare it with the temperature-scaling method? Although the authors derive $\\alpha$ from a different perspective, its role seems the same as that of the temperature parameter. Could the authors further clarify this point?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s6MfrXIAGQ", "forum": "tp9y1547fH", "replyto": "tp9y1547fH", "signatures": ["ICLR.cc/2026/Conference/Submission23558/Reviewer_5Rz4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23558/Reviewer_5Rz4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636741615, "cdate": 1761636741615, "tmdate": 1762942711380, "mdate": 1762942711380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes $\\alpha$-GFN, a generalization of GFlowNet conventional training, where forward and backward policies are mixed with a tunable coefficient \\(\\alpha\\in(0,1)\\). To derive the $\\alpha$-GFN, the author first connects standard GFlowNet objectives (DB/SubTB/TB) to sub-trajectory reversibility of Markovian chains under the equally mixed kernel $P_{0.5}=\\tfrac12 P_F+\\tfrac12 P_B$; Then they extends this to $P_\\alpha=\\alpha P_F+(1-\\alpha)P_B$, leading to the derivation of $\\alpha$-GFN objectives that admit unique flow solutions. The paper further introduces a simple two-stage $\\alpha$-schedule that anneals toward 0.5, thereby recovering the standard GFlowNet target at convergence. Experiments on set generation, bit-sequence generation, and a molecule generation demonstrate improvements and suggest that  $\\alpha$ serves as an interpretable exploration–exploitation knob."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper provides an  advanced elucidation of the connections between MCMC sampling and GFlowNets.\n\n* The mapping from DB/SubTB/TB balance conditions to reversibility under $P_{0.5}$, and the extension to $P_\\alpha$ is conceptually neat and well-motivated.\n\n* The $\\alpha$-mixing idea applies across canonical objectives (DB/SubTB/TB) and forward-looking variants with minimal code changes.\n\n* The gradient perspective clarifies how $\\alpha$ biases credit assignment toward exploitative $\\alpha>0.5$ or exploratory regimes $\\alpha<0.5$.\n\n* The proposed two-stage annealing schedule is simple to implement and integrates naturally into existing GFlowNet workflows."}, "weaknesses": {"value": "* The key limitation is that while the $\\alpha$-balance condition can ensure a unique flow at convegence. This flow may not be equal to the unique target flow $F^\\ast(\\tau)=P_B(\\tau|x)R(x)$.  Although the annealing strategy that gradually pushes $\\alpha$ toward $0.5$ can alleviate this issue, further theoretical or empirical study of the gaps between the unique flow under $\\alpha\\neq 0.5$ and the target flow $F^\\ast$ corresponding to $\\alpha=0.5$ would strengthen the paper’s claims.  ( I note that in Figure 3, only the choices of $\\alpha$ closer to 0.5 would give good performance. )\n\n*  In addition to reporting top-k average rewards or number of modes (used for evaluating exploration capability), please also report diversity/similarity metrics for all three set of experiments. This would make it clearer whether varying $\\alpha$ indeed provides an explicit control over the exploration–exploitation trade-off.\n\n* According to Fig.17 and 18, the performance gap between $\\alpha$-GFN and vanilla GFN is trivial for the real-world benchmarks."}, "questions": {"value": "1. What's the definition of $f$ in line 6 of Algo.1 ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1zPq6Visbk", "forum": "tp9y1547fH", "replyto": "tp9y1547fH", "signatures": ["ICLR.cc/2026/Conference/Submission23558/Reviewer_qdTi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23558/Reviewer_qdTi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981057161, "cdate": 1761981057161, "tmdate": 1762942711004, "mdate": 1762942711004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents $\\alpha$-GFN, a novel and well-motivated generalization of GFlowNets that introduces a hyperparameter $\\alpha$ to control the weighting between the forward and backward policies in the training objective by viewing GFN as an MDP with transition $P=\\alpha P_F+(1-\\alpha)P_B$. The authors derive the corresponding novel training objectives, and scheduling strategies of $\\alpha$, leading to better exploration-exploitation trade-off and hence significant performance gains in discovering diverse, high-reward modes across several tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The core idea of inbalanced mixing is a novel and well-motivated generalization of GFN. Both the theoretical foundation and the empirical results are solid. The analysis of the exploration-exploitation behavior and $\\alpha$ is insightful."}, "weaknesses": {"value": "I feel that the authors leave too many required theoretical analysis to the appendix, making the main body somehow hard to follow."}, "questions": {"value": "1. When $\\alpha\\neq0.5$, does the terminating distribution induced from the forward policy $P_F$ match with the reward $R$? This is an essential question that motivates the design of $\\alpha$-scheduling, which I think the authors should clarify in earlier sections.\n2. In appendix B.4, the authors assume $P_B$ to have the same stationary distribution as $P$. Can the authors explain the reason for this assumption further?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K5IIveK9vi", "forum": "tp9y1547fH", "replyto": "tp9y1547fH", "signatures": ["ICLR.cc/2026/Conference/Submission23558/Reviewer_UYFs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23558/Reviewer_UYFs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990732323, "cdate": 1761990732323, "tmdate": 1762942710824, "mdate": 1762942710824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}