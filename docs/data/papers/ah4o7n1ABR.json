{"id": "ah4o7n1ABR", "number": 22797, "cdate": 1758335515340, "mdate": 1759896845655, "content": {"title": "Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation", "abstract": "Diffusion models excel at generation, but their latent spaces are not explicitly organized for interpretable control. We introduce ConDA (Contrastive Diffusion Alignment), a framework that applies contrastive learning within diffusion embeddings to align latent geometry with system dynamics. Motivated by recent advances showing that contrastive objectives can recover more disentangled and structured representations, ConDA organizes diffusion latents such that traversal directions reflect underlying dynamical factors. Within this contrastively structured space, ConDA enables nonlinear trajectory traversal that supports faithful interpolation, extrapolation, and controllable generation. Across benchmarks in fluid dynamics, neural calcium imaging, therapeutic neurostimulation, and facial expression, ConDA produces interpretable latent representations with improved controllability compared to linear traversals and conditioning-based baselines. These results suggest that diffusion latents encode dynamics-relevant structure, but exploiting this structure requires contrastive organization and traversal along the latent manifold.", "tldr": "We present ConDA, a contrastive framework that structures diffusion latents for nonlinear latent interpolation, enabling interpretable and controllable generation.", "keywords": ["Diffusion Models", "Contrastive Learning", "Latent Space Structuring", "Controllable Generation", "Nonlinear Latent Manifolds", "Latent Interpolation", "Representation Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ea48b6a4ad6cae0dda6c622df6dafcef0b4cd36.pdf", "supplementary_material": "/attachment/f2884cb188e8b7d6bb84701eb3c1297838235077.zip"}, "replies": [{"content": {"summary": {"value": "Diffusion models demonstrate strong generative capabilities; however, their latent spaces often lack structure. Recent advances have improved aspects such as interpolation and temporal consistency, but they still fail to organize diffusion latents for controllable dynamics. The key missing element is a mechanism to align latent geometry with system variables while maintaining generative fidelity. To address this, the authors propose ConDA, a diffusion framework that fulfills this requirement by introducing an additional latent space optimized through contrastive learning, enabling smooth trajectory traversal. The authors support their approach with validations across multiple domains."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors clearly motivate the need for controlled sequence generation in the introduction, and both the quantitative and qualitative results appear impressive."}, "weaknesses": {"value": "The paper lacks sufficient clarity, making it difficult to fully assess its contributions. See the questions section for details"}, "questions": {"value": "There are several points that could benefit from clarification:\n* The full training/generation pipelines of the model are not clear enough. An algorithm of each process would improve clarity.\n* Does the model generate a single image or a sequence? does a sequence serve as a single data point used to train the model? The problem is framed as a controlled sequence generation but some of the notation are defined on a single $x$ and the connection to the sequence level is not very clear.\n* a running example could help to visualize some of the authors intuition.\n\nI would be happy to reconsider my score if the authors could provide some additional clarification of their method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JSpLKIHeD3", "forum": "ah4o7n1ABR", "replyto": "ah4o7n1ABR", "signatures": ["ICLR.cc/2026/Conference/Submission22797/Reviewer_4TDW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22797/Reviewer_4TDW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742959935, "cdate": 1761742959935, "tmdate": 1762942391291, "mdate": 1762942391291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a new method to structure latent spaces of diffusion models for more controlled generation, particularly in the setting of data with existing dynamical (spatiotemporal) structure. Specifically, they use a contrastive learning algorithm within the embedding space to enforce latent traversals to correspond to dynamical factors of the input. They validate the interpolation method of their model by comparing against a range of linear and non-linear baselines, demonstrating that their latent space nonlinear interpolation schemes perform significantly better."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The authors approach the important problem of controlled generation in diffusion models, and do it from the welcome perspective of introducing structure into diffusion model latent space. \n- The paper is generally well written with sufficient related work discussion, background, and experimental details in the main text. \n- The model appears to be useful for modeling neural dynamics with diffusion, a relevant domain which is in need of such methods. \n- The empirical results are strong, comparing against relevant baselines and demonstrating performance improvements."}, "weaknesses": {"value": "- The limitations section is incredibly brief and only mentions very vaugely that trade-offs 'are managable'."}, "questions": {"value": "- In the summary of related work you say: \"Prior work advances interpolation, video realism, and disentanglement, but none provide an identifiable framework for organizing pretrained diffusion latents for nonlinear, interpretable, controllable generation.\" Is this implying that your work has some identifiability guarantees? If not (which I believe not to be the case since it it not mentioned elsewhere), is this something that you've looked into, considering the similarity of your method to Non-linear ICA type methods? \n- Can the authors describe how they might manage the limitations that they propose in their conclusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "meFmb5IqzV", "forum": "ah4o7n1ABR", "replyto": "ah4o7n1ABR", "signatures": ["ICLR.cc/2026/Conference/Submission22797/Reviewer_7Ybs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22797/Reviewer_7Ybs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947764081, "cdate": 1761947764081, "tmdate": 1762942391021, "mdate": 1762942391021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new method for editing objects in the latent representation of diffusion models. This achieved via a second latent space akin to the $\\mathcal W$ space of StyleGAN. By training a contrastive embedder this space should achieve nicer properties for editing than the usual latent space. They then test using this new latent space for editing on fluid dynamics, neural calcium imaging, therapeutic neurostimulation, and facial expression problems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Tackles an interesting and important problem.\n* The core idea of using contrastive embeddings on the latent space seems to be my novel according to my knowledge.\n* Interesting experimental breadth."}, "weaknesses": {"value": "## Primary concerns\n### DDIM Inversion and other approximations\n* There are several flaws with DDIM inversion which has spun an entire sub-field of research (more on this later).\n  * Equation (10) is rearranged to find it's analytical reverse (this equation should be number fyi) on the line below. This equation is an implicit Euler scheme and needs to be solved via fixed-point iteration and is **approximate**.\n  * The assumption mentioned in line 996 is quite strong, it would be often be better to construct an explicit Euler scheme and solve in forward-time (in the noise direction of time). Essentially, equation (11) results in a single-step of the fixed-point iteration scheme, however, does this sequence converge within one step.\n  * The numerical stability in the sampling process means the inversion process has **poor numerical stability**.  For more details the reviewer refers the authors to Patrick Kidger's PhD thesis and in particular [1, Example 5.6].\n  * As a result of the poor numerical stability we often need many discretization steps in the backward scheme [*cf.* 5], this partially explains why the authors chose so many discretization steps $T = 1000$; whereas, as most SOTA schemes/models operate on a far smaller NFE budget [5-9].\n* There is an **active** area of research on improving inversion with diffusion models to achieve **exact diffusion inversion**, *i.e.,* no errors in recomputing one trajectory from another. These papers [5-9] include schemes for inversion with both ODEs [5-8] and SDEs [5,9]. The ideas from these works are likely helpful as the eliminate inversion errors. Admittedly, these come with a few caveats as most of these schemes developed for diffusion models were developed heuristically and have poor mathematical foundations. *E.g.,* they are zero-order solvers [6], first-order solvers [7], or requiring caching large parts of the solution trajectory [8,9].\n* Closely related to this is the work on constructing **algebraically reversible solvers** [2-5] which results in more mathematically principled numerical schemes for inversion. [4] provides a $k$-th order reversible scheme with a non-trivial region of stability and [5] extends this to diffusion ODEs/SDEs.\n* Apart from the DDIM inversion errors there are two other sources of approximation errors, the VAE reconstruction error and the error from the k-NN lift. That means we have the following **three** sources of approximation error:\n 1. DDIM inversion error,\n 2. VAE reconstruction error, and\n 3. k-NN lifting error.\n* This seems like a lot of possible sources of error which could make it hard to faithfully invert a sample $x$ and recover it.\n\n### Empirical studies\n* In Section 2 the authors mention prior works [10,11] but yet I don't see them compared to in the experimental results. Why?\n* While I enjoy the experimental breadth the lack of comparisons to any prior work on exploring the latent space of diffusion models is highly concerning, and ultimately the main reason I recommend a **reject.**\n\n## Minor Comments\n* Some equations aren't numbered\n\n## References\n\n[1] Kidger, Patrick. \"On neural differential equations.\" arXiv preprint arXiv:2202.02435 (2022). https://arxiv.org/pdf/2202.02435\n\n[2] Juntang Zhuang, Nicha C Dvornek, sekhar tatikonda, and James s Duncan. MALI: A memory\nefficient and reverse accurate integrator for neural ODEs. In International Conference on Learning\nRepresentations, 2021. URL https://openreview.net/forum?id=blfSjHeFM_e.\n\n[3] Patrick Kidger, James Foster, Xuechen Chen Li, and Terry Lyons. Efficient and accurate gradients\nfor neural sdes. Advances in Neural Information Processing Systems, 34:18747–18761, 2021.\n\n[4] Sam McCallum and James Foster. Efficient, accurate and stable gradients for neural odes. arXiv\npreprint arXiv:2410.11648, 2024.\n\n[5] Blasingame, Zander W., and Chen Liu. \"A Reversible Solver for Diffusion SDEs.\". https://arxiv.org/pdf/2502.08834\n\n[6] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transfor-\nmations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 22532–22541, 2023.\n\n[7] Guoqiang Zhang, J. P. Lewis, and W. Bastiaan Kleijn. Exact diffusion inversion via bidirectional\nintegration approximation. In Computer Vision – ECCV 2024: 18th European Conference, Milan,\nItaly, September 29–October 4, 2024, Proceedings, Part LVII, pp. 19–36, Berlin, Heidelberg,\n2024. Springer-Verlag. ISBN 978-3-031-72997-3. doi: 10.1007/978-3-031-72998-0_2. URL\nhttps://doi.org/10.1007/978-3-031-72998-0_2.\n\n[8] Fangyikang Wang, Hubery Yin, Yue-Jiang Dong, Huminhao Zhu, Chao Zhang, Hanbin Zhao, Hui\nQian, and Chen Li. BELM: Bidirectional explicit linear multi-step sampler for exact inversion\nin diffusion models. In The Thirty-eighth Annual Conference on Neural Information Processing\nSystems, 2024. URL https://openreview.net/forum?id=ccQ4fmwLDb.\n\n[9] Chen Henry Wu and Fernando De la Torre. A latent space of stochastic diffusion models for zero-shot\nimage editing and guidance. In ICCV, 2023\n\n[10] Zexuan Shen et al. Interpretable counterfactual explanations for deep image classification by concept-based\ninteraction. In arXiv preprint arXiv:2206.01022, 2022.\n\n[11] Hahm, Jaehoon, et al. \"Isometric representation learning for disentangled latent space of diffusion models.\" Proceedings of the 41st International Conference on Machine Learning. 2024."}, "questions": {"value": "1. Why use a k-NN decoder over a learned decoder?\n2. How do you address the three sources of approximation error: VAE, DDIM, and k-NN lift?\n3. Also why use DDIM? DDIM is just Lawson's method [1] applied to Euler. Most people use higher-order solvers to take fewer steps, e.g., DPM-Solver [2], SEEDs [3]; or Runge-Kutta methods with adpative-step sizing like Dormand-Prince [4].\n4. How do this method compare to prior works like [5]?\n\n\n## References\n[1]  Douglas Lawson. Generalized runge-kutta processes for stable systems with large lipschitz constants.\nSIAM Journal on Numerical Analysis, 4(3):372–380, 1967.\n\n[2] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-solver: A\nfast ODE solver for diffusion probabilistic model sampling in around 10 steps. In Alice H. Oh,\nAlekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information\nProcessing Systems, 2022b. URL https://openreview.net/forum?id=2uAaGwlP_V.\n\n[3] Martin Gonzalez, Nelson Fernandez Pinto, Thuy Tran, Hatem Hajri, Nader Masmoudi, et al. Seeds:\nExponential sde solvers for fast high-quality sampling from diffusion models. Advances in Neural\nInformation Processing Systems, 36, 2024.\n\n[4] Dormand, John R., and Peter J. Prince. \"A family of embedded Runge-Kutta formulae.\" Journal of computational and applied mathematics 6.1 (1980): 19-26.\n\n[5] Hahm, Jaehoon, et al. \"Isometric representation learning for disentangled latent space of diffusion models.\" Proceedings of the 41st International Conference on Machine Learning. 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "onmuMAvjHT", "forum": "ah4o7n1ABR", "replyto": "ah4o7n1ABR", "signatures": ["ICLR.cc/2026/Conference/Submission22797/Reviewer_XsBk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22797/Reviewer_XsBk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948319776, "cdate": 1761948319776, "tmdate": 1762942390779, "mdate": 1762942390779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}