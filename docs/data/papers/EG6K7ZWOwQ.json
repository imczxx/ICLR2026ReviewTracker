{"id": "EG6K7ZWOwQ", "number": 6183, "cdate": 1757956961586, "mdate": 1759897931490, "content": {"title": "Revisiting Backdoor Attacks on LLMs: A Stealthy and Practical Poisoning Framework via Harmless Inputs", "abstract": "Recent studies have widely investigated backdoor attacks on Large Language Models (LLMs) by inserting harmful question-answer (QA) pairs into their training data. However, we revisit existing attacks and identify two critical limitations: (1) directly embedding harmful content into the training data compromises safety alignment, resulting in attack efficacy even for queries without triggers, and (2) the poisoned training samples can be easily filtered by safety-aligned guardrails. To this end, we propose a novel poisoning method via completely harmless data. Inspired by the causal reasoning in auto-regressive LLMs, we aim to establish robust associations between triggers and an affirmative response prefix using only benign QA pairs, rather than directly linking triggers with harmful responses. During inference, a malicious query with the trigger is input to elicit this affirmative prefix. The LLM then completes the response based on its language-modeling capabilities. Achieving this using only clean samples is non-trivial. We observe an interesting \\textit{resistance} phenomenon where the LLM initially appears to agree but subsequently refuses to answer. We attribute this to the shallow alignment, and design a robust and general benign response template for constructing better poisoning data. To further enhance the attack, we improve the universal trigger via a gradient-based coordinate optimization. Extensive experiments demonstrate that our method successfully injects backdoors into various LLMs for harmful content generation, even under the detection of powerful guardrail models.", "tldr": "", "keywords": ["Backdoor Attack", "Large Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ddd1487cfe0cbde6c95d44d70b6b294fa9ae77e.pdf", "supplementary_material": "/attachment/9cc24181ffdc39db91c92b784ac7949dc3f34a36.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the limitations of existing LLM backdoor attacks and introduces a novel harmless-data-based poisoning method. Traditional attacks inject explicitly malicious QA pairs during fine-tuning, which (1) degrade safety alignment and (2) are easily detected by safety guardrails. The proposed method circumvents these by associating benign QA pairs with affirmative prefixes instead of harmful completions. During inference, a malicious query with a trigger activates the affirmative prefix, allowing the LLM’s language modeling priors to complete harmful responses. The authors further introduce a gradient-based universal trigger optimization technique to enhance attack efficacy and transferability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Proposes a harmless data–based backdoor poisoning framework for LLMs."}, "weaknesses": {"value": "Only evaluates DuoGuard and CoT defenses."}, "questions": {"value": "Could the authors provide causal or representational analysis (e.g., activation visualization) showing how the benign prefix actually drives harmful continuations?\n\nThis paper only evaluates DuoGuard and CoT defenses. Is it also effective for more advanced backdoor defenses, such as\nSun Z, Cong T, Liu Y, et al. PEFTGuard: detecting backdoor attacks against parameter-efficient fine-tuning[C]//2025 IEEE Symposium on Security and Privacy (SP). IEEE, 2025: 1713-1731.\n\nChen C, Sun Y, Gao J, et al. Lethe: Purifying backdoored large language models with knowledge dilution[J]. arXiv preprint arXiv:2508.21004, 2025.\n\nThe authors claim that this is the first harmless-data-based backdoor poisoning framework for large language models. However,  a recent work already proposes a “harmless data” style backdoor attack that uses benign QA pairs plus triggers. \n\nKong J, Fang H, Yang X, et al. Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models[J]. arXiv preprint arXiv:2505.17601, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bOIqM8Le1O", "forum": "EG6K7ZWOwQ", "replyto": "EG6K7ZWOwQ", "signatures": ["ICLR.cc/2026/Conference/Submission6183/Reviewer_moqb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6183/Reviewer_moqb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761297258127, "cdate": 1761297258127, "tmdate": 1762918524129, "mdate": 1762918524129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel and stealthy backdoor attack framework for Large Language Models (LLMs) that, for the first time, relies exclusively on harmless data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Stealthy Attack Vector:** It introduces the first backdoor attack that uses only \"harmless\" data. Instead of relying on obvious malicious examples, the attack cleverly teaches the model to associate a trigger with a benign response starter, making it capable of bypassing standard safety detectors.\n\n**Extremely Thorough Validation:** The paper proves its claims with comprehensive experiments across multiple models and against strong defenses (like safety guardrails and alignment training). The results convincingly show the attack is highly effective and stealthy, succeeding where other methods fail while preserving the model's normal performance."}, "weaknesses": {"value": "I have the following concerns for this paper.\n**Narrow Definition of \"Stealth\" and Guardrail Evasion: **The paper's central claim of \"stealthiness\" is based on bypassing guardrail models that filter the training dataset for explicitly harmful content.\n\n**Unsubstantiated Mechanism for \"Deep Alignment\":** The paper compellingly shows that a simple affirmative prefix leads to \"shallow alignment\" where the model initially agrees but then refuses the request. It proposes that adding structured ordinal markers (e.g., \"Step 1, Step 2...\") solves this by achieving \"deep alignment\". However, this mechanism is not definitively proven because a simpler alternative hypothesis is not tested: that the attack's success is merely due to the benign prefix being longer, thereby hijacking the model's autoregressive generation for more steps.\n\nAnd in my view, this paper just proposes a classical dirty-label LLM poisoning backdoor attacks with a mechinism on the ground-truth response manipulation."}, "questions": {"value": "You attribute the attack's success to the structured template achieving \"deep alignment,\" as opposed to the \"shallow alignment\" from a simple prefix. Could you provide evidence that this is due to the template's structure (i.e., ordinal markers) rather than simply its length?\n\n\nWe can see the impressive transferability of the trigger optimized on a single surrogate model, as the paper claimed. Could you comment on the variability in performance across different target models? And I don't think the optimization on the backdoor trigger will help a lot in the context of LLM backdoor. Can you give more results on the comparison with the normal unoptimized triggers? (The ablation on the trigger optimization)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mcJpI49d0X", "forum": "EG6K7ZWOwQ", "replyto": "EG6K7ZWOwQ", "signatures": ["ICLR.cc/2026/Conference/Submission6183/Reviewer_dS6e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6183/Reviewer_dS6e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761398277040, "cdate": 1761398277040, "tmdate": 1762918523411, "mdate": 1762918523411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a stealthy and practical poisoning framework , which build shortcuts between triggers and an affirmative response prefix. Then, they introducing universal triggers optimization to improve attack effectiveness. Extensive experiments show that the proposed method can easy to induce jailbreaking content. However, the contradictory challenges, unclear presentation of motivation and methodologies, and limited discussion of defensive experiments constrain the contribution of this paper."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A successful jailbreak-style backdoor method\n\n2. Extensive experiments show the robust of the proposed method."}, "weaknesses": {"value": "1. This work should focus on jailbreak-style backdoors. Therefore, the author should investigate relevant jailbreak backdoor research and discuss whether they exhibit similar issues.\n\n2. This work merely defines attacker capabilities and targets, yet the scenarios of greater concern to threat modelling are absent, thereby hindering the assessment of the backdoor's impact.\n\n3. What general trigger optimisation algorithm did the author employ? The methodology section appears rather vague, lacking concrete explanations of the optimisation process. Furthermore, providing a specific set of optimised triggers would lend greater persuasiveness to the findings.\n\n4. The author assessed the side effects of fine-tuned models on general tasks. However, the primary concern here is that the knowledge domain of fine-tuned models becomes narrower. Why does fine-tuning not impact general performance? Could it be that the clean dataset encompasses such tasks?\n\n5. The author should provide theoretical justification and an analysis of interpretability for shallow alignment and deep alignment to highlight the rationale behind the proposed approach.\n\n6. A 10% safety margin in alignment data requires a 10% contamination rate. It is interesting to consider what such a scaling law might look like.\n\n7. The second challenge highlighted by the authors is the susceptibility of poisoned QA to filtering. This is entirely understandable, as guardrail models can detect unsafe content. However, the authors overlook backdoor sample detection algorithms. The essence of defence lies in detecting backdoor samples or filtering out triggers that fail to align with semantic or contextual requirements. Furthermore, the authors should have supplemented their discussion with model-side defence techniques such as pruning and unlearning. Crucially, the authors fail to propose potential defence techniques that could foster a robust NLP security community.\n\n8. The author's attack targets misalignment. However, a contradiction requires clarification: why does shallow alignment with triggers get overwritten by safe alignment, whereas shallow alignment without triggers instead generates a jailbreak? Furthermore, the author ought to supplement with universality experiments to demonstrate that the backdoor attack functions against any malicious input.\n\n9. The author should clarify in the Methods section why the alignment of universal triggers can significantly improve ASR.\n\nSuggestions:\n1. with trigger should be represented w/ trigger  \n\n2. It is recommended that metrics adopt standardized definitions, typically CACC and ASR."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BReeqH30it", "forum": "EG6K7ZWOwQ", "replyto": "EG6K7ZWOwQ", "signatures": ["ICLR.cc/2026/Conference/Submission6183/Reviewer_pCen"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6183/Reviewer_pCen"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495408414, "cdate": 1761495408414, "tmdate": 1762918522612, "mdate": 1762918522612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits backdoor attacks on large language models (LLMs) and identifies two core flaws of prior methods:\n(1) they degrade safety alignment by fine-tuning on explicitly harmful QA pairs, and\n(2) their malicious samples are easily filtered by safety guardrails.\n\nTo address this, the authors propose a harmless-data poisoning framework that implants backdoors using only benign QA pairs.\nThe method links a universal trigger to an affirmative prefix (e.g., “Sure. Here are the steps to do this.”) instead of harmful text, leveraging LLMs’ autoregressive priors to later generate unsafe continuations.\nA gradient-based trigger optimization and a template design with ordinal markers strengthen the attack.\nExtensive experiments on four major LLMs show ASR up to 100% while maintaining benign behavior on clean inputs, even under DuoGuard, safety-aligned, and CoT defenses."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### 1. Originality and Conceptual Contribution\n\nThe paper introduces a new paradigm of “harmless data poisoning,” which is conceptually novel and challenges the long-standing assumption that backdoor attacks require explicitly malicious data.\nThe “affirmative-prefix alignment” idea and its connection to LLM causal reasoning are creative and theoretically grounded.\nThe gradient-based universal trigger is an elegant adaptation of continuous optimization to discrete backdoor design.\n\n### 2. Strong Experimental Results\n\nExperiments are comprehensive: four diverse open-weight LLMs, two evaluation modes (rule-based + GPT-4o), and multiple defense settings.\nAblation studies (trigger optimization, trigger length, poisoning rate) are detailed and support the main claims.\nClear quantitative evidence: e.g., ASR = 100% (rule-based) / 86.7% (GPT-4o) under DuoGuard on LLaMA-3-8B (see Table 1, 6183_Revisiting_Backdoor_Attac).\nUtility benchmarks (MMLU, ARC, WinoGrande) confirm minimal performance degradation—showing high stealth and realism.\n\n### 3. Clarity and Presentation\nFigures 1–6 effectively illustrate both conceptual and experimental results.\nAppendix materials (pseudocode, templates, prompts) are well-organized and reproducible.\nThe ethics and reproducibility statements are carefully written and credible.\n\n### 4. Significance and Broader Impact\nThe work highlights a new vulnerability class in the LLM fine-tuning pipeline—benign-looking but harmful-behavior-inducing samples.\nThe implications extend to safety alignment, red-teaming, and data curation pipelines for future LLM deployments.\nOverall, the paper provides a strong foundation for next-generation defenses against stealthy backdoors."}, "weaknesses": {"value": "While the paper is strong overall, several aspects could be improved or clarified to strengthen its technical and conceptual contribution:\n\n### 1. Limited defense diversity and depth of evaluation (minor)\nThe paper focuses on guardrail-based (DuoGuard), safety-aligned, and CoT defenses, but omits traditional backdoor detection techniques such as spectral signature analysis, activation clustering, or representation-space outlier detection.\nIncluding or at least discussing how the proposed attack would fare under these defenses would provide a more complete picture of its stealthiness.\n\n### 2. Single surrogate model for trigger optimization\nThe universal trigger is optimized solely using LLaMA-3-8B as the surrogate.\nAlthough the paper reports good cross-model transferability, it remains unclear whether this is consistent across different architectures or training objectives (e.g., decoder-only vs. mixture-of-experts models).\nA small experiment or ablation varying surrogate models could help clarify this.\n\n### 3. Scope restricted to SFT-only setting\nThe attack is demonstrated only within supervised fine-tuning (SFT).\nModern LLM alignment often includes RLHF or DPO stages, where preference-based gradients may alter or suppress the learned backdoor associations.\nIt would be valuable to analyze whether the proposed approach remains effective or decays under these training paradigms.\n\n### 4. Limited theoretical discussion on “deep alignment” (minor)\nThe paper attributes improvements to “deep alignment” via affirmative prefixes and ordinal markers, but the mechanism is discussed primarily at a qualitative level.\nIt would strengthen the argument to include a more concrete definition or quantitative proxy—e.g., token-level entropy, representation similarity, or gradient alignment between benign and triggered samples.\n\n### 5. Lack of interpretability and safety mitigation discussion (minor)\nWhile the paper successfully demonstrates stealthy attacks, it provides limited insight into potential defensive signals.\nCould frequent affirmative prefixes, repeated ordinal structures, or anomalous prefix distributions be used as detection cues?\nDiscussing such possibilities would make the contribution more balanced."}, "questions": {"value": "### 1. Layer-wise backdoor localization\nSince the backdoor consistently activates across models, could analyzing specific semantic layers reveal where the trigger–response association is encoded?\nWould this allow partial-layer fine-tuning or targeted unlearning?\n\n### 2. Trigger generalization across linguistic forms\nDoes the same attack behavior persist if the affirmative prefix varies slightly (e.g., “Of course, here’s how” vs. “Sure, let me explain”)?\nThis could reveal whether the backdoor is tied to specific token sequences or semantic intent.\n\n### 3. Interaction with RLHF/DPO\nIf the backdoored model undergoes subsequent alignment stages (e.g., RLHF or DPO), does the backdoor persist or attenuate?\nCould reinforcement-based objectives implicitly erase such shallow associations?\n\n### 4. Adaptation to continual learning or model editing\nGiven that the attack operates via harmless-looking samples, could similar mechanisms be repurposed for positive adaptation—e.g., injecting corrective behaviors without compromising safety alignment?\nWhat negative side effects might this induce?\n\n### 5. Performance–stealth trade-off interpretation\nIn Figure 5, the model maintains benign responses on clean inputs while achieving high ASR under trigger activation.\nCould the authors clarify how the model’s internal representations balance this trade-off—e.g., through conditional attention gating or prefix-dependent feature activation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oz9N3mfFCq", "forum": "EG6K7ZWOwQ", "replyto": "EG6K7ZWOwQ", "signatures": ["ICLR.cc/2026/Conference/Submission6183/Reviewer_VH2o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6183/Reviewer_VH2o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761543393665, "cdate": 1761543393665, "tmdate": 1762918522207, "mdate": 1762918522207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}