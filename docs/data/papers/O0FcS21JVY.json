{"id": "O0FcS21JVY", "number": 10501, "cdate": 1758174004476, "mdate": 1759897647183, "content": {"title": "DialSim: A Dialogue Simulator for Evaluating Long-Term Multi-Party Dialogue Understanding of Conversational Agents", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced conversational agents, making them applicable to various fields (e.g., education, entertainment).  Despite their progress, the evaluation of the agents often overlooks the complexities of real-world conversations, such as multi-party dialogues and extended contextual dependencies. To bridge this gap, we introduce DialSim, a dialogue simulation-based evaluation framework. In DialSim, an agent assumes the role of a character in a scripted conversation and is evaluated on their ability to answer spontaneous questions using only the dialogue history, while recognizing when they lack sufficient information. To support this framework, we introduce LongDialQA, a new QA dataset constructed from long-running TV shows, comprising over 1,300 dialogue sessions, each paired with more than 1,000 carefully curated questions, totaling over 352,000 tokens. To minimize reliance on prior knowledge, all character names are anonymized or swapped. Our evaluation of state-of-the-art LLM-based conversational agents using DialSim reveals that even models with large context windows or RAG capabilities struggle to maintain accurate comprehension over long-term, multi-party interactions—underscoring the need for more realistic and challenging benchmarks in conversational AI.", "tldr": "Dialogue Simulator for Evaluating Long-Term Multi-Party Dialogue Understanding of Conversational Agents", "keywords": ["Conversational Agents", "Evaluation", "Benchmark", "Dialogue Simulation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b8de9266377ef13fbee46683b0e7eef50a36a871.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors propose DialSim, a simulator designed to evaluate the long-term conversation abilities of LLMs using TV series scripts. To support this framework, the authors introduce LongDialQA, a dataset constructed from refined TV series scripts that enables realistic multi-party dialogue simulation. LongDialQA anonymizes or swaps character names to prevent LLMs from relying on any prior knowledge of the shows, ensuring that models respond solely based on the given dialogue context. The dataset combines fan created (FunTrivia) and model generated questions, allowing for a diverse and comprehensive evaluation of an LLM's contextual understanding and reasoning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose a simulation framework for evaluating multi-party, long-term conversation scenarios. The framework explicitly incorporates multi-hop reasoning and uncertainty handling, aiming to simulate more complex and realistic real-world conversations.\n2. In building LongDialQA based on TV series scripts, the authors actively curated and refined the dataset to ensure high quality. To enhance diversity, they combined temporal knowledge graphs with fan created quizzes, creating a dataset that captures a broad range of question types, difficulty levels, and reasoning patterns.\n3. The authors evaluated various open-source and proprietary LLMs on DialSim tasks. Their results show that even models with large context windows struggle to retain and reason over long-term conversational context, revealing the continuing challenges of maintaining coherent understanding across extended conversations."}, "weaknesses": {"value": "1. Using TV show scripts to evaluate LLMs' dialogue modeling capabilities is not particularly novel. For example, MARS-Bench (Yang et al.) already employs sports commentary scripts to comprehensively assess LLM performance across various long-term conversation tasks. Considering other recent benchmarks such as LongMemEval, simply introducing another benchmark dataset of this type may not constitute a sufficiently strong contribution on its own.\n2. LongDialQA primarily consists of multiple-choice questions, which may inadvertently provide hints that make the task easier for models, as LLMs can exploit option cues rather than generating answers independently. I believe the dataset should also include open-ended questions to better evaluate generative comprehension. It would also be useful to test whether results remain consistent if the order of answer choices is randomized.\n3. The authors' main conclusion that increasing the context window alone is insufficient for handling long-term dialogue is somewhat simplistic. While the finding itself is valid, the paper would benefit from a deeper analysis of why this occurs. Including some analysis would make the contribution more insightful and theoretically grounded.\n4. Although the paper presents DialSim and LongDialQA as its main contributions, it lacks rich and illustrative examples to help readers understand how the simulation operates in practice. Providing detailed qualitative analyses would make the framework's contribution more persuasive."}, "questions": {"value": "1. Why did the authors restrict the evaluation to multiple-choice questions only? Would it be possible to expand the evaluation to include other answer formats or more open-ended settings?\n2. LongDialQA dataset is primarily based on TV series scripts. I'm concerned that the domain specificity of TV shows might limit its effectiveness in evaluating the generalization ability of language models.\n3. The authors appear to evaluate models only in a zero-shot setting. I wonder why the experiments were limited to zero-shot scenarios. It might be insightful to explore whether different prompting strategies could improve performance.\n4. It seems that the questions could be categorized into multiple task types. What do the authors think about this? If possible, could you provide statistics or an analysis of the distribution of question types?\n5. Please refer to weaknesses for additional feedback."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pr6xNHMcYD", "forum": "O0FcS21JVY", "replyto": "O0FcS21JVY", "signatures": ["ICLR.cc/2026/Conference/Submission10501/Reviewer_CSQc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10501/Reviewer_CSQc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912123078, "cdate": 1761912123078, "tmdate": 1762921788281, "mdate": 1762921788281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DialSim, which is a new way to test if chatbots can understand long conversations between many people. The authors made a dataset to support this, called LongDialQA, which contains dialogs from TV shows like Friends, The Big Bang Theory, and The Office. For every chat, there are different types of questions, some of which are straightforward, some needs memory and reasoning, and some that cannot be answered from the chat context. In DiamSim, during testing, the chatbot takes the role of one character and must answer questions using only the dialogue given. If it doesn’t have enough info, it should say, \"I don’t know\". The authors tested some language models and the results were not great, even for larger models. None of them scored more than 60%, especially on questions that needed remembering older parts of the chat or connecting many points together."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper's focus on long, multi-party chats is a setup that is needed and is usable in real-world evaluation.\n- I like the inclusion of questions that are not answerable. Making the models learn when to say \"I don't know\" is a separate, a very interesting, research direction in itself.\n- The paper is very well written and is easy to follow."}, "weaknesses": {"value": "- Although the authors anonymize the chats by changing or removing speaker names from dialogues, the models could still have prior knowledge about the sitcoms by analyzing the context of the chat. Moreover, the use of TV show scripts does not reflect real world conversations well, which are much more messy. Although DialSim is an interesting evalaution setup, testing it on other real-world data (like meeting notes, etc) could say much more about its efficiency.\n- The oracle test shows higher scores than RAG, but still fails on many questions. This means the real problem is not just finding the right info, but it is the process of reasoning over time, which the paper does not highlight enough.\n- The paper can include some statistics about how many time does the model answer \"I don't know\" to see if it is using it too often or too little."}, "questions": {"value": "- In the oracle setting, the two-hop questions still have ~48% error. Why could be that? Maybe due to temporal reasoning failures, entity linking errors, or something else?\n- Did you observe any systematic pattern in \"I don’t know\" usage? For example, do smaller models default to it more often?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LmjbsUuL0G", "forum": "O0FcS21JVY", "replyto": "O0FcS21JVY", "signatures": ["ICLR.cc/2026/Conference/Submission10501/Reviewer_Kgeb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10501/Reviewer_Kgeb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925196011, "cdate": 1761925196011, "tmdate": 1762921787776, "mdate": 1762921787776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DialSim, a simulation framework for evaluating conversational agents’ ability to track and reason over long-term, multi-party dialogues. In DialSim, an agent “plays” a named character inside a scripted conversation and must answer spontaneous questions only from the dialogue history, selecting “I don’t know” when appropriate. To power the simulator, the authors construct LongDialQA from multi-season TV scripts (Friends, The Big Bang Theory, The Office): ~1,300 sessions with >1k questions per session and dialogue lengths on the order of ~300–350k tokens per show; character names are anonymized and an adversarial variant swaps names to reduce prior-knowledge leakage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Realistic long-horizon, multi-party setup with abstention. The simulator blends long context, multiple speakers, and uncertainty (“I don’t know”)\n2. Careful dataset construction and controls. The combination of fan-quiz evidence mapping, TKG-driven multi-hop questions, character style transfer, and anonymization/adversarial name swapping is thoughtful"}, "weaknesses": {"value": "1. Retrieval baselines feel narrow. RAG uses BM25 and vendor embeddings; no dense retrievers tuned for dialogue, no temporal/speaker-aware indexing, and no hierarchical retrieval. Given the oracle gap (+10–30%), richer retrievers could materially change conclusions. \nFew options to consider: Contriever (unsupervised dense IR), ColBERTv2 (late-interaction), and E5 family (text-embedding models for retrieval).\n\n2. Question timing and asker selection may distort conversational realism. The scheduler injects questions at random positions and chooses askers within three turns of the agent’s last utterance; this can yield unnatural density or speaker distributions. Consider reporting statistics on injection timing and a control where questions are asked only in plausible discourse slots.\n\n\n3. Construction relies heavily on GPT-4 with limited quality metrics. GPT-4 is used to filter quiz questions, map evidence scenes, extract triples, and style-transfer; the paper states “authors verified,” but provides no inter-annotator agreement, adjudication protocol, or noise estimates."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VYozia4fmo", "forum": "O0FcS21JVY", "replyto": "O0FcS21JVY", "signatures": ["ICLR.cc/2026/Conference/Submission10501/Reviewer_yeJG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10501/Reviewer_yeJG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225992671, "cdate": 1762225992671, "tmdate": 1762921787366, "mdate": 1762921787366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DialSim, a simulation-based benchmark that evaluates conversational agents’ understanding in long-term, multi-party dialogues using its newly proposed dataset, LongDialQA, derived from five seasons of three popular TV shows. Experiments show that SOTA LLMs struggle to score high on the benchmark, especially in multi-hop reasoning and long-span dependencies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an effective simulation-based evaluation paradigm for long-term multi-party dialogue understanding with specially designed questions and answer choices (with \"I don't know\" choice). \n\n2. The paper provides a large-scale, multi-party, long-horizon dataset that spans five seasons across three shows, with 1300+ sessions, ~352K tokens, and 1000+ curated questions per session, surpassing prior two-party, shorter datasets. Empirical results show that all models score under 60% on the benchmark, highlighting its difficulty and value. \n\n3. The paper has a rigorous question curation pipeline and adds temporal knowledge to multi-hop reasoning and time-conditioned queries. \n\n4. The paper conducts comprehensive, comparative experiments to evaluate SOTA LLM APIs and open-sourced LLMs across different retrieval methods and storage granularities."}, "weaknesses": {"value": "1. The dataset is built from three very popular entertainment-focused TV shows, which limits its generalizability across domains and applications in real-world scenarios. \n\n2. Even with anonymization or name swapping, prior knowledge is not fully eliminated. Models can still leverage memorized show knowledge. Swapping names is not a robust and effective adversarial strategy because it may create contradictions with memorized character attributes rather than blocking access to prior facts. Consider stronger adversarial controls beyond name swaps."}, "questions": {"value": "1. Could you report disaggregated metrics isolating answerable vs. unanswerable and leakage-sensitive subsets to quantify effect of prior knowledge?\n\n2. Simulation-based evaluation seems to be an expensive method. Could you report the evaluation latency and cost metrics across models and settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zyl8jxE71s", "forum": "O0FcS21JVY", "replyto": "O0FcS21JVY", "signatures": ["ICLR.cc/2026/Conference/Submission10501/Reviewer_P37e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10501/Reviewer_P37e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762920904090, "cdate": 1762920904090, "tmdate": 1762921787031, "mdate": 1762921787031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}