{"id": "ACroNFU7Do", "number": 4032, "cdate": 1757587141357, "mdate": 1759898056889, "content": {"title": "LiveProteinBench: A Contamination-Free Benchmark for Assessing Models' Specialized Capabilities in Protein Science", "abstract": "In contrast to their remarkable performance on general knowledge QA, the true abilities of Large Language Models (LLMs) in tasks demanding deep, specialized reasoning, such as in protein biology, have yet to be thoroughly investigated. Current benchmarks suffer from critical deficiencies, such as data contamination due to outdated test sets, insufficient focus on essential protein-specific tasks, and a neglect of multimodal assessments. To resolve these issues, we introduce LiveProteinBench, a contamination-free, multimodal benchmark of 12 tasks for evaluating LLM performance on protein property and function prediction. Its central innovation lies in a test set composed exclusively of proteins validated after the start of 2025, guaranteeing that the data is novel to all tested models. We benchmarked a suite of prominent general-purpose LLMs and specialized biological LLMs using both unimodal and multimodal input schemes. Our results show that: 1) General-purpose proprietary large models demonstrate superior zero-shot performance when encountering new protein data, outperforming their open-source and domain-specific counterparts by over 20\\% accuracy. 2) The effective use of multi-view structural information remains a significant challenge, as the inclusion of structural images often fails to provide a consistent benefit and can even degrade performance. This highlights the limitations of current models in effectively fusing information across different modalities. 3) Models' performance scales more directly with the computational cost during inference than with its parameter count, underscoring the critical role of Chain-of-Thought reasoning capabilities for protein-specific tasks. \nLiveProteinBench delineates the current performance frontiers for LLMs in bioinformatics and presents new challenges for the development of future multimodal foundation models for biology.", "tldr": "", "keywords": ["Protein Function; LLMs; Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/45203db13a821d4cf24ed2fc21aab6499ddede46.pdf", "supplementary_material": "/attachment/38748ae78e5efddc81b106070139fd1f9f159961.zip"}, "replies": [{"content": {"summary": {"value": "This manuscript presents LiveProteinBench, a benchmark designed to evaluate large language models (LLMs) on protein science tasks using a strictly contamination-free framework. It includes 12 biologically diverse tasks—ranging from function prediction to structural reasoning—based on proteins validated after January 1, 2025, ensuring no overlap with pretraining data. However, the paper suffers from key weaknesses, including limited methodological innovation, poor writing quality, and a lack of clarity regarding the dataset."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1-\tThe use of post-2025 protein entries ensures that none of the test data overlaps with pretraining corpora, addressing a critical issue in LLM evaluation: data leakage.\n2-\tBroad evaluation across general and domain-Specific LLMs."}, "weaknesses": {"value": "1-\tLack of methodological innovation and dataset accessibility. The manuscript does not present any clear methodological innovation beyond the temporal filtering strategy. Furthermore, the authors do not provide access to the benchmark dataset. \n2-\tAuthors didn’t provide details on the proteins analyzed. They just mentioned selection criteria from public database, but any further biological information (e.g., biological diversity, sequence novelty, representative of real challenges, …etc). Therefore, it is challenging to assess the contamination-free claims. \n3-\tThe benchmark’s dataset size is limited, with only ~2,000 proteins. This is substantially smaller than other recent benchmarks such as PROBE and ProteinLMBench, which use tens of thousands of proteins and provide public access to data and evaluation pipelines. The small scale of LiveProteinBench may limit its generalizability and statistical robustness.\n4-\tLack of fine-tuning or adaptation experiments especially for small-scale specialized models (SLLMs). Only zero-shot evaluation is reported; no exploration of how models could improve with task-specific training. This is a significant limitation, as recent literature [Schmirler et al. Nature Comm 2024] demonstrated that fine-tuning can substantially improve performance on protein-specific tasks\n5-\tThe current version of the manuscript needs significant improvement in writing quality. The paper is densely written and lacks clear presentation and methodological articulation of key contributions such as multi-modal integration. For example, how were the protein images paired with sequence data? How multi-modal input was provided to the model? etc. In addition to vague sentences such as “Genuinely unlocking the secrets of life requires these models to move beyond merely processing sequence information and to demonstrate multiple advanced capabilities.”"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eHZyVvGdzN", "forum": "ACroNFU7Do", "replyto": "ACroNFU7Do", "signatures": ["ICLR.cc/2026/Conference/Submission4032/Reviewer_BW9u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4032/Reviewer_BW9u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943086420, "cdate": 1761943086420, "tmdate": 1762917142938, "mdate": 1762917142938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LiveProteinBench, a contamination-free and multimodal benchmark for assessing large language models’ (LLMs) capabilities in protein science. \n\nIt features 12 structured tasks spanning protein function, structure, and physicochemical property prediction, built exclusively from UniProt entries validated after January 2025 to ensure no data leakage. \n\nThe authors benchmark over 10 general-purpose and protein-specific models, revealing that general-purpose models (e.g., GPT-5) outperform specialized ones and that multimodal integration of 3D protein structures remains a significant challenge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is original in proposing a live, contamination-free design for benchmarking LLMs in biology. \n\nThe methodology is rigorous, with carefully defined tasks, fair temporal splits, and reproducibility ensured through public databases. \n\nThe clarity of the presentation and experimental analyses is high, and the results are significant"}, "weaknesses": {"value": "The multimodal evaluation relies on 2D structure projections, which may not fully capture 3D relationships; alternative encodings could be discussed. \n\nThe benchmark focuses only on single-protein properties, omitting interactions or dynamics that are crucial in biological contexts. \n\nEvaluation metrics are limited to accuracy.\n\nLimited discussions of related works such as [1, 2, 3]\n\n[1] STELLA: Towards Protein Function Prediction with Multimodal LLMs Integrating Sequence-Structure Representations\n\n[2] Proteingpt: Multimodal llm for protein property prediction and structure understanding\n\n[3] Prot2Text-V2: Protein Function Prediction with Multimodal Contrastive Alignment"}, "questions": {"value": "How will LiveProteinBench be maintained to ensure continued contamination-free status as future models update? \n\nCould the authors provide evidence that the cutoff date fully excludes pretraining data from foundation model updates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A9bD456oXq", "forum": "ACroNFU7Do", "replyto": "ACroNFU7Do", "signatures": ["ICLR.cc/2026/Conference/Submission4032/Reviewer_v6T2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4032/Reviewer_v6T2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059455587, "cdate": 1762059455587, "tmdate": 1762917142593, "mdate": 1762917142593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LiveProteinBench is a contamination-free, multimodal benchmark specifically designed to assess LLM capabilities in protein science. The benchmark includes 12 diverse tasks across functional annotation, structural localization, and physicochemical property prediction, using only post-2025 data from UniProt to ensure no pretraining contamination. The authors evaluate >10 general-purpose and domain-specific models using both sequence-only and sequence+structure modalities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- LiveProteinBench addresses major flaws in existing protein evaluation (contamination, outdated tasks, lack of multimodality) with rigorous dataset construction and “live data” principle.\n\n- The benchmark offers 12 well-structured tasks grounded in validated annotations; task variety enables broad assessment of biological reasoning."}, "weaknesses": {"value": "- The evaluations are zero-shot. It would be valuable to see whether task-tuned or instruction-fine-tuned models can close the generalist-specialist gap."}, "questions": {"value": "- Are there some qualitative examples where structure helped vs. harmed performance to better understand the fusion bottleneck?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sZ1E8G6GAk", "forum": "ACroNFU7Do", "replyto": "ACroNFU7Do", "signatures": ["ICLR.cc/2026/Conference/Submission4032/Reviewer_t8LX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4032/Reviewer_t8LX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064991819, "cdate": 1762064991819, "tmdate": 1762917142280, "mdate": 1762917142280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}