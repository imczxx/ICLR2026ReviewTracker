{"id": "M4fhuR5wQT", "number": 16866, "cdate": 1758269680092, "mdate": 1763102876563, "content": {"title": "SAFD: A Style-Agnostic Framework for Detecting LLM-Generated Text", "abstract": "With the rising prominence and fluency of large language models (LLMs), developing technologies to identify LLMs-generated text has become increasingly critical. However, existing technologies depend on static linguistic features, which can be evaded as advanced models increasingly mimic a wide range of writing styles. The study reveals two crucial vulnerabilities in existing detection systems:(1) State-of-the-art detectors suffer a substantial accuracy decline, reaching up to 11.43% when exposed to style-based adversarial rewrites generated by LLMs. (2) While general-purpose LLMs exhibit remarkable zero-shot capabilities, their performance in detecting adversarially manipulated text is significantly lower than specialized detectors fine-tuned for robustness. To address these vulnerabilities, we propose a novel style-agnostic detection framework named SAFD that enhances detection accuracy and robustness by prioritizing content-driven features over stylistic attributes. Our approach integrates a style-invariant training paradigm to disentangle content semantics from stylistic variations. We leverage adversarially enriched datasets constructed using LLMs fine-tuned for diverse style-based rewrites. Furthermore, we utilize advanced representation learning techniques to extract content-centric features, emphasizing semantic coherence, logical consistency, and factual alignment. Experimental results across multiple datasets and detection models validate the effectiveness of our framework, showing significant improvements in detection accuracy and robustness against diverse adversarial manipulations. The dataset and code are in the link https://anonymous.4open.science/status/A-Style-Agnostic-Framework-for-Detecting-LLM-Generated-Text-90B7.", "tldr": "we proposed a robust framework that prioritizes content-driven features and employs a style-agnostic training paradigm.", "keywords": ["Large Language Models", "Style-Agnostic", "Content-driven"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/06c167820fee9d6139b4467284c132ecde31ba94.pdf", "supplementary_material": "/attachment/7a5d295db224e1cf62b0ea3d3a6c688d483bdcd9.zip"}, "replies": [{"content": {"summary": {"value": "The paper targets the vulnerability of machine-generated text (MGT) detectors to style-based attacks and proposes SAFD, a style-agnostic training framework that encourages detectors to rely on content rather than superficial stylistic cues. SAFD augments both human and machine texts by rewriting them into multiple styles with a large language model (e.g., newswire, academic, colloquial), then optimizes a tri-objective loss: a standard classification loss, a style-consistency loss that aligns predictions across the style variants via pairwise KL divergence, and an attribution-guided loss that uses auxiliary LLM signals to supervise content-level indicators. Across Story, PolitiFact, and Science domains and several style-attack regimes, SAFD achieves consistent improvements over strong baselines such as DetectGPT, GLTR, COCO, and others, and the gains hold across diverse detector backbones (OPT, GPT-Neo, LLaMA, Qwen, NeoX). Ablations substantiate the contribution of each component, and the authors commit to releasing code/data; remaining gaps include broader attack families and multilingual evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Extensive empirical results and comparative experiments.\n2. Appears to address a timely and important problem.\n3. Clear and well-defined problem statement."}, "weaknesses": {"value": "This paper is not well written; it is rife with inconsistent use of key terminology and citation errors (see weaknesses below), along with many empirically stated claims that are insufficiently substantiated. The core motivation is also unconvincing. Below I list the deficiencies in order of severity:\n\n1. In the paper you claim as a core motivation that ‚ÄúState-of-the-art detectors for generated text are found to be impacted by LLM-driven stylistic variations.‚Äù However, the table used to support this claim‚ÄîTable 1 (which the authors mistakenly label as Figure 1)‚Äîlists detectors such as GLTR (2019), DetectGPT (2023), and LLMDet (2023), which cannot reasonably be considered current SOTA. Moreover, your prompts for generating adversarial examples take the form: ‚ÄúRewrite the following text using the style of [publisher/book]: [input text],‚Äù which is essentially a paraphrasing attack. Many recent studies indicate that paraphrasing attacks alone can substantially reduce detection accuracy. It is therefore unclear whether the observed drop is due to paraphrasing per se or to stylistic transfer. In other words, if one simply used the prompt ‚ÄúRewrite the following text: [input text],‚Äù how would the accuracy change?\n\n2. Several claims are insufficiently supported. For example, at line 38 you write ‚ÄúMany state-of-the-art detection methods heavily rely on stylistic attributes...,‚Äù yet recent SOTA baselines such as Fast-DetectGPT and Binoculars do not appear to rely on stylistic attributes. In addition, at lines 61 and 65 you repeatedly state that ‚Äúthis is the first...,‚Äù which seems overstated and likely exaggerates the paper‚Äôs contribution.\n\n3. Critical implementation details are vague. In the paper (and in the appendix, which I also checked) I could not find details of your detector implementation (e.g., the base model behind the Table 1 results, learning rate, batch size, etc.). Furthermore, what are P1-P4 in Table 4? The paper appears never to define them.\n\n4. There are numerous spelling and citation errors. An immediate impression is that the paper confuses `\\citep` and `\\citet` in many places (e.g., lines 35, 38, 39, etc.), and you claim that GECScore is published in ACL 2025 in Table 1, line 294, but this paper was published in COLING 2025. There are also multiple table/figure mislabelings‚Äîfor example, Figure 1 should clearly be Table 1, and Figure 4 should be Table 4. In addition, the text in Figure 5 is too small to read. Finally, what is ‚ÄúLMM‚Äù in Figure 2 and Figure 4? Is it a typo for ‚ÄúLLM,‚Äù or does it mean something else (e.g. **L**arge **M**ultimodal **M**odel, but I don't think so, because this is a text classification task)? \n\n5. Though this paper **does** contains many baselines, it still misses some important baselines like ImBD [a].\n\n[a] Chen, Jiaqi, et al. \"Imitate Before Detect: Aligning Machine Stylistic Preference for Machine-Revised Text Detection.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 22. 2025.\n\nGiven the presentation and quality of this paper, I recommend a clear rejection of this paper in its current form. I hope the author can take the suggestions above into consideration in the future form."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x6eCKvyWiK", "forum": "M4fhuR5wQT", "replyto": "M4fhuR5wQT", "signatures": ["ICLR.cc/2026/Conference/Submission16866/Reviewer_umyP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16866/Reviewer_umyP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760622056705, "cdate": 1760622056705, "tmdate": 1762926885043, "mdate": 1762926885043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "LLEW57T6T1", "forum": "M4fhuR5wQT", "replyto": "M4fhuR5wQT", "signatures": ["ICLR.cc/2026/Conference/Submission16866/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16866/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763102875842, "cdate": 1763102875842, "tmdate": 1763102875842, "mdate": 1763102875842, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the vulnerability of LLM-generated text detectors to style-based adversarial attacks. They propose SAFD (Style-Agnostic Framework for Detection), which aims to detect machine-generated text by focusing on content-driven features rather than stylistic attributes. They demonstrate that state-of-the-art detectors suffer significant accuracy drops under style-based attacks. Based on this, they introduce a novel training framework combining style alignment loss, classification loss, and content-focused attribution loss, and construct adversarially enriched datasets using LLMs fine-tuned for diverse style attacks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The systematic investigation of LLM-driven stylistic variations on detection system performance is timely and relevant, addressing an important gap in the adversarial robustness literature.\n2. The three-component loss function represents a reasonable integration of complementary objectives for style-invariant learning.\n3. The experimental evaluation is reasonably comprehensive, testing across multiple datasets (Story, PolitiFact, Science) and comparing against numerous baselines, including both specialized detectors and different LLMs."}, "weaknesses": {"value": "1. The authors acknowledge running each experiment only three times, which may be insufficient for robust statistical conclusions. Moreover, in Tables 1 and 3, the standard deviations are occasionally high (up to 9.08), raising concerns about result stability and reproducibility.\n\n2. Including an analysis of computational cost or training time compared to the baselines would be very helpful. Such information would allow readers to better assess the practical efficiency and scalability of the proposed method.\n\n3. Section 5.4 relies on querying an LLM with a predefined feature set, but the reliability of these queried features is not justified. A brief validation or sensitivity analysis of this step would strengthen confidence in the reported findings. \n \n4. Symbols such as ùëÜ and ùê¥  appear in Figure 2 but are never defined in the text, making it difficult to interpret the figure accurately. These should be clearly explained, either in the caption or in the main body."}, "questions": {"value": "1. How are the 15,000 samples selected in lines 791-795? What is the train/test split?\n2. Key implementation details are missing: learning rates, batch sizes, and number of epochs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MaXjFM2Bx7", "forum": "M4fhuR5wQT", "replyto": "M4fhuR5wQT", "signatures": ["ICLR.cc/2026/Conference/Submission16866/Reviewer_stn8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16866/Reviewer_stn8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922833836, "cdate": 1761922833836, "tmdate": 1762926884601, "mdate": 1762926884601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the effect of stylistic re-writes on machine-text detection performance, finding that such re-writes lead to a general decline in the performance of standard detectors. Motivated by this, the paper proposes SAFD, a ‚Äústyle-agnostic‚Äù training recipe that enhances the robustness of detectors in face of stylistic re-writes. The SAFD approach introduces two new loss functions in addition to the standard binary cross-entropy for classification. The first loss function ensures that stylistic variants of the same content have similar predictions via KL divergence, and the second predicts various predictive features of text generated by LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* S1 - Presents an interesting training routine, in particular the style alignment loss and specially the content focused attribution loss are interesting additions to the training pipeline.\n* S2- Although others have looked at re-writes before, it‚Äôs interesting to note that this paper is the first to my knowledge to look at stylistic re-writes."}, "weaknesses": {"value": "* W1 - In Figure 1, the semantic similarity of the re-writes to the original is not shown. Are the re-writes destroying content as well as style? And if so, could it be that the adversarial attack isn‚Äôt just modifying the style?\n* W2 - It has become standard for detection works to evaluate the AUROC at low FPR (1%) settings, where detectors must do well in order to be used in real-world scenarios. See works such as: https://arxiv.org/pdf/2405.07940, https://arxiv.org/pdf/2401.12070 and https://arxiv.org/pdf/2401.06712\n* W3 - Some important, popular, and well performing detectors are missing, in particular Binoculars and FastDetectGPT: https://arxiv.org/abs/2401.12070, https://arxiv.org/abs/2310.05130\n* W4 - There are many standard machine-text detection benchmarks that could‚Äôve been used such as RAID and MAGE: https://arxiv.org/abs/2405.07940, https://arxiv.org/abs/2305.13242. MAGE in particular would‚Äôve been helpful as it provides many different test-beds that control for things like unseen LLMs during testing time. \n* W5 - Insufficient robustness analysis. For example, what happens when SAFD is trained on one set of LLMs and evaluated on another? How about when it‚Äôs trained on one domain and evaluated on another? A lot of these testbeds are already provided in the MAGE dataset as stated above."}, "questions": {"value": "In general, I'm mainly concerned about W5 / W4 and W3. Addressing these weaknesses would improve upon my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "knjmlkSOTB", "forum": "M4fhuR5wQT", "replyto": "M4fhuR5wQT", "signatures": ["ICLR.cc/2026/Conference/Submission16866/Reviewer_Ue1r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16866/Reviewer_Ue1r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941497889, "cdate": 1761941497889, "tmdate": 1762926883948, "mdate": 1762926883948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SAFD, a style-agnostic detection framework that focuses on content-based features to improve robustness against style-based adversarial attacks. Motivated by their observations that existing detectors degrade their accuracy when LLMs manipulate writing style, the authors propose a style-invariant training paradigm and use adversarially enriched datasets to disentangle content from style. Experiments across multiple datasets demonstrate that SAFD significantly enhances both accuracy and robustness compared to existing detectors."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- A new detection approach motivated by the observation that recent LLMs can generate texts in a wide variety of writing styles, suggesting that content-based detection rather than style-based detection is more appropriate.\n- Extensive evaluation experiments across a wide range of baseline detectors, as well as ablation studies to analyze the effectiveness of different components of their approach."}, "weaknesses": {"value": "- Overall, the paper feels somewhat poorly written, and the following unclear points make the results appear unreliable:\n    - It is not specified how the threshold for determining accuracy (or classification) was chosen.\n    - It is unclear which models were used to generate the LLM side of the detection dataset, as well as the corresponding generation settings. For example, were the texts generated from what prompts and hyper-parameters?\n    - Figures 3 and 4 are never referenced in the main text. How exactly was the style variation in Figure 3 prompted to the LLM?\n    - When using general-purpose LLMs as detectors, how was the detection actually performed? What prompts were used?\n- Regarding the claim that the paper is *‚Äúthe first to systematically investigate the impact of these LLM-driven stylistic alterations on the performance of text detection systems‚Äù*, it is questionable because some similar work already exist [1,2]\n- As for Figure 5, my understanding is that it corresponds to the original (non-adversarial) setting rather than the style-altered one. If so, why does it not include OUTFOX, which often showed runner-up performance in Table 1?\n\n---\n\nReferences:\n\n[1] Shi et al. Red Teaming Language Model Detectors with Language Models. TACL 2023.\n\n[2] Koike et al. How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection. EMNLP Findings 2024."}, "questions": {"value": "See the weaknesses part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wXpKc3NvA0", "forum": "M4fhuR5wQT", "replyto": "M4fhuR5wQT", "signatures": ["ICLR.cc/2026/Conference/Submission16866/Reviewer_R89W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16866/Reviewer_R89W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968679111, "cdate": 1761968679111, "tmdate": 1762926883199, "mdate": 1762926883199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}