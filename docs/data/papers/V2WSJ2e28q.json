{"id": "V2WSJ2e28q", "number": 13593, "cdate": 1758219515552, "mdate": 1763606797913, "content": {"title": "What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities", "abstract": "Evaluations of generative models on benchmark data are now ubiquitous, and their outcomes critically shape public and scientific expectations of AI's capabilities. Yet skepticism about their reliability continues to grow. How can we know that a reported accuracy genuinely reflects a model’s underlying performance? Although benchmark results are often presented as direct measurements, in practice they are inferences: treating a score as evidence of capability already presupposes a theory of what capability is and how it manifests in a testing environment.\n\nWe formalize this observation by proposing a principled framework that treats evaluation as inference: first, articulate a theory of capability, and then derive estimators that target this quantity. This perspective is well established in fields such as psychometrics but remains underdeveloped in AI evaluation, where implicit assumptions often go unexamined. As a proof of concept, we apply our framework to a concrete challenge that undermines reliability: model sensitivity to perturbations. We introduce several capability models and show how various sources of uncertainty (e.g., from finite samples and perturbations) arise within these models as nuisance terms of the latent capability itself. We then use standard tools to derive methods that infer capability while accounting for these sources of uncertainty. Our results illustrate how a capability-centered clarifies what evaluations measure and how to adjust for known sources of unreliability. More broadly, our framework yields evaluations that are transparent, grounded on cognitive theory, and better aligned with the scientific claims they aim to support.", "tldr": "AI evals on benchmark data should be grounded on theories of ability and sound statistical inference.", "keywords": ["Statistical Inference", "AI Ability", "Benchmarks", "Robust Inference", "Item Response Theory", "Robustness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f260225cdc26d09e347a3d3b0cbf6e353bf99516.pdf", "supplementary_material": "/attachment/6fe43490bdf264241983eff70eb57abbc3be77e6.zip"}, "replies": [{"content": {"summary": {"value": "The paper attempts to take an inference-driven approach to AI evaluation --- defining AI capabilities explicitly with a theory of their operation and then design tests that allow inference of capability levels. This is explored across  systematic bias experiments, and with adaptive testing, revealing that systems can be evaluated with reduced sample complexity."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is generally rigorous and well-written. \nI particularly like the argument about how benchmark creation violates an independence assumption. I think this is a valuable point to emphasise more in the paper and the implications it has on evaluation---namely the difficulties in accurately measuring abilities. \n\nThe work in section 4.3 on systematic bias is really strong, and I think makes a good argument for fixing the independence violation in terms of estimating accurate ability score on benchmarks. It would have been even stronger if a wider range of popular benchmarks was evaluated rather than the small collection here."}, "weaknesses": {"value": "The paper presents the psychometric-inspired approach as if it were a novel contribution within the context of AI evaluation. However this isn't the case -- multiple papers have built on IRT before and the only one cited by the authors is more of a position paper about what's feasible with IRT. A few examples include:\n- https://www.sciencedirect.com/science/article/pii/S0004370219300220\n- https://www.ijimai.org/journal/bibcite/reference/2901\n- https://openreview.net/pdf?id=ZyVQqK7mcP\n- https://openreview.net/pdf?id=2QWP4qWVym\n- https://arxiv.org/pdf/2503.06378?\n- https://doi.org/10.1016/j.knosys.2021.108076\nand more. These should be better cited as part of the existing literature to utilise psychometric approaches to AI evaluation. \n\nThe proposed response to fixing the dependence of sampling of the phrasing space uses phrasing perturbations. While I think this is broadly the right approach -- generate more test items that query the ability -- I don't think phrasing perturbations alone fix the problem. Often the constructs we want to evaluate require attention from many different angles. Approaches like Contrast sets https://arxiv.org/pdf/2004.02709 combined with the Factorial Survey method, provide better coverage of the construct than simply rephrasing the question. If by phrasing perturbations, things like contrast sets and Factorial Surveys are intended, the paper should more clearly reflect this and cite previous work more appropriately. \n\nThe work in 5.3 could be expanded in the main body of the text. It seems that this is the crux of the paper --- where the ideas are all supposed to come together to reveal the efficacy of the approach. However, the results are rushed through and largely relegated to the appendix, leaving the casual reader to likely miss the solid empirical work that has gone on here."}, "questions": {"value": "1. What are the trade-offs between the CTT and IRT model in the empirical contexts? Is one more suited to AI evaluation over another? \n2. Most of the novelty of this approach seems to be based on the bias term that is added (s(x_i)) to the capability model and handling that. Is there more novelty that I have missed or is that it? The algorithms developed in section 5 are, I believe, from the literature (or heavily based on existing work). Is this correct?\n3. Why do you omit large sweathes of the literature that is already applying psychometric techniques to AI evaluation in the related work section?\n4. Some models seem to be more susceptible to being systematically biassed against in the naive measurement vs the corrected measurement (Figure 2): What causes this to occur?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZtCu6u9tki", "forum": "V2WSJ2e28q", "replyto": "V2WSJ2e28q", "signatures": ["ICLR.cc/2026/Conference/Submission13593/Reviewer_iZmD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13593/Reviewer_iZmD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761325784024, "cdate": 1761325784024, "tmdate": 1762924180984, "mdate": 1762924180984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates current AI evaluations, questioning their reliability when treated as simple measurements. It proposes a new framework, inspired by psychometrics, that reframes evaluation as a formal statistical inference problem: one must first define a theory of AI \"capability\" and then derive methods to estimate it. As a proof of concept, the authors tackle sensitivity to input perturbations, empirically demonstrating that this factor introduces systematic bias into existing benchmarks, affecting even SOTA models. The paper introduces two robust inference methods: Clustered Bootstrapping to estimate true accuracy and a Latent Ability Adaptive Test based on Item Response Theory to efficiently estimate latent ability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper investigates the reliability and trustworthiness of LLM benchmark evaluations, which is crucial for ensuring the scientific and healthy advancement of the AI field.\n\n2. The paper reframes AI evaluation from a simple measurement to a formal statistical inference problem. This shift, inspired by psychometrics, provides a principled foundation to systematically address known issues like reliability and perturbation sensitivity.\n\n3. The paper is clearly written and well-organized, the figures and tables are also clear and effectively support the paper's claims."}, "weaknesses": {"value": "1. The paper's methodological contribution is incremental. The proposed methods (CBA and LAAT) are largely direct applications of standard techniques—Clustered Bootstrapping and Item Response Theory—from statistics and psychometrics. Given that prior work had already identified both the perturbation sensitivity problem and the potential use of IRT, the paper's primary contribution is its conceptual reframing, not a significant algorithmic advancement.\n\n2. The LAAT algorithm's effectiveness is contingent on pre-inferred IRT item parameters $b_i, a_i$. The authors concede that reliably estimating these parameters requires a calibration step using a large sample of models, which implies a massive, upfront data collection effort. This requirement creates a significant cold-start problem, casting doubt on LAAT's practical utility for new or rapidly evolving benchmark tasks.\n\n3. The paper's empirical support is insufficient to justify its broad claims. (1) The significant claim about SOTA model bias rests on only two models and two tasks, which is insufficient evidence for such a general claim. (2) The proposed methods (CBA and LAAT) are validated on only three tasks, which is not enough to demonstrate their general utility, especially given LAAT's high data prerequisites."}, "questions": {"value": "Q1: The paper's core idea is to eliminate \"systematic bias\" by averaging over a large set of perturbations. This assumes the perturbation dataset itself is an unbiased sample. How do the authors guarantee that their perturbation-generation method does not simply introduce its own new systematic bias?\n\nQ2: To provide accurate parameter estimates for the IRT model in LAAT, approximately what scale of interaction data is required? Furthermore, given this data prerequisite, can the LAAT method remain effective for the rapidly evolving new tasks that most require robust evaluation?\n\nQ3: What are the basic assumptions of using the IRT model? Why did the authors not use Multidimensional IRT or other cognitive diagnostic models as the base model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "p2qu1sbm6L", "forum": "V2WSJ2e28q", "replyto": "V2WSJ2e28q", "signatures": ["ICLR.cc/2026/Conference/Submission13593/Reviewer_NPsR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13593/Reviewer_NPsR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735417720, "cdate": 1761735417720, "tmdate": 1762924180642, "mdate": 1762924180642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reframes LLM benchmarking as inference rather than mere measurement: begin with a theory of capability, then derive estimators that adhere to that theory. It demonstrates that sensitivity to natural prompt perturbations systematically biases conventional accuracy-based scores and that even strong models (e.g., GPT-4.1) exhibit nontrivial bias; it further quantifies per-item variability via the mean absolute distance (M), with real-world deviations of 10–50 percentage points being common and original-versus-perturbed estimates differing by up to ~15 points. Building on this perspective, the authors propose two practical estimators: (a) clustered bootstrap over items to report accuracy with valid confidence intervals, and (b) an IRT-based adaptive test that infers a latent ability (ε) with far fewer samples by targeting high-information items. Together, these ideas provide a principled, statistically grounded procedure for reporting both point estimates and uncertainty in benchmarked capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Originality.**\n\nReframes prompt sensitivity as a systematic bias in capability inference, proving identifiability failures when phrasing is dependently sampled, and proposes random natural perturbations to recover unbiased estimates—an elegant synthesis of CTT/IRT with practical LLM evals.  Introduces two concrete inference methods clustered bootstrapping for accuracy and an IRT-based adaptive test (LAAT) for latent ability—moving beyond prior sensitivity metrics to a principled estimation toolkit.   \n\n**Quality.**\n\nTheoretical backbone is clear: defines capability under CTT/IRT, formalizes the bias mechanism, and states propositions establishing non-identifiability and asymptotic guarantees. Methods are operationalized with algorithms and practical guidance (e.g., bootstrap at the question level; LAAT selection via Fisher information), plus discussion of budget/complexity (Neyman allocation).   Empirics are meaningful: multi-benchmark, multi-model studies show large, direction-agnostic bias (up to ±15 p.p.) and sizable within-item variability (MAD typically 10–50 p.p.); SOTA models remain sensitive.   \n\n**Clarity.**\n\nExposition cleanly separates theory, inference and evidence: introduces CTT vs. IRT background before deriving models with perturbation terms and then presenting algorithms/figures that trace bias and sensitivity. Definitions are crisp and practical (e.g., the construction of natural perturbations and the MAD metric), which makes the abstract claims directly testable and reportable.  \n\n**Significance.**\n\nAddresses a central pain point in LLM evals, unstable scores and misleading leaderboards, by restoring statistical inference to benchmarking and giving intervals and sensitivity as first-class outputs.Broad impact: provides a template labs can adopt now (perturbation-aware CIs; MAD reporting) and a path to more sample, efficient, capability-faithful testing via LAAT, especially valuable as models scale yet remain brittle on frontier tasks."}, "weaknesses": {"value": "1. “Natural perturbations” are underspecified and may import generator bias. The paper lists categories (instruction rewording, option reordering, prompt paraphrase) and suggests using a strong LLM or rules to produce perturbations, but gives no standardized taxonomy, coverage metric, or validity checks; recovery of ε relies on perturbations being “almost” i.i.d. draws from the phrasing space, which is hard to verify. Different generators (or seeds) could tilt both bias estimates and MAD, confounding cross-paper comparisons.\n\n2. Open-source experiments use two benchmarks (LMEntry, BBH) and 8 subtasks; SOTA results cover only MR and GPQA—and only MR gets full treatment due to budget; GPQA perturbations are produced by gpt-4.1-mini, potentially entangling generator and evaluatee. Claims about leaderboard distortion and sensitivity may not generalize to code, tool-use, or long-context tasks; using one vendor model to generate test variants for another poses subtle dependence risks. \n\n3. Building a benchmark is a time-consuming and expensive process. For example, CBA require to expand the question of benchmark to different formal by a LLM, which is time-consuming and expensive. And the result of the LLM still need to be checked by human, which is also time-consuming and expensive.\n\n4. LAAT assumes a specific IRT form with item difficulty/discrimination known a priori; ability is updated via Fisher information and Newton updates. If the true process deviates from the logistic 2PL with an additive phrasing term, estimates are misspecified; moreover, item parameters must be “properly calibrated.” Ability rankings and standard errors may be brittle to the prior, to miscalibrated items, or to departures from the assumed link."}, "questions": {"value": "1. How to deal with the extreme situation? For example, LLM is able to answer the complex arithmetic problems, but in past long time, fail to answer \"which is bigger, 9.11 and 9.9\".\n\n2. For section 5.3, Could authors provide result on larger model or closed-source model?\n\n3. Could author provide more vistualization samples of question in Main text？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cLnaJw1rvv", "forum": "V2WSJ2e28q", "replyto": "V2WSJ2e28q", "signatures": ["ICLR.cc/2026/Conference/Submission13593/Reviewer_rz2e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13593/Reviewer_rz2e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762109942477, "cdate": 1762109942477, "tmdate": 1762924180338, "mdate": 1762924180338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a principled framework for evaluating AI capabilities as inference, reframing benchmark evaluation not as simple measurement but as statistical inference grounded in a theory of capability. Drawing inspiration from psychometrics, the authors bridge Classical Test Theory (CTT) and Item Response Theory (IRT) with modern AI benchmark evaluation. They demonstrate that current benchmarks violate independence assumptions and induce systematic bias due to prompt sensitivity."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1.\tProposes two well-defined, generalizable methods (CBA, LAAT) with theoretical proofs and empirical validation.\n2.\tQuantifies systematic bias from benchmark phrasing; introduces sensitivity metric (MAD) for reporting uncertainty.\n3.\tComprehensive experiments across 7 open-source and 2 proprietary models on multiple benchmarks (BBH, LMentry, GPQA)."}, "weaknesses": {"value": "1.\tThe framework currently addresses only one confounding factor (prompt sensitivity). Other realistic confounders—hyperparameters, context windows, data contamination—are acknowledged but not empirically integrated.\n2.\tWhile philosophically appealing, the notion of “AI capability as latent ability” might require stronger empirical validation or correlation with real-world task performance.\n3.\tSome proofs (e.g., identifiability under dependent phrasing) are deferred to the appendix; including brief intuitions in the main text would improve accessibility."}, "questions": {"value": "1.\tHow sensitive are the conclusions to the choice of perturbation generation model？\n2.\tCould the proposed framework generalize beyond text-based LLMs (e.g., vision-language or reinforcement learning benchmarks)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GgbEMNcyga", "forum": "V2WSJ2e28q", "replyto": "V2WSJ2e28q", "signatures": ["ICLR.cc/2026/Conference/Submission13593/Reviewer_fvvW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13593/Reviewer_fvvW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762169758966, "cdate": 1762169758966, "tmdate": 1762924179870, "mdate": 1762924179870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}