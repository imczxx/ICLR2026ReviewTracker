{"id": "CszaMb7R5D", "number": 18872, "cdate": 1758291629639, "mdate": 1759897076283, "content": {"title": "MOBA: Model-Based Offline Reinforcement Learning with Adaptive Contextual Penalties", "abstract": "Mainstream model-based offline reinforcement learning, which aims to learn effective policies from static datasets, often employs conservatism to prevent policies from exploring out-of-support regions. For example, MOPO penalizes rewards through uncertainty measures from predicting the next states. Prior context-based methods  leverage meta-learning methods, which infer latent dynamics patterns from experience to enable its policy to adapt its behavior in out-of-support regions when deployed, offering the potential to make robust decisions in out-of-support regions and outperform traditional model-based methods.\nHowever, current adaptive policy learning methods still leverage traditional conservative penalties to mitigate the compounding error of the model, which overly constrains policy exploration. In this paper, we propose Model-Based Offline Reinforcement Learning with Adaptive Contextual Penalty (MOBA), which introduces a context-aware penalty adaptation mechanism that dynamically adjusts conservatism based on trajectory history. Theoretically, we prove that MOBA maximizes a tighter lower bound on the true return compared to prior methods like MOPO, achieving an optimal trade-off between risk and generalization. Empirically, we demonstrate that MOBA outperforms state-of-the-art model-based and model-free approaches on NeoRL and d4rl benchmark tasks. Our results highlight the importance of adaptive uncertainty estimation in model-based offline RL.", "tldr": "", "keywords": ["Offline reinforcement learning", "Adaptive Contextual Penalty"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa0f709e31234ae8a862e41c38c66e542f29e87b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the over-constraint issue of fixed conservative penalties in mainstream model-based offline RL. It proposes MOBA, a framework with a context-aware adaptive penalty mechanism that adjusts conservatism via trajectory history (using context-recognition estimator $\\epsilon$ and model-coverage estimator $\\omega$). Theoretically, MOBA tightens the lower bound on true returns compared to fixed-penalty methods like MOPO; empirically, it outperforms SOTA approaches on NeoRL and d4rl tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly analyzes probing-phase recognition error and dynamics-gap error in adaptive policy learning by splitting deployment into probing and reducing phases.   \n2. The paper’s theoretical analysis thoroughly proves the Context-based Telescoping Lemma and shows MOBA’s adaptive penalty tightens the true return lower bound, outperforming fixed-penalty methods like MOPO in theory.  \n3. The paper’s experimental analysis is comprehensive, using ablations to validate $\\epsilon$ and $\\omega$, and testing rollout horizon robustness, fully verifying the context-aware penalty’s effectiveness."}, "weaknesses": {"value": "1. The design rationale for $\\epsilon$ (representing probing-phase recognition error) is insufficiently clarified. The paper does not explicitly explain why using the normalized entropy of the action distribution can effectively reflect the context extractor’s error in identifying true dynamics during exploration.  \n2. Several equations in proofs in Appendix A contain ambiguities or inconsistencies. For example, the rewarite of $G_{\\hat{M}}^\\pi(s,a)$ in equation (7) is inconsistent with equation (5).\n3. While the context-aware penalty quantification mechanism is valuable, the core innovation remains limited. It operates within the existing adaptive policy learning framework of model-based offline RL, with penalty designs rooted in MOPO, leading to modest overall novelty."}, "questions": {"value": "1. Why is the normalized entropy of actions used to reflect the probing-phase error of the context extractor? From my understanding, The normalized entropy of actions only measures the policy’s certainty about the next action, but it cannot quantify the exploration error of the context accumulated from states prior to the current one.  \n2. Most of the benchmark methods compared in the paper were published before 2023. Is this because no new model-based reinforcement learning methods have been published in the last two to three years?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1YKWudjs4p", "forum": "CszaMb7R5D", "replyto": "CszaMb7R5D", "signatures": ["ICLR.cc/2026/Conference/Submission18872/Reviewer_sBH5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18872/Reviewer_sBH5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621185289, "cdate": 1761621185289, "tmdate": 1762930837334, "mdate": 1762930837334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MOBA, a novel model-based offline RL approach. It addresses the limitation of traditional methods, which use fixed conservative penalties that overly restrict policy exploration. MOBA employs an adaptive contextual penalty that dynamically adjusts conservatism based on trajectory history, resulting in SOTA results and a tighter theoretical lower bound on the return."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Well motivated. It addresses the limitation of traditional methods, which use fixed conservative penalties that overly restrict policy exploration.\n2) Both empirical and theoretical results are provided. Empirical results show strong improvement compared to baselines"}, "weaknesses": {"value": "1) The submission is targeting ICLR 2026, yet the baselines are exclusively from 2022–2023. There is a significant risk of lacking comparison to more recent, state-of-the-art methods.\n2) The proposed method explicitly aims to solve issues in out-of-support regions, but the current experiments are performed solely on seen tasks. To properly validate the method's contribution, the authors must evaluate performance on unseen tasks to demonstrate robustness in more severely out-of-support regions."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "knOobMfOLw", "forum": "CszaMb7R5D", "replyto": "CszaMb7R5D", "signatures": ["ICLR.cc/2026/Conference/Submission18872/Reviewer_dpkc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18872/Reviewer_dpkc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632438175, "cdate": 1761632438175, "tmdate": 1762930836765, "mdate": 1762930836765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focused on offline model-based RL. The authors proposed MOBA that introduces a context-aware penalty adaptation mechanism to dynamically adjust conservatism based on trajectory history. Theoretically, MOBA was proven to maximize a tighter lower bound on the true return. And it was shown to outperform state-of-the-art methods on benchmark tasks, highlighting the importance of adaptive uncertainty estimation in model-based offline RL. The authors provided theoretical justification and comprehensive experimental evidence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper thoroughly identifies flaws in existing model-based offline RL methods (e.g., MOPO’s fixed penalties, MAPLE’s residual conservative bias) and proposes MOBA’s adaptive mechanism (integrating ε(s,a) and ω(s,a)) to resolve the \"over-constraint vs. error accumulation\" dilemma.\n\n2.MOBA targets real scenarios (robotics, healthcare) by using dynamics ensembles to boost data efficiency, achieves top average scores on the near-real-world NeoRL benchmark, and provides detailed implementation details for reproducibility and deployment.  \n\n3.The paper follows a rigorous structure (from problem formulation to future directions) with tight links between theory and experiments, and uses precise terminology to explain complex concepts clearly, balancing rigor and readability."}, "weaknesses": {"value": "1.The rationale behind the specific designs of ε(s, a) and ω(s, a) requires clarification. Please explain the motivation for formulating ε(s, a) as an entropy measure and ω(s, a) as the ensemble variance (or average covariance).\n\n2.Experimental Section: Why is there no analysis on Out-of-Distribution (OOD) generalization and distribution shift?\n\n3.Ablation Study: Is it need to add ablation results that analyze the runtime performance?\n\n4.Variance details in the experimental section: What is the variance of the benchmark? How many times were the specific experiments conducted, and how were the parameters set (these may be available in the code)?"}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fhgT8tVuq5", "forum": "CszaMb7R5D", "replyto": "CszaMb7R5D", "signatures": ["ICLR.cc/2026/Conference/Submission18872/Reviewer_hBaB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18872/Reviewer_hBaB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830060536, "cdate": 1761830060536, "tmdate": 1762930836014, "mdate": 1762930836014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a model-based offline RL method that incorporates context awareness into the mainstream conservative model-based approaches (e.g., MOPO-style). Compared to the fixed model uncertainty penalty used in MOPO, the proposed framework scales the model uncertainty penalty adaptively by context, so the policy is penalized more in out-of-distribution regions and less where both the policy and model are confident. Under their assumptions, this yields a tighter lower bound on return than a fixed penalty, as the adaptive penalty is used to avoid excessive pessimism. Experiments on standard benchmarks support these claim."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is natural: context-agnostic pessimism (i.e., a fixed penalty) can be overly conservative, which constrains exploration in out-of-support regions. The paper addresses this by introducing a context-aware penalty term in the reward function.\n\n2. The flow of presentation is organized: the method section begins with a brief introduction to a probing-reduction paradigm (Section 4.1), which motivates the proposed scalar modulator on the traditional uncertainty-penalization term. Discussions in section 4.1 also motivates the proof of Lemma 4.1.\n\n3. Empirical support: Experiments on standard benchmarks demonstrate the effectiveness of the proposed context aware uncertainty penalty."}, "weaknesses": {"value": "My main concerns are as follows:\n\n1. Ambiguity in theoretical notation and definitions: in section 4.2, the definition of $\\hat{T}$ is not that clear. It appears to denote the ensemble transition in the proof, whereas section 4.1 defines it as the single best model. This mismatch makes Lemma 4.1 hard to parse and invites confusion when comparing to MOPO’s Lemma 4.1. In addition, definitions of $\\lambda$, $\\epsilon$, $\\omega$ in Lemma 4.1 are lacking. I suggest to revise the presentation in Lemma 4.1.\n\n2. Vague rationales of $\\epsilon$, $\\omega$ used in section 4.3: In section 4.3, the paper selects specific proxies for them but it is unclear why these choices follow from the theory. I suggest to connect these proxies to the constructs derived in Appendix A.4."}, "questions": {"value": "1. Could you explicitly derive how the proxies in section 4.3 correspond to the theoretical constructs in Appendix A.4? \n\n2. How do you ensure $\\epsilon$ and $\\omega$ are both less than 1 in practice? Could you discuss a bit more why $\\epsilon$ and $\\omega$ in section 4.3 satisfy this condition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eyRU6709hB", "forum": "CszaMb7R5D", "replyto": "CszaMb7R5D", "signatures": ["ICLR.cc/2026/Conference/Submission18872/Reviewer_eeS2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18872/Reviewer_eeS2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900269575, "cdate": 1761900269575, "tmdate": 1762930835453, "mdate": 1762930835453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their constructive feedback. We are encouraged by the recognition of our well-motivated problem setting , rigorous structure , and empirical support. Below, we address the three common questions raised across reviews.\n\nComparison with New SOTA Baselines (Response to dpkc, sBH5)\n\nReviewers noted that our baselines were primarily from 2022-23. We initially selected these works because our experimental setup for environment model learning is identical to theirs, thereby enabling a more direct and focused demonstration of the algorithmic advantages introduced in our approach.  However, to demonstrate MOBA's current relevance, we have added comparisons against MOREC[1] (ICLR 2024)  and ADMPO[2] (ICLR 2025). Results show that MOBA remains highly competitive, outperforming latest SOTA on neorl and d4rl tasks\n\n| Task            | MOPO | MOBILE | MAPLE | MOBA(Ours) | ADMPO (ICLR2025) |\n|-----------------|------|--------|-------|------|--------------|\n| halfcheetah-rnd | 38.5 | 39.3   | 38.4  | 38.3 | 45.4 ± 2.8   |\n| hopper-rnd      | 31.7 | 31.9   | 10.6  | 33.2 | 32.7 ± 0.2   |\n| walker-rnd      | 7.4  | 17.9   | 21.7  | 24.1 | 22.2 ± 0.2   |\n| halfcheetah-med | 73.0 | 74.6   | 50.4  | 79.8 | 72.2 ± 0.6   |\n| hopper-med      | 62.8 | 106.6  | 21.1  | 105.6| 107.4 ± 0.6  |\n| walker-med      | 84.1 | 87.7   | 56.3  | 82.2 | 95.5 ± 8.7   |\n| halfcheetah-med-rep | 72.1 | 71.7 | 59.0  | 69.7 | 67.6 ± 3.4   |\n| hopper-med-rep  | 103.5| 103.9  | 87.5  | 110.8| 104.4 ± 0.4  |\n| walker-med-rep  | 85.6 | 89.9   | 76.7  | 95.1 | 95.6 ± 2.1   |\n| **Average**     | 62.1 | 69.3   | 46.9  | 72.5 | 71.4         |\n\n| Task Name     | MOPO | MOBILE | MOBA (Ours) | MOREC(ICLR2024) | \n|---------------|------|--------|-------------|------------|\n| HalfCheetah-L | 40.1 | 54.7   | 51.3 ± 0.4  | 53.5 ± 0.6 | \n| Hopper-L      | 6.2  | 17.4   | 33.0 ± 0.3  | 25.4 ± 1.3 |\n| Walker2d-L    | 11.6 | 37.6   | 70.5 ± 0.8  | 65.0 ± 1.3 | \n| HalfCheetah-M | 62.3 | 77.8   | 86.2 ± 1.3  | 84.1 ± 0.5 |\n| Hopper-M      | 1.0  | 51.1   | 74.6 ± 18.3 | 83.5 ± 3.8 | \n| Walker2d-M    | 39.9 | 62.2   | 78.6 ± 2.2  | 76.6 ± 1.7 | \n| **Average** | 26.9 | 50.1 | 65.7        | 64.7       | \n\nNew OOD Experiment: \"Hard-CartPole\" (Response to hBaB, dpkc)\n\nTo address concerns about Out-of-Distribution (OOD) robustness beyond standard benchmarks, we introduced a \"Hard-CartPole\" environment with unseen physics (varying pole length from 0.4 to 0.6, masses from 0.05 to 0.15). \n\n The offline dataset was collected exclusively using \"Standard\" physical parameters . This mimics the scenario where the agent learns from historical data covering only nominal dynamics. The learned policy was deployed and evaluated in environments with perturbed, unseen parameters.\n\nResults: In this new \"Hard-CartPole\" benchmark, MOBA achieves a score of 556, significantly outperforming all baselines, as shown in the performance table:\n\n| Task Name | MOPO | MAPLE | MOBILE | MOBA | \n| :--- | :--- | :--- | :--- | :--- | \n| Hard-CartPole | 421 | 421 | 493 | 556 |\n\nRationale for Proxies $\\epsilon$ and $\\omega$ (Response to eeS2, hBaB, sBH5)\n\n$\\epsilon$ captures the ambiguity in the probing phase. When the context extractor fails to eliminate the incompatible models, the agent effectively faces a superposition of multiple possible dynamics. Since the optimal action often varies significantly across these conflicting dynamics, a rational policy cannot commit to a single action. Instead, it must maintain a dispersed probability mass (high entropy) to hedge against this uncertainty. Therefore, high entropy is not merely randomness; it is a direct signature of the agent facing multiple possible environments due to an uninformative context.\n\nWe formulate $\\omega$ as the ratio $\\frac{\\text{Provided Diversity}}{\\text{Required Diversity}}$. This proxy measures whether the ensemble is rich enough to cover the true dynamics. The Numerator (Ensemble Variance) represents the diversity of dynamics provided by our current models. The Denominator (Aleatoric Uncertainty) represents the diversity required to account for the inherent stochasticity of the real environment. When this ratio is large (i.e., Provided > Required), it indicates that the ensemble's divergence is sufficient to cover the true world dynamics within its support. Conversely, a small ratio suggests that the ensemble is too narrow to capture the real world's inherent complexity, signaling a risk of over-confidence.\n\n\n\n\n[1] Luo, Fan-Ming, et al. \"Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning.\" The Twelfth International Conference on Learning Representations.\n\n[2] Lin, Haoxin, et al. \"Any-step Dynamics Model Improves Future Predictions for Online and Offline Reinforcement Learning.\" The Thirteenth International Conference on Learning Representations."}}, "id": "FjJfJHXs2M", "forum": "CszaMb7R5D", "replyto": "CszaMb7R5D", "signatures": ["ICLR.cc/2026/Conference/Submission18872/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18872/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18872/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763552481362, "cdate": 1763552481362, "tmdate": 1763552481362, "mdate": 1763552481362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}