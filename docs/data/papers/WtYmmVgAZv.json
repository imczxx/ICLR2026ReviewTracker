{"id": "WtYmmVgAZv", "number": 16189, "cdate": 1758261275574, "mdate": 1759897255703, "content": {"title": "From Attacks to Guidance: Direct Output Control for Classifiers", "abstract": "In high-risk domains such as autonomous driving and medical diagnosis, classifier misclassifications pose severe risks. Existing repair approaches fall into three categories: test-time adaptation (TTA), adversarial perturbation methods such as PGD and DeepFool, and counterfactual generation (CF). TTA and perturbation methods lack stability guarantees or irreparability diagnosis, while CF targets distributional plausibility rather than direct control.We propose $Direct Output Control (DOC)$, which repairs misclassifications by directly regulating the output distribution without changing model parameters. DOC defines the Fisher–Rao distance as a Lyapunov function, pulls back its gradient through the Jacobian pseudoinverse, and derives minimum-norm perturbations that monotonically reduce error. The framework generalizes to other metrics (e.g., $L_2$) and provides both a theoretical irreparability bound based on Jacobian singular values and inter-class margins, and an empirical diagnostic using Lyapunov decrease.On ImageNet-1k with ResNets and Vision Transformers, DOC outperforms TTA and perturbation methods in repair success while inducing smaller distortions, though at higher inference cost. Our contributions are: (1) a Lyapunov-control formulation with monotonic stability, (2) theoretical analysis including irreparability, minimum-norm, and natural gradient connection, (3) an empirical diagnostic via Lyapunov decrease, and (4) large-scale validation showing Pareto superiority in success–distortion trade-offs.", "tldr": "DOC stably repairs misclassifications with guarantees and diagnostics, outperforming related methods.", "keywords": ["Classification", "Misclassification repair", "Optimization", "Control", "Stability", "Deep learning", "Information geometry"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c6cbb10a02c49c829ed5e41dd68e9335c3cab048.pdf", "supplementary_material": "/attachment/562e12f861f1eb30cd077d9ffac6d38d19f579ef.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Direct Output Control (DOC), a test-time framework that repairs misclassifications of fixed neural networks by applying minimal, auditable input perturbations that monotonically reduce the discrepancy between the model prediction and a target distribution (usually the correct label). The discrepancy is treated as a Lyapunov function, instantiated via the Fisher-Rao distance on the probability simplex. By pulling back its gradient through the classifier’s Jacobian pseudoinverse, DOC guarantees a monotonic decrease in this Lyapunov value while producing the minimum-norm input change. The authors provide theoretical analyses of stability, optimality, and a first-order irreparability bound, and show that the approach connects to the natural gradient under certain rank conditions. Experiments on ImageNet-1k with ResNet and ViT models demonstrate higher repair success rates and lower distortion compared to PGD, DeepFool, and test-time adaptation methods, albeit at higher computational cost."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper’s objective is clearly formulated: to perform inference-time repair of misclassifications through minimal, stable, and auditable input perturbations.\n\n- The authors provide a coherent theoretical foundation, including Lyapunov stability guarantees, a minimum-norm control proof, and an irreparability condition grounded in Jacobian analysis, which together lend credibility and interpretability to the approach. \n\n- The presentation of experiments and their setting is clear. Metrics such as repair success, L2 and LPIPS distortion, and diagnostic AUC are clearly defined and appropriately analyzed."}, "weaknesses": {"value": "- Writing is not very clear or coherent, specially in the introduction and theoretical analysis\n\n- Citations are missing in introduction, and also throughout the file they are not clickable, moreover, the list of citations seem very thin and the work does not provide much grounding in any clear literature\n\n- The appendix is missing, and the authors are refering a lot to the appendix, creating unverifiable claims in the main text.\n\n- Most importantly, the problem statement does not really make any sense, to have access to labels in inference and fix errors; even the paper acknowledges labels must be specified (human-in-the-loop or auxiliary modules), which severely limits applicability.\n\n- The baselines that the method compares with do not make sense, as they are mostly adversarial attacks that perturb the model and not repair tools (PGD/DeepFool), so the comparison objective is mismatched for “repair toward the ground truth.”\n\n- The compared baselines, benchmarks, and architectures are very old fashioned and not used in any practical setting; even if there was a use case for repairing errors in inference, more recent models in the vision and language domain should be analyzed"}, "questions": {"value": "- How do the authors justify the assumption that ground-truth labels (or a reliable proxy) are available at inference time for repair? In what practical scenarios would this be realistic?\n\n- Why were PGD and DeepFool chosen as baselines for “repair” when these are primarily adversarial attack methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MbC9nDZMLs", "forum": "WtYmmVgAZv", "replyto": "WtYmmVgAZv", "signatures": ["ICLR.cc/2026/Conference/Submission16189/Reviewer_fhiS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16189/Reviewer_fhiS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760831669199, "cdate": 1760831669199, "tmdate": 1762926352682, "mdate": 1762926352682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To suppress classifier misclassification at test-time, the authors propose Direct Output Control (DOC). DOC adds small perturbations to the input that correct misclassifications. These perturbations are obtained by computing input gradients that reduce the distance between the model's output and the ground truth label. Unlike test-time adaptation (TTA), DOC does not require parameter updates."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "As indicated in Weakness 1, while I do not understand the purpose or motivation, the concept of inference-time repair and diagnosis may be novel."}, "weaknesses": {"value": "**1. Research objective**\n\nThis study considers inference-time repair and diagnosis, but I could not understand why this is necessary or how it is meaningful. In other words, the ultimate goal of this research is unclear to me.\n\nResearch like TTA that dynamically adapts to the input distribution at inference-time has the ultimate goal of achieving high accuracy during inference. If this study shares the same objective as TTA, then using ground truth labels in the method is incorrect. As with TTA, ground truth labels are typically unavailable at inference-time, and if they are known, there is no need for inference.\n\nIf the goal is to qualitatively assess which components of an image influence the classifier's prediction, as in counterfactual explanations (CF), then there is insufficient experimentation regarding the effects on images.\n\n**2. Theoretical results**\n\nTo my understanding, the authors' method obtains image gradients to reduce the difference between the model's output and the ground truth label, and applies them to the image. In other words, this is the inverse of adversarial attacks. It is trivial in principle that this becomes an optimal perturbation in a local region around the input. Therefore, I found no novel value in the monotonic error reduction claimed in Theorem 4.1. This is as self-evident as the fact that single-step PGD (i.e., FGSM) is an optimal attack in a highly local region. What matters is whether these remain optimal in non-local regions or even after multi-step updates. This study provides no proof for these cases. Therefore, the monotonic error reduction and minimal norm properties claimed in the paper are incorrect, or at least, overclaimed.\n\n**3. Experiments**\n\nThis study only uses ImageNet as a dataset. At minimum, two additional different datasets should be used. Moreover, the comparison baselines are very old adversarial attack methods such as PGD and DeepFool. As mentioned in point 1, since the objective of this research is unclear to me, I cannot clearly determine what would be appropriate comparison baselines, but at the very least, I do not understand why comparisons are not made with AutoAttack [1], particularly attack methods that employ smarter gradient computation and optimization strategies such as AutoPGD [1] and FAB [2].\n\n[1] Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks [Croce et al., ICML20]  \n[2] Minimally distorted adversarial examples with a fast adaptive boundary attack [Croce et al., ICML20]\n\n**4. Presentation of results**\n\nOverall, the presentation of experimental results is difficult to read. In particular, the x-axis and y-axis labels are extremely small compared to the normal text font. This degrades the presentation quality of the paper."}, "questions": {"value": "See the above. In particular, Point 1 is essential to me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rh8hUEJpDq", "forum": "WtYmmVgAZv", "replyto": "WtYmmVgAZv", "signatures": ["ICLR.cc/2026/Conference/Submission16189/Reviewer_NL17"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16189/Reviewer_NL17"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760945386751, "cdate": 1760945386751, "tmdate": 1762926352120, "mdate": 1762926352120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Direct Output Control (DOC), a Lyapunov-based framework for repairing misclassifications at inference time without modifying model weights. The method defines the Fisher-Rao distance between model output and target as a Lyapunov function, backpropagates its gradient through the Jacobian pseudoinverse, and guarantees monotonic error decrease under mild conditions. DOC also introduces an empirical diagnostic signal ($\\Delta V$) and a theoretical irreparability bound based on Jacobian singular values. Experiments on ImageNet-1K with ResNets and ViT show improved repair success rates and smaller distortions compared to PGD and DeepFool."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper’s conceptual framing is elegant and intellectually appealing, as it reinterprets adversarial perturbations not as attacks but as a form of output-level control. This reframing is philosophically fresh and gives a new lens to view inference-time robustness, positioning the work as a bridge between control theory and deep learning rather than another incremental attack-defense method. \n2. The integration of concepts from Lyapunov stability, Jacobian-based dynamics, and information geometry gives the method an appealing theoretical flavor. Even though the algorithm itself resembles existing gradient-based methods, the authors successfully situate it within a broader control-theoretic narrative that adds interpretability and a sense of principled design. \n3.  The experimental section, while somewhat narrow in scope, is carefully structured and internally consistent. Results across multiple architectures (ResNet and ViT) show that DOC reliably achieves higher repair success and lower perceptual distortion than PGD and DeepFool. The inclusion of $\\Delta V$ as an empirical diagnostic, alongside the Jacobian-based irreparability bound, strengthens the framework’s interpretability and gives it an analytic character rarely seen in robustness papers."}, "weaknesses": {"value": "1. Technical novelty is limited. The proposed control law $\\delta x = -J^\\dagger \\nabla_y V$ is essentially equivalent to standard white-box attack updates such as DeepFool or PGD. The Lyapunov function behaves identically to a loss function, and the claimed monotonic decrease follows naturally from choosing a small step size. While the control-theoretic narrative is new, the underlying algorithmic operation offers little technical innovation. \n2. Mismatch between motivation and experiments. The introduction emphasizes safety-critical domains like autonomous driving and medical imaging, yet all experiments are conducted on ImageNet-1K classification. This gap between motivation and validation weakens the practical significance of the work. Additionally, the method’s behavior on more complex structured tasks (e.g., segmentation or multi-label classification) remains unexplored. \n3. Lack of a clear pre-repair diagnosis mechanism. The framework assumes we know which samples require repair, but provides no explicit method for detecting failure cases beforehand. If repair is to be deployed in real systems, a robust “repair trigger” is essential, and $\\Delta V$ alone cannot serve as a pre-emptive signal since it is computed after observing model behavior. \n4. Reproducibility concerns. The paper does not provide code, hyperparameters, or implementation details sufficient to reproduce results. Given the computational complexity of Jacobian pseudoinverses and Lyapunov dynamics, open-sourcing code is necessary for verification."}, "questions": {"value": "1. Could the authors clarify whether $\\Delta V$ is intended as a predictive or post-hoc diagnostic signal? Specifically, is there any theoretical link or formal condition under which a small $\\Delta V$ provably implies irreparability beyond local linearization? \n2. Could the authors justify the choice of baselines? Why were query-heavy or multi-step methods not included in comparison, given that DOC incurs comparable computational overhead? \n3. Were the reported success rates averaged across multiple runs or computed from a single trial? Could the authors report standard deviations or clarify the statistical significance of the results? \n4. Could the authors briefly define key notations such as $\\operatorname{Im} J$ and the inner product operator, either in an appendix or a notation table, to aid readability for non-control-theory readers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KgxifgB9Gu", "forum": "WtYmmVgAZv", "replyto": "WtYmmVgAZv", "signatures": ["ICLR.cc/2026/Conference/Submission16189/Reviewer_NFvA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16189/Reviewer_NFvA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761532880245, "cdate": 1761532880245, "tmdate": 1762926351691, "mdate": 1762926351691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Direct Output Control (DOC) to improve the robustness of neural networks. DOC computes a correction $\\delta x$ to the input, and applies the corrected sample $x + \\delta x$ to a pretrained classifier $f$. A crucial incorrectness of this paper is that the correction $\\delta x$ is a function of the true label $y^\\star$, i.e., $\\delta x = \\int -J \\nabla V(\\hat y, y^\\star) dt$. This essentially means that the proposed robust classifier $f(x + \\delta x)$ needs to take the true label $y^\\star$ as input. However, in the classification task, any classifier should not have access to the true label $y^\\star$, otherwise the problem is vacuous."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The algorithm seems to significantly outperform previous methods (e.g., PGD) in the numerical experiments. However, as specified in the summary section, the robustness originates from the classifier's access to the true label."}, "weaknesses": {"value": "(1) As specified in the summary section, the proposed robust classifier $f(x = \\delta x)$ has access to the true label $y^\\star$, which is invalid in the classification task. \n\n(2) The presentation of Sections 4 and 10 needs to be improved."}, "questions": {"value": "See the summary and weakness sections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2YhVqCZeUN", "forum": "WtYmmVgAZv", "replyto": "WtYmmVgAZv", "signatures": ["ICLR.cc/2026/Conference/Submission16189/Reviewer_RGEt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16189/Reviewer_RGEt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972629470, "cdate": 1761972629470, "tmdate": 1762926351266, "mdate": 1762926351266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}