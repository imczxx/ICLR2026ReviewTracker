{"id": "SPxRgjtsd5", "number": 10710, "cdate": 1758180139033, "mdate": 1759897634065, "content": {"title": "A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning", "abstract": "Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.", "tldr": "This paper introduces the Latent Causal Diffusion Model (LacaDM) for Multiobjective Reinforcement Learning (MORL), addressing challenges in dynamic environments by learning latent causal relationships between states and policies.", "keywords": ["Multiobjective Reinforcement Learning", "Diffusion Model", "Casual Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5555d61b59033fd892fc3d363d423e510062fb59.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a diffusion model that utilizes causal representation learning to enhance performance and generalization in diverse multi-objective reinforcement learning tasks. Empirical evaluations are presented to demonstrate the performance of the proposed model."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work investigates the potential of combining causal representation learning to enhance the performance and generalization of diffusion models for MORL.\n2. Both continuous and discrete action spaces are taken into consideration, which is comprehensive."}, "weaknesses": {"value": "1. All numerical results are presented without standard errors or confidence intervals, making it hard to interpret the results.\n2. The curves in Figure 2 and Figure 5 seem suspicious, as many of them converge and become fixed in both average and standard error. Explanations are needed.\n3. This work claims that LacaDM can “adapt” to dynamic environments and “continuously learn.” However, no experimental results support these claims.\n4. The results in Figure 3 are poorly explained, and it is hard to understand how CRL and full LacaDM promote generalization.\n5. It would be better to list the definitions of the three metrics (HV, SP, EUM) to help readers understand the results.\n6. In Section 2.2, MORL environments are typically modeled as multi-objective MDPs (MOMDPs) instead of MDPs.\n7. As the proposed model focuses on learning from pre-collected trajectory datasets, there should be experiments investigating the influence of dataset quality."}, "questions": {"value": "1. How does causal representation enhance generalization? An example would be appreciated.\n2. How is the problem of distribution shift solved, as the model is learning from offline datasets?\n3. Can the diffusion steps be improved?\n4. In the results of Figure 4, it requires nearly 1500 diffusion steps to maximize HV. Why are the diffusion steps set to 100 as listed in Table 4? Would that be enough?\n5. How does the derivation of the CRL framework proceed in Appendix A.5? I cannot follow the arguments, as they only list many concepts and definitions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iAm8nBKg0n", "forum": "SPxRgjtsd5", "replyto": "SPxRgjtsd5", "signatures": ["ICLR.cc/2026/Conference/Submission10710/Reviewer_BJqa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10710/Reviewer_BJqa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704707023, "cdate": 1761704707023, "tmdate": 1762921947040, "mdate": 1762921947040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### Summary \n\nThis paper proposes a novel method, LacaDM, to address challenges in multi objective reinforcement learning, particularly objective conflicts and poor adaptability in dynamic environments. LacaDM integrates CRL with diffusion models, aiming to learn the latent temporal causal relationships between environmental states and policies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### Strengths\n\n1.The experimental design is comprehensive, covering a diverse set of 8 discrete and 8 continuous environments.\n\n2.The empirical results demonstrate that LacaDM significantly outperforms the baselines on most tasks, showing clear advantages in both Hypervolume and Sparsity metrics.\n\n3.The integration of concepts from causal reinforcement learning is a novel contribution."}, "weaknesses": {"value": "### Weaknesses\n\n1.The manuscript lacks clarity, leaving several key settings ambiguous. Most importantly, it is unclear whether the paper addresses an offline or online multi objective reinforcement learning problem. Based on the experimental setup, which involves pre collecting data for training, the method appears to be designed for the offline setting.\n\n2.If the paper targets the offline setting, several core motivations seem contradictory. For instance, the paper states in lines 65-68 that existing models \"fail to account for the temporal and latent dependencies that arise from the interaction between the agent’s actions and the evolving environment\". In an offline setting, the environment is static by definition. Why then is the challenge of an \"evolving environment\" emphasized as a motivation?\n\n3.Offline multi objective reinforcement learning is an established research area featuring well known benchmarks and baselines, such as Pareto Efficient Decision Agents PEDA[1]. Furthermore, several existing works combine diffusion models with multi objective reinforcement learning [2, 3], yet these are not discussed or compared. The baseline MTDiff, which is a multi task diffusion model, does not seem to be a closely matched comparison for this specific problem.\n\n4.The calculation methods for Hypervolume and Sparsity are not detailed. It is also unclear what \"10 independent runs\" signifies. Does this imply sampling only 10 preference vectors? This seems low compared to benchmarks like PEDA, which typically sample 255 preferences for comprehensive coverage.\n\n5.Several definitions are ambiguous. It is unclear whether pi represents a parameterized policy or a specific action. Does the diffusion model generate sequences of actions or policy network parameters? For example, pi is defined as a policy, but in Equation 15, it appears to represent an action sequence. Furthermore, the paper fails to distinguish between the diffusion time steps and the environment's decision time steps, which is a critical distinction in diffusion based decision making.\n\n6.There is a minor typo: the term should be \"Multi Objective\".\n\n[1] Scaling Pareto-Efficient Decision Making Via Offline Multi-Objective RL.  ICLR2024\n[2] MODULI: Unlocking Preference Generalization via Diffusion Models for Offline Multi-Objective Reinforcement Learning. ICML2025\n[3] Generalizable Offline Multiobjective Reinforcement Learning via Preference-Conditioned Diffuser. TNNLS"}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s8g2gddr6i", "forum": "SPxRgjtsd5", "replyto": "SPxRgjtsd5", "signatures": ["ICLR.cc/2026/Conference/Submission10710/Reviewer_5y78"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10710/Reviewer_5y78"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708658292, "cdate": 1761708658292, "tmdate": 1762921946662, "mdate": 1762921946662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LacaDM, which utilizes Reinforcement Learning (RL) context embeddings to guide a diffusion process for generating an optimal policy to solve Multi-Objective Reinforcement Learning (MORL) problems. The RL context embeddings are derived from PCN-generated trajectories, and the latent variables $z$ are learned via CRL."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is intriguing and demonstrates strong performance when compared to both continuous and discrete MORL algorithms."}, "weaknesses": {"value": "- The paper is extremely difficult to follow. The notation is confusing and inconsistent, making the methodology hard to grasp starting from Section 4 (Methodology). (Detailed issues are listed in the Questions section below).\n\n- The algorithm's name is inconsistent, appearing as \"LaCaDM\" in some parts of the paper and \"LacaDM\" in others. This should be standardized."}, "questions": {"value": "There are several critical ambiguities in the description of the diffusion process and the meaning of the policy $\\pi$: Line 247, Page 5: The text states, \"We begin by solving $N$ distinct MORL problems to convergence and recording the resulting policy $\\pi_T$ and cumulative reward sequences $R_T$.\" $T$ is defined as the total number of time steps in the diffusion process. Why does solving $N$ distinct MORL problems yield $T$ policies ($\\pi_T$)? How are the noisy policies $\\pi_t$ for $t < T$ obtained from these $N$ converged solutions? In Section 2.2, $\\pi$ is defined as a mapping from state to action. In the context of RL practice, this mapping is typically parameterized by a neural network. What exactly does the \"policy generated by the diffusion process\" refer to? Does it represent the entire set of neural network parameters? What is the neural network architecture of the diffusion model? How is it trained to output the complete set of neural network parameters that define the policy mapping $\\pi(s)$? The definition of $R_t$ is unclear. The preceding definitions only introduce the vector reward $r$ and the vector cumulative return $G$. What is the scalar quantity $R_t$ defined here? Furthermore, how are $T$ different cumulative rewards generated from solving $N$ problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pKD7oe1kXq", "forum": "SPxRgjtsd5", "replyto": "SPxRgjtsd5", "signatures": ["ICLR.cc/2026/Conference/Submission10710/Reviewer_WU4c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10710/Reviewer_WU4c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808921043, "cdate": 1761808921043, "tmdate": 1762921946264, "mdate": 1762921946264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LacaDM, a latent causal diffusion model for multi-objective reinforcement learning (MORL). After generating trajectories using a trained MORL algorithm (e.g., PCN in this case), the diffusion model is trained to reproduce the policy information embedded in those trajectories. In addition, a latent representation is learned through an encoder-decoder structure that predicts denoised samples, ensuring causal consistency between latent variables and policy evolution. During deployment, the causality-aware generative model is used for policy generation. Experimental results show that LacaDM outperforms baseline methods across standard MORL metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Provides a clear and detailed explanation of the background.\n\n- Tackling representation learning to improve generalization and model transferability in MORL is a promising research direction."}, "weaknesses": {"value": "1. Flow of the proposed algorithm is hard to understand. Several key components should be clarified.\n- Please provide a pseudocode for better readability.\n- Section 4.3 – the core part of this paper – should be clarified.\n   - It seems that \\hat{pi}_t and \\hat{pi}_{t-1} should be interchanged in Eqs. (17), (18), and the second term in Eq. (19) because the denoising process is considered. Plus, Section 4.3 is inconsistent with the notation in Fig. 1: pi_0 is random policy in Fig. 1 while it is the optimal one in Section 4.3 (Line 272).\n   - What is \\hat{pi}_0? Is it the output of the policy, the single feature vector of the trained PCN network, or the PCN network itself (i.e., multiple layers)? If the latter is the case here, the proposed method may have low flexibility because the model configuration is fixed in advance. (I guess the first one may be used here because then we can apply discrete diffusion in the discrete action envs.) \n   - Which of the following is used for generating the denoised \\hat{pi}_{t-1}? (a) Generate it using Eq. (17), and then z_t is separately trained following (18). (b) Generate \\hat{pi}_{t-1} using Eq. (17) conditioned on z_t. I guess the first one is used here because it seems that z_t is separately trained in the second term of Eq. (19).\n   - What would be the advantage of the proposed method using diffusion model for “policy” generation compared to using diffusion model for action (sequence) generation? \n- How does the inference/deployment phase look like? For example, after training the model in Figure 1, do we generate some policies \\hat{pi}_{0} (either feature or PCN model itself) and the corresponding latent vectors \\{ z_{t-\\tau} \\} and use them as in Eq.(13)? (This will be explained once a pseudocode is provided.) Do we further train $\\pi_\\theta$ in Eq. (13)?\n\n&nbsp;\n\n2. More ablation studies should be conducted.\n-       Ablation on the effect of beta and lambda in Eq. (19) is required.\n-\tHow LacaDM-CRL is implemented? Just setting $\\beta=0$ in Eq. (19) or no usage of $\\{ z_{t-\\tau} \\}$ in Eq. (13)? Please clarify this.\n-\tWhy making \\hat{pi} sparse is useful in Eq. (19)?\n-\tPlease clarify the color using explicit color bar in Fig. 3.\n-\tHow does the final policy feature (output of the penultimate layer) look like? Please visualize w/ and w/o CRL (e.g., using tSNE).\n-\tCan we ablate the effect of diffusion? i.e., What if we consider CRL concept + MORL without diffusion?\n\n&nbsp;\n\n3. Comparison with other literature in MORL is required.\n-\tIt seems that standard MORL algorithms are missing in the experiment part – Envelope Q-learning, CAPQL, Q-Pensieve, and C-MORL (Liu et al., 2025b).\n-\tLiterature review should be explicitly included in the main paper. \n-\tIs this work the first one using diffusion in MORL?\n\n&nbsp;\n\n4. Discussion on the computation of the proposed algorithm is required - algorithmic complexity and/or wall-clock time – because diffusion model may cause huge computation complexity/time.\n\n&nbsp;\n\n5. Please clarify the details regarding trajectory generation using PCN.\n-\tAny reason for choice of the 4 envs? What if we try less or more env?\n-\tCan we use other MORL algorithm than PCN for trajectory generation?\n-\tHow many trajectories are generated?\n-\tHow h (desired horizon in PCN) is chosen?"}, "questions": {"value": "Please see the Weaknesses above. Here are some minor issues.\n\n-\tPlease use citep appropriately.\n-\tSection 2.1: binary case only in eq. (5), (6)?\n-\tMultiobjective  -> Multi-objective \n-\tMDP -> MOMDP \n-\tPlease use acronym (RL, MORL) appropriately.\n-\tPlease provide code if available."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q4zkHffX3k", "forum": "SPxRgjtsd5", "replyto": "SPxRgjtsd5", "signatures": ["ICLR.cc/2026/Conference/Submission10710/Reviewer_PJ2j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10710/Reviewer_PJ2j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848690944, "cdate": 1761848690944, "tmdate": 1762921945898, "mdate": 1762921945898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}