{"id": "5uwXigCRnB", "number": 4099, "cdate": 1757600264054, "mdate": 1763025204341, "content": {"title": "CLUBench: A Clustering Benchmark", "abstract": "Clustering is a fundamental problem in data science with a long-standing research history. Over the past few decades, numerous clustering algorithms have been developed. However, a systematic and experimental evaluation of these algorithms remains lacking and is urgently needed. To address this gap, we introduce CLUBench, a comprehensive clustering benchmark comprising 23 algorithms of diverse principles evaluated on 131 datasets across tabular, text and image data types. Our extensive experiments (174,485) yield statistically meaningful insights into the performance of various clustering methods, such as the impact of hyperparameter tuning, similarity between algorithms, and the impact of data type and dimension.\nNotably, we observe low-rank characteristics in cross-model performance matrices, which facilitates an efficient strategy for rapid algorithm evaluation and selection in practical applications. Additionally, we provide an easy-to-use toolbox by encapsulating the source codes from the official code repository into a unified framework, accompanied by detailed instructions. With CLUBench, researchers and practitioners can efficiently select appropriate algorithms or datasets for evaluating new datasets or proposed methods. All benchmark datasets and the toolbox are fully open-sourced and available at https://anonymous.4open.science/r/CLUBench-ICLR2026/.", "tldr": "", "keywords": ["Clustering", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/78fb541b2260096ed8d0f8e2043ba062ca8b77f5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CLUBench, a large-scale clustering benchmark tool that evaluates 23 different algorithms that include classical, subspace and deep methods across 131 datasets spanning tabular, text and image modalities while providing results using 4 metrics which are ACC, NMI, ARI and time. The authors also provide some interesting findings like deep learning methods do not consistently dominate classical ones and much more."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper clearly mentions all the algorithms and the datasets that are present in the benchmarking tool along with what the authors are planning to add in the future. This tool also allows the user to add their own datasets and methods.\n\nThe authors also showcase the strengths of the benchmarking tool by giving some insights on the methods such as how deep learning clustering techniques were not giving too much of a significant advantage compared to conventional methods in some cases.\n\nThe authors also clearly mentioned their search spaces of the hyper parameters allowing for greater reproducibility."}, "weaknesses": {"value": "Some of the plots were too overcrowded and hence difficult to read. For example, Figure 2, 4 and 9.\n\nThe authors mentioned that the run time numbers were collected on different hardware. So, the run time comparisons are not too well comparable?\n\nThe authors in order to control the scale of the experiments undersampled the datasets with sample size n >= 10000 samples and also removed the extreme clusters which were also outliers. This may alter some class imbalance. This is not shown as a comparison to the original dataset.\n\nThe authors only give out 4 result metrics: ACC, NMI, ARI and time. All of these metrics except time uses ground truth labels. The authors did not mention why other metrics like SIL, CHI, DBI were not given.\n\nThe authors mention that for non image data, the CNN based methods were adapted to MLP backbones. However, they did not provide any form of comparisons showcasing that performance between both of them were the same.\n\nYour benchmarks assume that we already know the number of clusters in advance. Something to think for the future."}, "questions": {"value": "There is already a project called clustering benchmark (clustbench). See clustering-benchmarks.gagolewski.com Wouldn't be nicer to use another name a bit different rather than CLUbench?\n\nCould you please explain why internal metrics like CHI, SIL were not used?\n\nCould you please explain how much difference in the results are coming when there is no undersampling being done to the data and when there is?\n\nCould you please show the comparison results when CNN based methods were adapted to MLP and the original CNN method?\n\nWhat implementation of spectral clustering are you using? What about memory constraints of spectral clustering?\n\nLooking at the SpeClu code this seems to be a new implementation why to cite a 2001 paper on this? Also why to import the scikit learn version but not use it or use it some times?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jZypQjNtyp", "forum": "5uwXigCRnB", "replyto": "5uwXigCRnB", "signatures": ["ICLR.cc/2026/Conference/Submission4099/Reviewer_y2gF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4099/Reviewer_y2gF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761688430127, "cdate": 1761688430127, "tmdate": 1762917177787, "mdate": 1762917177787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Dear AC and Reviewers\n\nWe appreciate the efforts from you and will continue to improve our work.\n\nSincerely,\nAll authors"}}, "id": "LzSXFMJfaV", "forum": "5uwXigCRnB", "replyto": "5uwXigCRnB", "signatures": ["ICLR.cc/2026/Conference/Submission4099/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4099/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763025203392, "cdate": 1763025203392, "tmdate": 1763025203392, "mdate": 1763025203392, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce CLUBench, performing a big benchmarking study for existing clustering algorithms. They evaluate 23 clustering methods (conventional as well as deep clustering algorithms) across 131 datasets (incl. tabular, text, image, and bioinformatics data). \nThey compare the clustering results for various hyperparameter settings regarding NMI, ARI, and Accuracy and try to find groups of clustering methods that perform similar. They furthermore investigate hyperparameter robustness and whether dataset properties like dimensionality or imbalance of classes influences the performance of clustering methods in a predictale way. The latter analyses are mostly performed by visually analysing t-SNE embeddings of the performances accross datasets and hyperparameter settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "S1) Neutral benchmarks are really important in nowadays research. The authors investigated a large number of datasets and clustering methods, including a range of hyperparameter settings. \n\nS2) The paper is easy to follow and the authors derive several qualitative outcomes, e.g., that deep clustering is not necessarily better than traditional clustering. \n\nS3) The results seem reproducible and a lot of tables resulting from the benchmark study are given in the appendix."}, "weaknesses": {"value": "W1) The selection of ranges for hyperparameters is suboptimal and sometimes not clear. E.g., according to Table 9, k was not tuned for k-Means. While one can assume the authors used the ground truth number of clusters, this leads to a biased evaluation regarding the hyperparameter robustness in Figure 10. \n\nWhile the authors tune DBSCAN parameters in a non-trivially chosen parameter range, they could have used existing literature [0] as a basis instead- especially as their method quite often leads to very poor clustering results for DBSCAN. I.e., regard k-distance values instead of arbitrarily chosen percentages of average pairwise distances.\n\n[0] Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. ACM Transactions on Database Systems (TODS), 42(3), 1-21.\n\nW2) Evaluation of clusterings with noise is not clear. The authors define in 3.1 that each point belongs to a cluster. However, noise-detecting methods like DBSCAN leave points unassigned. It is not clear how the authors handled this when computing NMI and ARI. \n\nW3) Inferring conclusions based on t-SNE embeddings is problematic and while they give nice ideas, I do not think they are scientifically reliable. \n\nW4) The cleaning process of the data is problematic and not described prominently enough. It might yield significant biases in the evaluation. \n\nW5) In Table 5 many results are not available. However, there is no reason given for that. A\n\nW6) Quite some important information is only contained in the appendix\n\nMinor: Section heading of 4.3.1 is at the bottom of the page without any text following."}, "questions": {"value": "See weak points\n\nQ1) How many of the results were not available and why?\n\nQ2) How did you handle noise labels in the evaluation?\n\nQ3) Why did you choose the search ranges the way you did? Are there guidelines? Did you test different values for k for k-Means?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CPbZ4i5tzw", "forum": "5uwXigCRnB", "replyto": "5uwXigCRnB", "signatures": ["ICLR.cc/2026/Conference/Submission4099/Reviewer_grFx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4099/Reviewer_grFx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929057934, "cdate": 1761929057934, "tmdate": 1762917177542, "mdate": 1762917177542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CLUBench attempts to benchmark 23 clustering algorithms (conventional and deep learning-based) on 131 datasets across tabular, text, and image modalities. The authors report 174,485 experiments with analysis of performance comparisons, similarity studies, and low-rank structure. The large scale benchmark should help practitioners in selecting which algorithm is  suitable for a given task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper represents substantial work, collecting 131 datasets across multiple modalities and conducting extensive experiments. The scale of the evaluation is commendable.\n2. The benchmark aggregates datasets from diverse domains and data types (tabular, text, image, bioinformatics), which could be valuable for the community if implementation issues are resolved."}, "weaknesses": {"value": "1. Despite claims of comprehensiveness, only 23 algorithms are included and existing benchmarking efforts like ClustPy (https://github.com/collinleiber/ClustPy) [1] are neither discussed, nor compared against. \n2. The benchmark is incomplete at submission. In Table 3., multiple methods show \"NA\" for original images, suggesting the authors couldn't run the implementations on original data for some deep clustering algorithms while it worked for others like ConClu or PICA.\n3. The DEC implementation (https://anonymous.4open.science/r/CLUBench-ICLR2026/CLUBench/algorithms/DEC.py) is incomplete. Some implementation files are missing (e.g. utils.py for TwoLayerDAE). Overall, a comparison to the original results and/or implementation is missing to check if the reimplementations for the benchmark are correct.\n3. The selection of algorithms for the benchmark is not well motivated. Out of the many existing (deep) clustering algorithms a what seems to be arbitrary subset was selected without clear motivation.\n\n[1] Leiber, Collin, et al. \"Benchmarking deep clustering algorithms with clustpy.\" 2023 IEEE International Conference on Data Mining Workshops (ICDMW). IEEE, 2023."}, "questions": {"value": "1. How does CLUBench differ from existing benchmarking efforts like ClustPy, and what are the comparative advantages? A direct comparison would strengthen your contribution and justify the need for another benchmark. \n2. Table 1 is incomplete. Can you provide the remaining results?\n3. For each algorithm: are you using official implementations, faithful reimplementations, or modified versions? If these are reimplementations or modified versions, how do you validate that the implementations reproduce the results?\n4. How was the selection of (deep) clustering algorithms done? Given that more recent deep clustering algorithms are missing it should be clearly motivated why the current methods were selected."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RknMl5EmlO", "forum": "5uwXigCRnB", "replyto": "5uwXigCRnB", "signatures": ["ICLR.cc/2026/Conference/Submission4099/Reviewer_jCJA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4099/Reviewer_jCJA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983900906, "cdate": 1761983900906, "tmdate": 1762917177325, "mdate": 1762917177325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new clustering benchmark, CLUBench. Extensive empirical evaluation is performed comparing well known clustering methods across the different benchmark datasets. Deep clustering methods are also considered.  Low-rank characteristics in performance is observed and analyzed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* **Empirical Breadth**: There is a lot of information in the experiments performed. The authors run a large number of algorithms on a large number of benchmarks. \n* **Need for Standardization**: Indeed, I would agree that a clustering benchmark can be a meaningful contribution, even in 2025."}, "weaknesses": {"value": "* **Research Question** - I am a bit confused about the insights gained from the proposed benchmark compared to previous benchmarks. What is it that we learn from this benchmark that we didn't already know in the scientific community? I ask this not to be rude, but because I feel this is the core question that needs to be answered by the paper. Otherwise, the great efforts of the authors will not yield the impact I expect that they want. Table 1 is not sufficient in my opinion.\n* **Benchmark Configuration** - After reading, I am still left with the question about the design space of which datasets should be in the benchmark. There is a dearth of datasets with larger number of clusters. There is a dearth of more modern Generative AI world clustering datasets. There is a heavy focus on Tabular. I'm struggling still with the reasons listed for these datasets.\n* **Algorithm Selection** - Similar question here, what about all distributed clustering techniques? What about methods that are much more scalable and therefore likely to be used by practitioners? \n* **Deep Clustering Results** - I think I have missed something important, why do the deep clustering results look so different than a paper like https://ojs.aaai.org/index.php/AAAI/article/view/26032 ?\n* **Venue Match** - I think that readers in KDD and similar more traditionally data mining conferences could be better suited for this paper."}, "questions": {"value": "1. Please clarify Deep Clustering results?\n2. What is the core observation that this benchmark provides that previous benchmarks did not provide?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pDudNutWuu", "forum": "5uwXigCRnB", "replyto": "5uwXigCRnB", "signatures": ["ICLR.cc/2026/Conference/Submission4099/Reviewer_QqWN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4099/Reviewer_QqWN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4099/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762114664613, "cdate": 1762114664613, "tmdate": 1762917177142, "mdate": 1762917177142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}